o my children  lemar  sivan  and aaron and my nicolette avi silberschatz to my wife  carla  and my children  gwen  owen  and maddie peter baer galvin to my wife  pat  and our sons  tom and jay greg gagne abraham silberschatz is the sidney j weinberg professor & chair of computer science at yale university prior to joining yale  he was the vice president of the information sciences research center at bell laboratories prior to that  he held a chaired professorship in the department of computer sciences at the university of texas at austin  professor silberschatz is an acm fellow and an ieee fellow he received the 2002 ieee taylor l booth education award  the 1998 acm karl v karlstrom outstanding educator award  and the 1997 acm sigmod contribution award in recognition of his outstanding level of innovation and technical excellence  he was awarded the bell laboratories president 's award for three different projects-the qtm project  1998   the datablitz project  1999   and the netlnventory project  2004   professor silberschatz ' writings have appeared in numerous acm and ieee publications and other professional conferences and journals he is a coauthor of the textbook database system concepts he has also written op-ed articles for the new york times  the boston globe  and the hartford courant  among others  peter baer galvin is the chief technologist for corporate technologies  www.cptech.com   a computer facility reseller and integrator before that  mr  galvin was the systems manager for brown university 's computer science department he is also sun columnist for ; login  magazine mr galvin has written articles for byte and other magazines  and has written columns for sun world and sysadmin magazines as a consultant and trainer  he has given talks and taught tutorials on security and system administration worldwide  greg gagne is chair of the computer science department at westminster college in salt lake city where he has been teaching since 1990 in addition to teaching operating systems  he also teaches computer networks  distributed systems  and software engineering he also provides workshops to computer science educators and industry professionals  operating systems are an essential part of any computer system similarly  a course on operating systems is an essential part of any computer-science education this field is undergoing rapid change  as computers are now prevalent in virtually every application  from games for children through the most sophisticated planning tools for governments and multinational firms  yet the fundamental concepts remain fairly clear  and it is on these that we base this book  we wrote this book as a text for an introductory course in operating systems at the junior or senior undergraduate level or at the first-year graduate level  we hope that practitioners will also find it useful it provides a clear description of the concepts that underlie operating systems as prerequisites  we assume that the reader is familiar with basic data struchues  computer organization  and a high-level language  such as c or java the hardware topics required for an understanding of operating systems are included in chapter 1 for code examples  we use predominantly c  with some java  but the reader can still understand the algorithms without a thorough knowledge of these languages  concepts are presented using intuitive descriptions important theoretical results are covered  but formal proofs are omitted the bibliographical notes at the end of each chapter contain pointers to research papers in which results were first presented and proved  as well as references to material for further reading in place of proofs  figures and examples are used to suggest why we should expect the result in question to be true  the fundamental concepts and algorithms covered in the book are often based on those used in existing conunercial operating systems our aim is to present these concepts and algorithms in a general setting that is not tied to one particular operating system we present a large number of examples that pertain to the most popular and the most im1.ovative operating systems  including sun microsystems ' solaris ; linux ; microsoft windows vista  windows 2000  and windows xp ; and apple mac os x when we refer to windows xp as an example operating system  we are implying windows vista  windows xp  and windows 2000 if a feature exists in a specific release  we state this explicitly  vii viii the organization of this text reflects our many years of teaching courses on operating systems consideration was also given to the feedback provided by the reviewers of the text  as well as comments submitted by readers of earlier editions in addition  the content of the text corresponds to the suggestions from computing curricula 2005 for teaching operating systems  published by the joint task force of the ieee computing society and the association for computing machinery  acm   on the supporting web site for this text  we provide several sample syllabi that suggest various approaches for using the text in both introductory and advanced courses as a general rule  we encourage readers to progress sequentially through the chapters  as this strategy provides the most thorough study of operating systems however  by using the sample syllabi  a reader can select a different ordering of chapters  or subsections of chapters   on-line support for the text is provided by wileyplus on this site  students can find sample exercises and programming problems  and instructors can assign and grade problems in addition  in wileyplus  students can access new operating-system simulators  which are used to work through exercises and hands-on lab activities references to the simulators and associated activities appear at the ends of several chapters in the text  the text is organized in nine major parts  overview chapters 1 and 2 explain what operating systems are  what they do  and how they are designed and constructed these chapters discuss what the common features of an operating system are  what an operating system does for the user  and what it does for the computer-system operator the presentation is motivational and explanatory in nature we have avoided a discussion of how things are done internally in these chapters therefore  they are suitable for individual readers or for students in lower-level classes who want to learn what an operating system is without getting into the details of the internal algorithms  process management and process coordination chapters 3 through 7 describe the process concept and concurrency as the heart of modern operating systems a process is the unit of work in a system  such a system consists of a collection of concurrently executing processes  some of which are operating-system processes  those that execute system code  and the rest of which are user processes  those that execute user code   these chapters cover n1.ethods for process scheduling  interprocess communication  process synchronization  and deadlock handling also included is a discussion of threads  as well as an examination of issues related to multicore systems  memory management chapters 8 and 9 deal with the management of main memory during the execution of a process to improve both the utilization of the cpu and the speed of its response to its users  the computer must keep several processes in memory there are many different ix management  and the effectiveness of a particular algorithm depends on the situation  storage management chapters 10 through 13 describe how the file system  mass storage  and i/0 are handled in a modern computer system the file system provides the mechanism for on-line storage of and access to both data and programs we describe the classic internal algorithms and structures of storage management and provide a firm practical understanding of the algorithms used -their properties  advantages  and disadvantages our discussion of storage also includes matters related to secondary and tertiary storage since the i/0 devices that attach to a computer vary widely  the operating system needs to provide a wide range of functionality to applications to allow them to control all aspects of these devices we discuss system i/o in depth  including i/o system design  interfaces  and internal system structures and functions in many ways  i/o devices are the slowest major components of the computer because they represent a performance bottleneck  we also examine performance issues associated with i/0 devices  protection and security chapters 14 and 15 discuss the mechanisms necessary for the protection and security of computer systems the processes in an operating system must be protected from one another 's activities  and to provide such protection  we must ensure that only processes that have gained proper authorization from the operating system can operate on the files  memory  cpu  and other resources of the system  protection is a mechanism for controlling the access of programs  processes  or users to the resources defined by a computer system this mechanism must provide a means of specifying the controls to be imposed  as well as a means of enforcement security protects the integrity of the information stored in the system  both data and code   as well as the physical resources of the system  from 1.mauthorized access  malicious destruction or alteration  and accidental introduction of inconsistency  distributed systems chapters 16 through 18 deal with a collection of processors that do not share memory or a clock-a distributed system by providing the user with access to the various resources that it maintains  a distributed system can improve computation speed and data availability and reliability such a system also provides the user with a distributed file system  which is a file-service system whose users  servers  and storage devices are dispersed among the sites of a distributed system a distributed system must provide various mechanisms for process synchronization and communication  as well as for dealing with deadlock problems and a variety of failures that are not encountered in a centralized system  special-purpose systems chapters 19 and 20 deal with systems used for specific purposes  including real-time systems and multimedia systems  these systems have specific requirements that differ from those of the general-purpose systems that are the focus of the remainder of the text  real-time systems may require not only that computed results be correct but also that the results be produced within a specified deadline period  multimedia systems require quality-of-service guarantees ensuring that the multimedia data are delivered to clients within a specific time frame  x case studies chapters 21 through 23 in the book  and appendices a through c  which are available on www.wiley.comj go i global/ silberschatz and in wileyplus   integrate the concepts described in the earlier chapters by describing real operating systems these systems include linux  windows xp  freebsd  mach  and windows 2000 we chose linux and freebsd because unix-at one time-was almost small enough to understand yet was not a toy operating system most of its internal algorithms were selected for simplicity  rather than for speed or sophistication both linux and freebsd are readily available to computer-science departments  so many students have access to these systems we chose windows xp and windows 2000 because they provide an opporhmity for us to study a modern operating system with a design and implementation drastically different from those of unix chapter 23 briefly describes a few other influential operating systems  this book uses examples of many real-world operating systems to illustrate fundamental operating-system concepts however  particular attention is paid to the microsoft family of operating systems  including windows vista  windows 2000  and windows xp  and various versions of unix  including solaris  bsd  and mac os x   we also provide a significant amount of coverage of the linux operating system reflecting the most recent version of the kernel -version 2.6-at the time this book was written  the text also provides several example programs written in c and java these programs are intended to run in the following programming environments  windows systems the primary programming environment for windows systems is the win32 api  application programming interface   which provides a comprehensive set of functions for managing processes  threads  memory  and peripheral devices we provide several c programs illustrating the use of the win32 api example programs were tested on systems rum1.ing windows vista  windows 2000  and windows xp  posix posix  which stands for portable operating system inte1jace  represents a set of standards implemented primarily for unix-based operating systems although windows vista  windows xp  and windows 2000 systems can also run certain posix programs  our coverage of posix focuses primarily on unix and linux systems posix-compliant systems must implement the posix core standard  posix.1   linux  solaris  and mac os x are examples of posix-compliant systems posix also defines several extensions to the standards  including real-time extensions  posixl.b  and an extension for a threads library  posix1.c  better known as pthreads   we provide several programn1.ing examples written inc illustrating the posix base api  as well as pthreads and the extensions for real-time programming  these example programs were tested on debian linux 2.4 and 2.6 systems  mac os x 10.5  and solaris 10 using the gee 3.3 and 4.0 compilers  java java is a widely used programming language with a rich api and built-in language support for thread creation and management java xi programs run on any operating system supporting a java virtual machine  or jvm   we illustrate various operating system and networking concepts with several java programs tested using the java 1.5 jvm  we have chosen these three programming environments because it is our opinion that they best represent the two most popular models of operating systems  windows and unix/linux  along with the widely used java environment  most programming examples are written in c  and we expect readers to be comfortable with this language ; readers familiar with both the c and java languages should easily understand most programs provided in this text  in some instances-such as thread creation-we illustrate a specific concept using all three programming environments  allowing the reader to contrast the three different libraries as they address the same task in other situations  we may use just one of the apis to demonstrate a concept  for example  we illustrate shared memory using just the posix api ; socket programming in tcp /ip is highlighted using the java api  as we wrote the eighth edition of operating system concepts  we were guided by the many comments and suggestions we received from readers of our previous editions  as well as by our own observations about the rapidly changing fields of operating systems and networking we have rewritten material in most of the chapters by bringing older material up to date and removing material that was no longer of interest or relevance  we have made substantive revisions and organizational changes in many of the chapters most importantly  we have added coverage of open-source operating systems in chapter 1 we have also added more practice exercises for students and included solutions in wileyplus  which also includes new simulators to provide demonstrations of operating-system operation below  we provide a brief outline of the major changes to the various chapters  chapter 1  introduction  has been expanded to include multicore cpus  clustered computers  and open-source operating systems  chapter 2  system structures  provides significantly updated coverage of virtual machines  as well as multicore cpus  the grub boot loader  and operating-system debugging  chapter 3  process concept  provides new coverage of pipes as a form of interprocess communication  chapter 4  multithreaded programming  adds new coverage of programming for multicore systems  chapter 5  process scheduling  adds coverage of virtual machine scheduling and multithreaded  multicore architectures  chapter 6  synchronization  adds a discussion of mutual exclusion locks  priority inversion  and transactional memory  chapter 8  memory-management strategies  includes discussion of numa  xii chapter 9  virtual-memory management  updates the solaris example to include solaris 10 memory managernent  chapter 10  file system  is updated with current technologies and capacities  chapter 11  implementing file systems  includes a full description of sun 's zfs file system and expands the coverage of volumes and directories  chapter 12  secondary-storage structure  adds coverage of iscsi  volumes  and zfs pools  chapter 13  i/0 systems  adds coverage of pcix pci express  and hypertransport  chapter 16  distributed operating systems  adds coverage of 802.11 wireless networks  chapter 21  the limix system  has been updated to cover the latest version of the limix kernel  chapter 23  influential operating systems  increases coverage of very early computers as well as tops-20  cp/m  ms-dos  windows  and the original mac os  to emphasize the concepts presented in the text  we have added several programming problems and projects that use the posix and win32 apis  as well as java we have added more than 15 new programming problems  which emphasize processes  threads  shared memory  process synchronization  and networking in addition  we have added or modified several programming projects that are more involved than standard programming exercises these projects include adding a system call to the linux kernel  using pipes on both unix and windows systems  using unix message queues  creating multithreaded applications  and solving the producer-consumer problem using shared memory  the eighth edition also incorporates a set of operating-system simulators designed by steven robbins of the university of texas at san antonio the simulators are intended to model the behavior of an operating system as it performs various tasks  such as cpu and disk-head schedulil1.g  process creation and interprocess communication  starvation  and address translation these simulators are written in java and will run on any computer systern with java 1.4 students can download the simulators from wileyplus and observe the behavior of several operating system concepts in various scenarios in addition  each simulator includes several exercises that ask students to set certain parameters of the simulator  observe how the system behaves  and then explain this behavior these exercises can be assigned through wileyplus the wileyplus course also includes algorithmic problems and tutorials developed by scott m pike of texas a&m university  xiii the following teaching supplencents are available in wileyplus and on www.wiley.coml go i global/ silberschatz  a set of slides to accompany the book  model course syllabi  all c and java source code  up-to-date errata  three case study appendices and the distributed communication appendix  the wileyplus course also contains the simulators and associated exercises  additional practice exercises  with solutions  not found in the text  and a testbank of additional problems students are encouraged to solve the practice exercises on their own and then use the provided solutions to check their own answers  to obtain restricted supplements  such as the solution guide to the exercises in the text  contact your local j orne wiley & sons sales representative note that these supplements are available only to faculty who use this text  we use the mailman system for communication among the users of operating system concepts if you wish to use this facility  please visit the following url and follow the instructions there to subscribe  http  i i mailman.cs.yale.edul mailmanllistinfo i os-book the mailman mailing-list system provides many benefits  such as an archive of postings  as well as several subscription options  including digest and web only to send messages to the list  send e-mail to  os-book @ cs.yale.edu depending on the message  we will either reply to you personally or forward the message to everyone on the mailing list the list is moderated  so you will receive no inappropriate mail  students who are using this book as a text for class should not use the list to ask for answers to the exercises they will not be provided  we have attempted to clean up every error in this new edition  but-as happens with operating systems-a few obscure bugs may remain we would appreciate hearing from you about any textual errors or omissions that you identify  if you would like to suggest improvements or to contribute exercises  we would also be glad to hear from you please send correspondence to os-book-authors @ cs.yale.edu  this book is derived from the previous editions  the first three of which were coauthored by james peterson others who helped us with previous editions include hamid arabnia  rida bazzi  randy bentson  david black  xiv joseph boykin  jeff brumfield  gael buckley  roy campbell  p c capon  john carpenter  gil carrick  thomas casavant  bart childs  ajoy kum.ar datta  joe deck  sudarshan k dhall  thomas doeppner  caleb drake  m racsit eskicioglu  hans flack  robert fowler  g scott graham  richard guy  max hailperin  rebecca i-iartncan  wayne hathaway  christopher haynes  don heller  bruce hillyer  mark holliday  dean hougen  michael huangs  ahmed kamet marty kewstet richard kieburtz  carol kroll  marty k westet thomas leblanc  john leggett  jerrold leichter  ted leung  gary lippman  carolyn miller  michael molloy  euripides montagne  yoichi muraoka  jim m ng  banu ozden  ed posnak  boris putanec  charles qualline  john quarterman  mike reiter  gustavo rodriguez-rivera  carolyn j c schauble  thomas p  skimcer  yannis smaragdakis  jesse st laurent  john stankovic  adam stauffer  steven stepanek  john sterling  hal stern  louis stevens  pete thomas  david umbaugh  steve vinoski  tommy wagner  larry l wear  jolm werth  james m westall  j s weston  and yang xiang parts of chapter 12 were derived from a paper by hillyer and silberschatz  1996   parts of chapter 17 were derived from a paper by levy and silberschatz  1990   chapter 21 was derived from an unpublished manuscript by stephen tweedie chapter 22 was derived from an unpublished manuscript by dave probert  cliff martin  and avi silberschatz appendix c was derived from an unpublished manuscript by cliff martin cliff martin also helped with updating the unix appendix to cover freebsd some of the exercises and accompanying solutions were supplied by arvind krishnamurthy  mike shapiro  bryan cantrill  and jim mauro answered several solarisrelated questions bryan cantrill from sun microsystems helped with the zfs coverage steve robbins of the university of texas at san antonio designed the set of simulators that we incorporate in wileyplus reece newman of westminster college initially explored this set of simulators and their appropriateness for this text josh dees and rob reynolds contributed coverage of microsoft 's .net the project for posix message queues was contributed by john trona of saint michael 's college in colchester  vermont  marilyn turnamian helped generate figures and presentation slides mark wogahn has made sure that the software to produce the book  e.g  latex macros  fonts  works properly  our associate publisher  dan sayre  provided expert guidance as we prepared this edition he was assisted by carolyn weisman  who managed many details of this project smoothly the senior production editor ken santor  was instrumental in handling all the production details lauren sapira and cindy jolmson have been very helpful with getting material ready and available for wileyplus  beverly peavler copy-edited the manuscript the freelance proofreader was katrina avery ; the freelance indexer was word co  inc  abraham silberschatz  new haven  ct  2008 peter baer galvin  burlington  ma 2008 greg gagne  salt lake city  ut  2008 part one overview chapter 1 introduction 1.1 what operating systems do 3 1.2 computer-system organization 6 1.3 computer-system architecture 12 1.4 operating-system sh ucture 18 1.5 operating-system operations 20 1.6 process management 23 1.7 memory management 24 1.8 storage management 25 chapter 2 system structures 2.1 operating-system services 49 2.2 user operating-system interface 52 2.3 system calls 55 2.4 types of system calls 58 2.5 system programs 66 2.6 operating-system design and implementation 68 2.7 operating-system structure 70 1.9 protection and security 29 1.10 distributed systems 30 1.11 special-purpose systems 32 1.12 computing environments 34 1.13 open-source operating systems 37 1.14 summary 40 exercises 42 bibliographical notes 46 2.8 virtual machines 76 2.9 operating-system debugging 84 2.10 operating-system generation 88 2.11 system boot 89 2.12 summary 90 exercises 91 bibliographical notes 97 part two process management chapter 3 process concept 3.1 process concept 101 3.2 process scheduling 105 3.3 operations on processes 110 3.4 interprocess communication 116 3.5 examples of ipc systems 123 3.6 communication in clientserver systems 128 3.7 summary 140 exercises 141 bibliographical notes 152 xv xvi chapter 4 multithreaded programming 4.1 overview 153 4.2 multithreading models 157 4.3 thread libraries 159 4.4 threading issues 165 chapter 5 process scheduling 5.1 basic concepts 183 5.2 scheduling criteria 187 5.3 scheduling algorithms 188 5.4 thread scheduling 199 5.5 multiple-processor scheduling 200 4.5 operating-system examples 171 4.6 summary 174 exercises 174 bibliographical notes 181 5.6 operating system examples 206 5.7 algorithm evaluation 213 5.8 summary 217 exercises 218 bibliographical notes 222 part three process coordination chapter 6 synchronization 6.1 backgrmmd 225 6.2 the critical-section problem 227 6.3 peterson 's solution 229 6.4 synchronization hardware 231 6.5 semaphores 234 6.6 classic problems of synchronization 239 chapter 7 deadlocks 7.1 system model 283 7.2 deadlock characterization 285 7.3 methods for handling deadlocks 290 7.4 deadlock prevention 291 7.5 deadlock avoidance 294 6.7 monitors 244 6.8 synchronization examples 252 6.9 atomic transactions 257 6.10 summary 267 exercises 267 bibliographical notes 280 7.6 deadlock detection 301 7.7 recovery from deadlock 304 7.8 summary 306 exercises 307 bibliographical notes 310 part four memory management chapter 8 memory-management strategies 8.1 background 315 8.2 swapping 322 8.3 contiguous memory allocation 324 8.4 paging 328 8.5 structure of the page table 337 8.6 segmentation 342 8.7 example  the intel pentium 345 8.8 summary 349 exercises 350 bibliographical notes 354 xvii chapter 9 virtual-memory management 9.1 background 357 9.2 demand paging 361 9.3 copy-on-write 367 9.4 page replacement 369 9.5 allocation of frames 382 9.6 thrashing 386 9.7 memory-mapped files 390 9.8 allocating kernel memory 396 9.9 other considerations 399 9.10 operating-system examples 405 9.11 summary 407 exercises 409 bibliographical notes 416 part five storage management chapter 10 file system 10.1 file concept 421 10.2 access methods 430 10.3 directory and disk structure 433 10.4 file-system mounting 444 10.5 file sharing 446 10.6 protection 451 10.7 summary 456 exercises 457 bibliographical notes 458 chapter 11 implementing file systems 11.1 file-system structure 461 11.2 file-system implementation 464 11.3 directory implementation 470 11.4 allocation methods 471 11.5 free-space management 479 11.6 efficiency and performance 482 11.7 recovery 486 11.8 nfs 490 11.9 example  the wafl file system 496 11.10 summary 498 exercises 499 bibliographical notes 502 chapter 12 secondary-storage structure 12.1 overview of mass-storage structure 505 12.2 disk structure 508 12.3 disk attachment 509 12.4 disk scheduling 510 12.5 disk man.agement 516 12.6 swap-space management 520 chapter 13 i/0 systems 13.1 overview 555 13.2 i/0 hardware 556 13.3 application i/0 interface 565 13.4 kernel i/0 subsystem 571 13.5 transforming i/0 requests to hardware operations 578 12.7 raid structure 522 12.8 stable-storage implementation 533 12.9 tertiary-storage struchue 534 12.10 summary 543 exercises 545 bibliographical notes 552 13.6 streams 580 13.7 performance 582 13.8 summary 585 exercises 586 bibliographical notes 588 xviii part six protection and security chapter 14 system protection 14.1 goals of protection 591 14.2 principles of protection 592 14.3 domain of protection 593 14.4 access matrix 598 14.5 implementation of access matrix 602 14.6 access control 605 chapter 15 system security 15.1 the security problem 621 15.2 program threats 625 15.3 system and network threats 633 15.4 cryptography as a security tool 638 15.5 user authentication 649 15.6 implementing security defenses 654 15.7 firewalling to protect systems and networks 661 14.7 revocation of access rights 606 14.8 capability-based systems 607 14.9 language-based protection 610 14.10 surnmary 615 exercises 616 bibliographical notes 618 15.8 computer-security classifications 662 15.9 an example  windows xp 664 15.10 summary 665 exercises 666 bibliographical notes 667 part seven distributed systems chapter 16 distributed operating systems 16.1 motivation 673 16.2 types of networkbased operating systems 675 16.3 network structure 679 16.4 network topology 683 16.5 communication structure 684 16.6 communication protocols 690 16.7 robustness 694 16.8 design issues 697 16.9 an example  networking 699 16.10 summary 701 exercises 701 bibliographical notes 703 chapter 17 distributed file systems 17.1 background 705 17.2 naming and transparency 707 17.3 remote file access 710 17.4 stateful versus stateless service 715 17.5 file replication 716 17.6 an example  afs 718 17.7 summary 723 exercises 724 bibliographical notes 725 chapter 18 distributed synchronization 18.1 event ordering 727 18.2 mutual exclusion 730 18.3 atomicity 733 18.4 concurrency control 736 18.5 deadlock handling 740 18.6 election algorithms 747 18.7 reaching agreement 750 18.8 summary 752 exercises 753 bibliographical notes 754 part eight special purpose systems chapter 19 real-time systems 19.1 overview 759 19.2 system characteristics 760 19.3 features of real-time kernels 762 19.4 implementing real-time operating systems 764 19.5 real-time cpu scheduling 768 19.6 an example  vxworks 5.x 774 19.7 summary 776 exercises 777 bibliographical notes 777 chapter 20 multimedia systems 20.1 what is multimedia 779 20.2 compression 782 20.3 requirements of multimedia kernels 784 20.4 cpu scheduling 786 20.5 disk scheduling 787 20.6 network management 789 20.7 an example  cineblitz 792 20.8 summary 795 exercises 795 bibliographical notes 797 part nine case studies chapter 21 the linux system 21.1 linux history 801 21.2 design principles 806 21.3 kernel modules 809 21.4 process management 812 21.5 scheduling 815 21.6 memory management 820 21.7 file systems 828 chapter 22 windows xp 22.1 history 847 22.2 design principles 849 22.3 system components 851 22.4 environmental subsystems 874 22.5 file system 878 21.8 input and output 834 21.9 interprocess communication 837 21.10 network structure 838 21.11 security 840 21.12 summary 843 exercises 844 bibliographical notes 845 22.6 networking 886 22.7 programmer interface 892 22.8 sum.mary 900 exercises 900 bibliographical notes 901 chapter 23 influential operating systems 23.1 feature migration 903 23.2 early systems 904 23.3 atlas 911 23.4 xds-940 912 23.5 the 913 23.6 rc 4000 913 23.7 ctss 914 23.8 multics 915 23.9 ibm os/360 915 23.10 tops-20 917 23.11 cp/m and ms/dos 917 23.12 macintosh operating system and windows 918 23.13 mach 919 23.14 other systems 920 exercises 921 xix xx chapter a bsd unix a1 unix history 1 a2 design principles 6 a3 programmer interface 8 a.4 user interface 15 as process management 18 a6 memory management 22 appendix b the mach system b.l history of the mach system 1 b.2 design principles 3 b.3 system components 4 b.4 process management 7 b.s interprocess conununication 13 b.6 memory management 18 appendix c windows 2000 c.1 history 1 c.2 design principles 2 c.3 system components 3 c.4 enviromnental subsystems 19 c.s file system 22 bibliography 923 credits 941 index 943 a7 file system 25 as i/0 system 32 a9 interprocess communication 35 alo summary 40 exercises 41 bibliographical notes 42 b.7 programmer interface 23 b.s summary 24 exercises 25 bibliographical notes 26 credits 27 c.6 networking 28 c.7 programmer interface 33 c.s summary 40 exercises 40 bibliographical notes 41 part one an operating system acts as an intermediary between the user of a computer and the computer hardware the purpose of an operating system is to provide an environment in which a user can execute programs in a convenient and efficient manner  an operating system is software that manages the computer hardware  the hardware must provide appropriate mechanisms to ensure the correct operation of the computer system and to prevent user programs from interfering with the proper operation of the system  internally  operating systems vary greatly in their makeup  since they are organized along many different lines the design of a new operating system is a major task it is impmtant that the goals of the system be well defined before the design begins these goals form the basis for choices among various algorithms and strategies  because an operating system is large and complex  it must be created piece by piece each of these pieces should be a well delineated portion of the system  with carefully defined inputs  outputs  and functions  1.1 ch er an is a program that manages the computer hardware it also provides a basis for application programs and acts as an intermediary between the computer user and the computer hardware an amazing aspect of operating systems is how varied they are in accomplishing these tasks  mainframe operating systems are designed primarily to optimize utilization of hardware personal computer  pc  operating systems support complex games  business applications  and everything in between operating systems for handheld computers are designed to provide an environment in which a user can easily interface with the computer to execute programs thus  some operating systems are designed to be convenient  others to be efficient  and others some combination of the two  before we can explore the details of computer system operation  we need to know something about system structure we begin by discussing the basic functions of system startup  i/0  and storage we also describe the basic computer architecture that makes it possible to write a functional operating system  because an operating system is large and complex  it must be created piece by piece each of these pieces should be a well-delineated portion of the system  with carefully defined inputs  outputs  and functions in this chapter  we provide a general overview of the major components of an operating system  to provide a grand tour of the major components of operating systems  to describe the basic organization of computer systems  we begin our discussion by looking at the operating system 's role in the overall computer system a computer system can be divided roughly into 3 4 chapter 1 compiler assembler text editor operating system database system figure 1.1 abstract view of the components of a computer system  four components  the hardware/ the operating system  the application programs/ and the users  figure 1.1   the hardwa.te-the the and the ievices-provides the basic computing resources for the system the as word processors/ spreadsheets/ compilers  and web browsers-define the ways in which these resources are used to solve users ' computing problems the operating system controls the hardware and coordinates its use among the various application programs for the various users  we can also view a computer system as consisting of hardware/ software/ and data the operating system provides the means for proper use of these resources in the operation of the computer system an operating system is similar to a government like a government  it performs no useful function by itself it simply provides an environment within which other programs can do useful work  to understand more fully the operating systemfs role  we next explore operating systems from two viewpoints  that of the user and that of the system  1.1.1 user view the user 's view of the computer varies according to the interface being used most computer users sit in front of a pc  consisting of a monitor/ keyboard/ mouse  and system unit such a system is designed for one user to monopolize its resources the goal is to maximize the work  or play  that the user is performing in this case/ the operating system is designed mostly for with some attention paid to performance and none paid to various hardware and software resources are shared performance is  of course  important to the user ; but such systems 1.1 5 are optimized for the single-user experience rather than the requirements of multiple users  in other cases  a user sits at a terminal connected to a or a other users are accessing the sance computer through other terminals these users share resources and may exchange information the operating system in s llclc cases is designed to maximize resource utilizationto assure that all available cpu time  memory  and i/0 are used efficiently and tbat no individual user takes more than her fair share  in still otber cases  users sit at connected to networks of other workstations and these users have dedicated resources at their disposal  but they also share resources such as networking and servers-file  compute  and print servers therefore  their operating system is designed to compromise between individual usability and resource utilization  recently  many varieties of handheld computers have come into fashion  most of these devices are standalone units for individual users some are connected to networks  either directly by wire or  more often  through wireless modems and networking because of power  speed  and interface limitations  they perform relatively few remote operations their operating systems are designed mostly for individual usability  but performance per unit of battery life is important as well  some computers have little or no user view for example  embedded computers in home devices and automobiles may have numeric keypads and may turn indicator lights on or off to show status  but they and their operating systems are designed primarily to run without user intervention  1.1.2 system view from the computer 's point of view  the operating system is the program most intimately involved with the hardware in this context  we can view an operating system as a  a computer system has many resources that may be required to solve a problem  cpu time  memory space  file-storage space  i/0 devices  and so on the operating system acts as the manager of these resources facing numerous and possibly conflicting requests for resources  the operating system must decide how to allocate them to specific programs and users so that it can operate the computer system efficiently and fairly as we have seen  resource allocation is especially important where many users access the same mainframe or minicomputer  a slightly different view of an operating system emphasizes the need to control the various i/0 devices and user programs an operating system is a control program a manages the execution of user programs to prevent errors and improper use of the computer it is especially concerned with the operation and control of i/o devices  1.1.3 defining operating systems we have looked at the operating system 's role from the views of the user and of the system how  though  can we define what an operating system is in general  we have no completely adequate definition of an operating system operating systems exist because they offer a reasonable way to solve the problem of creating a usable computing system the fundamental goal of computer systems is to execute user programs and to make solving user 6 chapter 1 1.2 storage definitions and notation a is the basic unit of computer storage it can contain one of two values  zero and one all other storage in a computer is based on collections of bits  given enough bits  it is amazing how many things a computer can represent  numbers  letters  images  movies  sounds  documents  and programs  to name a few a is 8 bits  and on most computers it is the smallest convenient chunk of storage for example  most computers do n't have an instruction to move a bit but do have one to move a byte a less common term is which is a given computer architecture 's native storage unit a word is generally made up of one or more bytes for example  a computer may have instructions to move 64-bit  8-byte  words  a kilobyte  or kb  is 1,024 bytes ; a megabyte  or mb  is 1,0242 bytes ; and a gigabyte  or gb  ! s 1,0243 bytes computer manufacturers often round off these numbers and say that a megabyte is 1 million bytes and a gigabyte is 1 billion bytes  problems easier toward this goal  computer hardware is constructed since bare hardware alone is not particularly easy to use  application programs are developed these programs require certain common operations  such as those controlling the ii 0 devices the common functions of controlling and allocating resources are then brought together into one piece of software  the operating system  in addition  we have no universally accepted definition of what is part of the operating system a simple viewpoint is that it includes everything a vendor ships when you order the operating system the features included  however  vary greatly across systems some systems take up less than 1 megabyte of space and lack even a full-screen editor  whereas others require gigabytes of space and are entirely based on graphical windowing systems a more common definition  and the one that we usually follow  is that the operating system is the one program running at all times on the computer-usually called the   along with the kernel  there are two other types of programs  which are associated with the operating system but are not part of the kernel  and which include all programs not associated with the operation of the system  the matter of what constitutes an operating system has become increasingly important in 1998  the united states deparhnent of justice filed suit against microsoft  in essence claiming that microsoft included too much functionality in its operating systems and thus prevented application vendors from competing for example  a web browser was an integral part of the operating systems as a result  microsoft was found guilty of using its operating-system monopoly to limit competition  before we can explore the details of how computer systems operate  we need general knowledge of the structure of a computer system in this section  we look at several parts of this structure the section is mostly concerned 1.2 the study ofoperating systems there has neverbeenarnore interestirighnwtostud yoperating systems  and it has neverb.een.e ~ sier.theopen-sourc ; e movernent has overtaken .operating systems  caj.tsing marly ofthenctobemadeavailable in both source and binary  e ~ ecuta  jle  fonnat .this iistindud ~ ~ linu    bsdunix/solat is,and part of  \ ii ~ cos.x th ~ availa ~ ilityqf source.code.q,llowsus.tostudyoperq,til  .gsy tems frorrt theinsid,eout '  questionsthat previo  1sly could onlyb ~ answerecl ~ y looking atdocumentaticmor thebehayior.ofan op ~ rating system c.annow be answered by examining the code itself  in additi n the rise of virtualization as a ll  .ainsfreafll  andfrequelltly free  cmnp  1ter ftmctionmakesitpos ; ~ i1jlet   runnmnyoperqtingsystems.ontop.of onecoresystem  forexample,vmware  j  lttp  //www  vmware   .com   provides afree ''player' ' on which hundreds.of free .''virtualappliilnces' ' cann.m.using this method,students call tryolit hundreds ofoperatingsystems.withintheir existing operatingsystems .atno cost          operating .sy ~ temsthat are no lortge ~ ~ ofllmerci ~ lly viableltave been opell ~ o  lrced asvvell  enablirtg .usto study how system ~ pperated i ~ time.of  f ~ v.r ~ r cpu  ll  .emory  etnd.storcrge .resoj.trces  .an  exten ~ iye.b  .it not complete   list   f 9pen'-sourct operafirtg system pr j ~ ts is  availa ~ le rom ht ~ p  // dm   ~ ' org/ c  omp  1ters/softp  lre /operati g  -systems/p ~ ~ m._sourc ~ / s i..m   .u l a t .o r s  o .f s  p e  c i ..f i c  ....h a  ...r ....d w  .a  r e   ar..e  .a l s .o   a   v  a.i.l 1 b  .le  .i n    s om   .e   .c  a  s e s '  al i ....o w   m ...g th ~ operat ~ g systell  .to.runon.''na ~ ve''.hardware   all ~ ithrrtthec l  .fines of a modem co ! tipj-iter and moderj1 opf/'atirtg ~ ystem for  example  a decsystemc20 simulator running on mac os x can boot tops-20  loa ~  the ~ ource.tages ;  and modify al'ld comp ~ le l .j  t.evvtops-20 .k ~ rnel art interested stltdent ~ ar search theint ~ rnet to find the origillal papers that de ~ cribe the operating systemand  the.origipa ~ manuals  tl e adve ~ t fogen-source operafirtg sy ~ te1tis also l   lal es it easy t .make the move fromstu ~ enttooper  lting ~ systemdeveloper.with some knov.rledge  som ~ effo1't  a11d an internet connection,a student c ; al'leven create a new operating-systemdistribution ! justa fev.r years  ~ go itwas diffic  _llt or if1lpossible to get acce ~ s  to source co e   n v.r  that access is lijnited only bylt   wmuchtimeand disk space a student has  7 with computer-system organization  so you can skim or skip it if you already understand the concepts  1.2.1 computer-system operation a modern general-purpose computer system consists of one or more cpus and a number of device controllers connected through a common bus that provides access to shared memory  figure 1.2   each device controller is in charge of a specific type of device  for example  disk drives  audio devices  and video displays   the cpu and the device controllers can execute concurrently  competing for memory cycles to ensure orderly access to the shared memory  a memory controller is provided whose function is to synchronize access to the memory  for a computer to start rum ing-for instance  when it is powered up or rebooted-it needs to have an initial program to run this initial 8 chapter 1 mouse keyboard printer monitor o ~ ~ ~  _rlo i-nneh b figure 1.2 a modern computer system  program  or tends to be simple typically  it is stored in read-only memory or electrically erasable programmable read-only memory known by the general term within the computer hardware it initializes all aspects of the system  from cpu registers to device controllers to memory contents the bootstrap program must know how to load the operating system and how to start executing that system to accomplish this goal  the bootstrap program must locate and load into memory the operatingsystem kernel the operating system then starts executing the first process  such as init  and waits for some event to occur  the occurrence of an event is usually signaled by an from either the hardware or the software hardware may trigger an interrupt at any time by sending a signal to the cpu  usually by way of the system bus software may trigger an interrupt executing a special operation called a  also called a when the cpu is interrupted  it stops what it is doing and immediately transfers execution to a fixed location the fixed location usually contains the starting address where the service routine for the interrupt is located  the interrupt service routine executes ; on completion  the cpu resumes the interrupted computation a time line of this operation is shown in figure 1.3  interrupts are an important part of a computer architecture each computer design has its own interrupt mechanism  but several functions are common  the interrupt must transfer control to the appropriate interrupt service routine  the straightforward method for handling this transfer would be to invoke a generic routine to examine the interrupt information ; the routine  in turn  would call the interrupt-specific handler however  interrupts must be handled quickly since only a predefined number of interrupts is possible  a table of pointers to interrupt routines can be used instead to provide the necessary speed the interrupt routine is called indirectly through the table  with no intermediate routine needed generally  the table of pointers is stored in low memory  the first hundred or so locations   these locations hold the addresses of the interrupt service routines for the various devices this array  or of addresses is then indexed by a unique device number  given with the interrupt request  to provide the address of the interrupt service routine for cpu user 1/0 device process executing 1/0 interrupt processing idle ~ ~  ~  tmcefeniog i l  1/0 request 1.2 ll v  ~ ~ ' ''''' ' ~ ' ' '  ~ ~  ~ ~   t ~  ' 'm '  l  ~ ~ ~ ~ transfer done 1/0 transfer request done figure 1.3 interrupt time line for a single process doing output  9 the interrupting device operating systems as different as windows and unix dispatch interrupts in this manner  the interrupt architecture must also save the address of the interrupted instruction many old designs simply stored the interrupt address in a fixed location or in a location indexed by the device number more recent architectures store the return address on the system stack if the interrupt routine needs to modify the processor state-for instance  by modifying register values-it must explicitly save the current state and then restore that state before returning after the interrupt is serviced  the saved return address is loaded into the program counter  and the interrupted computation resumes as though the interrupt had not occurred  1.2.2 storage structure the cpu can load instructions only from memory  so any programs to run must be stored there general-purpose computers run most of their programs from rewriteable memory  called main memory  also called or ram   main commonly is implemented in a semiconductor technology called computers use other forms of memory as well because the read-only memory  rom  camwt be changed  only static programs are stored there the immutability of rom is of use in game cartridges eeprom camwt be changed frequently and so contains mostly static programs for example  smartphones have eeprom to store their factory-il stalled programs  all forms of memory provide an array of words each word has its own address interaction is achieved through a sequence of load or store instructions to specific memory addresses the load instruction moves a word from main memory to an internal register within the cpu  whereas the store instruction moves the content of a register to main memory aside from explicit loads and stores  the cpu automatically loads instructions from main memory for execution  a typical instruction-execution cycle  as executed on a system with a architecture  first fetches an il1struction from memory and stores that instruction in the  the instruction is then decoded and may cause operands to be fetched from memory and stored in some 10 chapter 1 internal register after the instruction on the operands has been executed  the result may be stored back in memory notice that the memory unit sees only a stream of memory addresses ; it does not know how they are generated  by the instruction counter  indexing  indirection  literal addresses  or some other means  or what they are for  instructions or data   accordingly  we can ignore how a memory address is generated by a program we are interested only in the sequence of memory addresses generated by the running program  ideally  we want the programs and data to reside in main ncemory permanently this arrangement usually is not possible for the following two reasons  main memory is usually too small to store all needed programs and data permanently  main memory is a volatile storage device that loses its contents when power is turned off or otherwise lost  thus  most computer systems provide as an extension of main memory the main requirement for secondary storage is that it be able to hold large quantities of data permanently  the most common secondary-storage device is a which provides storage for both programs and data most programs  system and application  are stored on a disk until they are loaded into memory many programs then use the disk as both the source and the destination of their processing hence  the proper management of disk storage is of central importance to a computer system  as we discuss in chapter 12  in a larger sense  however  the storage structure that we have describedconsisting of registers  main memory  and magnetic disks-is only one of many possible storage systems others include cache memory  cd-rom  magnetic tapes  and so on each storage system provides the basic functions of storing a datum and holding that datum until it is retrieved at a later time the main differences among the various storage systems lie in speed  cost  size  and volatility  the wide variety of storage systems in a computer system can be organized in a hierarchy  figure 1.4  according to speed and cost the higher levels are expensive  but they are fast as we move down the hierarchy  the cost per bit generally decreases  whereas the access time generally increases this trade-off is reasonable ; if a given storage system were both faster and less expensive than another-other properties being the same-then there would be no reason to use the slower  more expensive memory in fact  many early storage devices  including paper tape and core memories  are relegated to museums now that magnetic tape and have become faster and cheaper the top four levels of memory in figure 1.4 may be constructed using semiconductor memory  in addition to differing in speed and cost  the various storage systems are either volatile or nonvolatile as mentioned earlier  loses its contents when the power to the device is removed in the absence of expensive battery and generator backup systems  data must be written to for safekeeping in the hierarchy shown in figure 1.4  the the electronic disk are volatile  whereas those below 1.3 15 figure 1.6 symmetric multiprocessing architecture  solaris the benefit of this model is that many processes can run simultaneously -n processes can run if there are n cpus-without causing a significant deterioration of performance however  we must carefully control i/0 to ensure that the data reach the appropriate processor also  since the cpus are separate  one may be sitting idle while another is overloaded  resulting in inefficiencies these inefficiencies can be avoided if the processors share certain data structures a multiprocessor system of this form will allow processes and resources-such as memory-to be shared dynamically among the various processors and can lower the variance among the processors such a system must be written carefully  as we shall see in chapter 6 virtually all modern operating systems-including windows  windows xp  mac os x  and linux -now provide support for smp  the difference between symmetric and asymmetric multiprocessing may result from either hardware or software special hardware can differentiate the multiple processors  or the software can be written to allow only one master and multiple slaves for instance  sun 's operating system sunos version 4 provided asymmetric multiprocessing  whereas version 5  solaris  is symmetric on the same hardware  multiprocessing adds cpus to increase computing power if the cpu has an integrated memory controller  then adding cpus can also increase the amount of memory addressable in the system either way  multiprocessing can cause a system to change its memory access model from uniform memory access to non-uniform memory access uma is defined as the situation in which access to any ram from any cpu takes the same amount of time with numa  some parts of memory may take longer to access than other parts  creating a performance penalty operating systems can minimize the numa penalty through resource management_  as discussed in section 9.5.4  a recent trend in cpu design is to in.clude multiple computing on a single chip in essence  these are multiprocessor chips they can be more efficient than multiple chips with single cores because on-chip communication is faster than between-chip communication in addition  one chip with multiple cores uses significantly less power than multiple single-core chips as a result  multicore systems are especially well suited for server systems such as database and web servers  16 chapter 1 figure 1.7 a dual-core design with two cores placed on the same chip  in figure 1.7  we show a dual-core design with two cores on the same chip in this design  each core has its own register set as well as its own local cache ; other designs might use a shared cache or a combination of local and shared caches aside from architectural considerations  such as cache  memory  and bus contention  these multicore cpus appear to the operating system as n standard processors this tendency puts pressure on operating system designers-and application programmers-to make use of those cpus  finally  are a recent development in which multiple processor boards  i/0 boards  and networking boards are placed in the same chassis  the difference between these and traditional multiprocessor systems is that each blade-processor board boots independently and runs its own operating system some blade-server boards are n1.ultiprocessor as well  which blurs the lines between types of computers in essence  these servers consist of multiple independent multiprocessor systems  1.3.3 clustered systems another type of multiple-cpu system is the like multiprocessor systems  clustered systems gather together multiple cpus to accomplish computational work clustered systems differ from multiprocessor systems  however  in that they are composed of two or more individual systems-or nodes-joined together the definition of the term clustered is not concrete ; many commercial packages wrestle with what a clustered system is and why one form is better than another the generally accepted definition is that clustered computers share storage and are closely linked via a jc'.h.a  o.x  as described in section 1.10  or a faster interconnect  such as infiniband  clustering is usually used to provide service ; that is  service will continue even if one or more systems in the cluster fail high availability is generally obtained by adding a level of redundancy in the system a layer of cluster software runs on the cluster nodes each node can monitor one or more of the others  over the lan   if the monitored machine fails  the monitoring machine can take ownership of its storage and restart the applications that were running on the failed machine the users and clients of the applications see only a brief interruption of service  1.3 beowulf clusters beowulf clusters are designed for solving high-performance computing tasks these clusters are built using comm.odi ty hard ware-such as personal computers-that are connected via a simple local area network interestingly  a beowulf duster uses no one specific software package but rather consists of a set of open-source software libraries that allow the con1puting nodes in the cluster to communicate with one another  thus,.there are a variety of approaches for constructing a beowulf cluster  although beowulf computing nodes typically run the linux operating system since beowulf clusters require no special hardware and operate using open ~ source software that is freely available  they offer a low-cost strategy for building a high ~ performance computing cluster in fact  some beowulf clusters built from collections of discarded personal computers are using ht.mdreds of cornputing nodes to solve computationally expensive problems in scientific computing  clusterin.g can be structured or symmetrically in 17 one machine is in while the other is rmming the applications the hot-standby host machine does nothing but monitor the active server if that server fails  the hot-standby host becomes the active server in two or more hosts are rmming applications and are monitoring each other this mode is obviously more efficient  as it uses all of the available hardware it does require that more than one application be available to run  as a cluster consists of several clusters may also be used to provide environments  such systems can supply significantly greater computational power than single-processor or even smp systems because they are capable of running an application concurrently on all computers in the cluster however  applications must be written to take advantage of the cluster by using a technique known as which consists of dividing a program into separate components that run in parallel on individual computers in the cluster typically  these applications are designed so that once each computing node in the cluster has solved its portion of the problem  the results from all the nodes are combined into a final solution  other forms of clusters include parallel clusters and clustering over a wide-area network  wan   as described in section 1.10   parallel clusters allow multiple hosts to access the same data on the shared storage because most operating systems lack support for simultaneous data access by multiple hosts  parallel clusters are usually accomplished by use of special versions of software and special releases of applications for example  oracle real application cluster is a version of oracle 's database that has been designed to run on a parallel cluster each machine runs oracle  and a layer of software tracks access to the shared disk each machine has full access to all data in the database to provide this shared access to data  the system must also supply access control and locking to ensure that no conflicting operations occur this function  commonly known as a is included in some cluster technology  18 chapter 1 1.4 interconnect interconnect computer computer computer figure 1.8 general structure of a clustered system  cluster technology is changing rapidly some cluster products support dozens of systems in a cluster  as well as clustered nodes that are separated by miles many of these improvements are made possible by  saj ~ is   as described in section 12.3.3  which allow many systems to attach to a pool of storage if the applications and their data are stored on the san  then the cluster software can assign the application to run on any host that is attached to the san if the host fails  then any other host can take over in a database cluster  dozens of hosts can share the same database  greatly increasing performance and reliability figure 1.8 depicts the general structure of a clustered system  now that we have discussed basic information about computer-system organization and architecture  we are ready to talk about operating systems  an operating system provides the envirorunent within which programs are executed internally  operating systems vary greatly in their makeup  since they are organized along many different lines there are  however  many commonalities  which we consider in this section  one of the most important aspects of operating systems is the ability to multiprogram a single program can not  in generat k ~ ~ p ~ ith_er thg cpu ortbt j/qgey  ic  es 1jusy_c1t all times  single users frequently have multiple programs running il.ul increases cpu utilization byorganizing jobs  codeand datafso      _ hasoi1  0to execl1te   fhe idea is as follows  the op-ei  atlng system keeps several jobs in memory simultaneously  figure 1.9   since  in generat main memory is too small to accommodate all jobs  the jobs are kept initially on the disk in the this pool consists of all processes residing on disk awaiting allocation of main memory  ih ~ setofjobs inmemg_ry_canbe asubt  ; et of the jobs kept in thejql  jpoo1  the operating system picks and begins to execute one of the jobs in memory  eventually  the job may have to wait for some task  such as an i/o operation  1.4 19 figure 1.9 memory layout for a multiprogramming system  !   _c   _tnpl ~ te  in a non-multiprogrammed system  the cpu would sit idle in a multiprogrammed system  the operatilcg system simply switches to  and executes  another job when that job needs to wait  the cpu is switched to another job  and so on eventually the first job finishes waiting and gets the cpu back as long as at least one job needs to execute  the cpu is never idle  this idea is common in other life situations a lawyer does not work for only one client at a time  for example while one case is waiting to go to trial or have papers typed  the lawyer can work on another case if he has enough clients  the lawyer will never be idle for lack of work  idle lawyers tend to become politicians  so there is a certain social value in keeping lawyers busy  multiprogrammed systems provide an environment in which the various system resources  for example  cpu  memory  and peripheral devices  are utilized effectively  but they do not provide for user interaction with the computer system is_ ~ l   gi ~ alex_tension of multiprogramming ~ ' time-s ! caring syste ~ s,the cpl  execu ~ eslnl1ltiplejobs by switcll.ing ~ ainong them  but the switches occur so frequently that the ~ 1sers canh ~ teract with eachprograffi ~ v ere l.t1sil.mning. -ti1ne shar  il ~ g requi.i-es an    or  which provides direct communication between the user and the system the user gives instructions to the operating system or to a program directly  using a input device such as a keyboard or a mouse  and waits for immediate results on an output device accordingly  ! ! 'te sho ~ 1ld be sh   rt = typically less than one second  a time-shared operating system allows many users to share the computer simultaneously since each action or command in a time-shared system tends to be short  only a little cpu time is needed for each user as the system switches rapidly from one user to the next  each user is given the impression that the entire computer system is dedicated to his use  even though it is being shared among many users  a time-shared operating system 11ses cpu scheduling and multiprogramming to provide each user with a small portion of a time-shared computer  eachuserhas atleast or  t_e s parateprogra111inmemory a program loaded into 20 1.5 chapter 1 memory and executing is called a when a process executes  it typically executes for only a short tirne it either finishes or needs to perform i/0  i/0 may be interactive ; that is  output goes to a display for the user  and input comes from a user keyboard  mouse  or other device since interactive i/0 typically runs at people speeds  it may take a long time to complete input  for example  may be bounded by the user 's typing speed ; seven characters per second is fast for people but incredibly slow for computers rather than let the cpu sit idle as this interactive input takes place  the operating system will rapidly switch the cpu to the program of some other user  time sharing and multiprogramming require that several jobs be kept simultaneously in memory if several jobs are ready to be brought into memory  and if there is not enough room for all of them  then the system must choose among them making this decision is which is discussed in chapter 5 when the operating system selects a job from the job pool  it loads that job into memory for execution having several programs in memory at the same time requires some form of memory management  which is covered in chapters 8 and 9 in addition  ! f_s ~ verajjq  jsaxere  lcly to rw ~ at the same time  the system must choose among them making this decision i ~ _ _ sd1,2dviii lg  which is discussed in chapter 5 finally  running multiple jobscoi ~ cl.lrl  ei1hy requires that their ability to affect one another be limited in all phases of the operating system  including process scheduling  disk storage  and memory management these considerations are discussed throughout the text  in a time-sharing system  the operating system must ensure reasonable response time  which is sometimes accomplished through where processes are swapped in and out of main memory to the disk a more common method for achieving this goal tec  hdiql1 ~ _fuc ! t _ cillqws._ the execution of aprocessthat isnot completely inl1le1yl_cld ~   chapter 9   the main advai1tage of the virtual-memory scheme is that it enables users to run programs that are larger than actual  further  it abstracts main memory into a large  uniform array of storage  separating logical as viewed by the user from physical memory this arrangement frees programmers from concern over memory-storage limitations  time-sharing systems must also provide a file system  chapters 10 and 11   the file system resides on a collection of disks ; hence  disk management must be provided  chapter 12   also  time-sharing systems provide a mechanism for protecting resources from inappropriate use  chapter 14   to ensure orderly execution  the system must provide mechanisms for job synchronization and communication  chapter 6   and it may ensure that jobs do not get stuck in a deadlock  forever waiting for one another  chapter 7    \ si1  e11tio11ecl ~ arlier  rn   cletnopexatli1ksystems_m ~ e _ if there are no processes to execute  no i/0 devices to service  and no users to whom to respond  an operating system will sit quietly waiting for something to happen events are almost always signaled by the occurrence of an interrupt or a trap  or an is_ a software ~ generated interruptca ~ seci ~ it  ler byan error  for division byzero or invalid memory acc ~ ss_  or by a specific request from a user program that an operating-system service 1.5 21 be performed the interrupt-driven nature of an operating system defines that system 's general structure for each type of interrupt  separate segments of code in the operating system determine what action should be taken an interrupt service routine is provided that is responsible for dealing with the interrupt  since the operating system and the users share the hardware and software resources of the computer system  we need to make sure that an error in a user program could cause problems only for the one program running with sharing  many processes could be adversely affected by a bug in one program  for example  if a process gets stuck in an infinite loop  this loop could prev.ent the correct operation of many other processes more subtle errors can occur in a multiprogramming system  where one erroneous program might modify another program  the data of another program  or even the operating system itself  without protection against these sorts of errors  either the computer must execute only one process at a time or all output must be suspect a properly designed operating system must ensure that an incorrect  or malicious  program can not cause other program ~ to  ~ x.t ; cute incorrectly  ~ ~  ; ~ ,_c  ; ..c ~ 1.5.1 dual-mode operation in order to ensure the proper execution of the operating system  we must be able to distinguish between the execution of operating-system code and userdefined code the approach taken by most computer systems is to provide hardware support that allows us to differentiate among various modes of execution  at the very least we need two and  also called or a bit  called the is added to the hardware of the computer to indicate the current mode  kernel  0  or user  1   \ ! viththeplode1  jit \ ! ve2lrea  jle to distinguishbetween a task that is executed onbehalf of the operating system aicd one that is executeci on behalfofthejjser  when tl ~ e computer systel.n1s executing on behalf of a user application  the system is in user mode however  when a user application requests a service from the operating system  via a  system call   it must transition from user to kernel mode to fulfill the request  / this is shown in figure 1.10 as we shall see  this architectural enhancement is useful for many other aspects of system operation as well  execute system call figure 1 i 0 transition from user to kernel mode  user mode  mode bit = i  kernel mode  mode bit = 0  22 chapter 1 at system boot time  the hardware starts in kernel mode the operating system is then loaded and starts user applications in user mode whenever a trap or interrupt occurs  the hardware switches from user mode to kernel mode  that is  changes the state of the mode bit to 0   thus  whenever the operating system gains control of the computer  it is in kernel mode the system always switches to user mode  by setting the mode bit to 1  before passing control to a user program  the dual mode of operation provides us with the means for protecting the operating system from errant users-and errant users from one another  ye _  ! cc011lplishthis protection by designating some ofthe machineine ; tructions ~ ha !  trlijjt cal1_sej ~ i  i ~ l11 ins trucrci \   l  il1e hardware all ~ \ \ 'spl iyileg ~ d instrl  ctionsto be o11ly inkern ~ ll11qq_ ~  if an attempt is made to execute a privileged instruction in user mode  the hardware does not execute the instruction but rather treats it as illegal and traps it to the operating system  the instruction to switch to kernel mode is an example of a privileged instruction some other examples include i/0 controt timer management and interrupt management as we shall see throughout the text  there are many additional privileged instructions  we can now see the life cycle of instruction execution in a computer system  initial control resides in the operating system  where instructions are executed in kernel mode when control is given to a user application  the mode is set to user mode eventually  control is switched back to the operating system via an interrupt  a trap  or a system call  _5ysiemcalls proyide the means for auser program to ask the operating 2  'st ~ m to perforp  t tasks re_ erved forjhe operating syst ~ m gr1 the 1.lser .12l  qgra1ll'sbeha,lf a system call is invoked in a variety of ways  depending on the functionality provided by the underlying processor in all forms  it is the method used by a process to request action by the operating system a system call usually takes the form of a trap to a specific location in the interrupt vector  this trap can be executed by a generic trap instruction  although some systems  such as the mips r2000 family  have a specific syscall instruction  when asystep1 calljs e   ecutect it is treated by the hardware as a software -i  rlt ~ rr  l.l   if  c   iltrol passes through the interrupt vector to a service routine in the operating system/ and the m   de bit is set to kernel mode the systemcaflserv1ce routine is a part of the operating system the-kernel examines the interrupting instruction to determine what system call has occurred ; a ~ parameter indicates what type of service the user program is requesting  additional information needed for the r ~ quest_may be passed in registers  on the stack/ or in memory  with pointers to the memory locations passed in registers   the kernel vedfies that the parameters are correct and legat executes ti1erequest  and returns control to the instruction following the system call we describe system calls more fully in section 2.3  the lack of a hardware-supported dual mode can cause serious shortcomings in an operating system for instance  ms-dos was written for the intel 8088 architecture  which has no mode bit and therefore no dual mode a user program rum1ing awry can wipe out the operating system by writing over it with data ; and multiple programs are able to write to a device at the same time  with potentially disastrous results recent versions of the intel cpu do provide dual-mode operation accordingly  most contemporary operating systemssuch as microsoft vista and windows xp  as well as unix  linux  and solaris 1.6 1.6 23 -take advantage of this dual-mode feature and provide greater protection for the operating system  once hardware protection is in place  it detects errors that violate modes  these errors are normally handled by the operating system if a user program fails in some way-such as by making an attempt either to execute an illegal instruction or to access memory that is not in the user 's address space-then the hardware traps to the operating system the trap transfers control through the interrupt vector to the operating system  just as an interrupt does when a program error occurs  the operating system must terminate the program abnormally this situation is handled by the same code as a user-requested abnormal termination an appropriate error message is given  and the memory of the program may be dumped the memory dump is usually written to a file so that the user or programmer can examine it and perhaps correct it and restart the program  1.5.2 timer wer  r1,_ust ensure th t ! the ope  j ; atil  gsystemij  taintains t  ontrol overthe c  j_ !  l_ ~ we cam1.ot allow a userp ~ ogram to_ get stuc  kin e1ninfinite loop or to fail to call syste1n seryices and never retltrn control to the c  perating system to ~ c  9 ! ll 1i  s ~ tl1.1s = g ~ at we_can usea _a_tirn ~ r_can beset to interrupt th ~ c  c  mp_ut ~ r af_t ~ ril p ~ c  ified peri   d the period may be fixed  for example  1/60 second  or variable  for example  from 1 millisecond to 1 second   a is generally implemented by a fixed-rate clock and a counter  the operating system sets the counter every time the clock ticks  the counter is decremented when the counter reaches 0  an interrupt occurs for instance  a 10-bit counter with a 1-millisecond clock allows interrupts at intervals from 1 millisecond to 1,024 milliseconds  in steps of 1 millisecond  before turning over control to the user  the operating system ensures that the timer is set to interrupt ll ~ ll ~ __ tij11e_ _il1t ~ rrl1pts/control transfers automatically totll.e   pel  9  t ~ ~ y ! epl,_ \ thicfl__ ! l-1  1ytreat the interrupt as a faiaf error or n  taygi-y_etll.ep_rograrn rnc  r ~ ! i  rn ~   clearly,il ~ structions that modify the content of the timer are privileged  thus  we can use the timer to prevent a user program from running too long a simple technique is to il1.itialize a counter with the amount of time that a program is allowed to run a program with a 7-minute time limit  for example  would have its counter initialized to 420 every second  the timer interrupts and the counter is decremented by 1 as long as the counter is positive  control is returned to the user program when the counter becomes negative  the operating system terminates the program for exceeding the assigned time limit  a program does nothing unless its instructions are executed by a cpu a program in execution  as mentioned  is a process a time-shared user program such as a compiler is a process a word-processing program being run by an individual user on a pc is a process a system task  such as sending output to a printer  can also be a process  or at least part of one   for now  you can consider a process to be a job or a time-shared program  but later you will learn 24 chapter 1 1.7 that the concept is more general as we shall see in chapter 3  it is possible to provide system calls that allow processes to create subprocesses to execute concurrent ! y  a process needs certain resources---including cpu time  me111ory  files  and-i ; o devices    _  _ to accomplish its  task these i esources are e ! tl1er given to the process when it is created or allocated to it while it is running in addition to the various physical and logical resources that a process obtains when it is created  various initialization data  input  may be passed along for example  consider a process whose function is to display the status of a file on the screen of a terminal the process will be given as an input the name of the file and will execute the appropriate instructions and system calls to obtain and display on the terminal the desired information when the process terminates  the operating system will reclaim any reusable resources  l ve ~ _111pl  t21size that a program by itselfis nota process ; a program is a y_assive er ~ ! ~ ty  likt  tl1e c   i1terltsof a fil  storecl_m1 c ! iskl ~ a.thereasc \ _pr  jce ~ ~ s_1s 21 ~ 1 active entity a si-dgl ~   1hr  eaded proc ~ ss has on ~ _pr_ogra111 cou11 ! er s  eecifying the nexf1il ~ r  uc_tiogt   _ex ~ cljte  threads are covered in chapter 4  the -execi.rtioil  of such a process must be sequential the cpu executes one instruction of the process after another  until the process completes further  at any time  one instruction at most is executed on behalf of the process thus  although two processes may be associated with the same program  they are nevertheless considered two separate execution sequences a multithreaded process has multiple program counters  each pointing to the next instruction to execute for a given thread  a process is the unit of work in a system such a system consists of a collection of processes  some of which are operating-system processes  those that execute system code  and the rest of which are user processes  those that execute user code   al  jheseprocesses canp   t ~ ! ltially execute concurrently _lly.ij  lli  ! p_l ~   _i ! lg   i ' \ a sir1gle _c  pl  ,for_ ~   ample      the operating system is responsible for the following activities in connection with process management  scheduling processes and threads on the cpus creating and deleting both user and system processes suspending and resuming processes providing mechanisms for process synchronization providing mechanisms for process communication we discuss process-management techniques in chapters 3 through 6  as we discussed in section 1.2.2  the main memory is central to the operation of a modern computer system main memory is a large array of words or bytes  ranging in size from hundreds of thousands to billions each word or byte has its own address main memory is a repository of quickly accessible data shared by the cpu and i/0 devices the central processor reads instructions from main 1.8 1.8 25 memory during the instruction-fetch cycle and both reads and writes data from main memory during the data-fetch cycle  on a von neumann architecture   as noted earlier  the main memory is generallythe only large storage device that the cpu is able to address and access directly for example  for the cpu to process data from disk  those data mu.st first be transferred to main n lemory by cpu-generated i/0 calls in the same way  instructions must be in memory for the cpu to execute them  for a program to be executed  it must be mapped to absolute addresses and loaded into memory as the program executes  it accesses program instructions and data from memory by generating these absolute addresses eventually  the program terminates  its memory space is declared available  and the next program can be loaded and executed  to improve both the utilization of the cpu and the speed of the computer 's response to its users  general-purpose computers must keep several programs in memory  creating a need for memory management many different memorymanagement schemes are used these schemes reflect various approaches  and the effectiveness of any given algorithm depends on the situation in selecting a memory-management scheme for a specific system  we must take into account many factors-especially the hardware design of the system each algorithm requires its own hardware support  the operating system is responsible for the following activities in connection with memory management  keeping track of which parts of memory are currently being used and by whom deciding which processes  or parts thereof  and data to move into and out of memory allocating and deallocating memory space as needed memory-management techniques are discussed il1 chapters 8 and 9  to make the computer system convenient for users  the operating system provides a uniform  logical view of information storage the operating system abstracts from the physical properties of its storage devices to define a logical storage unit  the file the operating system maps files onto physical media and accesses these files via the storage devices  1.8.1 file-system management pile management is one of the most visible components of an operating system  computers can store information on several different types of physical media  magnetic disk  optical disk  and magnetic tape are the most common each of these media has its own characteristics and physical organization each medium is controlled by a device  such as a disk drive or tape drive  that also has its own unique characteristics these properties include access speed  capacity  data-transfer rate  and access method  sequential or randmn   26 chapter 1 a file is a collection of related information defined by its creator commonly  files represent programs  both source and object forms  and data data files may be numeric  alphabetic  alphanumeric  or binary files may be free-form  for example  text files   or they may be formatted rigidly  for example  fixed fields   clearly  the concept of a file is an extremely general one  the operating system implements the abstract concept of a file by managing mass-storage media  such as tapes and disks  and the devices that control them  also  files are normally organized into directories to make them easier to use  finally  when multiple users have access to files  it may be desirable to control by whom and in what ways  for example  read  write  append  files may be accessed  the operating system is responsible for the following activities in connection with file management  creating and deleting files creating and deleting directories to organize files supporting primitives for manipulating files and directories mapping files onto secondary storage backing up files on stable  nonvolatile  storage media file-management teclmiques are discussed in chapters 10 and 11  1.8.2 mass-storage management as we have already seen  because main memory is too small to accommodate all data and programs  and because the data that it holds are lost when power is lost  the computer system must provide secondary storage to back up main memory most modern computer systems use disks as the principal on-line storage medium for both programs and data most programs-including compilers  assemblers  word processors  editors  and formatters-are stored on a disk until loaded into memory and then use the disk as both the source and destination of their processing hence  the proper management of disk storage is of central importance to a computer system the operating system is responsible for the following activities in connection with disk management  free-space management storage allocation disk scheduling because secondary storage is used frequently  it must be used efficiently the entire speed of operation of a computer may hinge on the speeds of the disk subsystem and the algorithms that manipulate that subsystem  there are  however  many uses for storage that is slower and lower in cost  and sometimes of higher capacity  than secondary storage backups of disk data  seldom-used data  and long-term archival storage are some examples  magnetic drives and their tapes and cd and dvd drives and platters are typical devices the media  tapes and optical platters  vary between  write-once  read-many-times  and  read-write  formats  1.8 27 tertiary storage is not crucial to systern performance  but it still must be managed some operating systems take on this task  while others leave tertiary-storage management to application progran1s some of the functions that operating systerns can provide include mounting and unmounting rnedia in devices  allocating and freeing the devices for exclusive use by processes  and migrating data from secondary to tertiary storage  techniques for secondary and tertiary storage management are discussed in chapter 12  1.8.3 caching is an important principle of computer systems information is normally kept in some storage system  such as main memory   as it is used  it is copied into a faster storage system-the cache-on a temporary basis  when we need a particular piece of information  we first check whether it is in the cache if it is  we use the information directly from the cache ; if it is not  we use the information from the source  putting a copy in the cache under the assumption that we will need it again soon  in addition  internal programmable registers   such as index registers  provide a high-speed cache for main memory the programmer  or compiler  implements the register-allocation and register-replacement algorithms to decide which information to keep in registers and which to keep in main memory there are also caches that are implemented totally in hardware  for instance  most systems have an instruction cache to hold the instructions expected to be executed next without this cache  the cpu would have to wait several cycles while an instruction was fetched from main memory for similar reasons  most systems have one or more high-speed data caches in the memory hierarchy we are not concerned with these hardware-only caches in this text  since they are outside the control of the operating system  because caches have limited size  is an important design problem careful selection of the cache size and of a replacement policy can result in greatly increased performance figure 1.11 compares storage performance in large workstations and small servers various replacement algorithms for software-controlled caches are discussed in chapter 9  typical size 16mb 64gb 100gb implementation custom memory with on-chip or off-chip cmos dram magnetic disk technology multiple ports  cmos cmossram access time  ns  0.25-0.5 0.5-25 80-250 5,000.000 bandwidth  mb/sec  20,000 ~ 100,000 5000 10,000 1000-5000 20-150 managed by compiler hardware operating system operating system backed by cache main memory disk cd or tape figure 1.11 performance of various levels of storage  28 chapter 1 main memory can be viewed as a fast cache for secondary storage  since data in secondary storage must be copied into main memory for use  and data must be in main memory before being moved to secondary storage for safekeeping the file-system data  which resides permanently on secondary storage  may appear on several levels in the storage hierarchy at the highest level  the operating system may maintain a cache of file-system data in main memory in addition  electronic ram disks  also known as may be used for high-speed storage that is accessed through the file-system interface the bulk of secondary storage is on magnetic disks the magneticdisk storage  in turn  is often backed up onto magnetic tapes or removable disks to protect against data loss in case of a hard-disk failure some systems autoinatically archive old file data from secondary storage to tertiary storage  such as tape jukeboxes  to lower the storage cost  see chapter 12   the movement of information between levels of a storage hierarchy may be either explicit or implicit  depending on the hardware design and the controlling operating-system software f_o   '  ! lstilnce,datatransfe ~ from cache _l ~ cpu ~ '11 ~ cl_ ! ~ g ~ ~ ! ~  r-_s _is_ _ ~ 1suall y ahardvvare function  with no op-era t  ii.g = sy-s tern intervention in contrast  transfer of data-from aisk to memory is usually controlledby the-op ~ ra-t !  ri.g system   fn a 11ier2rrchical storage structure  the same data may appear in different levels of the storage system for example  suppose that an integer a that is to be incremented by 1 is located in file b  and file b resides on magnetic disk  the increment operation proceeds by first issuing an i/o operation to copy the disk block on which a resides to main memory this operation is followed by copying a to the cache and to an internal register thus  the copy of a appears in several places  on the magnetic disk  in main memory  in the cache  and in an internal register  see figure 1.12   once the increment takes place in the internal register  the value of a differs in the various storage systems the value of a becomes the same only after the new value of a is written from the internal register back to the magnetic disk  in a computing environment where only one process executes at a tim.e  this arrangement poses no difficulties  since an access to integer a will always be to the copy at the highest level of the hierarchy however  in a multitasking environment  where the cpu is switched back and -forth-among var1ous processes ~ extreme care must be taken to ensure that  if several processe ~ vv  is  l i  o-accessa  then each of these processes will obtain the most recently updated ___ c_ = --.c of a     the situation becomes more complicated in a multiprocessor environment where  in addition to maintaining internal registers  each of the cpus also contains a local cache  figure 1.6   ~ ' 1_ su ~ bc1  .1l_  _n ~ i o ! _l  _il'l_ ~ 1lt  ~ s 2ey   f_a ij.t ~ y exist simultaneouslyinseyeral caches since the variouscpus can all execute .s  2 ~ 1c ~ r ~ ~ l ~ tly  \ ,ve-must1nake surethat an to the value ofa in one cache figure 1.12 migration of integer a from disk to register  1.9 1.9 29 1.8.4 1/0 systems one of the purposes of a11 operating system is to hide the peculiarities ofspecific hardware d ~ ~ ic  ~ jro1n th ~ l1s ~ j   for example  in unix  the peculiarities of i/o devices are hidden from the bulk of the operating system itself by the i/0 subsystem the i/o subsystem consists of several components  a memory-management component that includes buffering  caching  and spooling a general device-driver interface drivers for specific hardware devices only the device driver knows the peculiarities of the specific device to which it is assigned  we discussed in section 1.2.3 how interrupt handlers and device drivers are used in the construction of efficient i/o subsystems in chapter 13  we discuss how the i/o subsystem interfaces to the other system components  manages devices  transfers data  and detects i/0 completion  if a computer system has multiple users and allows the concurrent execution of multiple processes  then access to data must be regulated for that purpose  mechanisms ensure that files  memory segments  cpu  and other resources can be operated on by only those processes that have gained proper authorization from the operating system for example  memory-addressing hardware ensures that a process can execute only within its own address space the timer ensures that no process can gain control of the cpu without eventually relinquishing control device-control registers are not accessible to users  so the integrity of the various peripheral devices is protected  protection  then  is any mechanism for controlling the access of processes or users-to the resourcesdefined by a computer system this mechanism rni1st provide means to specify the confrols to be imposed and means to enforce the controls  protection can improve reliability by detecting latent errors at the interfaces between component subsystems early detection of interface errors can often prevent contamination of a healthy subsystem by another subsystem that is 30 chapter 1 1.10 malfunctioning furthermore  an unprotected resource can not defend against use  or n isuse  by an unauthorized or incompetent user a protection-oriented system provides a means to distinguish between authorized and unauthorized usage  as we discuss in chapter 14  6 yt  ; terl _ca1lhave adequateprotection but still be prone to failure and ado_w inappr   priat ~ acs ~ s ~  consider a user whose authentication information  her means of identifying herself to the system  is stolen her data could be copied or deleted  even though file and memory protection are working it is the job of to defend a system from external and internal attacks such attacks spread across a huge range and include viruses and worms  denial-ofservice attacks  which use all of a system 's resources and so keep legitimate users out of the system   identity theft  and theft of service  unauthorized use of a system   prevention of some of these attacks is considered an operatingsystem function on some systems  while other systems leave the prevention to policy or additional software due to the alarming rise in security incidents  operating-system security features represent a fast-growing area of research and implementation security is discussed in chapter 15  protection and security require the system to be able to distinguish among all its users most maintain a list of user names and    in windows vista parlance  this is _ 1_ these numerical ids are unique  one per user when a user logs in system  the authentication stage determines the appropriate user id for the user that user id is associated with all of the user 's processes and threads when an id needs to be user readable  it is translated back to the user name via the user name list  in some circumstances  we wish to distinguish among sets of users rather than individual users for example  the owner of a file on a unix system may be allowed to issue all operations on that file  whereas a selected set of users may only be allowed to read the file to accomplish this  we need to define a group name and the set of users belonging to that group group functionality can be implemented as a system-wide list of group names and ic'1entifiers  a user can be in one or more groups  depending on operating-system design decisions the user 's group ids are also included in every associated process and thread  in the course of normal use of a system  the user id and are s-l.iffici.e11t hov \ tever ; a user sometimes needs to to gain extra permissions for an activity the user may need access to a fhatis resh ; icted,for examp1e.operatmg systems provide various methods to allow privilege escalation on unix  for example  the setuid attribute on a program causes that program to run with the user id of the owner of the file  rather than the current user 's id the process runs with this until it turns off the extra privileges or terminates  a distributed system is a collection of physically separate  possibly heterogeneous  computer systems that are networked to provide the users with access to the various resources that the system maintains access to a shared resource 1.10 31 increases computation speed  functionality  data availability  and reliability  some operating systems generalize network access as a form of file access  with the details of networking contained in the network interface 's device driver  others make users specifically invoke network functions generally  systems contain a mix of the two modes-for example ftp and nfs the protocols that create a distributed system can greatly affect that system 's utility and popularity  a in the simplest terms  is a communication path between two or more systems distributed systems depend on networking for their functionality networks vary by the protocols used  the distances between nodes  and the transport media tcp /ip is the most common network protocol  although atm and other protocols are in widespread use likewise  operatingsystem support of protocols varies most operating systems support tcp /ip  including the windows and unix operating systems some systems support proprietary protocols to suit their needs to an operating system  a network protocol simply needs an interface device-a network adapter  for examplewith a device driver to manage it  as well as software to handle data these concepts are discussed throughout this book  networks are characterized based on the distances between their nodes  a computers within a room  a floor  or a building a n  usually links buildings  cities  or countries a global company may have a wan to com1ect its offices worldwide these networks may run one protocol or several protocols the continuing advent of new technologies brings about new forms of networks  for example  a   '/ ! al i  could link buildings within '   a city bluetooth and 802.11 devices use wireless technology to commt.micate over a distance of several feet  in essence creating a such as might be found in a home  the media to carry networks are equally varied they include copper wires  fiber strands  and wireless transmissions between satellites  microwave dishes  and radios when computing devices are connected to cellular phones  they create a network even very short-range infrared communication can be used for networking at a rudimentary level  whenever computers communicate  they use or create a network these networks also vary in their performance and reliability  some operating systems have taken the concept of networks and distributed systems further than the notion of providing network connectivity a is an operating system that provides features such as file sharing across the network and that includes a communication scheme that allows different processes on different computers to exchange messages  a computer rmming a network operating system acts autonomously from all other computers on the network  although it is aware of the network and is able to communicate with other networked computers a distributed operating system provides a less autonomous envirorunent  the different operating systems comm lmicate closely enough to provide the illusion that only a single operating system controls the network  we cover computer networks and distributed systems in chapters 16 through 18  32 chapter 1 1.11 the discussion thus far has focused on the general-purpose computer systems that we are all familiar with there are  however  other classes of computer systems whose functions are more limited and whose objective is to deal with limited computation domains  1.11.1 real-time embedded systems embedded computers are the most prevalent form of computers in existence  these devices are found everywhere  from car engines and manufacturing robots to dvds and microwave ovens they tend to have very specific tasks  the systencs they run on are usually primitive  and so the operating systems provide limited features usually  they have little or no user interface  preferring to spend their time monitoring and managing hardware devices  such as automobile engines and robotic arms  these embedded systems vary considerably some are general-purpose computers  running standard operating systems-such as unix-with special-purpose applications to implement the functionality others are hardware devices with a special-purpose embedded operating system providing just the functionality desired yet others are hardware devices with application-specific integrated circuits that perform their tasks without an operating system  the use of embedded systems continues to expand the power of these devices  both as standalone units and as elements of networks and the web  is sure to increase as well even now  entire houses can be computerized  so that a central computer-either a general-purpose computer or an embedded system-can control heating and lighting  alarm systems  and even coffee makers web access can enable a home owner to tell the house to heat up before she arrives home someday  the refrigerator may call the grocery store when it notices the milk is gone  embedded systems almost always run a real-time system is used when rigid time requirements been placed on the operation of a processor or the flow of data ; thus  it is often used as a control device in a dedicated application sensors bring data to the computer  the computer must analyze the data and possibly adjust controls to modify the sensor inputs systems that control scientific experiments  medical imaging systems  industrial control systems  and certain display systems are realtime systems some automobile-engine fuel-injection systems  home-appliance controllers  and weapon systems are also real-time systems  a real-time system has well-defined  fixed time constraints processing must be done within the defined constraints  or the system will fail for instance  it would not do for a robot arm to be instructed to halt after it had smashed into the car it was building a real-time system functions correctly only if it returns the correct result within its time constraints contrast this system with a time-sharing system  where it is desirable  but not mandatory  to respond quickly or a batch system  which may have no time constraints at all  in chapter 19  we cover real-time embedded systems in great detail in chapter 5  we consider the scheduling facility needed to implement real-time functionality in an operating system in chapter 9  we describe the design 1.11 33 of memory management for real-time computing finally  in chapter 22  we describe the real-time components of the windows xp operating system  1.11.2 multimedia systems most operating systems are designed to handle conventional data such as text files  progran'ls  word-processing documents  and spreadsheets however  a recent trend in technology is the incorporation of multimedia data into computer systems multimedia data consist of audio and video files as well as conventional files these data differ from conventional data in that multimedia data-such as frames of video-must be delivered  streamed  according to certain time restrictions  for example  30 frames per second   multimedia describes a wide range of applications in popular use today  these include audio files such as mp3  dvd movies  video conferencing  and short video clips of movie previews or news stories downloaded over the internet multimedia applications may also include live webcasts  broadcasting over the world wide web  of speeches or sporting events and even live webcams that allow a viewer in manhattan to observe customers at a cafe in paris multimedia applications need not be either audio or video ; rather  a multimedia application often includes a combination of both for example  a movie may consist of separate audio and video tracks nor must multimedia applications be delivered only to desktop personal computers increasingly  they are being directed toward smaller devices  including pdas and cellular telephones for example  a stock trader may have stock quotes delivered wirelessly and in real time to his pda  in chapter 20  we explore the demands of multimedia applications  describe how multimedia data differ from conventional data  and explain how the nature of these data affects the design of operating systems that support the requirements of multimedia systems  1.11.3 handheld systems include personal digital assistants  pdas   such as palm and pocket-pes  and cellular telephones  many of which use special-purpose embedded operating systems developers of handheld systems and applications face many challenges  most of which are due to the limited size of such devices for example  a pda is typically about 5 inches in height and 3 inches in width  and it weighs less than one-half pound because of their size  most handheld devices have small amounts of memory  slow processors  and small display screens we take a look now at each of these limitations  the amount of physical memory in a handheld depends on the device  but typically it is somewhere between 1 mb and 1 gb  contrast this with a typical pc or workstation  which may have several gigabytes of memory  as a result  the operating system and applications must manage memory efficiently this includes returning all allocated memory to the memory manager when the memory is not being used in chapter 9  we explore virtual memory  which allows developers to write programs that behave as if the system has more memory than is physically available currently  not many handheld devices use virtual memory techniques  so program developers must work within the confines of limited physical memory  34 chapter 1 1.12 a second issue of concern to developers of handheld devices is the speed of the processor used in the devices processors for most handheld devices run at a fraction of the speed of a processor in a pc faster processors require more power to include a faster processor in a handheld device would require a larger battery  which would take up more space and would have to be replaced  or recharged  more frequently most handheld devices use smaller  slower processors that consume less power therefore  the operating system and applications must be designed not to tax the processor  the last issue confronting program designers for handheld devices is l/0  a lack of physical space limits input methods to small keyboards  handwriting recognition  or small screen-based keyboards the small display screens limit output options whereas a monitor for a home computer may measure up to 30 inches  the display for a handheld device is often no more than 3 inches square familiar tasks  such as reading e-mail and browsing web pages  must be condensed into smaller displays one approach for displaying the content in web pages is where only a small subset of a web page is delivered and displayed on the handheld device  some handheld devices use wireless technology  such as bluetooth or 802.11  allowing remote access to e-mail and web browsing cellular telephones with connectivity to the internet fall into this category however  for pdas that do not provide wireless access  downloading data typically requires the user first to download the data to a pc or workstation and then download the data to the pda some pdas allow data to be directly copied from one device to another using an infrared link generally  the limitations in the functionality of pdas are balanced by their convenience and portability their use continues to expand as network com1ections become more available and other options  such as digital cameras and mp3 players  expand their utility  so far  we have provided an overview of computer-system organization and major operating-system components we conclude with a brief overview of how these are used in a variety of computing environments  1.12.1 traditional computing as computing matures  the lines separating many of the traditional computing environments are blurring consider the typical office environment just a few years ago  this environment consisted of pcs connected to a network  with servers providing file and print services remote access was awkward  and portability was achieved by use of laptop computers terminals attached to mainframes were prevalent at many companies as well  with even fewer remote access and portability options  the current trend is toward providing more ways to access these computing environments web technologies are stretching the boundaries of traditional computing companies establish which provide web accessibility to their internal servers ccxepu1as are essentially terminals that understand web-based computing handheld computers can synchronize with 1.12 35 pcs to allow very portable use of con1pany information handheld pdas can also connect to to use the company 's web portal  as well as the myriad other web resources   at home  most users had a single computer with a slow modem connection to the office  the internet  or both today  network-connection speeds once available only at great cost are relatively inexpensive  giving home users more access to more data these fast data connections are allowing home computers to serve up web pages and to run networks that include printers  client pcs  and servers some homes even have to protect their networks from security breaches those firewalls cost thousands of dollars a few years ago and did not even exist a decade ago  in the latter half of the previous century  computing resources were scarce   before that  they were nonexistent !  for a period of time  systems were either batch or interactive batch systems processed jobs in bulk  with predetermined input  from files or other sources of data   interactive systems waited for input from users to optimize the use of the computing resources  multiple users shared time on these systems time-sharing systems used a timer and scheduling algorithms to rapidly cycle processes through the cpu  giving each user a share of the resources  today  traditional time-sharing systems are uncommon the same scheduling technique is still in use on workstations and servers  but frequently the processes are all owned by the same user  or a single user and the operating system   user processes  and system processes that provide services to the user  are managed so that each frequently gets a slice of computer time consider the windows created while a user is working on a pc  for example  and the fact that they may be performing different tasks at the same time  1.12.2 client-server computing as pcs have become faste1 ~ more powerful  and cheaper  designers have shifted away from centralized system architecture terminals connected to centralized systems are now being supplanted by pcs correspondingly  user-interface functionality once handled directly by centralized systems is increasingly being handled by pcs as a result  many of today ' s systems act as to satisfy requests generated by this form of specialized distributed system  called a system  has the general structure depicted in figure 1.13  server systems can be broadly categorized as compute servers and file servers  figure 1.13 general structure of a client-server system  36 chapter 1 the provides an interface to which a client can send a request to perform an action  for example  read data  ; in response  the server executes the action and sends back results to the client a server running a database that responds to client requests for data is an example of such a system  the provides a file-system interface where clients can create  update  read  and delete files an example of such a system is a web server that delivers files to clients running web browsers  1.12.3 peer-to-peer computing another structure for a distributed system is the peer-to-peer  p2p  system model in this model  clients and servers are not distinguished from one another ; instead  all nodes within the system are considered peers  and each ncay act as either a client or a server  depending on whether it is requesting or providing a service peer-to-peer systems offer an advantage over traditional client-server systems in a client-server system  the server is a bottleneck ; but in a peer-to-peer system  services can be provided by several nodes distributed throughout the network  to participate in a peer-to-peer system  a node must first join the network of peers once a node has joined the network  it can begin providing services to-and requesting services from -other nodes in the network determining what services are available is accomplished in one of two general ways  when a node joins a network  it registers its service with a centralized lookup service on the network any node desiring a specific service first contacts this centralized lookup service to determine which node provides the service the remainder of the communication takes place between the client and the service provider  a peer acting as a client must first discover what node provides a desired service by broadcasting a request for the service to all other nodes in the network the node  or nodes  providing that service responds to the peer making the request to support this approach  a discovery protocol must be provided that allows peers to discover services provided by other peers in the network  peer-to-peer networks gained widespread popularity in the late 1990s with several file-sharing services  such as napster and gnutella  that enable peers to exchange files with one another the napster system uses an approach similar to the first type described above  a centralized server maintains an index of all files stored on peer nodes in the napster network  and the actual exchanging of files takes place between the peer nodes the gnutella system uses a technique similar to the second type  a client broadcasts file requests to other nodes in the system  and nodes that can service the request respond directly to the client the future of exchanging files remains uncertain because many of the files are copyrighted  music  for example   and there are laws governing the distribution of copyrighted material in any case  though  peerto peer technology undoubtedly will play a role in the future of many services  such as searching  file exchange  and e-mail  1.13 1.13 37 1.12.4 web-based computing the web has become ubiquitous/ leading to more access by a wider variety of devices than was dreamt of a few years ago pcs are still the most prevalent access devices/ with workstations/ handheld pdas1 and even cell phones also providing access  web computing has increased the emphasis on networking devices that were not previously networked now include wired or wireless access devices that were networked now have faster network connectivity/ provided by either improved networking technology optimized network implementation code/ or both  the implementation of web-based computing has given rise to new categories of devices/ such as which distribute network connections an1.ong a pool of similar servers operating systems like windows 951 which acted as web clients/ have evolved into linux and windows xp 1 which can act as web servers as well as clients generally/ the web has increased the complexity of devices because their users require them to be web-enabled  the study of operating systems/ as noted earlier/ is made easier by the availability of a vast number of open-source releases  are those made available in source-code format rather than as compiled binary code linux is the most famous open source operating system  while microsoft windows is a well-known example of the opposite dosedapproach  starting with the source code allows the programmer to produce binary code that can be executed on a system doing the oppositethe source code from the binaries-is quite a lot of work1 and useful items such as comments are never recovered learning operating systems by examining the actual source code1 rather than reading summaries of that code/ can be extremely useful with the source code in hand/ a student can modify the operating system and then compile and nm the code to try out those changes1 which is another excellent learning tool this text indudes projects that involve modifying operating system source code/ while also describing algorithms at a high level to be sure all important operating system topics are covered throughout the text1 we provide pointers to examples of open-source code for deeper study  there are many benefits to open-source operating systems/ including a commtmity of interested  and usually unpaid  programmers who contribute to the code by helping to debug it analyze it/ provide support/ and suggest changes arguably/ open-source code is more secure than closed-source code because many more eyes are viewing the code certainly open-source code has bugs/ but open-source advocates argue that bugs tend to be found and fixed faster owing to the number of people using and viewing the code companies that earn revenue from selling their programs tend to be hesitant to open-source their code/ but red hat/ suse1 sun/ and a myriad of other companies are doing just that and showing that commercial companies benefit/ rather than suffer/ when they open-source their code revenue can be generated through support contracts and the sale of hardware on which the software runs/ for example  38 chapter 1 1.13.1 history in the early days of modern computing  that is  the 1950s   a great deal of software was available in open-source format the original hackers  computer enthusiasts  at mit 's tech model railroad club left their programs in drawers for others to work on homebrew user groups exchanged code during their meetings later  company-specific user groups  such as digital equipment corporation 's dec  accepted contributions of source-code programs  collected them onto tapes  and distributed the tapes to interested ncembers  computer and software companies eventually sought to limit the use of their software to authorized computers and paying customers releasing only the binary files compiled from the source code  rather than the source code itself  helped them to achieve this goal  as well as protecting their code and their ideas from their competitors another issue involved copyrighted material operating systems and other programs can limit the ability to play back movies and music or display electronic books to authorized computers  such or digital would not be effective if the source code that implemented these limits were published  laws in many countries  including the u.s digital millennium copyright act  dmca   make it illegal to reverse-engineer drm code or otherwise try to circumvent copy protection  to counter the move to limit software use and redistribution  richard stallman in 1983 started the gnu project to create a free  open-source unixcompatible operating system in 1985  he published the gnu manifesto  which argues that all software should be free and open-sourced he also formed the with the goal of encouraging the free exchange of software source code and the free use of that software rather than copyright its software  the fsf copylefts the software to encourage sharing and improvement the gercera  ! codifies copylefting and is a common license under which free software is released ftmdamentally  gpl requires that the source code be distributed with any binaries and that any changes made to the source code be released under the same gpl license  1.13.2 linux as an example of an open-source operating system  consider the gnu project produced many unix-compatible tools  including compilers  editors  and utilities  but never released a kernel in 1991  a student in finland  linus torvalds  released a rudimentary unix-like kernel using the gnu compilers and tools and invited contributions worldwide the advent of the internet meant that anyone interested could download the source code  modify it  and submit changes to torvalds releasing updates once a week allowed this so-called linux operating system to grow rapidly  enhanced by several thousand programmers  the gnu /linux operating system has spawned hundreds of unique or custom builds  of the system major distributions include redhat  suse  fedora  debian  slackware  and ubuntu distributions vary in function  utility  installed applications  hardware support  user interface  and purpose for example  redhat enterprise lim1x is geared to large commercial use pclinuxos is a  jvc  cd-an operating system that can be booted and run from a cd-rom without being installed on a system 's hard 1.13 39 disk one variant of pclinuxos  pclinuxos supergamer dvd  is a that includes graphics drivers and games a gamer can run it on any compatible system simply by booting from the dvd when the gamer is finished  a reboot of the system resets it to its installed operating system  access to the linux source code varies by release here  we consider ubuntu linux ubuntu is a popular linux distribution that comes in a variety of types  including those tuned for desktops  servers  and students its founder pays for the printing and mailing of dvds containing the binary and source code  which helps to make it popular   the following steps outline a way to explore the ubuntu kernel source code on systems that support the free vmware player tool  download the player from http  i /www vrnware com/ download/player i and install it on your system  download a virtual machine containing ubuntu hundreds of appliances  or virtual machirte images  pre-installed with operating systems and applications  are available from vmware at http  //www.vmware.com/appliances/  boot the virtual machine within vmware player  get the source code of the kernel release of interest  such as 2.6  by executing wget http  //www.kernel.org/pub/linux/kernel/v2.6/linux 2 6 18 1 tar bz2 within the ubuntu virtual machine  uncompress and untar the downloaded file via tar xj f linux 2.6.18.1.tar.bz2  explore the source code of the ubuntu kernel  which is now in  /linux 2 6.18 .1  for more about linux  see chapter 21 for more about virtual machines  see section 2.8  1.13.3 bsd unix has a longer and more complicated history than linux it started in 1978 as a derivative of at&t 's unix releases from the university of california at berkeley  ucb  came in source and binary form  but they were not opensource because a license from at&t was required bsd unix 's development was slowed by a lawsuit by at&t  but eventually a fully functional  open-source version  4.4bsd-lite  was released in 1994  just as with lim.ix  there are many distributions of bsd unix  including freebsd  netbsd  openbsd  and dragonflybsd to explore the source code of freebsd  simply download the virtual machine image of the version of interest and boot it within vmware  as described above for ubuntu linux the source code comes with the distribution and is stored in /usr i src/ the kernel source code is in /usr/src/sys for example  to examine the virtual-memory implementation code in the freebsd kernel  see the files in /usr/src/sys/vrn  darwin  the core kernel component of mac  is based on bsd unix and is open-sourced as well that source code is available from http  i /www opensource apple corn/ darwinsource/ every mac release 40 chapter 1 1.14 has its open-source components posted at that site the name of the package that contains the kernel is xnu the source code for mac kernel revision 1228  the source code to mac leopard  can be found at www.opensource.apple.coml darwinsource i tar balls i apsll xnu-1228 tar.gz  apple also provides extensive developer tools  documentation  and support at http  i i connect apple com for more information  see appendix a  1.13.4 solaris is the commercial unix-based operating system of sun microsystems  originally  sun 's operating system was based on bsd unix sun moved to at&t 's system v unix as its base in 1991 in 2005  sun open-sourced some of the solaris code  and over time  the company has added more and more to that open-source code base unfortunately  not all of solaris is open-sourced  because some of the code is still owned by at&t and other companies however  solaris can be compiled from the open source and linked with binaries of the close-sourced components  so it can still be explored  modified  compiled  and tested  the source code is available from http  i i opensolaris org/ os/ downloads/  also available there are pre-compiled distributions based on the source code  docun1.entation  and discussion groups it is not necessary to download the entire source-code bundle from the site  because sun allows visitors to explore the source code on-line via a source code browser  1.13.5 utility the free software movement is driving legions of programmers to create thousands of open-source projects  including operating systems sites like http  i /freshmeat net/ and http  i i distrowatch com/ provide portals to many of these projects open-source projects enable students to use source code as a learning tool they can modify programs and test them  help find and fix bugs  and otherwise explore mature  full-featured operating systems  compilers  tools  user interfaces  and other types of programs the availability of source code for historic projects  such as multics  can help students to understand those projects and to build knowledge that will help in the implementation of new projects  gnu ilinux  bsd unix  and solaris are all open-source operating systems  but each has its own goals  utility  licensing  and purpose sometimes licenses are not mutually exclusive and cross-pollination occurs  allowing rapid improvements in operating-system projects for example  several major components of solaris have been ported to bsd unix the advantages of free software and open sourcing are likely to increase the number and quality of open-source projects  leading to an increase in the number of individuals and companies that use these projects  an operating system is software that manages the cornputer hardware  as well as providing an environment for application programs to run perhaps the 1.14 41 most visible aspect of an operating system is the interface to the computer system it provides to the human user  for a computer to do its job of executing programs  the program.s must be in main memory main memory is the only large storage area that the processor can access directly it is an array of words or bytes  ranging in size from millions to billions each word in memory has its own address the main mem.ory is usually a volatile storage device that loses its contents when power is turned off or lost most computer systems provide secondary storage as an extension of main memory secondary storage provides a form of nonvolatile storage that is capable of holding large quantities of data permanently the most common secondary-storage device is a magnetic disk  which provides storage of both programs and data  the wide variety of storage systems in a computer system can be organized in a hierarchy according to speed and cost the higher levels are expensive  but they are fast as we move down the hierarchy  the cost per bit generally decreases  whereas the access time generally increases  there are several different strategies for designing a computer system  uniprocessor systems have only a single processor  while multiprocessor systems contain two or more processors that share physical memory and peripheral devices the most common multiprocessor design is symmetric multiprocessing  or smp   where all processors are considered peers and run independently of one another clustered systems are a specialized form of multiprocessor systems and consist of multiple computer systems connected by a local area network  to best utilize the cpu  modern operating systems employ multiprogramming  which allows several jobs to be in memory at the same time  thus ensuring that the cpu always has a job to execute time-sharing systems are an extension of multiprogramming wherein cpu scheduling algorithms rapidly switch between jobs  thus providing the illusion that each job is nmning concurrently  the operating system must ensure correct operation of the computer system to prevent user programs from interfering with the proper operation of the system  the hardware has two modes  user mode and kernel mode various instructions  such as i/0 instructions and halt instructions  are privileged and can be executed only in kernel mode the memory in which the operating system resides must also be protected from modification by the user a tin1.er prevents infinite loops these facilities  dual mode  privileged instructions  memory protection  and timer interrupt  are basic building blocks used by operating systems to achieve correct operation  a process  or job  is the fundamental unit of work in an operating system  process management includes creating and deleting processes and providing mechanisms for processes to communicate and synchronize with each other  an operating system manages memory by keeping track of what parts of memory are being used and by whom the operating system is also responsible for dynamically allocating and freeing memory space storage space is also managed by the operating system ; this includes providing file systems for representing files and directories and managing space on mass-storage devices  operating systems must also be concerned with protecting and securing the operating system and users protection measures are mechanisms that control the access of processes or users to the resources made available by the 42 chapter 1 computer system security measures are responsible for defending a computer system from external or internal attacks  distributed systems allow users to share resources on geographically dispersed hosts connected via a computer network services may be provided through either the client-server model or the peer-to-peer n10del in a clustered system  multiple machines can perform computations on data residing on shared storage  and computing can continue even when some subset of cluster members fails  lans and wans are the two basic types of networks lans enable processors distributed over a small geographical area to communicate  whereas wans allow processors distributed over a larger area to communicate lans typically are faster than wans  there are several computer systems that serve specific purposes these include real-time operating systems designed for embedded environments such as consumer devices  automobiles  and robotics real-time operating systems have well-defined  fixed-time constraints processing must be done within the defined constraints  or the system will fail multimedia systems involve the delivery of multimedia data and often have special requirements of displaying or playing audio  video  or synchronized audio and video streams  recently  the influence of the internet and the world wide web has encouraged the development of operating systems that include web browsers and networking and communication software as integral features  the free software movement has created thousands of open-source projects  including operating systems because of these projects  students are able to use source code as a learning tool they can modify programs and test them  help find and fix bugs  and otherwise explore mature  full-featured operating systems  compilers  tools  user interfaces  and other types of programs  gnu /linux  bsd unix  and solaris are all open-source operating systems  the advantages of free software and open sourcing are likely to increase the number and quality of open-source projects  leadi.j.1.g to an increase in the number of individuals and companies that use these projects  1.1 how are network computers different from traditional personal computers describe some usage scenarios in which it is advantageous to use network computers  1.2 what network configuration would best suit the following environments a a dormitory floor b a university campus c a state d a nation 43 1.3 give two reasons why caches are useful what problems do they solve vvbat problems do they cause if a cache can be made as large as the device for which it is caching  for instance  a cache as large as a disk   why not make it that large and eliminate the device 1.4 under what circumstances would a user be better off using a timesharing system rather than a pc or a single-user workstation 1.5 list the four steps that are necessary to run a program on a completely dedicated machine-a computer that is running only that program  1.6 how does the distinction between kernel mode and user mode function as a rudimentary form of protection  security  system 1.7 in a multiprogramming and time-sharing environment  several users share the system simultaneously this situation can result in various security problems  a what are two such problems b can we ensure the same degree of security in a time-shared machine as in a dedicated machine explain your answer  1.8 describe a mechanism for enforcing memory protection in order to prevent a program from modifying the memory associated with other programs  1.9 what are the tradeoffs inherent in handheld computers 1.10 distinguish between the client-server and peer-to-peer models of distributed systems  1.11 some computer systems do not provide a privileged mode of operation in hardware is it possible to construct a secure operating system for these computer systems give arguments both that it is and that it is not possible  1.12 what are the main differences between operating systems for mainframe computers and personal computers 1.13 which of the following instructions should be privileged a set value of timer  b read the clock  44 chapter 1 c clear memory  d issue a trap instruction  e turn off interrupts  f modify entries in device-status table  g switch from user to kernel mode  h access i/o device  1.14 discuss  with examples  how the problem of maintaining coherence of cached data manifests itself in the following processing environments  a single-processor systems b multiprocessor systems c distributed systems 1.15 identify several advantages and several disadvantages of open-source operating systems include the types of people who would find each aspect to be an advantage or a disadvantage  1.16 how do clustered systems differ from multiprocessor systems what is required for two machines belonging to a cluster to cooperate to provide a highly available service 1.17 what is the main difficulty that a programmer must overcome in writing an operating system for a real-time environment 1.18 direct memory access is used for high-speed i/o devices in order to avoid increasing the cpu 's execution load  a how does the cpu interface with the device to coordinate the transfer b how does the cpu know when the memory operations are complete c the cpu is allowed to execute other programs while the dma controller is transferring data does this process interfere with the execution of the user programs if so  describe what forms of interference are caused  1.19 identify which of the functionalities listed below need to be supported by the operating system for  a  handheld devices and  b  real-time systems  a batch programming b virtual memory c time sharing 45 1.20 some cpus provide for more than two modes of operation what are two possible uses of these multiple modes 1.21 define the essential properties of the following types of operating systems  a batch b interactive c time sharing d real time e network f parallel a distributed b h clustered 1 handheld 1.22 describe the differences between symmetric and asymmetric multiprocessing  what are three advantages and one disadvantage of multiprocessor systems 1.23 the issue of resource utilization shows up in different forms in different types of operating systems list what resources must be managed carefully in the following settings  a mainframe or minicomputer systems b workstations connected to servers c handheld computers 1.24 what is the purpose of interrupts what are the differences between a trap and an interrupt can traps be generated intentionally by a user program if so  for what purpose 1.25 consider an smp system sincilar to what is shown in figure 1.6 illustrate with an example how data residing in memory could in fact have two different values in each of the local caches  1.26 consider a computing cluster consisting of two nodes running a database describe two ways in which the cluster software can manage access to the data on the disk discuss the benefits and disadvantages of each  46 chapter 1 brookshear  2003  provides an overview of computer science in general an overview of the linux operating system is presented in bovet and cesati  2006   solomon and russinovich  2000  give an overview of microsoft windows and considerable technical detail abmrt the systern internals and components russinovich and solomon  2005  update this information to windows server 2003 and windows xp mcdougall and mauro  2007  cover the internals of the solaris operating system mac os x is presented at http  i /www apple com/macosx mac os x internals are discussed in singh  2007   coverage of peer-to-peer systems includes parameswaran et al  2001   gong  2002   ripeanu et al  2002   agre  2003   balakrishnan et al  2003   and loo  2003   a discussion of peer-to-peer file-sharing systems can be found in lee  2003   good coverage of cluster computing is provided by buyya  1999   recent advances in cluster computing are described by ahmed  2000   a survey of issues relating to operating-system support for distributed systems can be found in tanenbaum and van renesse  1985   many general textbooks cover operating systems  including stallings  2000b   nutt  2004   and tanenbaum  2001   hamacher et al  2002  describe cmnputer organization  and mcdougall and laudon  2006  discuss multicore processors hennessy and patterson  2007  provide coverage of i/o systems and buses  and of system architecture in general blaauw and brooks  1997  describe details of the architecture of many computer systems  including several from ibm stokes  2007  provides an illustrated introduction to microprocessors and computer architecture  cache memories  including associative memory  are described and analyzed by smith  1982   that paper also includes an extensive bibliography on the subject  discussions concerning magnetic-disk technology are presented by freedman  1983  and by harker et al  1981   optical disks are covered by kenville  1982   fujitani  1984   o'leary and kitts  1985   gait  1988   and olsen and kenley  1989   discussions of floppy disks are offered by pechura and schoeffler  1983  and by sarisky  1983   general discussions concerning mass-storage technology are offered by chi  1982  and by hoagland  1985   kurose and ross  2005  and tanenbaum  2003  provide general overviews of computer networks fortier  1989  presents a detailed discussion of networking hardware and software kozierok  2005  discuss tcp in detail mullender  1993  provides an overview of distributed systems  2003  discusses recent developments in developing embedded systems issues related to handheld devices can be found in myers and beigl  2003  and dipietro and mancini  2003   a full discussion of the history of open sourcing and its benefits and challenges is found in raymond  1999   the history of hacking is discussed in levy  1994   the free software foundation has published its philosophy on its web site  http  //www.gnu.org/philosophy/free-software-for-freedom.html  detailed instructions on how to build the ubuntu linux kernel are on 47 http  i /www howtof orge com/kernelcompilation_ubuntu the open-source components of mac are available from http  i i developer apple com/ opensource/ index.html  wikipedia  http  i i en wikipedia org/wiki/richard_stallman  has an informative entry about richard stallman  the source code of multics is available at http  i /web .mit edu/multicshistory/ source/multics_internet_server/multics_sources.html  2.1 an operating system provides the environment within which programs are executed internally  operating systems vary greatly in their makeup  since they are organized along many different lines the design of a new operating system is a major task it is important that the goals of the system be well defined before the design begins these goals form the basis for choices among various algorithms and strategies  we can view an operating system from several vantage points one view focuses on the services that the system provides ; another  on the interface that it makes available to users and programmers ; a third  on its components and their interconnections in this chapter  we explore all three aspects of operating systems  showin.g the viewpoints of users  programmers  and operating-system designers we consider what services an operating system provides  how they are provided  how they are debugged  and what the various methodologies are for designing such systems finally  we describe how operating systems are created and how a computer starts its operating system  to describe the services an operating system provides to users  processes  and other systems  to discuss the various ways of structuring an operating system  to explain how operating systems are installed and customized and how they boot  an operating system provides an environment for the execution of programs  it provides certain services to programs and to the users of those programs  the specific services provided  of course  differ from one operating system to another  but we can identify common classes these operating-system services are provided for the convenience of the programmer  to n1.ake the programming 49 50 chapter 2 user and other system programs hardware figure 2 i a view of operating system services  task easier figure 2.1 shows one view of the various operating-system services and how they interrelate  one set of operating-system services provides functions that are helpfuj to the user ~ ~ user interface almost all operating systems have a this interface can take several forms one is a dcfr '  c ; ~  which uses text commands and a method for entering them  say  a program to allow entering and editing of commands   another is a batch in which commands and directives to control those commands are entered into files  and those files are executed most commonly  a is used here  the interface is a window system with a pointing device to direct i/0  choose from menus  and make selections and a keyboard to enter text some systems provide two or all three of these variations  program execution the system must be able to load a program into memory and to run that program the program must be able to end its execution  either normally or abnormally  indicating error   i/o operations a running program may require i/0  which may involve a file or an i/0 device for specific devices  special functions may be desired  such as recording to a cd or dvd drive or blanking a display screen   for efficiency and protection  users usually can not control i/0 devices directly  therefore  the operating system must provide a means to do i/0  file-system manipulation the file system is of particular interest obviously  programs need to read and write files and directories they also need to create and delete them by name  search for a given file  and list file information finally  some programs include permissions management to allow or deny access to files or directories based on file ownership many operating systems provide a variety of file systems  sometimes to allow personal choice  and sometimes to provide specific features or performance characteristics  2.1 51 communications there are many circumstances in which one process needs to exchange information with another process such communication ncay occur between processes that are executing on the same computer or between processes that are executing on different computer systems tied together by a computer network communications may be implemented via shared rnenwry or through message passing  in which packets of information are moved between processes by the operating system  error detection the operating system needs to be constantly aware of possible errors errors may occur in the cpu and memory hardware  such as a memory error or a power failure   in i/0 devices  such as a parity error on tape  a connection failure on a network  or lack of paper in the printer   and in the user program  such as an arithmetic overflow  an attempt to access an illegal memory location  or a too-great use of cpu time   for each type of error  the operating system should take the appropriate action to ensure correct and consistent computing of course  there is variation in how operating systems react to and correct errors debugging facilities can greatly enhance the user 's and programmer 's abilities to use the system efficiently  another set of operating-system functions exists not for helping the user but rather for ensuring the efficient operation of the system itself systems with multiple users can gain efficiency by sharing the computer resources among the users  resource allocation when there are i  lultiple usersormultiple jobs rmuung at the sametime  resources must be allocated to each of them  many d1herent -types of resources are managed by the operating system  some  such as cpu cycles  main memory  and file storage  may have special allocation code  whereas others  such as i/0 devices  may have much more general request and release code for instance  in determining how best to use the cpu  operating systems have cpu-scheduling routines that take into account the speed of the cpu  the jobs that must be executed  the number of registers available  and other factors there may also be routines to allocate printers  modems  usb storage drives  and other peripheral devices  accounting vl  e want to_keeptrack of whichusers use  lovy rnl1c  hand what kindsofcomputer resources this record keeping may be used for accoun  tii1g  so thai  users can be billed  or simply for accumulating usage statistics usage statistics may be a valuable tool for researchers who wish to reconfigure the system to improve computing services  protection and security the owners of information stored in a multiuser or networked computer system may want to control use of that information  when several separate processes execute concurrently  it ~ hould not be possible for one process to interfere with the others or with the operating system itself protection iiwolves ensuring that all access to systerr1 resources 1s -controlled security of the system from outsiders is also important such security starts with requiring each user to authenticate himself or herself to the system  usually by means of a password  to gain access to system resources it extends to defending external i/0 devices  52 chapter 2 2.2 including modems and network adapters  from invalid access attempts and to recording all such connections for detection of break-ins if a system is to be protected and secure  precautions must be instituted throughout it a chain is only as strong as its weakest link  we mentioned earlier that there are several ways for users to interface with the operating system here  we discuss two fundamental approaches one provides a command-line interface  or that allows users to directly enter commands to be performed by the operating system the other allows users to interface with the operating system via a graphical user interface  or gui  2.2.1 command interpreter some operating systems include the command interpreter in the kernel others  such as windows xp and unix  treat the command interpreter as a special program that is rmming when a job is initiated or when a user first logs on  on interactive systems   on systems with multiple command interpreters to choose from  the interpreters are known as shells for example  on unix and linux systems  a user may choose among several different shells  including the bourne shell  c shell  bourne-again shell  korn shell  and others third-party shells and free user-written shells are also available most shells provide similar functionality  and a user 's choice of which shell to use is generally based on personal preference figure 2.2 shows the bourne shell command interpreter being used on solaris 10  the main function of the command interpreter is to get and execute the next user-specified command many of the commands given at this level manipulate files  create  delete  list  print  copy  execute  and so on the ms-dos and unix shells operate in this way these commands can be implemented in two general ways  in one approach  the command interpreter itself contains the code to execute the command for example  a command to delete a file may cause the command interpreter to jump to a section of its code that sets up the parameters and makes the appropriate system call in this case  the number of comn'lands that can be given determines the size of the command interpreter  since each command requires its own implementing code  an alternative approach -used by unix  among other operating systems -implements most commands through system programs in this case  the command interpreter does not understand the cmnmand in any way ; it merely uses the command to identify a file to be loaded into memory and executed  thus  the unix command to delete a file rm file.txt would search for a file called rm  load the file into memory  and execute it with the parameter file txt the function associated with the rm command would be defined completely by the code in the file rm in this way  programmers can add new commands to the system easily by creating new files with the proper 0.0 0.0 r/s 0.0 0.6 console 2.2 0.2 0.0 0.2 0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 extended device statistics w/s 0.0 0.0 kr./s klv/s 0.0 0.0 0.0 1 ogi nell idle 1sj un0718days wai t actv svc_t 9  tn 1i ~ b 0.0 0.0 0.0 0 0 0 0 0 load average  0.09  0.11  8.66 jcpu pcpu what 1 /usr/bin/ssh-agent  /usr/bi 18 4 w figure 2.2 the bourne shell command interpreter in solaris i 0  53 names the command-interpreter program  which can be small  does not have to be changed for new commands to be added  2.2.2 graphical user interfaces a second strategy for interfacing with the operating system is through a userfriendly graphical user interface  or cui here  rather than entering commands directly via a command-line interface  users employ a mouse-based windowand nl.enu system characterized by a metaphor the user moves the mouse to position its pointer on images  or on the screen  the desktop  that represent programs  files  directories  and system functions depending on the mouse pointer 's location  clicking a button on the mouse can invoke a program  select a file or directory-known as a folder-or pull down a menu that contains commands  graphical user interfaces first appeared due in part to research taking place in the early 1970s at xerox parc research facility the first cui appeared on the xerox alto computer in 1973 however  graphical interfaces became more widespread with the advent of apple macintosh computers in the 1980s the user interface for the macintosh operating system  mac os  has undergone various changes over the years  the most significant being the adoption of the aqua interface that appeared with mac os x microsoft 's first version of windows-version 1.0-was based on the addition of a cui interface to the ms-dos operating system later versions of windows have made cosmetic changes in the appearance of the cui along with several enhancements in its functionality  including windows explorer  54 chapter 2 traditionally  unix systencs have been dominated by command-line interfaces  various gul interfaces are available  however  including the common desktop environment  cde  and x-windows systems  which are common on commercial versions of unix  such as solaris and ibm 's aix system in addition  there has been significant development in gui designs from various projects  such as i desktop environment  or kde  and the gnome desktop by the gnu project both the kde and gnome desktops run on linux and various unix systems and are available under open-source licenses  which means their source code is readily available for reading and for modification under specific license terms  the choice of whether to use a command-line or gui interface is mostly one of personal preference as a very general rule  many unix users prefer command-line interfaces  as they often provide powerful shell interfaces  in contrast  most windows users are pleased to use the windows gui environment and almost never use the ms-dos shell interface the various changes undergone by the macintosh operating systems provide a nice study in contrast historically  mac os has not provided a command-line interface  always requiring its users to interface with the operating system using its gui  however  with the release of mac os x  which is in part implemented using a unix kernel   the operating system now provides both a new aqua interface and a command-line interface figure 2.3 is a screenshot of the mac os x gui  the user interface can vary from system to system and even from user to user within a system it typically is substantially removed from the actual system structure the design of a useful and friendly user interface is therefore figure 2.3 the mac os x gui  2.3 2.3 55 not a direct function of the operating systenc in this book  we concentrate on the fundamental problems of providing adequate service to user programs  from the point of view of the operating system  we do not distinguish between user programs and systern programs  system calls provide an interface to the services made available by an operating system these calls are generally available as routines written in c and c + +  although certain low-level tasks  for example  tasks where hardware must be accessed directly   may need to be written using assembly-language instructions  before we discuss how an operating system makes system calls available  let 's first use an example to illustrate how system calls are used  writing a simple program to read data from one file and copy them to another file the first input that the program will need is the names of the two files  the input file and the output file these names can be specified in many ways  depending on the operating-system design one approach is for the program to ask the user for the names of the two files in an interactive system  this approach will require a sequence of system calls  first to write a prompting message on the screen and then to read from the keyboard the characters that define the two files on mouse-based and icon-based systems  a menu of file names is usually displayed in a window the user can then use the mouse to select the source name  and a window can be opened for the destination name to be specified  this sequence requires many i/0 system calls  once the two file names are obtained  the program must open the input file and create the output file each of these operations requires another system call  there are also possible error conditions for each operation when the program tries to open the input file  it may find that there is no file of that name or that the file is protected against access in these cases  the program should print a message on the console  another sequence of system calls  and then terminate abnormally  another system call   if the input file exists  then we must create a new output file we may find that there is already an output file with the same name this situation may cause the program to abort  a system call   or we may delete the existing file  another system call  and create a new one  another system call   another option  in an interactive system  is to ask the user  via a sequence of system calls to output the prompting message and to read the response from the termin.al  whether to replace the existing file or to abort the program  now that both files are set up  we enter a loop that reads from the input file  a system call  and writes to the output file  another system call   each read and write must return status information regarding various possible error conditions on input  the program may find that the end of the file has been reached or that there was a hardware failure in the read  such as a parity error   the write operation may encounter various errors  depending on the output device  no more disk space  printer out of paper  and so on   finally  after the entire file is copied  the program may close both files  another system call   write a message to the console or window  more system calls   and finally terminate normally  the final system call   as we 56 chapter 2 can see1 even simple programs may make heavy use of the operating system  frequently/ systems execute thousands of system calls per second this systemcall sequence is shown in figure 2a  most programmers never see this level of detail however typically/ applicatiol1 developers design program.s accordir1g to an   ~ jl ~ j'i   tl1e aj'ispecifies a set of functions application programmer/ including the parameters that are passed to each function and the return values the programmer can expect three of the most common apis available to application programmers are the win32 api for windows systems  the posix api for posix-based systems  which include virtually all versions of unix  linux/ and mac os x   and the java api for designing programs that run on the java virtual machine note that-unless specified -the system-call names used throughout this text are generic examples each operating system has its own name for each system call  behind the scenes/ the functions that make up an api typically invoke the actual system calls on behalf of the application programmer for example  the win32 function createprocess    which unsurprisingly is used to create a new process  actually calls the ntcreateprocess   system call in the windows kernel why would an application programnl.er prefer programming according to an api rather than invoking actual system calls there are several reasons for doing so one benefit of programming according to an api concerns program portability  an application programmer designing a program using an api can expect her program to compile and run on any system that supports the same api  although in reality/ architectural differences often make this more difficult than it may appear   furthermore/ actual system calls can often be more detailed and difficult to work with than the api available to an application programmer regardless/ there often exists a strong correlation between a function in the api and its associated system call within the kernel  example system call sequence acquire input file name write prompt to screen accept input acquire output file name write prompt to screen accept input open the input file if file does n't exist  abort create output file if file exists  abort loop read from input file write to output file until read fails close output file write completion message to screen terminate normally figure 2.4 example of how system calls are used  2.3 example of standard api as an example of a standard apt  consider the readfile 0 unction in the win32 api-a function for reading rom a file the api for this function appears in figure 2.5    return value ~ bool readfile c t function name  handle lpvoid dword lpdword lpoverlapped file  ~ buffer  bytes to read  parameters bytes read  ovl  ; figure 2.5 the api for the readfile   function  a description of the parameters passed to readfile 0 is as follows  handle file-the file to be read lpvoid buffer-a buffer where the data will be read into and written from dword bytestoread-the number of bytes to be read into the buffer lpdword bytesread -the number of bytes read during the last read lpoverlapped ovl-indicates if overlapped i/0 is being used 57 in fact  many of the posix and win32 apis are similar to the native system calls provided by the unix  linux  and windows operating systems  the run-time support system  a set of functions built into libraries included with a compiler  for most programming languages provides a system-call interface that serves as the link to system calls made available by the operating system the system-call interface intercepts function calls in the api and invokes the necessary system calls within the operating system typically  a number is associated with each system call  and the system-call interface maintains a table indexed according to these nun'lbers the system call interface then invokes the intended system call in the operating-system kernel and returns the status of the system call and any return values  the caller need know nothing about how the system call is implemented or what it does during execution rathel ~ it need only obey the api and understand what the operating system will do as a result of the execution of that system call thus  most of the details of the operating-system interface are hidden from the programmer by the api and are managed by the run-time support library  the relationship between an api  the system-call interface  and the operating 58 chapter 2 2.4 user mode kernel mode user application opeo    j open   implementation of open   system call return figure 2.6 the handling of a user application invoking the open   system call  system is shown in figure 2.6  which illustrates how the operating system handles a user application invoking the open   system call  system calls occur in different ways  depending onthe coj  rlpl1te.rjjlll e  often  more information is required than simply the identity of the desired system call the exact type and ammmt of information vary according to the particular operating system and call for example  to get input  we may need to specify the file or device to use as the source  as well as the address and length of the memory buffer into which the input should be read of course  the device or file and length may be implicit in the call  three general methods are used to pass parameters to the operating system the simplest approach is to pass the param.eters in registers in some cases  however  there may be more parameters than registers in these cases  the parameters are generally stored in a block  or table  in memory  and the address of the block is passed as a parameter in a register  figure 2.7   this is the approach taken by linux and solaris parameters also can be placed  or pushed  onto the stack by the program and popped oh the stacl  by the operatirl  g ~ yste111  some operating syste1ns prefer the block or stack method because those approaches do not limit the number or length of parameters being passed  system calls can be grouped ~ oughly intc  six major categories  process control  file manipujation  device manipulation  information maintenance  coinmuiii ~ a1ioii ~ ;  lndpr   tediol   in seci  lo  ri.s 2.4.l ~ hi.i = o ~ l.gli 2.l  6 ~ we discllss briefly the types of system calls that may be provided by an operating system  most of these system calls support  or are supported by  concepts and functions x  parameters for call load address x system call 13 +  ~  user program 2.4 register operating system figure 2.7 passing of parameters as a table  59 that are discussed in later chapters figure 2.8 summarizes the types of system calls normally provided by an operating system  2.4.1 process control a running program needs to be able to halt its execution either normally  end  or abnormally  abort   if a system call is made to terminate the currently ruru1il1g program abnormally  or if the program runs into a problem and causes an error trap  a dump of memory is sometimes taken and an error message generated the dump is written to disk and may be examined by a system program designed to aid the programmer in finding and correcting bugs-to determine the cause of the problem under either normal or abnormal circumstances  the operating system must transfer control to the invoking command mterpreter the command interpreter then reads the next cominand in an interactive system  the command interpreter simply continues with the next command ; it is assumed that the user will issue an appropriate command to respond to any error in a gui system  a pop-up wmdow might alert the user to the error and ask for guidance in a batch system  the command interpreter usually terminates the entire job and continues with the next job  some systems allow control cards to indicate special recovery actions in case an error occurs a is a batch-system concept it is a command to manage the execution of a process if the program discovers an error in its input and wants to terminate abnormally  it may also want to define an error level  more severe errors can be indicated by a higher-level error parameter it is then possible to combi11e normal and abnormal termination by defining a normal termination as an error at level 0 the command interpreter or a following program can use this error level to determine the next action automatically  a process or jobexecuting one p !   gral11_11l  ly _ \ ; \ '  ll1tto joad andexecut ~ anoteer pro-gra1  n  . th1s feafl  11  e allows the cmnmand i11terpreter to execute a program as directed by  for example  a user command  the click of a mouse  or a batch command an interesting question is where to return control when the loaded program terminates this question is related to the problem of 60 chapter 2 process control o end  abort o load  execute o create process  terminate process o get process attributes  set process attributes o wait for time o wait event  signal event o allocate and free memory file management o create file  delete file o open  close o read  write  reposition o get file attributes  set file attributes e  device management o request device  release device o read  write  reposition o get device attributes  set device attributes o logically attach or detach devices information maintenance o get time or date  set time or date o get system data  set system data o get process  file  or device attributes o set process  file  or device attributes communications o create  delete communication connection o send  receive messages o transfer status information o attach or detach remote devices figure 2.8 types of system calls  whether the existing program is lost  saved  or allowed to continue execution concurrently with the new program  if control returns to the existing program when the new program terminates  we must save the memory image of the existing program ; thus  we have effectively created a mechanism for one program to call another program if both programs continue concurrently  we have created a new job or process to 2.4 61 examples of windows and unix system calls windows unix process createprocesso fork   control exi tprocess   exit   waitforsingleobject   wait   file createfile   open   manipulation readfile   read   writefile   write   closehandle   close   device setconsolemode   ioctl   manipulation readconsole   read   writeconsole   write   information getcurrentprocessid   getpid   maintenance settimero alarm   sleep   sleep   communication createpipe   pipe   createfilemapping   shmget   mapviewoffile   mmapo protection setfilesecurity   chmod   initlializesecuritydescriptor   umask   setsecuritydescriptorgroup   chown   be multi programmed often  there is a system call specifically for this purpose  create process or submit job   if we create a new job or process  or perhaps even a set of jobs or processes  we should be able to control its execution this control requires the ability to determine and reset the attributes of a job or process  including the job 's priority  its maximum allowable execution time  and so on  get process attributes and set process attributes   we may also want to terminate a job or process that we created  terminate process  if we find that it is incorrect or is no longer needed  having created new jobs or processes  we may need to wait for them to finish their execution we may want to wait for a certain amount of time to pass  wait time  ; more probably  we will want to wait for a specific event to occur  wait event   the jobs or processes should then signal when that event has occurred  signal event   quite often  two or more processes may share data to ensure the integrity of the data being shared  operating systems often provide system calls allowing a process to lock shared data  thus preventing another process from accessing the data while it is locked typically such system calls include acquire lock and release lock system calls of these 62 chapter 2 example of standard c library the standard c library provides a portion o the system-call interface for many versions of unix and linux as an example  let 's assume a c program invokes the printf   statement the c library intercepts this call and invokes the necessary system call  s  in the operating system-in this instance  the write   system call the c library takes the value returned by write   and passes it back to the user program this is shown in figure 2.9  user mode kernel mode i i # include stdio.h int main     printf  greetings  ; i + return 0 ; standard c library write   system call i i  figure 2.9 standard c library handling of write    types  dealilcg with the coordination of concurrent processes  are discussed in great detail in chapter 6  there are so many facets of and variations in process and job control that we next use two examples-one involving a single-tasking system and the other a multitasking system -to clarify these concepts the ms-dos operating system is an example of a single-tasking system it has a command interpreter that is invoked when the computer is started  figure 2.10  a    because ms-dos is single-tasking  it uses a sincple method to run a program and does not create a new process it loads the program into memory  writing over most of itself to give the program as much memory as possible  figure 2.10  b    next  it sets the instruction pointer to the first instruction of the program the program then runs  and either an error causes a trap  or the program executes a system call to terminate in either case  the error code is saved in the system memory for later use following this action  the small portion of the command interpreter that was not overwritten resumes execution its first task is to reload the rest free memory command interpreter  a  2.4 free memory process command interpreter  b  figure 2.10 ms-dos execution  a  at system startup  b  running a program  63 of the command interpreter from disk then the command interpreter makes the previous error code available to the user or to the next program  fre ~ _j _s_i   der_i_ \ ' ~ c ! jr   in b   j  ~ eley unix  is an example of a multitasking syst   ' ~ when a user logs on to the system ~ the shell otthe user 's-choiceis run this shell is similar to the ms-dos shell in that it accepts commands and executes programs that the user requests however  since freebsd is a multitasking system  the command interpreter may continue running while another program is executed  figure 2.11   io startanew  __process,_th_es1w1l execu ~ 2 \ _  for-k   sy ~ tem call then  the selected program is loaded into memory via an exec   system call  and the program is executed depending on the way the command was issued  the shell then either waits for the process to finish or runs the process in the background in the latter case  the shell immediately requests another command when a process is rmming in the background  it can not receive input directly fron1 the keyboard  because the process d free memory process c interpreter figure 2.11 freebsd running multiple programs  64 chapter 2 shell is using this resource i/o is therefore done through files or through a cui interface meanwhile  the user is free to ask the shell to run other programs  to monitor the progress of the running process  to change that program 's priority  and so on when the process is done  it executes an exit   system call to terminate  returning to the invoking process a status code of 0 or a nonzero error code this status or error code is then available to the shell or other programs processes are discussed in chapter 3 with a program example using thefork   and exec   systemcalls  2.4.2 file management the file system is discussed in more detail in chapters 10 and 11 we can  however  identify several common system calls dealing with files  we first need to be able to create and delete files either system call requires the name of the file and perhaps some of the file 's attributes once the file is created  we need to open it and to use it we may also read  write  or reposition  rewinding or skipping to the end of the file  for example   finally  we need to close the file  indicating that we are no longer using it  we may need these same sets of operations for directories if we have a directory structure for organizing files in the file system in addition  for either files or directories  we need to be able to determine the values of various attributes and perhaps to reset them if necessary file attributes include the file name  file type  protection codes  accounting information  and so on at least two system calls  get file attribute and set file attribute  are required for this function some operating systems provide many more calls  such as calls for file move and copy others might provide an api that performs those operations using code and other system calls  and others might just provide system programs to perform those tasks if the system programs are callable by other programs  then each can be considered an api by other system programs  2.4.3 device management a process may need several resources to execute-main memory  disk drives  access to files  and so on if the resources are available  they can be granted  and control can be returned to the user process otherwise  the process will have to wait until sufficient resources are available  the various resources controlled by the operating system can be thought of as devices some of these devices are physical devices  for example  disk drives   while others can be thought of as abstract or virtual devices  for example  files   a system with multiple users may require us to first request the device  to ensure exclusive use of it after we are finished with the device  we release it these functions are similar to the open and close system calls for files other operating systems allow llnmanaged access to devices  the hazard then is the potential for device contention and perhaps deadlock  which is described in chapter 7  once the device has been requested  and allocated to us   we can read  write  and  possibly  reposition the device  just as we can with files in fact  the similarity between i/0 devices and files is so great that many operating systems  including unix  merge the two into a combined file-device structure  in this case  a set of system calls is used on both files and devices sometimes  2.4 65 l/0 devices are identified by special file names  directory placement  or file attributes  the user interface can also ncake files and devices appear to be similar1 even though the underlying system calls are dissimilar this is another example of the many design decisions that go into building an operating system and user interface  2.4.4 information maintenance many system calls exist simply for the purpose of transferring information between the user program and the operating system for example  most systems have a system call to return the current time and date other system calls may return information about the system  such as the number of current users  the version number of the operating system  the amount of free memory or disk space  and so on  another set of system calls is helpful in debugging a program many systems provide system calls to dump memory this provision is useful for debugging a program trace lists each system call as it is executed even microprocessors provide a cpu mode known as single step  in which a trap is executed by the cpu after every instruction the trap is usually caught by a debugger  many operating systems provide a time profile of a program to indicate the amount of time that the program executes at a particular location or set of locations a time prof ~ ~ ~ ~  c_92  1i ! ~ ~ ~ i ! ! 'cee a t ~  lc ~ ki2l  ility_s e !  egl1lar tii ! '_ee interrupts at every occurrence of the timer interrupt  the value of the program c6l-i  i ~ te1 -ls recorded with sufficiently frequent timer interrupts  a statistical picture of the time spent on various parts of the program can be obtained  in addition  the operating system keeps information about all its processes  and system calls are used to access this information generally  calls are also used to reset the process information  get process attributes and set process attributes   in section 3.1.3  we discuss what information is normally kept  2.4.5 communication th ~ ~ e ~ e two c   ll1l ~ cj  ji1_ _m od_e_l ~ _   fi ! ' ! e_ ! el   _c ~ ss_col'rll ! ' ~ ~ nica tion  the  l ! ' ~ ssag_e   _ passing model and the shared-memory model ! nth ~ il ! ~ s ~ ~ g_e  pa,s ~ ij1gl ! '   'lel t_l  t_ ~ _c    rrtll12injfa_fii ~ gpr c ~ ~  ~ ~ -e   c lailg ~ il'l-es ~ ~ ges with one another to transfer i  tcfo_rillaji   j   messages can be exchanged between the processes either directly or indirectly through a common mailbox before communication can take place  a connection must be opened the name of the other communicator must be known  be it another process on the same system or a process on another computer comcected by a communications network each computer in a network has a host name by which it is commonly known a host also has a network identifier  such as an ip address similarly  each process has a process narne  and this name is translated into an identifier by which the operating systemcanrefertotheprocess the get hostidand get processid system calls do this translation the identifiers are then passed to the generalpurpose open and close calls provided by the file system or to specific open connection and close connection system calls  depending on the system 's model of communication the recipient process usually must give its 66 chapter 2 2.5 permission for comnmnication to take place with an accept connection call  most processes that will be receiving connections are special-purpose daemons  which are systems programs provided for that purpose they execute a wait for connection call and are awakened when a connection is rna de the source of the communication  known as the client  and the receiving daenwn  known as a server  then exchange messages by using read message and write message system calls the close connection call terminates the communication  _ ! 11 the shared-me_1llorytllodel,proc ~ sses use s  tlared memorycreate and shared memory attach system calls to create 2rt1d gain access toi egions ot n1emory owned by other processes recall that  normally  the operatinisystein hiesf   prevei1foiie process-from accessing another process 's memory shared memory requires that two or more processes agree to remove this restriction  they can then exchange information by reading and writing data in the shared areas the form of the data is determined by the processes and are not under the operating system 's control the processes are also responsible for ensuring that they are not writing to the same location sirnultaneously such mechanisms are discussed in chapter 6 in chapter 4  we look at a variation of the process scheme-threads-in which memory is shared by default  both of the models just discussed are common in operating systems  and most systems implement both message passing is useful for exchanging smaller amounts of data  because no conflicts need be avoided it is also easier to implement than is shared memory for intercomputer communication shared memory allows maximum speed and convenience of communication  since it can be done at memory transfer speeds when it takes place within a computer  problems exist  however  in the areas of protection and synchronization between the processes sharing memory  2.4.6 protection protection provides a mechanism for controlling access to the resources provided by a computer system historically  protection was a concern only on multiprogrammed computer systems with several users however  with the advent of networking and the internet  all computer systems  from servers to pdas  must be concerned with protection  typically  system calls providing protection include set permission and get permission  which manipulate the permission settings of resources such as files and disks the allow user and deny user system calls specify whether particular users can-or can not-be allowed access to certain resources  we cover protection in chapter 14 and the much larger issue of security in chapter 15  another aspect of a modern system is the collection of system programs recall figure 1.1  which depicted the logical computer hierarchy at the lowest level is hardware next is the operating system  then the system programs  and finally the application programs system programs  also known as system utilities  provide a convenient enviromnenf1orprograrn-aevelopmeiita1inexecuhon  2.5 67 some of them are simply user interfaces to system calls ; others are considerably more complex they can be divided into these categories  file management these programs create  delete  copy  rename  print  dump  list  and generally ncanipulate files and directories  status information some programs simply ask the system for the date  time  amount of available memory or disk space  number of users  or similar status information others are more complex  providing detailed performance  logging  and debugging information typically  these programs format and print the output to the terminal or other output devices or files or display it in a window of the gui some systems also support a which is used to store and retrieve configuration information  file modification several text editors may be available to create and modify the content of files stored on disk or other storage devices there may also be special commands to search contents of files or perform transformations of the text  programming-language support compilers  assemblers  debuggers  and interpreters for common programming languages  such as c  c + +  java  visual basic  and perl  are often provided to the user with the operating system  program loading and execution once a program is assembled or compiled  it must be loaded into memory to be executed the system may provide absolute loaders  relocatable loaders  linkage editors  and overlay loaders debugging systems for either higher-level languages or machine language are needed as well  communications these programs provide the mechanism for creating virtual comcections among processes  users  and computer systems they allow users to send rnessages to one another 's screens  to browse web pages  to send electronic-mail messages  to log in remotely  or to transfer files from one machine to another  in addition to systems programs  most operating systems are supplied with programs that are useful in solving common problems or performing common operations such application  jj  ogr ! lj1ls iitclllde'if \ t ~ l  l l  jrg_wsf2r ~  worg processors an i text f6-rinatteis,spreadsheets  database systems  compilers  plott1i1g ana s-tafistica  -analysis packages  ancl gan1es ~        ___ tne viewoclne opei ; ating-sysrerri-seen b  t inost users is defined by the application and system programs  rather than by the actual systern calls  consider a user 's pc when a user 's computer is rumcing the mac os x operating system  the user might see the gui  featuring a mouse-and-windows interface alternatively  or even in one of the windows  the user might have a command-line unix shell both use the same set of system calls  but the system calls look different and act in different ways further confusing the user view  consider the user dual-booting from mac os x into windows vista  now the same user on the same hardware has two entirely different interfaces and two sets of applications using the same physical resources on the same 68 chapter 2 2.6 hardware  then  a user can be exposed to multiple user interfaces sequentially or concurrently  in this section  we discuss problems we face in designing and implementing an operating system there are  of course  no complete solutions to such problems  but there are approaches that have proved successful  2.6.1 design goals the first problem in designing a system is to define goals and specifications  at the highest level  the design of the system will be affected by the choice of hardware and the type of system  batch  time shared  single user  multiuser  distributed  real time  or general purpose  beyond this highest design level  the requirements may be much harder to specify the requirements can  however  be divided into two basic groups  user goals and system goals  users desire certain obvious properties in a system the system should be convenient to use  easy to learn and to use  reliable  safe  and fast of course  these specifications are not particularly useful in the system design  since there is no general agreement on how to achieve them  a similar set of requirements can be defined by those people who must design  create  maintain  and operate the system the system should be easy to design  implement  and maintain ; and it should be flexible  reliable  error free  and efficient again  these requirements are vague and may be interpreted in various ways  there is  in short  no unique solution to the problem of defining the requirements for an operating system the wide range of systems in existence shows that different requirements can result in a large variety of solutions for different environments for example  the requirements for vxworks  a realtime operating system for embedded systems  must have been substantially different from those for mvs  a large multiuser  multiaccess operating system for ibm mainframes  specifying and designing an operating system is a highly creative task  although no textbook can tell you how to do it  general principles have been developed in the field of software engineering  and we turn now to a discussion of some of these principles c  2.6.2 mechanisms and policies   i one important principle is the separation of policy from mechanisiil ~ echa   1'lis ~ s   leter111il1e hcnu ! q_c  @ -son'l ~ tl-til1g ; p   lic  les  i ~ termir e  zul1dt wilcbe done  for example  the timer construct  see section 1.5.2  is a mechani.sril  -forensill1ng cpu protection  but deciding how long the timer is to be set for a particular user is a policy decision  _  'h ~ _s_ 122l ! cl_tig ! l    fp.qli_cy_an_ci ~ t1_echanism is imp   rtant for flexibility policies are likely to change across places o1  'over time 'rri tll'e worst case  each change in policy would require a change in the underlying mechanism a general mechanism insensitive to changes in policy would be more desirable a change 2.6 69 in policy would then require redefinition of only certain parameters of the system for instance  consider a mechanism for giving priority to certain types of programs over others if the mechanism is properly separated from policy  it can be used either to support a policy decision that i/o-intensive progran1.s should have priority over cpu-intensive ones or to support the opposite policy  microkernel = based operati1lg sy_sh  ~ ms  section 2 .3  take the separation of mechai ~ 1sinai ~ cfp hcyto one extreme byimplementing a basicset   j_pri111.iti_y ~ 1jiwding bfocks these blocks are almost policy free  allowing more advanced -1necharnsms and policies to be added via user-created kernel modules or via user programs themselves as an example  consider the history of unix at first  it had a time-sharing scheduler in the latest version of solaris  scheduling is controlled by loadable tables depending on the table currently loaded  the system can be time shared  batch processing  real time  fair share  or any combination making the scheduling mechanism general purpose allows vast policy changes to be made with a single load-new-table command at th_ ~   th ~ r extreme is_il_ ~ ~ ~ t ~ l il ~  ttc  l ~ -as _ \ 1 \ t'i_ ! l_t  l   ! yj'c_ ~ \ 1 \ t ~ ~ icj  l ~ qt ~ j1 lec  ! '.c  l ~ ~ ~ 1l ~ and_p   _1i_c_y__a_  r ~ _epc    ciec  lj ~ 1._ ! he sy ~ te ~ _ t  j_e_ilforce__ ~ gl   ~ ~ l   ok an_cl_ fe_el all applications have similar interfaces  because the interface itself is built into the kernel and system libraries the mac os x operating system has similar functionality  policy decisions are important for all resource allocation whenever it is necessary to decide whether or not to allocate a resource  a policy decision must be made whenever the question is how rather than what  it is a mechanism that must be determined  2.6.3 implementation once an operating system is designed  it must be implemented traditionally  operating systems have been written in assembly language now  however  they are most commonly written in higher-level languages such as cor c + +  the first system that was not written in assembly language was probably the master control program  mcp  for burroughs computers mcp was written in a variant of algol multics  developed at mit  was written mainly in pl/1 the linux and windows xp operating systems are written mostly in c  although there are some small sections of assembly code for device drivers and for saving and restoring the state of registers  the advantages of using a higher-level language  or at least a systemsimplementation language  for implementing operating systems are the same as those accrued when the language is used for application programs  the code can be written faster  is more compact  and is easier to understand and debug in addition  improvements in compiler technology will improve the generated code for the entire operating system by simple recompilation finally  an operating system is far easier to port-to move to some other hardware-if it is written in a higher-level language for example  ms-dos was written in intel 8088 assembly language consequently  it runs natively only on the intel x86 family of cpus  although ms-dos runs natively only on intel x86  emulators of the x86 instruction set allow the operating system to run non-nativelyslower  with more resource use-on other cpus are programs that duplicate the functionality of one system with another system  the linux 70 chapter 2 2.7 operating system  in contrast  is written mostly inc and is available natively on a number of different cpus  including intel x86  sun sparc  and ibmpowerpc  the only possible disadvantages of implementing an operating system in a higher-level language are reduced speed and increased storage requirements  this  howeve1 ~ is no longer a major issue in today 's systems although an expert assembly-language programmer can produce efficient small routines  for large programs a modern compiler can perform complex analysis and apply sophisticated optimizations that produce excellent code modern processors have deep pipelining and n1.ultiple functional units that can handle the details of complex dependencies much more easily than can the human mind  as is true in other systems  major performance improvements in operating systems are more likely to be the result of better data structures and algorithms than of excellent assembly-language code in addition  although operating systems are large  only a small amount of the code is critical to high performance ; the memory manager and the cpu scheduler are probably the most critical routines  after the system is written and is working correctly  bottleneck routines can be identified and can be replaced with assembly-language equivalents  a system as large and complex as a modern operating system must be engineered carefully if it is to function properly and be modified easily a common approach is to partition the task into small components rather than have one monolithic system each of these modules should be a well-defined portion of the system  with carefully defined inputs  outputs  and functions  we have already discussed briefly in chapter 1 the common components of operating systems in this section  we discuss how these components are interconnected and melded into a kernel  2.7.1 simple structure many commercial operating systen1.s do not have well-defined structures  frequently  such systems started as small  simple  and limited systems and then grew beyond their original scope ms-dos is an example of such a systen1  it was originally designed and implemented by a few people who had no idea that it would become so popular it was written to provide the most functionality in the least space  so it was not divided into modules carefully  figure 2.12 shows its structure  in ms-dos  the interfaces and levels of functionality are not wellseparated  for rnstai1.ce  appii.cat1on programs aie able to access the basic i  b 1  outiri.es to write directly to the display and disk drives such freedom leaves ms-dos vulnerable to errant  or malicio lls  programs  causing entire system crashes when user programs fail of course  ms-dos was also limited by the hardware of its era because the intel 8088 for which it was written provides no dual mode and no hardware protection  the designers of ms-dos had no choice but to leave the base hardware accessible  another example of limited structuring is the original unix operating systein like ms ~ dc5s  unix initially was limited by hard ware ft1il.cfionali.ty ft consistsoftwo separahlepai ; fs  thei  eril.el ai1d the system prograrns  thekei  nel 2.7 71 rom bios device drivers figure 2.12 ms-dos layer structure  is further separated into a series of interfaces and device drivers  which have been added and expanded over the years as unix has evolved we can view the traditional unix operating system as being layered  as shown in figure 2.13  everything below the system-call interface and above the physical hardware is the kernel tb ~ l  ol ll ~ lp  rgvides__i  h ~ _fil ~ syste  rn  c   p_l ! _s ~ h ~ dulijl,g  memory management  and other operating-system fm1ctions through system calls  taken i.n sum ~ thatl.sai1 enormous an1ol.lnt of functionality to be combined into one level this monolithic structure was difficult to implement and maintain  2.7.2 layered approach withproper j  tarc  l \ a ! lre support  operating systems can be brokeninto pieces that are smaller and more app1  opriate thar  t  hose allowed by the _ _2 ! i2 ; g ~ af  the users  shells and commands compilers and interpreters system libraries signals terminal handling character 1/0 system terminal drivers file system swapping block 1/0 system disk and tape drivers cpu scheduling page replacement demand paging virtual memory figure 2.13 traditional unix system structure  72 chapter 2 figure 2.14 a layered operating system  m ~   .qoi'ilncil  l'j_ix systeill ~ the operating system can then retain much greater control over the computer and over the applications that make use of that computer implementers have more freedom in changing the inner workin.gs of the system and in creating modular operating systems under a topdown approach  the overall functionality and features are determined and are separated into components information hiding is also important  because it leaves programmers free to implement the low-level routines as they see fit  provided that the external interface of the routine stays unchanged and that the routine itself performs the advertised task  a system can be made modular in many ways qne method is the layered approach  in which the operating system is broken ii1to a 1l.umberoflayers  lever8j.ti1eoottom.iiiyer  layer 0  .1sthetiarawai ; e ; the nig  ytesl   layern   1sfhe user interface this layering structure is depicted in figure 2.14  an operating-system layer is an implementation of an abstract object made up of data and the operations that can manipulate those data a typical operating-system layer-say  layer m -consists of data structures and a set of routines that can be invoked by higher-level layers layer m  in turn  can invoke operations on lower-level layers  the main advantage of the layered approach is simplicity of construction and debugging the layers are selected so that each uses functions  operations  and services of only lower-level layers this approach simplifies debugging and .system verification the first layer can be debugged without any concern for the rest of the system  because  by definition  it uses only the basic hardware  which is assumed correct  to implement its functions once the first layer is debugged  its correct functioning can be assumed while the second layer is debugged  and so on if an error is found during the debugging of a particular layer  the error must be on that layer  because the layers below it are already debugged thus  the design and implementation of the system are simplified  2.7 73 each layer is implemented with only those operations provided by lowerlevel layers a layer does not need to know how these operations are implemented ; it needs to know only what these operations do hence  each layer hides the existence of certain data structures  operations  and hardware from higher-level layers  the major difficulty with the layered approach involves appropriately defining the various layers because a layer can use only lower-level layers  careful planning is necessary for example  the device driver for the backing store  disk space used by virtual-memory algorithms  must be at a lower level than the memory-management routines  because memory management requires the ability to use the backing store  other requirements may not be so obvious the backing-store driver would normally be above the cpu scheduler  because the driver may need to wait for i/0 and the cpu can be rescheduled during this time however  on a large system  the cpu scheduler m.ay have more information about all the active processes than can fit in memory therefore  this u1.formation may need to be swapped u1 and out of memory  requiring the backu1.g-store driver routine to be below the cpu scheduler  a final problem with layered implementations is that they tend to be less efficient than other types for instance  when a user program executes an i/0 operation  it executes a system call that is trapped to the i/0 layer  which calls the memory-management laye1 ~ which in tum calls the cpu-scheduling layer  which is then passed to the hardware at each layer  the parameters may be modified  data may need to be passed  and so on each layer adds overhead to the system call ; the net result is a system call that takes longer than does one on a nonlayered system  these limitations have caused a small backlash against layering in recent years fewer layers with more functionality are beu1.g designed  providu1.g most of the advantages of modularized code while avoidu1.g the difficult problems of layer definition and interaction  2.7.3 microkernels we have already seen that as unix expanded  the kernel became large and difficult to manage in the mid-1980s  researchers at carnegie mellon university developed an operatu1.g system called mach that modularized the kernel using the ~ i ~ roke ~ ll  ~ _ ! _ ~ ee1 ~   2lc  ~ ~ i.b ~ ._gl ~ ! b_   _  ! _0ructl ~ ~ ~ ~ --t ! ~ e operatingsystem by removing all nonessential cornponentsfrom thekemel and 1mp ~ e_l  l ~ -ll  ! ~ ~ ~ itil ~ !   t ~ ~ s ~ ~ fe_l il ~ ~ ~ ~ rl.ls_ ~ l  ~ i ~ \ r ~   jr   greili ~ ~  the.reslin is-a smarrei  kernel there is little consensus regarding which services should remain u1 the kernel and which should be implemented in user space typically  however  microkernels provide minimal process and memory management  in addition to a communication facility  the main function of the micro kernel is to provide a communication facility between the client program and the various services that are also rum1.ing in user space communication is provided by message passing  which was described in section 2.4.5 for example  if the client program wishes to access a file  it must interact with the file server the client program and service never interact directly rathel ~ they communicate indirectly by exchanging messages with the microkemel  74 chapter 2 one benefit of the microkernel approach is ease of extending the operating system all new services are added to user space and consequently do not require modification of the kernel when the kernel does have to be modified  the changes tend to be fewer  because the microkernel is a smaller kernel  the resulting operating system is easier to port from one hardware design to another the microkernel also provides more security and reliability  since most services are running as user-rather than kernel-processes if a service fails  the rest of the operating system remains untouched  several contemporary operating systems have used the microkernel approach tru64 unix  formerly digital unix  provides a unix interface to the user  but it is implemented with a mach kernel the mach kernel maps unix system calls into messages to the appropriate user-level services the mac os x kernel  also known as darwin  is also based on the mach micro kernel  another example is qnx  a real-time operating system the qnx nl.icrokernel provides services for message passing and process scheduling it also handles low-level network communication and hardware interrupts all other services in qnx are provided by standard processes that run outside the kernel in user mode  unfortunately  microkernels can suffer from performance decreases due to increased system function overhead consider the history of windows nt  the first release had a layered microkernel organization however  this version delivered low performance compared with that of windows 95 windows nt 4.0 partially redressed the performance problem by moving layers from user space to kernel space and integrating them more closely by the time windows xp was designed  its architecture was more monolithic than microkernel  2.7.4 modules perhaps the best current methodology for operating-system design involves using object-oriented programming techniques to create a modular kernel  here  the kernel has a set of core components and links in additional services either during boot time or during run time such a strategy uses dynamically loadable modules and is common in modern implementations of unix  such as solaris  linux  and mac os x for example  the solaris operating system structure  shown in figure 2.15  is organized armmd a core kernel with seven types of loadable kernel modules  scheduling classes file systems loadable system calls executable formats streams modules miscellaneous device and bus drivers such a design allows the kernel to provide core services yet also allows certain features to be implemented dynamically for example  device and 2.7 file systems figure 2.15 solaris loadable modules  loadable system calls 75 bus drivers for specific hardware can be added to the kernel  and support for different file systems can be added as loadable modules the overall result resembles a layered system in that each kernel section has defined  protected interfaces ; but it is more flexible than a layered system in that any module can call any other module furthermore  the approach is like the microkernel approach in that the primary module has only core functions and knowledge of how to load and communicate with other modules ; but it is more efficient  because modules do not need to invoke message passing in order to communicate  the apple mac os x operating system uses a hybrid structure it is a layered system in which one layer consists of the mach microkernel the structure of mac os x appears in figure 2.16 the top layers include application environments and a set of services providing a graphical interface to applications  below these layers is the kernel environment  which consists primarily of the mach microkernel and the bsd kernel mach provides memory management ; support for remote procedure calls  rpcs  and interprocess communication  ipc  facilities  including message passing ; and thread scheduling the bsd component provides a bsd command line interface  support for networking and file systems  and an implementation of posix apis  including pthreads  kernel environment application environments and common services figure 2.16 the mac os x structure  76 chapter 2 2.8 in addition to mach and bsd  the kernel environment provides an i/0 kit for development of device drivers and dynamically loadable modules  which mac os x refers to as kernel extensions   as shown in the figure  applications and comn  10n services can make use of either the mach or bsd facilities directly  the layered approach described in section 2.7.2 is taken to its logical conclusion in the concept of a the fundamental idea behind a virtual machine is to abstract the hardware of a si11.gle computer  the cpu  memory  disk drives  network interface cards  and so forth  into several different execution environments  thereby creating the illusion that each separate execution environment is run.ning its own private computer  by using cpu scheduling  chapter 5  and virtual-memory techniques  chapter 9   an operating system can create the illusion that a process has its own processor with its own  virtual  memory the virtual machine provides an interface that is identical to the underlying bare hardware each process is provided with a  virtual  copy of the underlying computer  figure 2.17   usually  the guest process is in fact an operating system  and that is how a single physical machine can run multiple operating systems concurrently  each in its own virtual machine  2.8.1 history virtual machines first appeared commercially on ibm mainframes via the vm operating system in 1972 vm has evolved and is still available  and many of processes programming/ / interface 1 ~ -----1 kernel  a  processes processes processes kernel kernel kernel vm1 vm2 vm3 virtual-machine implementation  b  figure 2.17 system models  a  nonvirtual machine  b  virtual machine  2.8 77 the original concepts are found in other systems  making this facility worth exploring  ibm vm370 divided a mainframe into nmltiple virtual machines  each numing its own operating system a ncajor difficulty with the vm virtualmachine approach involved disk systems suppose that the physical machine had three disk drives but wanted to support seven virtual machines clearly  it could not allocate a disk drive to each virtual machine  because the virtualmachine software itself needed substantial disk space to provide virtual memory and spooling the solution was to provide virtual disks-termed minidislcs in ibm 's vm operating system -that are identical in all respects except size the system implemented each minidisk by allocating as many tracks on the physical disks as the minidisk needed  once these virtual machines were created  users could run any of the operating systems or software packages that were available on the underlying machine for the ibm vm system  a user normally ran cms-a single-user interactive operating system  2.8.2 benefits there are several reasons for creating a virtual machine most of them are fundarnentally related to being able to share the same hardware yet run several different execution environments  that is  different operating systems  concurrently  one important advantage is that the host system is protected from the virtual machines  just as the virtual machines are protected from each other a virus inside a guest operating system might damage that operating system but is unlikely to affect the host or the other guests because each virtual machine is completely isolated from all other virtual machines  there are no protection problems at the same time  however  there is no direct sharing of resources  two approaches to provide sharing have been implemented first  it is possible to share a file-system volume and thus to share files second  it is possible to define a network of virtual machines  each of which can send information over the virtual communications network the network is modeled after physical communication networks but is implemented in software  a virtual-machine system is a perfect vehicle for operating-systems research and development normally  changing an operating system is a difficult task operating systems are large and complex programs  and it is difficult to be sure that a change in one part will not cause obscure bugs to appear in some other part the power of the operating system makes changing it particularly dangerous because the operating system executes in kernel mode  a wrong change in a pointer could cause an error that would destroy the entire file system thus  it is necessary to test all changes to the operating system carefully  the operating system  however  runs on and controls the entire machine  therefore  tlle current system must be stopped and taken out of use while changes are made and tested this period is comnconly called systemdevelopment time since it makes the system unavailable to users  systemdevelopment time is often scheduled late at night or on weekends  when system load is low  78 chapter 2 a virtual-machine system can eliminate much of this problem system programmers are given their own virtual machine  and system development is done on the virtual machine instead of on a physical machine normal system operation seldom needs to be disrupted for system development  another advantage of virtual machines for developers is that multiple operating systems can be running on the developer 's workstation concurrently  this virtualized workstation allows for rapid porting and testing of programs in varying enviromnents sin'lilarly  quality-assurance engineers can test their applications in multiple environments without buying  powering  and maintaining a computer for each environment  a major advantage of virtual machines in production data-center use is system which involves taking two or more separate systems and running them in virtual machines on one system such physical-to-virtual conversions result in resource optimization  as many lightly used systems can be combined to create one more heavily used system  if the use of virtual machines continues to spread  application deployment will evolve accordingly if a system can easily add  remove  and move a virtual machine  then why install applications on that system directly instead  application developers would pre-install the application on a tuned and customized operating system in a virh1al machine that virtual environment would be the release mechanism for the application this method would be an improvement for application developers ; application management would become easier  less tuning would required  and technical support of the application would be more straightforward system administrators would find the environment easier to manage as well installation would be simple  and redeploying the application to another system would be much easier than the usual steps of uninstalling and reinstalling for widespread adoption of this methodology to occur  though  the format of virtual machines must be standardized so that any virtual machine will run on any virtualization platform the open virtual machine format is an attempt to do just that  and it could succeed in unifying virtual-machine formats  2.8.3 simulation system virtualization as discussed so far is just one of many system-emulation methodologies virtualization is the most common because it makes guest operating systems and applications believe they are running on native hardware because only the system 's resources need to be virtualized  these guests run at almost full speed  another methodology is in which the host system has one system architecture and the guest system was compiled for a different architecture  for example  suppose a company has replaced its outdated computer system with a new system but would like to continue to run certain important programs that were compiled for the old system the programs could be run in an e1nulator that translates each of the outdated system 's instructions into the native instruction set of the new system emulation can increase the life of programs and allow us to explore old architectures without having an actual old machine  but its major challenge is performance instruction-set emulation can run an order of magnitude slower than native instructions thus  unless the new machine is ten times faster than the old  the program running on 2.8 79 the new machine will run slower than it did on its native hardware another challenge is that it is difficult to create a correct emulator because  in essence  this involves writing an entire cpu in software  2.8.4 para-virtualization is another vanat10n on this theme rather than try to trick a guest operating system into believing it has a system to itself  paravirtualization presents the guest with a system that is similar but not identical to the guest 's preferred system the guest must be modified to run on the paravirtualized hardware the gain for this extra work is more efficient use of resources and a smaller virtualization layer  solaris 10 includes or that create a virtual layer between the operating system and the applications in this system  only one kernel is installed  and the hardware is not virtualized rather  the operating system and its devices are virtualized  providing processes within a container with the impression that they are the only processes on the system one or more containers can be created  and each can have its own applications  network stacks  network address and ports  user accounts  and so on cpu resources can be divided up among the containers and the systemwide processes figure 2.18 shows a solaris 10 system with two containers and the standard global user space  user programs system programs cpu resources memory resources global zone user programs system programs network addresses device access cpu resources user programs system programs network addresses device access cpu resources memory resources memory resources zone 1 zone 2 virtual platform device management figure 2.18 solaris i 0 with two containers  80 chapter 2 2.8.5 implementation although the virtual-machine concept is usefut it is difficult to implement  much work is required to provide an exact duplicate of the underlying machine  remember that the underlying machine typically has two modes  user mode and kernel mode the virtual-machine software can run in kernel mode  since it is the operating system the virtual machine itself can execute in only user mode just as the physical machine has two modes  however  so must the virtual machine consequently  we must have a virtual user mode and a virtual kernel mode  both of which run in a physical user mode those actions that cause a transfer from user mode to kernel mode on a real machine  such as a system call or an attempt to execute a privileged instruction  must also cause a transfer from virtual user mode to virtual kernel mode on a virtual machine  such a transfer can be accomplished as follows when a system calt for example  is made by a program running on a virtual machine in virtual user mode  it will cause a transfer to the virtual-machine monitor in the real machine  when the virtual-machine monitor gains controt it can change the register contents and program counter for the virtual machine to simulate the effect of the system call it can then restart the virtual machine  noting that it is now in virtual kernel mode  the major difference  of course  is time whereas the real i/o might have taken 100 milliseconds  the virtual i/o might take less time  because it is spooled  or more time  because it is interpreted   in addition  the cpu is being multi programmed among many virtual machines  further slowing down the virtual machines in unpredictable ways in the extreme case  it may be necessary to simulate all instructions to provide a true virtual machine vm  discussed earlier  works for ibm machines because normal instructions for the virtual machines can execute directly on the hardware only the privileged instructions  needed mainly for i/0  must be simulated and hence execute more slowly  without some level of hardware support  virtualization would be impossible  the more hardware support available within a system  the more feature rich  stable  and well performing the virtual machines can be all major generalpurpose cpus provide some amount of hardware support for virtualization  for example  amd virtualization technology is found in several amd processors  it defines two new modes of operation-host and guest virtual machine software can enable host mode  define the characteristics of each guest virtual machine  and then switch the system to guest mode  passing control of the system to the guest operating system that is running in the virtual machine  in guest mode  the virtualized operating system thinks it is rum1.ing on native hardware and sees certain devices  those included in the host 's definition of the guest   if the guest tries to access a virtualized resource  then control is passed to the host to manage that interaction  2.8.6 examples despite the advantages of virtual machines  they received little attention for a number of years after they were first developed today  however  virtual machines are coming into fashion as a means of solving system compatibility problems in this section  we explore two popular contemporary virtual machines  the vmware workstation and the java virtual machine as you 2.8 81 will see  these virtual machines can typically run on top of operating systems of any of the design types discussed earlier thus  operating system design methods-simple layers  microkernels  n  wdules  and virtual machines-are not mutually exclusive  2.8.6.1 vmware most of the virtualization techniques discussed in this section require virtualization to be supported by the kernel another method involves writing the virtualization tool to run in user mode as an application on top of the operating system virtual machines running within this tool believe they are rum ing on bare hardware but in fact are running inside a user-level application  is a popular commercial application that abstracts intel x86 and compatible hardware into isolated virtual machines vmware workstation runs as an application on a host operating system such as windows or linux and allows this host system to concurrently run several different guest operating systems as independent virtual machines  the architecture of such a system is shown in figure 2.19 in this scenario  linux is running as the host operating system ; and freebsd  windows nt  and windows xp are rum ing as guest operating systems the virtualization layer is the heart of vmware  as it abstracts the physical hardware into isolated virtual machines running as guest operating systems each virtual machine has its own virtual cpu  memory  disk drives  network interfaces  and so forth  the physical disk the guest owns and manages is really just a file within the file system of the host operating system to create an identical guest instance  we can simply copy the file copying the file to another location protects the guest instance against a disaster at the original site moving the file to another application application application application guest operating guest operating guest operating system system system  free bsd   windows nt   windows xp  virtual cpu virtual cpu virtual cpu virtual memory virtual memory virtual memory virtual devices virtual devices virtual devices virtualization layer hardware i qpu   i r ! jemgfy figure 2.19 vmware architecture  82 chapter 2 location moves the guest system these scenarios show how virtualization can improve the efficiency of system administration as well as system resource use  2.8.6.2 the java virtual machine java is a popular object-oriented programming language introduced by sun microsystems in 1995 in addition to a language specification and a large api library  java also provides a specification for a java virtual machine-or jvm  java objects are specified with the class construct ; a java program consists of one or more classes for each java class  the compiler produces an architecture-neutral bytecode output  .class  file that will run on any implementation of the jvm  the jvm is a specification for an abstract computer it consists of a class loader and a java interpreter that executes the architecture-neutral bytecodes  as diagrammed in figure 2.20 the class loader loads the compiled  class files from both the java program and the java api for execution by the java interpreter after a class is loaded  the verifier checks that the  class file is valid java bytecode and does not overflow or underflow the stack it also ensures that the bytecode does not perform pointer arithmetic  which could provide illegal memory access if the class passes verification  it is run by the java interpreter the jvm also automatically manages memory by performing garbage collection -the practice of reclaiming memory from objects no longer in use and returning it to the system much research focuses on garbage collection algorithms for increasing the performance of java programs in the virtual machine  the jvm may be implemented in software on top of a host operating system  such as windows  linux  or mac os x  or as part of a web browser  alternatively  the jvm may be implemented in hardware on a chip specifically designed to nm java programs if the jvm is implemented in software  the java interpreter interprets the bytecode operations one at a time a faster software technique is to use a just-in-time  jit  compiler here  the first time a java method is invoked  the bytecodes for the method are turned into native machine language for the host system these operations are then cached so that subsequent invocations of a method are performed using the native machine instructions and the bytecode operations need not be interpreted all over again  a technique that is potentially even faster is to nm the jvm in hardware on a java program .class files  i class loader 1 +  + i java i interpreter t host system  windows  linux  etc  figure 2.20 the java virtual machine  2.8 the .net framework the .net framework is a collection of technologies  including a set of class libraries  and an execution environment that come together to provide a platform for developing software this platform allows programs to be written to target the .net framework instead of a specific architecture a program written for the .net framework need not worry aboutthe specifics of the hardware or the operating system on which it will run thus  any architecture implementing .net will be able to successfully execute the program this is because the execution environment abstracts these details and provides a virtual machine as an intermediary between the executing program and the underlying architecture  at the core of the .net framework is the common language runtime  clr   the clr is the implementation of the .net virtual machine itprovides an environment for execution of programs written in any of the languages targeted at the .net framework programs written in languages such as c #  pronounced c-sharp  and vb.net are compiled into an intermediate  architecture-independent language called microsoft intermediate language  ms-il   these compiled files  called assemblies  include ms-il instructions and metadata they have file extensions of either .exe or .dll upon execution of a program  the clr loads assemblies into what .is known as the application domain as instructions are requested by the executing program  the clr converts the ms-il instructions inside the assemblies into native code that is specific to the underlying architecture using just-in-time compilation once instructions have been converted to native code  they are kept and will continue to run as native code for the cpu the architecture of the clr for the .net framework is shown in figure 2.21  compilation clr c + + source ms-il assembly vb.net source ms-il assembly host system figure 2.21 architecture ofthe.clr for the .net framework  83 84 chapter 2 2.9 special java chip that executes the java bytecode operations as native code  thus bypassing the need for either a software interpreter or a just-in-tim.e compiler  broadly  is the activity of finding and fixing errors  or in a system debugging seeks to find and fix errors in both hardware and software  performance problems are considered bugs  so debugging can also include which seeks to improve performance by removing  ' '  in the processing taking place within a system a discussion of hardware debugging is outside of the scope of this text in this section  we explore debugging kernel and process errors and performance problems  2.9.1 failure analysis if a process fails  most operating systems write the error information to a to alert system operators or users that the problem occurred the operating system can also take a capture of the memory  referred to as the core in the early days of computing  of the process this core image is stored in a file for later analysis running programs and core dumps can be probed by a a tool designed to allow a programmer to explore the code and memory a process  debugging user-level process code is a challenge operating system kernel debugging even more complex because of the size and complexity of the kernel  its control of the hardware  and the lack of user-level debugging tools a kernel failure is called a as with a process failure  error information is saved to a log file  and the memory state is saved to a operating system debugging frequently uses different tools and techniques than process debugging due to the very different nature of these two tasks  consider that a kernel failure in the file-system code would make it risky for the kernel to try to save its state to a file on the file system before rebooting  a common technique is to save the kernel 's memory state to a section of disk set aside for this purpose that contains no file system  if the kernel detects an unrecoverable error  it writes the entire contents of memory  or at least the kernel-owned parts of the system memory  to the disk area when the system reboots  a process runs to gather the data from that area and write it to a crash dump file within a file system for analysis  2.9.2 performance tuning to identify bottlenecks  we must be able to monitor system performance code must be added to compute and display measures of system behavior in a number of systems  the operating system does this task by producing trace listings of system behavior all interesting events are logged with their time and important parameters and are written to a file later  an analysis program can process the log file to determine system performance and to identify bottlenecks and inefficiencies these same traces can be run as input for a simulation of a suggested improved system traces also can help people to find errors in operating-system behavior  2.9 kernighan 's law debugging is twice as hard as writing the code in the first place therefore  if you write the code as cleverly as possible  you are  by definition  not smart enough to debug it  85 another approach to performance tuning is to include interactive tools with the system that allow users and administrators to question the state of various components of the system to look for bottlenecks the unix command top displays resources used on the system  as well as a sorted list of the top resource-using processes other tools display the state of disk i/0  memory allocation  and network traffic the authors of these single-purpose tools try to guess what a user would want to see while analyzing a system and to provide that information  making running operating systems easier to understand  debug  and tune is an active area of operating system research and implementation the cycle of enabling tracing as system problems occur and analyzing the traces later is being broken by a new generation of kernel-enabled performance analysis tools further  these tools are not single-purpose or merely for sections of code that were written to emit debugging data the solaris 10 dtrace dynamic tracing facility is a leading example of such a tool  2.9.3 dtrace is a facility that dynamically adds probes to a running system  both i11 user processes and in the kernel these probes can be queried via the d programming language to determine an astonishing amount about the kernel  the system state  and process activities for example  figure 2.22 follows an application as it executes a system call  ioctl  and further shows the functional calls within the kernel as they execute to perform the system call lines ending with u are executed in user mode  and lines ending in k in kernel mode  debugging the interactions between user-level and kernel code is nearly impossible without a toolset that understands both sets of code and can instrument the interactions for that toolset to be truly useful  it must be able to debug any area of a system  including areas that were not written with debugging in mind  and do so without affecting system reliability this tool must also have a minimum performance impact-ideally it should have no impact when not in use and a proportional impact during use the dtrace tool meets these requirements and provides a dynamic  safe  low-impact debugging environncent  until the dtrace framework and tools became available with solaris 10  kernel debugging was usually shrouded in mystery and accomplished via happenstance and archaic code and tools for example  cpus have a breakpoint feature that will halt execution and allow a debugger to examine the state of the system then execution can continue until the next breakpoint or termination  this method can not be used in a multiuser operating-system kernel without negatively affecting all of the users on the system pn  reen,g  which periodically samples the instruction pointer to determine which code is being executed  can show statistical trends but not individual activities code can be included in the kernel to emit specific data under specific circumstances  but that code 86 chapter 2 # ./all.d 'pgrep xclock ' xeventsqueued dtrace  script './all.d ' matched 52377 probes cpu function 0  xeventsqueued 0  _xeventsqueued u u 0  _xlltransbytesreadable u 0  _xlltransbytesreadable u 0  _xlltranssocketbytesreadable u 0  _xlltranssocketbytesreadable u 0  ioctl u 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0      ioctl   getf  set active fd  set active fd  getf  get udatamodel  get udatamodel  releasef  clear active   clear active  cv broadcast  cv broadcast  releasef ioctl ioctl xeventsqueued xeventsqueued fd fd k k k k k k k k k k k k k k u u u figure 2.22 solaris 10 dtrace follows a system call within the kernel  slows down the kernel and tends not to be included in the part of the kernel where the specific problem being debugged is occurring  in contrast  dtrace runs on production systems-systems that are running important or critical applications-and causes no harm to the system it slows activities while enabled  but after execution it resets the system to its pre-debugging state it is also a broad and deep tool it can broadly debug everything happening in the system  both at the user and kernel levels and between the user and kernel layers   dtrace can also delve deeply into code  showing individual cpu instructions or kernel subroutine activities  is composed of a compiler  a framework  of written within that framework  and of those probes dtrace providers create probes kernel structures exist to keep track of all probes that the providers have created the probes are stored in a hash table data structure that is hashed by name and indexed according to unique probe identifiers  when a probe is enabled  a bit of code in the area to be probed is rewritten to call dtrace_probe  probe identifier  and then continue with the code 's original operation different providers create different kinds of probes for example  a kernel system-call probe works differently from a user-process probe  and that is different from an i/o probe  dtrace features a compiler that generates a byte code that is run in the kernel this code is assured to be safe by the compiler for example  no 2.9 87 loops are allowed  and only specific kernel state modifications are allowed when specifically requested only users with the dtrace privileges  or root users  are allowed to use dt ! ace  as it can retrieve private kernel data  and modify data if requested   the generated code runs in the kernel and enables probes it also enables consumers in user mode and enables communications between the two  a dt ! ace consumer is code that is interested in a probe and its results  a consumer requests that the provider create one or more probes when a probe fires  it emits data that are managed by the kernel within the kernel  actions called or are performed when probes fire one probe can cause multiple ecbs to execute if more than one consumer is interested in that probe each ecb contains a predicate  if statement  that can filter out that ecb otherwise  the list of actions in the ecb is executed the most usual action is to capture some bit of data  such as a variable 's value at that point of the probe execution by gathering such data  a complete picture of a user or kernel action can be built further  probes firing from both user space and the kernel can show how a user-level action caused kernel-level reactions  such data are invaluable for performance monitoril1.g and code optimization  once the probe consumer tennil1.ates  its ecbs are removed if there are no ecbs consuming a probe  the probe is removed that involves rewriting the code to remove the dtrace_probe call and put back the original code thus  before a probe is created and after it is destroyed  the system is exactly the same  as if no probing occurred  dtrace takes care to assure that probes do not use too much memory or cpu capacity  which could harm the running system the buffers used to hold the probe results are monitored for exceeding default and maximum limits  cpu time for probe execution is monitored as well if limits are exceeded  the consumer is terminated  along with the offending probes buffers are allocated per cpu to avoid contention and data loss  an example ofd code and its output shows some of its utility the following program shows the dtrace code to enable scheduler probes and record the amount of cpu time of each process running with user id 101 while those probes are enabled  that is  while the program nms   sched    on-cpu uid = = 101  self ts timestamp ;  sched    off -cpu self ts   time  execname  self ts = 0 ; sum  timestamp self ts  ; the output of the program  showing the processes and how much time  in nanoseconds  they spend running on the cpus  is shown in figure 2.23  88 chapter 2 2.10 # dtrace -s sched.d dtrace  script 'sched.d ' matched 6 probes ac grwme-settings-d gnome-vfs-daemon dsdm wnck-applet gnome-panel clock-applet mapping-daemon xscreensaver meta city xorg gnome-terminal mixer applet2 java 142354 158243 189804 200030 277864 374916 385475 514177 539281 2579646 5007269 7388447 10769137 figure 2.23 output of the 0 code  because dtrace is part of the open-source solaris 10 operating system  it is being added to other operating systems when those systems do not have conflicting license agreements for example  dtrace has been added to mac os x 10.5 and freebsd and will likely spread further due to its unique capabilities other operating systems  especially the linux derivatives  are adding kernel-tracing functionality as well still other operating systems are beginning to include performance and tracing tools fostered by research at various institutions  including the paradyn project  it is possible to design  code  and implement an operating system specifically for one machine at one site more commonly  however  operating systems are designed to nm on any of a class of machines at a variety of sites with a variety of peripheral configurations the system must then be configured or generated for each specific computer site  a process sometimes known as system generation  sysgen   the operating system is normally distributed on disk  on cd-rom or dvd-rom  or as an iso image  which is a file in the format of a cd-rom or dvd-rom to generate a system  we use a special program this sysgen program reads from a given file  or asks the operator of the system for information concerning the specific configuration of the hardware systenc  or probes the hardware directly to determine what components are there the following kinds of information must be determined  what cpu is to be used what options  extended instruction sets  floatingpoint arithmetic  and so on  are installed for multiple cpu systems  each cpu may be described  2.11 2.11 89 how will the boot disk be formatted how many sections  or partitions  will it be separated into  and what will go into each partition how much memory is available some systems will determine this value themselves by referencing memory location after memory location until an illegal address fault is generated this procedure defines the final legal address and hence the amount of available memory  what devices are available the system will need to know how to address each device  the device number   the device interrupt number  the device 's type and model  and any special device characteristics  what operating-system options are desired  or what parameter values are to be used these options or values might include how many buffers of which sizes should be used  what type of cpu-scheduling algorithm is desired  what the maximum number of processes to be supported is  and so on  once this information is determined  it can be used in several ways at one extreme  a system administrator can use it to modify a copy of the source code of the operating system the operating system then is completely compiled data declarations  initializations  and constants  along with conditional compilation  produce an output-object version of the operating system that is tailored to the system described  at a slightly less tailored level  the system description can lead to the creation of tables and the selection of modules from a precompiled library  these modules are linked together to form the generated operating system  selection allows the library to contain the device drivers for all supported i/0 devices  but only those needed are linked into the operating system because the system is not recompiled  system generation is faster  but the resulting system may be overly general  at the other extreme  it is possible to construct a system that is completely table driven all the code is always part of the system  and selection occurs at execution time  rather than at compile or lil1.k time system generation involves simply creating the appropriate tables to describe the system  the major differences among these approaches are the size and generality of the generated system and the ease of modifying it as the hardware configuration changes consider the cost of modifying the system to support a newly acquired graphics termil1.al or another disk drive balanced against that cost  of course  is the frequency  or infrequency  of such changes  after an operating system is generated  it must be made available for use by the hardware but how does the hardware know where the kernel is or how to load that kernel the procedure of starting a computer by loading the kernel is known as booting the system on most computer systems  a small piece of code known as the bootstrap program or bootstrap loader locates the kernel  loads it into main memory  and starts its execution some computer systems  such as pcs  use a two-step process in which a simple bootstrap loader fetches a more complex boot program from disk  which in turn loads the kernel  90 chapter 2 2.12 when a cpu receives a reset event-for instance  when it is powered up or rebooted -the instruction register is loaded with a predefined memory location  and execution starts there at that location is the initial bootstrap program this program is in the form of read-only memory  rom   because the ram is in an unknown state at system startup rom is convenient because it needs no initialization and can not easily be infected by a computer virus  the bootstrap program can perform a variety of tasks usually  one task is to run diagnostics to determine the state of the machine if the diagnostics pass  the program can continue with the booting steps it can also initialize all aspects of the system  from cpu registers to device controllers and the contents of main memory sooner or later  it starts the operating system  some systems-such as cellular phones  pdas  and game consoles-store the entire operating system in rom storing the operating system in rom is suitable for small operating systems  simple supporting hardware  and rugged operation a problem with this approach is that changing the bootstrap code requires changing the rom hardware chips some systems resolve this problem by using erasable programmable read-only memory  eprom   which is readonly except when explicitly given a command to become writable all forms of rom are also known as firmware  since their characteristics fall somewhere between those of hardware and those of software a problem with firmware in general is that executing code there is slower thart executing code in ram  some systems store the operating system in firmware and copy it to ram for fast execution a final issue with firmware is that it is relatively expensive  so usually only small ammmts are available  for large operating systems  including most general-purpose operating systems like windows  mac os x  and unix  or for systems that change frequently  the bootstrap loader is stored in firmware  and the operating system is on disk in this case  the bootstrap nms diagnostics and has a bit of code that can read a single block at a fixed location  say block zero  from disk into memory and execute the code from that b ! ock the program stored in the boot block may be sophisticated enough to load the entire operating system into memory and begin its execution more typically  it is simple code  as it fits in a single disk block  and knows only the address on disk and length of the remainder of the bootstrap program is an example of an open-source bootstrap program for linux systems all of the disk-bound bootstrap  and the operating system itself  can be easily changed by writing new versions to disk  a disk that has a boot partition  more on that in section 12.5.1  is called a boot disk or system disk  now that the full bootsh ap program has been loaded  it can traverse the file system to find the operating system kernel  load it into memory  and start its execution it is only at this point that the system is said to be running  operating systems provide a number of services at the lowest level  system calls allow a running program to make requests from the operating system directly at a higher level  the command interpreter or shell provides a mechanism for a user to issue a request without writing a program commands may come from files during batch-mode execution or directly from a terminal 91 when in an interactive or time-shared mode system programs are provided to satisfy many common u.ser requests  the types of requests vary accord  ilcg to level the system-call level must provide the basic functions  such as process control and file and device manipulation higher-level requests  satisfied by the command interpreter or system programs  are translated into a sequence of system calls system services can be classified into several categories  program controt status requests  and i/0 requests program errors can be considered implicit requests for service  once the system services are defined  the structure of the operating system can be developed various tables are needed to record the information that defines the state of the computer system and the status of the system 's jobs  the design of a new operating system is a major task it is important that the goals of the system be well def  ilced before the design begins the type of system desired is the foundation for choices among various algorithms and strategies that will be needed  s  iilce an operating system is large  modularity is important designing a system as a sequence of layers or using a microkernel is considered a good technique the virtual-machine concept takes the layered approach and treats both the kernel of the operat  ilcg system and the hardware as though they were hardware even other operating systems may be loaded on top of this virtual machine  throughout the entire operating-system design cycle  we must be careful to separate policy decisions from implementation details  mechanisms   this separation allows maximum flexibility if policy decisions are to be changed later  operating systems are now almost always written in a systemsimplementation language or in a higher-level language this feature improves their implementation  maintenance  and portability to create an operating system for a particular machine configuration  we must perform system generation  debugging process and kernel failures can be accomplished through the use of de buggers and other tools that analyze core dumps tools such as dtrace analyze production systems to fucd bottlenecks and understand other system behavior  for a computer system to begin running  the cpu must initialize and start executing the bootstrap program in firmware the bootstrap can execute the operating system directly if the operating system is also in the firmware  or it can complete a sequence in which it loads progressively smarter programs from firmware and disk until the operating system itself is loaded into memory and executed  2.1 what are the five major activities of an operat  ilcg system with regard to file management 2.2 what are the three major activities of an operating system with regard to memory management 92 chapter 2 2.3 why is a just-in-time compiler useful for executing java programs 2.4 the services and functions provided by an operating system can be divided into two main categories briefly describe the two categories and discuss how they differ  2.5 why is the separation of mechanism and policy desirable 2.6 would it be possible for the user to develop a new command interpreter using the system-call interface provided by the operating system 2.7 what is the purpose of the command interpreter why is it usually separate from the kernel 2.8 what is the main advantage for an operating-system designer of using a virtual-machine architecture what is the main advantage for a user 2.9 it is sometimes difficult to achieve a layered approach if two components of the operating system are dependent on each other identify a scenario in which it is unclear how to layer two system components that require tight coupling of their functionalities  2.10 what is the main advantage of the layered approach to system design what are the disadvantages of using the layered approach 2.11 what is the relationship between a guest operating system and a host operating system in a system like vmware what factors need to be considered in choosing the host operating system 2.12 describe three general methods for passing parameters to the operating system  2.13 what is the main advantage of the microkemel approach to system design how do user programs and system services interact in a microkernel architecture what are the disadvantages of usil1.g the microkernel approach 2.14 what system calls have to be executed by a command interpreter or shell in order to start a new process 2.15 what are the two models of interprocess conununication what are the strengths and weaknesses of the two approaches 2.16 the experimental synthesis operating system has an assembler incorporated in the kernel to optimize system-call performance  the kernel assembles routines within kernel space to minimize the path that the system call must take through the kernel this approach is the antithesis of the layered approach  in which the path through the kernel is extended to make buildu1.g the operating system easier discuss the pros and cons of the synthesis approach to kernel design and system-performance optimization  2.17 in what ways is the modular kernel approach similar to the layered approach in what ways does it differ from the layered approach 2.18 how could a system be designed to allow a choice of operating systems from which to boot what would the bootstrap program need to do 93 2.19 what are the advantages and disadvantages of using the same systemcall interface for manipulating both files and devices 2.20 describe how you could obtain a statistical profile of the amount of time spent by a program executing different sections of its code discuss the importance of obtaining such a statistical profile  2.21 why do some systems store the operating system in firmware  while others store it on disk 2.22 in section 2.3  we described a program that copies the contents of one file to a destination file this program works by first prompting the user for the name of the source and destilcation files write this program using either the win32 or posix api be sure to include all necessary error checking  including ensuring that the source file exists  once you have correctly designed and tested the program  if you used a system that supports it  run the program using a utility that traces system calls linux systems provide the ptrace utility  and solaris systems use the truss or dtrace command on mac os x  the ktrace facility provides similar functionality as windows systems do not provide such features  you will have to trace through the win32 version of this program using a debugger  2.23 adding a system call to the linux kernel in this project you will study the system-call interface provided by the linux operating system and learn how user programs communicate with the operating system kernel via this interface your task is to i11corporate a new system call into the kernet thereby expanding the functionality of the operating system  part 1  getting started a user-mode procedure call is performed by passing arguments to the called procedure either on the stack or through registers  saving the current state and the value of the program counter  and jumping to the beginning of the code corresponding to the called procedure the process continues to have the same privileges as before  system calls appear as procedure calls to user programs but result i11 a change in execution context and privileges in linux on the intel386 architecture  a system call is accomplished by storing the system-call number into the eax register  storing arguments to the system call in other hardware registers  and executing a trap instruction  which is the 94 chapter 2 int ox80 assembly instruction   after the trap is executed  the systemcall number is used to index into a table of code pointers to obtain the starting address for the handler code implementing the system call the process then juxnps to this address  and the privileges of the process are switched from user to kernel mode with the expanded privileges  the process can now execute kernel code  which may include privileged instructions that can not be executed in user mode the kernel code can then carry out the requested services  such as interacting with i/o devices  and can perform process management and other activities that can not be performed in user mode  the system call numbers for recent versions of the linux kernel are listed in lusr i src/linux-2 xl include/ asm-i386/unistd h   for instance  __ nr_close corresponds to the system call close 0  which is invoked for closin.g a file descriptor  and is defined as value 6  the list of pointers to system-call handlers is typically stored in the file lusrlsrcllinux-2.x/arch/i386/kernel/entry.s under the heading entry  sys_calltable   notice that sys_close is stored at entry number 6 in the table to be consistent with the system-call number defined in the unistd h file  the keyword .long denotes that the entry will occupy the same number of bytes as a data value of type long  part 2  building a new kernel before adding a system call to the kernel  you must familiarize yourself with the task of building the binary for a kernel from its source code and booting the machine with the newly built kernel this activity comprises the following tasks  some of which depend on the particular installation of the linux operating system in use  obtain the kernel source code for the linux distribution if the source code package has already been installed on your machine  the corresponding files might be available under lusr i srcllinux or /usr i src/linux-2 x  where the suffix corresponds to the kernel version number   if the package has not yet been installed  it can be downloaded from the provider of your linux distribution or from http  l/www.kernel.org  learn how to configure  compile  and install the kernel binary this will vary among the different kernel distributions  but some typical commands for building the kernel  after entering the directory where the kernel source code is stored  include  o make xconfig o make dep o make bzimage add a new entry to the set of boatable kernels supported by the system the linux operating system typically uses utilities such as lilo and grub to maintain a list ofbootable kernels from which the 95 user can choose during machine boot-up if your system supports lilo  add an entry to lilo conf  such as  image = /boot/bzimage.mykernel label = mykernel root = /dev/hda5 read-only where lbootlbzimage my kernel is the kernel image and my kernel is the label associated with the new kernel this step will allow you to choose the new kernel during the boot-up process you will then have the option of either booting the new kernel or booting the unmodified kernel if the newly built kernel does not ftmction properly  part 3  extending the kernel source you can now experiment with adding a new file to the set of source files used for compiling the kernel typically  the source code is stored in the lusr i srcllinux-2 xlkernel directory  although that location may differ in your linux distribution there are two options for adding the system call the first is to add the system call to an existing source file in this directory the second is to create a new file in the source directory and modify lusr i srcllinux-2 xlkernelimakef ile to include the newly created file in the compilation process the advantage of the first approach is that when you modify an existing file that is already part of the compilation process  the makefile need not be modified  part 4  adding a system call to the kernel now that you are familiar with the various background tasks corresponding to building and booting linux kernels  you can begin the process of adding a new system call to the linux kernel in this project  the system call will have limited functionality ; it will simply transition from user mode to kernel mode  print a message that is logged with the kernel messages  and transition back to user mode we will call this the helloworld system call while it has only limited functionality  it illustrates the system-call mechanism and sheds light on the interaction between user programs and the kernel  create a new file called helloworld c to define your system call  include the header files linuxllinkage h and linuxlkernel h  add the following code to this file  # include linuxllinkage.h # include linuxlkernel.h asmlinkage int sysjhelloworld    printk  kern_emerg hello world !  ; return 1 ;  96 chapter 2 this creates a system call with the name sys_helloworld    if you choose to add this system call to an existing file in the source directory  all that is necessary is to add the sys_hellowor ld   function to the file you choose in the code  asmlinkage is a rellli ant from the days when linux used both c + + and c code and is used to indicate that the code is written in c the printk   function is used to print messages to a kernel log file and therefore may be called only from the kernel the kernel messages specified in the parameter to printk   are logged in the file /var/log/kernel/warnings the function prototype for the printk   call is defined in /usr /include/linux/kernel h  define a new system call number for __ nr_helloworld in /usr/src/linux-2.x/include/asm-i386/unistd.h a user program can use this number to identify the newly added system call also be sure to increment the value for __ nr_syscalls  which is stored in the same file this constant tracks the number of system calls currently defuced in the kernel  add an entry .long sys_helloworld to the sys_calltable definedinthe/usr/src/linux-2.x/arch/i386/kernel/entry.s file as discussed earlier  the system-call number is used to index into this table to find the position of the handler code for the invoked system call  add your file helloworld c to the makefile  if you created a new file for your system call  save a copy of your old kernel binary image  in case there are problems with your newly created kernel   you can now build the new kernet rename it to distinguish it from the unmodified kernet and add an entry to the loader configuration files  such as lilo conf   after completing these steps  you can boot either the old kernel or the new kernel that contains your system call  part 5  using the system call from a user program when you boot with the new kernet it will support the newly defined system call ; you now simply need to invoke this system call from a user program ordinarily  the standard c library supports an interface for system calls defined for the linux operating system as your new system call is not linked into the standard c library  however  invoking your system call will require manual intervention  as noted earlie1 ~ a system call is invoked by storing the appropriate value in a hardware register and performing a trap instruction unfortunately  these low-level operations can not be performed using c language statements and instead require assembly instructions fortunately  linux provides macros for instantiating wrapper functions that contain the appropriate assembly instructions for instance  the following c program uses the _syscallo   macro to invoke the newly defined system call  # include linux/errno.h # include sys/syscall.h # include linux/unistd.h _syscallo  int  helloworld  ; main    helloworld   ;  97 the _syscallo macro takes two arguments the first specifies the type of the value returned by the system call ; the second is the name of the system call the name is used to identify the systemcall number that is stored in the hardware register before the trap instruction is executed if your system call requires arguments  then a different macro  such as _syscallo  where the suffix indicates the number of arguments  could be used to instantiate the assembly code required for performing the system call  compile and execute the program with the newly built kernel  there should be a message hello world ! in the kernel log file /var/log/kernel/warnings to indicate that the system call has executed  as a next step  consider expanding the functionality of your system call  how would you pass an integer value or a character string to the system call and have it printed illto the kernel log file what are the implications of passing pointers to data stored in the user program 's address space as opposed to simply passing an integer value from the user program to the kernel using hardware registers dijkstra  1968  advocated the layered approach to operating-system desigll  brinch-hansen  1970  was an early proponent of constructing an operating system as a kernel  or nucleus  on which more complete systems can be built  system instrumentation and dynamic tracing are described in tamches and miller  1999   dtrace is discussed in cantrill et al  2004   the dtrace source code is available at http  i i src opensolaris org/ source/ cheung and loong  1995  explore issues of operating-system structure from microkernel to extensible systems  ms-dos  version 3.1  is described in microsoft  1986   windows nt and windows 2000 are described by solomon  1998  and solomon and russinovich  2000   windows 2003 and windows xp internals are described in russinovich and solomon  2005   hart  2005  covers windows system $ programming in detail bsd unix is described in mckusick et al  1996   bovet and cesati  2006  thoroughly discuss the linux kernel several unix systems-including mach-are treated in detail in vahalia  1996   mac os x is presented at 98 chapter 2 http  i lwww apple comlmacosx and in singh  2007   solaris is fully described in mcdougall and mauro  2007   the first operating system to provide a virtual machine was the cp i 67 on an ibm 360167 the commercially available ibm vmi370 operating system was derived from cp 167 details regarding mach  a microkernel-based operating system  can be found in young et al  1987   kaashoeket al  1997  present details regarding exokernel operating systems  wherein the architecture separates management issues from protection  thereby giving untrusted software the ability to exercise control over hardware and software resources  the specifications for the java language and the java virtual machine are presented by gosling et al  1996  and by lindholm and yellin  1999   respectively  the internal workings of the java virtual machine are fully described by ven11ers  1998   golm et al  2002  highlight the jx operating system ; back et al  2000  cover several issues in the design of java operating systems more information on java is available on the web at http  i lwww j a vas oft com  details about the implementation of vmware can be found in sugerman et al   2001   information about the open virh1al machine format can be found at http  llwww.vmware.comlappliancesllearnlovf.html  part two a process can be thought of as a program in execution a process will need certain resources-such as cpu time  memory  files  and 1/0 devices -to accomplish its task these resources are allocated to the process either when it is created or while it is executing  a process is the unit of work in most systems systems consist of a collection of processes  operating-system processes execute system code  and user processes execute user code all these processes may execute concurrently  although traditionally a process contained only a single thread of control as it ran  most modem operating systems now support processes that have multiple threads  the operating system is responsible for the following activities in connection with process and thread management  the creation and deletion of both user and system processes ; the scheduling of processes ; and the provision of mechanisms for synchronization  communication  and deadlock handling for processes  3.1 chapter early computer systems allowed only one program to be executed at a time this program had complete control of the system and had access to all the system 's resources in contrast  current-day computer systems allow multiple programs to be loaded into memory and executed concurrently  this evolution required firmer control and more compartmentalization of the various programs ; and these needs resulted in the notion of a process/ which is a program in execution a process is the unit of work in a modern time-sharing system  the more complex the operating system is  the more it is expected to do on behalf of its users although its main concern is the execution of user programs  it also needs to take care of various system tasks that are better left outside the kernel itself a system therefore consists of a collection of processes  operatingsystem processes executing system code and user processes executing user code potentially/ all these processes can execute concurrently/ with the cpu  or cpus  multiplexed among them by switching the cpu between processes  the operating system can make the computer more productive in this chapter/ you will read about what processes are and how they work  to introduce the notion of a process a program in execution  which forms the basis of all computation  to describe the various features of processes  including scheduling  creation and termination  and communication  to describe communication in client-server systems  a question that arises in discussing operating systems involves what to call all the cpu activities,_f \ _qcij  c  hj3ystem ~ xeq_l_ ~ _lqq.s_ ; .i ' \ 'b 'x  c9   _2l_ ! _i_ ! 11_e     _l  it ~ ds_ys ! ~  r  tl has user programs  or tas ~ ~  even on a single-user system such as microsoft 101 102 chapter 3 windows  a user may be able to run several programs at one time  a word processor  a web browse1 ~ and an e-mail package and even if the user can execute only one program at a time  the operating system may need to support its own internal programmed activities  such as memory management in many respects  all these activities are similar  so we call all of them processes  _the ten  ns ~ job i  ! dq pl ~ e ~ .s etif  '_lised almost interchangeably in this te   t  although we personally prefer the term process  much of operat1ng-system theory and terminology was developed during a time when the major activity of operating systems was job processing it would be misleading to avoid the use of commonly accepted terms that include the word job  such as job scheduling  simply because process has superseded job  3.1.1 the process informally  as mentioned earlier  a process is a program in execution a process is more than the program code  which is sometimes known as the text section  it also includes the current activity  as represented by the value of the program counter and the contents of the processor 's registers a process generally also includes the process stack  which contains temporary data  such as function parameters  return addresses  and local variables   and a data section  which contains global variables a process may also include a heap  which is memory thatis dynamically allocated during process run time the structure of a process in memory is shown in figure 3.1  we emphasize that a program by itself is not a process ; a program is a passive entity  such as a file containing a list of instructions stored on disk  often called an executable file   whereas a process is an active entity  with a program counter specifying the next instruction to execute and a set of associated resources a program becomes a process when an executable file is loaded into memory  two common techniques for loading executable files are double-clicking an icon representing the executable file and entering the name of the executable file on the command line  as in prog exe or a out  0 figure 3.1 process in memory  3.1 103 1/0 or event completion figure 3.2 diagram of process state  although two processes may be associated with the same program  they are nevertheless considered two separate execution sequences for instance  several users may be running different copies of the mail program  or the same user may invoke many copies of the web browser program each of these is a separate process ; and although the text sections are equivalent  the data  heap  and stack sections vary it is also common to have a process that spawns many processes as it runs we discuss such matters in section 3.4  3.1.2 process state as a proces   ; excr  utes  it changes state the state of a process is defil1.ed in part by the current activity of that process each process may be in one of the following states  new the process is being created  running instructions are being executed  waiting the process is waiting for some event to occur  such as an i/0 completion or reception of a signal   ready the process is waiting to be assigned to a processor  terminated the process has finished execution  these names are arbitrary  and they vary across operating systems the states that they represent are found on all systems  however certain operating systems also more finely delineate process states it is important to realize that only one process can be running on any processor at any instant many processes may be ready and waiting  however the state diagram corresponding to these states is presented in figure 3.2  3.1.3 process control block _  1cb pr   cess isrepreserlt ~ pjnthe operatir1,g system l  jy a process_ coptrol blo_ck _  pcb  -alsocalled a taskcontrozbloclc a pcb is shown in figure 3.3 it contains mi  my pieces of iil.format1o11assodated with a specific process  including these  104 chapter 3 figure 3.3 process control block  pcb   process state the state may be new  ready runnil g  waiting  halted  and so on  program counter the counter indicates the address of the next instruction to be executed for this process  cpu registers the registers vary in number and type  depending on the computer architecture they mclude accumulators  index registers  stack pointers  and general-purpose registers  plus any condition-code information along with the program counter  this state information must be saved when an mterrupt occurs  to allow the process to be continued correctly afterward  figure 3.4   cpu-scheduling information this information includes a process priority  pointers to scheduling queues  and any other scheduling parameters   chapter 5 describes process scheduling  memory-management information this information may include such information as the value of the base and limit registers  the page tables  or the segment tables  dependmg on the memory system used by the operating system  chapter 8   accounting information this mformation includes the amount of cpu and real time used  time limits  account numbers  job or process numbers  and so on  i/o status information this information includes the list of i/o devices allocated to the process  a list of open files  and so on  in briet the pcb simply serves as the repository for any information that may vary from process to process  3.1.4 threads the process model discussed so far has implied that a process is a program that performs a single thread of execution for example  when a process is running a word-processor program  a single thread of instructions is being executed this single thread of control allows the process to perform only one 3.2 process p0 idle 3.2 operating system interrupt or system call process p1 executing idle figure 3.4 diagram showing cpu switch from process to process  105 task at one time the user can not simultaneously type in characters and run the spell checker within the same process  for example many modern operatin.g systems have extended the process concept to allow a process to have multiple threads of execution and thus to perform more than one task at a time on a system that supports threads  the pcb is expanded to include information for each thread other changes throughout the system are also needed to support threads chapter 4 explores multithreaded processes in detail  the objective of multiprogramming is to have some process nnming at all times  to maximize cpu utilization the objective of time sharing is to switch the cpu among processes so frequently that users can interact with each program while it is run.ning to meet these objectives  the process scheduler selects an available process  possibly from a set of several available processes  for program execution on the cpu for a single-processor system  there will never be more than one running process if there are more processes  the rest will have to wait until the cpu is free and can be rescheduled  3.2.1 scheduling queues as processes enter the system  they are put into a job queue  which consists of all processes in the system the processes that are residing in main memory and are ready and waiting to execute are kept on a list called the ready queue  106 chapter 3 process representation in linux the process control block in the linux operating system is represented by the c struch1re task_struct this structure contains all the necessary information for representing a process  including the state of the process  scheduling and memory-management information  list of open files  and pointers to the process 's parent and any of its children  a process 's parent is the process that created it ; its children are any processes that it creates  some of these fields include  pid_t pid ; i process identifier i long state ; i state of the process i unsigned int time_slice i scheduling information i struct task_struct parent ; i this process 's parent i struct list__head children ; i this process 's children i struct files_struct files ; i list of open files i struct mm_struct mm ; i address space of this process i for example  the state of a process isrepresented by the field long state in this structure within the linux kernel  all active processes are represented using a doubly linked list of task_struct  and the kernel maintains a pointer -current -to the process currently executing on the system this is shown in figure 3.5  struct task_struct process information struct task_struct process information t current  currently executing proccess  figure 3.5 active processes in linux  struct task_struct process information as an illustration of how the kernel might manipulate one of the fields in the task_struct for a specified process  let 's assume the system would like to change the state of the process currently running to the value new_state  if currentis a pointer to the process currently executing  its state is changed with the following  current state = new_state ; this queue is generally stored as a linked list a ready-queue header contains pointers to the first and final pcbs in the list each pcb includes a pointer field that points to the next pcb in the ready queue  3.2 107 queue header mag tape ~ = 7  c 77 ~ unit 0 k  \ \ t_82_11 ~ ~ il = = figure 3.6 the ready queue and various 1/0 device queues  the system also includes other queues when a process is allocated the cpu  it executes for a while and eventually quits  is interrupted  or waits for the occurrence of a particular event  such as the completion of an i/0 request  suppose the process makes an i/o request to a shared device  such as a disk  since there are many processes in the system  the disk may be busy with the i/0 request of some other process the process therefore may have to wait for the disk the list of processes waiting for a particular i/0 device is called a device queue each device has its own device queue  figure 3.6   a common representation of process scheduling is a queueing diagram  such as that in figure 3.7 each rectangular box represents a queue two types of queues are present  the ready queue and a set of device queues the circles represent the resources that serve the queues  and the arrows indicate the flow of processes in the system  a new process is initially put in the ready queue it waits there until it is selected for execution  or is dispatched once the process is allocated the cpu and is executing  one of several events could occur  the process could issue an i/0 request and then be placed in an i/0 queue  the process could create a new subprocess and wait for the subprocess 's termination  the process could be removed forcibly from the cpu  as a result of an interrupt  and be put back in the ready queue  108 chapter 3 figure 3.7 queueing-diagram representation of process scheduling  in the first two cases  the process eventually switches from the waiting state to the ready state and is then put back in the ready queue a process continues this cycle until it terminates  at which time it is removed from all queues and has its pcb and resources deallocated  3.2.2 schedulers a process migrates among the various scheduling queues throughout its lifetime the operating system must select  for scheduling purposes  processes from these queues in some fashion the selection process is carried out by the appropriate scheduler  often  in a batch system  more processes are submitted than can be executed immediately these processes are spooled to a mass-storage device  typically a disk   where they are kept for later execution the long-term scheduler  or job scheduler  selects processes from this pool and loads them into memory for execution the short-term scheduler  or cpu scheduler  selects from among the processes that are ready to execute and allocates the cpu to one of them  the primary distinction between these two schedulers lies in frequency of execution the short-term scheduler must select a new process for the cpu frequently a process may execute for only a few milliseconds before waiting for an i/0 request often  the short-term scheduler executes at least once every 100 milliseconds because of the short time between executions  the short-term scheduler must be fast if it takes 10 milliseconds to decide to execute a process for 100 milliseconds  then 10 i  100 + 10  = 9 percent of the cpu is being used  wasted  simply for scheduling the work  the long-term scheduler executes much less freqvently ; minutes may separate the creation of one new process and the next the long-term scheduler controls the degree of multiprogramming  the number of processes in memory   if the degree of multiprogramming is stable  then the average rate of process creation must be equal to the average departure rate of processes leaving the system thus  the long-term scheduler may need to be invoked 3.2 109 only when a process leaves the system because of the longer interval between executions  the long-term scheduler can afford to take more tin e to decide which process should be selected for execution  it is important that the long-term scheduler make a careful selection in general  most processes can be described as either i/ 0 bound or cpu bound an i/o-bound process is one that spends more of its time doing i/o than it spends doing computations a cpu-bound process  in contrast  generates i/0 requests infrequently  using more of its time doing computations it is important that the long-term scheduler select a good process mix of i/o-bound and cpu-bound processes if all processes are i/0 bound  the ready queue will almost always be empty  and the short-term scheduler will have little to do if all processes are cpu bound  the i/0 waiting queue will almost always be empty  devices will go unused  and again the system will be unbalanced the system with the best performance will thus have a combination of cpu-bound and i/o-bound processes  on some systems  the long-term scheduler may be absent or minimal  for example  time-sharing systems such as unix and microsoft windows systems often have no long-term scheduler but simply put every new process in memory for the short-term scheduler the stability of these systems depends either on a physical limitation  such as the number of available terminals  or on the self-adjusting nature of human users if performance declines to m acceptable levels on a multiuser system  some users will simply quit  some operating systems  such as time-sharing systems  may introduce an additional  intermediate level of scheduling this medium-term scheduler is diagrammed in figure 3.8 the key idea behind a medium-term scheduler is that sometimes it can be advantageous to remove processes from memory  and from active contention for the cpu  and thus reduce the degree of multiprogramrning later  the process can be reintroduced into memory  and its execution can be continued where it left off this scheme is called swapping the process is swapped out  and is later swapped in  by the medium-term scheduler swapping may be necessary to improve the process mix or because a change in memory requirements has overcommitted available memory  requiring memory to be freed up swapping is discussed in chapter 8  swap in      partiaii  ' exec ~ t ~ d sw11pped-out processes  swap out figure 3.8 addition of medium-term scheduling to the queueing diagram  110 chapter 3 3.3 3.2.3 context switch as mentioned in section 1.2.1  interrupts cause the operating system to change a cpu from its current task and to run a kernel routine such operations happen frequently on general-purpose systems when an interrupt occurs  the system needs to save the current of the process running on the cpu so that it can restore that context when its processing is done  essentially suspending the process and then resuming it the context is represented in the pcb of the process ; it includes the value of the cpu registers  the process state  see figure 3.2   and memory-management information generically  we perform a of the current state of the cpu  be it in kernel or user mode  and then a to resu.me operations  switching the cpu to another process requires performing a state save of the current process and a state restore of a different process this task is known as a when a context switch occurs  the kernel saves the context of the old process in its pcb and loads the saved context of the new process scheduled to run context-switch time is pure overhead  because the system does no useful work while switching its speed varies from machine to machine  depending on the memory speed  the number of registers that must be copied  and the existence of special instructions  such as a single instruction to load or store all registers   typical speeds are a few milliseconds  context-switch times are highly dependent on hardware support for instance  some processors  such as the sun ultrasparc  provide multiple sets of registers a context switch here simply requires changing the pointer to the current register set of course  if there are more active processes than there are register sets  the system resorts to copying register data to and from memory  as before also  the more complex the operating system  the more work must be done during a context switch as we will see in chapter 8  advanced memory-management techniques may require extra data to be switched with each context for instance  the address space of the current process must be preserved as the space of the next task is prepared for use how the address space is preserved  and what amount of work is needed to preserve it  depend on the memory-management method of the operating system  the processes in most systems can execute concurrently  and they may be created and deleted dynamically thus  these systems must provide a mechanism for process creation and termination in this section  we explore the n1.echanisms involved in creating processes and illustrate process creation on unix and windows systems  3.3.1 process creation a process may create several new processes  via a create-process system call  during the course of execution the creating process is called a parent process  and the new processes are called the children of that process each of these new processes may in turn create other processes  forming a tree of processes  most operating systems  including unix and the windows family of operating systems  identify processes according to a unique process identifier 3.3 111  or pid   which is typically an integer number figure 3.9 illustrates a typical process tree for the solaris operating system  showing the name of each process and its pid in solaris  the process at the top of the tree is the sched process  with pid of 0 the sched process creates several children processes-including pageout and fsflush these processes are responsible for managing memory and file systems the sched process also creates the ini t process  which serves as the root parent process for all user processes in figure 3.9  we see two children of ini t-inetd and dtlogin inetd is responsible for networking services such as telnet and ftp ; dtlogin is the process representing a user login screen when a user logs in  dtlogin creates an x-windows session  xsession   which in turns creates the sdt_shel process below sdlshel  a user 's command-line shell-the c-shell or csh-is created in this commandline interface  the user can then invoke various child processes  such as the ls and cat commands we also see a csh process with pid of 7778 representing a user who has logged onto the system using telnet this user has started the netscape browser  pid of 7785  and the emacs editor  pid of 8105   on unix  we can obtain a listing of processes by using the ps command for example  the command ps -el will list complete information for all processes currently active in the system it is easy to construct a process tree similar to what is shown in figure 3.9 by recursively tracing parent processes all the way to the ini t process  in general  a process will need certain resources  cpu time  memory  files  i/0 devices  to accomplish its task when a process creates a subprocess  that inetd pid = 140 dtlogin pid = 251 figure 3.9 a tree of processes on a typical solaris system  112 chapter 3 subprocess may be able to obtain its resources directly from the operating system  or it may be constrained to a subset of the resources of the parent process the parent may have to partition its resources among its children  or it may be able to share some resources  such as ncemory or files  among several of its children restricting a child process to a subset of the parent 's resources prevents any process from overloading the system by creating too many subprocesses  in addition to the various physical and logical resources that a process obtains when it is created  initialization data  input  may be passed along by the parent process to the child process for example  consider a process whose function is to display the contents of a file-say  img.jpg-on the screen of a terminal when it is created  it will get  as an input from its parent process  the name of the file img.jpg  and it will use that file name  open the file  and write the contents out it may also get the name of the output device some operating systems pass resources to child processes on such a system  the new process may get two open files  img.jpg and the terminal device  and may simply transfer the datum between the two  when a process creates a new process  two possibilities exist in terms of execution  the parent continues to execute concurrently with its children  the parent waits until some or all of its children have terminated  there are also two possibilities in terms of the address space of the new process  the child process is a duplicate of the parent process  it has the same program and data as the parent   the child process has a new program loaded into it  to illustrate these differences  let 's first consider the unix operating system  in unix  as we 've seen  each process is identified by its process identifier  which is a tmique integer a new process is created by the fork   system call the new process consists of a copy of the address space of the original process this mechanism allows the parent process to communicate easily with its child process both processes  the parent and the child  continue execution at the instruction after the fork    with one difference  the return code for the fork   is zero for the new  child  process  whereas the  nonzero  process identifier of the child is returned to the parent  typically  the exec   system call is used after a fork   system call by one of the two processes to replace the process 's memory space with a new program the exec   system call loads a binary file into memory  destroying the memory image of the program containing the exec   system call  and starts its execution in this manner  the two processes are able to communicate and then go their separate ways the parent can then create more children ; or  if it has nothing else to do while the child runs  it can issue await   system call to move itself off the ready queue until the termination of the child  the c program shown in figure 3.10 illustrates the unix system calls previously described we now have two different processes running copies of the same program the only difference is that the value of pid  the process 3.3 # include sysltypes.h # include stdio.h # include unistd.h int main    pid_t pid ;  i fork a child process i pid = fork   ; if  pid 0   i error occurred i fprintf  stderr  fork failed  ; return 1 ;  else if  pid = = 0   i child process i execlp  lbinlls  ls ,null  ;  else  i parent process i  i parent will wait for the child to complete i wait  null  ; printf  child complete  ; return 0 ; figure 3.10 creating a separate process using the unix fork   system call  113 identifier  for the child process is zero  while that for the parent is an integer value greater than zero  in fact  it is the actual pid of the child process   the child process inherits privileges and scheduling attributes from the parent  as well certain resources  such as open files the child process then overlays its address space with the unix command lbin/ls  used to get a directory listing  using the execlp   system call  execlp   is a version of the exec   system call   the parent waits for the child process to complete with the wait   system call when the child process completes  by either implicitly or explicitly invoking exit    the parent process resumes from the call to wait   ,where it completes using the exit   system call this is also illustrated in figure 3.11  parent wait resumes child ~  +  exit   figure 3.11 process creation using fork   system call  114 chapter 3 # include stdio.h # include windows.h int main  void   startupinfo si ; process_information pi ;  ii allocate memory zeromemory  &si  sizeof  si   ; si.cb = sizeof  si  ; zeromemory  &pi  sizeof  pi   ; ii create child process if  ! createprocess  null  ii use command line c  \ \ windows \ \ system32 \ \ mspaint.exe  ii command line null  ii do n't inherit process handle   null  ii do n't inherit thread handle false  ii disable handle inheritance 0  ii no creation flags null  ii use parent 's environment block null  ii use parent 's existing directory &si  &pi   fprintf  stderr  create process failed  ; return -1 ; ii parent will wait for the child to complete waitforsingleobject  pi.hprocess  infinite  ; printf  child complete  ; ii close handles closehandle  pi.hprocess  ; closehandle  pi.hthread  ; figure 3.12 creating a separate process using the win32 api  as an alternative examplef we next consider process creation in windows  processes are created in the win32 api using the createprocess   functionf which is similar to fork   in that a parent creates a new child process howeverf whereas fork   has the child process inheriting the address space of its parent createprocess   requires loading a specified program into the address space of the child process at process creation furthermoref whereas fork   is passed no parametersf createprocess   expects no fewer than ten parameters  the c program shown in figure 3.12 illustrates the createprocess   functionf which creates a child process that loads the application mspaint ex e  we opt for many of the default values of the ten parameters passed to createprocess    readers interested in pursuing the details of process 3.3 115 creation and management in the win32 api are encouraged to consult the bibliographical notes at the end of this chapter  two parameters passed to createprocess   are instances of the startupinfo and process_information structures startupinfo specifies many properties of the new process  such as window size and appearance and handles to standard input and output files the process_information structure contains a handle and the identifiers to the newly created process and its thread  we invoke the zeromemory   function to allocate memory for each of these structures before proceeding with createprocess    the first two parameters passed to createprocess   are the application name and command-line parameters if the application name is null  as it is in this case   the command-line parameter specifies the application to load in this instance  we are loading the microsoft windows mspaint.exe application beyond these two initial parameters  we use the default parameters for inheriting process and thread handles as well as specifying no creation flags  we also use the parent 's existing environment block and starting directory  last  we provide two pointers to the startupinfo and process.lnformation structures created at the beginning of the program in figure 3.10  the parent process waits for the child to complete by invoking the wait   system call the equivalent of this in win32 is wai tforsingleobj ect    which is passed a handle of the child process-pi hprocess-and waits for this process to complete once the child process exits  control returns from the wai tforsingleobj ect   function in the parent process  3.3.2 process termination a process terminates when it finishes executing its final statement and asks the operating system to delete it by using the exit   system call at that point  the process may return a status value  typically an integer  to its parent process  via the wait   system call   all the resources of the process-including physical and virtual memory  open files  and i/0 buffers-are deallocated by the operating system  termination can occur in other circumstances as well a process can cause the termination of another process via an appropriate system call  for example  terminateprocess   in win32   usually  such a system call can be invoked only by the parent of the process that is to be terminated otherwise  users could arbitrarily kill each other 's jobs note that a parent needs to know the identities of its children thus  when one process creates a new process  the identity of the newly created process is passed to the parent  a parent may terminate the execution of one of its children for a variety of reasons  such as these  the child has exceeded its usage of some of the resources that it has been allocated  to determine whether this has occurred  the parent m.ust have a mechanism to inspect the state of its children  the task assigned to the child is no longer required  the parent is exiting  and the operating system does not allow a child to continue if its parent terminates  116 chapter 3 3.4 some systencs  including vms  do not allow a child to exist if its parent has terminated in such systems  if a process terminates  either normally or abnormally   then all its children must also be terminated this phenomenon  referred to as cascading termination  is normally initiated by the operating system  to illustrate process execution and termination  consider that  in unix  we can terminate a process by using the exit   system call ; its parent process may wait for the termination of a child process by using the wait   system call the wait   system call returns the process identifier of a terminated child so that the parent can tell which of its children has terminated if the parent terminates  however  all its children have assigned as their new parent the ini t process thus  the children still have a parent to collect their status and execution statistics  processes executing concurrently in the operating system may be either independent processes or cooperating processes a process is independent if it can not affect or be affected by the other processes executing in the system  any process that does not share data with any other process is independent a process is cooperating if it can affect or be affected by the other processes executing in the system clearly  any process that shares data with other processes is a cooperating process  there are several reasons for providing an environment that allows process cooperation  information sharing since several users may be interested in the same piece of information  for instance  a shared file   we must provide an environment to allow concurrent access to such information  computation speedup if we want a particular task to run faster  we must break it into subtasks  each of which will be executing in parallel with the others notice that such a speedup can be achieved only if the computer has multiple processing elements  such as cpus or i/o channels   modularity we may want to construct the system in a modular fashion  dividing the system functions into separate processes or threads  as we discussed in chapter 2  convenience even an individual user may work on many tasks at the same time for instance  a user may be editing  printing  and compiling in parallel  cooperating processes require an interprocess communication  ipc  mechanism that will allow them to exchange data and information there are two fundamental models of interprocess communication   1  shared memory and  2  message passing in the shared-memory model  a region of memory that is shared by cooperating processes is established processes can then exchange information by reading and writing data to the shared region in the messagepassing model  communication takes place by means of messages exchanged 3.4 117 process a process a 2 2 kernel  a   b  figure 3.13 communications models  a  message passing  b  shared memory  between the cooperating processes the two communications models are conh asted in figure 3.13  both of the models just discussed are common in operating systems  and many systems implement both message passing is useful for exchanging smaller ammmts of data  because no conflicts need be avoided message passing is also easier to implement than is shared memory for intercomputer communication shared memory allows maximum speed and convenience of communication shared memory is faster than message passing  as messagepassing system.s are typically implemented using system calls and thus require the more time-consuming task of kernel irttervention in contrast  in sharedmemory systems  system calls are required only to establish shared-memory regions once shared memory is established  all accesses are treated as routine memory accesses  and no assistance from the kernel is required in the ren1.ainder of this section  we explore each of these ipc models in more detail  3.4.1 shared-memory systems interprocess communication using shared memory requires communicating processes to establish a region of shared memory typically  a shared-memory region resides in the address space of the process creating the sharedmemory segment other processes that wish to communicate using this sharedmemory segment must attach it to their address space recall that  normally  the operating system tries to prevent one process from accessing another process 's memory shared memory requires that two or more processes agree to remove this restriction they can then excbange information by reading and writing data in the shared areas the form of the data and the location are determined by these processes and are not under the operating system 's control the processes are also responsible for ensuring that they are not writing to the same location simultaneously  118 chapter 3 to illustrate the concept of cooperating processes  let 's consider the producer-consumer problem  which is a common paradigm for cooperating processes a producer process produces information that is consumed by a consumer process for example  a compiler may produce assembly code  which is consumed by an assembler the assembler  in turn  ncay produce object modules  which are consumed by the loader the producer-consumer problem also provides a useful metaphor for the client-server paradigm we generally think of a server as a producer and a client as a consumer for example  a web server produces  that is  provides  html files and images  which are consumed  that is  read  by the client web browser requesting the resource  one solution to the producer-consumer problem uses shared memory to allow producer and consumer processes to run concurrently  we must have available a buffer of items that can be filled by the producer and emptied by the consumer this buffer will reside in a region of memory that is shared by the producer and consumer processes a producer can produce one item while the consumer is consuming another item the producer and consumer must be synchronized  so that the consumer does not try to consume an item that has not yet been produced  two types of buffers can be used the places no practical limit on the size of the buffer the consumer may have to wait for new items  but the producer can always produce new items the assumes a fixed buffer size in this case  the consumer must wait if the buffer is empty  and the producer must wait if the buffer is full  let 's look more closely at how the bounded buffer can be used to enable processes to share memory the following variables reside in a region of memory shared by the producer and consumer processes  # define buffer_size 10 typedef struct  item ; item buffer  buffer_size  ; int in = 0 ; int out = 0 ; the shared buffer is implemented as a circular array with two logical pointers  in and out the variable in points to the next free position in the buffer ; out points to the first full position in the buffer the buffer is empty when in = = out ; the buffer is full when   in + 1  % buffer_size  = = out  the code for the producer and consumer processes is shown in figures 3.14 and 3.15  respectively the producer process has a local variable nextproduced in which the new item to be produced is stored the consumer process has a local variable next consumed in which the item to be consumed is stored  this scheme allows at most buffer_size  1 items in the buffer at the same time we leave it as an exercise for you to provide a solution where buffer_size items can be in the buffer at the same time in section 3.5.1  we illustrate the posix api for shared memory  3.4 item nextproduced ; while  true    i produce an item in nextproduced i while    in + 1  % buffer_size  = = out  ; i do nothing i buffer  in  = nextproduced ; in =  in + 1  % buffer_size ; figure 3.'14 the producer process  119 one issue this illustration does not address concerns the situation in which both the producer process and the consumer process attempt to access the shared buffer concurrently in chapter 6  we discuss how synchronization among cooperating processes can be implemented effectively in a sharedmemory environment  3.4.2 message-passing systems lrt section 3.4.1  we showed how cooperating processes can communicate in a shared-memory environment the scheme requires that these processes share a region of memory and that the code for accessing and manipulating the shared memory be written explicitly by the application programmer another way to achieve the same effect is for the operating system to provide the means for cooperating processes to comm lmicate with each other via a message-passing facility  message passing provides a mechanism to allow processes to communicate and to synchronize their actions without sharing the same address space and is particularly useful in a distributed environment  where the communicating processes may reside on different computers connected by a network for example  a chat program used on the world wide web could be designed so that chat participants communicate with one another by exchanging messages  a message-passing facility provides at least two operations  send  message  and recei ve  message   messages sent by a process can be of either fixed or variable size if only fixed-sized messages can be sent  the system-level implementation is straightforward this restriction  however  makes the task item nextconsumed ; while  true    while  in = = out  ; ii do nothing nextconsumed = buffer  out  ; out =  out + 1  % buffer_size ; i consume the item in nextconsumed i figure 3.15 the consumer process  120 chapter 3 of programming more difficult conversely  variable-sized messages require a 1nore complex system-level implementation  but the programming task becomes simpler this is a coitlmon kind of tradeoff seen throughout operatingsystem design  if processes p and q want to communicate  they must send messages to and receive messages from each other ; a communication link must exist between them this link can be implemented in a variety of ways we are concerned here not with the link 's physical implementation  such as shared memory  hardware bus  or network  which are covered in chapter 16  but rather with its logical implementation here are several methods for logically implementing a link and the send 0 i receive   operations  direct or indirect communication synchronous or asynchronous communication automatic or explicit buffering we look at issues related to each of these features next  3.4.2.1 naming processes that want to communicate must have a way to refer to each other  they can use either direct or indirect communication  under direct communication  each process that wants to comm lmicate must explicitly name the recipient or sender of the communication in this scheme  the send   and receive   primitives are defined as  send  p  message  -send a message to process p  receive  q  message  -receive a message from process q  a communication link in this scheme has the following properties  a link is established automatically between every pair of processes that want to communicate the processes need to know only each other 's identity to communicate  a link is associated with exactly two processes  between each pair of processes  there exists exactly one link  this scheme exhibits symmetry in addressing ; that is  both the sender process and the receiver process must name the other to communicate a variant of this scheme employs asymmetry in addressing here  only the sender names the recipient ; the recipient is not required to name the sender in this scheme  the send   and receive   primitives are defined as follows  send  p  message  -send a message to process p  receive  id  message  -receive a message from any process ; the variable id is set to the name of the process with which communication has taken place  the disadvantage in both of these schemes  symmetric and asymmetric  is the limited modularity of the resulting process definitions changing the identifier of a process may necessitate examining all other process definitions  all references to the old identifier must be found  so that they can be modified 3.4 121 to the new identifier in general  any such hard-coding techniques  where identifiers must be explicitly stated  are less desirable than techniques involving indirection  as described next  with indirect communication  the messages are sent to and received from mailboxes  or ports a mailbox can be viewed abstractly as an object into which messages can be placed by processes and from which messages can be removed  each mailbox has a w1.ique identification for example  posix message queues use an integer value to identify a mailbox in this scheme  a process can communicate with some other process via a number of different mailboxes  two processes can communicate only if the processes have a shared mailbox  however the send   and receive 0 primitives are defined as follows  send  a  message  -send a message to mailbox a  receive  a  message  -receive a message from mailbox a  in this scheme  a communication link has the following properties  a link is established between a pair of processes only if both members of the pair have a shared mailbox  a link may be associated with more than two processes  between each pair of communicating processes  there may be a number of different links  with each link corresponding to one mailbox  now suppose that processes p1  p2  and p3 all share mailbox a process p1 sends a message to a  while both p2 and p3 execute a receive 0 from a  which process will receive the message sent by p1 the answer depends on which of the following methods we choose  allow a link to be associated with two processes at most  allow at most one process at a time to execute a receive 0 operation  allow the system to select arbitrarily which process will receive the message  that is  either p2 or p3  but not both  will receive the message   the system also may define an algorithm for selecting which process will receive the message  that is  round robin  where processes take turns receiving messages   the system may identify the receiver to the sender  a mailbox may be owned eith ~ r by a process or by the operating system  if the mailbox is owned by a process  that is  the mailbox is part of the address space of the process   then we distinguish between the owner  which can only receive messages through this mailbox  and the user  which can only send messages to the mailbox   since each mailbox has a unique owner  there can be no confusion about which process should receive a message sent to this mailbox when a process that owns a mailbox terminates  the mailbox disappears any process that subsequently sends a message to this mailbox must be notified that the mailbox no longer exists  in contrast  a mailbox that is owned by the operating system has an existence of its own it is independent and is not attached to any particular process the operating system then must provide a mechanism that allows a process to do the following  122 chapter 3 create a new mailbox  send and receive messages through the mailbox  delete a mailbox  the process that creates a new mailbox is that mailbox 's owner by default  initially  the owner is the only process that can receive messages through this n  tailbox however  the ownership and receiving privilege may be passed to other processes through appropriate system calls of course  this provision could result in multiple receivers for each mailbox  3.4.2.2 synchronization communication between processes takes place through calls to send   and receive   primitives there are different design options for implementing each primitive message passing may be either blocking or nonblockingalso known as synchronous and asynchronous  blocking send the sending process is blocked until the message is received by the receiving process or by the mailbox  nonblocking send the sending process sends the message and resumes operation  blocking receive the receiver blocks until a message is available  nonblocking receive the receiver retrieves either a valid message or a null  different combinations of send   and receive   are possible when both send   and receive   are blocking  we have a rendezvous between the sender and the receiver the solution to the producer-consumer problem becomes trivial when we use blocking send   and receive   statements  the producer merely invokes the blocking send   call and waits until the message is delivered to either the receiver or the mailbox likewise  when the consumer invokes receive    it blocks until a message is available  note that the concepts of synchronous and asynchronous occur frequently in operating-system i/0 algorithms  as you will see throughout this text  3.4.2.3 buffering whether communication is direct or indirect  messages exchanged by communicating processes reside in a temporary queue basically  such queues can be implemented in three ways  zero capacity the queue has a maximum length of zero ; thus  the link can not have any messages waiting in it in this case  the sender must block until the recipient receives the message  bounded capacity the que ~ ue has finite length n ; thus  at most n messages can reside in it if the queue is not full when a new message is sent  the message is placed in the queue  either the message is copied or a pointer to the message is kept   and the sender can continue execution without 3.5 3.5 123 waiting the link 's capacity is finite  however if the link is full  the sender must block until space is available in the quelie  unbounded capacity the queue 's length is potentially infinite ; thus  any number of messages can wait in it the sender never blocks  the zero-capacity case is sometimes referred to as a message system with no buffering ; the other cases are referred to as systems with automatic buffering  in this section  we explore three different ipc systems we first cover the posix api for shared memory and then discuss message passing in the mach operating system we conclude with windows xp  which interestingly uses shared memory as a mechanism for providing certain types of message passing  3.5.1 an example  posix shared memory several ipc mechanisms are available for posix systems  including shared memory and message passing here  we explore the posix api for shared memory  a process must first create a shared memory segment using the shmget   system call  shmget   is derived from shared memory get   the following example illustrates the use of shmget    segment_id = shmget  ipcprivate  size  s_lrusr i s_lwusr  ; this first parameter specifies the key  or identifier  of the shared-memory segment if this is set to ipcprivate  a new shared-memory segment is created  the second parameter specifies the size  in bytes  of the shared-memory segment finally  the third parameter identifies the mode  which indicates how the shared-memory segment is to be used-that is  for reading  writing  or both by setting the mode to s_lrusr 1 s_lwusr  we are indicating that the owner may read or write to the shared-memory segment a successful call to shmget   returns an integer identifier for the shared-memory segment other processes that want to use this region of shared memory must specify this identifier  processes that wish to access a shared-memory segment must attach it to their address space using the shmat    shared memory attach  system call  the call to shmat   expects three parameters as well the first is the integer identifier of the shared-memory segment being attached  and the second is a pointer location in memory indicating where the shared ncemory will be attached if we pass a value of null  the operating system selects the location on the user 's behalf the third parameter identifies a flag that allows the sharedmemory region to be attached in read-only or read-write mode ; by passing a parameter of 0  we allow both reads and writes to the shared region we attach a region of shared memory using shmat   as follows  shared_memory =  char  shmat  id  null  0  ; if successful  shmat   returns a pointer to the beginning location in memory where the shared-memory region has been attached  124 chapter 3 once the region of shared ncemory is attached to a process 's address space  the process can access the shared memory as a routine memory access using the pointer returned from shmat    in this example  shmat   returns a pointer to a character string thus  we could write to the shared-memory region as follows  sprintf  shared_memory  writing to shared memory  ; other processes sharing this segment would see the updates to the sharedmemory segment  typically  a process using an existing shared-memory segment first attaches the shared-memory region to its address space and then accesses  and possibly updates  the region of shared memory when a process no longer requires access to the shared-memory segment it detaches the segment from its address space to detach a region of shared memory  the process can pass the pointer of the shared-memory region to the shmdt   system call  as follows  shmdt  shared_memory  ; finally  a shared-memory segment can be removed from the system with the shmctl   system call  which is passed the identifier of the shared segrnent along with the flag ipcrmid  the program shown in figure 3.16 illustrates the posix shared-memory api just discussed this program creates a 4,096-byte shared-memory segment  once the region of shared memory is attached  the process writes the message hi there ! to shared memory after outputting the contents of the updated memory  it detaches and removes the shared-memory region we provide further exercises using the posix shared-memory api in the programming exercises at the end of this chapter  3.5.2 an example  mach as an example of a message-based operating system  we next consider the mach operating system  developed at carnegie mellon university we introduced mach in chapter 2 as part of the mac os x operating system the mach kernel supports the creation and destruction of multiple tasks  which are similar to processes but have multiple threads of control most communication in mach-including most of the system calls and all intertask informationis carried out by messages messages are sent to and received from mailboxes  called ports in mach  even system calls are made by messages when a task is created  two special n  tailboxes-the kernel mailbox and the notify mailbox-are also created the kernel mailbox is used by the kernel to communicate with the task the kernel sends notification of event occurrences to the notify port only three system calls are needed for message transfer the msg_send   call sends a message to a mailbox a message is received via msg_recei ve    remote procedure calls  rpcs  are executed via msg_rpc   ,which sends a message and waits for exactly one return message from the sender in this way  the rpc models a typical subroutine procedure call but can work between systems-hence the term remote  the porlallocate   system call creates a new mailbox and allocates space for its queue of messages the maximum size of the message queue # include stdio.h # include syslshm.h # include syslstat.h int main    3.5 i the identifier for the shared memory segment i int segmenlid ; i a pointer to the shared memory segment i char shared_memory ; i the size  in bytes  of the shared memory segment i canst int size = 4096 ; i allocate a shared memory segment i 125 segment_id = shmget  ipc_private  size  s_irusr i s_iwusr  ;  i attach the shared memory segment i shared_memory =  char  shmat  segment_id  null  0  ; i write a message to the shared memory segment i sprint  shared_memory  hi there !  ; i now print out the string from shared memory i printf  % s \ n  shared_memory  ; i now detach the shared memory segment i shmdt  shared_memory  ; i now remove the shared memory segment i shmctl  segment_id  ipc_rmid  null  ; return 0 ; figure 3.16 c program illustrating posix shared-memory api  defaults to eight messages the task that creates the mailbox is that mailbox 's owner the owner is also allowed to receive from the mailbox only one task at a time can either own or receive from a mailbox  but these rights can be sent to other tasks if desired  the mailbox 's message queue is initially empty as messages are sent to the mailbox  the messages are copied into the mailbox all messages have the same priority mach guarantees that multiple messages from the same sender are queued in first-in  first-out  fifo  order but does not guarantee an absolute ordering for instance  messages from two senders may be queued in any order  the messages themselves consist of a fixed-length header followed by a variable-length data portion the header indicates the length of the message and  indudes two mailbox names one mailbox name is the mailbox to which the message is being sent commonly  the sending thread expects a reply ; so 126 chapter 3 the mailbox name of the sender is passed on to the receiving task  which can use it as a return address  the variable part of a message is a list of typed data items each entry in the list has a type  size  and value the type of the objects specified in the message is important  since objects defined by the operating system-such as ownership or receive access rights  task states  and memory segments-n ay be sent in messages  the send and receive operations themselves are flexible for instance  when a message is sent to a mailbox  the mailbox may be full if the mailbox is not full  the message is copied to the mailbox  and the sending thread continues if the mailbox is full  the sending thread has four options  wait indefinitely until there is room in the mailbox  wait at most n milliseconds  do not wait at all but rather return immediately  temporarily cache a message one message can be given to the operating system to keep  even though the mailbox to which that message is being sent is full when the message can be put in the mailbox  a message is sent back to the sender ; only one such message to a full mailbox can be pending at any time for a given sending thread  the final option is meant for server tasks  such as a line-printer driver after finishing a request  such tasks may need to send a one-time reply to the task that had requested service ; but they must also continue with other service requests  even if the reply mailbox for a client is full  the receive operation must specify the mailbox or mailbox set from which a message is to be received a mailbox set is a collection of mailboxes  as declared by the task  which can be grouped together and treated as one mailbox for the purposes of the task threads in a task can receive only from a mailbox or mailbox set for which the task has receive access a porlstatus   system call returns the number of messages in a given mailbox the receive operation attempts to receive from  1  any mailbox in a mailbox set or  2  a specific  named  mailbox if no message is waiting to be received  the receiving thread can either wait at most n milliseconds or not wait at all  the mach system was especially designed for distributed systems  which we discuss in chapters 16 through 18  but mach is also suitable for singleprocessor systems  as evidenced by its inclusion in the mac os x system the major problem with message systems has generally been poor performance caused by double copying of messages ; the message is copied first from the sender to the mailbox and then from the mailbox to the receiver the mach message system attempts to avoid double-copy operations by using virtual-memory-management techniques  chapter 9   essentially  mach maps the address space containing the sender 's message into the receiver 's address space the message itself is never actually copied this message-management technique provides a large performance boost but works for only intrasystem messages the mach operating system is discussed in an extra chapter posted on our website  3.5 127 3.5.3 an example  windows xp the windows xp operating system is an example of modern design that employs modularity to increase functionality and decrease the time needed to implement new features windows xp provides support for multiple operating environments/ or subsystems/ with which application programs communicate via a n1.essage-passing mechanism the application programs can be considered clients of the windows xp subsystem server  the message-passing facility in windows xp is called the facility the lpc in windows xp communicates between two processes on the same machine it is similar to the standard rpc mechanism that is widely used/ but it is optimized for and specific to windows xp like mach/ windows xp uses a port object to establish and maintain a connection between two processes every client that calls a subsystem needs a communication channet which is provided by a port object and is never inherited windows xp uses two types of ports  connection ports and communication ports they are really the same but are given different names according to how they are used  cmmection ports are named objects and are visible to all processes ; they give applications a way to set up communication channels  chapter 22   the communication works as follows  the client opens a handle to the subsystem/ s connection port object  the client sends a cmmection request  the server creates two private conimunication ports and returns the handle to one of them to the client  the client and server use the corresponding port handle to send messages or callbacks and to listen for replies  windows xp uses two types of rnessage-passing techniques over a port that the client specifies when it establishes the channel the simplest/ which is used for small messages/ uses the port 's message queue as intermediate storage and copies the message from one process to the other under this method  messages of up to 256 bytes can be sent  if a client needs to send a larger message  it passes the message through a which sets up a region of shared memory the client has to decide when it sets up the channel whether or not it will need to send a large message if the client determines that it does want to send large messages/ it asks for a section object to be created similarly  if the server decides that replies will be large  it creates a section object so that the section object can be used  a small message is sent that contains a pointer and size information about the section object this method is more complicated than the first method  but it avoids data copying in both cases  a callback mechanism can be used when either the client or the server can not respond immediately to a request the callback mechanism allows them to perform asynchronous message handling  the structure of local procedure calls in windows xp is shown in figure 3.17  it is important to note that the lpc facility in windows xp is not part of the win32 api and hence is not visible to the application programmer rather  applications using the win32 api invoke standard remote procedure calls  128 chapter 3 3.6 client server connection request j connection i handle i port i handle i client i 1 communication port i ~ i server i handle communication port i shared section object  = 256 bytes  figure 3.17 local procedure calls in windows xp  when the rpc is being invoked on a process on the same system  the rpc is indirectly handled through a local procedure call lpcs are also used in a few other functions that are part of the win32 api  in section 3.4  we described how processes can communicate using shared memory and message passing these techniques can be used for communication in client-server systems  section 1.12.2  as well in this section  we explore three other strategies for communication ir1 client-server systems  sockets  remote procedure calls  rpcs   and pipes  3.6.1 sockets a is defined as an endpoint for communication a pair of processes communicating over a network employ a pair of sockets-one for each process  a socket is identified by an ip address concatenated with a port number in general  sockets use a client-server architecture the server waits for incoming client requests by listening to a specified port once a request is received  the server accepts a cmmection from the client socket to complete the com1ection  servers implementing specific services  such as telnet  ftp  and i-htp  listen to well-known ports  a telnet server listens to port 23 ; an ftp server listens to port 21 ; and a web  or http  server listens to port 80   all ports below 1024 are considered well known ; we can use them to implement standard services  when a client process initiates a request for a connection  it is assigned a port by its host computer this port is some arbitrary number greater than 1024 for example  if a client on host x with ip address 146.86.5.20 wishes to establish a connection with a web server  which is listening on port 80  at address 161.25.19.8  host x may be assigned port 1625 the connection will consist of a pair of sockets   146.86.5.20  1625  on host x and  161.25.19.8  80  on the web server this situation is illustrated in figure 3.18 the packets traveling between the hosts are delivered to the appropriate process based on the destination port number  3.6 129 host x  i 46.86.5.20  socket  i 46.86.5.20  1 625  web server  i 61 .25 i 9.8  socket  i 61 .25 i 9.8  80  figure 3.18 communication using sockets  all com1.ections must be unique therefore  if another process also on host x wished to establish another connection with the same web server  it would be assigned a port number greater than 1024 and not equal to 1625 this ensures that all com1.ections consist of a unique pair of sockets  although most program examples in this text use c  we will illustrate sockets using java  as it provides a much easier interface to sockets and has a rich library for networking utilities those interested in socket programming inc or c + + should consult the bibliographical notes at the end of the chapter  java provides three different types of sockets  are implemented with the socket class  use the datagram.socket class finally  the mul ticastsocket class is a subclass of the datagram.socket class a multicast socket allows data to be sent to multiple recipients  our example describes a date server that uses connection-oriented tcp sockets the operation allows clients to request the current date and time from the server the server listens to port 6013  although the port could have any arbitrary number greater than 1024 when a cmmection is received  the server returns the date and time to the client  the date server is shown in figure 3.19 the server creates a serversocket that specifies it will listen to port 6013 the server then begins listening to the port with the accept   method the server blocks on the accept   method waiting for a client to request a com1.ection when a connection request is received  accept   returns a socket that the server can use to communicate with the client the details of how the server communicates with the socket are as follows  the server first establishes a printwri ter objectthatitwill use to communicate with the client a printwri ter object allows the server to write to the socket using the routine print   and println   methods for output the server process sends the date to the client  calling the method println    once it has written the date to the socket  the server closes the socket to the client and resumes listening for more requests  a client communicates with the server by creating a socket and connecting to the port on which the server is listening we implement such a client in the 130 chapter 3 import java.net ; import java.io ; public class dateserver   public static void main  string   args   try    serversocket sock = new serversocket  6013  ; ii now listen for connections while  true    socket client = sock.accept   ; printwriter pout = new printwriter  client.getoutputstream    true  ; ii write the date to the socket pout.println  new java.util.date   .tostring    ; ii close the socket and resume ii listening for connections client close   ; catch  ioexception ioe   system.err.println  ioe  ;  figure 3.19 date server  java program shown in figure 3.20 the client creates a socket and requests a connection with the server at ip address 127.0.0.1 on port 6013 once the connection is madef the client can read from the socket using normal stream i/0 statements after it has received the date from the serverf the client closes the socket and exits the ip address 127.0.0.1 is a special ip address known as the when a computer refers to ip address 127.0.0.t it is referring to itself  this mechanism allows a client and server on the same host to communicate using the tcp /ip protocol the ip address 127.0.0.1 could be replaced with the ip address of another host running the date server in addition to an ip addressf an actual host namef such as www.westminstercollege.eduf can be used as well  communication using sockets-although common and efficient-is considered a low-level form of communication between distributed processes  one reason is that sockets allow only an unstructured stream of bytes to be exchanged between the communicating threads it is the responsibility of the client or server application to impose a structure on the data in the next two subsectionsf we look at two higher-level methods of communication  remote procedure calls  rpcs  and pipes  3.6 import java.net ; import java.io ; public class dateclient   public static void main  string   args   try    //make connection to server socket socket sock = new socket  127.0.0.1 ,6013  ; inputstream in = sock.getinputstream   ; bufferedreader bin = new bufferedreader  new inputstreamreader  in   ; ii read the date from the socket string line ; while   line = bin.readline    ! = null  system.out.println  line  ; ii close the socket connection sock close   ; catch  idexception ioe   system.err.println  ioe  ;  figure 3.20 date client  3.6.2 remote procedure calls 131 one of the most common forms of remote service is the rpc paradigm  which we discussed briefly in section 3.5.2 the rpc was designed as a way to abstract the procedure-call mechanism for use between systems with network connections it is similar in many respects to the ipc mechanism described in section 3.4  and it is usually built on top of such a system here  howeve1 ~ because we are dealing with an environment in which the processes are executing on separate systems  we must use a message-based communication scheme to provide remote service in contrast to the ipc facility  the messages exchanged in rpc communication are well structured and are thus no longer just packets of data each message is addressed to an rpc daemon listening to a port on the remote system  and each contains an identifier of the ftmction to execute and the parameters to pass to that function the function is then executed as requested  and any output is sent back to the requester in a separate message  a port is simply a number included at the start of a message packet whereas a system normally has one network address  it can have many ports within that address to differentiate the many network services it supports if a rencote process needs a service  it addresses a message to the proper port for instance  132 chapter 3 if a system wished to allow other systems to be able to list its current users  it would have a daemon supporting such an rpc attached to a port-say port 3027 any remote system could obtain the needed information  that is  the list of current users  by sending an rpc message to port 3027 oil the server ; the data would be received in a reply message  the semantics of rpcs allow a client to invoke a procedure on a remote host as it would invoke a procedure locally the rpc system hides the details that allow comnmnication to take place by providing a on the client side  typically  a separate stub exists for each separate remote procedure when the client invokes a remote procedure  the rpc system calls the appropriate stub  passing it the parameters provided to the remote procedure this stub locates the port on the server and marshals the parameters parameter marshalling involves packaging the parameters into a form that can be transmitted over a network the stub then transmits a message to the server using message passing a similar stub on the server side receives this message and invokes the procedure on the server if necessary  return values are passed back to the client using the same teclu1.ique  one issue that must be dealt with concerns differences in data representation on the client and server machines consider the representation of 32-bit integers some systems  known as big-endian  store the most significant byte first  while other systems  known as little-endian  store the least significant byte first neither order is better per se ; rather  the choice is arbitrary within a computer architecture to resolve differences like this  many rpc systems define a machine-independent representation of data one such representation is known as data on the client side  parameter marshalling involves converting the machine-dependent data into xdr before they are sent to the server on the server side  the xdr data are m1.marshalled and converted to the machine-dependent representation for the server  another important issue involves the semantics of a call whereas local procedure calls fail only under extreme circumstances  rpcs can fait or be duplicated and executed more than once  as a result of common network errors one way to address this problem is for the operating system to ensure that messages are acted on exactly once  rather than at most once most local procedure calls have the exactly once functionality  but it is more difficult to implement  first  consider at most once  this semantic can be implemented by attaching a timestamp to each message the server must keep a history of all the timestamps of messages it has already processed or a history large enough to ensure that repeated messages are detected incoming messages that have a timestamp already in the history are ignored the client can then send a message one or more times and be assured that it only executes once   generation of these timestamps is discussed in section 18.1  for exactly once/ ' we need to remove the risk that the server will never receive the reqliest to accomplish this  the server must implement the at most once protocol described above but must also acknowledge to the client that the rpc call was received and executed these ack messages are common throughout networking the client must resend each rpc call periodically until it receives the ack for that call  another important issue concerns the communication between a server and a client with standard procedure calls  some form of binding takes place 3.6 133 during link  load  or execution time  chapter 8  so that a procedure call 's name is replaced by the memory address of the procedure call the rpc scheme requires a similar binding of the client and the server port  but how does a client know the port numbers on the server neither system has full information about the other because they do not share memory  two approaches are common first  the binding information may be predetermined  in the form of fixed port addresses at compile time  an rpc call has a fixed port number associated with it once a program is compiled  the server can not change the port number of the requested service second  binding can be done dynamically by a rendezvous mechanism an operating system provides a rendezvous  also called a daemon on a fixed rpc port a client then sends a message containing the name of the rpc to the rendezvous daemon requesting the port address of the rpc it needs to execute the port number is returned  and the rpc calls can be sent to that port until the process terminates  or the server crashes   this method requires the extra overhead of the initial request but is more flexible than the first approach figure 3.21 shows a sample interaction  client kejyt ! .c3  sends rness tqe  t     matchrnakecto fit  1d pgrtpuml   er messages from  client to  server f \ port  matchmakerf + 1  re  address for rpc x from  server to  client port  kernel re  rpcx port  p server figure 3.21 execution of a remote procedure call  rpc   134 chapter 3 the rpc scheme is useful in implementing a distriblited file system  chapter 17   such a system can be implemented as a set of rpc daemons and clients the messages are addressed to the distributed file system port on a server on which a file operation is to take place the message contains the disk operation to be performed the disk operation might be read  write  rename  delete  or status  corresponding to the usual file-related system calls the return message contains any data resulting from that call  which is executed by the dfs daemon on behalf of the client for instance  a message might contain a request to transfer a whole file to a client or be limited to a simple block request in the latter case  several such requests may be needed if a whole file is to be transferred  3.6.3 pipes a acts as a conduit allowin.g two processes to communicate pipes were one of the first ipc mechanisms in early unix systems and typically provide one of the simpler ways for processes to communicate with one another  although they also have some limitations in implementing a pipe  four issues must be considered  does the pipe allow unidirectional communication or bidirectional communication if two-way communication is allowed  is it half duplex  data can travel only one way at a time  or full duplex  data can travel in both directions at the same time  must a relationship  such as parent-child  exist between the commlmicating processes can the pipes communicate over a network  or must the communicating processes reside on the same machine in the following sections  we explore two common types of pipes used on both unix and windows systems  3.6.3.1 ordinary pipes ordinary pipes allow two processes to communicate in standard producerconsumer fashion ; the producer writes to one end of the  the and the consumer reads from the other end  the a result  ordinary pipes are unidirectional  allowing only one-way communication if two-way communication is required  two pipes must be used  with each pipe sending data in a different direction we next illustrate constructing ordinary pipes on both unix and windows systems in both program examples  one process writes the message greetings to the pipe  while the other process reads this message front the pipe  on unix systems  ordinary pipes are constructed using the function pipe  int fd    this function creates a pipe that is accessed through the int fd   file descriptors  fd  0  is the read-end of the pipe  and fd  1  is the write end  3.6 135 parent child fd  o  fd  1  fd  o  fd  1  u  -pip-e -ou figure 3.22 file descriptors for an ordinary pipe  unix treats a pipe as a special type of file ; thus  pipes can be accessed using ordinary read   and write   system calls  an ordinary pipe can not be accessed from outside the process that creates it thus  typically a parent process creates a pipe and uses it to comnmnicate with a child process it creates via fork    recall from section 3.3.1 that a child process inherits open files from its parent since a pipe is a special type of file  the child inherits the pipe from its parent process figure 3.22 illustrates the relationship of the file descriptor fd to the parent and child processes  in the unix progranc shown in figure 3.23  the parent process creates a pipe and then sends a fork   call creating the child process what occurs after the fork   call depends on how the data are to flow through the pipe in this instance  the parent writes to the pipe and the child reads from it it is important to notice that both the parent process and the child process initially close their unused ends of the pipe although the program shown in figure 3.23 does not require this action  it is an important step to ensure that a process reading from the pipe can detect end-of-file  read   returns 0  when the writer has closed its end of the pipe  # include sys/types.h # include stdio.h # include string.h # include unistd.h # define buffer_size 25 # define read_end 0 # define write_end 1 int main  void   char write_msg  buffer_size  char read_msg  buffer_size  ; int fd  2  ; pid_t pid ; greetings ; program continues in figure 3.24 figure 3.23 ordinary pipes in unix  136 chapter 3  i create the pipe i if  pipe  fd  = = -1   fprintf  stderr  pipe failed  ; return 1 ;  i fork a child process i pid = fork   ; if  pid 0   i error occurred i fprintf  stderr  fork failed  ; return 1 ;  if  pid 0   i parent process i  i close the unused end of the pipe i close  fd  read_end   ; i write to the pipe i write  fd  write_end   write_msg  strlen  write_msg  + 1  ; i close the write end of the pipe i close  fd  write_end   ; else  i child process i  i close the unused end of the pipe i close  fd  write_end   ; i read from the pipe i read  fd  read_end   read_msg  buffer_size  ; printf  read % s  read_msg  ; i close the write end of the pipe i close  fd  read_end   ; return 0 ; figure 3.24 continuation of figure 3.23 program  ordinary pipes on windows systems are termed and they behave similarly to their unix counterparts  they are unidirectional and employ parent-child relationships between the communicating processes  in addition  reading and writing to the pipe can be accomplished with the ordinary readfile   and wri tefile   functions the win32 api for creating pipes is the createpi pe   function  which is passed four parameters  separate handles for  1  reading and  2  writing to the pipe  as well as  3  an instance of the startupinfo structure  which is used to specify that the child process is to 3.6 # include stdio.h # include stdlib.h # include windows.h # define buffer_size 25 int main  void   handle readhandle  writehandle ; startupinfo si ; process_information pi ; char message  buffer_size  greetings ; dword written ; program continues in figure 3.26 figure 3.25 windows anonymous pipes parent process  137 inherit the handles of the pipe furthermore   4  the size of the pipe  in bytes  may be specified  figure 3.25 illustrates a parent process creating an anonymous pipe for communicating with its child unlike unix systems  in which a child process automatically inherits a pipe created by its parent  windows requires the programmer to specify which attributes the child process will inherit this is accomplished by first initializing the security--attributes structure to allow handles to be inherited and then redirecting the child process 's handles for standard input or standard output to the read or write handle of the pipe  since the child will be reading from the pipe  the parent must redirect the child 's standard input to the read handle of the pipe furthermore  as the pipes are half duplex  it is necessary to prohibit the child from inheriting the write end of the pipe creating the child process is similar to the program in figure 3.12  except that the fifth parameter is set to true  indicating that the child process is to inherit designated handles from its parent before writing to the pipe  the parent first closes its unused read end of the pipe the child process that reads from the pipe is shown in figure 3.27 before reading from the pipe  this program obtains the read handle to the pipe by invoking getstdhandle    note that ordinary pipes require a parent-child relationship between the communicating processes on both unix and windows systems this means that these pipes can be used only for communication between processes on the same machine  3.6.3.2 named pipes ordinary pipes provide a simple communication mechanism between a pair of processes however  ordinary pipes exist only while the processes are communicating with one another on both unix and windows systems  once the processes have finished communicating and terminated  the ordinary pipe ceases to exist  138 chapter 3 i set up security attributes allowing pipes to be inherited i securi1yattributes sa =  sizeof  securityattributes   null  true  ; i allocate memory i zeromemory  &pi  sizeof  pi   ; i create the pipe i if  ! createpipe  &readhandle  &writehandle  &sa  0    fprintf  stderr  create pipe failed  ; return 1 ;  i establish the startjnfo structure for the child process i getstartupinfo  &si  ; si.hstdoutput = getstdhandle  std_outputjhandle  ; i redirect standard input to the read end of the pipe i si.hstdinput = readhandle ; si dwflags = startf _usestdhandles ; i do n't allow the child to inherit the write end of pipe i sethandleinformation  wri tehandle  handle_flagjnherit  0  ; i create the child process i createprocess  null  child.exe  null,null  true  i inherit handles i 0  null  null  &si  &pi  ; i close the unused end of the pipe i closehandle  readhandle  ; i the parent writes to the pipe i if  ! wri tefile  wri tehandle  message  buffer_size  &written  null   fprintf  stderr  error writing to pipe  ; i close the write end of the pipe i closehandle  writehandle  ; i wait for the child to exit i wai tforsingleobj ect  pi hprocess  infinite  ; closehandle  pi.hprocess  ; closehandle  pi.hthread  ; return 0 ;  figure 3.26 continuation of figure 3.25 program  named pipes provide a much more powerful communication tool ; communication can be bidirectional  and no parent-child relationship is required once a named pipe is established  several processes can use it for 3.6 # include stdio.h # include windows.h # define buffer_stze 25 int main  void   handle readhandle ; char buffer  buffer_size  ; dword read ; i get the read handle of the pipe i readhandle getstdhandle  std_inpuli-iandle  ; i the child reads from the pipe i 139 if  readfile  readhandle  buffer  buffer_size  &read  null   printf  child read % s ,buffer  ; else fprintf  stderr  error reading from pipe  ; return 0 ;  figure 3.27 windows anonymous pipes -child process  communication in fact  in a typical scenario  a named pipe has several writers additionally  named pipes continue to exist after communicating processes have finished both unix and windows systems support named pipes  although the details of implementation vary greatly next  we explore named pipes in each of these systems  named pipes are referred to as fifos in unix systems once created  they appear as typical files in the file system a fifo is created with the mkfifo   system call and manipulated with the ordinary open    read    write    and close   system calls it will contirme to exist m til it is explicitly deleted from the file system although fifos allow bidirectional communication  only half-duplex transmission is permitted if data must travel in both directions  two fifos are typically used additionally  the communicating processes must reside on the same machine ; sockets  section 3.6.1  must be used if intermachine communication is required  named pipes on windows systems provide a richer communication mechanism than their unix counterparts full-duplex communication is allowed  and the communicating processes may reside on either the same or different machines additionally  only byte-oriented data may be transmitted across a unix ftfo  whereas windows systems allow either byte or message-oriented data named pipes are created with the createnamedpipe   function  and a client can connect to a named pipe using connectnamedpipe    communication over the named pipe can be accomplished using the readfile   and wri tefile   functions  140 chapter 3 3.7 pipes in practice pipes are used quite often in the unix command-line environment for situations in which the output of one command serves as input to the second for example ; the unix ls command produces a directory listing  for especially long directory listings ; the output may scroll through several screens the command more manages output by displaying only one screen of output at a time ; the user must press the space bar to move from one screen to the next setting up a pipe between the ls and more commands  which are running as individual processes  allows the output of ls to be delivered as the input to moref enabling the user to display a large directory listing a screen at a time a pipe can be constructed on the command line using the i character the complete command is ls i more in this scenario ; the ls corrm1and serves as the producer  and its output is consumed by the more command  windows systems provide a more command for the dos shell with functionality similar to that of its unix cmmterpart the dos shell also uses the i character for establishing a pipe the only difference is that to get a directory listing  dos uses the dir command rather than ls the equivalent command in dos to what is shown above is dir i more a process is a program in execution as a process executes/ it changes state the state of a process is defined by that process 's current activity each process may be in one of the following states  new  readyf running  waiting ; or terminated  each process is represented in the operating system by its own process control block  pcb   a process ; when it is not executing ; is placed in some waiting queue there are two major classes of queues in an operating system  i/0 request queuecc and the ready queue the ready queue contains all the processes that are ready to execute and are waiting for the cpu each process is represented by a pcbf and the pcbs can be linked together to form a ready queue long-term  job  scheduling is the selection of processes that will be allowed to contend for the cpu normally  long-term scheduling is heavily influenced by resourceallocation considerations  especially memory management short-term  cpu  scheduling is the selection of one process from the ready queue  operating systems must provide a mechanism for parent processes to create new child processes the parent may wait for its children to terminate before proceeding  or the parent and children may execute concurrently there are several reasons for allowing concurrent execution  information sharing  computation speedup  modularity  and convenience  141 the processes executing in the operating system may be either independent processes or cooperating processes cooperating processes require an interprocess communication mechanisnc to commlmicate with each other principally  communication is achieved through two schemes  shared mernory and message passing the shared-memory method requires communicating processes to share some variables the processes are expected to exchange information through the use of these shared variables in a shared-memory system  the responsibility for providing communication rests with the application programmers ; the operating system needs to provide only the shared memory  the message-passing method allows the processes to exchange messages  the responsibility for providing communication may rest with the operating system itself these two schemes are not mutually exclusive and can be used simultaneously within a single operating system  communication in client-server systems may use  1  sockets   2  remote procedure calls  rpcs   or  3  pipes a socket is defined as an endpoint for communication a connection between a pair of applications consists of a pair of sockets  one at each end of the communication chamcel rpcs are another form of distributed commlmication an rpc occurs when a process  or thread  calls a procedure on a remote application ordinary pipes allow communication between parent and child processes  while named pipes permit unrelated processes to communicate with one another  3.1 what are the benefits and the disadvantages of each of the following consider both the system level and the programmer level  a synchronous and asynchronous commmucation b automatic and explicit buffering c send by copy and send by reference d fixed-sized and variable-sized messages 3.2 consider the rpc mechanism describe the undesirable consequences that could arise from not enforcing either the at most once or exactly once semantic describe possible uses for a mechanism that has neither of these guarantees  3.3 with respect to the rpc mechanism  consider the exactly once semantic  does the algorithm for implementing this semantic execute correctly even if the ack message back to the client is lost due to a network problem describe the sequence of messages and discuss whether exactly once is still preserved  3.4 palm os provides no means of concurrent processing discuss three major complications that concurrent processing adds to an operating system  142 chapter 3 3.5 describe the actions taken by a kernel to context-switch between processes  3.6 the sun ultrasparc processor has multiple register sets describe what happens when a context switch occurs if the new context is already loaded into one of the register sets what happens if the new context is in memory rather than in a register set and all the register sets are in use 3.7 construct a process tree similar to figure 3.9 to obtain process information for the unix or linux system  use the command ps -ael use the command man ps to get more information about the ps command on windows systems  you will have to use the task manager  3.8 give an example of a situation in which ordinary pipes are more suitable than named pipes and an example of a situation in which named pipes are more suitable than ordinary pipes  3.9 describe the differences among short-term  medium-term  and longterm scheduling  3.10 including the initial parent process  how many processes are created by the program shown in figure 3.28 3.11 using the program in figure 3.29  identify the values of pid at lines a  b  c  and d  assume that the actual pids of the parent and child are 2600 and 2603  respectively  # include stdio.h # include unistd.h int main     i fork a child process i fork   ; i fork another child process i fork   ; i and fork another i fork   ; return 0 ; figure 3.28 how many processes are created # include sysltypes.h # include stdio h # include unistd.h int main    pid_t pid ' pid1 ;  i fork a child process i pid = fork   ; if  pid 0   i error occurred i fprintf  stderr  fork failed  ; return 1 ;  else if  pid = = 0   i child process i pid1 = getpid   ;  printf  child  pid = % d ,pid  ; i a i printf  child  pid1 = % d ,pid1  ; i b i else  i parent process i pid1 = getpid   ;  printf  parent  pid = % d ,pid  ; i c i printf  parent  pid1 = % d ,pid1  ; i d i wait  null  ; return 0 ; figure 3.29 what are the pid values 143 3.12 using the program shown in figure 3.30  explain what the output will be at line a  3.13 the fibonacci sequence is the series of numbers 0  1  1  2  3  5  8    formally  it can be expressed as  fib 0 = 0 fibl = 1 jibn = jibn-l + jibn-2 write a c program using the fork   system call that generates the fibonacci sequence in the child process the number of the sequence will be provided in the comm_and line for example  if 5 is provided  the first five numbers in the fibonacci sequence will be output by the child 144 chapter 3 # include sysltypes.h # include stdio.h # include unistd.h int value = 5 ; int main    pid_t pid ;  pid = fork   ; if  pid = = 0   i child process i value + = 15 ; return 0 ;  else if  pid 0   i parent process i wait  null  ;  printf  parent  value = % d ,value  ; i line a i return 0 ; figure 3.30 what output will be at line a process because the parent and child processes have their own copies of the dataf it will be necessary for the child to output the sequence  have the parent invoke the wait   call to wait for the child process to complete before exiting the program perform necessary error checking to ensure that a non-negative number is passed on the command line  3.14 repeat the preceding exercisef this time using the createprocess   function in the win32 api in this instancef you will need to specify a separate program to be invoked from createprocess    it is this separate program that will run as a child process outputting the fibonacci sequence perform necessary error checking to ensure that a non-negative number is passed on the command line  3.15 modify the date server shown in figure 3.19 so that it delivers random jokes rather than the current date allow the jokes to contain multiple lines the date client shown in figure 3.20 can be used to read the multi-line jokes returned by the joke server  3.16 an echo server echoes back whatever it receives from a client for examplef if a client sends the server the string hello there ! the server will respond with the exact data it received from the client-that isf hello there ! 145 write an echo server using the java networking api described in section 3.6.1 this server will wait for a client connection using the accept   method when a client connection is received  the server will loop  perfonning the following steps  read data from the socket into a buffer  write the contents of the buffer back to the client  the server will break out of the loop only when it has determined that the client has closed the connection  the server shown in figure 3.19 uses the java io bufferedreader class bufferedreader extends the java io reader class  which is used for reading character streams however  the echo server can not guarantee that it will read characters from clients ; it may receive binary data as well the class java io input stream deals with data at the byte level rather than the character level thus  this echo server must use an object that extends java io inputstrearn the read   method in the java io inputstrearn class returns -1 when the client has closed its end of the socket connection  3.17 in exercise 3.13  the child process must output the fibonacci sequence  since the parent and child have their own copies of the data another approach to designing this program is to establish a shared-memory segment between the parent and child processes this technique allows the child to write the contents of the fibonacci sequence to the sharedmemory segment and has the parent output the sequence when the child completes because the memory is shared  any changes the child makes will be reflected in the parent process as well  this program will be structured using posix shared memory as described in section 3.5.1 the program first requires creating the data structure for the shared-memory segment this is most easily accomplished using a struct this data structure will contain two items   1  a fixed-sized array of size malsequence that will hold the fibonacci values and  2  the size of the sequence the child process is to generatesequence_ size  where sequence_size     malsequence these items can be represented in a struct as follows  # define max_sequence 10 typedef struct  long fib_sequence  max_sequence  ; int sequence_size ;  shared_data ; the parent process will progress thmugh the following steps  a accept the parameter passed on the command line and perform error checking to ensure that the parameter is     max_sequence  b create a shared-memory segment of size shared_data  c attach the shared-memory segment to its address space  146 chapter 3 d set the value of sequence_size to the parameter on the command line  e fork the child process and invoke the wait   systen1 call to wait for the child to finish  f output the value of the fibonacci sequence in the shared-memory segment  g detach and remove the shared-memory segment  because the child process is a copy of the parent  the shared-memory region will be attached to the child 's address space as well as the parent 's the child process will then write the fibonacci sequence to shared memory and finally will detach the segment  one issue of concern with cooperating processes involves synchronization issues in this exercise  the parent and child processes must be synchronized so that the parent does not output the fibonacci sequence until the child finishes generating the sequence these two processes will be synchronized using the wait   system call ; the parent process will invoke wait    which will cause it to be suspended until the child process exits  3.18 design a program using ordinary pipes in which one process sends a string message to a second process  and the second process reverses the case of each character in the message and sends it back to the first process for example  if the first process sends the message hi there  the second process will return hi there this will require using two pipes  one for sending the original message from the first to the second process  and the other for sending the modified message from the second back to the first process you may write this program using either unix or windows pipes  3.19 design a file-copying program named filecopy using ordinary pipes  this program will be passed two parameters  the first is the name of the file to be copied  and the second is the name of the copied file the program will then create an ordinary pipe and write the contents of the file to be copied to the pipe the child process will read this file from the pipe and write it to the destination file for example  if we invoke the program as follows  filecopy input.txt copy.txt the file input txt will be written to the pipe the child process will read the contents of this file and write it to the destination file copy txt  you may write this program using either unix or windows pipes  3.20 most unix and linux systems provide the ipcs command this command lists the status of various posix interprocess communication mechanisms  including shared-memory segments much of the information for the command comes from the data structure struct shmid_ds  147 which is available in the /usr/include/sys/shm.h file some of the fields in this structure include  int shm_segsz-size of the shared-memory segment short shm__nattch-number of attaches to the shared-memory segment struct ipc_perm shm_perm-permission structure of the sharedmemory segment the struct ipc_perm data structure  which is available in the file /usr/include/sys/ipc .h  contains the fields  unsigned short uid -identifier of the user of the shared -memory segment unsigned short mode-permission modes key_t key  on linux systems  __ key  -user-specified key identifier the permission modes are set according to how the shared-memory segment is established with the shmget   system call permissions are identified according to the following  write permission of owner  0040 read permission of group  0020 write permission of group  0004 read permission of world  0002 write permissionof world  permissions can be accessed by using the bitwise and operator &  for example  if the statement mode & 0400 evaluates to true  the permission mode gives read permission to the owner of the sharedmemory segment  a shared-memory segment can be identified according to a userspecified key or according to the integer value returned from the shmget   system call  which represents the integer identifier of the shared-memory segment created the shm_ds structure for a given integer segment identifier can be obtained with the following shmctl   system call  i identifier of the shared memory segment / int segment_id ; shm_ds shmbuffer ; shmctl  segment_id  ipc_stat  &shmbuffer  ; 148 chapter 3 if successful  shmctl   returns 0 ; otherwise  it returns -1 indicating an error condition  the global variable errno can be accessed to determine the error condition   write a c program that is passed an identifier for a shared-memory segment this program will invoke the shmctl   function to obtain its shm_ds structure it will then output the following values of the shared-memory segment  segmentid key mode owner did size number of attaches 3.21 posix message passing  this project consists of using posix message queues for communicating temperatures between each of four external processes and a central process the project can be completed on systems that support posix message passing  such as unix  linux  and mac os x  part 1  overview four external processes will communicate temperatures to a central process  which in turn will reply with its own temperature and will indicate whether the entire system has stabilized each process will receive its initial temperature upon creation and will recalculate a new temperature according to two formulas  new external temp =  mytemp 3 + 2 centraltemp  i 5 ; new central temp =  2 centraltemp + four temps received from external processes  i 6 ; initially  each external process will send its temperature to the mailbox for the central process if all four temperatures are exactly the same as those sent by the four processes during the last iteration  the system has stabilized in this case  the central process will notify each external process that it is now finished  along with the central process itself   and each process will output the final stabilized temperature if the system has not yet become stable  the central process will send its new temperature to the mailbox for each of the outer processes and await their replies the processes will continue to run until the temperature has stabilized  149 part 2  the message passing system processes can exchange messages by using four system calls  msgget    msgsnd    msgrcv    and msgctl    the msgget   function converts a mailbox name to a message queue id  msqid  a mailbox name is an externally known message queue name that is shared among the cooperating processes  msqid  the internal identifier returned by msgget    must be passed to all subsequent system calls using this message queue to facilitate interprocess communication a typical invocation of msgget   is seen below  msqid = msgget  1234  0600 i ipc_creat  ; the first parameter is the name of the mailbox  and the second parameter instructs the operating system to create the message queue if it does not already exist  with read and write privileges only for processes with the same user id as this process if a message queue already exists for this mailbox name  msgget   returns the msqid of the existing mailbox to avoid attaching to an existing message queue  a process can first attempt to attach to the mailbox by omitting ipc_creat and then checking the return value from msgget    if msq id is negative  an error has occurred during the system calt and the globally accessible variable errno can be consulted to determine whether the error occurred because the message queue already exists or for some other reason if the process determines that the mailbox does not currently exist it can then create it by including ipc_creat  for the current project  this strategy should not be necessary if students are using standalone pcs or are assigned unique ranges of mailbox names by the instructor  once a valid msqid has been established  a process can begin to use msgsnd   to send messages and msgrcv   to receive messages  the messages being sent and received are similar in format to those described in section 3.5.2  as they include a fixed-length portion at the beginning followed by a variable-length portion obviously  the senders and receivers must agree on the format of the messages being exchanged  since the operating system specifies one field in the fixed-length portion of every message format and at least one piece of information will be sent to the receiving process  it is logical to create a data aggregate for each type of message using a struct the first field of any such struct must be a long  and it will contain the priority of the message   this project does not use this functionality ; we recommend that you simply make the first field in every message equal to the same integral value  such as 2  other fields in the messages contain the information to be shared between the communicating processes three additional fields are recommended   1  the temperature being sent   2  the process number of the external process sending the message  0 for the central process   and  3  a flag that is set to 0 but that the central process will set to 1 when it notices stability a recommended struct appears as follows  150 chapter 3 struct  long priority ; int temp ; int pid ; int stable ;  msgp ; assuming the msqid has been established  examples of msgsnd   and msgrcv   appear as such  int stat  msqid ; stat = msgsnd  msqid  &msgp  sizeof  msgp  -sizeof  long   0  ; stat msgrcv  msqid  &msgp  sizeof  msgp  -sizeof  long   2  0  ; the first parameter in both system calls must be a valid msq id ; otherwise a negative value is returned  both functions return the number of bytes sent or received upon successful completion of the operation  the second parameter is the address of where to find or store the message to be sent or received  followed by the number of information bytes to be sent or received the final parameter of 0 indicates that the operations will be synchronous and that the sender will block if the message queue is full  ipc_nowait would be used if asynchronous  or nonblocking  operations were desired each individual message queue can hold a maximum number of messages-or bytes-so it is possible for the queue to become filled  which is one reason a sender may block when attempting to transmit a message  the 2 that appears before this final parameter in msgrcv   indicates the minimum priority level of the messages the process wishes to receive ; the receiver will wait until a message of that priority  or higher  is sent to the msqid if this is a synchronous operation  once a process is finished using a message queue  it must be removed so that the mailbox can be reused by other processes unless it is removed  the message queue-and any messages that have not yet been received-will remain in the storage space that has been provided for this mailbox by the kernel to remove the message queue  and delete any unread messages therein  it is necessary to invoke msgctl    as follows  struct msgid_ds dummyparam ; status = msgctl  msqid  ipc_rmid  &dummyparam  ; the third parameter is necessary because this function requires it but it is used only if it the programmer wishes to collect some statistics about the usage of the message queue this is accomplished by substituting ipc_stat as the second parameter  all programs should include the following three header files  which are found in /usr/include/sys  ipc.h  types.h  and msg.h one possibly confusing artifact of the message queue implementation bears 151 mentioning at this point after a mailbox is removed via msgctl   ,any subsequent attempts to create another mailbox with that same name using msgget   will typically generate a different msqid  part 3  creating the processes each external process  as well as the central server  will create its own mailbox with the name x + i  where i is a numeric identifier of the external process 1..4 or zero for the central process thus  if x were 70  then the central process would receive messages in the mailbox named 70  and it would send its replies to mailboxes 71-74 outer process 2 would receive in mailbox 72 and would send to mailbox 70  and so forth  thus  each external process will attach to two mailboxes  and the central process will attach to five if each process specifies ipc_creat when invoking msgget    the first process that invokes msgget   actually creates the mailbox ; subsequent calls to msgget   attach to the existing mailbox the protocol for removal should be that the mailbox/message queue that each process is listening to should be the only one it removes -via msgctl     each external process will be uniquely identified by a command-line parameter the first parameter to each external process will be its initial temperature  and the second parameter will be its unique number  1  2  3  or 4 the central server will be passed one parameter-its initial temperature assuming the executable name of the external process is external and the central server is central  invoking all five processes will be done as follows  ./external 100 1 & ./external 22 2 & ./external 50 3 & ./external 40 4 & ./central 60 & part 4  implementation hints it might be best to start by sending one message successfully from the central process to a single outer process  and vice versa  before trying to write all the code to solve this problem it is also wise to check all the return values from the four message queue system calls for possible failed requests and to output a message to the screen after each one successfully completes the message should indicate what was accomplished and by whom -for instance  mailbox 71 has been created by outer process 1/ ' message received by central process from external process 2/ ' and so forth these messages can be removed or commented out after the problem is solved processes should also verify that they were passed the correct number of command-line parameters  via the argc parameter in main     finally  extraneous messages residing in a queue can cause a collection of cooperating processes that function correctly to appear erroneous for that reason  it is wise to remove all mailboxes relevant to this project to ensure that mailboxes are empty before the processes begin the easiest way to do this is to use the 152 chapter 3 ipcs command to list all message queues and the ipcrm command to remove existing message queues the ipcs command lists the msqid of all message queues on the system use ipcrm to remove message queues according to their msqid for example  if msqid 163845 appears with the output of ipcs  it can be deleted with the following command  ipcrm -q 163845 interprocess communication in the rc 4000 system is discussed by brinchhansen  1970   schlichting and schneider  1982  discuss asynchronous message-passing prirnitives the ipc facility implemented at the user level is described by bershad et al  1990   details of interprocess communication in unix systems are presented by gray  1997   barrera  1991  and vahalia  1996  describe interprocess communication in the mach system russinovich and solomon  2005   solomon and russinovich  2000   and stevens  1999  outline interprocess communication in windows 2003  windows 2000 and unix respectively hart  2005  covers windows systems programming in detail  the implementation of rpcs is discussed by birrell and nelson  1984   shrivastava and panzieri  1982  describes the design of a reliable rpc mechanism  and tay and ananda  1990  presents a survey of rpcs stankovic  1982  and stmmstrup  1982  discuss procedure calls versus message-passing communication  harold  2005  provides coverage of socket programming in java  hart  2005  and robbins and robbins  2003  cover pipes in windows and unix systems  respectively  4.1 chapter the process model introduced in chapter 3 assumed that a process was an executing program with a single thread of control most modern operating systems now provide features enabling a process to contain multiple threads of control this chapter introduces many concepts associated with multithreaded computer systems  including a discussion of the apis for the pthreads  win32  and java thread libraries we look at many issues related to multithreaded programming and its effect on the design of operating systems finally  we explore how the windows xp and linux operating systems support threads at the kernel level  to introduce the notion of a thread a fundamental unit of cpu utilization that forms the basis of multithreaded computer systems  to discuss the apis for the pthreads  win32  and java thread libraries  to examine issues related to multithreaded programming  a thread is a basic unit of cpu utilization ; it comprises a thread id  a program counter  a register set  and a stack it shares with other threads belonging to the same process its code section  data section  and other operating-system resources  such as open files and signals a traditional  or heavrvveighl   process has a single thread of control if a process has multiple threads of control  it can perform more than one task at a time figure 4.1 illustrates the difference between a traditional process and a process  4.1.1 motivation many software packages that run on modern desktop pcs are multithreaded  an application typically is implemented as a separate process with several threads of control a web browser might have one thread display images or 153 154 chapter 4 thread + single-threaded process multithreaded process figure 4.1 single-threaded and multithreaded processes  text while another thread retrieves data from the network  for example a word processor may have a thread for displaying graphics  another thread for responding to keystrokes from the user  and a third thread for performing spelling and grammar checking in the background  in certain situations  a single application may be required to perform several similar tasks for example  a web server accepts client requests for web pages  images  sound  and so forth a busy web server may have several  perhaps thousands of  clients concurrently accessing it if the web server ran as a traditional single-tlu eaded process  it would be able to service only one client at a time  artd a client might have to wait a very long time for its request to be serviced  one solution is to have the server run as a single process that accepts requests when the server receives a request  it creates a separate process to service that request in fact  this process-creation method was in common use before threads became popular process creation is time consuming and resource intensive  however if the new process will perform the same tasks as the existing process  why incur all that overhead it is generally more efficient to use one process that contains multiple threads if the web-server process is multithreaded  the server will create a separate thread that listens for client requests when a request is made  rather than creating another process  the server will create a new thread to service the request and resume listening for additional requests this is illustrated in figure 4.2  threads also play a vital role in remote procedure call  rpc  systems recall from chapter 3 that rpcs allow interprocess communication by providing a communication mechanism similar to ordinary function or procedure calls  typically  rpc servers are multithreaded when a server receives a message  it services the message using a separate thread this allows the server to service several concurrent requests  finally  most operating system kernels are now multithreaded ; several threads operate in the kernel  and each thread performs a specific task  such client  1  request 4.1  2  create new thread to service the request 1 1 thread '------.--r---10 server  3  resume listening for additional client requests figure 4.2 multithreaded server architecture  155 as managing devices or interrupt handling for examplef solaris creates a set of threads in the kernel specifically for interrupt handling ; linux uses a kernel thread for managing the amount of free memory in the system  4.1.2 benefits the benefits of multithreaded programming can be broken down into four major categories  responsiveness multithreading an interactive application may allow a program to continue running even if part of it is blocked or is performing a lengthy operation  thereby increasing responsiveness to the user for instancef a multithreaded web browser could allow user interaction in one thread while an image was being loaded in another thread  resource sharing processes may only share resources through techniques such as shared memory or message passing such techniques must be explicitly arranged by the programmer however  threads share the memory and the resources of the process to which they belong by default  the benefit of sharing code and data is that it allows an application to have several different threads of activity within the same address space  3 economy allocating memory and resources for process creation is costly  because threads share the resources of the process to which they belong  it is more economical to create and context-switch threads empirically gauging the difference in overhead can be difficult  but in general it is much more time consuming to create and manage processes than threads  in solarisf for example  creating a process is about thirty times slower than is creating a thread  and context switching is about five times slower  scalability the benefits of multithreading can be greatly increased in a multiprocessor architecture  where threads may be running in parallel on different processors a single-threaded process can only run on one processor  regardless how many are available multithreading on a multicpu machine increases parallelism we explore this issue further in the following section  156 chapter 4 time figure 4.3 concurrent execution on a single-core system  4.1.3 multicore programming a recent trend in system design has been to place multiple computing cores on a single chip  where each core appears as a separate processor to the operating system  section 1.3.2   multithreaded programming provides a mechanism for more efficient use of multiple cores and improved concurrency consider an application with four threads on a system with a single computing core  concurrency merely means that the execution of the threads will be interleaved over time  figure 4.3   as the processing core is capable of executing only one thread at a time on a system with multiple cores  however  concurrency means that the threads can run in parallel  as the system can assign a separate thread to each core  figure 4.4   the trend towards multicore systems has placed pressure on system designers as well as application programmers to make better use of the multiple computing cores designers of operating systems must write scheduling algorithms that use multiple processing cores to allow the parallel execution shown in figure 4.4 for application programmers  the challenge is to modify existing programs as well as design new programs that are multithreaded to take advantage of multicore systems in general  five areas present challenges in programming for multicore systems  dividing activities this involves examining applications to find areas that can be divided into separate  concurrent tasks and thus can run in parallel on individual cores  balance while identifying tasks that can run in parallel  programmers must also ensure that the tasks perform equal work of equal value in some instances  a certain task may not contribute as much value to the overall process as other tasks ; using a separate execution core to run that task may not be worth the cost  data splitting just as applications are divided into separate tasks  the data accessed and manipulated by the tasks must be divided to run on separate cores  core 1 l t1 i t3 t1 t3 i ti core 2  i  t4 t2 t4 i tz time figure 4.4 parallel execution on a multicore system  4.2 4.2 157 data dependency the data accessed by the tasks must be examined for dependencies between two or more tasks in instances where one task depends on data from another  programmers must ensure that the execution of the tasks is synchronized to accommodate the data dependency we examine such strategies in chapter 6  testing and debugging when a program is running in parallel on multiple cores  there are many different execution paths testing and debugging such concurrent programs is inherently more difficult than testing and debugging single-threaded applications  because of these challenges  many software developers argue that the advent of multicore systems will require an entirely new approach to designing software systems in the future  our discussion so far has treated threads in a generic sense however  support for threads may be provided either at the user level  for or by the kernel  for threads user threads are supported above the kernel and are managed without kernel support  whereas kernel threads are supported and managed directly by the operating system virtually all contemporary operating systems-including wiridows xp  linux  mac os x  solaris  and tru64 unix  formerly digital unix  -support kernel threads  ultimately  a relationship must exist between user threads and kernel threads in this section  we look at three common ways of establishing such a relationship  4.2.1 many-to-one model the many-to-one model  figure 4.5  maps many user-level threads to one kernel thread thread management is done by the thread library in user figure 4.5 many-to-one model  158 chapter 4  user thread figure 4.6 one-to-one model  space  so it is efficient ; but the entire process will block if a thread makes a blocking system call also  because only one thread can access the kernel at a time  multiple threads are unable to nm in parallel on multiprocessors  -a thread library available for solaris-uses this modet as does gnu 4.2.2 one-to-one model the one-to-one model  figure 4.6  maps each user thread to a kernel thread it provides more concurrency than the many-to-one model by allowing another thread to run when a thread makes a blocking system call ; it also allows multiple threads to run in parallel on multiprocessors the only drawback to this model is that creating a user thread requires creating the corresponding kernel thread because the overhead of creating kernel threads can burden the performance of an application  most implementations of this model restrict the number of threads supported by the system linux  along with the family of windows operating systems  implement the one-to-one model  4.2.3 many-to-many model the many-to-many model  figure 4.7  multiplexes many user-level threads to a smaller or equal number of kernel threads the number of kernel threads may be specific to either a particular application or a particular machine  an application may be allocated more kernel threads on a multiprocessor than on a uniprocessor   whereas the many-to-one model allows the developer to user thread k +  kernel thread figure 4.7 many-to-many model  4.3 4.3 159 2      i ~   / ' / '  '  ......._ user thread    0 -kernel thread figure 4.8 two-level model  create as many user threads as she wishes  true concurrency is not gained because the kernel can schedule only one thread at a time the one-to-one model allows for greater concurrency  but the developer has to be careful not to create too many threads within an application  and in some instances may be limited in the number of threads she can create   the many-to-many model suffers from neither of these shortcomings  developers can create as many user threads as necessary  and the corresponding kernel threads can run in parallel on a multiprocessor also  when a thread performs a blocking system call  the kernel can schedule another thread for execution  one popular variation on the many-to-many model still multiplexes many user-level threads to a smaller or equal number of kernel threads but also allows a user-level thread to be bound to a kernel thread this variation  sometimes referred to as the two-level model  figure 4.8   is supported by operating systems such as irlx  hp-ux  and tru64 unix the solaris operating system supported the two-level model in versions older than solaris 9 however  beginning with solaris 9  this system uses the one-to-one model  a provides the programmer with an api for creating and managing threads there are two primary ways of implementii g a thread library the first approach is to provide a library entirely in user space with no kernel support all code and data structures for the library exist ii user space  this means that invoking a function in the library results in a local function call in user space and not a system call  the second approach is to implement a kernel-level library supported directly by the operating system in this case  code and data structures for the library exist in kernel space invoking a function in the api for the library typically results in a system call to the kernel  three main thread libraries are in use today   1  posix pthreads   2  win32  and  3  java pthreads  the threads extension of the posix standard  may be provided as either a user or kernel-level library the win32 thread library is a kernel-level library available on windows systems the java thread api allows threads to be created and managed directly in java programs however  because in most instances the jvm is running on top of a host operating system  160 chapter 4 the java thread api is generally implemented using a thread library available on the host system this means that on windows systems  java threads are typically implemented using the win32 api ; unix and linux systems often use pthreads  in the remainder of this section  we describe basic thread creation using these three thread libraries as an illustrative example  we design a multithreaded program that performs the summation of a non-negative integer in a separate thread using the well-known summation function  n sum = i ~ i = o for example  if n were 5  this function would represent the summation of integers from 0 to 5  which is 15 each of the three programs will be n.m with the upper bounds of the summation entered on the command line ; thus  if the user enters 8  the summation of the integer values from 0 to 8 will be output  4.3.1 pthreads refers to the posix standard  ieee 1003.lc  defining an api for thread creation and synchronization this is a specification for thread behavim ~ not an implementation operating system designers may implement the specification in any way they wish numerous systems implement the pthreads specification  including solaris  linux  mac os x  and tru64 unix shareware implementations are available in the public domain for the various windows operating systems as well  the c program shown in figure 4.9 demonstrates the basic pthreads api for constructing a multithreaded program that calculates the summation of a nonnegative integer in a separate thread in a pthreads program  separate threads begin execution in a specified function in figure 4.9  this is the runner   function when this program begins  a single thread of control begins in main    after some initialization  main   creates a second thread that begins control in the runner   function both threads share the global data sum  let 's look more closely at this program all pthreads programs must include the pthread h header file the statement pthread_t tid declares the identifier for the thread we will create each thread has a set of attributes  including stack size and scheduling information the pthread_attr_t attr declaration represents the attributes for the thread we set the attributes in the function call pthread_attr ini t  &attr   because we did not explicitly set any attributes  we use the default attributes provided  in chapter 5  we discuss some of the scheduling attributes provided by the pthreads api  a separate thread is created with the pthread_create   function call in addition to passirtg the thread identifier and the attributes for the thread  we also pass the name of the function where the new thread will begin execution-in this case  the runner   function last  we pass the integer parameter that was provided on the command line  argv  1   at this point  the program has two threads  the initial  or parent  thread in main   and the summation  or child  thread performing the summation 4.3 # include pthread.h # include stdio.h int sum ; i this data is shared by the thread  s  i void runner  void param  ; i the thread i int main  int argc  char argv      pthread_t tid ; i the thread identifier i pthread_attr_t attr ; i set of thread attributes i if  argc ! = 2    fprintf  stderr  usage  a.out integer value \ n  ; return -1 ; if  atoi  argv  1   0    fprintf  stderr  % d must be = 0 \ n ,atoi  argv  1    ; return -1 ; i get the default attributes i pthread_attr_init  &attr  ; i create the thread i pthread_create  &tid,&attr,runner,argv  1   ; i wait for the thread to exit i pthread_join  tid,null  ; printf  sum = % d \ n ,sum  ; i the thread will begin control in this function i void runner  void param    inti  upper = atoi  param  ; sum = 0 ; for  i = 1 ; i = upper ; i + +  sum + = i ; pthread_exi t  0  ; figure 4.9 multithreaded c program using the pthreads api  161 operation in the runner   function after creating the summation threadf the parent thread will wait for it to complete by calling the pthread_j oin   function the summation thread will complete when it calls the function pthread_exi t    once the summation thread has returnedf the parent thread will output the value of the shared data sum  162 chapter 4 4.3.2 win32 threads the technique for creating threads using the win32 thread library is similar to the pthreads technique in several ways we illustrate the win32 thread api in the c program shown in figure 4.10 notice that we must include the windows  h header file when using the win32 api  just as in the pthreads version shown in figure 4.9  data shared by the separate threads-in this case  sum-are declared globally  the dword data type is an unsigned 32-bit integer   we also define the summation   function that is to be performed in a separate thread this function is passed a pointer to a void  which win32 defines as lpvoid the thread performing this function sets the global data sum to the value of the summation from 0 to the parameter passed to summation    threads are created in the win32 api using the createthread   function  and-just as in pthreads-a set of attributes for the thread is passed to this function these attributes il1.clude security information  the size of the stack  and a flag that can be set to indicate if the thread is to start in a suspended state in this program  we use the default values for these attributes  which do not initially set the thread to a suspended state and instead make it eligible to be rm1 by the cpu scheduler   once the summation thread is created  the parent must wait for it to complete before outputting the value of sum  as the value is set by the summation thread recall that the pthread program  figure 4.9  had the parent thread wait for the summation thread using the pthread_j oin   statement we perform the equivalent of this in the win32 api using the wai tforsingleobj ect   function  which causes the creatil1.gthread to block until the summation thread has exited  we cover synchronization objects in more detail in chapter 6  4.3.3 java threads tlu eads are the fundamental model of program execution in a java program  and the java language and its api provide a rich set of features for the creation and management of threads all java programs comprise at least a single thread of control-even a simple java program consisting of only a main   method runs as a single thread in the jvm  there are two teclmiques for creating threads in a java program one approach is to create a new class that is derived from the thread class and to override its run   method an alternative-and more commonly usedteclmique is to define a class that implements the runnable interface the runnable interface is defined as follows  public interface runnable  public abstract void run   ; when a class implements runnable  it must define a run   method the code implementing the run   method is what runs as a separate thread  figure 4.11 shows the java version of a multithreaded program that determines the summation of a non-negative integer the summation class implements the runnable interface thread creation is performed by creating 4.3 # include windows.h # include stdio.h dword sum ; i data is shared by the thread  s  i i the thread runs in this separate function i dword winapi sumrnation  lpvoid param    dword upper =  dword  param ; for  dword i = 0 ; i = upper ; i + +  sum + = i ; return 0 ; int main  int argc  char argv      dword threadid ; handle threadhandle ; int param ; i perform some basic error checking i if  argc ! = 2    fprintf  stderr  an integer parameter is required \ n  ; return -1 ; param = atoi  argv  1   ; if  param 0    fprintf  stderr  an integer = 0 is required \ n  ; return -1 ; ii create the thread threadhandle = createthread  null  ii default security attributes 0  ii default stack size summation  ii thread function &param  ii parameter to thread function 0  ii default creation flags &threadid  ; ii returns the thread identifier if  threadhandle ! = null    ii now wait for the thread to finish waitforsingleobject  threadhandle,infinite  ; ii close the thread handle closehandle  threadhandle  ; printf  surn = % d \ n ,sum  ; figure 4.10 multithreaded c program using the win32 api  163 164 chapter 4 class sum   private int sum ; public int getsum    return sum ;  public void setsum  int sum   this.sum sum ;  class summation implements runnable   private int upper ; private sum sumvalue ; public summation  int upper  sum sumvalue   this.upper = upper ; this.sumvalue = sumvalue ;  public void run    int sum = 0 ;  for  int i = 0 ; i = upper ; i + +  sum + = i ; sumvalue.setsum  sum  ; public class driver   public static void main  string   args   if  args.length 0    if  integer.parseint  args  o   0  system.err.println  args  o  + must be = 0  ; else  ii create the object to be shared sum sumobject = new sum   ; int upper = integer.parseint  args  o   ; thread thrd = new thread  new summation  upper  sumobject   ; thrd.start   ; try  thrd join   ; system.out.println  the sum of + upper + is + sumobject.getsum    ;  catch  interruptedexception ie     else system.err.println  usage  summation integer value  ;  figure 4.11 java program for the summation of a non-negative integer  4.4 4.4 165 an object instance of the thread class and passing the constructor a runnable object  creating a thread object does not specifically create the new thread ; rather  it is the start   method that creates the new thread calling the start   method for the new object does two things  it allocates memory and initializes a new thread in the jvm  it calls the run   method  making the thread eligible to be run by the jvm  note that we never call the run   method directly rathel ~ we call the start   method  and it calls the run   method on our behalf  when the summation program runs  two threads are created by the jvm  the first is the parent thread  which starts execution in the main   method  the second thread is created when the start   method on the thread object is invoked this child thread begins execution in the run   method of the summation class after outputting the value of the summation  this thread terminates when it exits from its run   method  sharing of data between threads occurs easily in win32 and pthreads  since shared data are simply declared globally as a pure object-oriented language  java has no such notion of global data ; if two or more threads are to share data in a java program  the sharing occurs by passing references to the shared object to the appropriate threads in the java program shown in figure 4.11  the main thread and the summation thread share the object instance of the sum class this shared object is referenced through the appropriate get sum   and setsum   methods  you might wonder why we do n't use an integer object rather than designing a new sum class the reason is that the integer class is immutable-that is  once its value is set  it can not change  recall that the parent threads in the pthreads and win32 libraries use pthread_j oin   and wai tforsingledbj ect    respectively  to wait for the summation threads to finish before proceeding the join   method in java provides similar functionality  notice that join   can throw an interruptedexception  which we choose to ignore  in this section  we discuss some of the issues to consider with multithreaded programs  4.4.1 the fork   and exec   system calls in chapter 3  we described how the fork   system call is used to create a separate  duplicate process the semantics of the fork   and exec   system calls change in a multithreaded program  if one thread in a program calls fork    does the new process duplicate all threads  or is the new process single-threaded some unix systems have chosen to have two versions of fork    one that duplicates all threads and another that duplicates only the thread that invoked the fork   system call  the exec   system call typically works in the same way as described in chapter 3 that is  if a thread invokes the exec   system call  the program 166 chapter 4 the jvm and the host operating system the jvm is typically implemented on top of a host operating system  see figure 2.20   this setup allows the jvm to hide the implementation details of the underlying operating system and to provide a consistent  abstract environment that allows java programs to operate on any platform that supports a jvm the specification for the jvm does not indicate how java threads are to be mapped to the underlying operating system  instead leaving that decision to the particular implementation of the jvm for example  the windows xp operating system uses the one-to-one model ; therefore  each java thread for a jvm running on such a system maps to .a kernel thread  on operating systems that use the many-to-many model  such as tru64 unix   a java thread is mapped according to the many-to-manymodel solaris initially implemented the jvm using themany ~ to-one model  the greenthreads library  mentioned earlier   later releases of the jvm were implementedusing the many-to  inany model beginning with solaris 9  java threads were mapped using the one ~ to-one model in addition  there may be a relationship between the java thread library and the thread library on the host operating system  for .example  implementations of a jvm for the windows family of operating systems might use the win32 api when creating java threads ; linux  solaris  and mac os x systems might use the pthreads api  specified in the parameter to exec   will replace the entire process-including all threads  which of the two versions of fork   to use depends on the application  if exec   is called immediately after forking  then duplicating all threads is unnecessary  as the program specified in the parameters to exec   will replace the process in this instance  duplicating only the calling thread is appropriate  if  however  the separate process does not call exec   after forking  the separate process should duplicate all threads  4.4.2 cancellation '''-'c ' ' ' ' is the task of terminating a thread before it has completed  for example  if multiple threads are concurrently searching through a database and one thread returns the result  the remaining threads might be canceled  another situation might occur when a user presses a button on a web browser that stops a web page from loading any further often  a web page is loaded using several threads-each image is loaded in a separate thread when a user presses the stop button on the browser  all threads loading the page are canceled  a thread that is to be canceled is often referred to as the cancellation of a target thread may occur in two different scenarios  asynchronous cancellation one thread immediately terminates the target thread  4.4 167 deferred cancellation the target thread periodically checks whether it should terminate  allowing it an opportunity to terminate itself in an orderly fashion  the difficulty with cancellation occurs in situations where resources have been allocated to a canceled thread or where a thread is canceled while in the midst of updating data it is sharing with other threads this becomes especially troublesome with asynchronous cancellation often  the operating system will reclaim system resources from a canceled thread but will not reclaim all resources therefore  canceling a thread asynchronously may not free a necessary system-wide resource  with deferred cancellation  in contrast  one thread indicates that a target thread is to be canceled  but cancellation occurs only after the target thread has checked a flag to determine whether or not it should be canceled the thread can perform this check at a at which it can be canceled safely pthreads refers to such points as 4.4.3 signal handling a is used in unix systems to notify a process that a particular event has occurred a signal may be received either synchronously or asynchronously  depending on the source of and the reason for the event being signaled all signals  whether synchronous or asynchronous  follow the same pattern  a signal is generated by the occurrence of a particular event  a generated signal is delivered to a process  once delivered  the signal must be handled  examples of synchronous signals include illegal memory access and division by 0 if a running program performs either of these actions  a signal is generated synchronous signals are delivered to the same process that performed the operation that caused the signal  that is the reason they are considered synchronous   when a signal is generated by an event external to a running process  that process receives the signal asynchronously examples of such signals include terminating a process with specific keystrokes  such as control c  and having a timer expire typically  an asynchronous signal is sent to another process  a signal may be handled by one of two possible handlers  a default signal handler a user-defilced signal handler every signal has a that is run by the kernel when handling that signal this default action can be overridden by a signal handle ~ that is called to handle the signal signals are handled in different ways some signals  such as changing the size of a window  are simply ignored ; others  such as an illegal memory access  are handled by terminating the program  168 chapter 4 handling signals in single-threaded programs is straightforward  signals are always delivered to a process however  delivering signals is more complicated in multithreaded programs  where a process may have several threads where  then  should a signal be delivered in generat the following options exist  deliver the signal to the thread to which the signal applies  deliver the signal to every thread in the process  deliver the signal to certain threads in the process  assign a specific thread to receive all signals for the process  the method for delivering a signal depends on the type of signal generated  for example  synchronous signals need to be delivered to the thread causing the signal and not to other threads in the process however  the situation with asynchronous signals is not as clear some asynchronous signals-such as a signal that terminates a process  control c  for example  -should be sent to all threads  most multithreaded versions of unix allow a thread to specify which signals it will accept and which it will block therefore  in some cases  an asynchronous signal may be delivered only to those threads that are not blocking it however  because signals need to be handled only once  a signal is typically delivered only to the first thread found that is not blocking it  the standard unix function for delivering a signal is kill  pid_t pid  int signal   which specifies the process  pi d  to which a particular signal is to be delivered posix pthreads provides the pthread_kill  pthread_t tid  int signal  function  which allows a signal to be delivered to a specified thread  tid   although windows does not explicitly support for signals  they can be emulated using  apcs   the apc facility allows a user thread to specify a function that is to be called when the user thread receives notification of a particular event as indicated by its name  an apc is roughly equivalent to an asynchronous signal in unix however  whereas unix must contend with how to deal with signals in a multithreaded environment  the apc facility is more straightforward  since an apc is delivered to a particular thread rather than a process  4.4.4 thread pools in section 4.1  we mentioned multithreading in a web server in this situation  whenever the server receives a request  it creates a separate thread to service the request whereas creating a separate thread is certainly superior to creating a separate process  a multithreaded server nonetheless has potential problems  the first issue concerns the amount of time required to create the thread prior to servicing the request  together with the fact that this thread will be discarded once it has completed its work the second issue is more troublesome  if we allow all concurrent requests to be serviced in a new thread  we have not placed a bound on the number of threads concurrently active in the system unlimited threads could exhaust system resources  such as cpu tince or memory one solution to this problem is to use a 4.4 169 the general idea beh_ind a thread pool is to create a number of threads at process startup and place them into a pool  where they sit and wait for work  when a server receives a request  it awakens a thread from this pool-if one is available-and passes it the request for service once the thread completes its service  it returns to the pool and awaits more work if the pool contains no available thread  the server waits until one becomes free  thread pools offer these benefits  servicing a request with an existing thread is usually faster than waiting to create a thread  a thread pool limits the number of threads that exist at any one point  this is particularly important on systems that can not support a large number of concurrent threads  the number of threads in the pool can be set heuristically based on factors such as the number of cpus in the system  the amount of physical memory  and the expected number of concurrent client requests more sophisticated thread-pool architectures can dynamically adjust the number of threads in the pool according to usage patterns such architectures provide the further benefit of having a smaller pool-thereby consuming less memory-when the load on the system is low  the win32 api provides several functions related to thread pools using the thread pool api is similar to creating a thread with the thread create   function  as described in section 4.3.2 here  a function that is to run as a separate thread is defin_ed such a function may appear as follows  dword winapi poolfunction  avoid param  / this function runs as a separate thread  / a pointer to poolfunction   is passed to one of the functions in the thread pool api  and a thread from the pool executes this function one such member in the thread pool api is the queueuserworkitemo function  which is passed three paranceters  lpthread_starlroutine function-a pointer to the function that is to nm as a separate thread pvoid param-the parameter passed to function ulong flags-flags indicating how the thread pool is to create and manage execution of the thread an example of invoking a function is  queueuserworkitem  &poolfunction  null  0  ; this causes a thread from the thread pool to invoke poolfunction   on behalf of the programmer in this instance  we pass no parameters to 170 chapter 4  lightweight process '-----'-' d0 ~ kamalthcead figure 4.12 lightweight process  lwp   poolfunction    because we specify 0 as a flag  we provide the thread pool with no special instructions for thread creation  other members in the win32 thread pool api include utilities that invoke functions at periodic intervals or when an asynchronous i/0 request completes  the java util concurrent package in java 1.5 provides a thread pool utility as well 4.4.5 thread-specific data threads belonging to a process share the data of the process indeed  this sharing of data provides one of the benefits of multithreaded programming  however  in some circumstances  each thread need its own copy of certain data we will call such data for example  in a transaction-processing system  we might service each transaction in a separate thread furthermore  each transaction might be assigned a unique identifier to associate each thread with its unique identifier  we could use thread-specific data most thread libraries-including win32 and pthreads-provide some form of support for thread-specific data java provides support as well  4.4.6 scheduler activations a final issue to be considered with multithreaded programs concerns communication between the kernel and the thread library  which may be required by the many-to-many and two-level models discussed in section 4.2.3 such coordination allows the number of kernel threads to be dynamically adjusted to help ensure the best performance  many systems implementing either the many-to-many or the two-level model place an intermediate data structure between the user and kernel threads this data structure-typically known as a lightweight process  or lwp-is shown in figure 4.12 to the user-thread library  the lwp appears to be a virtual processor on which the application can schedule a user thread to run each lwp is attached to a kernel thread  and it is kernel threads that the operating system schedules to run on physical processors if a kernel thread blocks  such as while waiting for an i/0 operation to complete   the lwp blocks as well up the chain  the user-level thread attached to the lwp also blocks  an application may require any number of lwps to run efficiently consider a cpu-bound application running on a single processor in this scenario  only 4.5 4.5 171 one thread can run at once  so one lwp is sufficient an application that is i/ointensive may require multiple lwps to execute  however typically  an lwp is required for each concurrent blocking system call suppose  for example  that five different file-read requests occur simultaneously five lwps are needed  because all could be waiting for i/0 completion in the kernel if a process has only four lwps  then the fifth request must wait for one of the lwps to return from the kernel  one scheme for communication between the user-thread library and the kernel is known as it works as follows  the kernel provides an application with a set of virtual processors  lwps   and the application can schedule user threads onto an available virtual processor  furthermore  the kernel must inform an application about certain events this procedure is known as an upcalls are handled by the thread library with an and upcall handlers must run on a virtual processor  one event that triggers an upcall occurs when an application thread is about to block in this scenario  the kernel makes an upcall to the application informing it that a thread is about to block and identifying the specific thread the kernel then allocates a new virtual processor to the application the application runs an upcall handler on this new virtual processor  which saves the state of the blocking thread and relinquishes the virtual processor on which the blocking thread is running the upcall handler then schedules another thread that is eligible to run on the new virtual processor when the event that the blocking thread was waiting for occurs  the kernel makes another upcall to the thread library informilcg it that the previously blocked thread is now eligible to run  the up call handler for this event also requires a virtual processor  and the kernel may allocate a new virtual processor or preempt one of the user threads and run the upcall handler on its virtual processor after marking the 1-mblocked thread as eligible to run  the application schedules an eligible thread to run on an available virtual processor  in this section  we explore how threads are implemented in windows xp and linux systems  4.5.1 windows xp threads windows xp implements the win32 api  which is the primary api for the family of microsoft operating systems  windows 95  98  nt  2000  and xp   indeed  much of what is mentioned in this section applies to this entire family of operating systems  a windows xp application runs as a separate process  and each process may contain one or more threads the win32 api for creating threads is covered in section 4.3.2 windows xp uses the one-to-one mapping described in section 4.2.2  where each user-level thread maps to an associated kernel thread however  windows xp also provides support for a library  which provides the functionality of the many-to-many model  section 4.2.3   by using the thread library  any thread belonging to a process can access the address space of the process  172 chapter 4 the general components of a thread include  a thread id uniquely identifying the thread a register set representing the status of the processor a user stack  employed when the thread is running in user mode  and a kernel stack  employed when the thread is running in kernel mode a private storage area used by various run-time libraries and dynamic link libraries  dlls  the register set  stacks  and private storage area are known as the rcc   nw yt of the thread the primary data structures of a thread include  ethread-executive thread block kthread-kernel thread block tee-thread environment block the key components of the ethread include a pointer to the process to which the thread belongs and the address of the routine in which the thread starts control the ethread also contains a pointer to the corresponding kthread  ethread kernel space user space figure 4.13 data structures of a windows xp thread  4.5 173 the kthread includes scheduling and synchronization inforn1.ation for the thread in addition  the kthread includes the kernel stack  used when the thread is running in kernel mode  and a pointer to the teb  the ethread and the kthread exist entirely in kernel space ; this means that only the kernel can access thern the teb is a user-space data structure that is accessed when the thread is running in user mode among other fields  the teb contains the thread identifie1 ~ a user-mode stack  and an array for threadspecific data  which windows xp terms the structure of a windows xp thread is illustrated in figure 4.13  4.5.2 linux threads linux provides the fork   system call with the traditional functionality of duplicating a process  as described in chapter 3 linux also provides the ability to create threads using the clone   system call however  linux does not distinguish between processes and threads in fact  linux generally uses the term task-rather than process or thread-when referring to a flow of control within a program  when clone   is invoked  it is passed a set of flags  which determine how much sharing is to take place between the parent and child tasks some of these flags are listed below  flag meaning clone fs  file-system information is shared  clone vm  the same memory space is shared  clone  sighand signal handlers are shared  clone files the set of open files is shared  for example  if clone   is passed the flags clone_fs  clone_vm  clone_sighand  and clone_files  the parent and child tasks will share the same file-system information  such as the current working directory   the same memory space  the same signal handlers  and the same set of open files  using clone   in this fashion is equivalent to creating a thread as described in this chapter  since the parent task shares most of its resources with its child task however  if none of these flags is set when clone   is invoked  no sharing takes place  resulting in functionality similar to that provided by the fork   system call  the varying level of sharing is possible because of the way a task is represented in the linux kernel a unique kernel data structure  specifically  struct task_struct  exists for each task in the system this data structure  instead of storing data for the task  contains pointers to other data structures where these data are stored -for example  data structures that represent the list of open files  signal-handling information  and virtual memory when fork   is invoked  a new task is created  along with a copy of all the associated data structures of the parent process a new task is also created when the clone   system call is made howevet ~ rather than copying all data structures  the new 174 chapter 4 4.6 task points to the data structures of the parent task  depending on the set of flags passed to clone    several distributions of the linux kernel now include the nptl thread library nptl  which stands for native posix thread library  provides a posix-compliant thread model for linux systems along with several other features  such as better support for smp systems  as well as taking advantage of numa support in addition  the start-up cost for creating a thread is lower with nptl than with traditional linux threads finally  with nptl  the system has the potential to support hundreds of thousands of threads such support becomes more important with the growth of multicore and other smp systems  a thread is a flow of control within a process a multithreaded process contains several different flows of control within the same address space the benefits of multithreading include increased responsiveness to the use1 ~ resource sharing within the process  economy  and scalability issues such as more efficient use of multiple cores  user-level threads are threads that are visible to the programmer and are unknown to the kernel the operating-system kernel supports and manages kernel-level threads in general  user-level threads are faster to create and manage than are kernel threads  as no intervention from the kernel is required  three different types of models relate user and kernel threads  the many-to-one model maps many user threads to a single kernel thread the one-to-one model maps each user thread to a corresponding kernel thread the many-to-many model multiplexes many user threads to a smaller or equal number of kernel threads  most modern operating systems provide kernel support for threads ; among these are windows 98  nt  2000  and xp  as well as solaris and linux  thread libraries provide the application programmer with an api for creating and managing threads three primary thread libraries are in common use  posix pthreads  win32 threads for windows systems  and java threads  multithreaded programs introduce many challenges for the programmer  including the semantics of the fork   and exec   system calls other issues include thread cancellation  signal handling  and thread-specific data  4.1 provide two programming examples in which multithreading does not provide better performance than a single-threaded solution  4.2 write a ncultithreaded java  pthreads  or win32 program that outputs prime numbers this program should work as follows  the user will run the program and will enter a number on the command line the 175 program will then create a separate thread that outputs all the prime numbers less than or equal to the number entered by the user  4.3 which of the following components of program state are shared across threads in a multithreaded process a register values b heap memory c global variables d stack memory 4.4 the program shown in figure 4.14 uses the pthreads api what would be the output from the program at line c and line p # include pthread.h # include stdio.h int value = 0 ; void runner  void param  ; i the thread i int main  int argc  char argv     int pid ; pthread_t tid ; pthread_attr t attr ;  pid = fork   ; if  pid = = 0   i child process i pthread_attr_init  &attr  ; pthread_create  &tid,&attr,runner,null  ; pthread_join  tid,null  ; printf  child  value = % d ,value  ; i line c i  else if  pid 0   i parent process i wait  null  ; printf  parent  value = % d ,value  ; i line p i  void runner  void param   value = 5 ; pthread_exi t  0  ;  figure 4.14 c program for exercise 4.4  176 chapter 4 4.5 consider a multiprocessor system and a multithreaded program written using the many-to-many threading rnodel let the number of user-level threads in the program be more than the number of processors in the system discuss the performance implicatiorts of the following scenarios  a the number of kernel threads allocated to the program is less than the number of processors  b the number of kernel threads allocated to the program is equal to the number of processors  c the number of kernel threads allocated to the program is greater than the number of processors but less than the number of userlevel threads  4.6 what are two differences between user-level threads and kernel-level threads under what circumstances is one type better than the other 4.7 exercise 3.16 in chapter 3 involves designing an echo server using the java threading api however  this server is single-threaded  meaning that the server can not respond to concurrent echo clients until the current client exits modify the solution to exercise 3.16 so that the echo server services each client in a separate request  4.8 modify the socket-based date server  figure 3.19  in chapter 3 so that the server services each client request in a separate thread  4.9 can a multithreaded solution using multiple user-level threads achieve better performance on a multiprocessor system than on a singleprocessor system explain  4.10 what resources are used when a thread is created how do they differ from those used when a process is created 4.11 under what circumstances does a multithreaded solution using multiple kernel threads provide better performance than a single-threaded solution on a single-processor system 4.12 the fibonacci sequence is the series of numbers 0  1  1  2  3  5 8    formally  it can be expressed as  fib0 = 0 fih = 1 jibn = jibn-1 + jibn-2 write a multithreaded program that generates the fibonacci sequence using either the java  pthreads  or win32 thread library this program 177 should work as follows  the user will enter on the command line the number of fibonacci numbers that the program ~ is to generate  the program will then create a separate thread that will generate the fibonacci numbers  placing the sequence in data that can be shared by the threads  an array is probably the most convenient data structure   when the thread finishes execution  the parent thread will output the sequence generated by the child thread because the parent thread can not begin outputting the fibonacci sequence until the child thread finishes  this will require having the parent thread wait for the child thread to finish  using the techniques described in section 4.3  4.13 a pthread program that performs the smmnation function was provided in section 4.3.1 rewrite this program in java  4.14 as described in section 4.5.2  linux does not distinguish between processes and threads instead  linux treats both in the same way  allowing a task to be more akin to a process or a thread depending on the set of flags passed to the clone   system call however  many operating systems-such as windows xp and solaris-treat processes and threads differently typically  such systems use a notation wherein the data structure for a process contains pointers to the separate threads belonging to the process contrast these two approaches for modeling processes and threads within the kernel  4.15 describe the actions taken by a thread library to context-switch between user-level threads  the set of projects below deal with two distinct topics-naming service and matrix muliplication  project 1  naming service project a naming service such as dns  for domain name system  can be used to resolve ip names to ip addresses for example  when someone accesses the host www westminstercollege edu  a naming service is used to determine the ip address that is mapped to the ip name www westminstercollege edu  this assignment consists of writing a multithreaded nan ling service in java using sockets  see section 3.6.1   the java net api provides the following mechanism for resolving ip names  inetaddress hostaddress = inetaddress.getbyname  www.westminstercollege.edu  ; string ipaddress = hostaddress.gethostaddress   ; where getbyname   throws an unknownhostexception if it is unable to resolve the host name  178 chapter 4 the server the server will listen to port 6052 waiting for client connections when a client connection is made  the server will service the connection in a separate thread and will resume listening for additional client connections once a client makes a connection to the server  the client will write the ip name it wishes the server to resolve-such as www westminstercollege eduto the socket the server thread will read this ip name from the socket and either resolve its ip address or  if it can not locate the host address  catch an unknownhostexception the server will write the ip address back to the client or  in the case of an unknownhostexception  will write the message unable to resolve host host name  once the server has written to the client  it will close its socket connection  the client initially  write just the server application and connect to it via telnet  for example  assuming the server is running on the localhost a telnet session would appear as follows  client responses appear in telnec localhost 6052 connected to localhost  escape character is 'a  '  \ i ~ /vv \ h 'destrninstercollege edu 146.86.1.17 connection closed by foreign host  by initially having telnet act as a client  you can more easily debug any problems you may have with your server once you are convinced your server is working properly  you can write a client application the client will be passed the ip name that is to be resolved as a parameter the client will open a socket connection to the server and then write the if name that is to be resolved it will then read the response sent back by the server as an example  if the client is named nsclient  it is invoked as follows  java nsclient www.westminstercollege.edu and the server will respond with the corresponding if address or unknown host message once the client has output the if address  it will close its socket connection  project 2  matrix multiplication project given two matrices  a and b  where matrix a contains m rows and k columns and matrix b contains k rows and n columns  the of a and b is matrix c  where c contains m rows and n coh.11m1s the entry in matrix c for row i  column j  c.j  is the sum of the products of the elements for row i in matrix a and column j in matrix b that is  179 k c,j = l a ;  11 x bn,j 11 =  1 for example  if a is a 3-by-2 matrix and b is a 2-by-3 m.atrix  element c3,1 is the sum of a3,1 x b1.1 and a3,2 x b2,1 for this project  calculate each element c ; ,j in a separate worker thread this will involve creating m x n worker threads the main-or parent-thread will initialize the matrices a and b and allocate sufficient memory for matrix c  which will hold the product of matrices a and b these matrices will be declared as global data so that each worker thread has access to a  b  and c  matrices a and b can be initialized statically  as shown below  # define m 3 # define k 2 # define n 3 int a  m   k  int b  k   n  int c  m   n  ;   1,4    2,5    3,6   ;   8,7,6    5,4,3   ; alternatively  they can be populated by reading in values from a file  passing parameters to each thread the parent thread will create m x n worker threads  passing each worker the values of row i and column j that it is to use in calculating the matrix product  this requires passing two parameters to each thread the easiest approach with pthreads and win32 is to create a data structure using a struct the members of this structure are i and j  and the structure appears as follows  i structure for passing data to threads i struct v   ; int i ; i row i int j ; i column i both the pthreads and win32 programs will create the worker threads using a strategy similar to that shown below  i we have to create m n worker threads i for  i = 0 ; i m  i + +   for  j = 0 ; j n ; j + +    struct v data =  struct v  rnalloc  sizeof  struct v   ; data i = i ; data j = j ; i now create the thread passing it data as a parameter i 180 chapter 4 public class workerthread implements runnable   private int row ; private int col ; private int     a ; private int     b ; private int     c ; public workerthread  int row  int col  int     a   int     b  int     c   this.row = row ; this.col = col ; this.a a ; this.b this.c b ' c ; public void run    i calculate the matrix product in c  row   col  i  figure 4.15 worker thread in java  the data pointer will be passed to either the pthread_create    pthreads  function or the createthread    win32  function  which in turn will pass it as a parameter to the function that is to run as a separate thread  sharing of data between java threads is different from sharing between threads in pthreads or win32 one approach is for the main thread to create and initialize the matrices a  b  and c this main thread will then create the worker threads  passing the three matrices-along with row i and column jto the constructor for each worker thus  the outline of a worker thread appears in figure 4.15  waiting for threads to complete once all worker threads have completed  the main thread will output the product contained in matrix c this requires the main thread to wait for all worker threads to finish before it can output the value of the matrix product several different strategies can be used to enable a thread to wait for other threads to finish section 4.3 describes how to wait for a child thread to complete using the win32  pthreads  and java thread libraries  win32 provides the wai tforsingleobj ect   function  whereas pthreads and java use pthread_j oin   and join    respectively however  in these programming examples  the parent thread waits for a single child thread to finish ; completing this exercise will require waiting for multiple threads  in section 4.3.2  we describe the wai tforsingleobj ect   function  which is used to wait for a single thread to finish however  the win32 api also provides the wai tformultipledbj ects   function  which is used when waiting for multiple threads to complete waitformultipleobjectso is passed four parameters  # define num_threads 10 i an array of threads to be joined upon i pthread_t workers  num_threads  ; for  int i = 0 ; i num_threads ; i + +  pthread_join  workers  i   null  ; figure 4.16 pthread code for joining ten threads  the num.ber of objects to wait for a pointer to the array of objects a flag indicating if all objects have been signaled a timeout duration  or infinite  181 for example  if thandles is an array of thread handle objects of size n  the parent thread can wait for all its child threads to complete with the statement  waitformultipleobjects  n  thandles  true  infinite  ; a simple strategy for waiting on several threads using the pthreads pthread_join   or java 's join   is to enclose the join operation within a simple for loop for example  you could join on ten threads using the pthread code depicted in figure 4.16 the equivalent code using java threads is shown in figure 4.17  final static int num_threads = 10 ; i an array of threads to be joined upon i thread   workers = new thread  num_threads  ; for  int i = 0 ; i num_threads ; i + +   try  workers  i   join   ;  catch  interruptedexception ie     figure 4.17 java code for joining ten threads  threads have had a long evolution  starting as cheap concurrency in programming languages and moving to lightweight processes  with early examples that included the thotll system  cheriton et al  1979   and the pilot system  redell et al  1980    binding  1985  described moving threads into the unix kernel mach  accetta et al  1986   tevanian et al  1987a   and v  cheriton  1988   made extensive use of threads  and eventually almost all major operating systems implemented them in some form or another  182 chapter 4 thread performance issues were discussed by anderson et al  1989   who continued their work in anderson et al  1991  by evaluating the performance of user-level threads with kernel support bershad et al  1990  describe combining threads with rpc engelschall  2000  discusses a technique for supporting user-level threads an analysis of an optimal thread-pool size can be found in ling et al  2000   scheduler activations were first presented in anderson et al  1991   and williams  2002  discusses scheduler activations in the netbsd system_ other mechanisms by which the user-level thread library and the kernel cooperate with each other are discussed in marsh et al  1991   govindan and anderson  1991   draves et al  1991   and black  1990   zabatta and young  1998  compare windows nt and solaris threads on a symmetric multiprocessor pinilla and gill  2003  compare java thread performance on lim1x  windows  and solaris  vahalia  1996  covers threading in several versions of unix mcdougall and mauro  2007  describe recent developments in threading the solaris kernel  russinovich and solomon  2005  discuss threading in the windows operating system family bovet and cesati  2006  and love  2004  explain how linux handles threading and singh  2007  covers threads in mac os x  information on pthreads programming is given in lewis and berg  1998  and butenhof  1997   oaks and wong  1999   lewis and berg  2000   and holub  2000  discuss multithreading in java goetz et al  2006  present a detailed discussion of concurrent programming in java beveridge and wiener  1997  and cohen and woodring  1997  describe multithreading using win32  5.1 cpu scheduling is the basis of multiprogrammed operating systems by switching the cpu among processes  the operating system can make the computer more productive in this chapter  we introduce basic cpu-scheduling concepts and present several cpu-scheduling algorithms we also consider the problem of selecting an algorithm for a particular system  in chapter 4  we introduced threads to the process model on operating systems that support them  it is kernel-level threads-not processes-that are in fact being scheduled by the operating system however  the terms process scheduling and thread scheduling are often used interchangeably in this chapter  we use process scheduling when discussing general scheduling concepts and thread scheduling to refer to thread-specific ideas  to introduce cpu scheduling  which is the basis for multiprogrammed operating systems  to describe various cpu-scheduling algorithms  to discuss evaluation criteria for selecting a cpu-scheduling algorithm for a particular system  in a single-processor system  only one process can run at a time ; any others must wait until the cpu is free and can be rescheduled the objective of multiprogramming is to have some process rum1ing at all times  to maximize cpu utilization the idea is relatively simple a process is executed until it must wait  typically for the completion of some i/o request in a simple computer system  the cpu then just sits idle all this waiting time is wasted ; no useful work is accomplished with multiprogramming  we try to use this time productively several processes are kept in memory at one time when one process has to wait  the operating system takes the cpu away from that 183 184 chapter 5 load store add store read from file wait for 110 store increment index write to file wait for 1/0 load store add store read from file  wait.tor ; l/0 cpu burst 1/0 burst cpu burst 1/0 burst cpu burst 1/0 burst figure 5.i alternating sequence of cpu and 1/0 bursts  process and gives the cpu to another process this pattern continues every time one process has to wait  another process can take over use of the cpu  scheduling of this kind is a fundamental operating-system function  almost all computer resources are scheduled before use the cpu is  of course  one of the primary computer resources thus  its scheduling is central to operating-system design  5.1.1 cpu-i/o burst cycle the success of cpu scheduling depends on an observed property of processes  process execution consists of a cycle of cpu execution and i/0 wait processes alternate between these two states process execution begins with a cpu burst  that is followed by an i/o burst  which is followed by another cpu burst  then another i/0 burst  and so on eventually  the final cpu burst ends with a system request to terminate execution  figure 5.1   the durations of cpu bursts have been measured extensively although they vary greatly from process to process and from computer to compute1 ~ they tend to have a frequency curve similar to that shown in figure 5.2 the curve is generally characterized as exponential or hyperexponential  with a large number of short cpu bursts and a small number of long cpu bursts  an i/o-bound program typically has many short cpu bursts a cpu-bound 5.1 185 160 140 120  0 100 c aj    l u 80 ~ 60 40 20 0 8 16 24 32 40 burst duration  milliseconds  figure 5.2 histogram of cpu-burst durations  program might have a few long cpu bursts this distribution can be important in the selection of an appropriate cpu-scheduling algorithm  5.1.2 cpu scheduler whenever the cpu becomes idle  the operating system must select one of the processes in the ready queue to be executed the selection process is carried out by the short-term scheduler  or cpu scheduler   the scheduler selects a process from the processes in memory that are ready to execute and allocates the cpu to that process  note that the ready queue is not necessarily a first-in  first-out  fifo  queue  as we shall see when we consider the various scheduling algorithms  a ready queue can be implen ented as a fifo queue  a priority queue  a tree  or sirnply an unordered linked list conceptually  howeve1 ~ all the processes in the ready queue are lined up waiting for a chance to run on the cpu the records in the queues are generally process control blocks  pcbs  of the processes  5.1.3 preemptive scheduling cpu-scheduling decisions may take place under the following four circumstances  when a process switches from the running state to the waiting state  for example  as the result of an i/0 request or an invocation of wait for the termination of one of the child processes  when a process switches from the numing state to the ready state  for example  when an interrupt occurs  186 chapter 5 when a process switches from the waiting state to the ready state  for example  at completion of i/0  when a process terminates for situations 1 and 4  there is no choice in terms of scheduling a new process  if one exists in the ready queue  must be selected for execution there is a choice  however  for situations 2 and 3  when scheduling takes place only under circumstances 1 and 4  we say that the scheduling scheme is nonpreemptive or cooperative ; otherwise  it is preemptive under nonpreemptive scheduling  once the cpu has been allocated to a process  the process keeps the cpu until it releases the cpu either by terminating or by switching to the waiting state this scheduling method was used by microsoft windows 3.x ; windows 95 introduced preemptive scheduling  and all subsequent versions of windows operating systems have used preemptive scheduling the mac os x operating system for the macintosh also uses preemptive scheduling ; previous versions of the macintosh operating system relied on cooperative scheduling cooperative scheduling is the only method that can be used on certain hardware platforms  because it does not require the special hardware  for example  a timer  needed for preemptive scheduling  unfortunately  preemptive scheduling incurs a cost associated with access to shared data consider the case of two processes that share data while one is updating the data  it is preempted so that the second process can run the second process then tries to read the data  which are in an inconsistent state in such situations  we need new mechanisms to coordinate access to shared data ; we discuss this topic in chapter 6  preemption also affects the design of the operating-system kernel during the processing of a system call  the kernel may be busy with an activity on behalf of a process such activities may involve changing important kernel data  for instance  i/0 queues   what happens if the process is preempted in the middle of these changes and the kernel  or the device driver  needs to read or modify the same structure chaos ensues certain operating systems  including most versions of unix  deal with this problem by waiting either for a system call to com.plete or for an i/o block to take place before doing a context switch this scheme ensures that the kernel structure is simple  since the kernel will not preempt a process while the kernel data structures are in an inconsistent state unfortunately  this kernel-execution model is a poor one for supportil1g real-time computing and multiprocessing  these problems  and their solutions  are described i.j.1 sections 5.5 and 19.5  because interrupts can  by definition  occur at any time  and because they can not always be ignored by the kernel  the sections of code affected by interrupts must be guarded from simultaneous use the operating system needs to accept interrupts at almost all times ; otherwise  input might be lost or output overwritten so that these sections of code are not accessed concurrently by several processes  they disable interrupts at entry and reenable interrupts at exit it is important to note that sections of code that disable interrupts do not occur very often and typically contain few instructions  5.2 5.2 187 5.1.4 dispatcher another component involved in the cpu-scheduling function is the dispatcher  the dispatcher is the module that gives control of the cpu to the process selected by the short-term scheduler this function involves the following  switching context switching to user mode jumping to the proper location in the user program to restart that program the dispatcher should be as fast as possible  since it is invoked during every process switch the time it takes for the dispatcher to stop one process and start another running is known as the dispatch latency  different cpu-scheduling algorithms have different properties  and the choice of a particular algorithm may favor one class of processes over another in choosing which algorithm to use in a particular situation  we must consider the properties of the various algorithms  many criteria have been suggested for comparing cpu-scheduling algorithms  which characteristics are used for comparison can make a substantial difference in which algorithm is judged to be best the criteria include the following  cpu utilization we want to keep the cpu as busy as possible conceptually  cpu utilization can range from 0 to 100 percent in a real system  it should range from 40 percent  for a lightly loaded system  to 90 percent  for a heavily used system   throughput if the cpu is busy executing processes  then work is being done one measure of work is the number of processes that are completed per time unit  called throughput for long processes  this rate may be one process per hour ; for short transactions  it may be ten processes per second  turnaround time from the point of view of a particular process  the important criterion is how long it takes to execute that process the interval from the time of submission of a process to the time of completion is the turnaround time turnaround tim.e is the sum of the periods spent waiting to get into memory  waiting in the ready queue  executing on the cpu  and doing i/0  waiting time the cpu-scheduling algorithm does not affect the amount of time during which a process executes or does i/0 ; it affects only the an1.ount of time that a process spends waiting in the ready queue waiting time is the sum of the periods spent waiting in the ready queue  response time in an interactive system  turnaround time may not be the best criterion often  a process can produce some output fairly early and can continue computing new results while previous results are being 188 chapter 5 5.3 output to the user thus  another measure is the time from the submission of a request until the first response is produced this measure  called response time  is the tince it takes to start responding  not the time it takes to output the response the turnaround time is generally limited by the speed of the output device  it is desirable to maximize cpu utilization and throughput and to minirnize turnaround time  waiting time  and response time in most cases  we optimize the average measure however  under some circumstances  it is desirable to optimize the minimum or maximum values rather than the average for example  to guarantee that all users get good service  we may want to minirnize the maximum response time  investigators have suggested that  for interactive systems  such as timesharing systerns   it is more important to minimize the variance in the response time than to minimize the average response time a system with reasonable and predictable response time may be considered more desirable than a system that is faster on the average but is highly variable howeve1 ~ little work has been done on cpu-scheduling algorithms that minimize variance  as we discuss various cpu-scheduling algorithms in the following section  we illustrate their operation an accurate illustration should involve many processes  each a sequence of several hundred cpu bursts and i/o bursts  for simplicity  though  we consider only one cpu burst  in milliseconds  per process in our examples our measure of comparison is the average waiting time more elaborate evaluation mechanisms are discussed in section 5.7  cpu scheduling deals with the problem of deciding which of the processes in the ready queue is to be allocated the cpu there are many different cpu-scheduling algorithms in this section  we describe several of them  5.3.1 first-come  first-served scheduling by far the simplest cpu-scheduling algorithm is the first-come  first-served  fcfs  scheduling algorithm with this scheme  the process that requests the cpu first is allocated the cpu first the implementation of the fcfs policy is easily managed with a fifo queue when a process enters the ready queue  its pcb is linked onto the tail of the queue when the cpu is free  it is allocated to the process at the head of the queue the running process is then removed from the queue the code for fcfs scheduling is simple to write and understand  on the negative side  the average waiting time under the fcfs policy is often quite long consider the following set of processes that arrive at time 0  with the length of the cpu burst given in milliseconds  process burst time  p  24 p2 3 po   3 5.3 189 if the processes ani ve in the order p1  p2  p3  and are served in fcfs order  we get the result shown in the following gantt chart  which is a bar chart that illustrates a particular schedule  including the start and finish times of each of the participating processes  0 24 27 30 the waiting time is 0 milliseconds for process p1  24 milliseconds for process p2  and 27 milliseconds for process p3  thus  the average waiting time is  0 + 24 + 27  /3 = 17 ncilliseconds if the processes arrive in the order p2  p3  p1  however  the results will be as shown in the following gantt chart  0 3 6 30 the average waiting time is now  6 + 0 + 3  /3 = 3 milliseconds this reduction is substantial thus  the average waiting time under an fcfs policy is generally not minimal and may vary substantially if the processes cpu burst times vary greatly  in addition  consider the performance of fcfs scheduling in a dynamic situation assume we have one cpu-bound process and many i/o-bound processes as the processes flow armmd the system  the following scenario may result the cpu-bound process will get and hold the cpu during this time  all the other processes will finish their i/0 and will move into the ready queue  waiting for the cpu while the processes wait in the ready queue  the i/0 devices are idle eventually  the cpu-bound process finishes its cpu burst and moves to an i/0 device all the i/o-bound processes  which have short cpu bursts  execute quickly and move back to the i/0 queues at this point  the cpu sits idle the cpu-bound process will then move back to the ready queue and be allocated the cpu again  all the i/0 processes end up waiting in the ready queue until the cpu-bound process is done there is a convoy effect as all the other processes wait for the one big process to get off the cpu this effect results in lower cpu and device utilization than might be possible if the shorter processes were allowed to go first  note also that the fcfs scheduling algorithm is nonpreemptive once the cpu has been allocated to a process  that process keeps the cpu until it releases the cpu  either by terminating or by requesting i/0 the fcfs algorithm is thus particularly troublesome for time-sharing systems  where it is important that each user get a share of the cpu at regular intervals it would be disastrous to allow one process to keep the cpu for an extended period  5.3.2 shortest-job-first scheduling a different approach to cpu scheduling is the shortest-job-first  sjf  scheduling algorithm this algorithm associates with each process the length of the process 's next cpu burst when the cpu is available  it is assigned to the process 190 chapter 5 that has the smallest next cpu burst if the next cpu bursts of two processes are the same  fcfs scheduling is used to break the tie note that a more appropriate term for this scheduling method would be the shortest-next-cpu-burst algorithm  because scheduling depends on the length of the next cpu burst of a process  rather than its total length we use the term sjf because m.ost people and textbooks use this term to refer to this type of scheduling  as an example of sjf scheduling  consider the following set of processes  with the length of the cpu burst given in milliseconds  process burst time pl 6 p2 8 p3 7 p4 3 using sjf scheduling  we would schedule these processes according to the following gantt chart  0 3 9 16 24 the waiting time is 3 milliseconds for process p1  16 milliseconds for process p2  9 milliseconds for process p3  and 0 milliseconds for process p4  thus  the average waiting time is  3 + 16 + 9 + 0  i 4 = 7 milliseconds by comparison  if we were using the fcfs scheduling scheme  the average waiting time would be 10.25 milliseconds  the sjf scheduling algorithm is provably optimal  in that it gives the minimum average waiting time for a given set of processes moving a short process before a long one decreases the waiting time of the short process more than it increases the waiting time of the long process consequently  the average waiting time decreases  the real difficulty with the sjf algorithm is knowing the length of the next cpu request for long-term  job  scheduling in a batch system  we can use as the length the process time limit that a user specifies when he submits the job thus  users are motivated to estimate the process time limit accurately  since a lower value may mean faster response  too low a value will cause a time-limit-exceeded error and require resubmission  sjf scheduling is used frequently in long-term scheduling  although the sjf algorithm is optimal  it can not be implemented at the level of short-term cpu scheduling with short-term scheduling  there is no way to know the length of the next cpu burst one approach is to try to approximate sjf scheduling we may not know the length of the next cpu burst  but we may be able to predict its value we expect that the next cpu burst will be similar in length to the previous ones by computing an approximation of the length of the next cpu burst  we can pick the process with the shortest predicted cpu burst  5.3 191 the next cpu burst is generally predicted as an exponential average of the measured lengths of previous cpu bursts we can define the exponential average with the following formula let t11 be the length of the nth cpu burst  and let t11 + t be our predicted value for the next cpu burst then  for a  0  s a 1  define the value of tn contains our most recent information ; t11 stores the past history  the parameter a controls the relative weight of recent and past history in our prediction if a = 0  then tn + l = t11  and recent history has no effect  current conditions are assumed to be transient   if a = 1  then tn + l = t11  and only the most recent cpu burst matters  history is assumed to be old and irrelevant   more commonly  a = 1/2  so recent history and past history are equally weighted  the initial to can be defined as a constant or as an overall system average  figure 5.3 shows an exponential average with a = 1/2 and to = 10  to lmderstand the behavior of the exponential average  we can expand the formula for tn + l by substituting for t 11  to find  j jj ' 1 tn + l = atn +  1  a atn-1 + +  1 a  atn-j + +  1 a  'to  since both a and  1  a  are less than or equal to 1  each successive term has less weight than its predecessor  the sjf algorithm can be either preemptive or nonpreemptive the choice arises when a new process arrives at the ready queue while a previous process is still executing the next cpu burst of the newly arrived process may be shorter time + cpu burst  f  6 4 6 4 13 13 13 guess  t ;  10 8 6 6 5 9 1 1 12 figure 5.3 prediction of the length of the next cpu burst  192 chapter 5 than what is left of the currently executing process a preemptive sjf algorithm will preempt the currently executing process  whereas a nonpreemptive sjf algorithm will allow the currently running process to finish its cpu burst  preemptive sjf scheduling is sometimes called shortest-remaining-time-first scheduling  as an example  consider the following four processes  with the length of the cpu burst given in milliseconds  process arrival time burst time pl 0 8 p2 1 4 p3 2 9 p4 3 5 if the processes arrive at the ready queue at the times shown and need the indicated burst times  then the resulting preemptive sjf schedule is as depicted in the following gantt chart  0 5 10 17 26 process p1 is started at time 0  since it is the only process in the queue process p2 arrives at time 1 the remaining time for process p1  7 milliseconds  is larger than the time required by process p2  4 milliseconds   so process p1 is preempted  and process p2 is scheduled the average waiting time for this example is   10 1  +  1  1  +  17 2  +  5-3   / 4 = 26/4 = 6.5 milliseconds  nonpreemptive sjf scheduling would result in an average waiting time of 7.75 milliseconds  5.3.3 priority scheduling the sjf algorithm is a special case of the general priority scheduling algorithm  a priority is associated with each process  and the cpu is allocated to the process with the highest priority equal-priority processes are scheduled in fcfs order  an sjf algorithm is simply a priority algorithm where the priority  p  is the inverse of the  predicted  next cpu burst the larger the cpu burst  the lower the priority  and vice versa  note that we discuss scheduling in terms of high priority and low priority  priorities are generally indicated by some fixed range of numbers  such as 0 to 7 or 0 to 4,095 however  there is no general agreement on whether 0 is the highest or lowest priority some systems use low numbers to represent low priority ; others use low numbers for high priority this difference can lead to confusion in this text  we assume that low numbers represent high priority  as an example  consider the following set of processes  assumed to have arrived at time 0 in the order p1  p2   ps  with the length of the cpu burst given in milliseconds  5.3 193 process burst time ~  ~   rity pl 10   0 p2 1 1 p3 2 4 p4 1 5 ps 5 2 using priority scheduling  we would schedule these processes according to the following gantt chart  0 6 16 18 19 the average waiting time is 8.2 milliseconds  priorities can be defined either internally or externally internally defined priorities use some nceasurable quantity or quantities to compute the priority of a process for example  time limits  memory requirements  the number of open files  and the ratio of average i/0 burst to average cpu burst have been used in computing priorities external priorities are set by criteria outside the operating system  such as the importance of the process  the type and amount of funds being paid for computer use  the department sponsoring the work  and other  often politicat factors  priority scheduling can be either preemptive or nonpreemptive when a process arrives at the ready queue  its priority is compared with the priority of the currently running process a preemptive priority scheduling algorithm will preempt the cpu if the priority of the newly arrived process is higher than the priority of the currently running process a nonpreemptive priority scheduling algorithm will simply put the new process at the head of the ready queue  a rnajor problem with priority scheduling algorithms is indefinite blocking  or starvation a process that is ready to run but waiting for the cpu can be considered blocked a priority scheduling algorithm can leave some lowpriority processes waiting indefinitely in a heavily loaded computer system  a steady stream of higher-priority processes can prevent a low-priority process from ever getting the cpu generally  one of two things will happen either the process will eventually be run  at 2 a.m sunday  when the system is finally lightly loaded   or the cornputer systern will eventually crash and lose all unfinished low-priority processes  rumor has it that when they shut down the ibm 7094 at mit in 1973  they found a low-priority process that had been submitted in 1967 and had not yet been run  a solution to the problem of indefinite blockage of low-priority processes is aging aging is a techniqtje of gradually increasing the priority of processes that wait in the system for a long time for example  if priorities range from 127  low  to 0  high   we could increase the priority of a waiting process by 1 every 15 minutes eventually  even a process with an initial priority of 127 would have the highest priority in the system and would be executed in fact  it would take no more than 32 hours for a priority-127 process to age to a priority-0 process  194 chapter 5 5.3.4 round-robin scheduling the round-robin  rr  scheduling algorithm is designed especially for timesharing systems it is similar to fcfs scheduling  but preemption is added to enable the system to switch between processes a small unit of time  called a time quantum or time slice  is defined a time quantum is generally fronc 10 to 100 milliseconds in length the ready queue is treated as a circular queue  the cpu scheduler goes around the ready queue  allocating the cpu to each process for a time interval of up to 1 time quantum  to implement rr scheduling  we keep the ready queue as a fifo queue o processes new processes are added to the tail of the ready queue the cpu scheduler picks the first process from the ready queue  sets a timer to interrupt after 1 time quantum  and dispatches the process  one of two things will then happen the process may have a cpu burst of less than 1 time quantum in this case  the process itself will release the cpu voluntarily the scheduler will then proceed to the next process in the ready queue otherwise  if the cpu burst of the currently running process is longer than 1 time quantum  the timer will go off and will cause an interrupt to the operating system a context switch will be executed  and the process will be put at the tail o the ready queue the cpu scheduler will then select the next process in the ready queue  the average waiting time under the rr policy is often long consider the following set of processes that arrive at time 0  with the length of the cpu burst given in milliseconds  process burst time if we use a time quantum of 4 milliseconds  then process p1 gets the first 4 milliseconds since it requires another 20 milliseconds  it is preempted after the first time quantum  and the cpu is given to the next process in the queue  process p2  process p2 does not need 4 milliseconds  so it quits before its time quantum expires the cpu is then given to the next process  process p3 once each process has received 1 time quantum  the cpu is returned to process p1 for an additional time quantum the resulting rr schedule is as follows  0 4 7 10 14 18 22 26 30 let 's calculate the average waiting time for the above schedule p1 waits for 6 millisconds  10 4   p2 waits for 4 millisconds  and p3 waits for 7 millisconds  thus  the average waiting time is 17/3 = 5.66 milliseconds  in the rr scheduling algorithm  no process is allocated the cpu for more than 1 time quantum in a row  unless it is the only runnable process   if a 5.3 195 process 's cpu burst exceeds 1 time quantum  that process is preempted and is p11t back in the ready queue the rr scheduling algorithm is thus preemptive  if there are n processes in the ready queue and the time quantum is q  then each process gets 1 in of the cpu time in chunks of at most q time units  each process must wait no longer than  11  1  x q time units until its next time quantum for example  with five processes and a time quantum of 20 milliseconds  each process will get up to 20 milliseconds every 100 milliseconds  the performance of the rr algorithm depends heavily on the size of the time quantum at one extreme  if the time quantum is extremely large  the rr policy is the same as the fcfs policy in contrast  if the time quantum is extremely small  say  1 millisecond   the rr approach is called processor sharing and  in theory  creates the appearance that each of 11 processes has its own processor running at 1 i 11 the speed of the real processor this approach was used in control data corporation  cdc  hardware to implement ten peripheral processors with only one set of hardware and ten sets of registers  the hardware executes one instruction for one set of registers  then goes on to the next this cycle continues  resulting in ten slow processors rather than one fast one  actually  since the processor was much faster than memory and each instruction referenced memory  the processors were not much slower than ten real processors would have been  in software  we need also to consider the effect of context switching on the performance of rr scheduling assume  for example  that we have only one process of 10 time units if the quantum is 12 time units  the process finishes in less than 1 time quantum  with no overhead if the quantum is 6 time units  however  the process requires 2 quanta  resulting in a context switch if the time quantum is 1 time unit  then nine context switches will occur  slowing the execution of the process accordingly  figure 5.4   thus  we want the time quantum to be large with respect to the contextswitch time if the context-switch time is approximately 10 percent of the time quantum  then about 10 percent of the cpu time will be spent in context switching in practice  most modern systems have time quanta ranging from 10 to 100 milliseconds the time required for a context switch is typically less than 10 microseconds ; thus  the context-switch time is a small fraction of the time quantum  process time = 10 quantum context switches 12 0 0 10 6 0 6 10 r.r r  r  r    .r r -lr  -r r 9 0 2 3 4 5 6 7 8 9 10 figure 5.4 how a smaller time quantum increases context switches  196 chapter 5 process time 12.5 .p1 6 12.0 pz 3 p3 1 q  e 11.5 p4 7  ;   ; 0 c 11.0   j 0  a e 10.5 .2 q  10.0 en ~ q  9.5 c1l 9.0 2 3 4 5 6 7 time quantum figure 5.5 how turnaround time varies with the time quantum  turnaround time also depends on the size of the time quantum as we can see from figure 5.5  the average turnaround time of a set of processes does not necessarily improve as the time-quantum size increases in general  the average turnaround time can be improved if most processes finish their next cpu burst in a single time quantum for example  given three processes of 10 time units each and a quantum of 1 time unit  the average turnaround time is 29 if the time quantum is 10  however  the average turnaround time drops to 20 if context-switch time is added in  the average turnaround time increases even more for a smaller time quantum  since more context switches are required  although the time quantum should be large compared with the contextswitch time  it should not be too large if the time quantum is too large  rr scheduling degenerates to an fcfs policy a rule of thumb is that 80 percent of the cpu bursts should be shorter than the time quantum  5.3.5 multilevel queue scheduling another class of scheduling algorithms has been created for situations in which processes are easily classified into different groups for example  a common division is made between foreground  interactive  processes and background  batch  processes these two types of processes have different response-time requirements and so may have different scheduling needs in addition  foreground processes may have priority  externally defined  over background processes  a multilevel queue scheduling algorithm partitions the ready queue into several separate queues  figure 5.6   the processes are permanently assigned to one queue  generally based on some property of the process  such as memory 5.3 197 highest priority = = = = ~ '-------'i-'-n_te_r ~ ac_t_iv_e_e  .d_it ~ in_g'-'-p ~ r-.o'-c_ e'---ss ~ e-s  --'-'---l = = = = i = = = = = = ~ '---------'b_a_tc_h_p_r_o_ce_s_s_e_s ______ _j = = = = = = ~ = = = = = = ~ '-------s_tu_d_e_n_t_p_ro_c_e_s_s_es_ _____ _jl = = = = = = i lowest priority figure 5.6 multilevel queue scheduling  size  process priority  or process type each queue has its own scheduling algorithm for example  separate queues might be used for foreground and background processes the foreground queue might be scheduled by an rr algorithm  while the background queue is scheduled by an fcfs algorithm  in addition  there must be scheduling among the queues  which is commonly implemented as fixed-priority preemptive scheduling for example  the foreground queue may have absolute priority over the background queue  let 's look at an example of a multilevel queue scheduling algorithm with five queues  listed below in order of priority  system processes interactive processes interactive editing processes batch processes student processes each queue has absolute priority over lower-priority queues no process in the batch queue  for example  could run unless the queues for system processes  interactive processes  and interactive editing processes were all empty if an interactive editing process entered the ready queue while a batch process was running  the batch process would be preempted  another possibility is to time-slice among the queues here  each queue gets a certain portion of the cpu time  which it can then schedule among its various processes for instance  in the foreground-background queue example  the foreground queue can be given 80 percent of the cpu time for rr scheduling among its processes  whereas the background queue receives 20 percent of the cpu to give to its processes on an fcfs basis  198 chapter 5 5.3.6 multilevel feedback queue scheduling normally  when the multilevel queue scheduling algorithm is used  processes are permanently assigned to a queue when they enter the system if there are separate queues for foreground and background processes  for example  processes do not move from one queue to the other  since processes do not change their foreground or background nature this setup has the advantage of low scheduling overhead  but it is inflexible  the multilevel feedback queue scheduling algorithm  in contrast  allows a process to move between queues the idea is to separate processes according to the characteristics of their cpu bursts if a process uses too much cpu time  it will be moved to a lower-priority queue this scheme leaves i/o-bound and interactive processes in the higher-priority queues in addition  a process that waits too long in a lower-priority queue may be moved to a higher-priority queue this form of aging prevents starvation  for example  consider a multilevel feedback queue scheduler with three queues  numbered from 0 to 2  figure 5.7   the scheduler first executes all processes in queue 0 only when queue 0 is empty will it execute processes in queue 1 similarly  processes in queue 2 will only be executed if queues 0 and 1 are empty a process that arrives for queue 1 will preempt a process in queue 2 a process in queue 1 will in turn be preempted by a process arriving for queue 0  a process entering the ready queue is put in queue 0 a process in queue 0 is given a time quantum of 8 milliseconds if it does not filcish within this time  it is moved to the tail of queue 1 if queue 0 is empty  the process at the head of queue 1 is given a quantum of 16 milliseconds if it does not complete  it is preempted and is put into queue 2 processes in queue 2 are run on an fcfs basis but are run only when queues 0 and 1 are empty  this scheduling algorithm gives highest priority to any process with a cpu burst of 8 milliseconds or less such a process will quickly get the cpu  finish its cpu burst  and go off to its next i/0 burst processes that need more than 8 but less than 24 milliseconds are also served quickly  although with lower priority than shorter processes long processes automatically sink to queue 2 and are served in fcfs order with any cpu cycles left over from queues 0 and 1  figure 5.7 multilevel feedback queues  5.4 5.4 199 in general  a multilevel feedback queue scheduler is defined by the following parameters  the number of queues the scheduling algorithm for each queue the method used to determine when to upgrade a process to a higherpriority queue the method used to determine when to demote a process to a lowerpriority queue the method used to determine which queue a process will enter when that process needs service the definition of a multilevel feedback queue scheduler makes it the most general cpu-scheduling algorithm it can be configured to match a specific system under design unfortunately  it is also the most complex algorithm  since defining the best scheduler requires some means by which to select values for all the parameters  in chapter 4  we introduced threads to the process model  distinguishing between user-level and kernel-level threads on operating systems that support them  it is kernel-level threads-not processes-that are being scheduled by the operating system user-level threads are managed by a thread library  and the kernel is unaware of them to run on a cpu  user-level threads must ultimately be mapped to an associated kernel-level thread  although this mapping may be indirect and may use a lightweight process  lwp   in this section  we explore scheduling issues involving user-level and kernel-level threads and offer specific examples of scheduling for pthreads  5.4.1 contention scope one distinction between user-level and kernel-level threads lies in how they are scheduled on systems implementing the many-to-one  section 4.2.1  and many-to-many  section 4.2.3  models  the thread library schedules user-level threads to run on an available lwp  a scheme known as process-contention scope  pcs   since competition for the cpu takes place among threads belonging to the same process when we say the thread library schedules user threads onto available lwps  we do not mean that the thread is actually running on a cpu ; this would require the operating system to schedule the kernel thread onto a physical cpu to decide which kernel thread to schedule onto a cpu  the kernel uses system-contention scope  scs   competition for the cpu with scs scheduling takes place among all threads in the system systems usilcg the one-to-one model  section 4.2.2   such as windows xp  solaris  and linux  schedule threads using only scs  typically  pcs is done according to priority-the scheduler selects the runnable thread with the highest priority to run user-level thread priorities 200 chapter 5 5.5 are set by the programmer and are not adjusted by the thread library  although some thread libraries may allow the programmer to change the priority of a thread it is important to note that pcs will typically preempt the thread currently running in favor of a higher-priority thread ; however  there is no guarantee of time slicing  section 5.3.4  among threads of equal priority  5.4.2 pthread scheduling we provided a sample postx pthread program in section 4.3.1  along with an introduction to thread creation with pthreads now  we highlight the posix pthread api that allows specifying either pcs or scs during thread creation  pthreads identifies the following contention scope values  pthread_scope_process schedules threads using pcs scheduling  pthread_scope_system schedules threads using scs scheduling  on systems implementing the many-to-many model  the pthread_scope_process policy schedules user-level threads onto available lwps the number of lwps is maintained by the thread library  perhaps using scheduler activations  section 4.4.6   the pthread_scope_system scheduling policy will create and bind an lwp for each user-level thread on many-to-many systems  effectively mapping threads using the one-to-one policy  the pthread ipc provides two functions for getting-and setting-the contention scope policy  pthread_attr_setscope  pthread_attr_t attr  int scope  pthread_attr_getscope  pthread_attr_t attr  int scope  the first parameter for both functions contains a pointer to the attribute set for the thread the second parameter for the pthread_attr_setscope   function is passed either the pthread_scope_system or the pthread_scope_process value  indicating how the contention scope is to be set in the case of pthread_attr_getscope    this second parameter contaiils a pointer to an int value that is set to the current value of the contention scope if an error occurs  each of these functions returns a non-zero value  in figure 5.8  we illustrate a pthread scheduling api the program first determines the existing contention scope and sets it to pthread_scoplprocess it then creates five separate threads that will run using the scs scheduling policy note that on some systems  only certain contention scope values are allowed for example  linux and mac os x systems allow only pthread_scope_system  our discussion thus far has focused on the problems of scheduling the cpu in a system with a single processor if multiple cpus are available  load sharing becomes possible ; however  the scheduling problem becomes correspondingly 505 # include pthreadoh # include stdiooh # define num_threads 5 int main  int argc  char argv      int i  scope ; pthread_t tid  num_threads  ; pthread_attr_t attr ; i get the default attributes i pthread_attr_init  &attr  ; i first inquire on the current scope i if  pthread_attr_getscope  &attr  &scope  ! = 0  fprintf  stderr  unable to get scheduling scope \ n  ; else   if  scope = = pthread_scope_process  printf  pthread_scoplprocess  ; else if  scope = = pthread_scope_system  printf  pthread_scope_system  ; else fprintf  stderr  illegal scope valueo \ n  ; i set the scheduling algorithm to pcs or scs i pthread_attr_setscope  &attr  pthread_scope_system  ; i create the threads i for  i = 0 ; i num_threads ; i + +  pthread_create  &tid  i  ,&attr,runner,null  ; i now join on each thread i for  i = 0 ; i num_threads ; i + +  pthread_join  tid  i   null  ; i each thread will begin control in this function i void runner  void param   i do some work 0 0 0 i pthread_exi t  0  ;  figure 508 pthread scheduling api  201 more complex many possibilities have been tried ; and as we saw with singleprocessor cpu scheduling  there is no one best solution here  we discuss several concerns in multiprocessor scheduling we concentrate on systems 202 chapter 5 in which the processors are identical-homogeneous-in terms of their functionality ; we can then use any available processor to run any process in the queue  note  however  that even with homogeneous multiprocessors  there are sometimes limitations on scheduling consider a system with an l/0 device attached to a private bus of one processor processes that wish to use that device must be scheduled to run on that processor  5.5.1 approaches to multiple-processor scheduling one approach to cpu scheduling in a n1.ultiprocessor system has all scheduling decisions  i/o processing  and other system activities handled by a single processor-the master server the other processors execute only user code  this asymmetric multiprocessing is simple because only one processor accesses the system data structures  reducing the need for data sharing  a second approach uses symmetric multiprocessing  smp   where each processor is self-scheduling all processes may be in a common ready queue  or each processor may have its own private queue of ready processes regardless  scheduling proceeds by having the scheduler for each processor examine the ready queue and select a process to execute as we shall see in chapter 61 if we have multiple processors trying to access and update a common data structure  the scheduler must be programmed carefully we must ensure that two processors do not choose the same process and that processes are not lost from the queue virtually all modern operating systems support smp  including windows xp  windows 2000  solaris  linux  and mac os x in the remainder of this section  we discuss issues concerning smp systems  5.5.2 processor affinity consider what happens to cache memory when a process has been running on a specific processor the data most recently accessed by the process populate the cache for the processor ; and as a result  successive memory accesses by the process are often satisfied in cache memory now consider what happens if the process migrates to another processor the contents of cache memory must be invalidated for the first processor  and the cache for the second processor must be repopulated because of the high cost of invalidating and repopulating caches  most smp systems try to avoid migration of processes from one processor to another and instead attempt to keep a process rumung on the same processor this is known as processor affinity-that is  a process has an affinity for the processor on which it is currently rumting  processor affinity takes several forms when an operating system has a policy of attempting to keep a process running on the same processor-but not guaranteeing that it will do so-we have a situation known as soft affinity  here  it is possible for a process to migrate between processors some systems -such as lim.ix -also provide system calls that support hard affinity  thereby allowing a process to specify that it is not to migrate to other processors solaris allows processes to be assigned to limiting which processes can run on which cpus it also implements soft affinity  the main-memory architecture of a system can affect processor affinity issues figure 5.9 illustrates an architecture featuring non-uniform memory access  numa   in which a cpu has faster access to some parts of main memory than to other parts typically  this occurs in systems containing combined cpu 5.5 203 computer figure 5.9 numa and cpu scheduling  and memory boards the cpus on a board can access the memory on that board with less delay than they can access memory on other boards in the system  if the operating system 's cpu scheduler and memory-placement algorithms work together  then a process that is assigned affinity to a particular cpu can be allocated memory on the board where that cpu resides this example also shows that operating systems are frequently not as cleanly defined and implemented as described in operating-system textbooks rather  the solid lines between sections of an operating system are frequently only dotted lines  with algorithms creating connections in ways aimed at optimizing performance and reliability  5.5.3 load balancing on smp systems  it is important to keep the workload balanced among all processors to fully utilize the benefits of having more than one processor  otherwise  one or more processors may sit idle while other processors have high workloads  along with lists of processes awaiting the cpu load balancing attempts to keep the workload evenly distributed across all processors in an smp system it is important to note that load balancing is typically only necessary on systems where each processor has its own private queue of eligible processes to execute on systems with a common run queue  load balancing is often unnecessary  because once a processor becomes idle  it immediately extracts a rmmable process from the common run queue it is also important to note  howeve1 ~ that in most contemporary operating systems supporting smp  each processor does have a private queue of eligible processes  there are two general approaches to load balancing  push migration and pull migration with push migration  a specific task periodically checks the load on each processor and -if it finds an imbalance-evenly distributes the load by moving  or pushing  processes from overloaded to idle or less-busy processors pull migration occurs when an idle processor pulls a waiting task from a busy processor push and pull migration need not be mutually exclusive and are in fact often implemented in parallel on load-balancing systems for example  the linux scheduler  described in section 5.6.3  and the ule scheduler 204 chapter 5 available for freebsd systems implement both techniql1es linux runs its loadbalancing algorithm every 200 milliseconds  push migration  or whenever the run queue for a processor is empty  pull migration   interestingly  load balancing often counteracts the benefits of processor affinity  discussed in section 5.5.2 that is  the benefit of keeping a process running on the same processor is that the process can take advantage of its data being in that processor 's cache memory either pulling or pushing a process from one processor to another invalidates this benefit as is often the case in systems engineering  there is no absolute rule concerning what policy is best thus  in some systems  an idle processor always pulls a process from a non-idle processor ; and in other systems  processes are moved only if the imbalance exceeds a certain threshold  5.5.4 multicore processors traditionally  smp systems have allowed several threads to run concurrently by providing multiple physical processors however  a recent trend in computer hardware has been to place multiple processor cores on the same physical chip  resulting in a  each core has a register set to maintain its architectural state and appears to the operating system to be a separate physical processor smp systems that use multicore processors are faster and consume less power than systems in which each processor has its own physical chip  multicore processors may complicate scheduling issues let 's consider how this can happen researchers have discovered that when a processor accesses memory  it spends a significant amount of time waiting for the data to become available this situation  known as a may occur for various reasons  such as a cache miss  accessing data that is not in cache memory   figure 5.10 illustrates a memory stall in this scenario  the processor can spend up to 50 percent of its time waiting for data to become available from memory  to remedy this situation  many recent hardware designs have implemented multithreaded processor cores in which two  or more  hardware threads are assigned to each core that way  if one thread stalls while waiting for memory  the core can switch to another thread figure 5.11 illustrates a dual-threaded processor core on which the execution of thread 0 and the execution of thread 1 are interleaved from an operating-system perspective  each hardware thread appears as a logical processor that is available to run a software thread thus  on a dual-threaded  dual-core system  four logical processors are presented to the operating system the ultrasparc tl cpu has eight cores per chip and four 0 compute cycle ~ memory stall cycle thread c m c m c m c m time figure 5.10 memory stall  5.5 205 thread1 c m c m c m c thread0 c m c m c m c time figure 5.11 multithreaded multicore system  hardware threads per core ; from the perspective of the operating system  there appear to be 32 logical processors  in general  there are two ways to multithread a processor  ~ __u.,u c   ; u  chccu multithreading with coarse-grained multithreading  a thread executes on a processor until a long-latency event such as a memory stall occurs  because of the delay caused by the long-latency event  the processor must switch to another thread to begin execution however  the cost of switching between threads is high  as the instruction pipeline must be flushed before the other thread can begin execution on the processor core once this new thread begins execution  it begins filling the pipeline with its instructions  fine-grained  or interleaved  multithreading switches between threads at a much finer level of granularity-typically at the boundary of an instruction cycle however  the architectural design of fine-grained systems includes logic for thread switching as a result  the cost of switching between threads is small  notice that a multithreaded multicore processor actually requires two different levels of scheduling on one level are the scheduling decisions that must be made by the operating system as it chooses which software thread to run on each hardware thread  logical processor   for this level of scheduling  the operating system may choose any scheduling algorithm  such as those described in section 5.3 a second level of scheduling specifies how each core decides which hardware thread to run there are several strategies to adopt in this situation the ultrasparc tl  mentioned earlier  uses a simple roundrobin algorithm to schedule the four hardware threads to each core another example  the intel itanium  is a dual-core processor with hvo hardwaremanaged threads per core assigned to each hardware thread is a dynamic urgency value ranging from 0 to 7  with 0 representing the lowest urgency  and 7 the highest the itanium identifies five different events that may trigger a thread switch when one of these events occurs  the thread-switching logic compares the urgency of the two threads and selects the thread with the highest urgency value to execute on the processor core  5.5.5 virtualization and scheduling a system with virtualization  even a single-cpu system  frequently acts like a multiprocessor system the virtualization software presents one or more virtual cpus to each of the virtual machines rum1.ing on the system and then schedules the use of the physical cpus among the virtual machines  the significant variations between virtualization technologies make it difficult to summarize the effect of virtualization on scheduling  see section 2.8   in general  though  most virtualized environments have one host operating 206 chapter 5 5.6 system and many guest operating systems the host operating system creates and manages the virtual machines  and each virtual n achine has a guest operating system installed and applications running within that guest eacb guest operating system may be fine-tuned for specific use cases  applications  and users  including time sharing or even real-time operation  any guest operating-system scheduling algorithm that assumes a certain amount of progress in a given amount of time will be negatively impacted by virtualization consider a time-sharing operating system that tries to allot 100 milliseconds to each time slice to give users a reasonable response time within a virtual machine  this operating system is at the mercy of the virtualization system as to what cpu resources it actually receives a given 100-millisecond time slice may take much more than 100 milliseconds of virtual cpu time  depending on how busy the system is  the time slice may take a second or more  resulting in very poor response times for users logged into that virtual machine  the effect on a real-time operating system would be even more catastrophic  the net effect of such scheduling layering is that individual virtualized operating systems receive only a portion of the available cpu cycles  even though they believe they are receiving all of the cycles and indeed that they are scheduling all of those cycles commonly  the time-of-day clocks in virtual machines are incorrect because timers take longer to trigger than they would on dedicated cpus virtualization can thus lmdo the good scheduling-algorithm efforts of the operating systems within virtual machines  we turn next to a description of the scheduling policies of the solaris  windows xp  and linux operating systems it is important to remember that we are describing the scheduling of kernel tlueads with solaris and windows xp  recall that linux does not distinguish between processes and threads ; thus  we use the term task when discussing the linux scheduler  5.6.1 example  solaris scheduling solaris uses priority-based thread scheduling where each thread belongs to one of six classes  time sharing  ts  interactive  ia  real time  rt  system  sys  fair share  fss  fixed priority  fp  within each class there are different priorities and different scheduling algorithms  the default scheduling class for a process is time sharing the scheduling policy for the time-sharing class dynamically alters priorities and assigns time 5.6 207 10 160 0 51 15 160 5 51 20 120 10 52 25 120 15 52 30 80 20 53 35 80 25 54 40 40 30 55 45 40 35 56 50 40 40 58 55 40 45 58 59 20 49 59 figure 5.12 solaris dispatch table for time-sharing and interactive threads  slices of different lengths using a multilevel feedback queue by default  there is an inverse relationship between priorities and time slices the higher the priority  the smaller the time slice ; and the lower the priority  the larger the time slice interactive processes typically have a higher priority ; cpu-bound processes  a lower priority this scheduling policy gives good response time for interactive processes and good throughput for cpu-bound processes the interactive class uses the same scheduling policy as the time-sharing class  but it gives windowing applications-such as those created by the kde or gnome window managers-a higher priority for better performance  figure 5.12 shows the dispatch table for scheduling time-sharing and interactive threads these two scheduling classes include 60 priority levels  but for brevity  we display only a handful the dispatch table shown in figure 5.12 contains the following fields  priority the class-dependent priority for the time-sharing and interactive classes a higher number indicates a higher priority  time quantum the time quantum for the associated priority this illustrates the inverse relationship between priorities and time quanta  the lowest priority  priority 0  has the highest tince quantum  200 milliseconds   and the highest priority  priority 59  has the lowest time quantum  20 milliseconds   time quantum expired the new priority of a thread that has used its entire time quantum without blocking such threads are considered 208 chapter 5 cpu-intensive as shown in the table  these threads have their priorities lowered  return from sleep the priority of a thread that is returning from sleeping  such as waiting for i/0   as the table illustrates  when i/0 is available for a waiting thread  its priority is boosted to between 50 and 59  thus supporting the scheduling policy of providing good response time for interactive processes  threads in the real-time class are given the highest priority this assignment allows a real-time process to have a guaranteed response from the system within a bounded period of time a real-time process will run before a process in any other class in general  however  few processes belong to the real-time class  solaris uses the system class to run kernel threads  such as the scheduler and paging daemon once established  the priority of a system thread does not change the system class is reserved for kernel use  user processes rum1ing in kernel mode are not in the system class   the fixed-priority and fair-share classes were introduced with solaris 9  threads in the fixed-priority class have the same priority range as those in the time-sharing class ; however  their priorities are not dynamically adjusted  the fair-share scheduling class uses cpu instead of priorities to make scheduling decisions cpu shares indicate entitlement to available cpu resources and are allocated to a set of processes  known as a project   each scheduling class includes a set of priorities however  the scheduler converts the class-specific priorities into global priorities and selects the thread with the highest global priority to n.m the selected thread n.ms on the cpu until it  1  blocks   2  uses its time slice  or  3  is preempted by a higher-priority thread if there are multiple threads with the same priority  the scheduler uses a round-robin queue figure 5.13 illustrates how the six scheduling classes relate to one another and how they map to global priorities notice that the kernel maintains 10 threads for servicing interrupts these threads do not belong to any scheduling class and execute at the highest priority  160-169   as mentioned  solaris has traditionally used the many-to-many model  section 4.2.3  but switched to the one-to-one model  section 4.2.2  beginning with solaris 9  5.6.2 example  windows xp scheduling windows xp schedules threads using a priority-based  preemptive scheduling algorithm the windows xp scheduler ensures that the highest-priority thread will always run the portion of the windows xp kernel that handles scheduling is called the dispatcher a thread selected to run by the dispatcher will run until it is preempted by a higher-priority thread  until it terminates  until its time quantum ends  or until it calls a blocking system call  such as for i/0 if a higher-priority real-time thread becomes ready while a lower-priority thread is running  the lower-priority thread will be preempted this preemption gives a real-time thread preferential access to the cpu when the thread needs such access  the dispatcher uses a 32-level priority scheme to determine the order of thread execution priorities are divided into two classes the global priority highest lowest 169 160 159 100 99 60 59 0 5.6 figure 5.13 solaris scheduling  scheduling order first last 209 contains threads having priorities from 1 to 15  and the contains threads with priorities ranging from 16 to 31  there is also a thread running at priority 0 that is used for memory management  the dispatcher uses a queue for each scheduling priority and traverses the set of queues from highest to lowest until it finds a thread that is ready to run if no thread is found  the dispatcher will execute a special thread called the there is a relationship between the numeric priorities of the windows xp kernel and the win32 api the win32 api identifies several priority classes to which a process can belong these include  realtime_priority _class higf-lpriority _class abovknormalpriority class normalpriority class 210 chapter 5 .15 12 10 14 11 9 13 10 8 12 9 7 22 1.1 8 6 16 figure 5.14 windows xp priorities  below normal...priority _class idle...priority _class 8 6 7 5 6 4 5 3 4 2 priorities in all classes except the realtime...priority _class are variable  meaning that the priority of a thread belonging to one of these classes can change  a thread within a given priority classes also has a relative priority the values for relative priorities include  time_critical highest above_normal normal below normal lowest idle the priority of each thread is based on both the priority class it belongs to and its relative priority within that class this relationship is shown in figure 5.14 the values of the priority classes appear in the top row the left column contains the values for the relative priorities for example  if the relative priority of a thread in the above_normal...priority_class is normal  the nunceric priority of that thread is 10  furthermore  each thread has a base priority representing a value in the priority range for the class the thread belongs to by default  the base priority is the value of the normal relative priority for that class the base priorities for each priority class are  realtime...priority_class-24 higrlpriority class-13 5.6 above_normalpriority_class-10 normalpriority _class-8 below _normalpriority _class-6 idle_priority _class-4 211 processes are typically members of the normalpriority_class a process belongs to this class unless the parent of the process was of the idle_priority _class or unless another class was specified when the process was created the initial priority of a thread is typically the base priority of the process the thread belongs to  when a thread 's time quantun1 runs out  that thread is interrupted ; if the thread is in the variable-priority class  its priority is lowered the priority is never lowered below the base priority  however lowering the priority tends to limit the cpu consumption of compute-bound threads when a variablepriority thread is released from a wait operation  the dispatcher boosts the priority the amount of the boost depends on what the thread was waiting for ; for example  a thread that was waiting for keyboard i/0 would get a large increase  whereas a thread waiting for a disk operation would get a moderate one this strategy tends to give good response times to interactive threads that are using the mouse and windows it also enables i/o-bound threads to keep the i/0 devices busy while permitting compute-bound threads to use spare cpu cycles in the background this strategy is used by several time-sharing operating systems  including unix in addition  the window with which the user is currently interacting receives a priority boost to enhance its response time  when a user is running an interactive program  the system needs to provide especially good performance for this reason  windows xp has a special scheduling rule for processes in the normalpriority_class windows xp distinguishes between the foreground process that is currently selected on the screen and the background processes that are not currently selected when a process moves into the foreground  windows xp increases the scheduling quantum by some factor-typically by 3 this increase gives the foreground process three times longer to run before a time-sharing preemption occurs  5.6.3 example  linux scheduling prior to version 2.5  the linux kernel ran a variation of the traditional unix scheduling algorithm two problems with the traditional unix scheduler are that it does not provide adequate support for smp systems and that it does not scale well as the number of tasks on the system grows with version 2.5  the scheduler was overhauled  and the kernel now provides a scheduling algorithm that runs in constant time-known as 0  1  -regardless of the number of tasks on the system the new scheduler also provides increased support for smp  including processor affinity and load balancing  as well as providing fairness and support for interactive tasks  the linux scheduler is a preemptive  priority-based algorithm with two separate priority ranges  a real-time range from 0 to 99 and a nice value ranging from 100 to 140 these two ranges map into a global priority scheme wherein numerically lower values indicate higher priorities  212 chapter 5 numeric priority 0 99 100 140 relative priority highest lowest time quantum 200 ms 10 ms figure 5.15 the relationship between priorities and time-slice length  unlike schedulers for many other systems  including solaris  section 5.6.1  and windows xp  section 5.6.2   lim1x assigns higher-priority tasks longer time quanta and lower-priority tasks shorter time quanta the relationship between priorities and tim.e-slice length is shown in figure 5.15  a runnable task is considered eligible for execution on the cpu as long as it has time remaining in its time slice when a task has exhausted its time slice  it is considered expired and is not eligible for execution again until all other tasks have also exhausted their time quanta the kernel maintains a list of all runnable tasks in a data structure because of its support for smp  each processor maintains its own nmqueue and schedules itself independently  each runqueue contains two priority arrays  and the active array contains all tasks with time remaining in their time slices  and the expired array contains all expired tasks each of these priority arrays contains a list of tasks indexed according to priority  figure 5.16   the scheduler chooses the task with the highest priority from the active array for execution on the cpu on multiprocessor machines  this means that each processor is scheduling the highest-priority task from its own runqueue structure when all tasks have exhausted their time slices  that is  the active array is empty   the two priority arrays are exchanged ; the expired array becomes the active array  and vice versa  linux implements real-time scheduling as defined by posix.1b  which is described in section 5.4.2 real-time tasks are assigned static priorities all active array priority  0   1   140  task lists o-o 0--0--0 0 expired array priority  0   1   140  task lists o--o-o 0 figure 5.16 list of tasks indexed according to priority  5.7 5.7 213 other tasks have dynamic priorities that are based on their nice values plus or minus the value 5 the interactivity of a task determines whether the value 5 will be added to or subtracted from the nice value a task 's interactivity is deterncined by how long it has been sleeping while waiting for i/0 tasks that are more interactive typically have longer sleep times and therefore are more likely to have adjustments closer to -5  as the scheduler favors interactive tasks the result of such adjustments will be higher priorities for these tasks  conversely  tasks with shorter sleep times are often more cpu-bound and thus will have their priorities lowered  a task 's dynamic priority is recalculated when the task has exhausted its time quantum and is to be moved to the expired array thus  when the two arrays are exchanged  all tasks in the new active array have been assigned new priorities and corresponding time slices  how do we select a cpu-scheduling algorithm for a particular system as we saw in section 5.3  there are many scheduling algorithms  each with its own parameters as a result  selecting an algorithm can be difficult  the first problem is defining the criteria to be used in selecting an algorithm  as we saw in section 5.2  criteria are often defined in terms of cpu utilization  response time  or thxoughput to select an algorithm  we must first define the relative importance of these elements our criteria may include several measures  such as  maximizing cpu utilization under the constraint that the maximum response time is 1 second maximizing throughput such that turnaround time is  on average  linearly proportional to total execution time once the selection criteria have been defined  we want to evaluate the algorithms under consideration we next describe the various evaluation methods we can use  5.7.1 deterministic modeling one major class of evaluation methods is analytic evaluation analytic evaluation uses the given algorithm and the system workload to produce a formula or number that evaluates the performance of the algorithm for that workload  deterministic modeling is one type of analytic evaluation this method takes a particular predetermined workload and defines the performance of each algorithm for that workload for example  assume that we have the workload shown below all five processes arrive at time 0  in the order given  with the length of the cpu burst given in milliseconds  214 chapter 5 process burst time  ~ pj 10 p2 29 po c  j 0 p4 7 ps 12 consider the fcfs  sjf  and rr  quantum = 10 milliseconds  scheduling algorithms for this set of processes which algorithm would give the minimum average waiting time for the fcfs algorithm  we would execute the processes as 0 10 39 42 49 61 the waiting time is 0 milliseconds for process p1  10 milliseconds for process p2  39 milliseconds for process p3  42 milliseconds for process p4  and 49 milliseconds for process p5  thus  the average waiting time is  0 + 10 + 39 + 42 + 49  /5 = 28 milliseconds  with nonpreemptive sjf scheduling  we execute the processes as 0 3 10 20 p 5 32 61 the waiting time is 10 milliseconds for process p11 32 milliseconds for process p2  0 milliseconds for process p3  3 milliseconds for process p4  and 20 milliseconds for process p5  thus  the average waiting time is  10 + 32 + 0 + 3 + 20  i 5 = 13 milliseconds  0 with the rr algorithm  we execute the processes as p 1 10 20 23 30 40 50 52 61 the waiting time is 0 milliseconds for process p1  32 milliseconds for process p2  20 milliseconds for process p3  23 milliseconds for process p4  and 40 milliseconds for process p5  thus  the average waiting time is  0 + 32 + 20 + 23 + 40  /5 = 23 milliseconds  we see that  in this case  the average waiting time obtained with the sjf policy is less than half that obtained with fcfs scheduling ; the rr algorithm gives us an intermediate value  deterministic modeling is simple and fast it gives us exact numbers  allowing us to compare the algorithms however  it requires exact numbers for input  and its answers apply only to those cases the main uses of deterministic modeling are in describing scheduling algorithms and providing examples in 5.7 215 cases where we are running the same program over and over again and can measure the program 's processing requirements exactly  we may be able to use deterministic modeling to select a scheduling algorithm furthermore  over a set of examples  deterministic modeling may indicate trends that can then be analyzed and proved separately for example  it can be shown that  for the environment described  all processes and their times available at tirne 0   the sjf policy will always result in the rninimum waiting time  5.7.2 queueing models on many systems  the processes that are run vary from day to day  so there is no static set of processes  or times  to use for deterministic modeling what can be determined  however  is the distribution of cpu and i/0 bursts these distributions can be measured and then approximated or simply estimated the result is a mathematical formula describing the probability of a particular cpu burst commonly  this distribution is exponential and is described by its mean  similarly  we can describe the distribution of times when processes arrive in the system  the arrival-time distribution   fron1 these two distributions  it is possible to compute the average throughput  utilization  waiting time  and so on for most algorithms  the computer system is described as a network of servers each server has a queue of waiting processes the cpu is a server with its ready queue  as is the i/0 system with its device queues knowing arrival rates and service rates  we can compute utilization  average queue length  average wait time  and so on this area of study is called queueing-network analysis  as an example  let n be the average queue length  excluding the process being serviced   let w be the average waiting time in the queue  and let a be the average arrival rate for new processes in the queue  such as three processes per second   we expect that during the time w that a process waits  a x w new processes will arrive in the queue if the system is in a steady state  then the number of processes leaving the queue must be equal to the number of processes that arrive thus  n = ax w  this equation  known as little 's formula  is particularly useful because it is valid for any scheduling algorithm and arrival distribution  we can use little 's formula to compute one of the three variables if we know the other two for example  if we know that 7 processes arrive every second  on average   and that there are normally 14 processes in the queue  then we can compute the average waiting time per process as 2 seconds  queueing analysis can be useful in comparing scheduling algorithms  but it also has limitations at the moment  the classes of algorithms and distributions that can be handled are fairly limited the mathematics of complicated algorithms and distributions can be difficult to work with thus  arrival and service distributions are often defined in mathematically tractable -but unrealistic-ways it is also generally necessary to make a number of independent assumptions  which may not be accurate as a result of these difficulties  queueing models are often only approximations of real systems  and the accuracy of the computed results may be questionable  216 chapter 5 performance statistics for fcfs performance statistics for sjf performance statistics for rr  q = 14  figure 5.17 evaluation of cpu schedulers by simulation  5.7.3 simulations to get a more accurate evaluation of scheduling algorithms  we can use simulations rumung simulations involves programming a model of the computer system software data structures represent the major components of the system the simulator has a variable representing a clock ; as this variable 's value is increased  the simulator modifies the system state to reflect the activities of the devices  the processes  and the scheduler as the simulation executes  statistics that indicate algorithm performance are gathered and printed  the data to drive the simulation can be generated in several ways the most common method uses a random-number generator that is programmed to generate processes  cpu burst times  arrivals  departures  and so on  according to probability distributions the distributions can be defined mathematically  uniform  exponential  poisson  or empirically if a distribution is to be defined empirically  measurements of the actual system under study are taken the results define the distribution of events in the real system ; this distribution can then be used to drive the simulation  a distribution-driven simulation may be inaccurate  however  because of relationships between successive events in the real system the frequency distribution indicates only how many instances of each event occur ; it does not indicate anything about the order of their occurrence to correct this problem  we can use trace tapes we create a trace tape by monitoring the real system and recording the sequence of actual events  figure 5.17   we then use this sequence to drive the simulation trace tapes provide an excellent way to compare two algorithms on exactly the same set of real inputs this method can produce accurate results for its inputs  simulations can be expensive  often requiring hours of computer time a more detailed simulation provides more accurate results  but it also takes more computer time in addition  trace tapes can require large amounts of storage 5.8 5.8 217 space finally  the design  coding  and debugging of the simulator can be a major task  5.7.4 implementation even a simulation is of limited accuracy the only con'lpletely accurate way to evaluate a scheduling algorithm is to code it up  put it in the operating system  and see how it works this approach puts the actual algorithm in the real system for evaluation under real operating conditions  the major difficulty with this approach is the high cost the expense is incurred not only in coding the algorithm and modifying the operating system to support it  along with its required data structures  but also in the reaction of the users to a constantly changing operating system most users are not interested in building a better operating system ; they merely want to get their processes executed and use their results a constantly changing operating system does not help the users to get their work done  another difficulty is that the environment in which the algorithm is used will change the environment will change not only in the usual way  as new programs are written and the types of problems change  but also as a result of the performance of the scheduler if short processes are given priority  then users may break larger processes into sets of smaller processes if interactive processes are given priority over noninteractive processes  then users may switch to interactive use  for example  researchers designed one system that classified interactive and noninteractive processes automatically by looking at the amount of terminal i/0 if a process did not input or output to the terminal in a 1-second interval  the process was classified as noninteractive and was moved to a lower-priority queue in response to this policy  one programmer modified his programs to write an arbitrary character to the terminal at regular intervals of less than 1 second the system gave his programs a high priority  even though the terminal output was completely meaningless  the most flexible scheduling algorithms are those that can be altered by the system managers or by the users so that they can be tuned for a specific application or set of applications a workstation that performs high-end graphical applications  for instance  may have scheduling needs different from those of a web server or file server some operating systemsparticularly several versions of unix-allow the system manager to fine-tune the scheduling parameters for a particular system configuration for example  solaris provides the dispadmin command to allow the system administrator to modify the parameters of the scheduling classes described  in section 5.6.1  another approach is to use apis that modify the priority of a process or thread the java  /posix  and /winapi/ provide such functions the downfall of this approach is that performance-tuning a system or application most often does not result in improved performance in more general situations  cpu scheduling is the task of selecting a waiting process from the ready queue and allocating the cpu to it the cpu is allocated to the selected process by the dispatcher  218 chapter 5 first-come  first-served  fcfs  scheduling is the simplest scheduling algorithm  but it can cause short processes to wait for very long processes shortestjob first  sjf  scheduling is provably optimal  providing the shortest average waiting time implementing sjf scheduling is difficult  howeve1 ~ because predicting the length of the next cpu burst is difficult the sjf algorithm is a special case of the general priority scheduling algorithm  which simply allocates the cpu to the highest-priority process both priority and sjf scheduling may suffer from starvation aging is a technique to prevent starvation  round-robin  rr  scheduling is more appropriate for a time-shared  interactive  system rr scheduling allocates the cpu to the first process in the ready queue for q time units  where q is the time quantum after q time units  if the process has not relinquished the cpu  it is preem.pted  and the process is put at the tail of the ready queue the major problem is the selection of the time quantum if the quantum is too large  rr scheduling degenerates to fcfs scheduling ; if the quantum is too small  scheduling overhead in the form of context-switch time becomes excessive  the fcfs algorithm is nonpreemptive ; the rr algorithm is preemptive the sjf and priority algorithms may be either preemptive or nonpreemptive  multilevel queue algorithms allow different algorithms to be used for different classes of processes the most common model includes a foreground interactive queue that uses rr scheduling and a background batch queue that uses fcfs scheduling multilevel feedback queues allow processes to move from one queue to another  many contemporary computer systems support multiple processors and allow each processor to schedule itself independently typically  each processor maintains its own private queue of processes  or threads   all of which are available to run additional issues related to multiprocessor scheduling include processor affinity  load balancing  and multicore processing as well as scheduling on virtualization systems  operating systems supporting threads at the kernel level must schedule threads-not processes-for execution this is the case with solaris and windows xp both of these systems schedule threads using preemptive  priority-based scheduling algorithms  including support for real-time threads  the linux process scheduler uses a priority-based algorithm with real-time support as well the scheduling algorithms for these three operating systems typically favor interactive over batch and cpu-bound processes  the wide variety of scheduling algorithms demands that we have methods to select among algorithms analytic methods use mathematical analysis to determine the performance of an algorithm simulation methods determine performance by imitating the scheduling algorithm on a representative sample of processes and computing the resulting performance however  simulation can at best provide an approximation of actual system performance ; the only reliable technique for evaluating a scheduling algorithm is to implencent the algorithm on an actual system and monitor its performance in a real-world environment  5.1 why is it important for the scheduler to distinguish t /0-bound programs from cpu-bound programs 219 5.2 a cpu-scheduling algorithm determines an order for the execution of its scheduled processes given n processes to be scheduled on one processor  how many different schedules are possible give a formula in tentls of n  5.3 consider a systenc running ten i/o-bound tasks and one cpu-bound task assume that the i/o-bound tasks issue an i/o operation once for every millisecond of cpu computing and that each i/0 operation takes 10 milliseconds to complete also assume that the context-switching overhead is 0.1 millisecond and that all processes are long-running tasks  describe the cpu utilization for a round-robin scheduler when  a the time quantum is 1 millisecond b the time quantum is 10 milliseconds 5.4 what advantage is there in having different time-quantum sizes at different levels of a multilevel queueing system 5.5 consider a system implementing multilevel queue scheduling what strategy can a computer user employ to maximize the amount of cpu time allocated to the user 's process 5.6 consider the scheduling algorithm in the solaris operating system for time-sharing threads  a what is the time quantum  in milliseconds  for a thread with priority 10 with priority 55 b assume that a thread with priority 35 has used its entire time quantum without blocking what new priority will the scheduler assign this thread c assume that a thread with priority 35 blocks for i/0 before its time quantum has expired what new priority will the scheduler assign this thread 5.7 explain the differences in how much the following scheduling algorithms discriminate in favor of short processes  a fcfs b rr c multilevel feedback queues 5.8 consider the exponential average formula used to predict the length of the next cpu burst what are the implications of assigning the following values to the parameters used by the algorithm a ex = 0 and to = 100 milliseconds b ex = 0.99 and to = 10 milliseconds 220 chapter 5 5.9 which of the following scheduling algorithms could result in starvation a first-come  first-served b shortest job first c round robin d priority 5.10 suppose that a scheduling algorithm  at the level of short-term cpu scheduling  favors those processes that have used the least processor time in the recent past why will this algorithm favor i/o-bound programs and yet not permanently starve cpu-bound programs 5.11 using the windows xp scheduling algorithm  determine the numeric priority of each of the following threads  a a thread in the realtimeyriority _class with a relative priority of highest b a thread in the normalyriority_class with a relative priority of normal c a thread in the highyriority _class with a relative priority of above..normal 5.12 consider a variant of the rr scheduling algorithm in which the entries in the ready queue are pointers to the pcbs  a what would be the effect of putting two pointers to the same process in the ready queue b what would be two major advantages and two disadvantages of this scheme c how would you modify the basic rr algorithm to achieve the same effect without the duplicate pointers 5.13 consider the following set of processes  with the length of the cpu burst given in milliseconds  process burst time priority  pt 10 3 p2 1 1 p3 2 3 p4 1 4 ps 5 2 221 the processes are assumed to have arrived in the order p1  p2  p3  p4  ps  all at time 0  a draw four gantt charts that illustrate the execution of these processes using the following scheduling algorithms  fcfs  sjf  nonpreemptive priority  a smaller priority number implies a higher priority   and rr  quantum = 1   b what is the turnaround time of each process for each of the scheduling algorithms in part a c what is the waiting ti1r1e of each process for each of these scheduling algorithms d which of the algorithms results in the minimum average waiting time  over all processes  5.14 the traditional unix scheduler enforces an inverse relationship between priority numbers and priorities  the higher the numbe1 ~ the lower the priority the scheduler recalculates process priorities once per second using the following function  priority =  recent cpu usage i 2  + base where base = 60 and recent cpu usage refers to a value indicating how often a process has used the cpu since priorities were last recalculated  assume that recent cpu usage for process p1 is 40  for process p2 is 18  and for process p3 is 10 what will be the new priorities for these three processes when priorities are recalculated based on this information  does the traditional unix scheduler raise or lower the relative priority of a cpu-bound process 5.15 discuss how the following pairs of scheduling criteria conflict in certain settings  a cpu utilization and response time b average turnaround time and maximum waiting time c i/0 device utilization and cpu utilization 5.16 consider a preemptive priority scheduling algorithm based on dynamically changing priorities larger priority numbers imply higher priority  when a process is waiting for the cpu  in the ready queue  but not running   its priority changes at a rate a ; when it is running  its priority changes at a rate ~  all processes are given a priority of 0 when they enter the ready queue the parameters a and ~ can be set to give many different scheduling algorithms  a what is the algorithm that results from ~ a 0 b what is the algorithm that results from a ~ 0 5.17 suppose that the following processes arrive for execution at the times indicated each process will run for the amount of time listed in answering the questions  use nonpreemptive scheduling  and base all 222 chapter 5 decisions on the information you have at the time the decision must be made  process arrival time burst time  pl 0.0 8 p2 0.4 4 p3 1.0 1 a what is the average turnaround time for these processes with the fcfs scheduling algorithm b what is the average turnaround time for these processes with the sjf scheduling algorithm c the sjf algorithm is supposed to improve performance  but notice that we chose to run process p1 at time 0 because we did not k11ow that two shorter processes would arrive soon compute what the average turnaround time will be if the cpu is left idle for the first 1 unit and then sjf scheduling is used remember that processes p1 and p2 are waiting durirtg this idle time  so their waiting time may increase this algorithm could be known as future-knowledge scheduling  feedback queues were originally implemented on the ctss system described in corbato et al  1962   this feedback queue scheduling system was analyzed by schrage  1967   the preemptive priority scheduling algorithm of exercise 5.16 was suggested by kleinrock  1975   anderson et al  1989   lewis and berg  1998   and philbin et al  1996  discuss thread scheduling multicore scheduling is examined in mcnairy and bhatia  2005  and kongetira et al  2005   scheduling techniques that take into account information regarding process execution times from previous runs are described in fisher  1981   hall et al  1996   and lowney et al  1993   fair-share schedulers are covered by henry  1984   woodside  1986   and kay and la uder  1988   scheduling policies used in the unix v operating system are described by bach  1987  ; those for unix freebsd 5.2 are presented by mckusick and neville-neil  2005  ; and those for the mach operating system are discussed by black  1990   love  2005  covers scheduling in lim.ix details of the ule scheduler can be found in roberson  2003   solaris scheduling is described by mauro and mcdougall  2007   solomon  1998   solomon and russinovich  2000   and russinovich and solomon  2005  discuss scheduling in windows internals butenhof  1997  and lewis and berg  1998  describe scheduling in pthreads systems siddha et al  2007  discuss scheduling challenges on multicore systems  part three 6.1 c er a cooperating process is one that can affect or be affected by other processes executing in the system cooperating processes can either directly share a logical address space  that is  both code and data  or be allowed to share data only through files or messages the former case is achieved through the use of threads  discussed in chapter 4 concurrent access to shared data may result in data inconsistency  however in this chapter  we discuss various mechanisms to ensure the orderly execution of cooperating processes that share a logical address space  so that data consistency is maintained  to introduce the critical-section problem  whose solutions can be used to ensure the consistency of shared data  to present both software and hardware solutions of the critical-section problem  to introduce the concept of an atomic transaction and describe mechanisms to ensure atomicity  in chapter 3  we developed a model of a system consisting of cooperating sequential processes or threads  all running asynchronously and possibly sharing data we illustrated this model with the producer-consumer problem  which is representative of operating systems specifically  in section 3.4.1  we described how a bounded buffer could be used to enable processes to share memory  let 's return to our consideration of the bounded buffer as we pointed out  our original solution allowed at most buffer_size  1 items in the buffer at the same time suppose we want to modify the algorithm to remedy this deficiency one possibility is to add an integer variable counter  initialized to 0 counter is incremented every time we add a new item to the buffer and is 225 226 chapter 6 decremented every time we remove one item from the buffer the code for the producer process can be modified as follows  while  true    i produce an item in nextproduced i while  counter = = buffer_size  ; i do nothing i buffer  in  = nextproduced ; in =  in + 1  % buffer_size ; counter + + ; the code for the consumer process can be modified as follows  while  true    while  counter = = 0  ; i do nothing i nextconsumed = buffer  out  ; out =  out + 1  % buffer_size ; counter ; i consume the item in nextconsumed i although both the producer and consumer routines shown above are correct separately  they may not function correctly when executed concurrently  as an illustration  suppose that the value of the variable counter is currently 5 and that the producer and consumer processes execute the statements counter + + and counter concurrently following the execution of these two statements  the value of the variable counter may be 4  5  or 6 ! the only correct result  though  is counter = = 5  which is generated correctly if the producer and consumer execute separately  we can show that the value of counter may be incorrect as follows note that the statement counter + + may be implemented in machine language  on a typical machine  as register1 = counter register1 = register1 + 1 counter = register1 where register1 is one of the local cpu registers similarly  the statement register2 counter is implemented as follows  register2 = counter register2 = register2 ~ 1 counter = register2 where again register2 is on eo the local cpu registers even though register1 and register2 may be the same physical register  an accumulator  say   remember that the contents of this register will be saved and restored by the interrupt handler  section 1.2.3   6.2 6.2 227 the concurrent execution of counter + + and counter is equivalent to a sequential execution in which the lower-level statements presented previously are interleaved in some arbitrary order  but the order within each high-level statement is preserved   one such interleaving is to  producer execute register1 = counter  register1 = 5  t1  producer execute register1 = register1 + 1  register1 = 6  t2  consumer execute register2 = counter  register2 = 5  t3  consumer execute register2 = register2 1  register2 = 4  t4  producer execute counter = register1  counter = 6  ts  consumer execute counter = register2  counter = 4  notice that we have arrived at the incorrect state counter = = 4  indicating that four buffers are full  when  in fact  five buffers are full if we reversed the order of the statements at t4 and t5  we would arrive at the incorrect state counter = = 6  we would arrive at this incorrect state because we allowed both processes to manipulate the variable counter concurrently a situation like this  where several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place  is called a to guard against the race condition above  we need to ensure that only one process at a time can be manipulating the variable counter to make such a guarantee  we require that the processes be synchronized in some way  situations such as the one just described occur frequently in operating systems as different parts of the system manipulate resources furthermore  with the growth of multicore systems  there is an increased emphasis on developing multithreaded applications wherein several threads-which are quite possibly sharing data-are rmming in parallel on different processing cores clearly  we want any changes that result from such activities not to interfere with one another because of the importance of this issue  a major portion of this chapter is concerned with and amongst cooperating processes  consider a system consisting of n processes  po  p1    p11 _ i   each process has a segment of code  called a cdticall in which the process may be changing common variables  updating a table  writing a file  and so on  the important feature of the system is that  when one process is executing in its critical section  no other process is to be allowed to execute in its critical section that is  no two processes are executing in their critical sections at the same time the critical-section problem is to design a protocol that the processes can use to cooperate each process must request permission to enter its critical section the section of code implementing this request is the the critical section may be followed by an exit the remaining code is the the general structure of a typical process pi is shown in 228 chapter 6 do  i entry section i critical section i exit section i remainder section  while  true  ; figure 6.1 general structure of a typical process a  figure 6.1 the entry section and exit section are enclosed in boxes to highlight these important segments of code  a solution to the critical-section problem must satisfy the following three requirements  1 mutual exclusion if process pi is executing in its critical section  then no other processes can be executing in their critical sections  2 progress if no process is executing in its critical section and some processes wish to enter their critical sections  then only those processes that are not executing in their remainder sections can participate in deciding which will enter its critical section next  and this selection carmot be postponed indefinitely  bounded waiting there exists a bound  or limit  on the number of times that other processes are allowed to enter their critical sections after a process has made a request to enter its critical section and before that request is granted  we assume that each process is executing at a nonzero speed however  we can make no assumption concerning the relative of the n processes  at a given point in time  many kernel-mode processes may be active in the operating system as a result  the code implementing an operating system  kernel code  is subject to several possible race conditions consider as an example a kernel data structure that maintains a list of all open files in the system this list must be modified when a new file is opened or closed  adding the file to the list or removing it from the list   if two processes were to open files simultaneously  the separate updates to this list could result in a race condition  other kernel data structures that are prone to possible race conditions include structures for maintaining memory allocation  for maintaining process lists  and for interrupt handling it is up to kernel developers to ensure that the operating system is free from such race conditions  two general approaches are used to handle critical sections in operating systems   1  preemptive kernels and  2  nonpreemptive kernels a preemptive kernel allows a process to be preempted while it is running in kernel mode  a nonpreemptive kernel does not allow a process running in kernel mode 6.3 6.3 229 to be preempted ; a kernel-mode process will run until it exits kernel mode  blocks  or voluntarily yields control of the cpu obviously  a nonpreemptive kernel is essentially free from race conditions on kernel data structures  as only one process is active in the kernel at a time we can not say the same about preemptive kernels  so they must be carefully designed to ensure that shared kernel data are free from race conditions preemptive kernels are especially difficult to design for smp architectures  since in these environments it is possible for two kernel-mode processes to run simultaneously on different processors  why  then  would anyone favor a preemptive kernel over a nonpreemptive one a preemptive kernel is more suitable for real-time programming  as it will allow a real-time process to preempt a process currently running in the kernel  furthermore  a preemptive kernel may be more responsive  since there is less risk that a kernel-mode process will run for an arbitrarily long period before relinquishing the processor to waiting processes of course  this effect can be minimized by designing kernel code that does not behave in this way later in this chapter  we explore how various operating systems manage preemption within the kernel  next  we illustrate a classic software-based solution to the critical-section problem known as peterson 's solution because of the way modern computer architectures perform basic machine-language instructions  such as load and store  there are no guarantees that peterson 's solution will work correctly on such architectures howeve1 ~ we present the solution because it provides a good algorithmic description of solving the critical-section problem and illustrates some of the complexities involved in designing software that addresses the requirements of mutual exclusion  progress  and bomcded waiting  peterson 's solution is restricted to two processes that alternate execution between their critical sections and remainder sections the processes are numbered po and p1 for convenience  when presenting pi  we use pj to denote the other process ; that is  j equals 1  i  peterson 's solution requires the two processes to share two data items  int turn ; boolean flag  2  ; the variable turn indicates whose turn it is to enter its critical section that is  if turn = = i  then process pi is allowed to execute in its critical section the flag array is used to indicate if a process is ready to enter its critical section  for example  if flag  i  is true  this value indicates that pi is ready to enter its critical section with an explanation of these data structures complete  we are now ready to describe the algorithm shown in figure 6.2  to enter the critical section  process pi first sets flag  i  to be true and then sets turn to the value j  thereby asserting that if the other process wishes to enter the critical section  it can do so if both processes try to enter at the same time  turn will be set to both i and j at roughly the sance time only one of these assignments will last ; the other will occur but will be overwritten immediately  230 chapter 6 do  flag  i  = true ; turn = j ; while  flag  j  && turn j  ; critical section i flag  i  = false ; i remainder section  while  true  ; figure 6.2 the structure of process a in peterson 's solution  the eventual value of turn determines which of the two processes is allowed to enter its critical section first  we now prove that this solution is correct we need to show that  mutual exclusion is preserved  the progress requirement is satisfied  the bounded-waiting requirement is met  to prove property 1  we note that each p ; enters its critical section only if either flag  j  = = false or turn = = i also note that  if both processes can be executing in their critical sections at the same time  then flag  0  = = flag  1  = = true these two observations imply that po and p1 could not have successfully executed their while statements at about the same time  since the value of turn can be either 0 or 1 but camwt be both hence  one of the processes -say  pi -must have successfully executed the while statencent  whereas p ; had to execute at least one additional statement  turn = = j   however  at that time  flag  j  = = true and turn = = j  and this condition will persist as long as pi is in its critical section ; as a result  mutual exclusion is preserved  to prove properties 2 and 3  we note that a process p ; can be prevented from entering the critical section only if it is stuck in the while loop with the condition flag  j  = = true and turn = = = j ; this loop is the only one possible if pi is not ready to enter the critical section  then flag  j  = = false  and p ; can enter its critical section if pj has set flag  j  to true and is also executing in its while statement  then either turn = = = i or turn = = = j if turn = = i  then p ; will enter the critical section if turn = = j  then pi will enter the critical section however  once pi exits its critical section  it will reset flag  j  to false  allowing p ; to enter its critical section if pi resets flag  j  to true  it must also set turn to i  thus  since p ; does not change the value of the variable turn while executing the while statement  p ; will enter the critical section  progress  after at most one entry by p1  bounded waiting   6.4 6.4 231 do  acquire lock critical section i release lock i remainder section  while  true  ; figure 6.3 solution to the critical-section problem using locks  we have just described one software-based solution to the critical-section problem however  as mentioned  software-based solutions such as peterson 's are not guaranteed to work on modern computer architectures instead  we can generally state that any solution to the critical-section problem requires a simple tool-a lock race conditions are prevented by requiring that critical regions be protected by locks that is  a process must acquire a lock before entering a critical section ; it releases the lock when it exits the critical section  this is illustrated in figure 6.3  in the following discussions  we explore several more solutions to the critical-section problem using techniques ranging from hardware to softwarebased apis available to application programmers all these solutions are based on the premise of locking ; however  as we shall see  the designs of such locks can be quite sophisticated  we start by presenting some simple hardware instructions that are available on many systems and showing how they can be used effectively in solving the critical-section problem hardware features can make any programming task easier and improve system efficiency  the critical-section problem could be solved simply in a uniprocessor environment if we could prevent interrupts from occurring while a shared variable was being modified in this manner  we could be sure that the current sequence of instructions would be allowed to execute in order without preemption no other instructions would be run  so no unexpected modifications could be made to the shared variable this is often the approach taken by nonpreemptive kernels  unfortunately  this solution is not as feasible in a multiprocessor environment  disabling interrupts on a multiprocessor can be time consuming  as the boolean testandset  boolean target   boolean rv = target ; target = true ; return rv ;  figure 6.4 the definition of the testandset   instruction  232 chapter 6 do  while  testandset  &lock   ; ii do nothing ii critical section lock = false ; ii remainder section  while  true  ; figure 6.5 mutual-exclusion implementation with testandset    message is passed to all the processors this message passing delays entry into each critical section  and system efficiency decreases also consider the effect on a system 's clock if the clock is kept updated by interrupts  many modern computer systems therefore provide special hardware instructions that allow us either to test and modify the content of a word or to swap the contents of two words is  as one unin.terruptible unit we can use these special instructions to solve the critical-section problem in a relatively simple manner rather than discussing one specific instruction for one specific machine  we abstract the main concepts behind these types of instructions by describing the testandset   and swap   instructions  the testandset   instruction can be defined as shown in figure 6.4 the important characteristic of this instruction is that it is executed atomically  thus  if two testandset   instructions are executed simultaneously  each on a different cpu   they will be executed sequentially in some arbitrary order if the machine supports the testandset   instruction  then we can implement mutual exclusion by declaring a boolean variable lock  initialized to false  the structure of process p ; is shown in figure 6.5  the swap   instruction  in contrast to the testandset   instruction  operates on the contents of two words ; it is defined as shown in figure 6.6  like the testandset   instruction  it is executed atomically if the machine supports the swap   instruction  then mutual exclusion can be provided as follows a global boolean variable lock is declared and is initialized to false  in addition  each process has a local boolean variable key the structure of process p ; is shown in figure 6.7  although these algorithms satisfy the mutual-exclusion requirement  they do not satisfy the bounded-waiting requirement in figure 6.8  we present another algorithm using the testandset   instruction that satisfies all the critical-section requirements the common data structures are void swap  boolean a  boolean b   boolean temp = a ; a b ; b = temp ;  figure 6.6 the definition of the swap   instruction  6.4 do  key = true ; while  key = = true  swap  &lock  &key  ; ii critical section lock = false ; ii remainder section  while  true  ; figure 6.7 mutual-exclusion implementation with the swap   instruction  boolean waiting  n  ; boolean lock ; 233 these data structures are initialized to false to prove that the mutualexclusion requirement is met  we note that process p ; can enter its critical section only if either waiting  i  = = false or key = = false the value of key can become false only if the testandset   is executed the first process to execute the testandset   will find key = = false ; all others must wait the variable waiting  i  can become false only if another process leaves its critical section ; only one waiting  i  is set to false  maintaining the mutual-exclusion requirement  do  waiting  i  = true ; key = true ; while  waiting  i  && key  key = testandset  &lock  ; waiting  i  = false ; ii critical section j =  i + 1  % n ; while   j ! = i  && ! waiting  j   j =  j + 1  % n ; if  j = = i  lock = false ; else waiting  j  = false ; ii remainder section  while  true  ; figure 6.8 bounded-waiting mutual exclusion with testandset    234 chapter 6 6.5 to prove that the progress requirement is met  we note that the arguments presented for mutual exclusion also apply here  since a process exiting the critical section either sets lock to false or sets waiting  j  to false both allow a process that is waiting to enter its critical section to proceed  to prove that the bounded-waiting requirement is met  we note that  when a process leaves its critical section  it scans the array waiting in the cyclic ordering  i + 1  i + 2    n 1  0    i 1   it designates the first process in this ordering that is in the entry section  waiting  j  = = true  as the next one to enter the critical section any process waiting to enter its critical section will thus do so within n  1 turns  unfortunately for hardware designers  implementing atomic testandset   instructions on multiprocessors is not a trivial task such implementations are discussed in books on computer architecture  the hardware-based solutions to the critical-section problem presented in section 6.4 are complicated for application programmers to use to overcmrte this difficulty  we can use a synchronization tool called a a semaphore s is an integer variable that  apart from initialization  is accessed only through two standard atomic operations  wait   and signal    the wait   operation was originally termed p  from the dutch proberen  to test  ; signal   was originally called v  from verhogen  to increment   the definition of wait   is as follows  wait  s    while s = 0 ii no-op s ' the definition of signal   is as follows  signal  s   s + + ;  all modifications to the integer value of the semaphore in the wait   and signal   operations must be executed indivisibly that is  when one process modifies the semaphore value  no other process can simultaneously modify that same semaphore value in addition  in the case of wait  s   the testing of the integer value of s  s  s 0   as well as its possible modification  s   must be executed without interruption we shall see how these operations can be implemented in section 6.5.2 ; first  let us see how semaphores can be used  6.5.1 usage operating systems often distinguish between counting and binary semaphores  the value of a counting semaphore can range over an unrestricted domain  the value of a binary semaphore can range only between 0 and 1 on some 6.5 235 systems  binary semaphores are lmown as mutex locks  as they are locks that provide mutual exclusion  we can use binary semaphores to deal with the critical-section problem or mljltiple processes then processes share a semaphore  mutex  initialized to 1  each process pi is organized as shown in figure 6.9  counting semaphores can be used to control access to a given resource consisting of a finite number o instances the semaphore is initialized to the number of resources available each process that wishes to use a resource performs a wait   operation on the semaphore  thereby decrementing the count   when a process releases a resource  it performs a signal   operation  incrementing the count   when the count for the semaphore goes to 0  all resources are being used after that  processes that wish to use a resource will block until the count becomes greater than 0  we can also use semaphores to solve various synchronization problems  for example  consider two concurrently numing processes  p1 with a statement 51 and p2 with a statement 52  suppose we require that 52 be executed only after 51 has completed we can implement this scheme readily by letting p1 and p2 share a common semaphore synch  initialized to 0  and by inserting the statements 51 ; signal  synch  ; in process p1 and the statements wait  synch  ; 52 ; in process p2 because synch is initialized to 0  p2 will execute 52 only after p1 has invoked signal  synch   which is after statement 51 has been executed  6.5.2 implementation the main disadvantage of the semaphore definition given here is thatit requires while a process is in its critical section  any other process that tries to enter its critical section must loop continuously in the entry code this continual looping is clearly a problem in a real multiprogramming system  do  wait  mutex  ; ii critical section signal  mutex  ; ii remainder section  while  true  ; figure 6.9 mutual-exclusion implementation with semaphores  236 chapter 6 where a single cpu is shared among ncany processes busy waiting wastes cpu cycles that some other process might be able to use productively this type of semaphore is also called a because the process spins while waiting for the lock  spinlocks do have an advantage in that no context switch is required when a process must wait on a lock  and a context switch may take considerable time thus  when locks are expected to be held for short times  spinlocks are useful ; they are often employed on multiprocessor systems where one thread can spin on one processor while another thread performs its critical section on another processor  to overcome the need for busy waiting  we can modify the definition of the wait   and signal   semaphore operations when a process executes the wait   operation and finds that the semaphore value is not positive  it must wait however  rather than engaging in busy waiting  the process can block itself the block operation places a process into a waiting queue associated with the semaphore  and the state of the process is switched to the waiting state then control is transferred to the cpu scheduler  which selects another process to execute  a process that is blocked  waiting on a semaphore s  should be restarted when some other process executes a signal   operation the process is restarted by a wakeup   operation  which changes the process from the waiting state to the ready state the process is then placed in the ready queue  the cpu may or may not be switched from the running process to the newly ready process  depending on the cpu-scheduling algorithm  to implement semaphores under this definition  we define a semaphore as a c ' struct  typedef struct  int value ; struct process list ;  semaphore ; each semaphore has an integer value and a list of processes list when a process must wait on a semaphore  it is added to the list of processes a signal   operation removes one process from the list of waiting processes and awakens that process  the wait   semaphore operation can now be defined as wait  semaphore s   s value ;  if  s value 0    add this process to s list ; block   ; the signal   semaphore operation can now be defined as signal  semaphore s   s value + + ; if  s value = 0   6.5 remove a process p fron s list ; wakeup  p  ;   237 the block   operation suspends the process that invokes it the wakeup  p  operation resumes the execution of a blocked process p these two operations are provided by the operating system as basic system calls  note that in this implementation  semaphore values may be negative  although semaphore values are never negative under the classical definition of semaphores with busy waiting if a semaphore value is negative  its magnitude is the number of processes waiting on that semaphore this fact results from switching the order of the decrement and the test in the implementation of the wait   operation  the list of waiting processes can be easily implemented by a link field in each process control block  pcb   each semaphore contains an integer value and a pointer to a list of pcbs one way to add and rernove processes from the list so as to ensure bounded waiting is to use a fifo queue  where the semaphore contains both head and tail pointers to the queue in general  howeve1 ~ the list can use any queueing strategy correct usage of semaphores does not depend on a particular queueing strategy for the semaphore lists  it is critical that semaphores be executed atomically we must guarantee that no two processes can execute wait   and signal   operations on the same semaphore at the same time this is a critical-section problem ; and in a single-processor environment  that is  where only one cpu exists   we can solve it by simply inhibiting interrupts during the time the wait   and signal   operations are executing this scheme works in a single-processor environment because  once interrupts are inhibited  instructions from different processes can not be interleaved only the currently running process executes until interrupts are reenabled and the scheduler can regain control  in a multiprocessor environment  interrupts must be disabled on every processor ; otherwise  instructions from different processes  running on different processors  may be interleaved in some arbitrary way disabling interrupts on every processor can be a difficult task and furthermore can seriously diminish performance therefore  smp systems must provide alternative locking techniques-such as spinlocks-to ensure that wait   and signal   are performed atomically  it is important to admit that we have not completely eliminated busy waiting with this definition of the wait   and signal   operations rather  we have moved busy waiting from the entry section to the critical sections of application programs furthermore  we have limited busy waiting to the critical sections of the wait   and signal   opera times  and these sections are short  if properly coded  they sbould be no more than about ten instructions   thus  the critical section is almost never occupied  and busy waiting occurs rarely  and then for only a short time an entirely different situation exists with application programs whose critical sections may be long  minutes or 238 chapter 6 even hours  or may almost always be occupied in such casesf busy waiting is extremely inefficient  6.5.3 deadlocks and starvation the implementation of a semaphore with a waiting queue may result in a situation where two or more processes are waiting indefinitely for an event that can be caused only by one of the waiting processes the event in question is the execution of a signal   when such a state is reached  these processes are said to be to illustrate this  we consider a system consisting of two processes  po and p1  each accessing two semaphores  s and q  set to the value 1  po wait  s  ; wait  q  ; signal  s  ; signal  q  ; pl wait  q  ; wait  s  ; signal  q  ; signal  s  ; suppose that po executes wait  s  and then p1 executes wait  q   when po executes wait  q   it must wait until p1 executes signal  q   similarly  when p1 executes wait  s   it must wait until po executes signal  s   since these signal   operations cam1ot be executed  po and p1 are deadlocked  we say that a set of processes is in a deadlock state when every process in the set is waiting for an event that can be caused only by another process in the set the events with which we are mainly concerned here are resource acquisition and release however  other types of events may result in deadlocks  as we show in chapter 7 in that chapter  we describe various mechanisms for dealing with the deadlock problem  another problem related to deadlocks is or a situation in which processes wait indefinitely within the semaphore  indefinite blocking may occur if we remove processes from the list associated with a semaphore in lifo  last-in  first-out  order  6.5.4 priority inversion a scheduling challenge arises when a higher-priority process needs to read or modify kernel data that are currently being accessed by a lower-priority process-or a chain of lower-priority processes since kernel data are typically protected with a lock  the higher-priority process will have to wait for a lower-priority one to finish with the resource the situation becomes more complicated if the lower-priority process is preempted in favor of another process with a higher priority as an example  assume we have three processes  lf m  and h  whose priorities follow the order l m h assume that process h requires resource r  which is currently being accessed by process l  ordinarily  process h would wait for l to finish using resource r however  now suppose that process m becomes runnable  thereby preempting process 6.6 6.6 239 priority inversion and the mars pathfinder priority inversion can be more than a scheduling inconvenience on systems with tight time constraints  such as real-time systems-see chapter 19   priority inversion can cause a process to take longer than it should to accomplish a task when that happens  other failures can cascade  resulting in system failure  consider the mars pathfinde1 ~ a nasa space probe that landed a robot  the sojourner rove1 ~ on mars in 1997 to conduct experiments shortly after the sojourner began operating  it started to experience frequent computer resets  each reset reinitialized all hardware and software  including communications  if the problem had not been solved  the sojourner would have failed in its mission  the problem was caused by the fact that one high-priority task  bcdist  was taking longer than expected to complete its work this task was being forced to wait for a shared resource that was held by the lower-priority asi/met task  which in turn was preempted by multiple medium-priority tasks the bcdist task would stall waiting for the shared resource  and ultimately the bc_sched task would discover the problem and perform the reset the sojourner was suffering from a typical case of priority inversion  the operating system on the sojourner was vxworks  see section 19.6   which had a global variable to enable priority inheritance on all semaphores  after testing  the variable was set on the sojourner  on mars !   and the problem was solved  a full description of the problem  its detection  and its solution was written by the software team lead and is available at research.microsoft.com/ mbj /marsyathfinder i authoritative_account.html  l indirectly  a process with a lower priority-process m-has affected how long process h must wait for l to relinquish resource r  this problem is known as it occurs only in systems with more than two priorities  so one solution is to have only two priorities that is insufficient for most general-purpose operating systems  however typically these systems solve the problem by implementing a 2tic x,u   according to this protocol  all processes that are accessing resources needed by a higher-priority process inherit the higher priority until they are finished with the resources in question when they are finished  their priorities revert to their original values in the exan1.ple above  a priority-inheritance protocol would allow process l to temporarily inherit the priority of process h  thereby preventing process m from preempting its execution when process l had finished using resource r  it would relinquish its inherited priority from hand assume its original priority because resource r would now be available  process h-not m-would run next  in this section  we present a number of synchronization problems as examples of a large class of concurrency-control problems these problems are used for 240 chapter 6 do  ii produce an item in nextp wait  empty  ; wait  mutex  ; ii add nextp to buffer signal  mutex  ; signal  full  ;  while  true  ; figure 6.10 the structure of the producer process  testing nearly every newly proposed synchronization scheme in our solutions to the problems  we use semaphores for synchronization  6.6.1 the bounded-buffer problem the bounded-buffer problem was introduced in section 6.1 ; it is commonly used to illustrate the power of synchronization primitives here  we present a general structure of this scheme without committing ourselves to any particular implementation ; we provide a related programming project in the exercises at the end of the chapter  we assume that the pool consists of n buffers  each capable of holding one item the mutex semaphore provides mutual exclusion for accesses to the buffer pool and is initialized to the value 1 the empty and full semaphores comct the number of empty and full buffers the semaphore empty is initialized to the value n ; the semaphore full is initialized to the value 0  the code for the producer process is shown in figure 6.10 ; the code for the consumer process is shown in figure 6.11 note the symmetry between the producer and the consumer we can interpret this code as the producer producing full buffers for the consumer or as the consumer producing empty buffers for the producer  do  wait  full  ; wait  mutex  ; ii remove an item from buffer to nextc signal  mutex  ; signal  empty  ; ii consume the item in nextc  while  true  ; figure 6.11 the structure of the consumer process  6.6 241 6.6.2 the readers-writers problem suppose that a database is to be shared among several concurrent processes  some of these processes may want only to read the database  whereas others may want to update  that is  to read and write  the database we distinguish between these two types of processes by referring to the former as readers and to the latter as writers obviously  if two readers access the shared data simultaneously  no adverse effects will result however  if a writer and some other process  either a reader or a writer  access the database simultaneously  chaos may ensue  to ensure that these difficulties do not arise  we require that the writers have exclusive access to the shared database while writing to the database this synchronization problem is referred to as the readers-writers problem since it was originally stated  it has been used to test nearly every new synchronization primitive the readers-writers problem has several variations  all involving priorities the simplest one  referred to as the first readers-writers problem  requires that no reader be kept waiting unless a writer has already obtained permission to use the shared object in other words  no reader should wait for other readers to finish simply because a writer is waiting the second readerswriters problem requires that  once a writer is ready  that writer performs its write as soon as possible in other words  if a writer is waiting to access the object  no new readers may start reading  a solution to either problem may result in starvation in the first case  writers may starve ; in the second case  readers may starve for this reason  other variants of the problem have been proposed next  we present a solution to the first readers-writers problem refer to the bibliographical notes at the end of the chapter for references describing starvation-free solutions to the second readers-writers problem  in the solution to the first readers-writers problem  the reader processes share the following data structures  semaphore mutex  wrt ; int readcount ; the semaphores mutex and wrt are initialized to 1 ; readcount is initialized to 0 the semaphore wrt is common to both reader and writer processes  the mutex semaphore is used to ensure mutual exclusion when the variable readcount is updated the readcount variable keeps track of how many processes are currently reading the object the semaphore wrt functions as a mutual-exclusion semaphore for the writers it is also used by the first or last reader that enters or exits the critical section it is not used by readers who enter or exit while other readers are in their critical sections  the code for a writer process is shown in figure 6.12 ; the code for a reader process is shown in figure 6.13 note that  if a writer is in the critical section and n readers are waiting  then one reader is queued on wrt  and n 1 readers are queued on mutex also observe that  when a writer executes signal  wrt   we may resume the execution of either the waiting readers or a single waiting writer the selection is made by the scheduler  the readers-writers problem and its solutions have been generalized to provide locks on some systems acquiring a reader-writer lock 242 chapter 6 do  wait  wrt  ; ii writing is performed signal  wrt  ;  while  true  ; figure 6 i 2 the structure of a writer process  requires specifying the mode of the lock either read or write access when a process wishes only to read shared data  it requests the reader-writer lock in read mode ; a process wishing to modify the shared data must request the lock in write mode multiple processes are permitted to concurrently acquire a reader-writer lock in read mode  but only one process may acquire the lock for writing  as exclusive access is required for writers  reader-writer locks are most useful in the following situations  in applications where it is easy to identify which processes only read shared data and which processes only write shared data  in applications that have more readers than writers this is because readerwriter locks generally require more overhead to establish than semaphores or mutual-exclusion locks the increased concurrency of allowing multiple readers compensates for the overhead involved in setting up the readerwriter lock  6.6.3 the dining-philosophers problem consider five philosophers who spend their lives thinking and eating the philosophers share a circular table surrounded by five chairs  each belonging do  wait  mutex  ; readcount + + ; if  readcount 1  wait  wrt  ; signal  mutex  ; ii reading is performed wait  mutex  ; readcount ; if  readcount 0  signal  wrt  ; signal  mutex  ;  while  true  ; figure 6.13 the structure of a reader process  6.6 243 figure 6.14 the situation of the dining philosophers  to one philosopher in the center of the table is a bowl of rice  and the table is laid with five single chopsticks  figure 6.14   when a philosopher thinks  she does not interact with her colleagues from time to time  a philosopher gets hungry and tries to pick up the two chopsticks that are closest to her  the chopsticks that are between her and her left and right neighbors   a philosopher may pick up only one chopstick at a time obviously  she cam1ot pick up a chopstick that is already in the hand of a neighbor when a htmgry philosopher has both her chopsticks at the same time  she eats without releasing her chopsticks when she is finished eating  she puts down both of her chopsticks and starts thinking again  the dining-philosophers problem is considered a classic synchronization problem neither because of its practical importance nor because computer scientists dislike philosophers but because it is an example of a large class of concurrency-control problems it is a simple representation of the need to allocate several resources among several processes in a deadlock-free and starvation-free mam1er  one simple solution is to represent each chopstick with a semaphore a philosopher tries to grab a chopstick by executing await   operation on that semaphore ; she releases her chopsticks by executing the signal   operation on the appropriate semaphores thus  the shared data are semaphore chopstick  5  ; where all the elements of chopstick are initialized to 1 the structure of philosopher i is shown in figure 6.15  although this solution guarantees that no two neighbors are eating simultaneously  it nevertheless must be rejected because it could create a deadlock suppose that all five philosophers become hungry simultaneously and each grabs her left chopstick all the elements of chopstick will now be equal to 0 when each philosopher tries to grab her right chopstick  she will be delayed forever  several possible remedies to the deadlock problem are listed next  allow at most four philosophers to be sitting simultaneously at the table  244 chapter 6 6.7 do  wait  chopstick  i   ; wait  chopstick   i + l  % 5   ; i i eat signal  chopstick  i   ; signal  chopstick   i + l  % 5   ; ii think  while  true  ; figure 6.15 the structure of philosopher i  allow a philosopher to pick up her chopsticks only if both chopsticks are available  to do this  she must pick them up in a critical section   use an asymmetric solution ; that is  an odd philosopher picks up first her left chopstick and then her right chopstick  whereas an even philosopher picks up her right chopstick and then her left chopstick in section 6.7  we present a solution to the dining-philosophers problem that ensures freedom from deadlocks note  however  that any satisfactory solution to the dining-philosophers problem must guard against the possibility that one of the philosophers will starve to death a deadlock-free solution does not necessarily eliminate the possibility of starvation  although semaphores provide a convenient and effective mechanism for process synchronization  using them incorrectly can result in timing errors that are difficult to detect  since these errors happen only if some particular execution sequences take place and these sequences do not always occur  we have seen an example of such errors in the use of counters in our solution to the producer-consumer problem  section 6.1   in that example  the timing problem happened only rarely  and even then the counter value appeared to be reasonable-off by only 1 nevertheless  the solution is obviously not an acceptable one it is for this reason that semaphores were introduced in the first place  unfortunately  such timing errors can still occur when semaphores are used to illustrate how  we review the semaphore solution to the critical-section problem all processes share a semaphore variable mutex  which is initialized to 1 each process must execute wait  mutex  before entering the critical section and signal  mutex  afterward if this sequence is not observed  two processes may be in their critical sections simultaneously next  we examine the various difficulties that may result note that these difficulties will arise even if a single process is not well behaved this situation may be caused by an honest programming error or an uncooperative programmer  6.7 245 suppose that a process interchanges the order in which the wait   and signal   operations on the semaphore mutex are executed  resulting in the following execution  signal  mutex  ; critical section wait  mutex  ; in this situation  several processes may be executing in their critical sections simultaneously  violating the mutual-exclusion requirement this error may be discovered only if several processes are simultaneously active in their critical sections note that this situation may not always be reproducible  suppose that a process replaces signal  mutex  with wait  mutex   that is  it executes wait  mutex  ; critical section wait  mutex  ; in this case  a deadlock will occur  suppose that a process omits the wait  mutex   or the signal  mutex   or both in this case  either mutual exclusion is violated or a deadlock will occur  these examples illustrate that various types of errors can be generated easily when programmers use sencaphores incorrectly to solve the critical-section problem similar problems may arise in the other synchronization models discussed in section 6.6  to deal with such errors  researchers have developed high-level language constructs in this section  we describe one fundamental high-level synchronization construct-the monitor type  6.7.1 usage a abstract data type or adt encapsulates private data with public methods to operate on that data a monitor type is an adt which presents a set of programmer-defined operations that are provided mutual exclusion within the monitor the monitor type also contains the declaration of variables whose values define the state of an instance of that type  along with the bodies of procedures or functions that operate on those variables the syntax of a monitor type is shown in figure 6.16 the representation of a monitor type can not be used directly by the various processes thus  a procedure defined within a monitor can access only those variables declared locally within the monitor and its formal parameters similarly  the local variables of a monitor can be accessed by only the local procedures  246 chapter 6 monitor rrwnitor name  ii shared variable declarations procedure p1        procedure p2        procedure pn        initialization code         figure 6.16 syntax of a monitor  the monitor construct ensures that only one process at a time is active within the monitor consequently  the programmer does not need to code this synchronization constraint explicitly  figure 6.17   howeve1 ~ the monitor construct  as defined so fa1 ~ is not sufficiently powerful for modeling some synchronization schemes for this purpose  we need to define additional synchronization mechanisms these mechanisms are provided by the condition construct a programmer who needs to write a tailor-made synchronization scheme can define one or more variables of type condition  condition x  y ; the only operations that can be invoked on a condition variable are wait   and signal    the operation x wait   ; means that the process invoking this operation is suspended until another process invokes x signal   ; the x signal   operation resumes exactly one suspended process if no process is suspended  then the signal   operation has no effect ; that is  the state of x is the same as if the operation had never been executed  figure shared data operations initialization code 6.7 figure 6.17 schematic view of a monitor  247 6.18   contrast this operation with the signal   operation associated with semaphores  which always affects the state of the semaphore  now suppose that  when the x signal   operation is invoked by a process p  there exists a suspended process q associated with condition x clearly  if the suspended process q is allowed to resume its execution  the signaling process p must wait otherwise  both p and q would be active simultaneously within the monitor note  however  that both processes can conceptually continue with their execution two possibilities exist  signal and wait p either waits until q leaves the monitor or waits for another condition  signal and continue q either waits until p leaves the monitor or waits for another condition  there are reasonable arguments in favor of adopting either option on the one hand  since p was already executing in the monitor  the signal-and-continue method seems more reasonable on the other hand  if we allow thread p to continue  then by the time q is resumed  the logical condition for which q was waiting may no longer hold a compromise between these two choices was adopted in the language concurrent pascal when thread p executes the signal operation  it imncediately leaves the monitor hence  q is immediately resumed  many programming languages have incorporated the idea of the monitor as described in this section  including concurrent pascal  mesa  c #  pronounced c-sharp   and java other languages-such as erlang-provide some type of concurrency support using a similar mechanism  248 chapter 6 queues associated with  x  y conditions ; -_  __ ~  ~  \   operations initialization code figure 6.18 monitor with condition variables  6.7.2 dining-philosophers solution using monitors next  we illustrate monitor concepts by presenting a deadlock-free solution to the dining-philosophers problem this solution imposes the restriction that a philosopher may pick up her chopsticks only if both of them are available to code this solution  we need to distinguish among three states in which we may find a philosopher for this purpose  we introduce the following data structure  enum  thinking  hungry  eating  state  5  ; philosopher i can set the variable state  i  = eating only if her two neighbors are not eating   state   i + 4  % 5  ! = eating  and  state   i + 1  % 5  ' = eating   we also need to declare condition sel  5  ; in which philosopher i can delay herself when she is hungry but is unable to obtain the chopsticks she needs  we are now in a position to describe our solution to the dining-philosophers problem the distribution of the chopsticks is controlled by the monitor diningphilosophers  whose definition is shown in figure 6.19 each philosopher  before starting to eat  must invoke the operation pickup    this act n'lay result in the suspension of the philosopher process after the successful completion of the operation  the philosopher may eat following this  the philosopher invokes 6.7 monitor dp   enum  thinking  hungry  eating  state  5  ; condition self  5  ; void pickup  int i   state  i  = hungry ; test  i  ;  if  state  i  ! = eating  self  i   wait   ; void putdown  int i   state  i  = thinking ; test   i + 4  % 5  ; test   i + 1  % 5  ;  void test  int i    if   state   i + 4  % 5  ! = eating  &&  state  i  = = hungry  &&   state   i + 1  % 5  ! = eating    state  i  = eating ; self  i  .signal   ; initialization_code     for  int i = 0 ; i 5 ; i + +  state  i  = thinking ; figure 6.19 a monitor solution to the dining-philosopher problem  249 the put down   operation thus  philosopher i must invoke the operations pickup   and put down   in the following sequence  diningphilosophers.pickup  i  ; eat diningphilosophers.putdown  i  ; it is easy to show that this solution ensures that no two neighbors are eating simultaneously and that no deadlocks will occur we note  however  that it is possible for a philosopher to starve to death we do not present a solution to this problem but rather leave it as an exercise for you  250 chapter 6 6.7.3 implementing a monitor using semaphores we now consider a possible implementation of the nwnitor mechanism using semaphores for each ltlonitor  a semaphore mutex  initialized to 1  is provided  a process must execute wait  mutex  before entering the n1onitor and must execute signal  mutex  after leaving the monitor  since a signaling process must wait until the resumed process either leaves or waits  an additional sernaphore  next  is introduced  initialized to 0 the signaling processes can use next to suspend themselves an integer variable next_count is also provided to count the number of processes suspended on next thus  each external procedure f is replaced by wait  mutex  ; body off if  next_count 0  signal  next  ; else signal  mutex  ; mutual exclusion within a monitor is ensured  we can now describe how condition variables are implemented as well  for each condition x  we introduce a semaphore x_sem and an integer variable x_count  both initialized to 0 the operation x wait   can now be implemented as x_count + + ; if  next_count 0  signal  next  ; else signal  mutex  ; wait  x_sem  ; x_count ; the operation x signal   can be implemented as if  x_count 0   next_count + + ; signal  x_sem  ; wait  next  ; next_count ;  this implementation is applicable to the definitions of monitors given by both hoare and brinch-hansen in some cases  however  the generality of the implementation is unnecessary  and a significant improvement in efficiency is possible we leave this problem to you in exercise 6.35  6.7.4 resuming processes within a monitor we turn now to the subject of process-resumption order within a monitor if several processes are suspended on condition x  and an x signal   operation monitor resourceallocator   boolean busy ; condition x ; void acquire  int time   if  busy  x.wait  time  ; busy = true ;  void release    busy = false ; x signal   ;  initialization_code    busy = false ;  6.7 figure 6.20 a monitor to allocate a single resource  251 is executed by some process  then how do we determine which of the suspended processes should be resumed next one simple solution is to use an fcfs ordering  so that the process that has been waiting the longest is resumed first in many circumstances  however  such a simple scheduling scheme is not adequate for this purpose  the construct can be used ; it has the form x.wait  c  ; where c is an integer expression that is evaluated when the wait   operation is executed the value of c  which is called a pdos ! ty is then stored with the name of the process that is suspended when x signal   is executed  the process with the smallest priority number is resumed next  to illustrate this new mechanism  consider the resourceallocator monitor shown in figure 6.20  which controls the allocation of a single resource among competing processes each process  when requesting an allocation of this resource  specifies the maximum time it plans to use the resource the monitor allocates the resource to the process that has the shortest time-allocation request a process that needs to access the resource in question must observe the following sequence  r.acquire  t  ; access the resource ; r release   ; where r is an instance of type resourceallocator  252 chapter 6 6.8 unfortunately  the monitor concept can not guarantee that the preceding access sequence will be observed in particular  the following problems can occur  a process might access a resource without first gaining access permission to the resource  a process ntight never release a resource once it has been granted access to the resource  a process might attempt to release a resource that it never requested  a process might request the same resource twice  without first releasing the resource   the same difficulties are encountered with the use of semaphores  and these difficulties are similar in nature to those that encouraged us to develop the monitor constructs in the first place previously  we had to worry about the correct use of semaphores now  we have to worry about the correct use of higher-level programmer-defined operations  with which the compiler can no longer assist us  one possible solution to the current problem is to include the resourceaccess operations within the resourceallocator monitor however  using this solution will mean that scheduling is done according to the built-in monitor-scheduling algorithm rather than the one we have coded  to ensure that the processes observe the appropriate sequences  we must inspect all the programs that make use of the resourceallocator monitor and its managed resource we must check two conditions to establish the correctness of this system first  user processes must always make their calls on the monitor in a correct sequence second  we must be sure that an uncooperative process does not simply ignore the mutual-exclusion gateway provided by the monitor and try to access the shared resource directly  without using the access protocols only if these two conditions can be ensured can we guarantee that no time-dependent errors will occur and that the scheduling algorithm will not be defeated  although this inspection may be possible for a small  static system  it is not reasonable for a large system or a dynamic system this access-control problem can be solved only through the use of additional mechanisms that are described in chapter 14  many programming languages have incorporated the idea of the monitor as described in this section  including concurrent pascal  mesa  c #  pronounced c-sharp   and java other languages-such as erlang-provide some type of concurrency support using a similar mechanism  we next describe the synchronization mechanisms provided by the solaris  windows xp  and linux operating systems  as well as the pthreads api we have chosen these three operating systems because they provide good examples of different approaches for synchronizing the kernel  and we have included the 6.8 253 java monitors java provides a monitor-like concurrency mechanisn1 for thread synchronization  every object in java has associated with it a single lock when a method is declared to be synchronized  calling the method requires owning the lock for the object we declare a synchronized method by placing the synchronized keyword in the method definition the following defines the safemethod   as synchronized  for example  public class simpleclass   public synchronized void safemethod    i implementation of safemethod   i  next  assume we create an object instance of simpleclass  such as  simpleclass sc = new simpleclass   ; invoking the sc safemethod   method requires owning the lock on the object instance sc if the lock is already owned by another thread  the thread calling the synchronized method blocks and is placed in the entry set for the object 's lock the entry set represents the set of threads waiting for the lock to become available if the lock is available when a synchronized method is called  the calling thread becomes the owner of the object 's lock and can enter the method the lock is released when the thread exits the method ; a thread from the entry set is then selected as the new owner of the lock  java also provides wait   and notify   methods  which are similar in function to the wait   and signal 0 statements for a monitor release 1.5 of the java language provides api support for semaphores  condition variables  and mutex locks  among other concurrency mechanisms  in the java util concurrent package  pthreads api because it is widely used for thread creation and synchronization by developers on unix and linux systems as you will see in this section  the synchronization methods available in these differing systems vary in subtle and significant ways  6.8.1 synchronization in solaris to control access to critical sections  solaris provides adaptive mutexes  condition variables  sernaphores  reader-writer locks  and turnstiles solaris implements semaphores and condition variables essentially as they are presented in sections 6.5 and 6.7 in this section  we describe adaptive mlltexes  readerwriter locks  and turnstiles  254 chapter 6 an protects access to every critical data item on a multiprocessor system  an adaptive mutex starts as a standard semaphore implemented as a spinlock if the data are locked and therefore already in use  the adaptive mutex does one of two things if the lock is held by a thread that is currently running on another cpu  the thread spins while waiting for the lock to become available  because the thread holding the lock is likely to finish soon if the thread holding the lock is not currently in run state  the thread blocks  going to sleep until it is awakened by the release of the lock it is put to sleep so that it will not spin while waiting  since the lock will not be freed very soon a lock held by a sleeping thread is likely to be in this category on a single-processor system  the thread holding the lock is never rwming if the lock is being tested by another thread  because only one thread can run at a time therefore  on this type of system  threads always sleep rather than spin if they encounter a lock  solaris uses the adaptive-mutex method to protect only data that are accessed by short code segments that is  a mutex is used if a lock will be held for less than a few hundred instructions if the code segment is longer than that  the spin-waiting method is exceedingly inefficient for these longer code segments  condition variables and semaphores are used if the desired lock is already held  the thread issues a wait and sleeps when a thread frees the lock  it issues a signal to the next sleeping thread in the queue the extra cost of putting a thread to sleep and waking it  and of the associated context switches  is less than the cost of wasting several hundred instructions waiting in a spinlock  reader-writer locks are used to protect data that are accessed frequently but are usually accessed in a read-only manner in these circumstances  reader-writer locks are more efficient than semaphores  because multiple threads can read data concurrently  whereas semaphores always serialize access to the data reader-writer locks are relatively expensive to implement  so again they are used only on long sections of code  solaris uses turnstiles to order the list of threads waiting to acquire either an adaptive n1.utex or a reader-writer lock a is a queue structure containing threads blocked on a lock for example  if one thread currently owns the lock for a synchronized object  all other threads trying to acquire the lock will block and enter the turnstile for that lock when the lock is released  the kernel selects a thread from the turnstile as the next owner of the lock  each synchronized object with at least one thread blocked on the object 's lock requires a separate turnstile however  rather than associating a turnstile with each synchronized object  solaris gives each kernel thread its own turnstile  because a thread can be blocked only on one object at a time  this is more efficient than having a turnstile for each object  the turnstile for the first thread to block on a synchronized object becomes the turnstile for the object itself threads subsequently blocking on the lock will be added to this turnstile when the initial thread ultimately releases the lock  it gains a new turnstile from a list of free turnstiles maintained by the kernel to prevent a priority inversion  turnstiles are organized according to a priorityinheritance protocol this means that if a lower-priority thread currently holds a lock on which a higher-priority thread is blocked  the thread with the lower priority will temporarily inherit the priority of the higher-priority thread upon releasing the lock  the thread will revert to its original priority  6.8 255 note that the locking mechanisms used by the kernel are implemented for user-level threads as well  so the same types of locks are available inside and outside the kernel a crucial implementation difference is the priorityinheritance protocol kernel-locking routines adhere to the kernel priorityinheritance methods used by the scheduler  as described in section 19.4 ; user-level thread-locking mechanisms do not provide this functionality  to optimize solaris performance  developers have refined and fine-tuned the locking methods because locks are used frequently and typically are used for crucial kernel functions  tuning their implem.entation and use can produce great performance gains  6.8.2 synchronization in windows xp the windows xp operating system is a multithreaded kernel that provides support for real-time applications and multiple processors when the windows xp kernel accesses a global resource on a uniprocessor system  it temporarily masks interrupts for all interrupt handlers that may also access the global resource on a multiprocessor system  windows xp protects access to global resources using spinlocks just as in solaris  the kernel uses spinlocks only to protect short code segments furthermore  for reasons of efficiency  the kernel ensures that a thread will never be preempted while holding a spinlock  for thread synchronization outside the kernel  windows xp provides ~   using a dispatcher object  threads synchronize according to several different mechanisms  including mutexes  semaphores  events  and timers the system protects shared data by requiring a tluead to gain ownership of a mutex to access the data and to release ownership when it is finished  semaphores behave as described in section 6.5 are similar to condition variables ; that is  they may notify a waiting thread when a desired condition occurs finally  timers are used to notify one  or more than one  thread that a specified amount of time has expired  dispatcher objects may be in either a signaled state or a nonsignaled state  a si ,7'2led indicates that an object is available and a thread will not block when acquiring the object a indicates that an object is not available and a thread will block when attempting to acquire the object we illustrate the state transitions of a mutex lock dispatcher object in figure 6.21  a relationship exists between the state of a dispatcher object and the state of a thread when a thread blocks on a nonsignaled dispatcher object  its state changes frmn ready to waiting  and the thread is placed in a waiting queue for that object when the state for the dispatcher object moves to signaled  the kernel checks whether any threads are waiting on the object if so  the owner thread releases mutex lock thread acquires mutex lock figure 6.21 mutex dispatcher object  256 chapter 6 kernel moves one thread -or possibly nlore threads-from the waiting state to the ready state  where they can resume executing the number of threads the kernel selects from the waiting queue depends on the type of dispatcher object for which it is waiting the kernel will select only one thread from the waiting queue for a mutex  since a mutex object may be owned by only a single thread for an event object  the kernel will select all threads that are waiting for the event  we can use a mutex lock as an illustration of dispatcher objects and thread states if a thread tries to acquire a mutex dispatcher object that is in a nonsignaled state  that thread will be suspended and placed in a waiting queue for the mutex object when the mutex moves to the signaled state  because another thread has released the lock on the mutex   the thread waiting at the front of the queue will be moved from the waiting state to the ready state and will acquire the mutex lock  we provide a programming project at the end of this chapter that uses mutex locks and semaphores in the win32 api  6.8.3 synchronization in linux prior to version 2.6  linux was a nonpreemptive kernel  meaning that a process running in kernel mode could not be preempted -even if a higher-priority process became available to run now  however  the linux kernel is fully preemptive  so a task can be preempted when it is running in the kernel the linux kernel provides spinlocks and semaphores  as well as readerwriter versions of these two locks  for locking in the kernel on smp machines  the fundamental locking mechanism is a spinlock  and the kernel is designed so that the spinlock is held only for short durations on single-processor machines  spinlocks are inappropriate for use and are replaced by enabling and disabling kernel preemption that is  on single-processor machines  rather than holding a spinlock  the kernel disables kernel preemption ; and rather than releasing the spinlock  it enables kernel preemption this is summarized below  disable kernel preemption  acquirespin lock  enable kernel preemption release spin lock  linux uses an interesting approach to disable and enable kernel preemption  it provides two simple system calls-preempldisable   and preempt_ enable   -for disabling and enabling kernel preemption in addition  however  the kernel is not preemptible if a kernel-mode task is holding a lock  to enforce this rule  each task irl the system has a thread-info structure containing a counter  preemplcount  to indicate the number of locks being held by the task when a lock is acquired  preemplcount is incremented it is decremented when a lock is released if the value of preempt_count for the task currently running is greater than zero  it is not safe to preempt the kernel  as this task currently holds a lock if the count is zero  the kernel can safely be interrupted  assuncing there are no outstanding calls to preempldisable     6.9 6.9 257 spinlocks-along with enabling and disabling kernel preemption-are used in the kernel only when a lock  or disabling kernel preemption  is held for a short duration when a lock must be held for a longer period  semaphores are appropriate for use  6.8.4 synchronization in pthreads the pthreads api provides mutex locks  condition variables  and read-write locks for thread synchronization this api is available for programmers and is not part of any particular kernel mutex locks represent the fundamental synchronization technique used with pthreads a mutex lock is used to protect critical sections of code-that is  a thread acquires the lock before entering a critical section and releases it upon exiting the critical section condition variables in pthreads behave much as described in section 6.7 read-write locks behave similarly to the locking mechanism described in section 6.6.2  many systems that implement pthreads also provide semaphores  although they are not part of the pthreads standard and instead belong to the posix sem extension other extensions to the pthreads api include spinlocks  but not all extensions are considered portable from one implementation to another we provide a programming project at the end of this chapter that uses pthreads mutex locks and semaphores  the mutual exclusion of critical sections ensures that the critical sections are executed atomically -that is  as one uninterruptible unit if two critical sections are instead executed concurrently  the result is equivalent to their sequential execution in some unknown order although this property is useful in many application domains  in many cases we would like to make sure that a critical section forms a single logical unit of work that either is performed in its entirety or is not performed at all an example is funds transfer  in which one account is debited and another is credited clearly  it is essential for data consistency either that both the credit and debit occur or that neither occurs  consistency of data  along with storage and retrieval of data  is a concern often associated with recently  there has been an upsurge of interest in using database-systems techniques in operating systems operating systems can be viewed as manipulators of data ; as such  they can benefit from the advanced techniques and models available from database research for instance  many of the ad hoc techniques used in operating systems to manage files could be more flexible and powerful if more formal database methods were used in their place in sections 6.9.2 to 6.9.4  we describe some of these database techniques and explain how they can be used by operating systems  first  however  we deal with the general issue of transaction atomicity it is this property that the database techniques are meant to address  6.9.1 system model a collection of instructions  or operations  that performs a single logical function is called a a major issue in processing transactions is the 258 chapter 6 preservation of atomicity despite the possibility of failures within the computer system  we can think of a transaction as a program unit that accesses and perhaps updates various data items that reside on a disk within some files from our point of view  such a transaction is simply a sequence of read and write operations terminated by either a commit operation or an abort operation  a commit operation signifies that the transaction has terminated its execution successfully  whereas an abort operation signifies that the transaction has transactional memory with the emergence of multicore systems has come increased pressure to develop multithreaded applications that take advantage of multiple processing cores however  multithreaded applications present an increased risk of race conditions and deadlocks traditionally  techniques such as locks  semaphores  and monitors have been used to address these issues however  provides an alternative strategy fordeveloping thread-safe concurrent applications  a is a sequence of memory read-write operations that are atomic if all operations in a transaction are completed  the memory transaction is committed ; otherwise  the operations must be aborted and rolled back the benefits of transactional memory can be obtained through features added to a programming language  consider an example suppose we have a function update   that modifies shared data traditionally  this function would be written using locks such as the following  update    acquire   ;  i modify shared data i release   ; however  using synchronization mechanisms such as locks and semaphores involves many potential problems  including deadlocks additionally  as the number of threads increases  traditional locking does not scale well  as an alternative to traditional methods  new features that take advantage of transactional memory can be added to a programming language in our example  suppose we add the construct atomic  s   which ensures that the operations in s execute as a transaction this allows us to rewrite the update   method as follows  update    atomic  i modify shared data i   continued on following page  6.9 259 transactional memory  continued  the advantage of using such a mechanism rather than locks is that the transactional memoi y system ~ not the developer-isrespon.sible for guaranteeing atomicity additionally  the system can identify which statements in atomic blocks can be executed concurrently  such as concurrent read access to a shared variable it is  of course  possible for a programmer to identify these situations and use reader-writer locks  but the task becomes increasingly difficult as the number ofthreads within anapplicationgrows  transactional memory can be implemented in either software or hardware  software transactional memory  stm   as the nam ~ suggests  imp lee ments transactional memory exclusivelyin software ~ nospecial hardware is needed stm works by inserting instrumentation code inside transaction blocks the code is inserted by a compiler and manages each transaction by examining where statements may run concurrently and where specific lowlevellockingis required hardware transactional memory  small htm  uses hardware cache hierarchies and cache coherency protocols to manage and resolve conflicts involving shared data residing in separate processors caches  htm requires no special code instmmentation and thus has less overhead than stm however  htm does require that existing cache hierarchies and cachecoherencyprotocolsbe modified to support transactional memory  transactional memory has existed for several years without widespread implementation however  the growth of multi core systems and the associated emphasis on concurrent programming have prompted a significant amolmt ofresearch in this area on the part of both academics and hard ware vendors  including intel and sun microsystems  ended its normal execution due to some logical error or a system failure  if a terminated transaction has completed its execution successfully  it is otherwise  it is since an aborted transaction may already have modified the data that it has accessed  the state of these data may not be the same as it would have been if the transaction had executed atomically so that atomicity is ensured  an aborted transaction must have no effect on the state of the data that it has already modified thus  the state of the data accessed by an aborted transaction must be restored to what it was just before the transaction started executing we say that such a transaction has been it is part of the responsibility of the system to ensure this property  to determ.ine how the system should ensure atomicity  we need first to identify the properties of devices used for storing the various data accessed by the transactions various types of storage media are distinguished by their relative speed  capacity  and resilience to failure  volatile storage information residing in volatile storage does not usually survive system crashes examples of such storage are main and cache merrwry access to volatile storage is extremely fast  both because of the speed of the memory access itself and because it is possible to access directly any data item in volatile storage  260 chapter 6 nonvolatile storage information residing in nonvolatile storage usually survives system crashes examples of m.edia for such storage are disks and magnetic tapes disks are more reliable than main memory but less reliable than magnetic tapes both disks and tapes  however  are subject to failure  which may result in loss of inform.ation currently  nonvolatile storage is slower than volatile storage by several orders of magnitude  becm1se disk and tape devices are electromechanical and require physical motion to access data  stable storage information residing in stable storage is never lost  never should be taken with a grain of salt  since theoretically such absolutes can not be guaranteed   to implement an approximation of such storage  we need to replicate information in several nonvolatile storage caches  usually disk  with independent failure modes and to update the inform.ation in a controlled manner  section 12.8   here  we are concerned only with ensuring transaction atomicity in an environment where failures result in the loss of inform.ation on volatile storage  6.9.2 log-based recovery one way to ensure atomicity is to record  on stable storage  information describing all the modifications made by the transaction to the various data it accesses the most widely used method for achieving this form of recording is here  the system maintains  on stable storage  a data structure called the each log record describes a single operation of a transaction write and has the following fields  transaction name the unique name of the transaction that performed the write operation data item name the unique name of the data item written old value the value of the data item prior to the write operation new value the value that the data item will have after the write other special log records exist to record significant events during transaction processing  such as the start of a transaction and the commit or abort of a transaction  before a transaction t ; starts its execution  the record t ; starts is written to the log during its execution  any write operation by t ; is preceded by the writing of the appropriate new record to the log when t ; commits  the record t ; commits is written to the log  because the information in the log is used in reconstructing the state of the data items accessed by the various transactions  we can not allow the actual update to a data item to take place before the corresponding log record is written out to stable storage we therefore require that  prior to execution of a wri te  x  operation  the log records corresponding to x be written onto stable storage  note the performance penalty inherent in this system two physical writes are required for every logical write requested also  more storage is needed  both for the data themselves and for the log recording the changes in cases 6.9 261 where the data are extremely important and fast failure recovery is necessary  however  the functionality is worth tl1e price  using the log  the system can handle any failure that does not result in the loss of information on nonvolatile storage the recovery algorithm uses two procedures  undo  t ;   which restores the value of all data updated by transaction t ; to the old values redo  t ;   which sets the value of all data updated by transaction t ; to the new values the set of data updated by t ; and the appropriate old and new values can be found in the log note that the undo and redo operations must be idempotent  that is  multiple executions must have the same result as does one execution  to guarantee correct behavior even if a failure occurs during the recovery process  if a transaction t ; aborts  then we can restore the state of the data that it has updated by simply executing undo  t ;   if a system failure occurs  we restore the state of all updated data by consulting the log to determine which transactions need to be redone and which need to be lmdone this classification of transactions is accomplished as follows  transaction t ; needs to be undone if the log contains the i ; starts record but does not contain the t ; corrnni ts record  transaction t ; needs to be redone if the log contains both the t ; starts and the t ; corrnni ts records  6.9.3 checkpoints when a system failure occurs  we must consult the log to determine which transactions need to be redone and which need to be undone in principle  we need to search the entire log to make these determinations there are two major drawbacks to this approach  the searching process is time consuming  most of the transactions that  according to our algorithm  need to be redone have already actually updated the data that the log says they need to modify although redoing the data modifications will cause no harm  due to idempotency   it will nevertheless cause recovery to take longer  to reduce these types of overhead  we introduce the concept of during execution  the system maintains the write-ahead log in addition  the system periodically performs checkpoints that require the following sequence of actions to take place  output all log records currently residing in volatile storage  usually main memory  onto stable storage  output all modified data residing in volatile storage to the stable storage  output a log record checkpoint onto stable storage  262 chapter 6 the presence of a checkpoint record in the log allows the systen'l to streamline its recovery procedure consider a transaction i ; that committed prior to the checkpoint the t ; commits record appears in the log before the checkpoint record any modifications made by t ; must have been written to stable storage either prior to the checkpoint or as part of the checkpoint itself thus  at recovery time  there is no need to perform a redo operation on t ;  this observation allows us to refine our previous recovery algorithm after a failure has occurred  the recovery routine examines the log to determine the most recent transaction t ; that started executing before the most recent checkpoint took place it finds such a transaction by searching the log backward to find the first checkpoint record and then finding the subsequent t ; start record  once transaction t ; has been identified  the redo and undo operations need be applied only to transaction t ; and all transactions t1 that started executing after transaction i ;  we 'll call these transactions set t the remainder of the log can be ignored the recovery operations that are required are as follows  for all transactions 'nc in t for which the record tic commits appears in the log  execute redo  t/c  for all transactions 'nc in t that have no tic commits record in the log  execute undo  t ; c   6.9.4 concurrent atomic transactions we have been considering an environment in which only one transaction can be executing at a time we now turn to the case where multiple transactions are active simultaneously because each transaction is atomic  the concurrent execution of transactions must be equivalent to the case where these transactions are executed serially in some arbih ary order this property  called can be maintained by simply executing each transaction within a critical section that is  all transactions share a common semaphore mutex  which is initialized to 1 when a transaction starts executing  its first action is to execute wai t  mutex   after the transaction either commits or aborts  it executes signal  mutex   although this scheme ensures the atomicity of all concurrently executing transactions  it is nevertheless too restrictive as we shall see  in many cases we can allow transactions to overlap their execution while maintaining serializability a number of different ensure serializability  and we describe these algorithms next  6.9.4.1 serializability consider a system with two data items  a and b  that are both read and written by two transactions  to and t1 suppose that these transactions are executed atomically in the order t0 followed by t1 this execution sequence  which is called a schedule  is represented in figure 6.22 in schedule 1 of figure 6.22  the sequence of instruction steps is in chronological order from top to bottom  with instructions of to appearing in the left column and instructions of t1 appearing in the right colunm  6.9 263 to t1 read  a  write  a  read  b  write  b  read  a  write  a  read  b  write  b  figure 6.22 schedule i  a serial schedule in which to is followed by t1 a schedule in which each transaction is executed atomically is called a a serial schedule consists of a sequence of instructions from various transactions wherein the instructions belonging to a particular transaction appear together thus  for a set of n transactions  there exist n ! different valid serial schedules each serial schedule is correct  because it is equivalent to the atomic execution of the various participating transactions in some arbitrary order  if we allow the two transactions to overlap their execution  then the resulting schedule is no longer serial a  cj,sef'i  al does not necessarily imply an incorrect execution  that is  an execution that is not equivalent to one represented by a serial schedule   to see that this is the case  we need to define the notion of nflic ; cing consider a schedule s in which there are two consecutive operations 0 ; and oi of transactions ~ and ti  respectively we say that 0 ; and oj conflict if they access the same data item and at least one of them is a write operation  to illustrate the concept of conflicting operations  we consider the nonserial schedule 2 of figure 6.23 the wri te  a  operation of to conflicts with the read  a  operation of t1 however  the wri te  a  operation of t1 does not conflict with the read  b  operation of to  because the two operations access different data items  to t1 read  a  write  a  read  a  write  a  read  b  write  b  read  b  write  b  figure 6.23 schedule 2  a concurrent serializable schedule  264 chapter 6 let 0 ; and 0 ; be consecutive operations of a schedule 5 if 0 ; and oi are operations of different transactions and 0 ; and oi do not conflict then we can swap the order of 0 ; and 0 ; to produce a new schedule 5' we expect 5 to be equivalent to 5 '  as all operations appear in the same order in both schedules  except for 0 ; and 0 1  whose order does not matter  we can illustrate the swapping idea by considering again schedule 2 of figure 6.23 as the wri te  a  operation of t1 does not conflict with the read  b  operation of t0  we can swap these operations to generate an equivalent schedule regardless of the initial system state  both schedules produce the same final system state continuing with this procedure of swapping nonconflicting operations  we get  swap the read  b  operation of to with the read  a  operation of t1  swap the write  b  operation of to with the write  a  operation of t1  swap the wri te  b  operation of to with the read  a  operation of t1  the final result of these swaps is schedule 1 in figure 6.22  which is a serial schedule thus  we have shown that schedule 2 is equivalent to a serial schedule this result implies that regardless of the initial system state  schedule 2 will produce the same final state as will some serial schedule  if a schedule 5 can be transformed into a serial schedule 5 ' swaps of nonconflicting operations  we say that a schedule 5 is izable thus  schedule 2 is conflict serializable  because it can be transformed into the serial schedule 1  6.9.4.2 locking protocol one way to ensure serializability is to associate a lock with each data item and to require that each transaction follow a that governs how locks are acquired and released there are various modes in which a data item can be locked in this section  we restrict our attention to two modes  shared if a transaction 7i has obtained a shared-mode lock  denoted by s  on data item q  then 1i can read this item but can not write q  exclusive if a transaction t ; has obtained an exclusive-mode lock  denoted by x  on data item q  then 7i can both read and write q  we require that every transaction request a lock in an appropriate m.ode on data item q  depending on the type of operations it will perform on q  to access data item q  transaction 1i must first lock q in the appropriate mode if q is not currently locked  then the lock is granted  and t ; can now access it however  if the data item q is currently locked by some other transaction  then t ; may have to wait more specifically  suppose that 1i requests an exclusive lock on q in this case  1i must wait until the lock on q is released  if t ; requests a shared lock on q  then t ; must wait if q is locked in exclusive mode otherwise  it can obtain the lock and access q notice that this scheme is quite similar to the readers-writers algorithm discussed in section 6.6.2  a transaction may unlock a data item that it locked at an earlier point  it must  however  hold a lock on a data item as long as it accesses that item  6.9 265 moreove1 ~ it is not always desirable for a transaction to unlock a data item immediately after its last access of that data item  because serializability may not be ensured  one protocol that ensures serializability is the this protocol requires that each transaction issue lock and unlock requests in two phases  growing phase a transaction may obtain locks but may not release any locks  shrinking phase a transaction may release locks but may not obtain any new locks  initially a transaction is in the growing phase the transaction acquires locks as needed once the transaction releases a lock  it enters the shrinking phase  and no more lock requests can be issued  the two-phase locking protocol ensures conflict serializability  exercise 6.14   it does not  however  ensure freedom from deadlock in addition  it is possible that  for a given set of transactions  there are conflict-serializable schedules that can not be obtained by use of the two-phase locking protocol  to improve performance over two-phase locking  we need either to have additional information about the transactions or to impose some structure or ordering on the set of data  6.9.4.3 timestamp-based protocols in the locking protocols described above  the order followed by pairs of conflicting transactions is determined at execution time another method for determining the serializability order is to select an order in advance the most common method for doing so is to use a ordering scheme  with each transaction ~ in the system  we associate a unique fixed timestamp  denoted by ts  t ;   this timestamp is assigned by the system before the transaction t ; starts execution if a transaction ~ has been assigned timestamp ts  ~   and later a new transaction ti enters the system  then ts  t ;  ts  tj   there are two simple methods for implementing this scheme  use the value of the system clock as the timestamp ; that is  a transaction 's timestamp is equal to the value of the clock when the transaction enters the system this method will not work for transactions that occur on separate systems or for processors that do not share a clock  use a logical counter as the timestamp ; that is  a transaction 's timestamp is equal to the value of the counter when the transaction enters the system  the counter is incremented after a new timestamp is assigned  the timestamps of the transactions determine the serializability order  thus  if ts  ti  ts  tj   then the system must ensure that the schedule produced is equivalent to a serial schedule in which transaction ~ appears before transaction tj  to implement this scheme  we associate with each data item q two timestamp values  266 chapter 6 w-timestamp  q  denotes the largest timestamp of any transaction that successfully executed wri te  q   r-timestamp  q  denotes the largest timestamp of any transaction that successfully executed read  q   these timestamps are updated whenever a new read  q  or wri te  q  instruction is executed  the timestamp ordering protocol ensures that any conflicting read and write operations are executed in timestamp order this protocol operates as follows  suppose that transaction t ; issues read  q   o if ts  ti  w-timestamp    then t ; needs to read a value of q that was already overwritten hence  the read operation is rejected  and t ; is rolled back  o if ts  t ;  2   w-timestamp  q   then the read operation is executed  and r-timestamp  q  is set to the maximum of r-timestamp  q  and ts  t ;   suppose that transaction t ; issues wri te  q   o if ts  t ;  r-timestamp  q   then the value of q that t ; is producing was needed previously and t ; assumed that this value would never be produced hence  the write operation is rejected  and t ; is rolled back  o if ts  t ;  w-timestamp  q   then t ; is attempting to write an obsolete value of q hence  this write operation is rejected  and t ; is rolled back  o otherwise  the write operation is executed  a transaction t ; that is rolled back as a result of either a read or write operation is assigned a new timestamp and is restarted  to illustrate this protocol  consider schedule 3 in figure 6.24  which includes transactions t2 and t3 we assume that a transaction is assigned a timestamp immediately before its first instruction thus  in schedule 3  ts  t2  ts  t3   and the schedule is possible under the timestamp protocol  this execution can also be produced by the two-phase locking protocol  howeve1 ~ some schedules are possible under the two-phase locking protocol but not under the timestamp protocol  and vice versa  t2 t3 read  b  read  b  write  b  read  a  read  a  write  a  figure 6.24 schedule 3  a schedule possible under the timestamp protocol  6.10 267 the timestamp protocol ensures conflict serializability this capability follows from the fact that conflicting operations are processed in timestamp order the protocol also ensures freedom fron1 deadlocl   because no transaction ever waits  given a collection of cooperating sequential processes that share data  mutual exclusion must be provided to ensure that a critical section of code is used by only one process or thread at a tince typically  computer hardware provides several operations that ensure mutual exclusion however  such hardware-based solutions are too complicated for most developers to use  semaphores overcome this obstacle semaphores can be used to solve various synchronization problems and can be implemented efficiently  especially if hardware support for atomic operations is available  various synchronization problems  such as the bounded-buffer problem  the readers-writers problem  and the dining-philosophers problem  are important mainly because they are examples of a large class of concurrency-control problems these problems are used to test nearly every newly proposed synchronization scheme  the operating system must provide the means to guard against timing errors several language constructs have been proposed to deal with these problems  monitors provide the synchronization mechanism for sharing abstract data types a condition variable provides a method by which a monitor procedure can block its execution until it is signaled to continue  operating systems also provide support for synchronization for example  solaris  windows xp  and linux provide mechanisms such as semaphores  mutexes  spinlocks  and condition variables to control access to shared data  the pthreads api provides support for mutexes and condition variables  a transaction is a program unit that must be executed atomically ; that is  either all the operations associated with it are executed to completion  or none are performed to ensure atomicity despite system failure  we can use a write-ahead log all updates are recorded on the log  which is kept in stable storage if a system crash occurs  the information in the log is used in restoring the state of the updated data items  which is accomplished by use of the undo and redo operations to reduce the overhead in searching the log after a system failure has occurred  we can use a checkpoint scheme  to ensure serializability when the execution of several transactions overlaps  we must use a concurrency-control scheme various concurrency-control schemes ensure serializability by delaying an operation or aborting the transaction that issued the operation the most common ones are locking protocols and timestamp ordering schemes  6.1 the first known correct software sohjtion to the critical-section problem for two processes was developed by dekker the two processes  p0 and p1  share the following variables  boolean flag  2  ; i initially false i int turn ; 268 chapter 6 do  flag  i  = true ; while  flag  j     if  turn = = j   flag  i  = false ; while  turn = = j  ; ii do nothing flag  i  = true ;  ii critical section turn = j ; flag  i  = false ; ii remainder section  while  true  ; figure 6.25 the structure of process a in dekker 's algorithm  the structure of process pi  i = = 0 or 1  is shown in figure 6.25 ; the other process is p1  j = = 1 or 0   prove that the algorithm satisfies all three requirements for the critical-section problem  6.2 explain why interrupts are not appropriate for implementing synchronization primitives in multiprocessor systems  6.3 the first known correct software solution to the critical-section problem for n processes with a lower bound on waiting of n  1 turns was presented by eisenberg and mcguire the processes share the following variables  enum pstate  idle  want_in  in_cs  ; pstate flag  n  ; int turn ; all the elements of flag are initially idle ; the initial value of turn is immaterial  between 0 and n-1   the structure of process pi is shown in figure 6.26 prove that the algorithm satisfies all three requiren'lents for the critical-section problem  6.4 write a monitor that implements an alarm clock that enables a calling program to delay itself for a specified number of tirne units  ticks   you may assume the existence of a real hardware clock that invokes a procedure hclc in your monitor at regular intervals  6.5 a file is to be shared among different processes  each of which has a unique number the file can be accessed simultaneously by several processes  subject to the following constraint  the sum of all unique do  while  true   flag  i  = want_in ; j = turn ;  while  j ! = i    if  flag  j  i = idle   j = turn ; else j =  j + 1  % n ; flag  i  j = 0 ; in_cs ; while   j n  &&  j j + + ; if   j = n  &&  turn break ; ii critical section j =  turn + 1  % n ; while  flag  j  = = idle  j =  j + 1  % n ; turn = j ; flag  i  = idle ; ii remainder section  while  true  ; i ii flag  j  ! = in_cs   i i i flag  turn  idle   figure 6.26 the structure of process a in eisenberg and mcguire 's algorithm  269 numbers associated with all the processes currently accessing the file must be less than n write a monitor to coordinate access to the file  6.6 the decrease_count   function in the previous exercise currently returns 0 if sufficient resources are available and -1 otherwise this leads to awkward programming for a process that wishes to obtain a number of resources  while  decrease_count  count  = = -1  rewrite the resource-manager code segment using a monitor and condition variables so that the decrease_count   function suspends 270 chapter 6 the process until sufficient resources are available this will allow a process to invoke decrease_count   by simply calling decrease_count  count  ; the process will return from this function call only when sufficient resources are available  6.7 exercise 4.12 requires the parent thread to wait for the child thread to finish its execution before printing out the computed values if we let the parent thread access the fibonacci numbers as soon as they have been computed by the child thread  rather than waiting for the child thread to terminate explain what changes would be necessary to the solution for this exercise implement your modified solution  6.8 in section 6.4  we mentioned that disabling interrupts frequently can affect the system 's clock explain why this can occur and how such effects can be mil1.imized  6.9 servers can be designed to limit the number of open coru1.ections for example  a server may wish to have only n socket com1.ections at any point in time as soon as n connections are made  the server will not accept another incoming connection until an existing connection is released explain how semaphores can be used by a server to limit the number of concurrent connections  6.10 why do solaris  lil1.ux  and windows xp use spinlocks as a synchronization mechanism only on multiprocessor systems and not on single-processor systems 6.11 show that  if the wait   and signal   semaphore operations are not executed atomically  then mutual exclusion may be violated  6.12 show how to implement the wait   and signal   semaphore operations in multiprocessor environments using the testandset   instruction  the solution should exhibit minimal busy waiting  6.13 suppose we replace the wait   and signal   operations of monitors with a single construct await  b   where b is a general boolean expression that causes the process executing it to wait until b becomes true  a write a monitor using this scheme to implement the readerswriters problem  b explain why  in general  this construct can not be implemented efficiently  c what restrictions need to be put on the await statement so that it can be implemented efficiently  hint  restrict the generality of b ; see kessels  1977    271 6.14 show that the two-phase locking protocol ensures conflict serializability  6.15 how does the signal   operation associated with monitors differ from the corresponding operation defined for semaphores 6.16 describe how volatile  nonvolatile  and stable storage differ in cost  6.17 explain why implementing synchronization primitives by disabling interrupts is not appropriate in a single-processor system if the synchronization primitives are to be used in user-level programs  6.18 consider a system consisting of processes p1  p2    p11  each of which has a unique priority number write a monitor that allocates three identical line printers to these processes  using the priority numbers for deciding the order of allocation  6.19 describe two kernel data structures in which race conditions are possible  be sure to include a description of how a race condition can occur  6.20 assume that a finite number of resources of a single resource type must be managed processes may ask for a number of these resources and -once finished-will return them as an example  many commercial software packages provide a given number of licenses  indicating the number of applications that may run concurrently when the application is started  the license count is decremented when the application is terminated  the license count is incremented if all licenses are in use  requests to start the application are denied such requests will only be granted when an existing license holder terminates the application and a license is returned  the following program segment is used to manage a finite number of instances of an available resource the maximum number of resources and the number of available resources are declared as follows  # define max_resources 5 int available_resources = max_resources ; when a process wishes to obtain a number of resources  it invokes the decrease_count   function  i decrease available_resources by count resources i i return 0 if sufficient resources available  i i otherwise return -1 i int decrease_count  int count    if  available_resources count  return -1 ; else  available_resources count ; return 0 ;  272 chapter 6 when a process wants to return a number of resourcesf it calls the increase_count   function  i increase available_resources by count i int increase_count  int count   available_resources + = count ; return 0 ;  the preceding program segment produces a race condition do the following  a identify the data involved in the race condition  b identify the location  or locations  in the code where the race condition occurs  c using a semaphoref fix the race condition it is ok to modify the decrease_count   fun.ction so that the calling process is blocked until sufficient resources are available  6.21 explain why spinlocks are not appropriate for single-processor systems yet are often used in multiprocessor systems  6.22 the cigarette-smokers problem consider a system with three smoker processes and one agent process each smoker continuously rolls a cigarette and then smokes it but to roll and smoke a cigarettef the smoker needs three ingredients  tobaccof paperf and matches one of the smoker processes has paperf another has tobaccof and the third has matches the agent has an infinite supply of all three materials the agent places two of the ingredients on the table the smoker who has the remaining ij.l.gredient then makes and smokes a cigarette  signaling the agent on completion the agent then puts out another two of the three ingredients  and the cycle repeats write a program to synchronize the agent and the smokers using java synchronization  6.23 describe how the swap   instruction can be used to provide mutual exclusion that satisfies the bounded-waiting requirement  6.24 a new lightweight synchronization tool called locks whereas most implementations of readerwriter locks favor either readers or writers  or perhaps order waiting threads using a fifo policy  slim reader-writer locks favor neither readers nor writers  nor are waiting threads ordered in a fifo queue  explain the benefits of providing such a synchronization tool  6.25 what are the implications of assigning a new timestamp to a transaction that is rolled back how does the system process transactions that were issued after the rolled -back transaction b-ut that have timestamps smaller than the new timestamp of the rolled-back transaction 273 6.26 discuss the tradeoff between fairness and throughput of operations in the readers-writers problem propose a method for solving the readers-writers problem without causing starvation  6.2'7 when a signal is performed on a condition inside a monitor  the signaling process can either continue its execution or transfer control to the process that is signaled how would the solution to the preceding exercise differ with these two different ways in which signaling can be performed 6.28 what is the meaning of the term busy waiting what other kinds of waiting are there in an operating system can busy waiting be avoided altogether explain your answer  6.29 demonstrate that monitors and semaphores are equivalent insofar as they can be used to implement the same types of synchronization problems  6.30 in log-based systems that provide support for transactions  updates to data items can not be performed before the corresponding entries are logged why is this restriction necessary 6.31 explain the purpose of the checkpoint mechanism how often should checkpoints be performed describe how the frequency of checkpoints affects  system performance when no failure occurs the time it takes to recover from a system crash the time it takes to recover from a disk crash 6.32 write a bounded-buffer monitor in which the buffers  portions  are embedded within the monitor itself  6.33 the strict mutual exclusion within a monitor makes the bounded-buffer monitor of exercise 6.32 mainly suitable for small portions  a explain why this is true  b design a new scheme that is suitable for larger portions  6.34 race conditions are possible in many computer systems consider a banking system with two functions  deposit  amount  and withdraw  amount   these two functions are passed the amount that is to be deposited or withdrawn from a bank account assume a shared bank account exists between a husband and wife and concurrently the husband calls the withdraw   function and the wife calls deposit    describe how a race condition is possible and what might be done to prevent the race condition from occurring  274 chapter 6 6.35 suppose the signal   statement can appear only as the last statement in a monitor procedure suggest how the implementation described in section 6.7 can be simplified in this situation  6.36 the sleeping-barber problem a barbershop consists of a waiting room with n chairs and a barber roorn with one barber chair if there are no customers to be served  the barber goes to sleep if a customer enters the barbershop and all chairs are occupied  then the customer leaves the shop if the barber is busy but chairs are available  then the customer sits in one of the free chairs if the barber is asleep  the customer wakes up the barber write a program to coordinate the barber and the customers  6.37 producer-consumer problem in section 6.6.1  we had presented a semaphore-based solution to the producer-consumer problem using a bounded buffer in this project  we will design a programming solution to the bounded-buffer problem using the producer and consumer processes shown in figures 6.10 and 6.11 the solution presented in section 6.6.1 uses three semaphores  empty and full  which count the number of empty and full slots in the buffer  and mutex  which is a binary  or mutual-exclusion  semaphore that protects the actual insertion or removal of items in the buffer for this project  standard counting semaphores will be used for empty and full  and a mutex lock  rather than a binary semaphore  will be used to represent mutex the producer and consumer-running as separate threads-will move items to and from a buffer that is synchronized with these empty  full  and mutex structures you can solve this problem using either pthreads or the win32 api  the buffer internally  the buffer will consist of a fixed-size array of type buffer_i tern  which will be defined using a typedef   the array of buffer_i tern objects will be manipulated as a circular queue the definition of buffer _i tern  along with the size of the buffer  can be stored in a header file such as the following  i buffer.h i typedef int buffer_item ; # define buffer_size 5 the buffer will be manipulated with two functions  insert_i tern   and remove_i tern   ,which are called by the producer and consumer threads  respectively a skeleton outlining these functions appears in figure 6.27  # include buffer.h i the buffer i buffer_item buffer  buffer_size  ; int insert_item  buffer_item item   i insert item into buffer return 0 if successful  otherwise return -1 indicating an error condition i  int remove_item  buffer_item item    i remove an object from buffer placing it in item return 0 if successful  otherwise return -1 indicating an error condition i figure 6.27 a skeleton program  275 the insert_item   and remove_item   functions will synchronize the producer and consumer using the algorithms outlined in figures 6.10 and 6.11 the buffer will also require an initialization function that initializes the mutual-exclusion object mutex along with the empty and full semaphores  the main   f-lmction will initialize the buffer and create the separate producer and consumer threads once it has created the producer and consumer threads  the main   function will sleep for a period of time and  upon awakening  will terminate the application the main   function will be passed three parameters on the command line  a how long to sleep before terminating b the number of producer threads c the nuncber of consumer threads a skeleton for this function appears in figure 6.28  # include buffer.h int main  int argc  char argv      i 1 get command line arguments argv  1  ,argv  2  ,argv  3  i i 2 initialize buffer i i 3 create producer thread  s  i i 4 create consumer thread  s  i i 5 sleep i i 6 exit i figure 6.28 a skeleton program  276 chapter 6 producer and consumer threads the producer thread will alternate between sleeping for a random period of time and inserting a random integer into the buffer random numbers will be produced using the rand   function  which produces random integers between 0 and rand..max the consumer will also sleep for a random period of time and  upon awakening  will attempt to remove an item from the buffer an outline of the producer and consumer threads appears in figure 6.29  in the following sections  we first cover details specific to pthreads and then describe details of the win32 api  pthreads thread creation creating threads using the pthreads api is discussed in chapter 4 please refer to that chapter for specific instructions regarding creation of the producer and consumer using pthreads  # include stdlib.h i required for rand   i # include buffer.h void producer  void pararn   buffer_item item ;  while  true   i sleep for a random period of time i sleep    ; i generate a random number i item = rand   ; if  insert_item  item   fprintf  report error condition  ; else printf  producer produced % d \ n ,item  ; void consumer  void pararn   buffer_item item ;  while  true   i sleep for a random period of time i sleep    ; if  remove_item  &item   fprintf  report error condition  ; else printf  consumer consumed % d \ n ,item  ; figure 6.29 an outline of the producer and consumer threads  # include pthread.h pthread_mutex_t mutex ; i create the mutex lock i pthread_mutex_init  &mutex,null  ; i acquire the mutex lock i pthread_mutex_lock  &mutex  ; i critical section i i release the mutex lock i pthread_mutex_unlock  &mutex  ; figure 6.30 code sample  pthreads mutex locks 277 the code sample depicted in figure 6.30 illustrates how mutex locks available in the pthread api can be used to protect a critical section  pthreads uses the pthread_mutex_t data type for mutex locks  a mutex is created with the pthread_mutex_ini t  &mutex  null  function  with the first parameter being a pointer to the mutex  by passing null as a second parameter  we initialize the mutex to its default attributes the mutex is acquired and released with the pthread_mutex_lock   and pthread_mutex_unlock   functions  if the mutex lock is unavailable when pthread_mutex_lock   is invoked  the callil1.g thread is blocked until the owner invokes pthread_mutex_unlock 0 all mutex ftmctions return a value of 0 with correct operation ; if an error occurs  these functions return a nonzero error code  pthreads semaphores pthreads provides two types of semaphores-named and unnamed for this project  we use unnamed semaphores the code below illush ates how a semaphore is created  # include semaphore.h sem_t sem ; i create the semaphore and initialize it to 5 i sem_init  &sem  0  5  ; the sem_ini t   creates and initializes a semaphore this function is passed three parameters  a a pointer to the semaphore b a flag indicating the level of sharing c the semaphore 's initial value 278 chapter 6 # include semaphore.h sem_t mutex ; i create the semaphore i sem_init  &mutex  0  1  ; i acquire the semaphore i sem_wait  &mutex  ; i critical section i i release the semaphore i sem_post  &mutex  ; figure 6.31 aaa5  in this example  by passing the flag 0  we are indicating that this semaphore can only be shared by threads belonging to the same process that created the semaphore a nonzero value would allow other processes to access the semaphore as well in this example  we initialize the semaphore to the value 5  in section 6.5  we described the classical wait   and signal   semaphore operations pthreads names the wait   and signal   operations sem_wai t   and sem_post   ,respectively the code example shown in figure 6.31 creates a binary semaphore mutex with an initial value of 1 and illustrates its use in protecting a critical section  win32 details concerning thread creation using the win32 api are available in chapter 4 please refer to that chapter for specific instructions  win32 mutex locks mutex locks are a type of dispatcher object  as described in section 6.8.2 the following illustrates how to create a mutex lock using the createmutex   function  # include windows.h handle mutex ; mutex = createmutex  null  false  null  ; the first parameter refers to a security attribute for the mutex lock by setting this attribute to null  we are disallowing any children of the process creating this mutex lock to inherit the handle of the mutex  the second parameter indicates whether the creator of the mutex is the initial owner of the mutex lock passing a value off alse indicates that the thread creating the mutex is not the initial owner ; we shall soon see how mutex locks are acquired the third parameter allows naming of 279 the mutex however  because we provide a value of null  we do not name the mutex if successful  createmutex   returns a handle to the mutex lock ; otherwise  it returns null  in section 6.8.2  we identified dispatcher objects as being either signaled or nonsignaled a signaled object is available for ownership ; once a dispatcher object  such as a mutex lock  is acquired  it moves to the nonsignaled state when the object is released  it returns to signaled  mutex locks are acquired by invoking the wai tforsingleobj ect   function  passing the function the handle to the lock and a flag indicating how long to wait the following code demonstrates how the mutex lock created above can be acquired  waitforsingleobject  mutex  infinite  ; the parameter value infinite indicates that we will wait an infinite amount of time for the lock to become available other values could be used that would allow the calling thread to time out if the lock did not become available within a specified time if the lock is in a signaled state  wai tforsingleobj ect   returns immediately  and the lock becomes nonsignaled a lock is released  moves to the signaled state  by invoking re leas emu t ex    such as  releasemutex  mutex  ; win32 semaphores semaphores in the win32 api are also dispatcher objects and thus use the same signaling mechanism as mutex locks semaphores are created as follows  # include windows.h handle sem ; sem = createsemaphore  null  1  5  null  ; the first and last parameters identify a security attribute and a name for the semaphore  similar to what was described for mutex locks the second and third parameters indicate the initial value and maximum value of the semaphore in this instance  the initial value of the semaphore is 1  and its maximum value is 5 if successful  createsemaphore   returns a handle to the mutex lock ; otherwise  it returns null  semaphores are acquired with the same wai tforsingleobj ect   function as mutex locks we acquire the semaphore sem created in this example by using the statement  waitforsingleobject  semaphore  infinite  ; if the value of the semaphore is 0  the semaphore is in the signaled state and thus is acquired by the calling thread otherwise  the calling thread blocks indefinitely-as we are specifying infinite-until the semaphore becomes signaled  280 chapter 6 the equivalent of the signal   operation on win32 semaphores is the releasesemaphore   function this function is passed three parameters  a the handle of the semaphore b the amount by which to increase the value of the semaphore c a pointer to the previous value of the semaphore we can increase sem by 1 using the following statement  releasesemaphore  sem  1  ~ ll  ; both releasesemaphore   and releasemutex   return nonzero if successful and zero otherwise  the mutual-exclusion problem was first discussed in a classic paper by dijkstra  1965a   dekker 's algorithm  exercise 6.1  -the first correct software solution to the two-process mutual-exclusion problem-was developed by the dutch mathematician t dekker this algorithm also was discussed by dijkstra  1965a   a simpler solution to the two-process mutual-exclusion problem has since been presented by peterson  1981   figure 6.2   dijkstra  1965b  presented the first solution to the mutual-exclusion problem for n processes this solution  however  does not have an upper bound on the amount of time a process must wait before it is allowed to enter the critical section knuth  1966  presented the first algorithm with a bound ; his bound was 211 turns a refinement of knuth 's algorithm by debruijn  1967  reduced the waiting time to n2 turns  after which eisenberg and mcguire  1972  succeeded in reducing the time to the lower bound of n-1 turns another algorithm that also requires n-1 turns but is easier to program and to understand is the bakery algorithm  which was developed by lamport  1974   burns  1978  developed the hardware-solution algorithm that satisfies the bounded-waiting requirement  general discussions concerning the mutual-exclusion problem were offered by lamport  1986  and lamport  1991   a collection of algorithms for mutual exclusion was given by raynal  1986   the semaphore concept was suggested by dijkstra  1965a   patil  1971  examined the question of whether semaphores can solve all possible synchronization problems parnas  1975  discussed some of the flaws in patil 's arguments kosaraju  1973  followed up on patil 's work to produce a problem that can not be solved by wait   and signal   operations lipton  1974  discussed the limitations of various synchronization primitives  the classic process-coordination problems that we have described are paradigms for a large class of concurrency-control problems the boundedbuffer problem  the dining-philosophers problem  and the sleeping-barber problem  exercise 6.36  were suggested by dijkstra  1965a  and dijkstra  1971   the cigarette-smokers problem  exercise 6.22 was developed by patil  1971   the readers-writers problem was suggested by courtois et al  1971   the 281 issue of concurrent reading and writing was discussed by lamport  1977   the problem of synchronization of independent processes was discussed by lamport  1976   the critical-region concept was suggested by hoare  1972  and by brinchhansen  1972   the monitor concept was developed by brinch-hansen  1973   a complete description of the monitor was given by hoare  1974   kessels  1977  proposed an extension to the monitor to allow automatic signalil1.g  experience obtained from the use of monitors in concurrent programs was discussed by lampson and redell  1979   they also examined the priority inversion problem general discussions concerning concurrent programming were offered by ben-ari  1990  and birrell  1989   optimizing the performance of lockil1.g primitives has been discussed in many works  such as lamport  1987   mellor-crummey and scott  1991   and anderson  1990   the use of shared objects that do not require the use of critical sections was discussed in herlihy  1993   bershad  1993   and kopetz and reisinger  1993   novel hardware instructions and their utility in implementing synchronization primitives have been described in works such as culler et al   1998   goodman et al  1989   barnes  1993   and herlihy and moss  1993   some details of the locking mechanisms used in solaris were presented in mauro and mcdougall  2007   note that the locking mechanisms used by the kernel are implemented for user-level threads as well  so the same types of locks are available inside and outside the kernel details of windows 2000 synchronization can be found in solomon and russinovich  2000   goetz et al   2006  presents a detailed discussion of concurrent programming in java as well as the java util concurrent package  the write-ahead log scheme was first mtroduced in system r by gray et al   1981   the concept of serializability was formulated by eswaran et al  1976  in connection with their work on concurrency control for system r the two-phase locking protocol was introduced by eswaran et al  1976   the timestampbased concurrency-control scheme was provided by reed  1983   various timestamp-based concurrency-control algorithms were explail1.ed by bernstem and goodman  1980   adl-tabatabai et al  2007  discusses transactional memory  7.1 ch er in a multiprogramming environment  several processes may compete for a finite number of resources a process requests resources ; if the resources are not available at that time  the process enters a waiting state sometimes  a waiting process is never again able to change state  because the resources it has requested are held by other waiting processes this situation is called a deadlock we discussed this issue briefly in chapter 6 in cmmection with semaphores  perhaps the best illustration of a deadlock can be drawn from a law passed by the kansas legislature early in the 20th century it said  in part  when two trains approach each other at a crossing  both shall come to a full stop and neither shall start up again until the other has gone  in this chapter  we describe methods that an operating system can use to prevent or deal with deadlocks although some applications can identify programs that may deadlock  operating systems typically do not provide deadlock-prevention facilities  and it remains the responsibility of programmers to ensure that they design deadlock-free programs deadlock problems can only become more common  given current trends  including larger numbers of processes  multithreaded programs  many more resources withirt a system  and an emphasis on long-lived file and database servers rather than batch systems  to develop a description of deadlocks  which prevent sets of concurrent processes from completing their tasks  to present a number of different methods for preventing or avoiding deadlocks in a computer system  a system consists of a finite number of resources to be distributed among a number of competing processes the resources are partitioned into several 283 284 chapter 7 types  each consisting of some number of identical instances memory space  cpu cycles  files  and i/0 devices  such as printers and dvd drives  are examples of resource types if a system has two cpus  then the resource type cpu has two instances similarly  the resource type printer may have five instances  if a process requests an instance of a resource type  the allocation of any instance of the type will satisfy the request if it will not  then the instances are not identical  and the resource type classes have not been defined properly for example  a system may have two printers these two printers may be defined to be in the same resource class if no one cares which printer prints which output  however  if one printer is on the ninth floor and the other is in the basement  then people on the ninth floor may not see both printers as equivalent  and separate resource classes may need to be defined for each printer  a process must request a resource before using it and must release the resource after using it a process may request as many resources as it requires to carry out its designated task obviously  the number of resources requested may not exceed the total number of resources available in the system in other words  a process can not request three printers if the system has only two  under the normal mode of operation  a process may utilize a resource in only the following sequence  request the process requests the resource if the request can not be granted immediately  for example  if the resource is being used by another process   then the requesting process must wait until it can acquire the resource  use the process can operate on the resource  for example  if the resource is a printer  the process can print on the printer   release the process releases the resource  the request and release of resources are system calls  as explained in chapter 2 examples are the request   and release   device  open   and close   file  and allocate   and free   memory system calls request and release of resources that are not managed by the operating system can be accomplished through the wait   and signal   operations on semaphores or through acquisition and release of a mutex lock for each use of a kernelmanaged resource by a process or thread  the operating system checks to make sure that the process has requested and has been allocated the resource  a system table records whether each resource is free or allocated ; for each resource that is allocated  the table also records the process to which it is allocated if a process requests a resource that is currently allocated to another process  it can be added to a queue of processes waiting for this resource  a set of processes is in a deadlocked state when every process in the set is waiting for an event that can be caused only by another process in the set the events with which we are mainly concerned here are resource acquisition and release the resources may be either physical resources  for example  printers  tape drives  memory space  and cpu cycles  or logical resources  for example  files  semaphores  and monitors   however  other types of events may result in deadlocks  for example  the ipc facilities discussed in chapter 3   to illustrate a deadlocked state  consider a system with three cd rw drives  suppose each of three processes holds one of these cd rw drives if each process 7.2 7.2 285 now requests another drive  the three processes will be in a deadlocked state  each is waiting for the event cd rw is released  which can be caused only by one of the other waiting processes this example illustrates a deadlock involving the same resource type  deadlocks may also involve different resource types for example  consider a system with one printer and one dvd drive suppose that process p ; is holding the dvd and process pi is holding the printer if p ; requests the printer and p1 requests the dvd drive  a deadlock occurs  a programmer who is developing multithreaded applications must pay particular attention to this problem multithreaded programs are good candidates for deadlock because multiple threads can compete for shared resources  in a deadlock  processes never finish executing  and system resources are tied up  preventing other jobs from starting before we discuss the various methods for dealing with the deadlock problem  we look more closely at features that characterize deadlocks  7.2.1 necessary conditions a deadlock situation can arise if the following four conditions hold simultaneously in a system  mutual exclusion at least one resource must be held in a nonsharable mode ; that is  only one process at a time can use the resource if another deadlock with mutex locks let 's see how deadlock can occur in a multithreaded pthread program using mutex locks the pthread....mutex_ini t   function initializes an unlocked mutex mutex locks are acquired and released using pthread....mutex_lock   and pthread....mutex_unlock    respectively  if a thread attempts to acquire a locked mutex  the call to pthread....mutex_lock 0 blocks the thread until the owner of the mutex lock invokes pthread....mutex_unlock    two mutex locks are created in the following code example  i create and initialize the mutex locks i pthread....mutex_t first....mutex ; pthread....mutex_t second_nmtex ; pthread....mutex_ini t  &first....mutex  null  ; pthread....mutex_ini t  &second....mutex  null  ; next  two threads-thread_one and thread_two-'-are created  and both these threads have access to both mutex locks thread_one and thread_ two run in the functions do_work_one   and do_work_two    respectively  as shown in figure 7.1  286 chapter 7 deadlock with mutex locks  continued  i thread_one runs in this function i void do_work_one  void param    pthread_mutex_lock  &first_mutex  ; pthread_mutex_lock  &second_mutex  ; i do some work i pthread_mutex  _unlock  &second_mutex  ; pthread_mutex_unlock  &first_mutex  ; pthread_exit  0  ; i thread_two runs in this function i void do_work_two  void param    pthread_mutex_lock  &second_mutex  ; pthread_mutex_lock  &first_mutex  ; i do some work i pthread_mutex_unlock  &first_mutex  ; pthread_mutex_unlock  &second_mutex  ; pthread_exi t  0  ; figure 7.1 deadlock example  in this example  thread_one attempts to acquire the mutex locks in the order  1  first_mutex   2  second_mutex  while thread_two attempts to acquire the mutex locks in the order  1  second__mutex   2  first_mutex  deadlock is possible if thread_one acquires first __mutex while thread_ two aacquites second__mutex  note that  even though deadlock is possible  it will not occur if thread_one is able to acquire and release the mutex locks for first_mutex and second_ mutex before thread_two attemptsto acquire the locks this example illustrates a problem with handling deadlocks  it is difficult to identify and test for deadlocks that may occur only under certain circumstances  process requests that resource  the requesting process must be delayed until the resource has been released  hold and wait a process must be holding at least one resource and waiting to acquire additional resources that are cmrently being held by other processes  7.2 287 no preemption resources can not be preempted ; that is  a resource can be released only voluntarily by the process holding it  after that process has completed its task  circular wait a set  p0  pl    p11  of waiting processes must exist such that po is waiting for a resource held by p1  p1 is waiting for a resource held by p2    pn-1 is waiting for a resource held by p,v and p11 is waiting for a resource held by po  we emphasize that all four conditions must hold for a deadlock to occur the circular-wait condition implies the hold-and-wait condition  so the four conditions are not completely independent we shall see in section 7.4  however  that it is useful to consider each condition separately  7.2.2 resource-allocation graph deadlocks can be described more precisely in terms of a directed graph called a graph this graph consists of a set of vertices v and a set of edges e the set of vertices vis partitioned into two different types of nodes  p = =  p1  p2    pn   the set consisting of all the active processes in the system  and r = =  r1  r2    rml the set consisting of all resource types in the system  a directed edge from process g to resource type rj is denoted by p ;  + rj ; it signifies that process p ; has requested an instance of resource type rj and is currently waiting for that resource a directed edge from resource type rj to process p ; is denoted by r1  + p ; ; it signifies that an instance of resource type r1 has been allocated to process p ;  a directed edge p ;  + rj is called a edge ; a directed edge r1  + p ; is called an pictorially we represent each process p ; as a circle and each resource type rj as a rectangle since resource type ri may have more than one instance  we represent each such instance as a dot within the rectangle note that a request edge points to only the rectangle r1  whereas an assignment edge must also designate one of the dots in the rectangle  when process p ; requests an instance of resource type ri  a request edge is inserted in the resource-allocation graph when this request can be fulfilled  the request edge is instantaneously transformed to an assignment edge when the process no longer needs access to the resource  it releases the resource ; as a result  the assignment edge is deleted  the resource-allocation graph shown in figure 7.2 depicts the following situation  the sets p  k and e  o p = =  p1  p2  p3  or = =  r1  r2  r3  ~  0 e = =  pl + rlf p2 + r3f rl + p2f r2 + p2f r2 + pl  r3 + p3  resource instances  o one instance of resource type r1 o two instances of resource type r2 288 chapter 7 figure 7.2 resource-allocation graph  o one instance of resource type r3 o three instances of resource type ~ process states  o process p1 is holding an instance of resource type r2 and is waiting for an instance of resource type r1  o process p2 is holding an instance of r1 and an instance of r2 and is waiting for an instance of r3  o process p3 is holding an instance of r3  given the definition of a resource-allocation graph  it can be shown that  if the graph contains no cycles  then no process in the system is deadlocked if the graph does contain a cycle  then a deadlock may exist  if each resource type has exactly one instance  then a cycle implies that a deadlock has occurred if the cycle involves only a set of resource types  each of which has only a single instance  then a deadlock has occurred each process involved in the cycle is deadlocked in this case  a cycle in the graph is both a necessary and a sufficient condition for the existence of deadlock  if each resource type has several instances  then a cycle does not necessarily imply that a deadlock has occurred in this case  a cycle in the graph is a necessary but not a sufficient condition for the existence of deadlock  to illustrate this concept  we return to the resource-allocation graph depicted in figure 7.2 suppose that process p3 requests an instance of resource type r2 since no resource instance is currently available  a request edge p3  + r2 is added to the graph  figure 7.3   at this point  two minimal cycles exist in the system  p1  + r 1  + p2  + r3  + p3  + r2  + p1 p2  + r3  + p3  + r2  + p2 7.2 deadlock characterization 289 figure 7.3 resource-allocation graph with a deadlock  processes p1  pz  and p3 are deadlocked process pz is waiting for the resource r3  which is held by process p3 process p3 is waiting for either process p1 or process pz to release resource r2 in addition  process p1 is waiting for process pz to release resource r1  now consider the resource-allocation graph in figure 7.4 in this example  we also have a cycle  however  there is no deadlock observe that process p4 may release its instance of resource type r2 that resource can then be allocated to p3  breaking the cycle  in summary  if a resource-allocation graph does not have a cycle  then the system is not in a deadlocked state if there is a cycle  then the system may or may not be in a deadlocked state this observation is important when we deal with the deadlock problem  figure 7.4 resource-allocation graph with a cycle but no deadlock  290 chapter 7 7.3 generally speaking  we can deal with the deadlock problem in one of three ways  we can use a protocol to prevent or avoid deadlocks  ensuring that the system will never enter a deadlocked state  we can allow the system to enter a deadlocked state  detect it  and recover  we can ignore the problem altogether and pretend that deadlocks never occur in the system  the third solution is the one used by most operating systems  including unix and windows ; it is then up to the application developer to write programs that handle deadlocks  next  we elaborate briefly on each of the three methods for handling deadlocks then  in sections 7.4 through 7.7  we present detailed algorithms  before proceeding  we should mention that some researchers have argued that none of the basic approaches alone is appropriate for the entire spectrum of resource-allocation problems in operating systems the basic approaches can be combined  however  allowing us to select an optimal approach for each class of resources in a system  to ensure that deadlocks never occur  the prevention or a deadlock-avoidance scheme provides a set of methods for ensuring that at least one of the necessary conditions  section 7.2.1  can not hold these methods prevent deadlocks by constraining how requests for resources can be made we discuss these methods in section 7.4  requires that the operating system be given in advance additional information concerning which resources a process will request and use during its lifetime with this additional knowledge  it can decide for each request whether or not the process should wait to decide whether the current request can be satisfied or must be delayed  the system must consider the resources currently available  the resources currently allocated to each process  and the future requests and releases of each process we discuss these schemes in section 7.5  if a system does not employ either a deadlock-prevention or a deadlockavoidance algorithm  then a deadlock situation may arise in this environment  the system can provide an algorithm that examines the state of the system to determine whether a deadlock has occurred and an algorithm to recover from the deadlock  if a deadlock has indeed occurred   we discuss these issues in section 7.6 and section 7.7  in the absence of algorithms to detect and recover from deadlocks  we may arrive at a situation in which the system is in a deadlock state yet has no way of recognizing what has happened in this case  the undetected deadlock will result in deterioration of the system 's performance  because resources are being held by processes that can not run and because more and more processes  as they make requests for resources  will enter a deadlocked state eventually  the system will stop functioning and will need to be restarted manually  7.4 7.4 291 although this method may not seem to be a viable approach to the deadlock problem  it is nevertheless used in most operating systems  as mentioned earlier in many systems  deadlocks occur infrequently  say  once per year  ; thus  this method is cheaper than the prevention  avoidance  or detection and recovery methods  which must be used constantly also  in some circumstances  a system is in a frozen state but not in a deadlocked state we see this situation  for example  with a real-time process running at the highest priority  or any process running on a nonpreemptive scheduler  and never returning control to the operating system the system must have manual recovery methods for such conditions and may simply use those techniques for deadlock recovery  as we noted in section 7.2.1  for a deadlock to occur  each of the four necessary conditions must hold by ensuring that at least one of these conditions can not hold  we can prevent the occurrence of a deadlock we elaborate on this approach by examining each of the four necessary conditions separately  7.4.1 mutual exclusion the mutual-exclusion condition must hold for nonsharable resources for example  a printer can not be simultaneously shared by several processes  sharable resources  in contrast  do not require mutually exclusive access and thus can not be involved in a deadlock read-only files are a good example of a sharable resource if several processes attempt to open a read-only file at the same time  they can be granted simultaneous access to the file a process never needs to wait for a sharable resource in general  however  we can not prevent deadlocks by denying the mutual-exclusion condition  because some resources are intrinsically nonsharable  7.4.2 hold and wait to ensure that the hold-and-wait condition never occurs in the system  we must guarantee that  whenever a process requests a resource  it does not hold any other resources one protocol that can be used requires each process to request and be allocated all its resources before it begins execution we can implement this provision by requiring that system calls requesting resources for a process precede all other system calls  an alternative protocol allows a process to request resources only when it has none a process may request some resources and use them before it can request any additional resources  however  it must release all the resources that it is currently allocated  to illustrate the difference between these two protocols  we consider a process that copies data from a dvd drive to a file on disk  sorts the file  and then prints the results to a printer if all resources must be requested at the beginning of the process  then the process must initially request the dvd drive  disk file  and printer it will hold the printer for its entire execution  even though it needs the printer only at the end  the second method allows the process to request initially only the dvd drive and disk file it copies from the dvd drive to the disk and then releases 292 chapter 7 both the dvd drive and the disk file the process must then again request the disk file and the printer after copying the disk file to the printer  it releases these two resources and terminates  both these protocols have two main disadvantages first  resource utilization may be low  since resources may be allocated but unused for a long period  in the example given  for instance  we can release the dvd drive and disk file  and then again request the disk file and printe1 ~ only if we can be sure that our data will remain on the disk file otherwise  we must request all resources at the beginning for both protocols  second  starvation is possible a process that needs several popular resources may have to wait indefinitely  because at least one of the resources that it needs is always allocated to some other process  7.4.3 no preemption the third necessary condition for deadlocks is that there be no preemption of resources that have already been allocated to ensure that this condition does not hold  we can use the following protocol if a process is holding some resources and requests another resource that can not be immediately allocated to it  that is  the process must wait   then all resources the process is currently holding are preempted in other words  these resources are implicitly released the preempted resources are added to the list of resources for which the process is waiting the process will be restarted only when it can regain its old resources  as well as the new ones that it is requesting  alternatively  if a process requests some resources  we first check whether they are available if they are  we allocate them if they are not  we check whether they are allocated to some other process that is waiting for additional resources if so  we preempt the desired resources from the waiting process and allocate them to the requesting process if the resources are neither available nor held by a waiting process  the requesting process must wait while it is waiting  some of its resources may be preempted  but only if another process requests them a process can be restarted only when it is allocated the new resources it is requesting and recovers any resources that were preempted while it was waiting  this protocol is often applied to resources whose state can be easily saved and restored later  such as cpu registers and memory space it can not generally be applied to such resources as printers and tape drives  7 .4.4 circular wait the fourth and final condition for deadlocks is the circular-wait condition one way to ensure that this condition never holds is to impose a total ordering of all resource types and to require that each process requests resources in an increasing order of enumeration  to illustrate  we let r =  r1  r2    rm  be the set of resource types we assign to each resource type a unique integer number  which allows us to compare two resources and to determine whether one precedes another in our ordering formally  we define a one-to-one hmction f  r ___ n  where n is the set of natural numbers for example  if the set of resource types r includes tape drives  disk drives  and printers  then the function f might be defined as follows  7.4 f  tape drive  = 1 f  disk drive  = 5 f  printer  = 12 293 we can now consider the following protocol to prevent deadlocks  each process can request resources only in an increasing order of enumeration that is  a process can initially request any number of instances of a resource type -say  r ;  after that  the process can request instances of resource type rj if and only if f  rj  f  r ;   for example  using the function defined previously  a process that wants to use the tape drive and printer at the same time must first request the tape drive and then request the printer alternatively  we can require that a process requesting an instance of resource type rj must have released any resources r ; such that f  ri    =   f  rj   it must also be noted that if several iilstances of the same resource type are needed  a single request for all of them must be issued  if these two protocols are used  then the circular-wait condition can not hold we can demonstrate this fact by assuming that a circular wait exists  proof by contradiction   let the set of processes involved in the circular wait be  p0  p1    p11   where pi is waiting for a resource r ;  which is held by process pi + l  modulo arithmetic is used on the indexes  so that p11 is waiting for a resource r11 held by p0   then  since process pi + l is holding resource ri while requesting resource ri + l ' we must have f  ri  f  r ; h  for all i but this condition means that f  ro  f  r1   f  r11  f  ro   by transitivity  f  ro  f  ro   which is impossible therefore  there can be no circular wait  we can accomplish this scheme in an application program by developing an ordering among all synchronization objects in the system all requests for synchronization objects must be made in increasing order for example  if the lock ordering in the pthread program shown in figure 7.1 was f  first_mutex  = 1 f  second_mutex  = 5 then thread_ two could not request the locks out of order  keep in mind that developing an ordering  or hierarchy  does not in itself prevent deadlock it is up to application developers to write programs that follow the ordering also note that the function f should be defined according to the normal order of usage of the resources in a system for example  because the tape drive is usually needed before the printer  it would be reasonable to define f  tape drive  f  printer   although ensuring that resources are acquired in the proper order is the responsibility of application developers  certain software can be used to verify that locks are acquired in the proper order and to give appropriate warnings when locks are acquired out of order and deadlock is possible one lock-order verifier  which works on bsd versions of unix such as freebsd  is known as witness witness uses mutual-exclusion locks to protect critical sections  as described in chapter 6 ; it works by dynamically maintaining the relationship of lock orders in a system let 's use the program shown in figure 7.1 as an example assume that thread_one is the first to acquire the locks and does so in the order  1  first_mutex   2  second_mutex wih1ess records the relationship that first_mutex must be acquired before second_mutex if thread_two later 294 chapter 7 7.5 acquires the locks out of order  witness generates a warning message on the system console  it is also important to note that imposing a lock ordering does not guarantee deadlock prevention if locks can be acquired dynamically for example  assume we have a function that transfers funds between two accounts to prevent a race condition  each account has an associated semaphore that is obtained from a get lock   function such as the following  void transaction  account from  account to  double amount    semaphore lock1  lock2 ; lock1 getlock  from  ; lock2 = getlock  to  ; wait  lock1  ; wait  lock2  ; withdraw  from  amount  ; deposit  to  amount  ; signal  lock2  ; signal  lock1  ; deadlock is possible if two threads simultaneously invoke the trans action   function  transposing different accounts that is  one thread might invoke transaction  checkingaccount  savingsaccount  25  ; and another might invoke transaction  savingsaccount  checkingaccount  50  ; we leave it as an exercise for students to fix this situation  deadlock-prevention algorithms  as discussed in section 7.4  prevent deadlocks by restraining how requests can be made the restraints ensure that at least one of the necessary conditions for deadlock can not occur and  hence  that deadlocks can not hold possible side effects of preventing deadlocks by this method  however  are low device utilization and reduced system throughput  an alternative method for avoiding deadlocks is to require additional information about how resources are to be requested for example  in a system with one tape drive and one printer  the system might need to know that process p will request first the tape drive and then the printer before releasing both resources  whereas process q will request first the printer and then the tape drive with this knowledge of the complete sequence of requests and releases for each process  the system can decide for each request whether or not the process should wait in order to avoid a possible future deadlock each request requires that in making this decision the system consider the resources 7.5 deadlock avoidance 295 currently available  the resources currently allocated to each process  and the future requests and releases of each process  the various algorithms that use this approach differ in the amount and type of information required the simplest and most useful model requires that each process declare the maximum number of resources of each type that it may need  given this a priori information  it is possible to construct an algorithm that ensures that the system will never enter a deadlocked state such an algorithm defines the deadlock-avoidance approach a deadlock-avoidance algorithm dynamically examines the resource-allocation state to ensure that a circularwait condition can never exist the resource-allocation state is defined by the number of available and allocated resources and the maximum demands of the processes in the following sections  we explore two deadlock-avoidance algorithms  7.5.1 safe state a state is safe if the system can allocate resources to each process  up to its maximum  in some order and still avoid a deadlock more formally  a system is in a safe state only if there exists a safe sequence a sequence of processes p1  p2    pn is a safe sequence for the current allocation state if  for each pi  the resource requests that pi can still make can be satisfied by the currently available resources plus the resources held by all pj  with j i in this situation  if the resources that pi needs are not immediately available  then pi can wait until all pj have finished when they have finished  pi can obtain all of its needed resources  complete its designated task  return its allocated resources  and terminate when pi terminates  pi + l can obtain its needed resources  and so on if no such sequence exists  then the system state is said to be unsafe  a safe state is not a deadlocked state conversely  a deadlocked state is an unsafe state not all unsafe states are deadlocks  however  figure 7.5   an unsafe state may lead to a deadlock as long as the state is safe  the operating system can avoid unsafe  and deadlocked  states in an unsafe state  the operating system can not prevent processes from requesting resources in such a way that a deadlock occurs the behavior of the processes controls unsafe states  figure 7.5 safe  unsafe  and deadlocked state spaces  296 chapter 7 deadlocks to illustrate  we consider a system with twelve magnetic tape drives and three processes  po  p1  and p2 process po requires ten tape drives  process p1 may need as many as four tape drives  and process p2 may need up to nine tape drives suppose that  at time to  process po is holding five tape drives  process p1 is holding two tape drives  and process p2 is holding two tape drives  thus  there are three free tape drives  maximum needs current needs 10 4 9 5 2 2 at time t0  the system is in a safe state the sequence p1  p0  p2 satisfies the safety condition process p1 can immediately be allocated all its tape drives and then return them  the system will then have five available tape drives  ; then process po can get all its tape drives and return them  the system will then have ten available tape drives  ; and finally process p2 can get all its tape drives and return them  the system will then have all twelve tape drives available   a system can go from a safe state to an unsafe state suppose that  at time t1  process p2 requests and is allocated one more tape drive the system is no longer in a safe state at this point  only process p1 can be allocated all its tape drives when it returns them  the system will have only four available tape drives since process po is allocated five tape drives but has a maximum of ten  it may request five more tape drives if it does so  it will have to wait  because they are unavailable similarly  process p2 may request six additional tape drives and have to wait  resulting in a deadlock our mistake was in granting the request from process p2 for one more tape drive if we had made p2 wait until either of the other processes had finished and released its resources  then we could have avoided the deadlock  given the concept of a safe state  we can define avoidance algorithms that ensure that the system will never deadlock the idea is simply to ensure that the system will always remain in a safe state initially  the system is in a safe state  whenever a process requests a resource that is currently available  the system must decide whether the resource can be allocated immediately or whether the process must wait the request is granted only if the allocation leaves the system in a safe state  in this scheme  if a process requests a resource that is currently available  it may still have to wait thus  resource utilization may be lower than it would otherwise be  7.5.2 resource-allocation-graph algorithm if we have a resource-allocation system with only one instance of each resource type  we can use a variant of the resource-allocation graph defined in section 7.2.2 for deadlock avoidance in addition to the request and assignment edges already described  we introduce a new type of edge  called a claim edge  a claim edge pi ~ rj indicates that process pi may request resource rj at some time in the future this edge resembles a request edge in direction but is represented in the graph by a dashed line when process pi requests resource 7.5 297 figure 7.6 resource-allocation graph for deadlock avoidance  r1  the claim edge p ;  + r1 is converted to a request edge similarly  when a resource r1 is released by p ;  the assignment edge rj  + p ; is reconverted to a claim edge p ;  + rj  we note that the resources must be claimed a priori in the system that is  before process p ; starts executing  all its claim edges must already appear in the resource-allocation graph we can relax this condition by allowing a claim edge p ;  + r1 to be added to the graph only if all the edges associated with process p ; are claim edges  now suppose that process p ; requests resource rj the request can be granted only if converting the request edge p ;  + rj to an assignment edge r1  + p ; does not result in the formation of a cycle in the resource-allocation graph we check for safety by using a cycle-detection algorithm an algorithm for detecting a cycle in this graph requires an order of n2 operations  where n is the number of processes in the system  if no cycle exists  then the allocation of the resource will leave the system in a safe state if a cycle is found  then the allocation will put the system in an unsafe state in that case  process p ; will have to wait for its requests to be satisfied  to illustrate this algorithm  we consider the resource-allocation graph of figure 7.6 suppose that p2 requests r2  although r2 is currently free  we can not allocate it to p2  since this action will create a cycle in the graph  figure 7.7   a cycle  as mentioned  indicates that the system is in an unsafe state if p1 requests r2  and p2 requests r1  then a deadlock will occur  figure 7.7 an unsafe state in a resource-allocation graph  298 chapter 7 7.5.3 banker 's algorithm the resource-allocation-graph algorithm is not applicable to a resourceallocation system with multiple instances of each resource type the deadlockavoidance algorithm that we describe next is applicable to such a system but is less efficient than the resource-allocation graph scheme this algorithm is commonly known as the banker 's algorithm the name was chosen because the algorithm could be used in a banking system to ensure that the bank never allocated its available cash in such a way that it could no longer satisfy the needs of all its customers  when a new process enters the system  it must declare the maximum number of instances of each resource type that it may need this nun1.ber may not exceed the total number of resources in the system when a user requests a set of resources  the system must determine whether the allocation of these resources will leave the system in a safe state if it will  the resources are allocated ; otherwise  the process must wait until some other process releases enough resources  several data structures must be maintained to implement the banker 's algorithm these data structures encode the state of the resource-allocation system we need the following data structures  where n is the number of processes in the system and m is the number of resource types  available a vector of length m indicates the number of available resources of each type if available  j  equals k  then k instances of resource type ri are available  max an n x m matrix defines the maximum demand of each process  if max  i   j  equals k  then process p ; may request at most k instances of resource type ri  allocation an 11 x m matrix defines the number of resources of each type currently allocated to each process if allocation  i   j  equals lc  then process p ; is currently allocated lc instances of resource type rj  need an n x m matrix indicates the remaining resource need of each process if need  i   j  equals k  then process p ; may need k more instances of resource type ri to complete its task note that need  i   j  equals max  i   j   allocation  i   j   these data structures vary over time in both size and value  to simplify the presentation of the banker 's algorithm  we next establish some notation let x andy be vectors of length 11 we say that x   = y if and only if x  i    = y  i  for all i = 1  2    n for example  if x =  1,7,3,2  and y =  0,3,2,1   then y   = x in addition  y x if y   = x andy # x  we can treat each row in the matrices allocation and need as vectors and refer to them as allocation ; and need ;  the vector allocation ; specifies the resources currently allocated to process p ; ; the vector need ; specifies the additional resources that process p ; may still request to complete its task  7.5.3.1 safety algorithm we can now present the algorithm for finding out whether or not a systern is in a safe state this algorithm can be described as follows  7.5 299 let work and finish be vectors of length m and n  respectively initialize work = available and finish  i  = false for i = 0  1    n  1  find an index i such that both a finish  i  = = false b need ;   ; work if no such i exists  go to step 4  work = work + allocation ; finish  i  = true go to step 2  if finish  i  = = true for all i  then the system is in a safe state  this algorithm may require an order of m x n2 operations to determine whether a state is safe  7.5.3.2 resource-request algorithm next  we describe the algorithm for determining whether requests can be safely granted  let request ; be the request vector for process p ;  if request ;  j  = = k  then process p ; wants k instances of resource type rj when a request for resources is made by process p ;  the following actions are taken  if request ;     ; need ;  go to step 2 otherwise  raise an error condition  since the process has exceeded its maximum claim  if request ;   ; available  go to step 3 otherwise  p ; must wait  since the resources are not available  have the system pretend to have allocated the requested resources to process p ; by modifyil1.g the state as follows  available = available request ; ; allocation ; = allocation ; + request ; ; need ; = need ;  request ; ; if the resulting resource-allocation state is safe  the transaction is completed  and process p ; is allocated its resources however  if the new state is unsafe  then p ; must wait for request ;  and the old resource-allocation state is restored  7.5.3.3 an illustrative example to illustrate the use of the banker 's algorithm  consider a system with five processes po through p4 and three resource types a  b  and c resource type a has ten instances  resource type b has five instances  and resource type c has seven instances suppose that  at time t0  the following snapshot of the system has been taken  300 chapter 7 allocation max available abc abc abc po 010 753 332 pl 200 322 p2 302 902 p3 2 11 222 p4 002 433 the content of the matrix need is defined to be max  allocation and is as follows  need abc po 743 pl 122 p2 600 p3 011 p4 431 we claim that the system is currently in a safe state indeed  the sequence plt p3  p4  p2  po satisfies the safety criteria suppose now that process p1 requests one additional instance of resource type a and two instances of resource type c  so request1 =  1,0,2   to decide whether this request can be immediately granted  we first check that request1 s available-that is  that  1,0,2  s  3,3,2   which is true we then pretend that this request has been fulfilled  and we arrive at the following new state  allocation need available abc abc abc po 010 743 230 pl 302 020 p2 302 600 p3 211 0 11 p4 002 431 we must determine whether this new system state is safe to do so  we execute our safety algorithm and find that the sequence p1  p3  p4  po  p2 satisfies the safety requirement hence  we can immediately grant the request of process p1  you should be able to see  however  that when the system is in this state  a request for  3,3,0  by p4 can not be granted  since the resources are not available  furthermore  a request for  0,2,0  by po can not be granted  even though the resources are available  since the resulting state is unsafe  we leave it as a programming exercise for students to implement the banker 's algorithm  7.6 7.6 301 if a system does not employ either a deadlock-prevention or a deadlockavoidance algorithm  then a deadlock situation may occur in this environment  the system may provide  an algorithm that examines the state of the system to determine whether a deadlock has occurred an algorithm to recover from the deadlock in the following discussion  we elaborate on these two requirements as they pertain to systems with only a single instance of each resource type  as well as to systems with several instances of each resource type at this point  however  we note that a detection-and-recovery scheme requires overhead that includes not only the run-time costs of maintaining the necessary information and executing the detection algorithm but also the potential losses inherent in recovering from a deadlock  7.6.1 single instance of each resource type if all resources have only a single instance  then we can define a deadlockdetection algorithm that uses a variant of the resource-allocation graph  called a wait-for graph we obtain this graph from the resource-allocation graph by removing the resource nodes and collapsing the appropriate edges  more precisely  an edge from pi to pi in a wait-for graph implies that process pz is waiting for process p1 to release a resource that p ; needs an edge pz  + pi exists iil a wait-for graph if and only if the corresponding resourceallocation graph contains two edges pz  + rq and rq  + pi for some resource rq for example  in figure 7.8  we present a resource-allocation graph and the corresponding wait-for graph  as before  a deadlock exists in the system if and only if the wait-for graph contains a cycle to detect deadlocks  the system needs to maintain the wait-for graph and periodically invoke an algorithm that searches for a cycle in the graph  an algorithm to detect a cycle in a graph requires an order of n2 operations  where n is the number of vertices in the graph  7.6.2 several instances of a resource type the wait-for graph scheme is not applicable to a resource-allocation system with multiple instances of each resource type we turn now to a deadlockdetection algorithm that is applicable to such a system the algorithm employs several time-varying data structures that are similar to those used in the banker 's algorithm  section 7.5.3   available a vector of length nz indicates the number of available resources of each type  allocation ann x nz matrix defines the number of resources of each type currently allocated to each process  302 chapter 7  a   b  figure 7.8  a  resource-allocation graph  b  corresponding wait-for graph  request an n x m matrix indicates the current request of each process  if request  i   j  equals k  then process p ; is requesting k more instances of resource type rj  the     relation between two vectors is defined as in section 7.5.3 to simplify notation  we again treat the rows in the matrices allocation and request as vectors ; we refer to them as allocation ; and request ;  the detection algorithm described here simply investigates every possible allocation sequence for the processes that remain to be completed compare this algorithm with the banker 's algorithm of section 7.5.3  let work and finish be vectors of length m and n  respectively initialize work = available fori = 0  1    n-1  if allocation ; # 0  then finish  i  = false ; otherwise  finish  i  = tme  2 find an index i such that both a finish  i  = = false b request ;     work if no such i exists  go to step 4  work = work + allocation ; finish  i  = true go to step 2  4 if finish  i  = = false for some i  0     i n  then the system is in a deadlocked state moreover  if finish  i  = = false  then process p ; is deadlocked  this algorithm requires an order o m x n2 operations to detect whether the system is in a deadlocked state  7.6 303 you may wonder why we reclaim the resources of process p ;  in step 3  as soon as we determine that request ;  s work  in step 2b   we know that p ; is currently not involved in a deadlock  since request ;  s work   thus  we take an optimistic attitude and assume that p ; will require no more resources to complete its task ; it will thus soon return all currently allocated resources to the system if our assumption is incorrect  a deadlock may occur later that deadlock will be detected the next tince the deadlock-detection algorithm is invoked  to illustrate this algorithm  we consider a system with five processes po through p4 and three resource types a  b  and c resource type a has seven instances  resource type b has two instances  and resource type c has six instances suppose that  at time t0  we have the following resource-allocation state  allocation request available abc abc abc po 0 1 0 000 000 pl 200 202 p2 303 000 p3 2 11 100 p4 002 002 we claim that the system is not in a deadlocked state indeed  if we execute our algorithm  we will find that the sequence po  p2  p3  plt p4 results in finish  i  = = true for all i  suppose now that process p2 makes one additional request for an instance of type c the request matrix is modified as follows  request abc po 000 pl 202 p2 001 p3 100 p4 002 we claim that the system is now deadlocked although we can reclaim the resources held by process po  the number of available resources is not sufficient to fulfill the requests of the other processes thus  a deadlock exists  consisting of processes p1  p2  p3  and p4  7.6.3 detection-algorithm usage when should we invoke the detection algorithm the answer depends on two factors  1 how often is a deadlock likely to occur how many processes will be affected by deadlock when it happens 304 chapter 7 7.7 if deadlocks occur frequently  then the detection algorithm should be invoked frequently resources allocated to deadlocked processes will be idle until the deadlock can be broken in addition  the number of processes involved in the deadlock cycle may grow  deadlocks occur only when some process makes a request that can not be granted immediately this request may be the final request that completes a chain of waiting processes in the extreme  then  we can invoke the deadlockdetection algorithm every time a request for allocation can not be granted immediately in this case  we can identify not only the deadlocked set of processes but also the specific process that caused the deadlock  in reality  each of the deadlocked processes is a link in the cycle in the resource graph  so all of them  jointly  caused the deadlock  if there are many different resource types  one request may create many cycles in the resource graph  each cycle completed by the most recent request and caused by the one identifiable process  of course  invoking the deadlock-detection algorithm for every resource request will incur considerable overhead in computation time a less expensive alternative is simply to invoke the algorithm at defined intervals-for example  once per hour or whenever cpu utilization drops below 40 percent  a deadlock eventually cripples system throughput and causes cpu utilization to drop  if the detection algorithm is invoked at arbitrary points in time  the resource graph may contain many cycles in this case  we generally can not tell which of the many deadlocked processes caused the deadlock when a detection algorithm determines that a deadlock exists  several alternatives are available one possibility is to inform the operator that a deadlock has occurred and to let the operator deal with the deadlock manually another possibility is to let the system recover from the deadlock automatically there are two options for breaking a deadlock one is simply to abort one or more processes to break the circular wait the other is to preempt some resources from one or more of the deadlocked processes  7.7.1 process termination to eliminate deadlocks by aborting a process  we use one of two methods in both methods  the system reclaims all resources allocated to the terminated processes  abort all deadlocked processes this method clearly will break the deadlock cycle  but at great expense ; the deadlocked processes may have computed for a long time  and the results of these partial computations must be discarded and probably will have to be recomputed later  abort one process at a time until the deadlock cycle is eliminated this method incurs considerable overhead  since after each process is aborted  a deadlock-detection algorithnc rnust be invoked to determine whether any processes are still deadlocked  7.7 305 aborting a process may not be easy if the process was in the midst of updating a file  terminating it will leave that file in an incorrect state similarly  if the process was in the midst of printing data on a printer  the system must reset the printer to a correct state before printing the next job  if the partial termination method is used  then we must determine which deadlocked process  or processes  should be terminated this determination is a policy decision  similar to cpu-scheduling decisions the question is basically an economic one ; we should abort those processes whose termination will incur the minimum cost unfortunately  the term minimum cost is not a precise one  many factors may affect which process is chosen  including  1 what the priority of the process is 2 how long the process has computed and how much longer the process will compute before completing its designated task how many and what types of resources the process has used  for example  whether the resources are simple to preempt  how many more resources the process needs in order to complete 5 how many processes will need to be terminated whether the process is interactive or batch 7.7.2 resource preemption to eliminate deadlocks using resource preemption  we successively preempt some resources from processes and give these resources to other processes 1-m til the deadlock cycle is broken  if preemption is required to deal with deadlocks  then three issues need to be addressed  selecting a victim which resources and which processes are to be preempted as in process termil ation  we must determine the order of preemption to minimize cost cost factors may include such parameters as the number of resources a deadlocked process is holding and the amount of time the process has thus far consumed during its execution  rollback if we preempt a resource from a process  what should be done with that process clearly  it can not contil ue with its normal execution ; it is missing some needed resource we must roll back the process to some safe state and restart it from that state  since  in general  it is difficult to determine what a safe state is  the simplest solution is a total rollback  abort the process and then restart it although it is more effective to roll back the process only as far as necessary to break the deadlock  this method requires the system to keep more information about the state of all running processes  starvation how do we ensure that starvation will not occur that is  how can we guarantee that resources will not always be preempted from the same process 306 chapter 7 7.8 in a system where victim selection is based primarily on cost factors  it may happen that the same process is always picked as a victim as a result  this process never completes its designated task  a starvation situation that must be dealt with in any practical system clearly  we must ensure that a process can be picked as a victim only a  small  finite number of times the most common solution is to include the number of rollbacks in the cost factor  a deadlocked state occurs when two or more processes are waiting indefinitely for an event that can be caused only by one of the waiting processes there are three principal methods for dealing with deadlocks  use some protocol to prevent or avoid deadlocks  ensuring that the system will never enter a deadlocked state  allow the system to enter a deadlocked state  detect it  and then recover  ignore the problem altogether and pretend that deadlocks never occur in the system  the third solution is the one used by most operating systems  including unix and windows  a deadlock can occur only if four necessary conditions hold simultaneously in the system  mutual exclusion  hold and wait  no preemption  and circular wait to prevent deadlocks  we can ensure that at least one of the necessary conditions never holds  a method for avoiding deadlocks  rather than preventing them  requires that the operating system have a priori information about how each process will utilize system resources the banker 's algorithm  for example  requires a priori information about the maximunl number of each resource class that each process may request using this information  we can define a deadlockavoidance algorithm  if a system does not employ a protocol to ensure that deadlocks will never occur  then a detection-and-recovery scheme may be employed a deadlockdetection algorithm must be invoked to detennine whether a deadlock has occurred if a deadlock is detected  the system must recover either by terminating some of the deadlocked processes or by preempting resources from some of the deadlocked processes  where preemption is used to deal with deadlocks  three issues must be addressed  selecting a victim  rollback  and starvation in a system that selects victims for rollback primarily on the basis of cost factors  starvation may occur  and the selected process can never complete its designated task  researchers have argued that none of the basic approaches alone is appropriate for the entire spectrum of resource-allocation problems in operating systems the basic approaches can be combined  however  allowing us to select an optimal approach for each class of resources in a system  307 7.1 a single-lane bridge connects the two vermont villages of north tunbridge and south tunbridge farmers in the two villages use this bridge to deliver their produce to the neighboring town the bridge can become deadlocked if a northbound and a southbound farmer get on the bridge at the same time  vermont farmers are stubborn and are unable to back up  using semaphores  design an algorithm that prevents deadlock initially  do not be concerned about starvation  the situation in which northbound farmers prevent southbound farmers from using the bridge  or vice versa   7.2 modify your solution to exercise 7.1 so that it is starvation-free  7.3 consider a system consisting of four resources of the same type that are shared by three processes  each of which needs at most two resources  show that the system is deadlock free  7.4 consider the traffic deadlock depicted in figure 7.9  a show that the four necessary conditions for deadlock hold in this example  b state a simple rule for avoiding deadlocks in this system  7.5 in a real computer system  neither the resources available nor the demands of processes for resources are consistent over long periods  months   resources break or are replaced  new processes come and go  and new resources are bought and added to the system if deadlock is controlled by the banker 's algorithm  which of the following changes figure 7.9 traffic deadlock for exercise 7.4 308 chapter 7 can be made safely  without introducing the possibility of deadlock   and under what circumstances a increase available  new resources added   b decrease available  resource permanently removed from system   c increase max for one process  the process needs or wants rnore resources than allowed   d decrease max for one process  the process decides it does not need that many resources   e increase the number of processes  f decrease the number of processes  7.6 we can obtain the banker 's algorithm for a single resource type from the general banker 's algorithm simply by reducing the dimensionality of the various arrays by 1 show through an example that we can not implement the multiple-resource-type banker 's scheme by applying the sil1.gle-resource-type scheme to each resource type individually  7.7 consider the following resource-allocation policy requests for and releases of resources are allowed at any time if a request for resources can not be satisfied because the resources are not available  then we check any processes that are blocked waiting for resources if a blocked process has the desired resources  then these resources are taken away from it and are given to the requestmg process the vector of resources for which the blocked process is waiting is increased to include the resources that were taken away  for example  consider a system with three resource types and the vector available initialized to  4,2,2   if process po asks for  2,2,1   it gets them if p1 asks for  1,0,1   it gets them then  if po asks for  0,0,1   it is blocked  resource not available   if p2 now asks for  2,0,0   it gets the available one  1,0,0  and one that was allocated to po  since po is blocked   po 's allocation vector goes down to  1,2,1   and its need vector goes up to  1,0,1   a can deadlock occur if you answer yes  give an example if you answer no  specify which necessary condition can not occur  b can indefinite blocking occur explain your answer  7.8 a possible method for preventing deadlocks is to have a single  higherorder resource that must be requested before any other resource for example  if multiple threads attempt to access the synchronization objects a e  deadlock is possible  such synchronization objects may include mutexes  semaphores  condition variables  and the like  we can prevent the deadlock by adding a sixth object f whenever a thread wants to acquire the synchronization lock for any object a e  it must first acquire the lock for object f this solution is known as containment  the locks for objects a e are contained within the lock for object f  compare this scheme with the circular-wait scheme of section 7.4.4  309 7.9 compare the circular-wait scheme with the various deadlock-avoidance schemes  like the banker 's algorithnc  with respect to the following issues  a runtime overheads b system throughput 7.10 consider the following snapshot of a system  allocation max available  abcd abcd abcd po 0012 0012 1520 pl 1000 1750 p2 1354 2356 p3 0632 0652 p4 0014 0656 answer the following questions using the banker 's algorithm  a what is the content of the matrix need b is the system in a safe state c if a request from process p1 arrives for  0,4,2,0   can the request be granted immediately 7.11 consider a system consisting of m resources of the same type being shared by n processes a process can request or release only one resource at a time show that the system is deadlock free if the following two conditions hold  a the maximum need of each process is between one resource and m resources  b the sum of all maximum needs is less than m + n  7.12 consider a computer system that runs 5,000 jobs per month and has no deadlock-prevention or deadlock-avoidance scheme deadlocks occur about twice per month  and the operator must terminate and rerun about 10 jobs per deadlock each job is worth about $ 2  in cpu time   and the jobs terminated tend to be about half-done when they are aborted  a systems programmer has estimated that a deadlock-avoidance algorithm  like the banker 's algorithm  could be installed in the system with an increase in the average execution time per job of about 10 percent  since the machine currently has 30 percent idle time  all 5,000 jobs per month could still be run  although turnaround time would increase by about 20 percent on average  a what are the arguments for installing the deadlock-avoidance algorithm b what are the arguments against installing the deadlock-avoidance algorithm 310 chapter 7 7.13 consider the deadlock situation that can occur in the diningphilosophers problem when the philosophers obtain the chopsticks one at a time discuss how the four necessary conditions for deadlock hold in this setting discuss how deadlocks could be avoided by eliminating any one of the four necessary conditions  7.14 what is the optimistic assumption made in the deadlock-detection algorithm how can this assumption be violated 7.15 consider the version of the dining-philosophers problem in which the chopsticks are placed at the center of the table and any two of them can be used by a philosopher assume that requests for chopsticks are made one at a time describe a simple rule for determining whether a particular request can be satisfied without causing deadlock given the current allocation of chopsticks to philosophers  7.16 is it possible to have a deadlock involving only a single process explain your answer  7.17 consider again the setting in the preceding question assume now that each philosopher requires three chopsticks to eat resource requests are still issued one at a time describe some simple rules for determining whether a particular request can be satisfied without causing deadlock given the current allocation of chopsticks to philosophers  7.18 in section 7.4.4  we describe a situation in which we prevent deadlock by ensuring that all locks are acquired in a certain order however  we also point out that deadlock is possible in this situation if two threads simultaneously invoke the transaction   function fix the transaction   function to prevent deadlocks  7.19 write a multithreaded program that implements the banker 's algorithm discussed in section 7.5.3 create n threads that request and release resources from the bank the banker will grant the request only if it leaves the system in a safe state you may write this program using either pthreads or win32 threads it is important that shared data be safe from concurrent access to ensure safe access to shared data  you can use mutex locks  which are available in both the pthreads and win32 apis the use of mutex locks in both of these libraries is described in the project entitled producer-consumer problem at the end of chapter 6  dijkstra  1965a  was one of the first and most influential contributors in the deadlock area holt  1972  was the first person to formalize the notion of deadlocks in terms of an allocation-graph model similar to the one presented in this chapter starvation was also covered by holt  1972   hyman  1985  provided the deadlock example from the kansas legislature a recent study of deadlock handling is provided in levine  2003   311 the various prevention algorithms were suggested by havender  1968   who devised the resource-ordering scheme for the ibm os/360 systen'l  the banker 's algorithm for avoiding deadlocks was developed for a single resource type by dijkstra  1965a  and was extended to multiple resource types by habermam'l  1969   exercises 7.3 and 7.11 are from holt  1971   the deadlock-detection algorithm for multiple instances of a resource type  which is described in section 7.6.2  was presented by coffman et al  1971   bach  1987  describes how many of the algorithms in the traditional unix kernel handle deadlock solutions to deadlock problems in networks are discussed in works such as culler et al  1998  and rodeheffer and schroeder  1991   the witness lock-order verifier is presented in baldwin  2002   part four the main purpose of a computer system is to execute programs these programs  together with the data they access  must be at least partially in main memory during execution  to improve both the utilization of the cpu and the speed of its response to users  a general-purpose computer must keep several processes in memory many memory-management schemes exist  reflecting various approaches  and the effectiveness of each algorithm depends on the situation selection of a memory-management scheme for a system depends on many factors  especially on the hardware design of the system most algorithms require hardware support  8.1 c in chapter 5  we showed how the cpu can be shared by a set of processes as a result of cpu scheduling  we can improve both the utilization of the cpu and the speed of the computer 's response to its users to realize this increase in performance  however  we must keep several processes in memory ; that is  we must share memory  in this chapter  we discuss various ways to manage memory the memorymanagement algorithms vary from a primitive bare-machine approach to paging and segmentation strategies each approach has its own advantages and disadvantages selection of a memory-management method for a specific system depends on many factors  especially on the hardware design of the system as we shall see  many algorithms require hardware support  although recent designs have closely integrated the hardware and operating system  to provide a detailed description of various ways of organizing memory hardware  to discuss various memory-management techniques  including paging and segmentation  to provide a detailed description of the intel pentium  which supports both pure segmentation and segmentation with paging  as we saw in chapter 1  memory is central to the operation of a modern computer system memory consists of a large array of words or bytes  each with its own address the cpu fetches instructions from memory according to the value of the program counter these instructions may cause additional loading from and storing to specific memory addresses  a typical instruction-execution cycle  for example  first fetches an instruction from memory the instruction is then decoded and may cause operands to be fetched from memory after the instruction has been executed on the 315 316 chapter 8 operands  results may be stored back in memory the mernory unit sees only a stream of memory addresses ; it does not know how they are generated  by the instruction counter  indexing  indirection  literal addresses  and so on  or what they are for  instructions or data   accordingly  we can ignore hozu a program generates a memory address we are interested only in the sequence of memory addresses generated by the running program  we begin our discussion by covering several issues that are pertinent to the various techniques for managing memory this coverage includes an overview of basic hardware issues  the binding of symbolic memory addresses to actual physical addresses  and the distinction between logical and physical addresses  we conclude the section with a discussion of dynamically loading and linking code and shared libraries  8.1.1 basic hardware main memory and the registers built into the processor itself are the only storage that the cpu can access directly there are machine instructions that take memory addresses as arguments  but none that take disk addresses therefore  any instructions in execution  and any data being used by the instructions  must be in one of these direct-access storage devices if the data are not in memory  they must be moved there before the cpu can operate on them  registers that are built into the cpu are generally accessible within one cycle of the cpu clock most cpus can decode instructions and perform simple operations on register contents at the rate of one or more operations per clock tick the same can not be said of main memory  which is accessed via a transaction on the memory bus completing a memory access may take many cycles of the cpu clock in such cases  the processor normally needs to stall  since it does not have the data required to complete the instruction that it is executing this situation is intolerable because of the frequency of memory accesses the remedy is to add fast memory between the cpu and 0 operating system 256000 process 300040 i soa  lj.o i process base 420940 i 120 ! 1go i i  limit process 880000 1024000 figure 8.1 a base and a limit register define a logical address space  8.1 317 main memory a memory buffer used to accommodate a speed differential  called a is described in section 1.8.3  not only are we concerned with the relative speed of accessing physical memory  but we also must ensure correct operation to protect the operating system from access by user processes and  in addition  to protect user processes from one another this protection must be provided by the hardware it can be implemented in several ways  as we shall see throughout the chapter in this section  we outline one possible implementation  we first need to make sure that each process has a separate memory space  to do this  we need the ability to determine the range of legal addresses that the process may access and to ensure that the process can access only these legal addresses we can provide this protection by using two registers  usually a base and a limit  as illustrated in figure 8.1 the base holds the smallest legal physical memory address ; the specifies the size of the range for example  if the base register holds 300040 and the limit register is 120900  then the program can legally access all addresses from 300040 through 420939  inclusive   protection of memory space is accomplished by having the cpu hardware compare every address generated in user mode with the registers any attempt by a program executing in user mode to access operating-system memory or other users ' memory results in a trap to the operating system  which treats the attempt as a fatal error  figure 8.2   this scheme prevents a user program from  accidentally or deliberately  modifying the code or data structures of either the operating system or other users  the base and limit registers can be loaded only by the operating system  which uses a special privileged instruction since privileged instructions can be executed only in kernel mode  and since only the operating system executes in kernel mode  only the operating system can load the base and limit registers  this scheme allows the operating system to change the value of the registers but prevents user programs from changing the registers ' contents  the operating system  executing in kernel mode  is given unrestricted access to both operating system memory and users ' memory this provision allows the operating system to load users ' programs into users ' memory  to yes no trap to operating system monitor-addressing error memory figure 8.2 hardware address protection with base and limit registers  318 chapter 8 dump out those programs in case of errors  to access and modify parameters of system calls  and so on  8.1.2 address binding usually  a program resides on a disk as a binary executable file to be executed  the program must be brought into memory and placed within a process  depending on the memory management in use  the process may be moved between disk and memory during its execution the processes on the disk that are waiting to be brought into memory for execution form the the normal procedure is to select one of the processes in the input queue and to load that process into memory as the process is executed  it accesses instructions and data from memory eventually  the process terminates  and its memory space is declared available  most systems allow a user process to reside in any part of the physical memory thus  although the address space of the computer starts at 00000  the first address of the user process need not be 00000 this approach affects the addresses that the user program can use in most cases  a user program will go through several steps-some of which may be optional-before bein.g executed  figure 8.3   addresses may be represented in different ways during these steps addresses in the source program are generally symbolic  such as count   a compiler will typically bind these symbolic addresses to relocatable addresses  such as 14 bytes from the beginning of this module   the lin.kage editor or loader will in turn bind the relocatable addresses to absolute addresses  such as 74014   each binding is a mapping from one address space to another  classically  the binding of instructions and data to memory addresses can be done at any step along the way  compile time if you know at compile time where the process will reside in memory  then can be generated for example  if you krww that a user process will reside starting at location r  then the generated compiler code will start at that location and extend up from there if  at some later time  the starting location changes  then it will be necessary to recompile this code the ms-dos .com-format programs are bound at compile time  load time if it is not known at compile time where the process will reside in memory  then the compiler must generate in this case  final binding is delayed until load time if the starting address changes  we need only reload the user code to incorporate this changed value  execution time if the process can be moved during its execution from one memory segment to another  then binding must be delayed until run time special hardware must be available for this scheme to work  as will be discussed in section 8.1.3 most general-purpose operating systems 11se this method  a major portion of this chapter is devoted to showing how these various bindings can be implemented effectively in a computer system and to discussing appropriate hardware support  8.1 compile time load time  execution time  run time  figure 8.3 multistep processing of a user program  8.1.3 logical versus physical address space an address generated by the cpu is commonly referred to as a 319 whereas an address seen by the memory unit-that is  the one loaded into the of the memory-is commonly referred to as a the compile-time and load-time address-binding methods generate identical logical and physical addresses however  the execution-time addressbinding scheme results in differing logical and addresses in this case  we usually refer to the logical address as a we use logical address and virtual address interchangeably in this text the set of all logical addresses generated by a program is a logical the set of all physical addresses corresponding to these logical addresses is a physical thus  in_ the execution-time address-binding scheme  the logical and physical address spaces differ  the run-time mapping from virtual to physical addresses is done by a hardware device called the we can choose from many different methods to accomplish such mapping  as we discuss in 320 chapter 8 figure 8.4 dynamic relocation using a relocation register  sections 8.3 through 8.7 for the time being  we illustrate this mapping with a simple mmu scheme that is a generalization of the base-register scheme described in section 8.1.1 the base register is now called a the value in the relocation register is added to every address generated by a user process at the time the address is sent to memory  see figure 8.4   for example  if the base is at 14000  then an attempt by the user to address location 0 is dynamically relocated to location 14000 ; an access to location 346 is mapped to location 14346 the ms-dos operating system running on the intel 80x86 family of processors used four relocation registers when loading and running processes  the user program never sees the real physical addresses the program can create a pointer to location 346  store it in memory  manipulate it  and compare it with other addresses-all as the number 346 only when it is used as a memory address  in an indirect load or store  perhaps  is it relocated relative to the base register the user program deals with logical addresses the memory-mapping hardware converts logical addresses into physical addresses this form of execution-time binding was discussed in section 8.1.2 the final location of a referenced memory address is not determined until the reference is made  we now have two different types of addresses  logical addresses  in the range 0 to max  and physical addresses  in the ranger + 0 tor + max for a base valuer   the user generates only logical addresses and thinks that the process runs in locations 0 to max the user program generates only logical addresses and thinks that the process runs in locations 0 to max however  these logical addresses must be mapped to physical addresses before they are used  the concept of a logical address space that is bound to a separate physical address space is central to proper memory management  8.1.4 dynamic loading in our discussion so far  it has been necessary for the entire program and all data of a process to be in physical memory for the process to execute the size of a process has thus been limited to the size of physical memory to obtain better memory-space utilization  we can use dynamic with dynancic 8.1 321 loading  a routine is not loaded until it is called all routines are kept on disk in a relocatable load format the main program is loaded into memory and is executed when a routine needs to call another routine  the calling routine first checks to see whether the other routine has been loaded if it has not  the relocatable linking loader is called to load the desired routine into menwry and to update the program 's address tables to reflect this change then control is passed to the newly loaded routine  the advantage of dynamic loading is that an unused routine is never loaded this method is particularly useful when large amounts of code are needed to handle infrequently occurring cases  such as error routines in this case  although the total program size may be large  the portion that is used  and hence loaded  may be much smaller  dynamic loading does not require special support from the operating system it is the responsibility of the users to design their programs to take advantage of such a method operating systems may help the programmer  however  by providing library routines to implement dynamic loading  8.1.5 dynamic linking and shared libraries figure 8.3 also shows some operating systems support only linking  in system language libraries are treated like any other object module and are combined by the loader into the binary program image dynamic linking  in contrast  is similar to dynamic loading  here  though  linking  rather than loading  is postponed until execution time  this feature is usually used with system libraries  such as language subroutine libraries without this facility  each program on a system must include a copy of its language library  or at least the routines referenced by the program  in the executable image this requirement wastes both disk space and main memory  with dynamic linking  a stub is included in the image for each libraryroutine reference the stub is a small piece of code that indicates how to locate the appropriate memory-resident library routine or how to load the library if the routine is not already present when the stub is executed  it checks to see whether the needed routine is already in memory if it is not  the program loads the routine into memory either way  the stub replaces itself with the address of the routine and executes the routine thus  the next time that particular code segment is reached  the library routine is executed directly  incurring no cost for dynamic linking under this scheme  all processes that use a language library execute only one copy of the library code  this feature can be extended to library updates  such as bug fixes   a library may be replaced by a new version  and all programs that reference the library will automatically use the new version without dynamic linking  all such programs would need to be relinked to gain access to the new library so that programs will not accidentally execute new  incompatible versions of libraries  version information is included in both the program and the library more than one version of a library may be loaded into memory  and each program uses its version information to decide which copy of the library to use versions with minor changes retain the same version number  whereas versions with major changes increment the number thus  only programs that are compiled with the new library version are affected by any incompatible changes incorporated 322 chapter 8 8.2 in it other programs linked before the new library was installed will continue using the older library this system is also known as 'h  = ' unlike dynamic loading  dynamic linking generally requires help from the operating system if the processes in memory are protected from one another  then the operating system is the only entity that can check to see whether the needed routine is in another process 's memory space or that can allow multiple processes to access the same memory addresses we elaborate on this concept when we discuss paging in section 8.4.4  a process must be in memory to be executed a process  however  can be temporarily out of memory to a and then brought into memory for continued execution for example  assume a multiprogramming environment with a round-robin cpu-scheduling algorithm when a quantum expires  the memory manager will start to swap out the process that just finished and to swap another process into the memory space that has been freed  figure 8.5   in the meantime  the cpu scheduler will allocate a time slice to some other process in memory when each process finishes its quantum  it will be swapped with another process ideally  the memory manager can swap processes fast enough that some processes will be in memory  ready to execute  when the cpu scheduler wants to reschedule the cpu in addition  the quantum must be large enough to allow reasonable amounts of computing to be done between swaps  a variant of this swapping policy is used for priority-based scheduling algorithms if a higher-priority process arrives and wants service  the memory manager can swap out the lower-priority process and then load and execute the higher-priority process when the higher-priority process finishes  the @ swap out @ swap in backing store main memory figure 8.5 swapping of two processes using a disk as a backing store  8.2 323 lower-priority process can be swapped back in and continued this variant of swapping is sometimes called roll normally  a process that is swapped out will be swapped back into the same memory space it occupied previously this restriction is dictated by the method of address binding if binding is done at assembly or load time  then the process can not be easily moved to a different location if execution-time binding is being used  however  then a process can be swapped into a different memory space  because the physical addresses are computed during execution time  swapping requires a backing store the backing store is commonly a fast disk it must be large enough to accommodate copies of all memory images for all users  and it must provide direct access to these memory images the system maintains a consisting of all processes whose memory images are on the backing store or in memory and are ready to run whenever the cpu scheduler decides to execute a process  it calls the dispatcher the dispatcher checks to see whether the next process in the queue is in memory  if it is not  and if there is no free memory region  the dispatcher swaps out a process currently in memory and swaps in the desired process it then reloads registers and transfers control to the selected process  the context-switch time in such a swapping system is fairly high to get an idea of the context-switch time  let us assume that the user process is 100 mb in size and the backing store is a standard hard disk with a transfer rate of 50mb per second the actual transfer of the 100-mb process to or from main memory takes 100mb/50mb per second = 2 seconds  assuming an average latency of 8 milliseconds  the swap time is 2008 milliseconds since we must both swap out and swap in  the total swap time is about 4016 milliseconds  notice that the major part of the swap time is transfer time the total transfer time is directly proportional to the amount of memory swapped if we have a computer system with 4 gb of main memory and a resident operating system taking 1 gb  the maximum size of the user process is 3gb however  many user processes may be much smaller than this-say  100 mb a 100-mb process could be swapped out in 2 seconds  compared with the 60 seconds required for swapping 3 gb clearly  it would be useful to know exactly how much memory a user process is using  not simply how much it might be using  then we would need to swap only what is actually used  reducing swap time  for this method to be effective  the user must keep the system informed of any changes in memory requirements thus  a process with dynamic memory requirements will need to issue system calls  request memory and release memory  to inform the operating system of its changing memory needs  swapping is constrained by other factors as well if we want to swap a process  we must be sure that it is completely idle of particular concern is any pending i/0 a process may be waiting for an i/0 operation when we want to swap that process to free up memory however  if the i/0 is asynchronously accessing the user memory for i/0 buffers  then the process can not be swapped assume that the i/0 operation is queued because the device is busy if we were to swap out process p1 and swap in process p2  the 324 chapter 8 8.3 i/0 operation might then attempt to use memory that now belongs to process p2  there are two main solutions to this problem  never swap a process with pending i/0  or execute i/0 operations only into operating-system buffers  transfers between operating-system buffers and process memory then occur only when the process is swapped in  the assumption  mentioned earlier  that swapping requires few  if any  head seeks needs further explanation we postpone discussing this issue until chapter 12  where secondary-storage structure is covered generally  swap space is allocated as a chunk of disk  separate from the file system  so that its use is as fast as possible  currently  standard swapping is used in few systems it requires too much swapping time and provides too little execution time to be a reasonable memory-management solution modified versions of swapping  however  are found on many systems  a modification of swapping is used in many versions of unix swapping is normally disabled but will start if many processes are running and are using a threshold amount of memory swapping is again halted when the load on the system is reduced memory management in unix is described fully in sections 21.7 and a.6  early pcs-which lacked the sophistication to implement more advanced memory-management methods-ran multiple large processes by using a modified version of swapping a prime example is the microsoft windows 3.1 operating system  which supports concurrent execution of processes in memory if a new process is loaded and there is insufficient main memory  an old process is swapped to disk this operating system does not provide full swapping  however  because the user  rather than the scheduler  decides when it is time to preempt one process for another any swapped-out process remains swapped out  and not executing  until the user selects that process to run subsequent versions of microsoft operating systems take advantage of the advanced mmu features now found in pcs we explore such features in section 8.4 and in chapter 9  where we cover virtual memory  the main memory must accommodate both the operating system and the various user processes we therefore need to allocate main menlory in the most efficient way possible this section explains one common method  contiguous memory allocation  the memory is usually divided into two partitions  one for the resident operating system and one for the user processes we can place the operating system in either low memory or high memory the major factor affecting this decision is the location of the interrupt vector since the interrupt vector is often in low memory  programmers usually place the operating system in low memory as well thus  in this text  we discuss only the situation in which the operating system resides in low memory the development of the other situation is similar  we usually want several user processes to reside in memory at the same time we therefore need to consider how to allocate available memory to the processes that are in the input queue waiting to be brought into memory  8.3 325 in contiguous memory allocation  each process is contained in a single contiguous section of memory  8.3.1 memory mapping and protection before discussing memory allocation further  we must discuss the issue of memory mapping and protection we can provide these features by using a relocation register  as discussed in section 8.1.3  together with a limit register  as discussed in section 8.1.1 the relocation register contaitls the value of the smallest physical address ; the limit register contains the range of logical addresses  for example  relocation = 100040 and limit = 74600   with relocation and limit registers  each logical address must be less than the limit register ; the mmu maps the logical address dynamically by adding the value in the relocation register this mapped address is sent to memory  figure 8.6   when the cpu scheduler selects a process for execution  the dispatcher loads the relocation and limit registers with the correct values as part of the context switch because every address generated by a cpu is checked against these registers  we can protect both the operating system and the other users ' programs and data from being modified by this running process  the relocation-register scheme provides an effective way to allow the operating system 's size to change dynamically this flexibility is desirable in many situations for example  the operating system contains code and buffer space for device drivers if a device driver  or other operating-system service  is not commonly used  we do not want to keep the code and data in memory  as we might be able to use that space for other purposes such code is sometimes called transient operating-system code ; it comes and goes as needed thus  using this code changes the size of the operating system during program execution  8.3.2 memory allocation now we are ready to turn to memory allocation one of the simplest methods for allocating memory is to divide memory into several fixed-sized each partition may contain exactly one process thus  the degree no trap  addressing error figure 8.6 hardware supportfor relocation and limit registers  326 chapter 8 of multiprogramming is bound by the number of partitions in this when a partition is free  a process is selected from the input queue and is loaded into the free partition when the process terminates  the partition becomes available for another process this method was originally used by the ibm os/360 operating system  called mft  ; it is no longer in use  the method described next is a generalization of the fixed-partition scheme  called mvt  ; it is used primarily in batch environments many of the ideas presented here are also applicable to a time-sharing environment in which pure segmentation is used for memory management  section 8.6   in the scheme  the operating system keeps a table indicating which parts of memory are available and which are occupied  initially  all memory is available for user processes and is considered one large block of available memory a eventually as you will see  memory contains a set of holes of various sizes  as processes enter the system  they are put into an input queue the operating system takes into account the memory requirements of each process and the amount of available memory space in determining which processes are allocated memory when a process is allocated space  it is loaded into memory  and it can then compete for cpu time when a process terminates  it releases its memory which the operating system may then fill with another process from the input queue  at any given time  then  we have a list of available block sizes and an input queue the operating system can order the input queue according to a scheduling algorithm memory is allocated to processes untit finally  the memory requirements of the next process can not be satisfied -that is  no available block of memory  or hole  is large enough to hold that process the operating system can then wait until a large enough block is available  or it can skip down the input queue to see whether the smaller memory requirements of some other process can be met  in generat as mentioned  the memory blocks available comprise a set of holes of various sizes scattered throughout memory when a process arrives and needs memory  the system searches the set for a hole that is large enough for this process if the hole is too large  it is split into two parts one part is allocated to the arriving process ; the other is returned to the set of holes when a process terminates  it releases its block of memory  which is then placed back in the set of holes if the new hole is adjacent to other holes  these adjacent holes are merged to form one larger hole at this point  the system may need to check whether there are processes waiting for memory and whether this newly freed and recombined memory could satisfy the demands of any of these waiting processes  this procedure is a particular instance of the general which concerns how to satisfy a request of size n from a there are many solutions to this problem the and strategies are the ones most commonly used to select a free hole from the set of available holes  first fit allocate the first hole that is big enough searching can start either at the beginning of the set of holes or at the location where the previous first-fit search ended we can stop searching as soon as we find a free hole that is large enough  8.3 327 best fit allocate the smallest hole that is big enough we must search the entire list  unless the list is ordered by size this strategy produces the smallest leftover hole  worst fit allocate the largest hole again  we must search the entire list  unless it is sorted by size this strategy produces the largest leftover hole  which may be more useful than the smaller leftover hole from a best-fit approach  simulations have shown that both first fit and best fit are better than worst fit in terms of decreasing time and storage utilization neither first fit nor best fit is clearly better than the other in terms of storage utilization  but first fit is generally faster  8.3.3 fragmentation both the first-fit and best-fit strategies for memory allocation suffer from external as processes are loaded and removed from memory  the free memory space is broken into little pieces external fragmentation exists when there is enough total memory space to satisfy a request but the available spaces are not contiguous ; storage is fragmented into a large number of small holes this fragmentation problem can be severe in the worst case  we could have a block of free  or wasted  memory between every two processes if all these small pieces of memory were in one big free block instead  we might be able to run several more processes  whether we are using the first-fit or best-fit strategy can affect the amount of fragmentation  first fit is better for some systems  whereas best fit is better for others  another factor is which end of a free block is allocated  which is the leftover piece-the one on the top or the one on the bottom  no matter which algorithm is used  however  external fragmentation will be a problem  depending on the total amount of memory storage and the average process size  external fragmentation may be a minor or a major problem statistical analysis of first fit  for instance  reveals that  even with some optimization  given n allocated blocks  another 0.5 n blocks will be lost to fragmentation  that is  one-third of memory may be unusable ! this property is known as the memory fragmentation can be internal as well as external consider a multiple-partition allocation scheme with a hole of 18,464 bytes suppose that the next process requests 18,462 bytes if we allocate exactly the requested block  we are left with a hole of 2 bytes the overhead to keep track of this hole will be substantially larger than the hole itself the general approach to avoiding this problem is to break the physical memory into fixed-sized blocks and allocate memory in units based on block size with this approach  the memory allocated to a process may be slightly larger than the requested memory the difference between these two numbers is internal memory that is internal to a partition  one solution to the problem of external fragmentation is the goal is to shuffle the memory contents so as to place all free n'lemory together in one large block compaction is not always possible  however if relocation is static and is done at assembly or load time  compaction can not be done ; compaction is possible only if relocation is dynamic and is done at execution 328 chapter 8 8.4 time if addresses are relocated dynamically  relocation requires only moving the program and data and then changing the base register to reflect the new base address when compaction is possible  we must determine its cost the simplest compaction algorithm is to move all processes toward one end of memory ; all holes move in the other direction  producing one large hole of available memory this scheme can be expensive  another possible solution to the external-fragmentation problem is to permit the logical address space of the processes to be noncontiguous  thus allowing a process to be allocated physical memory wherever such memory is available two complementary techniques achieve this solution  paging  section 8.4  and segmentation  section 8.6   these techniques can also be combined  section 8.7   is a memory-management scheme that permits the physical address space a process to be noncontiguous paging avoids external fragmentation and the need for compaction it also solves the considerable problem of fitting memory chunks of varying sizes onto the backin.g store ; most memorymanagement schemes used before the introduction of paging suffered from this problem the problem arises because  when some code fragments or data residing in main memory need to be swapped out  space must be fmmd on the backing store the backing store has the same fragmentation problems discussed in connection with main memory  but access is much slower  so compaction is impossible because of its advantages over earlier methods  paging in its various forms is used in most operating systems  physical address foooo  0000 f1111  1111 page table figure 8.7 paging hardware  1---------1 physical memory 8.4 329 traditionally  support for paging has been handled by hardware however  recent designs have implemented paging by closely integrating the hardware and operating system  especially on 64-bit microprocessors  8.4.1 basic method the basic method for implementing paging involves breaking physical memory into fixed-sized blocks called harnes and breaking logical memory into blocks of the same size called when a process is to be executed  its pages are loaded into any available memory frames from their source  a file system or the backing store   the backing store is divided into fixed-sized blocks that are of the san1.e size as the memory frames  the hardware support for paging is illustrated in figure 8.7 every address generated the cpu is divided into two parts  a  p  and a  the page number is used as an index into a the page table contains the base address of each page in physical memory this base address is combined with the page offset to define the physical memory address that is sent to the memory unit the paging model of memory is shown in figure 8.8  the page size  like the frame size  is defined by the hardware the size of a page is typically a power of 2  varying between 512 bytes and 16 mb per page  depending on the computer architecture the selection of a power of 2 as a page size makes the translation of a logical address into a page number and page offset particularly easy if the size of the logical address space is 2m  and a page size is 271 addressing units  bytes or wordst then the high-order m n bits of a logical address designate the page number  and the n low-order bits designate the page offset thus  the logical address is as follows  logical memory ~ w page table frame number physical memory figure 8.8 paging model of logical and physical memory  330 chapter 8 page number page offset d m -n n where p is an index into the page table and d is the displacement within the page  as a concrete  although minuscule  example  consider the memory in figure 8.9 here  in the logical address  n = 2 and m = 4 using a page size of 4 bytes and a physical memory of 32 bytes  8 pages   we show how the user 's view of memory can be mapped into physical memory logical address 0 is page 0  offset 0 indexing into the page table  we find that page 0 is in frame 5 thus  logical address 0 maps to physical address 20  =  5 x 4  + 0   logical address 3  page 0  offset 3  maps to physical address 23  =  5 x 4  + 3   logical address 4 is page 1  offset 0 ; according to the page table  page 1 is mapped to frame 6 thus  logical address 4 maps to physical address 24  =  6 x 4  + o   logical address 13 maps to physical address 9  you may have noticed that paging itself is a form of dynamic relocation  every logical address is bound by the paging hardware to some physical address using paging is similar to using a table of base  or relocation  registers  one for each frame of memory  ~ m6 2 1 3 2 page table logical memory physical memory figure 8.9 paging example for a 32-byte memory with 4-byte pages  8.4 331 when we use a paging scheme  we have no external fragmentation  any free frame can be allocated to a process that needs it however  we may have some internal fragmentation notice that frames are allocated as units if the memory requirements of a process do not happen to coincide with page boundaries  the last frame allocated may not be completely full for example  if page size is 2,048 bytes  a process of 72,766 bytes will need 35 pages plus 1,086 bytes it will be allocated 36 frames  resulting in internal fragmentation of 2,048  1,086 = 962 bytes in the worst case  a process would need 11 pages plus 1 byte it would be allocated 11 + 1 frames  resulting in internal fragmentation of almost an entire frame  if process size is independent of page size  we expect internal fragmentation to average one-half page per process this consideration suggests that small page sizes are desirable however  overhead is involved in each page-table entry  and this overhead is reduced as the size of the pages increases also  disk i/0 is more efficient when the amount data being transferred is larger  chapter 12   generally  page sizes have grown over time as processes  data sets  and main memory have become larger today  pages typically are between 4 kb and 8 kb in size  and some systems support even larger page sizes some cpus and kernels even support multiple page sizes for instance  solaris uses page sizes of 8 kb and 4 mb  depending on the data stored by the pages  researchers are now developing support for variable on-the-fly page size  usually  each page-table entry is 4 bytes long  but that size can vary as well  a 32-bit entry can point to one of 232 physical page frames if frame size is 4 kb  then a system with 4-byte entries can address 244 bytes  or 16 tb  of physical memory  when a process arrives in the system to be executed  its size  expressed in pages  is examined each page of the process needs one frame thus  if the process requires 11 pages  at least 11 frames must be available in memory if n frames are available  they are allocated to this arriving process the first page of the process is loaded injo one of the allocated frames  and the frame number is put in the page table for this process the next page is loaded into another frame  its frame number is put into the page table  and so on  figure 8.10   an important aspect of paging is the clear separation between the user 's view of memory and the actual physical memory the user program views memory as one single space  containing only this one program in fact  the user program is scattered throughout physical memory  which also holds other programs the difference between the user 's view of memory and the actual physical memory is reconciled by the address-translation hardware the logical addresses are translated into physical addresses this mapping is hidden from the user and is controlled by the operating system notice that the user process by definition is unable to access memory it does not own it has no way of addressing memory outside of its page table  and the table includes only those pages that the process owns  since the operating system is managing physical memory  it must be aware of the allocation details of physical memory-which frames are allocated  which frames are available  how many total frames there are  and so on this information is generally kept in a data structure called a frame the frame table has one entry for each physical page frame  indicating whether the latter is free or allocated and  if it is allocated  to which page of which process or processes  332 chapter 8 free-frame list free-frame list 14 13 15 13 13 18 20 14 14 15 15 15 16 16 17 17 18 18 19 01 19 1 13 20 2 18 20 3.20 21 new-process page table 21  a   b  figure 8.10 free frames  a  before allocation and  b  after allocation  in addition  the operating system must be aware that user processes operate in user space  and all logical addresses must be mapped to produce physical addresses if a user makes a system call  to do i/0  for example  and provides an address as a parameter  a buffe1 ~ for instance   that address must be mapped to produce the correct physical address the operating system maintains a copy of the page table for each process  just as it maintains a copy of the instruction counter and register contents this copy is used to translate logical addresses to physical addresses whenever the operating system must map a logical address to a physical address manually it is also used by the cpu dispatcher to define the hardware page table when a process is to be allocated the cpu paging therefore increases the context-switch time  8.4.2 hardware support each operating system has its own methods for storing page tables most allocate a page table for each process a pointer to the page table is stored with the other register values  like the instruction counter  in the process control block when the dispatcher is told to start a process  it must reload the user registers and define the correct hardware page-table values from the stored user page table  the hardware implementation of the page table can be done in several in the simplest case  the page table is implemented as a set of dedicated these registers should be built with very high-speed logic to make the paging-address translation efficient every access to memory nlust go through the paging map  so efficiency is a major consideration the cpu dispatcher reloads these registers  just as it reloads the other registers instructions to load or modify the page-table registers are  of course  privileged  so that only the operating system can change the memory map the dec pdp-11 is an example of such an architecture the address consists of 16 bits  and the page size is 8 kb the page table thus consists of eight entries that are kept in fast registers  8.4 333 the use of registers for the page table is satisfactory if the page table is reasonably sncall  for example  256 entries   most contemporary computers  however  allow the page table to be very large  for example  1 million entries   for these machines  the use of fast registers to implement the page table is not feasible rather  the page table is kept in main memory  and a points to the page table changing page tables requires changing only this one register  substantially reducing context-switch time  the problem with this approach is the time required to access a user memory location if we want to access location i  we must first index into the page table  using the value in the ptbr offset by the page number fori this task requires a memory access it provides us with the frame number  which is combined with the page offset to produce the actual address we can then access the desired place in memory with this scheme  two memory accesses are needed to access a byte  one for the page-table entry  one for the byte   thus  memory access is slowed by a factor of 2 this delay would be intolerable under most circumstances we might as well resort to swapping ! the standard solution to this problem is to use a special  small  fastlookup hardware cache  called a bc.1her the tlb is associative  high-speed memory each entry in the tlb consists of two parts  a key  or tag  and a value when the associative memory is presented with an item  the item is compared with all keys simultaneously if the item is found  the corresponding value field is returned the search is fast ; the hardware  however  is expensive typically  the number of entries in a tlb is small  often numbering between 64 and 1,024  the tlb is used with page tables in the following way the tlb contains only a few of the page-table entries when a logical address is generated by the cpu  its page number is presented to the tlb if the page number is found  its frame number is immediately available and is used to access memory the whole task may take less than 10 percent longer than it would if an unmapped memory reference were used  if the page number is not in the tlb  known as a a memory reference to the page table must be made when the frame number is obtained  we can use it to access memory  figure 8.11   in addition  we add the page number and frame number to the tlb  so that they will be found quickly on the next reference if the tlb is already full of entries  the operating system must select one for replacement replacement policies range from least recently used  lru  to random furthermore  some tlbs allow certain entries to be meaning that they can not be removed from the tlb typically  tlb entries for kernel code are wired down  some tlbs store in each tlb entry an asid uniquely identifies each process and is used to provide address-space protection for that process when the tlb attempts to resolve virtual page numbers  it ensures that the asid for the currently running process matches the asid associated with the virtual page if the asids do not match  the attempt is treated as a tlb miss in addition to providing address-space protection  an asid allows the tlb to contain entries for several different processes simultaneously  if the tlb does not support separate asids  then every time a new table is selected  for instance  with each context switch   the tlb must  or erased  to ensure that the next executing process does not use the wrong translation information otherwise  the tlb could include old entries that 334 chapter 8 tlb hit tlb p tlb miss page table figure 8.11 paging hardware with tlb  physical memory contain valid virtual addresses but have incorrect or invalid physical addresses left over from the previous process  the percentage of times that a particular page number is found in the tlb is called the an 80-percent hit ratio  for example  means that we find the desired page number in the tlb 80 percent of the time if it takes 20 nanoseconds to search the tlb and 100 nanoseconds to access memory  then a mapped-memory access takes 120 nanoseconds when the page number is in the tlb if we fail to find the page number in the tlb  20 nanoseconds   then we must first access memory for the page table and frame number  100 nanoseconds  and then access the desired byte in memory  100 nanoseconds   for a total of 220 nanoseconds to find the effective we weight the case by its probability  effective access time = 0.80 x 120 + 0.20 x 220 = 140 nanoseconds  in this example  we suffer a 40-percent slowdown in memory-access time  from 100 to 140 nanoseconds   for a 98-percent hit ratio  we have effective access time = 0.98 x 120 + 0.02 x 220 = 122 nanoseconds  this increased hit rate produces only a 22 percent slowdown in access time  we will further explore the impact of the hit ratio on the tlb in chapter 9  8.4 335 8.4.3 protection memory protection in a paged environment is accomplished by protection bits associated with each frame normally  these bits are kept in the page table  one bit can define a page to be read-write or read-only every reference to memory goes through the page table to find the correct frame nuncber at the same time that the physical address is being computed  the protection bits can be checked to verify that no writes are being made to a read-only page an attempt to write to a read-only page causes a hardware trap to the operating system  or memory-protection violation   we can easily expand this approach to provide a finer level of protection  we can create hardware to provide read-only  read-write  or execute-only protection ; or  by providing separate protection bits for each kind of access  we can allow any combination of these accesses illegal attempts will be trapped to the operating system  one additional bit is generally attached to each entry in the page table  a bit when this bit is set to valid  the associated page is in the process 's logical address space and is thus a legal  or valid  page when the bit is set to invalid  the page is not in the process 's logical address space illegal addresses are trapped by use of the valid -invalid bit the operating system sets this bit for each page to allow or disallow access to the page  suppose  for example  that in a system with a 14-bit address space  0 to 16383   we have a program that should use only addresses 0 to 10468 given a page size of 2 kb  we have the situation shown in figure 8.12 addresses in 0 frame number j valid-invalid bit 0 10,468 1 2,287 '-----'--'--' ' page n figure 8 i 2 valid  v  or invalid  i  bit in a page table  336 chapter 8 pages 0  1  2  3  4  and 5 are mapped normally through the page table any attempt to generate an address in pages 6 or 7  however  will find that the valid -invalid bit is set to invalid  and the computer will trap to flee operating system  invalid page reference   notice that this scheme has created a problem because the program extends only to address 10468  any reference beyond that address is illegal  howeve1 ~ references to page 5 are classified as valid  so accesses to addresses up to 12287 are valid only the addresses from 12288 to 16383 are invalid this problem is a result of the 2-kb page size and reflects the internal fragmentation of paging  rarely does a process use all its address range in fact many processes use only a small fraction of the address space available to them it would be wasteful in these cases to create a page table with entries for every page in the address range most of this table would be unused but would take up valuable memory space some systems provide hardware  in the form of a length to indicate the size of the page table value is checked against every logical address to verify that the address is in the valid range for the process failure of this test causes an error trap to the operating system  8.4.4 shared pages an advantage of paging is the possibility of sharing common code this consideration is particularly important in a time-sharing environment consider a system that supports 40 users  each of whom executes a text editor if the text editor consists of 150 kb of code and 50 kb of data space  we need 8,000 kb to support the 40 users if the code is  or pure however  it can be shared  as shown in figure 8.13 here we see a three-page editor-each page 50 kb in size  the large page size is used to simplify the figure  -being shared among three processes each process has its own data page  reentrant code is non-self-modifying code  it never changes during execution  thus  two or more processes can execute the same code at the same time  each process has its own copy of registers and data storage to hold the data for the process 's execution the data for two different processes wilt of course  be different  only one copy of the editor need be kept in physical memory each user 's page table maps onto the same physical copy of the editor  but data pages are mapped onto different frames thus  to support 40 users  we need only one copy of the editor  150 kb   plus 40 copies of the 50 kb of data space per user  the total space required is now 2  50 kb instead of 8,000 kb-a significant savings  other heavily used programs can also be shared -compilers  window systems  run-time libraries  database systems  and so on to be sharable  the code must be reentrant the read-only nature of shared code should not be left to the correctness of the code ; the operating system should enforce this property  the sharing of memory among processes on a system is similar to the sharing of the address space of a task by threads  described in chapter 4  furthermore  recall that in chapter 3 we described shared memory as a method 8.5 ed 1   ed 2 ed 3 data .1 process p1 process p3 page table for p1 page table for p3 8.5 ed 1 ed 2 ed 3 data 2 process p2 0 data 1 2 data 3 3 ed 1 ed 2 ed 3  4 5 6 data 2 page table for p2 7 8 9 10 11 figure 8.13 sharing of code in a paging environment  337 of interprocess corrununication some operating systems implement shared memory using shared pages  organizing memory according to pages provides numerous benefits in addition to allowing several processes to share the same physical pages we cover several other benefits in chapter 9  in this section  we explore some of the most common techniques for structuring the page table  8.5.1 hierarchical paging most modern computer systems support a large logical address space  232 to 264   in such an environment  the page table itself becomes excessively large for example  consider a system with a 32-bit logical address space if the page size in such a system is 4 kb  212   then a page table may consist of up to 1 million entries  232 /212   assuming that each entry consists of 4 bytes  each process may need up to 4mb of physical address space for the page table alone clearly  we would not want to allocate the page table contiguously in main memory one simple solution to this problem is to divide the page table into smaller pieces we can accomplish this division in several ways  one way is to use a two-level paging algorithm  in which the page table itself is also paged  figure 8.14   for example  consider again the system with 338 chapter 8 0 page table memory figure 8.14 a two-level page-table scheme  a 32-bit logical address space and a page size of 4 kb a logical address is divided into a page number consisting of 20 bits and a page offset consisting of 12 bits because we page the page table  the page number is further divided into a 10-bit page number and a 10-bit page offset thus  a logical address is as follows  page number page offset d 10 10 12 where p1 is an index into the outer page table and p2 is the displacement within the page of the outer page table the address-translation method for this architecture is shown in figure 8.15 because address translation works from the outer page table inward  this scheme is also known as a the vax architecture supports a variation of two-level paging the vax is a 32-bit machine with a page size of 512 bytes the logical address space of a process is divided into four equal sections  each of which consists of 230 bytes  each section represents a different part of the logical address space of a process  the first 2 high-order bits of the logical address designate the appropriate section the next 21 bits represent the logical page number of that section  and the final 9 bits represent an offset in the desired page by partitioning the page outer page table 8.5 figure 8 15 address translation for a two-level 32-bit paging architecture  339 table in this manner  the operating system can leave partitions unused until a process needs them an address on the vax architecture is as follows  section page offset s p d 2 21 9 where s designates the section number  p is an index into the page table  and d is the displacement within the page even when this scheme is used  the size of a one-level page table for a vax process using one section is 221 bits 4 bytes per entry = 8mb to further reduce main-memory use  the vax pages the user-process page tables  for a system with a 64-bit logical address space  a two-level paging scheme is no longer appropriate to illustrate this point  let us suppose that the page size in such a system is 4 kb  212   in this case  the page table consists of up to 252 entries if we use a two-level paging scheme  then the iml.er page tables can conveniently be one page long  or contain 210 4-byte entries the addresses look like this  outer page inner page offset i  pl  i p2  i d 42 10 12 the outer page table consists of 242 entries  or 244 bytes the obvious way to avoid such a large table is to divide the outer page table into smaller pieces   this approach is also used on some 32-bit processors for added flexibility and efficiency  we can divide the outer page table in various ways we can page the outer page table  giving us a three-level paging scheme suppose that the outer page table is made up of standard-size pages  210 entries  or 212 bytes   in this case  a 64-bit address space is still daunting  2nd outer page outer page inner page offset i pr   p2 i p3 i d 32 10 10 12 the outer page table is sti11234 bytes in size  340 chapter 8 the next step would be a four-level paging scheme  where the second-level outer page table itself is also paged  and so forth the 64-bit ultrasparc would require seven levels of paging-a prohibitive number of memory accessesto translate each logical address you can see from this example why  for 64-bit architectures  hierarchical page tables are generally considered inappropriate  8.5.2 hashed page tables a common approach for handling address spaces larger than 32 bits is to use a with the hash value being the virtual page number each entry in the hash table contains a linked list of elements that hash to the same location  to handle collisions   each element consists of three fields   1  the virtual page number   2  the value of the mapped page frame  and  3  a pointer to the next element in the linked list  the algorithm works as follows  the virtual page number in the virtual address is hashed into the hash table the virtual page number is compared with field 1 in the first element in the linked list if there is a match  the corresponding page frame  field 2  is used to form the desired physical address  if there is no match  subsequent entries in the linked list are searched for a matching virtual page number this scheme is shown in figure 8.16  a variation of this scheme that is favorable for 64-bit address spaces has been proposed this variation uses which are similar to hashed page tables except that each entry in the hash table refers to several pages  such as 16  rather than a single page therefore  a single page-table entry can store the mappings for multiple physical-page frames clustered page tables are particularly useful for address spaces  where memory references are noncontiguous and scattered throughout the address space  8.5.3 inverted page tables usually  each process has an associated page table the page table has one entry for each page that the process is using  or one slot for each virtual hash table figure 8.16 hashed page table  physical address physical memory 8.5 341 address  regardless of the latter 's validity   this table representation is a natural one  since processes reference pages through the pages ' virtual addresses the operating system must then translate this reference into a physical memory address since the table is sorted by virtual address  the operating system is able to calculate where in the table the associated physical address entry is located and to use that value directly one of the drawbacks of this method is that each page table may consist of millions of entries these tables may consume large amounts of physical memory just to keep track of how other physical memory is being used  to solve this problem  we can use an page an inverted page table has one entry for each real page  or frame  of memory each entry consists of the virtual address of the page stored in that real memory location  with information about the process that owns the page thus  only one page table is in the system  and it has only one entry for each page of physical memory figure 8.17 shows the operation of an inverted page table compare it with figure 8.7  which depicts a standard page table in operation inverted page tables often require that an address-space identifier  section 8.4.2  be stored in each entry of the page table  since the table usually contains several different address spaces mapping physical memory storing the address-space identifier ensures that a logical page for a particular process is mapped to the corresponding physical page frame examples of systems using inverted page tables include the 64-bit ultrasparc and powerpc  to illustrate this method  we describe a simplified version of the i11verted page table used in the ibm rt each virtual address in the system consists of a triple  process-id  page-number  offset  each inverted page-table entry is a pair process-id  page-number where the process-id assumes the role of the address-space identifier when a memory page table physical address figure 8.17 inverted page table  physical memory 342 chapter 8 8.6 reference occurs  part of the virtual address  consisting of process-id  pagenumber  is presented to the memory subsystem the inverted page table is then searched for a match if a match is found-say  at entry i-then the physical address i  offset is generated if no match is found  then an illegal address access has been attempted  although this scheme decreases the amount of memory needed to store each page table  it increases the amount of time needed to search the table when a page reference occurs because the inverted page table is sorted by physical address  but lookups occur on virtual addresses  the whole table might need to be searched for a match this search would take far too long to alleviate this problem  we use a hash table  as described in section 8.5.2  to limit the search to one-or at most a few-page-table entries of course  each access to the hash table adds a memory reference to the procedure  so one virtual memory reference requires at least two real memory reads-one for the hash-table entry and one for the page table  recall that the tlb is searched first  before the hash table is consulted  offering some performance improvement  systems that use inverted page tables have difficulty implementing shared memory shared memory is usually implemented as multiple virtual addresses  one for each process sharing the memory  that are mapped to one physical address this standard method can not be used with inverted page tables ; because there is only one virtual page entry for every physical page  one physical page can not have two  or more  shared virtual addresses a simple technique for addressing this issue is to allow the page table to contain only one mapping of a virtual address to the shared physical address this means that references to virtual addresses that are not mapped result in page faults  an important aspect of memory management that became unavoidable with paging is the separation of the user 's view of memory from the actual physical memory as we have already seen  the user 's view of memory is not the same as the actual physical memory the user 's view is mapped onto physical memory this mapping allows differentiation between logical memory and physical memory  8.6.1 basic method do users think of memory as a linear array of bytes  some containing instructions and others containing data most people would say no rather  users prefer to view memory as a collection of variable-sized segments  with no necessary ordering among segments  figure 8.18   consider how you think of a program when you are writing it you think of it as a main program with a set of methods  procedures  or functions it may also include various data structures  objects  arrays  stacks  variables  and so on each of these modules or data elements is referred to by name you talk about the stack  the math library  the n1.ain program  without caring what addresses in memory these elements occupy you are not concerned with whether the stack is stored before or after the sqrt   function each of these segments is of variable length ; the length is intrinsically defined by subroutine symbol table  main program logical address 8.6 figure 8.18 user 's view of a program  343 the purpose of the segment in the program elements within a segment are identified by their offset from the begim1.ing of the segment  the first statement of the program  the seventh stack frame entry in the stack  the fifth instruction of the sqrt    and so on  is a memory-management scheme that supports this user view of memory a logical address space is a collection of segments each segment has a name and a length the addresses specify both the segment name and the offset within the segment the user therefore specifies each address by two quantities  a segment name and an offset  contrast this scheme with the paging scheme  in which the user specifies only a single address  which is partitioned by the hardware into a page number and an offset  all invisible to the programmer  for simplicity of implementation  segments are numbered and are referred to by a segn lent number  rather than by a segment name thus  a logical address consists of a two tuple  segment-number  offset  normally  the user program is compiled  and the compiler automatically constructs segments reflecting the input program  a c compiler might create separate segments for the following  the code global variables the heap  from which memory is allocated the stacks used by each thread the standard c library 344 chapter 8 no segment table yes trap  addressing error + figure 8.19 segmentation hardware  physical memory libraries that are linked in during compile time might be assign.ed separate segments the loader would take all these segments and assign them segment numbers  8.6.2 hardware although the user can now refer to objects in the program by a two-dimensional address  the actual physical memory is still  of course  a one-dimensional sequence of bytes thus  we must define an implementation to map twodimensional user-defined addresses into one-dimensional physical addresses  this mapping is effected by a each entry in the segment table has a segment base and a segment limit the segment base contains the startilcg physical address where the segment resides in memory  and the segment limit specifies the length of the segment  the use of a segment table is illustrated in figure 8.19 a logical address consists of two parts  a segment number  s  and an offset into that segment  d  the segment number is used as an index to the segment table the offset d of the logical address must be between 0 and the segment limit if it is not  we trap to the operating system  logical addressing attempt beyond end of segment   when an offset is legal  it is added to the segment base to produce the address in physical memory of the desired byte the segment table is thus essentially an array of base-limit register pairs  as an example  consider the situation shown in figure 8.20 we have five segments numbered from 0 through 4 the segments are stored in physical memory as shown the segment table has a separate entry for each segment  giving the beginning address of the segment in physical memory  or base  and the length of that segment  or limit   for example  segment 2 is 400 bytes long and begins at location 4300 thus  a reference to byte 53 of segment 2 is mapped 8.7 subroutine segment o segment1 symbol table  segment 4 main program segment 2 logical address space 8.7 0 2 3 4 limit base 1000 1400 400 6300 400 4300 1100 3200 1000 4700 segment table figure 8.20 example of segmentation  14001---1 segment o 2400 3200 1-----1 segment 3 4300 1 ~ --1 4700 segment 2 segment 4 5700 f--------1 6300   s ~ gt \ 1e ! it 1 6700 physical memory 345 onto location 4300 + 53 = 4353 a reference to segment 3  byte 852  is mapped to 3200  the base of segment 3  + 852 = 4052 a reference to byte 1222 of segment 0 would result in a trap to the operating system  as this segment is only tooo bytes long  both paging and segmentation have advantages and disadvantages in fact some architectures provide both in this section  we discuss the intel pentium architecture  which supports both pure segmentation and segmentation with paging we do not give a complete description of the memory-management structure of the pentium in this text rather  we present the major ideas on which it is based we conclude our discussion with an overview of linux address translation on pentium systems  in pentium systems  the cpu generates logical addresses  which are given to the segmentation unit the segmentation unit produces a linear address for each logical address the linear address is then given to the paging unit  which in turn generates the physical address in main memory thus  the segmentation and paging units form the equivalent of the memory-management unit  mmu   this scheme is shown in figure 8.21  8.7.1 pentium segmentation the pentium architecture allows a segment to be as large as 4 gb  and the maximum number of segments per process is 16 k the logical-address space 346 chapter 8 i cpu i figure 8.21 logical to physical address translation in the pentium  of a process is divided into two partitions the first partition consists of up to 8 k segments that are private to that process the second partition consists of up to 8 k segments that are shared all the processes information about the first partition is kept in the information about the second partition is kept in the each entry in the ldt and gdt consists of an 8-byte segment descriptor with detailed information about a particular segment  including the base location and limit of that segment  the logical address is a pair  selector  offset   where the selector is a 16-bit number  g p 13 2 in which s designates the segment number  g indicates whether the segment is in the gdt or ldt  and p deals with protection the offset is a 32-bit number specifying the location of the byte  or word  within the segment in question  the machine has six segment registers  allowing six segments to be addressed at any one time by a process it also has six 8-byte microprogram registers to hold the corresponding descriptors from either the ldt or gdt  this cache lets the pentium avoid having to read the descriptor from memory for every memory reference  the linear address on the pentium is 32 bits long and is formed as follows  the segment register points to the appropriate entry in the ldt or gdt the base and limit information about the segment in question is used to generate a first  the limit is used to check for address validity if the address is not valid  a memory fault is generated  resulting in a trap to the operating system if it is valid  then the value of the offset is added to the value of the base  resulting in a 32-bit linear address this is shown in figure 8.22 in the following section  we discuss how the paging unit turns this linear address into a physical address  8.7.2 pentium paging the pentium architecture allows a page size of either 4 kb or 4 mb for 4-kb pages  the pentium uses a two-level paging schence in which the division of the 32-bit linear address is as follows  page number page offset d 10 10 12 the address-translation scheme for this architecture is similar to the scheme shown in figure 8.15 the intel pentium address translation is shown in more 8.7 347 logical address offset + 32-bit linear address figure 8.22 intel pentium segmentation  detail in figure 8.23 the 10 high-order bits reference an entry in the outern'lost page table  which the pentium terms the page directory  the cr3 register points to the page directory for the current process  the page directory entry points to an inner page table that is indexed by the contents of the innermost 10 bits in the linear address finally  the low-order bits 0-11 refer to the offset in the 4-kb page pointed to in the page table  one entry in the page directory is the page size flag  which-if setindicates that the size of the page frame is 4 mb and not the standard 4 kb  if this flag is set  the page directory points directly to the 4-mb page frame  bypassing the inner page table ; and the 22 low-order bits in the linear address refer to the offset in the 4-mb page frame  31 cr3 registe r page directory page directory page directory  logical address  page table 22 21 l 1211 page table  i offset 31 22 21 offset j 4-kb page 4-mb page figure 8.23 paging in the pentium architecture  0 0 3l ! 8 chapter 8 to improve the efficiency of physical memory use  intel pentium page tables can be swapped to disk in this case  an invalid bit is used in the page directory entry to indicate whether the table to which the entry is pointing is in memory or on disk if the table is on disk  the operating system can use the other 31 bits to specify the disk location of the table ; the table then can be brought into memory on demand  8.7.3 linux on pentium systems as an illustration  consider the linux operating system running on the intel pentium architecture because linux is designed to run on a variety of processors many of which may provide only limited support for segmentationlinux does not rely on segmentation and uses it minimally on the pentium  linux uses only six segments  a segment for kernel code a segment for kernel data a segment for user code a segment for user data a task-state segment  tss  1i a default ldt segment the segments for user code and user data are shared by all processes running in user mode this is possible because all processes use the same logical address space and all segment descriptors are stored in the global descriptor table  gdt   furthermore  each process has its own task-state segment  tss   and the descriptor for this segment is stored in the gdt the tss is used to store the hardware context of each process during context switches the default ldt segment is normally shared by all processes and is usually not used however  if a process requires its own ldt  it can create one and use that instead of the default ldt  as noted  each segment selector includes a 2-bit field for protection thus  the pentium allows four levels of protection of these four levels  limlx only recognizes two  user mode and kernel mode  although the pentium uses a two-level paging model  linux is designed to run on a variety of hardware platforms  many of which are 64-bit platforms where two-level paging is not plausible therefore  linux has adopted a threelevel paging strategy that works well for both 32-bit and 64-bit architectures  the linear address in linux is broken into the following four parts  global directory middle directory page table figure 8.24 highlights the three-level paging model in linux  the number of bits in each part of the linear address varies according to architecture however  as described earlier in this section  the pentium architecture only uses a two-level paging model how  then  does linux apply 8.8 lglobal directory global directory cr3 __,.c__ ___ __l register 8.8  linear address  middle directory figure 8.24 three-level paging in linux  offset page frame 349 its three-level model on the pentium in this situation  the size of the middle directory is zero bits  effectively bypassing the middle directory  each task in linux has its own set of page tables and -just as in figure 8.23 -the cr3 register points to the global directory for the task currently executing  during a context switch  the value of the cr3 register is saved and restored in the tss segments of the tasks involved in the context switch  memory-management algorithms for multiprogrammed operating systems range from the simple single-user system approach to paged segmentation  the most important determinant of the method used in a particular system is the hardware provided every memory address generated by the cpu must be checked for legality and possibly mapped to a physical address the checking can not be implemented  efficiently  in software hence  we are constrained by the hardware available  the various memory-management algorithms  contiguous allocation  paging  segmentation  and combinations of paging and segmentation  differ in many aspects in comparing different memory-management strategies  we use the following considerations  hardware support a simple base register or a base-limit register pair is sufficient for the single and multiple-partition schemes  whereas paging and segmentation need mapping tables to define the address map  performance as the memory-management algorithm becomes more complex  the time required to map a logical address to a physical address increases for the simple systems  we need only compare or add to the logical address-operations that are fast paging and segmentation can be as fast if the mapping table is implemented in fast registers if the table is 350 chapter 8 in memory  however  user memory accesses can be degraded substantially  a tlb can reduce the performance degradation to an acceptable level  fragmentation a multiprogrammed system will generally perform more efficiently if it has a higher level of multiprogramming for a given set of processes  we can increase the multiprogramming level only by packing more processes into memory to accomplish this task  we must reduce memory waste  or fragmentation systems with fixed-sized allocation units  such as the single-partition scheme and paging  suffer from internal fragmentation systems with variable-sized allocation units  such as the multiple-partition scheme and segmentation  suffer from external fragmentation  relocation one solution to the external-fragmentation problem is compaction  compaction involves shifting a program in memory in such a way that the program does not notice the change this consideration requires that logical addresses be relocated dynamically  at execution time  if addresses are relocated only at load time  we can not compact storage  swapping swapping can be added to any algorithm at intervals determined by the operating system  usually dictated by cpu-scheduling policies  processes are copied from main memory to a backing store and later are copied back to main memory this scheme allows more processes to be run than can be fit into memory at one time  sharing another means of increasing the multiprogramming level is to share code and data among different users sharing generally requires that either paging or segmentation be used to provide small packets of information  pages or segments  that can be shared sharing is a means of running many processes with a limited amount of memory  but shared programs and data must be designed carefully  protection if paging or segmentation is provided  different sections of a user program can be declared execute-only  read -only  or read-write this restriction is necessary with shared code or data and is generally useful in any case to provide simple run-time checks for common programming errors  8.1 explain the difference between internal and external fragmentation  8.2 compare the memory organization schemes of contiguous memory allocation  pure segmentation  and pure paging with respect to the following issues  a external fragmentation b internal fragmentation c ability to share code across processes 351 8.3 why are segmentation and paging sometimes combined into one scheme 8.4 most systems allow a program to allocate more memory to its address space during execution allocation of data in the heap segments of programs is an example of such allocated memory what is required to support dynamic memory allocation in the following schemes a contiguous memory allocation b pure segmentation c pure paging 8.5 consider the intel address-translation scheme shown in figure 8.22  a describe all the steps taken by the intel pentium in translatil g a logical address into a physical address  b what are the advantages to the operating system of hardware that provides such complicated memory translation c are there any disadvantages to this address-translation system if so  what are they if not  why is this scheme not used by every manufacturer 8.6 what is the purpose of paging the page tables 8.7 explain why sharil g a reentrant module is easier when segmentation is used than when pure paging is used  8.8 on a system with paging  a process can not access memory that it does not own why how could the operating system allow access to other memory why should it or should it not 8.9 compare the segmented pagil g scheme with the hashed page table scheme for handling large address spaces under what circumstances is one scheme preferable to the other 8.10 consider a paging system with the page table stored in memory  a if a memory reference takes 200 nanoseconds  how long does a paged memory reference take b if we add tlbs  and 75 percent of all page-table references are found in the tlbs  what is the effective memory reference time  assume that finding a page-table entry in the tlbs takes zero time  if the entry is there  352 chapter 8 8.11 compare paging with segmentation with respect to the amount of memory required by the address translation structures in order to convert virtual addresses to physical addresses  8.12 consider a system in which a program can be separated into two parts  code and data the cpu knows whether it wants an instruction  instruction fetch  or data  data fetch or store   therefore  two baselimit register pairs are provided  one for instructions and one for data  the instruction base-limit register pair is automatically read-only  so programs can be shared among different users discuss the advantages and disadvantages of this scheme  8.13 consider the following process for generating binaries a compiler is used to generate the object code for individual modules  and a linkage editor is used to combine multiple object modules into a single program bilcary how does the linkage editor change the bindmg of instructions and data to memory addresses what information needs to be passed from the compiler to the linkage editor to facilitate the memory-binding tasks of the linkage editor 8.14 consider a logical address space of 64 pages of 1,024 words each  mapped onto a physical memory of 32 frames  a how many bits are there in the logical address b how many bits are there in the physical address 8.15 consider the hierarchical paging scheme used by the vax architecture  how many memory operations are performed when a user program executes a memory-load operation 8.16 given five memory partitions of 100 kb  500 kb  200 kb  300 kb  and 600 kb  ill order   how would the first-fit  best-fit  and worst-fit algorithms place processes of 212 kb  417 kb  112 kb  and 426 kb  in order  which algorithm makes the most efficient use of memory 8.17 describe a mechanism by which one segment could belong to the address space of two different processes  8.18 consider a computer system with a 32-bit logical address and 4-kb page size the system supports up to 512mb of physical memory how many entries are there in each of the following a a conventional single-level page table b an inverted page table 353 8.19 assuming a 1-kb page size  what are the page numbers and offsets for the following address references  provided as decimal numbers   a 2375 b 19366 c 30000 d 256 e 16385 8.20 program binaries in many systems are typically structured as follows  code is stored starting with a small  fixed virtual address  such as 0 the code segment is followed by the data segment that is used for storing the program variables when the program starts executing  the stack is allocated at the other end of the virtual address space and is allowed to grow toward lower virtual addresses what is the significance of this structure for the following schemes a contiguous memory allocation b pure segmentation c pure paging 8.21 consider the following segment table  segment base length 0 219 600 1 2300 14 2 90 100 3 1327 580 4 1952 96 what are the physical addresses for the following logical addresses a 0,430 b 1,10 c 2,500 d 3,400 e 4,112 8.22 consider a logical address space of 32 pages with 1,024 words per page  mapped onto a physical memory of 16 frames  a how many bits are required in the logical address b how many bits are required in the physical address 354 chapter 8 8.23 sharing segments among processes without requiring that they have the same segment number is possible in a dynamically linked segmentation system  a define a system that allows static linking and sharing of segments without requiring that the segment numbers be the same  b describe a paging scheme that allows pages to be shared without requiring that the page numbers be the same  8.24 assume that a system has a 32-bit virtual address with a 4-kb page size  write a c program that is passed a virtual address  in decincal  on the command line and have it output the page number and offset for the given address as an example  your program would run as follows  ./a.out 19986 your program would output  the address 19986 contains  page number = 4 offset = 3602 writing this program will require using the appropriate data type to store 32 bits we encourage you to use unsigned data types as well  dynamic storage allocation was discussed by knuth  1973   section 2.5   who found through simulation results that first fit is generally superior to best fit  knuth  1973  also discussed the 50-percent rule  the concept of paging can be credited to the designers of the atlas system  which has been described by kilburn et al  1961  and by howarth et al   1961   the concept of segmentation was first discussed by dennis  1965   paged segmentation was first supported in the ge 645  on which multics was originally implemented  organick  1972  and daley and dennis  1967    inverted page tables are discussed in an article about the ibm rt storage manager by chang and mergen  1988   address translation in software is covered in jacob and mudge  1997   hennessy and patterson  2002  explains the hardware aspects of tlbs  caches  and mmus talluri et al  1995  discusses page tables for 64-bit address spaces alternative approaches to enforcing memory protection are proposed and studied in wahbe et al  1993a   chase et al  1994   bershad et al  1995   and thorn  1997   dougan et al  1999  and jacob and mudge  2001  discuss 355 tedmiques for managing the tlb fang et al  2001  evaluate support for large pages  tanenbaum  2001  discusses intel80386 paging memory management for several architectures-such as the pentiunl ii  powerpc  and ultrasparcare described by jacob and mudge  1998a   segmentation on lim1x systems is presented in bovet and cesati  2002   9.1 c er in chapter 8  we discussed various memory-management strategies used in computer systems all these strategies have the same goal  to keep many processes in memory simultaneously to allow multiprogramming however  they tend to require that an entire process be in memory before it can execute  virtual memory is a tecrucique that allows the execution of processes that are not completely in memory one major advantage of this scheme is that programs can be larger than physical memory further  virtual memory abstracts main memory into an extremely large  uniform array of storage  separating logical memory as viewed by the user from physical memory  this technique frees programmers from the concerns of memory-storage limitations virtual memory also allows processes to share files easily and to implement shared memory in addition  it provides an efficient mechanism for process creation virtual memory is not easy to implement  however  and may substantially decrease performance if it is used carelessly in this chapter  we discuss virtual memory in the form of demand paging and examine its complexity and cost  to describe the benefits of a virtual memory system  to explain the concepts of demand paging  page-replacement algorithms  and allocation of page frames  to discuss the principles of the working-set model  the memory-management algorithms outlined in chapter 8 are necessary because of one basic requirement  the instructions being executed must be in physical memory the first approach to meeting this requirement is to place the entire logical address space in physical memory dynamic loading can help to ease this restriction  but it generally requires special precautions and extra work by the programmer  357 358 chapter 9 the requirement that instructions m.ust be in physical memory to be executed seems both necessary and reasonable ; but it is also unfortunate  since it limits the size of a program to the size of physical memory in fact  an examination of real programs shows us that  in many cases  the entire program is not needed for instance  consider the following  programs often have code to handle unusual error conditions since these errors seldom  if ever  occur in practice  this code is almost never executed  arrays,lists  and tables are often allocated more memory than they actually need an array may be declared 100 by 100 elements  even though it is seldom larger than 10 by 10 elements an assembler symbol table may have room for 3,000 symbols  although the average program has less than 200 symbols  certain options and features of a program may be used rarely for instance  the routines on u.s government computers that balance the budget have not been used in many years  even in those cases where the entire program is needed  it may not all be needed at the same time  the ability to execute a program that is only partially in memory would confer many benefits  a program would no longer be constrained by the amount of physical memory that is available users would be able to write programs for an extremely large virtual address space  simplifying the programming task  page 0 page 1 page 2 page v virtual memory memory map physical memory figure 9.1 diagram showing virtual memory that is larger than physical memory  9.1 359 because each user program could take less physical memory  more programs could be run at the sance time  with a corresponding increase in cpu utilization and throughput but with no increase in response time or turnaround time  less i/o would be needed to load or swap user programs into memory  so each user program would run faster  thus  running a program that is not entirely in memory would benefit both the system and the user  involves the separation of logical memory as perceived by users from physical memory this separation allows an extremely large virtual memory to be provided for programmers when only a smaller physical memory is available  figure 9.1   virtual memory makes the task of programming much easier  because the programmer no longer needs to worry about the amount of physical memory available ; she can concentrate instead on the problem to be programmed  the address space of a process refers to the logical  or virtual  view of how a process is stored in memory typically  this view is that a process begins at a certain logical address-say  address 0-and exists in contiguous memory  as shown in figure 9.2 recall from chapter 8  though  that in fact physical memory may be organized in page frames and that the physical page frames assigned to a process may not be contiguous it is up to the memorymanagement unit  mmu  to map logical pages to physical page frames in memory  note in figure 9.2 that we allow for the heap to grow upward in memory as it is used for dynamic memory allocation similarly  we allow for the stack to grow downward in memory through successive function calls the large blank space  or hole  between the heap and the stack is part of the virtual address figure 9.2 virtual address space  360 chapter 9 space but will require actual physical pages only if the heap or stack grows  virtual address spaces that include holes are known as sparse address spaces  using a sparse address space is beneficial because the holes can be filled as the stack or heap segments grow or if we wish to dynam.ically link libraries  or possibly other shared objects  during program execution  in addition to separating logical memory from physical memory  virtual memory allows files and memory to be shared by two or more processes through page sharing  section 8.4.4   this leads to the following benefits  system libraries can be shared by several processes through mapping of the shared object into a virtual address space although each process considers the shared libraries to be part of its virtual address space  the actual pages where the libraries reside in physical memory are shared by all the processes  figure 9.3   typically  a library is mapped read-only into the space of each process that is linked with it  similarly  virtual memory enables processes to share memory recall from chapter 3 that two or more processes can communicate through the use of shared memory virtual memory allows one process to create a region of memory that it can share with another process processes sharing this region consider it part of their virtual address space  yet the actual physical pages of memory are shared  much as is illustrated in figure 9.3  virtual memory can allow pages to be shared during process creation with the fork   system calt thus speeding up process creation  we further explore these-and other-benefits of virtual memory later in this chapter first though  we discuss implementing virtual memory through demand paging  shared library shared pages shared library figure 9.3 shared library using virtual memory  9.2 9.2 361 consider how an executable program might be loaded from disk into n'lemory  one option is to load the entire program in physical memory at program execution time however  a problent with this approach is that we may not initially need the entire program in memory suppose a program starts with a list of available options from which the user is to select loading the entire program into memory results in loading the executable code for all options  regardless of whether an option is ultimately selected by the user or not an alternative strategy is to load pages only as they are needed this technique is known as paging and is commonly used in virtual memory systems  with demand-paged virtual memory  pages are only loaded when they are demanded during program execution ; pages that are never accessed are thus never loaded into physical memory  a demand-paging system is similar to a paging system with swapping  figure 9.4  where processes reside in secondary memory  usually a disk   when we want to execute a process  we swap it into memory rather than swapping the entire process into memory  however  we use a a lazy swapper never swaps a page into memory unless that page will be needed  since we are now viewing a process as a sequence of pages  rather than as one large contiguous address space  use of the term swapper is technically incorrect  a swapper manipulates entire processes  whereas a is concerned with the individual pages of a process we thus use pager  rather than swapper  in connection with demand paging  program a program b main memory swap out so 90100110 120130140150 swap in 16017 figure 9.4 transfer of a paged memory to contiguous disk space  362 chapter 9 9.2.1 basic concepts when a process is to be swapped in  the pager guesses which pages will be used before the process is swapped out again instead of swapping in a whole process  the pager brings only those pages into memory thus  it avoids reading into memory pages that will not be used anyway  decreasing the swap time and the amount of physical memory needed  with this scheme  we need some form of hardware support to distinguish between the pages that are in memory and the pages that are on the disk  the valid -invalid bit scheme described in section 8.4.3 can be used for this purpose this time  however  when this bit is set to valid/ ' the associated page is both legal and in n1.emory if the bit is set to invalid/ ' the page either is not valid  that is  not in the logical address space of the process  or is valid but is currently on the disk the page-table entry for a page that is brought into memory is set as usuat but the page-table entry for a page that is not currently in memory is either simply marked invalid or contains the address of the page on disk this situation is depicted in figure 9.5  notice that marking a page invalid will have no effect if the process never attempts to access that page hence  if we guess right and page in all and only those pages that are actually needed  the process will run exactly as though we had brought in all pages while the process executes and accesses pages that are execution proceeds normally  0 2 3 4 5 6 7 valid-invalid frame bit ' \  i 0 4 v logical memory physical memory dod d  1j   @ jtb  odd figure 9.5 page table when some pages are not in main memory  operating system reference   ; \  page is on \   v backing store trap restart instruction page table reset page table physical memory 9.2 0 bring in missing page figure 9.6 steps in handling a page fault  363 but what happens if the process tries to access a page that was not brought into memory access to a page marked invalid causes a the paging hardware  in translating the address through the page table  will notice that the invalid bit is set  causing a trap to the operating system this trap is the result of the operating system 's failure to bring the desired page into memory  the procedure for handling this page fault is straightforward  figure 9.6   we check an internal table  usually kept with the process control block  for this process to determine whether the reference was a valid or an invalid memory access  if the reference was invalid  we terminate the process if it was valid  but we have not yet brought in that page  we now page it in  we find a free frame  by taking one from the free-frame list  for example   we schedule a disk operation to read the desired page into the newly allocated frame  when the disk read is complete  we modify the internal table kept with the process and the page table to indicate that the page is now in memory  we restart the instruction that was interrupted by the trap the process can now access the page as though it had always been in memory  in the extreme case  we can start executing a process with no pages in memory when the operating system sets the instruction pointer to the first 364 chapter 9 instruction of the process  which is on a non-memory-resident page  the process immediately faults for the page after this page is brought into memory  the process continues to execute  faulting as necessary until every page that it needs is in memory at that it can execute with no more faults this scheme is never bring a page into memory until it is required  theoretically  some programs could access several new pages of memory with each instruction execution  one page for the instruction and many for data   possibly causing multiple page faults per instruction this situation would result in unacceptable system performance fortunately  analysis of running processes shows that this behavior is exceedingly unlikely programs tend to have described in section 9.6.1  which results in reasonable performance from demand paging  the hardware to support demand paging is the same as the hardware for paging and swapping  page table this table has the ability to mark an entry invalid through a valid -invalid bit or a special value of protection bits  secondary memory this memory holds those pages that are not present in main memory the secondary memory is usually a high-speed disk it is known as the swap device  and the section of disk used for this purpose is known as swap-space allocation is discussed in chapter 12  a crucial requirement for demand paging is the ability to restart any instruction after a page fault because we save the state  registers  condition code  instruction counter  of the interrupted process when the page fault occurs  we must be able to restart the process in exactly the same place and state  except that the desired page is now in memory and is accessible in most cases  this requirement is easy to meet a page fault may occur at any memory reference if the page fault occurs on the instruction fetch  we can restart by fetching the instruction again if a page fault occurs while we are fetching an operand  we must fetch and decode the instruction again and then fetch the operand  as a worst-case example  consider a three-address instruction such as add the content of a to b  placing the result in c these are the steps to execute this instruction  fetch and decode the instruction  add   fetch a fetch b  add a and b  store the sum in c  if we fault when we try to store inc  because c is in a page not currently in memory   we will have to get the desired page  bring it in  correct the page table  and restart the instruction the restart will require fetching the instruction again  decoding it again  fetching the two operands again  and 9.2 365 then adding again however  there is not much repeated work  less than one complete instruction   and the repetition is necessary only when a page fault occurs  the major difficulty arises when one instruction may modify several different locations for example  consider the ibm system 360/370 mvc  move character  instruction  which can ncove up to 256 bytes from one location to another  possibly overlapping  location if either block  source or destination  straddles a page boundary  a page fault might occur after the move is partially done in addition  if the source and destination blocks overlap  the source block may have been modified  in which case we can not simply restart the instruction  this problem can be solved in two different ways in one solution  the microcode computes and attempts to access both ends of both blocks if a page fault is going to occm ~ it will happen at this step  before anything is modified  the move can then take place ; we know that no page fault can occur  since all the relevant pages are in memory the other solution uses temporary registers to hold the values of overwritten locations if there is a page fault  all the old values are written back into memory before the trap occurs this action restores memory to its state before the instruction was started  so that the instruction can be repeated  this is by no means the only architectural problem resulting from adding paging to an existing architecture to allow demand paging  but it illustrates some of the difficulties involved paging is added between the cpu and the memory in a computer system it should be entirely transparent to the user process thus  people often assume that paging can be added to any system  although this assumption is true for a non-demand-paging environment  where a page fault represents a fatal errm ~ it is not true where a page fault means only that an additional page must be brought into memory and the process restarted  9.2.2 performance of demand paging demand paging can significantly affect the performance of a computer system  to see why  let 's compute the effective access time for a demand-paged memory for most computer systems  the memory-access time  denoted ma  ranges from 10 to 200 nanoseconds as long as we have no page faults  the effective access time is equal to the memory access time if  howeve1 ~ a page fault occurs  we must first read the relevant page from disk and then access the desired word  let p be the probability of a page fault  0    ; p    ; 1   we would expect p to be close to zero-that is  we would expect to have only a few page faults the t'tp r ' ! nrr access is then effective access time =  1  p  x ma + p x page fault time  to compute the effective access time  we must know how much time is needed to service a page fault a page fault causes the following sequence to occur  trap to the operating system  save the user registers and process state  366 chapter 9 deterncine that the interrupt was a page fault  check that the page reference was legal and determine the location of the page on the disk issue a read from the disk to a free frame  a wait in a queue for this device until the read request is serviced  b wait for the device seek and/ or latency time  c begin the transfer of the page to a free frame  while waiting  allocate the cpu to some other user  cpu scheduling  optional   receive an interrupt from the disk i/0 subsystem  i/0 completed   save the registers and process state for the other user  if step 6 is executed   determine that the interrupt was from the disk correct the page table and other tables to show that the desired page is now in memory  wait for the cpu to be allocated to this process again  restore the user registers  process state  and new page table  and then resume the interrupted instruction  not all of these steps are necessary in every case for example  we are assuming that  in step 6  the cpu is allocated to another process while the i/o occurs  this arrangement allows multiprogramming to maintain cpu utilization but requires additional time to resume the page-fault service routine when the i/0 transfer is complete  in any case  we are faced with tlu ee major components of the page-fault service time  service the page-fault interrupt  read in the page  restart the process  the first and third tasks can be reduced  with careful coding  to several hundred instructions these tasks may take from 1 to 100 microseconds each  the page-switch time  however  will probably be close to 8 milliseconds   a typical hard disk has an average latency of 3 milliseconds  a seek of 5 milliseconds  and a transfer time of 0.05 milliseconds thus  the total paging time is about 8 milliseconds  including hardware and software time  remember also that we are looking at only the device-service time if a queue of processes is waiting for the device  we have to add device-queueing time as we wait for the paging device to be free to service our request  increasing even more the time to swap  with an average page-fault service time of 8 milliseconds and a memoryaccess time of 200 nanoseconds  the effective access time in nanoseconds is 3 9.3 effective access time =  1  p  x  200  + p  8 milliseconds  =  1 p  x 200 + p x 8,000,000 = 200 + 7,999,800 x p  367 we see  then  that the effective access time is directly proportional to the if one access out of 1,000 causes a page fault  the effective access time is 8.2 microseconds the computer will be slowed down by a factor of 40 because of demand paging ! if we want performance degradation to be less than 10 percent  we need 220 200 + 7,999,800 x p  20 7,999,800 x p  p 0.0000025  that is  to keep the slowdown due to paging at a reasonable level  we can allow fewer than one memory access out of 399,990 to page-fault in sum  it is important to keep the page-fault rate low in a demand-paging system  otherwise  the effective access time increases  slowing process execution dramatically  an additional aspect of demand paging is the handling and overall use of swap space disk i/0 to swap space is generally faster than that to the file system it is faster because swap space is allocated in much larger blocks  and file lookups and indirect allocation methods are not used  chapter 12   the system can therefore gain better paging throughput by copying an entire file image into the swap space at process startup and then performing demand paging from the swap space another option is to demand pages from the file system initially but to write the pages to swap space as they are replaced this approach will ensure that only needed pages are read from the file system but that all subsequent paging is done from swap space  some systems attempt to limit the amount of swap space used through demand paging of binary files demand pages for such files are brought directly from the file system however  when page replacement is called for  these frames can simply be overwritten  because they are never modified   and the pages can be read in from the file system again if needed using this approach  the file system itself serves as the backing store howeve1 ~ swap space must still be used for pages not associated with a file ; these pages include the stack and heap for a process this method appears to be a good compromise and is used in several systems  including solaris and bsd unix  in section 9 .2  we illustrated how a process can start quickly by merely demandpaging in the page containing the first instruction however  process creation using the fork   system call may initially bypass the need for demand paging by using a technique similar to page sharing  covered in section 8.4.4   this technique provides for rapid process creation and minimizes the number of new pages that must be allocated to the newly created process  368 chapter 9 physical figure 9.7 before process i modifies page c  recall thatthe fork   system call creates a child process that is a duplicate of its parent traditionally  fork   worked by creating a copy of the parent 's address space for the child  duplicating the pages belonging to the parent  however  considering that many child processes invoke the exec   system call immediately after creation  the copying of the parent 's address space may be unnecessary instead  we can use a technique known as which works by allowing the parent and child processes initially to share the same pages these shared pages are marked as copy-on-write pages  meaning that if either process writes to a shared page  a copy of the shared page is created copy-on-write is illustrated in figures 9.7 and figure 9.8  which show the contents of the physical memory before and after process 1 modifies page c  for example  assume that the child process attempts to modify a page containing portions of the stack  with the pages set to be copy-on-write the operating system will create a copy of this page  nl.apping it to the address space of the child process the child process will then modify its copied page and not the page belonging to the parent process obviously  when the copy-on-write technique is used  only the pages that are modified by either process are copied ; all unmodified pages can be shared by the parent and child processes note  too  process1 physical memory figure 9.8 after process 1 modifies page c  process2 9.4 9.4 369 that only pages that can be nwdified need be m ~ arked as copy-on-write pages that can not be modified  pages containing executable code  can be shared by the parent and child copy-on-write is a common technique used by several operating systems  including windows xp  linux  and solaris  when it is determined that a page is going to be duplicated using copyon write  it is important to note the location from which the free page will be allocated many operating systems provide a of free pages for such requests these free pages are typically allocated when the stack or heap for a process must expand or when there are copy-on-write pages to be managed  operating systems typically allocate these pages using a technique known as zem-fhl-on-den  1and zero-fill-on-demand pages have been zeroed-out before being allocated  thus erasing the previous contents  several versions of unix  including solaris and linux  provide a variation ofthe fork   system call-vfork    for fori    that operates differently from fork   with copy-on-write with vfork    the parent process is suspended  and the child process uses the address space of the parent  because vfork   does not use copy-on-write  if the child process changes any pages of the parent 's address space  the altered pages will be visible to the parent once it resumes therefore  vf ork   must be used with caution to ensure that the child process does not modify the address space of the parent vf or k   is intended to be used when the child process calls exec   immediately after creation because no copying of pages takes place  vf ork   is an extremely efficient method of process creation and is sometimes used to implement unix command-line shell interfaces  in our earlier discussion of the page-fault rate  we assumed that each page faults at most once  when it is first referenced this representation is not strictly accurate  however if a process of ten pages actually uses only half of them  then demand paging saves the i/0 necessary to load the five pages that are never used we could also increase our degree of multiprogramming by running twice as many processes thus  if we had forty frames  we could run eight processes  rather than the four that could run if each required ten frames  five of which were never used   if we increase our degree of multiprogramming  we are memory if we run six processes  each of which is ten pages in size but uses only five pages  we have higher cpu utilization and throughput  ten frames to spare it is possible  however  that each of these processes  for a particular data set  may suddenly try to use all ten of its pages  resulting in a need for sixty frames when only forty are available  further  consider that system memory is not used only for holding program pages buffers for i/ 0 also consume a considerable amount of memory this use can increase the strain on memory-placement algorithms deciding how much memory to allocate to i/0 and how much to program pages is a significant challenge some systems allocate a fixed percentage of memory for i/0 buffers  whereas others allow both user processes and the i/0 subsystem to compete for all system memory  370 chapter 9 valid-invalid pc   -_ = ' ~ ~ = =  ! came f il logical memory for user 1 page table for user 1 valid-invalid 0 frame ~ bi ~ r ~ v v ~ -------' ' 2 3 logical memory for user 2 page table for user 2 0 monitor 2 3 4 5 j 6 a 7 e physical memory figure 9.9 need for page replacement over-allocation of memory manifests itself as follows while a user process is executing  a page fault occurs the operating system determines where the desired page is residing on the disk but then finds that there are no free frames on the free-frame list ; all memory is in use  figure 9.9   the operating system has several options at this point it could terminate the user process however  demand paging is the operating system 's attempt to improve the computer system 's utilization and throughput users should not be aware that their processes are running on a paged system-paging should be logically transparent to the user so this option is not the best choice  the operating system could instead swap out a process  freeing all its frames and reducing the level of multiprogramming this option is a good one in certain circumstances  and we consider it further in section 9.6 here  we discuss the most common solution  9.4.1 basic page replacement page replacement takes the following approach if no frame is free  we find one that is not currently being used and free it we can free a frame by writing its contents to swap space and changing the page table  and all other tables  to indicate that the page is no longer in memory  figure 9.10   we can now use the freed frame to hold the page for which the process faulted we modify the page-fault service routine to include page replacement  find the location of the desired page on the disk  find a free frame  a if there is a free frame  use it  9.4 371 b if there is no free frame  use a page-replacement algorithnc to select a c write the victim frame to the disk ; change the page and frame tables accordingly  read the desired page into the newly freed frame ; change the page and frame tables  restart the user process  notice that  if no frames are free  two page transfers  one out and one in  are required this situation effectively doubles the page-fault service time and increases the effective access time accordingly  we can reduce this overhead by using a  or when this scheme is used  each page or frame has a modify bit associated with it in the hardware the modify bit for a page is set by the hardware whenever any word or byte in the page is written into  indicating that the page has been modified  when we select a page for replacement  we examine its modify bit if the bit is set  we know that the page has been modified since it was read in from the disk in this case  we must write the page to the disk if the modify bit is not set  however  the page has not been modified since it was read into memory in this case  we need not write the memory page to the disk  it is already there this technique also applies to read-only pages  for example  pages of binary code   such pages can not be modified ; thus  they may be discarded when desired  this scheme can significantly reduce the time required to service a page fault  since it reduces i/o time by one-half if the page has not been modified  frame valid-invalid bit ' \  / physical memory figure 9.10 page replacement 372 chapter 9 page replacement is basic to demand paging it completes the separation between logical memory and physical memory with this mechanism  an enormous virtual memory can be provided for programn'lers on a smaller physical memory with no demand paging  user addresses are mapped into physical addresses  so the two sets of addresses can be different all the pages of a process still must be in physical memory  however with demand paging  the size of the logical address space is no longer constrained by physical memory  if we have a user process of twenty pages  we can execute it in ten frames simply by using demand paging and using a replacement algorithm to find a free frame whenever necessary if a page that has been modified is to be replaced  its contents are copied to the disk a later reference to that page will cause a page fault at that time  the page will be brought back into memory  perhaps replacing some other page in the process  we must solve two major problems to implement demand develop a algorithm and a ' ' '  ' ' l tceme ~ lu ~ ~ f ' ' ~ ~ ''h ' that is  if we have multiple processes in memory  we must decide how many frames to allocate to each process ; and when page replacement is required  we must select the frames that are to be replaced designing appropriate algorithms to solve these problems is an important task  because disk i/0 is so expensive even slight improvements in demand-paging methods yield large gains in system performance  there are many different page-replacement algorithms every operating system probably has its own replacement scheme how do we select a particular replacement algorithm in general  we want the one with the lowest page-fault rate  we evaluate an algorithm by running it on a particular string of memory references and computing the number of page faults the string of memory references is called a reference we can generate reference strings artificially  by using a random-number generator  for example   or we can trace a given system and record the address of each memory reference the latter choice produces a large number of data  on the order of 1 million addresses per second   to reduce the number of data  we use two facts  first  for a given page size  and the page size is generally fixed by the hardware or system   we need to consider only the page number  rather than the entire address second  if we have a reference to a page p  then any references to page p that immediately follow will never cause a page fault page p will be in memory after the first reference  so the immediately following references will not fault  for example  if we trace a particular process  we might record the following address sequence  0100,0432,0101,0612,0102,0103,0104,0101,0611,0102,0103  0104,0101,0610,0102,0103,0104,0101,0609,0102,0105 at 100 bytes per page  this sequence is reduced to the following reference string  1  4  1  6  1  6  1  6  1  6  1 9.4 373 16 g  14    j  ; 2 12 q  ol co 10 0  0 8 ' q  ..0 6 e    j c 4 2 2 3 4 5 6 number of frames figure 9.1 i graph of page faults versus number of frames  to determine the number of page faults for a particular reference string and page-replacement algorithm  we also need to know the number of page frames available obviously  as the number of frames available increases  the number of page faults decreases for the reference stril'lg considered previously  for example  if we had three or more frames  we would have only three faultsone fault for the first reference to each page in contrast  with only one frame available  we would have a replacement with every reference  resulting in eleven faults in general  we expect a curve such as that in figure 9.11 as the number of frames increases  the number of page faults drops to some minimal level of course  adding physical memory increases the number of frames  we next illustrate several page-replacement algorithms in doing so  we use the reference string for a memory with three frames  9.4.2 fifo page replacement the simplest page-replacement algorithm is a first-in  first-out  fifo  algorithm  a fifo replacement algorithm associates with each page the time when that page was brought into memory when a page must be replaced  the oldest page is chosen notice that it is not strictly necessary to record the time when a page is brought in we can create a fifo queue to hold all pages in memory  we replace the page at the head of the queue when a page is brought into memory  we insert it at the tail of the queue  for our example reference string  our three frames are initially empty the first three references  7  0  1  cause page faults and are brought into these empty frames the next reference  2  replaces page 7  because page 7 was brought in first since 0 is the next reference and 0 is already in memory  we have no fault for this reference the first reference to 3 results in replacement of page 0  since 374 chapter 9 reference string 7 0 2 0 3 0 4 2 3 0 3 2 2 0 7 0 page frames figure 9.12 fifo page-replacement algorithm  it is now first in line because of this replacement  the next reference  to 0  will fault page 1 is then replaced by page 0 this process continues as shown in figure 9.12 every time a fault occurs  we show which pages are in our three frames there are fifteen faults altogether  the fifo page-replacement algorithm is easy to lmderstand and program  however  its performance is not always good on the one hand  the page replaced may be an initialization module that was used a long time ago and is no longer needed on the other hand  it could contain a heavily used variable that was initialized early and is in constant use  notice that  even if we select for replacement a page that is in active use  everything still works correctly after we replace an active page with a new one  a fault occurs almost immediately to retrieve the active page some other page must be replaced to bring the active page back into memory thus  a bad replacement choice increases the page-fault rate and slows process execution  it does not  however  cause incorrect execution  to illustrate the problems that are possible with a fifo page-replacement algorithm  we consider the following reference string  1  2  3  4  1  2  5  1  2  3  4  5 figure 9.13 shows the curve of page faults for this reference string versus the number of available frames notice that the number of faults for four frames  ten  is greater than the number of faults for three frames  nine  ! this most unexpected result is known as  for some page-replacement algorithms  the page-fault rate may increase as the number of allocated frames increases we would expect that giving more memory to a process would improve its performance in some early research  investigators noticed that this assumption was not always true belady 's anomaly was discovered as a result  9.4.3 optimal page replacement of belady 's anomaly was the search for an which has the lowest page-fault rate of all algorithms and will never suffer from belady 's anomaly such an algorithm does exist and has been called opt or min it is simply this  replace the page that will not be used for the longest period of time  9.4 375 16 ~    5 2 12 cj  moj 10 0  0 8 cj _o e 6    5 c 4 2 number of frames figure 9.13 page-fault curve for fifo replacement on a reference string  use of this page-replacement algorithm guarantees the lowest possible pagefault rate for a fixed number of frames  for example  on our sample reference string  the optimal page-replacement algorithm would yield nine page faults  as shown in figure 9.14 the first three references cause faults that fill the three empty frames the reference to page 2 replaces page 7  because page 7 will not be used until reference 18  whereas page 0 will be used at 5  and page 1 at 14 the reference to page 3 replaces page 1  as page 1 will be the last of the three pages in memory to be referenced again with only nine page faults  optimal replacement is much better than a fifo algorithm  which results in fifteen faults  if we ignore the first three  which all algorithms must suffer  then optimal replacement is twice as good as fifo replacement  irt fact  no replacement algorithm can process this reference string in three frames with fewer than nine faults  unfortunately  the optimal page-replacement algorithm is difficult to implement  because it requires future knowledge of the reference string  we encountered a similar situation with the sjf cpu-schedulin.g algorithm in section 5.3.2  as a result  the optimal algorithm is used mainly for comparison studies for instance  it may be useful to know that  although a new algorithm reference string 7 0 2 0 3 0 4 2 3 0 3 2 2 0 7 0 page frames figure 9.14 optimal page-replacement algorithm  376 chapter 9 is not optimat it is within 12.3 percent of optimal at worst and within 4.7 percent on average  9.4.4 lru page replacement lf the optimal algorithm is not feasible  perhaps an approximation of the optimal algorithm is possible the key distinction between the fifo and opt algorithms  other than looking backward versus forward in time  is that the fifo algorithm uses the time when a page was brought into memory  whereas the opt algorithm uses the time when a page is to be used if we use the recent past as an approximation of the near future  then we can replace the that has not been used for the longest period of time this approach is the lru replacement associates with each page the time of that page 's last use  when a page must be replaced  lru chooses the page that has not been used for the longest period of time we can think of this strategy as the optimal page-replacement algorithm looking backward in time  rather than forward   strangely  if we let sr be the reverse of a reference strings  then the page-fault rate for the opt algorithm on sis the same as the page-fault rate for the opt algorithm on sr similarly  the page-fault rate for the lru algorithm on sis the same as the page-fault rate for the lru algorithm on sr  the result of applying lru replacement to our example reference string is shown in figure 9.15 the lru algorithm produces twelve faults notice that the first five faults are the same as those for optimal replacement when the reference to page 4 occurs  however  lru replacement sees that  of the three frames in memory  page 2 was used least recently thus  the lru algorithm replaces page 2  not knowing that page 2 is about to be used when it then faults for page 2  the lru algorithm replaces page 3  since it is now the least recently used of the three pages in memory despite these problems  lru replacement with twelve faults is much better than fifo replacement with fifteen  the lru policy is often used as a page-replacement algorithm and is considered to be good the major problem is how to implement lru replacement an lru page-replacement algorithm may require substantial hardware assistance the problem is to determine an order for the frames defined by the time of last use two implementations are feasible  counters in the simplest case  we associate with each page-table entry a time-of-use field and add to the cpu a logical clock or counter the clock is reference string 7 0 2 0 3 0 4 2 3 0 3 2 2 0 7 0 page frames figure 9.15 lru page-replacement algorithm  9.4 377 incremented for every memory reference whenever a reference to a page is made  the contents of the clock register are copied to the ti1ne-of-use field in the page-table entry for that page in this way  we always have the time of the last reference to each page we replace the page with the smallest time value this scheme requires a search of the page table to find the lru page and a write to memory  to the time-of-use field in the page table  for each memory access the times must also be m ~ aintained when page tables are changed  due to cpu scheduling   overflow of the clock must be considered  stack another approach to implementing lru replacement is to keep a stack of page numbers whenever a page is referenced  it is removed from the stack and put on the top in this way  the most recently used page is always at the top of the stack and the least recently used page is always at the bottom  figure 9.16   because entries must be removed from the middle of the stack  it is best to implement this approach by using a doubly linked list with a head pointer and a tail pointer removing a page and putting it on the top of the stack then requires changing six pointers at worst each update is a little more expensive  but there is no search for a replacement ; the tail pointer points to the bottom of the stack  which is the lru page this approach is particularly appropriate for software or microcode implementations of lru replacement  like optimal replacement  lru replacement does not suffer from belady 's both belong to a class of page-replacement algorithms  called si  ack that can never exhibit belady 's anomaly a stack algorithm is an algorithm for which it can be shown that the set of pages in memory for n frames is always a subset of the set of pages that would be in memory with n + 1 frames for lru replacement  the set of pages in memory would be the n most recently referenced pages if the number of frames is increased  these n pages will still be the most recently referenced and so will still be in memory  note that neither implementation of lru would be conceivable without hardware assistance beyond the standard tlb registers the updating of the reference string 4 7 0 7 stack before a 0 2 stack after b 2 7 2 i l a b figure 9.16 use of a stack to record the most recent page references  378 chapter 9 clock fields or stack must be done for every memory reference if we were to use an interrupt for every reference to allow software to update such data structures  it would slow every memory reference by a factor of at least ten  hence slowing every user process by a factor of ten few systems could tolerate that level of overhead for memory management  9.4.5 lru-approximation page replacement few computer systems provide sufficient hardware support for true lru page replacement some systems provide no hardware support  and other pagereplacement algorithms  such as a fifo algorithm  must be used many systems provide some help  however  in the form of a the reference bit for a page is set by the hardware whenever that page is referenced  either a read or a write to any byte in the page   reference bits are associated with each entry in the page table  initially  all bits are cleared  to 0  by the operating system as a user process executes  the bit associated with each page referenced is set  to 1  by the hardware after some time  we can determine which pages have been used and which have not been used by examining the reference bits  although we do not know the order of use this information is the basis for many page-replacement algorithms that approximate lru replacement  9.4.5.1 additional-reference-bits algorithm we can gain additional ordering information by recording the reference bits at regular intervals we can keep an 8-bit byte for each page in a table in memory  at regular intervals  say  every 100 milliseconds   a timer interrupt transfers control to the operating system the operating system shifts the reference bit for each page into the high-order bit of its 8-bit byte  shifting the other bits right by 1 bit and discarding the low-order bit these 8-bit shift registers contain the history of page use for the last eight time periods if the shift register contains 00000000  for example  then the page has not been used for eight time periods ; a page that is used at least once in each period has a shift register value of 11111111 a page with a history register value of 11000100 has been used more recently than one with a value of 01110111 if we interpret these 8-bit bytes as unsigned integers  the page with the lowest number is the lru page  and it can be replaced notice that the numbers are not guaranteed to be unique  however we can either replace  swap out  all pages with the smallest value or use the fifo method to choose among them  the number of bits of history included in the shift register can be varied  of course  and is selected  depending on the hardware available  to make the updating as fast as possible in the extreme case  the number can be reduced to zero  leaving only the reference bit itself this algorithm is called the 9.4.5.2 second-chance algorithm the basic algorithm of second-chance replacement is a fifo replacement algorithm when a page has been selected  however  we inspect its reference bit if the value is 0  we proceed to replace this page ; but if the reference bit is set to 1  we give the page a second chance and move on to select the next next victim 9.4 reference pages reference pages bits bits circular queue of pages circular queue of pages  a   b  figure 9.17 second-chance  clock  page-replacement algorithm  379 fifo page when a page gets a second chance  its reference bit is cleared  and its arrival time is reset to the current time thus  a page that is given a second chance will not be replaced until all other pages have been replaced  or given second chances   in addition  if a page is used often enough to keep its reference bit set  it will never be replaced  one way to implement the second-chance algorithm  sometimes referred to as the clock algorithm  is as a circular queue a poi11ter  that is  a hand on the clock  indicates which page is to be replaced next when a frame is needed  the pointer advances until it finds a page with a 0 reference bit as it advances  it clears the reference bits  figure 9.17   once a victim page is found  the page is replaced  and the new page is inserted in the circular queue in that position  notice that  in the worst case  when all bits are set  the pointer cycles through the whole queue  giving each page a second chance it clears all the reference bits before selecting the next page for replacement second-chance replacement degenerates to fifo replacement if all bits are set  9.4.5.3 enhanced second-chance algorithm we can enhance the second-chance algorithm by considering the reference bit and the modify bit  described in section 9.4.1  as an ordered pair with these two bits  we have the following four possible classes   0  0  neither recently used nor modified -best page to replace 380 chapter 9  0  1  not recently used hut modified-not quite as good  because the page will need to be written out before replacement  1  0  recently used but clean-probably will be used again soon  1  1  recently used and modified -probably will be used again soon  and the page will be need to be written out to disk before it can be replaced each page is in one of these four classes when page replacement is called for  we use the same scheme as in the clock algorithm ; but instead of examining whether the page to which we are pointing has the reference bit set to 1  we examine the class to which that page belongs we replace the first page encountered in the lowest nonempty class notice that we may have to scan the circular queue several times before we find a page to be replaced  the major difference between this algorithm and the simpler clock algorithm is that here we give preference to those pages that have been modified to reduce the number of i/os required  9.4.6 counting-based page replacement there are many other algorithms that can be used for page replacement for example  we can keep a counter of the number of references that have been made to each page and develop the following two schemes  the least frequently used  lfu  page-replacement algorithm requires that the page with the smallest count be replaced the reason for this selection is that an actively used page should have a large reference count  a problem arises  however  when a page is used heavily during the initial phase of a process but then is never used again since it was used heavily  it has a large count and remains in memory even though it is no longer needed one solution is to shift the counts right by 1 bit at regular intervals  forming an exponentially decaying average usage count  the most frequently used  mfu  page-replacement algorithm is based on the argument that the page with the smallest count was probably just brought in and has yet to be used  as you might expect  neither mfu nor lfu replacement is common the implementation of these algorithms is expensive  and they do not approxin'late opt replacement well  9.4.7 page-buffering algorithms other procedures are often used in addition to a specific page-replacement algorithm for example  systems commonly keep a pool of free frames when a page fault occurs  a victim frame is chosen as before however  the desired page is read into a free frame from the pool before the victim is written out this procedure allows the process to restart as soon as possible  without waiting for the victim page to be written out when the victim is later written out  its frame is added to the free-frame pool  9.4 381 an expansion of this idea is to maintain a list of modified pages whenever the paging device is idle  a modified page is selected and is written to the disk  its modify bit is then reset this scheme increases the probability that a page will be clean when it is selected for replacement and will not need to be written out  another modification is to keep a pool of free frames but to remember which page was in each frame since the frame contents are not modified when a frame is written to the disk  the old page can be reused directly fronc the free-frame pool if it is needed before that frame is reused no i/o is needed in this case when a page fault occurs  we first check whether the desired page is in the free-frame pool if it is not  we must select a free frame and read into it  this technique is used in the vax/vms system along with a fifo replacement algorithm when the fifo replacement algorithm mistakenly replaces a page that is still in active use  that page is quickly retrieved from the free-frame pool  and no i/o is necessary the free-frame buffer provides protection against the relatively poor  but sirnple  fifo replacement algorithm this method is necessary because the early versions of vax did not implement the reference bit correctly  some versions of the unix system use this method in conjunction with the second-chance algorithm it can be a useful augmentation to any pagereplacement algorithm  to reduce the penalty incurred if the wrong victim page is selected  9.4.8 applications and page replacement in certain cases  applications accessing data through the operating system 's virtual memory perform worse than if the operating system provided no buffering at all a typical example is a database  which provides its own memory management and i/0 buffering applications like this understand their memory use and disk use better than does an operating system that is implementing algorithms for general-purpose use if the operating system is buffering i/0  and the application is doing so as well  then twice the memory is being used for a set of i/0  in another example  data warehouses frequently perform massive sequential disk reads  followed by computations and writes the lru algorithm would be removing old pages and preserving new ones  while the application would more likely be reading older pages than newer ones  as it starts its sequential reads again   here  mfu would actually be more efficient than lru  because of such problems  some operating systems give special programs the ability to use a disk partition as a large sequential array of logical blocks  without any file-system data structures this array is sometimes called the raw disk  and i/o to this array is termed raw i/0 raw i/0 bypasses all the filesystem services  such as file i/0 demand paging  file locking  prefetching  space allocation  file names  and directories note that although certain applications are more efficient when implementing their own special-purpose storage services on a raw partition  most applications perform better when they use the regular file-system services  382 chapter 9 9.5 we turn next to the issue of allocation how do we allocate the fixed amount of free memory among the various processes if we have 93 free frames and two processes  how many frames does each process get the simplest case is the single-user system consider a single-user system with 128 kb of memory composed of pages 1 kb in size this system has 128 frames the operating system may take 35 kb  leaving 93 frames for the user process under pure demand paging  all 93 frames would initially be put on the free-frame list when a user process started execution  it would generate a sequence of page faults the first 93 page faults would all get free frames from the free-frame list when the free-frame list was exhausted  a page-replacement algorithm would be used to select one of the 93 in-memory pages to be replaced with the 94th  and so on when the process terminated  the 93 frames would once again be placed on the free-frame list  there are many variations on this simple strategy we can require that the operating system allocate all its buffer and table space from the free-frame list  when this space is not in use by the operating system  it can be used to support user paging we can try to keep three free frames reserved on the free-frame list at all times thus  when a page fault occurs  there is a free frame available to page into while the page swap is taking place  a replacement can be selected  which is then written to the disk as the user process continues to execute  other variants are also possible  but the basic strategy is clear  the user process is allocated any free frame  9.5.1 minimum number of frames our strategies for the allocation of frames are constrained in various ways we can not  for example  allocate more than the total number of available frames  unless there is page sharing   we must also allocate at least a minimum number of frames here  we look more closely at the latter requirement  one reason for allocating at least a minimum number of frames involves performance obviously  as the number of frames allocated to each process decreases  the page-fault rate increases  slowing process execution in addition  remember that when a page fault occurs before an executing ilcstruction is complete  the instruction must be restarted consequently we must have enough frames to hold all the different pages that any single ilcstruction can reference  for example  consider a machine in which all memory-reference instructions may reference only one memory address in this case  we need at least one frame for the instruction and one frame for the mernory reference in addition  if one-level indirect addressing is allowed  for example  a load instruction on page 16 can refer to an address on page 0  which is an indirect reference to page 23   then paging requires at least three frames per process think about what might happen if a process had only two frames  the minimum number of frames is defined by the computer architecture  for example  the move instruction for the pdp-11 includes more than one word for some addressing modes  and thus the ilcstruction itself may straddle two pages in addition  each of its two operands may be indirect references  for a total of six frames another example is the ibm 370 mvc instruction since the 9.5 383 instruction is from storage location to storage location  it takes 6 bytes and can straddle two pages the block of characters to move and the area to which it is to be m.oved can each also straddle two pages this situation would require six frames the worst case occurs when the mvc instruction is the operand of an execute instruction that straddles a page boundary ; in this case  we need eight frames  the worst-case scenario occurs in computer architectures that allow multiple levels of indirection  for example  each 16-bit word could contain a 15-bit address plus a 1-bit indirect indicator   theoretically  a simple load instruction could reference an indirect address that could reference an indirect address  on another page  that could also reference an indirect address  on yet another page   and so on  until every page in virtual memory had been touched  thus  in the worst case  the entire virtual memory must be in physical memory  to overcome this difficulty  we must place a limit on the levels of indirection  for example  limit an instruction to at most 16levels of indirection   when the first indirection occurs  a counter is set to 16 ; the counter is then decremented for each successive irtdirection for this instruction if the counter is decremented to 0  a trap occurs  excessive indirection   this limitation reduces the maximum number of memory references per instruction to 17  requiring the same number of frames  whereas the minimum number of frames per process is defined by the architecture  the maximum number is defined by the amount of available physical memory in between  we are still left with significant choice in frame allocation  9.5.2 allocation algorithms the easiest way to split m frames among n processes is to give everyone an equal share  m/n frames for instance  if there are 93 frames and five processes  each process will get 18 frames the three leftover frames can be used as a free-frame buffer pool this scheme is called an alternative is to recognize that various processes will need differing amounts of memory consider a system with a 1-kb frame size if a small student process of 10 kb and an interactive database of 127 kb are the only two processes running in a system with 62 free frames  it does not make much sense to give each process 31 frames the student process does not need more than 10 frames  so the other 21 are  strictly speaking  wasted  to solve this problem  we can use in which we allocate available memory to each process according to its size let the size of the virtual memory for process p ; be s ;  and define s = ls ;  then  if the total number of available frames is m  we allocate a ; frames to process p ;  where a ; is approximately a ; = s ; /s x m  384 chapter 9 of course  we must adjust each ai to be an integer that is greater than the ncinimum number of frames required by tl1e instruction set  with a sum not exceeding m  with proportional allocation  we would split 62 frames between two processes  one of 10 pages and one of 127 pages  by allocating 4 frames and 57 frames  respectively  since 10/137 x 62 ~ 4  and 127/137 x 62 ~ 57  in this way  both processes share the available frames according to their needs  rather than equally  in both equal and proportional allocation  of course  the allocation may vary according to the multiprogramming level if the multiprogramming level is increased  each process will lose some frames to provide the memory needed for the new process conversely  if the multiprogramming level decreases  the frames that were allocated to the departed process can be spread over the remaining processes  notice that  with either equal or proportional allocation  a high-priority process is treated the same as a low-priority process by its definition  however  we may want to give the high-priority process more memory to speed its execution  to the detriment of low-priority processes one solution is to use a proportional allocation scheme wherein the ratio of frames depends not on the relative sizes of processes but rather on the priorities of processes or on a combination of size and priority  9.5.3 global versus local allocation another important factor in the way frames are allocated to the various processes is page replacement with multiple processes competing for frames  we can classify page-replacement algorithms into two broad categories  ; .no ' '-c'u ~ ' ' and local global replacement allows a process to a replacement frame from the set of all frames  even if that frame is currently allocated to some other process ; that is  one process can take a frame from another local replacement requires that each process select from only its own set of allocated frames  for example  consider an allocation scheme wherein we allow high-priority processes to select frames from low-priority processes for replacement a process can select a replacement from among its own frames or the frames of any lower-priority process this approach allows a high-priority process to increase its frame allocation at the expense of a low-priority process with a local replacement strategy  the number of frames allocated to a process does not change with global replacement  a process may happen to select only frames allocated to other processes  thus increasing the number of frames allocated to it  assuming that other processes do not choose its frames for replacement   one problem with a global replacement algorithm is that a process can not control its own page-fault rate the set of pages in memory for a process depends not only on the paging behavior of that process but also on the paging behavior of other processes therefore  the same process may perform quite 9.5 385 differently  for example  taking 0.5 seconds for one execution and 10.3 seconds for the next execution  because of totally external circuntstances such is not the case with a local replacement algorithm under local replacement  the set of pages in memory for a process is affected by the paging behavior of only that process local replacement might hinder a process  however  by not making available to it other  less used pages of memory thus  global replacement generally results in greater system throughput and is therefore the more common method  9.5.4 non-uniform memory access thus far in our coverage of virtual memory  we have assumed that all main memory is created equal-or at least that it is accessed equally on many computer systems  that is not the case often  in systems with multiple cpus  section 1.3.2   a given cpu can access some sections of main memory faster than it can access others these performance differences are caused by how cpus and memory are interconnected in the system frequently  such a system is made up of several system boards  each containing multiple cpus and some memory the system boards are interconnected in various ways  ranging from system busses to high-speed network connections like infiniband as you might expect  the cpus on a particular board can access the memory on that board with less delay than they can access memory on other boards in the system systems in which memory access times vary significantly are known collectively as systems  and without exception  they are slower than systems in which memory and cpus are located on the same motherboard  managing which page frames are stored at which locations can significantly affect performance in numa systems if we treat memory as uniform in such a system  cpus may wait significantly longer for memory access than if we modify memory allocation algorithms to take numa into account similar changes must be rnade to the scheduling system the goal of these changes is to have memory frames allocated as close as possible to the cpu on which the process is running the definition of close is with minimum latency  which typically means on the same system board as the cpu  the algorithmic changes consist of having the scheduler track the last cpu on which each process ran if the scheduler tries to schedule each process onto its previous cpu  and the memory-management system tries to allocate frames for the process close to the cpu on which it is being scheduled  then improved cache hits and decreased memory access times will result  the picture is more complicated once threads are added for example  a process with many running threads may end up with those threads scheduled on many different system boards how is the memory to be allocated in this case solaris solves the problem by creating an entity in the kernel each lgroup gathers together close cpus and memory in fact  there is a hierarchy of lgroups based on the amount of latency between the groups solaris tries to schedule all threads of a process and allocate all memory of a process within an lgroup if that is not possible  it picks nearby lgroups for the rest of the resources needed in this manner  overall memory latency is minimized  and cpu cache hit rates are maximized  386 chapter 9 9.6 if the number of frames allocated to a low-priority process falls below the minimum number required by the computer architecture  we must suspend that process 's execution we should then page out its remaining pages  freeing all its allocated frames this provision introduces a swap-in  swap-out level of intermediate cpu scheduling  in fact  look at any process that does not have enough frames if the process does not have the num.ber of frames it needs to support pages in active use  it will quickly page-fault at this point  it must replace some page  however  since all its pages are in active use  it must replace a page that will be needed again right away consequently  it quickly faults again  and again  and again  replacing pages that it must back in immediately  this high paging activity is called a process is thrashing if it is spending more time paging than executing  9.6.1 cause of thrashing thrashing results in severe performance problems consider the following scenario  which is based on the actual behavior of early paging systems  the operating system monitors cpu utilization if cpu utilization is too low  we increase the degree of multiprogramming by introducing a new process to the system a global page-replacement algorithm is used ; it replaces pages without regard to the process to which they belong now suppose that a process enters a new phase in its execution and needs more frames it starts faulting and taking frames away from other processes these processes need those pages  however  and so they also fault  taking frames from other processes these faulting processes must use the pagin.g device to swap pages in and out as they queue up for the paging device  the ready queue empties as processes wait for the paging device  cpu utilization decreases  the cpu scheduler sees the decreasing cpu utilization and increases the degree of multiprogramming as a result the new process tries to get started by taking frames from running processes  causing more page faults and a longer queue for the paging device as a result  cpu utilization drops even further  and the cpu scheduler tries to increase the degree of multiprogramming even more thrashing has occurred  and system throughput plunges the pagefault rate increases tremendously as a result  the effective m.emory-access time increases no work is getting done  because the processes are spending all their time paging  this phenomenon is illustrated in figure 9.18  in which cpu utilization is plotted against the degree of multiprogramming as the degree of multiprogramming increases  cpu utilization also ilccreases  although more slowly  until a maximum is reached if the degree of multiprogramming is increased even further  thrashing sets in  and cpu utilization drops sharply at this point  to increase cpu utilization and stop thrashing  we must decrease the degree of multiprogramming  we can limit the effects of thrashing by using a  or with local replacement  if one process starts thrashing  it can not frames from another process and cause the latter to thrash as well however  the problem is not entirely solved if processes are 9.6 387 degree of multiprogramming figure 9.18 thrashing  thrashing  they will be in the queue for the paging device most of the time the average service time for a page fault will increase because of the longer average queue for the paging device thus  the effective access time will increase even for a process that is not thrashing  to prevent thtashing  we must provide a process with as many frames as it needs but how do we know how many frames it needs there are several teclmiques the working-set strategy  section 9.6.2  starts by looking at how frames a process is actually using this approach defines the locality of process execution  the locality model states that  as a process executes  it moves from locality to locality a locality is a set of pages that are actively used together  figure 9.19   a program is generally composed of several different localities  which may overlap  for example  when a function is called  it defines a new locality in this locality  memory references are made to the instructions of the function call  its local variables  and a subset of the global variables when we exit the function  the process leaves this locality  since the local variables and instructions of the function are no longer in active use we may return to this locality later  thus  we see that localities are defined by the program structure and its data structures the locality model states that all programs will exhibit this basic memory reference structure note that the locality model is the unstated principle behind the caching discussions so far in this book if accesses to any types of data were random rather than patterned  caching would be useless  suppose we allocate enough frames to a process to accommodate its current locality it will fault for the pages in its locality until all these pages are in memory ; then  it will not fault again until it changes localities if we do not allocate enough frames to accommodate the size of the current locality  the process will thrash  since it can not keep in memory all the pages that it is actively using  9.6.2 working-set model as mentioned  the is based on the assumption of locality  this model uses a paramete1 ~ / '    to define the vrindovv the idea 388 chapter 9 32 ~ ~  ~ ~ = = ~ ~ ~ ~ ~ wl ~ ~ #  ~ ~  ~ ~ ~  \ jjl  jlli111 28  j   j   !  0 0  lj 26 i ' c 0 i e i  lj e execution time   figure 9.19 locality in a memory-reference pattern  is to examine the most recent 6 references the set of pages in the most recent 6 page references is the  figure 9.20   if a page is in active use  it will be in the working set if it is no longer being used  it will drop from the working set 6 time units after its last reference thus  the working set is an approximation of the program 's locality  for example  given the sequence of memory references shown in figure 9.20  if 6 = 10 memory references  then the working set at time t1 is  1  2  5  6  7   by time t2  the working set has changed to  3  4   the accuracy of the working set depends on the selection of 6 if 6 is too small  it will not encompass the entire locality ; if 6 is too large  it may overlap 9.6 page reference table    2 6 1 5 7 7 7 7 5 1 6 2 3 4 1 2 3 4 4 4 3 4 3 4 4 4 1 3 2 3 4 4 4 3 4 4 4  ~ ~ r ~ r t1 ws  t1  =  1 ,2,5,6,7  figure 9.20 working-set model  389 several localities in the extrem.e  if l is infinite  the working set is the set of pages touched during the process execution  the most important property of the working set  then  is its size if we compute the working-set size  wss ;  for each process in the system  we can then consider that where dis the total demand for frames each process is actively using the pages in its working set thus  process i needs wss ; frames if the total demand is greater than the total number of available frames  d m   thrashing will occur  because some processes will not have enough frames  once l has been selected  use of the working-set model is simple the operating system monitors the working set of each process and allocates to that working set enough frames to provide it with its working-set size if there are enough extra frames  another process can be initiated if the sum of the working-set sizes increases  exceeding the total number of available frames  the operating system selects a process to suspend the process 's pages are written out  swapped   and its frames are reallocated to other processes the suspended process can be restarted later  this working-set strategy prevents thrashing while keeping the degree of multiprogramming as high as possible thus  it optimizes cpu utilization  the difficulty with the working-set model is keeping track of the working set the working-set window is a moving window at each memory reference  a new reference appears at one end and the oldest reference drops off the other end a page is in the working set if it is referenced anywhere in the working-set window  we can approximate the working-set model with a fixed-interval timer interrupt and a reference bit for example  assum.e that l equals 10,000 references and that we can cause a timer interrupt every 5,000 references  when we get a timer interrupt  we copy and clear the reference-bit values for each page thus  if a page fault occurs  we can examine the current reference bit and two in-memory bits to determine whether a page was used within the last 10,000 to 15,000 references if it was used  at least one of these bits will be on if it has not been used  these bits will be off those pages with at least one bit on will be considered to be in the working set note that this arrangement is not entirely accurate  because we can not tell where  within an interval of 5,000  a reference occurred we can reduce the uncertainty by increasing the number of history bits and the frequency of interrupts  for example  10 bits and interrupts every 1,000 references   however  the cost to service these more frequent interrupts will be correspondingly higher  390 chapter 9 9.7 number of frames figure 9.21 page-fault frequency  9.6.3 page-fault frequency the working-set model is successful  and knowledge of the working set can be useful for prepaging  section 9.9.1   but it seems a clumsy way to control thrashilcg a strategy that uses the takes a more direct approach  the specific problem is how to prevent thrashilcg thrashing has a high page-fault rate thus  we want to control the page-fault rate when it is too high  we know that the process needs more frames conversely  if the page-fault rate is too low  then the process may have too many frames we can establish upper and lower bounds on the desired page-fault rate  figure 9.21   if the actual page-fault rate exceeds the upper limit  we allocate the process another frame ; if the page-fault rate falls below the lower limit  we remove a frame from the process thus  we can directly measure and control the page-fault rate to prevent thrashing  as with the working-set strategy  we may have to suspend a process if the page-fault rate ilccreases and no free frames are available  we must select some process and suspend it the freed frames are then distributed to processes with high page-fault rates  consider a sequential read of a file on disk using the standard system calls open   ,read    and write    each file access requires a system call and disk access alternatively  we can use the virtual memory techniques discussed so far to treat file i/0 as routine memory accesses this approach  known as a file  allows a part of the virtual address space to be logically associated with the file as we shall see  this can lead to significant performance increases when performing i/0  9.7 391 working sets and page faultrates there is a directrelationship between the working set of a process and its page-fault rate typically as shown in figure 9.20  the working set ofa process changes pver time as references to data and code sections move from one locality to another assuming there is sufficient memory to store the working set of .a process  that is  the processis 11.ot thrashing   tbe page-fault rate of the process will transition between peaks and valleys over time this general behavior is shown in figure 9.22  page fault rate working set time figure 9.22 page fault rate over time  a peak in the page-fault rate occurs when we begin demand-paging a new locality however  once the working set of this new locality is in memory  the page-fault rate falls when the process moves to a new working set  the page  fault rate rises toward a peak once again  returning to a lower rate once the new working set is loaded into memory the span oftime between the start of one peak and the start of thenext peak represents the transition from one working set to another  9.7.1 basic mechanism memory mapping a file is accomplished by mapping a disk block to a page  or pages  in memory initial access to the file proceeds through ordinary demand paging  resulting in a page fault however  a page-sized portion of the file is read from the file system into a physical page  some systems may opt to read in more than a page-sized chunk of memory at a time   subsequent reads and writes to the file are handled as routine memory accesses  thereby simplifying file access and usage by allowing the system to manipulate files through memory rather than incurring the overhead of using the read   and write   system calls similarly  as file l/0 is done in memory as opposed to using system calls that involve disk i/0  file access is much faster as well  note that writes to the file mapped in memory are not necessarily imm.ediate  synchronous  writes to the file on disk some systems may choose to update the physical file when the operating system periodically checks 392 chapter 9 whether the page in memory has been modified when the file is closed  all the memory-mapped data are written back to disk and ren loved from the virtual memory of the process  some operating systems provide memory mapping only through a specific system call and use the standard system calls to perform all other file i/0  however  some systems choose to memory-map a file regardless of whether the file was specified as memory-mapped let 's take solaris as an example if a file is specified as memory-mapped  using the mmap   system call   solaris maps the file into the address space of the process if a file is opened and accessed using ordinary system calls  such as open    read    and write    solaris still memory-maps the file ; however  the file is mapped to the kernel address space regardless of how the file is opened  then  solaris treats all file i/0 as memory-mapped  allowing file access to take place via the efficient memory subsystem  multiple processes may be allowed to map the same file concurrently  to allow sharing of data writes by any of the processes modify the data in virtual memory and can be seen by all others that map the same section of the file given our earlier discussions of virtual memory  it should be clear how the sharing of memory-mapped sections of memory is implemented  the virtual memory map of each sharing process points to the same page of physical memory-the page that holds a copy of the disk block this memory sharing is illustrated in figure 9.23 the memory-mapping system calls can also support copy-on-write functionality  allowing processes to share a file in read-only mode but to have their own copies of any data they modify so that r i i i 1  r   ; i i 1 1 i -1 ii i i i i i j---r ' -rl..-r i i i i -r ' i i i i 1 -1 i i 1 _ i i i i i f +  =   .....c.c ~ ..-'---r ~   i i  .l i j i i i 1 i i i i i i i i i i l_ ~ i process a 1 1 1 virtual memory  ~ 1  disk file figure 9.23 memory-mapped files  process b virtual memory 9.7 memory-mapped file figure 9.24 shared memory in windows using memory-mapped 1/0  393 access to the shared data is coordinated  the processes involved might use one of the mechanisms for achieving mutual exclusion described in chapter 6  in many ways  the sharing of memory-mapped files is similar to shared memory as described in section 3.4.1 not all systems use the same mechanism for both ; on unix and linux systems  for example  memory mapping is accomplished with the mmap   system call  whereas shared memory is achieved with the posix-compliant shmget   and shmat   systems calls  section 3.5.1   on windows nt  2000  and xp systems  howeve1 ~ shared memory is accomplished by memory mapping files on these systems  processes can communicate using shared memory by having the communicating processes memory-map the same file into their virtual address spaces the memorymapped file serves as the region of shared memory between the communicating processes  figure 9.24   in the following section  we illustrate support in the win32 api for shared memory using memory-mapped files  9.7.2 shared memory in the win32 api the general outline for creating a region of shared memory using memorymapped files in the win32 api involves first creating a file mapping for the file to be mapped and then establishing a view of the mapped file in a process 's virtual address space a second process can then open and create a view of the mapped file in its virtual address space the mapped file represents the shared-menwry object that will enable communication to take place between the processes  we next illustrate these steps in more detail in this example  a producer process first creates a shared-memory object using the memory-mapping features available in the win32 api the producer then writes a message to shared m.emory after that  a consumer process opens a mapping to the shared-memory object and reads the message written by the consum.er  to establish a memory-mapped file  a process first opens the file to be mapped with the createfile   function  which returns a handle to the opened file the process then creates a mapping of this file handle using the createfilemapping   function once the file mapping is established  the process then establishes a view of the mapped file in its virtual address space with the mapviewdffile   function the view of the mapped file represents the portion of the file being mapped in the virtual address space of the process 394 chapter 9 # include windows.h # include stdio.h int main  int argc  char argv      handle hfile  hmapfile ; lpvoid lpmapaddress ; hfile = createfile  temp.txt  //file name genericjread i generic_write  // read/write access 0  ii no sharing of the file null  //default security open_always  //open new or existing file file_attribute_normal  //routine file attributes null  ; //no file template hmapfile = createfilemapping  hfile  //file handle null  //default security pagejreadwrite  //read/write access to mapped pages 0  ii map entire file 0  text  sharedobject   ; //named shared memory object lpmapaddress = mapviewdffile  hmapfile  //mapped object handle filejmap_all_access  //read/write access 0  ii mapped view of entire file 0  0  ; ii write to shared memory sprintf  lpmapaddress  shared memory message  ; unmapviewoffile  lpmapaddress  ; closehandle  hfile  ; closehandle  hmapfile  ; figure 9.25 producer writing to shared memory using the win32 api  -the entire file or only a portion of it may be mapped we illustrate this sequence in the program shown in figure 9 .25  we eliminate much of the error checking for code brevity  the call to createfilemapping   creates a named shared-memory object called sharedobj ect the consumer process will communicate using this shared-memory segment by creating a mapping to the same named object  the producer then creates a view of the memory-mapped file in its virtual address space by passing the last three parameters the value 0  it indicates that the mapped view is the entire file it could instead have passed values specifying an offset and size  thus creating a view containing only a subsection of the file  it is important to note that the entire mapping may not be loaded # include windows.h # include stdio.h int main  int argc  char argv     handle hmapfile ; lpvoid lpmapaddress ; 9.7 395 hmapfile = openfilemapping  file_map_all_access  // r/w access false  //no inheritance  text  sharedobject   ; //name of mapped file object lpmapaddress = mapviewoffile  hmapfile  //mapped object handle filejmap_all_access  //read/write access 0  ii mapped view of entire file 0  0  ; ii read from shared memory printf  read message % s  lpmapaddress  ; unmapviewoffile  lpmapaddress  ; closehandle  hmapfile  ; figure 9.26 consumer reading from shared memory using the win32 api  into memory when the mapping is established rather  the mapped file may be demand-paged  thus bringing pages into memory only as they are accessed  the mapviewoffile   fm1ction returns a pointer to the shared-memory object ; any accesses to this memory location are thus accesses to the memory-mapped file in this ii1stance  the producer process writes the message shared memory message to shared memory  a program illustrating how the consumer process establishes a view of the named shared-memory object is shown in figure 9.26 this program is somewhat simpler than the one shown in figure 9.25  as all that is necessary is for the process to create a mapping to the existii1g named shared-memory object the consumer process must also create a view of the mapped file  just as the producer process did ii1 the program in figure 9.25 the consumer then reads from shared memory the message shared memory message thatwas written by the producer process  finally  both processes remove the view of the mapped file with a call to unmapviewoffile    we provide a programming exercise at the end of this chapter using shared memory with memory mapping in the win32 api  9.7.3 memory-mapped i/0 in the case of i/0  as mentioned in section 1.2.1  each i/0 controller includes registers to hold commands and the data being transferred usually  special i/0 instructions allow data transfers between these registers and system memory  396 chapter 9 9.8 to allow more convenient access to i/0 devices1 many computer architectures provide in this case/ ranges of memory addresses are set aside and are mapped to the device registers reads and writes to these memory addresses cause the data to be transferred to and from the device registers this method is appropriate for devices that have fast response times/ such as video controllers in the ibm pc each location on the screen is mapped to a n1.emory location displaying text on the screen is almost as easy as writing the text into the appropriate memory-mapped locations  memory-mapped i/o is also convenient for other devices/ such as the serial and parallel ports used to connect modems and printers to a computer the cpu transfers data through these kinds of devices by reading and writing a few device registers/ called an i/0 to send out a long string of bytes through a memory-mapped serial port1 the cpu writes one data byte to the data register and sets a bit in the control register to signal that the byte is available the device takes the data byte and then clears the bit in the control register to signal that it is ready for the next byte then the cpu can transfer the next byte if the cpu uses polling to watch the control bit/ constantly looping to see whether the device is ready/ this method of operation is called if the cpu does not poll the control bit/ but instead receives an interrupt when the device is ready for the next byte/ the data transfer is said to be when a process running in user rnode requests additional memory/ pages are allocated from the list of free page frames maintained by the kernel  this list is typically populated using a page-replacement algorithm such as those discussed in section 9.4 and most likely contains free pages scattered throughout physical memory/ as explained earlier remember/ too/ that if a user process requests a single byte of memory/ internal fragmentation will result/ as the process will be granted an entire page frame  kernel memory/ however1 is often allocated from a free-memory pool different from the list used to satisfy ordinary user-mode processes there are two primary reasons for this  the kernel requests memory for data structures of varying sizes  some of which are less than a page in size as a result1 the kernel must use memory conservatively and attempt to minimize waste due to fragmentation this is especially important because many operating systems do not subject kernel code or data to the paging system  2 pages allocated to user-mode processes do not necessarily have to be in contiguous physical memory however/ certain hardware devices interact directly with physical memory-without the benefit of a virtual memory interface-and consequently may require memory residing in physically contiguous pages  in the following sections/ we examine two strategies for managing free memory that is assigned to kernel processes  the buddy system and slab allocation  9.8 397 9.8.1 buddy system tbe buddy system allocates memory from a fixed-size segment consisting of physically contiguous pages memory is allocated from this segment using a power-of-2 allocator  which satisfies requests in units sized as a power of 2  4 kb  8 kb  16 kb  and so forth   a request in units not appropriately sized is rounded up to the next highest power of 2 for example  if a request for 11 kb is made  it is satisfied with a 16-kb segment  let 's consider a simple example assume the size of a memory segment is initially 256 kb and the kernel requests 21 kb of memory the segment is initially divided into two buddies-which we will call al and ar -each 128 kb in size one of these buddies is further divided into two 64-kb buddiesbland br however  the next-highest power of 2 from 21 kb is 32 kb so either bt or br is again divided into two 32-kb buddies  cl and cr one of these buddies is used to satisfy the 21-kb request this scheme is illustrated in figure 9.27  where cl is the segment allocated to the 21 kb request  an advantage of the buddy system is how quickly adjacent buddies can be combined to form larger segments using a teclmique known as coalescing in figure 9.27  for example  when the kernel releases the cl unit it was allocated  the system can coalesce c l and c r into a 64-kb segment this segment  b l  can in turn be coalesced with its buddy b r to form a 128-kb segment ultimately  we can end up with the original256-kb segment  the obvious drawback to the buddy system is that rounding up to the next highest power of 2 is very likely to cause fragmentation within allocated segments for example  a 33-kb request can only be satisfied with a 64 kb segment in fact  we can not guarantee that less than 50 percent of the allocated unit will be wasted due to internal fragmentation in the following section  we explore a memory allocation scheme where no space is lost due to fragmentation  physically contiguous pages 256 kb figure 9.27 buddy system allocation  398 chapter 9 9.8.2 slab allocation a second strategy for allocating kernel memory is known as a is made up of one or nwre physically contiguous pages a consists of one or more slabs there is a single cache for each unique kernel data structure -for example  a separate cache for the data structure representing process descriptors  a separate cache for file objects  a separate cache for semaphores  and so forth each cache is populated with that are instantiations of the kernel data structure the cache represents for example  the cache representing semaphores stores instances of semaphore objects  the cache representing process descriptors stores instances of process descriptor objects  and so forth  the relationship between slabs  caches  and objects is shown in figure 9.28  the figure shows two kernel objects 3 kb in size and three objects 7 kb in size  these objects are stored in their respective caches  the slab-allocation algorithm uses caches to store kernel objects when a cache is created  a number of objects-which are initially marked as free-are allocated to the cache the number of objects in the cache depends on the size of the associated slab for example  a 12-kb slab  made up of three continguous 4-kb pages  could store six 2-kb objects initially  all objects in the cache are marked as free when a new object for a kernel data structure is needed  the allocator can assign any free object from the cache to satisfy the request the object assigned from the cache is marked as used  let 's consider a scenario in which the kernel requests memory from the slab allocator for an object representing a process descriptor in linux systems  a process descriptor is of the type struct task_struct  which requires approximately 1.7 kb of memory when the linux kernel creates a new task  it requests the necessary memory for the struct task_struct object from its cache the cache will fulfill the request using a struct task_struct object that has already been allocated in a slab and is marked as free  in linux  a slab may be in one of three possible states  kernel objects slabs 3-kb objects 7-kb objects figure 9.28 slab allocation  physically contiguous pages 9.9 9.9 full all objects in the slab are marked as used  empty all objects in the slab are marked as free  partial the slab consists of both used and free objects  399 the slab allocator first attempts to satisfy the request with a free object in a partial slab if none exist  a free object is assigned from an empty slab if no empty slabs are available  a new slab is allocated from contiguous physical pages and assigned to a cache ; memory for the object is allocated from this slab  the slab allocator provides two main benefits  no memory is wasted due to fragmentation fragn entation is not an issue because each unique kernel data structure has an associated cache  and each cache is made up of one or more slabs that are divided into chunks the size of the objects being represented thus  when the kernel requests memory for an object  the slab allocator returns the exact amount of memory required to represent the object  memory requests can be satisfied quickly the slab allocation scheme is thus particularly effective for mm aging memory when objects are frequently allocated and deallocated  as is often the case with requests from the kernel the act of allocating-and releasing-memory can be a time-consuming process however  objects are created in advance and thus can be quickly allocated from the cache furthermore  when the kernel has finished with an object and releases it  it is marked as free and returned to its cache  thus making it immediately available for subsequent requests fi om the kernel  the slab allocator first appeared in the solaris 2.4 kernel because of its general-purpose nature  this allocator is now also used for certain user-mode memory requests in solaris linux originally used the buddy system ; however  beginning with version 2.2  the linux kernel adopted the slab allocator  the major decisions that we make for a paging system are the selections of a replacement algorithm and an allocation policy  which we discussed earlier in this chapter there are many other considerations as well  and we discuss several of them here  9.9.1 prepaging an obvious property of pure demand paging is the large number of page faults that occur when a process is started this situation results from trying to get the initial locality into memory the same situation may arise at other times for instance  when a swapped-out process is restarted  all its are on the disk  and each must be brought in by its own page fault is an attempt to prevent this high level of initial paging the strategy is to bring into memory at 400 chapter 9 one tin1.e all the pages that will be needed some operating systerns-notably solaris-prepage the page frames for small files  in a system using the working-set model  for example  we keep with each process a list of the pages in its working set if we must suspend a process  due to an i/0 wait or a lack of free frames   we remember the working set for that process when the process is to be resumed  because i/0 has finished or enough free frames have become available   we automatically bring back into memory its entire working set before restarting the process  prepaging may offer an advantage in some cases the question is simply whether the cost of using prepaging is less than the cost of servicing the corresponding page faults it may well be the case that many of the pages brought back into memory by prepaging will not be used  assume that s pages are prepaged and a fraction a of these s pages is actually used  0  '    a  '    1   the question is whether the cost of the s .a saved page faults is greater or less than the cost of prepaging s   1  a  unnecessary pages if a is close to 0  prepaging loses ; if a is close to 1  prepaging wins  9.9.2 page size the designers of an operating system for an existing machine seldom have a choice concerning the page size however  when new machines are being designed  a decision regarding the best page size must be made as you might expect  there is no single best page size rather  there is a set of factors that support various sizes page sizes are invariably powers of 2  generally ranging from 4,096  212  to 4,194,304  222  bytes  how do we select a page size one concern is the size of the page table for a given virtual memory space  decreasing the page size increases the number of pages and hence the size of the page table for a virtual memory of 4 mb  222   for example  there would be 4,096 pages of 1,024 bytes but only 512 pages of 8,192 bytes because each active process must have its own copy of the page table  a large page size is desirable  memory is better utilized with smaller pages  however if a process is allocated memory starting at location 00000 and continuing until it has as much as it needs  it probably will not end exactly on a page boundary thus  a part of the final page must be allocated  because pages are the units of allocation  but will be unused  creating internal fragmentation   assuming independence of process size and page size  we can expect that  on the average  half of the final page of each process will be wasted this loss is only 256 bytes for a page of 512 bytes but is 4,096 bytes for a page of 8,192 bytes to minimize internal fragmentation  then  we need a small page size  another problem is the time required to read or write a page i/0 time is composed of seek  latency  and transfer times transfer time is proportional to the amount transferred  that is  the page size  -a fact that would seem to argue for a small page size howeve1 ~ as we shall see in section 12.1.1  latency and seek time normally dwarf transfer time at a transfer rate of 2 mb per second  it takes only 0.2 milliseconds to transfer 512 bytes latency time  though  is perhaps 8 milliseconds and seek time 20 milliseconds of the total i/0 time  28.2 milliseconds   therefore  only 1 percent is attributable to the actual transfer doubling the page size increases i/0 time to only 28.4 milliseconds it takes 28.4 milliseconds to read a single page of 1,024 bytes but 9.9 401 56.4 milliseconds to read the sam.e amount as two pages of 512 bytes each  thus  a desire to minimize 1/0 time argues for a larger page size  with a smaller page size  though  to tall /0 should be reduced  since locality will be improved a smaller page size allows each page to match program locality more accurately for example  consider a process 200 kb in size  of which only half  100 kb  is actually used in an execution if we have only one large page  we must bring in the entire page  a total of 200 kb transferred and allocated if instead we had pages of only 1 byte  then we could bring in only the 100 kb that are actually used  resulting in only 100 kb transferred and allocated with a smaller page size  we have better allowing us to isolate only the memory that is actually needed with a larger page size  we must allocate and transfer not only what is needed but also anything else that happens to be in the page  whether it is needed or not thus  a smaller page size should result in less i/0 and less total allocated memory  but did you notice that with a page size of 1 byte  we would have a page fault for each byte a process of 200 kb that used only half of that memory would generate only one page fault with a page size of 200 kb but 102,400 page faults with a page size of 1 byte each page fault generates the large amount of overhead needed for processing the interrupt  saving registers  replacing a page  queueing for the paging device  and updating tables to minimize the number of page faults  we need to have a large page size  other factors must be considered as well  such as the relationship between page size and sector size on the paging device   the problem has no best answer as we have seen  some factors  internal fragmentation  locality  argue for a small page size  whereas others  table size  i/0 time  argue for a large page size however  the historical trend is toward larger page sizes indeed  the first edition of operating system concepts  1983  used 4,096 bytes as the upper bound on page sizes  and this value was the most common page size in 1990  modern systems may now use much larger page sizes  as we will see in the following section  9.9.3 tlb reach in chapter 8  we introduced the of the tlb recall that the hit ratio for the tlb refers to the percentage of virtual address translations that are resolved in the tlb rather than the page table clearly  the hit ratio is related to the number of entries in the tlb  and the way to increase the hit ratio is by increasing the number of entries in the tlb this  however  does not come cheaply  as the associative memory used to construct the tlb is both expensive and power hungry  related to the hit ratio is a similar metric  the the tlb reach refers to the amount of memory accessible from the tlb and is simply the number of entries multiplied by the page size ideally  the working set for a process is stored in the tlb if it is not  the process will spend a considerable amount of time resolving memory references in the page table rather than the tlb if we double the number of entries in the tlb  we double the tlb reach however  for some memory-intensive applications  this may still prove insufficient for storing the working set  another approacl1 for increasing the tlb reach is to either increase the size of the page or provide multiple page sizes if we increase the page size-say  402 chapter 9 from 8 kb to 32 kb-we quadruple the tlb reach however  this may lead to an increase in fragmentation for some applications that do not require such a large page size as 32 kb alternatively  an operating system may provide several different page sizes for example  the ultrasparc supports page sizes of 8 kb  64 kb  512 kb  and 4mb of these available pages sizes  solaris uses both 8-kb and 4-mb page sizes and with a 64-entry tlb  the tlb reach for solaris ranges from 512 kb with 8-kb pages to 256mb with 4-mb pages for the majority of applications  the 8-kb page size is sufficient  although solaris maps the first 4 mb of kernel code and data with two 4-mb pages solaris also allows applications-such as databases-to take advantage of the large 4-mb page size  providing support for multiple page sizes requires the operating system -not hardware-to manage the tlb for example  one of the fields in a tlb entry must indicate the size of the page frame corresponding to the tlb entry managing the tlb in software and not hardware comes at a cost in performance howeve1 ~ the increased hit ratio and tlb reach offset the performance costs indeed  recent trends indicate a move toward softwaremanaged tlbs and operating-system support for multiple page sizes the ultrasparc  mips  and alpha architectures employ software-managed tlbs  the powerpc and pentium manage the tlb in hardware  9.9.4 inverted page tables section 8.5.3 introduced the concept of the inverted page table the purpose of this form of page management is to reduce the amount of physical memory needed to track virtual-to-physical address translations we accomplish this savings by creating a table that has one entry per page of physical memory  indexed by the pair process-id  page-number  because they keep information about which virtual memory page is stored in each physical frame  inverted page tables reduce the amount of physical memory needed to store this information however  the inverted page table no longer contains complete information about the logical address space of a process  and that information is required if a referenced page is not currently in memory demand paging requires this information to process page faults  for the information to be available  an external page table  one per process  must be kept each such table looks like the traditional per-process page table and contains information on where each virtual page is located  but do external page tables negate the utility of inverted page tables since these tables are referenced only when a page fault occurs  they do not need to be available quickly instead  they are themselves paged in and out of memory as necessary unfortunately  a page fault may now cause the virtual memory n1.anager to generate another page fault as it pages in the external page table it needs to locate the virtual page on the backing store this special case requires careful handling in the kernel and a delay in the page-lookup processing  9.9.5 program structure demand paging is designed to be transparent to the user program in many cases  the user is completely unaware of the paged nature of memory in other cases  however  system performance can be improved if the user  or compiler  has an awareness of the underlying demand paging  9.9 403 let 's look at a contrived but informative example assume that pages are 128 words in size consider a c program whose function is to initialize to 0 each element of a 128-by-128 array the following code is typical  inti  j ; int  128j  128j data ; for  j = 0 ; j 128 ; j + +  for  i = 0 ; i 128 ; i + +  data  ij  jj = 0 ; notice that the array is stored row major ; that is  the array is stored data  oj  oj  data  oj  1j   data  oj  127j  data  1j  oj  data  1j  1j   data  127j  127j for pages of 128 words  each row takes one page thus  the preceding code zeros one word in each page  then another word in each page  and so on if the operating system allocates fewer than 128 frames to the entire program  then its execution will result in 128 x 128 = 16,384 page faults  in contrast  suppose we change the code to inti  j ; int  128j  128j data ; for  i = 0 ; i 128 ; i + +  for  j = 0 ; j 128 ; j + +  data  ij  jj = 0 ; this code zeros all the words on one page before starting the next page  reducing the number of page faults to 128  careful selection of data structures and programming structures can increase locality and hence lower the page-fault rate and the number of pages in the working set for example  a stack has good locality  since access is always made to the top a hash table  in contrast  is designed to scatter references  producing bad locality of course  locality of reference is just one measure of the efficiency of the use of a data structure other heavily weighted factors include search speed  total number of memory references  and total number of pages touched  at a later stage  the compiler and loader can have a sigicificant effect on paging separating code and data and generating reentrant code means that code pages can be read-only and hence will never be modified clean pages do not have to be paged out to be replaced the loader can avoid placing routines across page boundaries  keeping each routine completely in one page  routines that call each other many times can be packed into the same page  this packaging is a variant of the bin-packing problem of operations research  try to pack the variable-sized load segments into the fixed-sized pages so that interpage references are minimized such an approach is particularly useful for large page sizes  the choice of programming language can affect paging as well for example  c and c + + use pointers frequently  and pointers tend to randomize access to memory  thereby potentially diminishing a process 's locality some studies have shown that object-oriented programs also tend to have a poor locality of reference  404 chapter 9 9.9.6 1/0 interlock when demand paging is used  we sometimes need to allow some of the pages to be in n emory one such situation occurs when i/0 is done to or from user  virtual  memory l/0 is often implemented by a separate i/0 processor  for example  a controller for a usb storage device is generally given the number of bytes to transfer and a memory address for the buffer  figure 9.29   when the transfer is complete  the cpu is interrupted  we must be sure the following sequence of events does not occur  a process issues an i/0 request and is put in a queue for that i/o device meanwhile  the cpu is given to other processes these processes cause page faults ; and one of them  using a global replacement algorithm  replaces the page containing the memory buffer for the waiting process the pages are paged out some time later  when the i/o request advances to the head of the device queue  the i/o occurs to the specified address however  this frame is now being used for a different page belonging to another process  there are two common solutions to this problem one solution is never to execute i/0 to user memory instead  data are always copied between system memory and user memory i/0 takes place only between system memory and the i/0 device to write a block on tape  we first copy the block to system memory and then write it to tape this extra copying may result in unacceptably high overhead  another solution is to allow pages to be locked into memory here  a lock bit is associated with every frame if the frame is locked  it can not be selected for replacement under this approach  to write a block on tape  we lock into memory the pages containing the block the system can then continue as usual locked pages can not be replaced when the i/o is complete  the pages are unlocked  figure 9.29 the reason why frames used for 1/0 must be in memory  9.10 9.10 405 lock bits are used in various situations frequently  some or all of the operating-system kernel is locked into memory  as many operating systems can not tolerate a page fault caused by the kernel  another use for a lock bit involves normal page replacement consider the following sequence of events  a low-priority process faults selecting a replacement frame  the paging system reads the necessary page into memory  ready to continue  the low-priority process enters the ready queue and waits for the cpu since it is a low-priority process  it may not be selected by the cpu scheduler for a time while the low-priority process waits  a high-priority process faults looking for a replacement  the paging system sees a page that is in memory but has not been referenced or modified  it is the page that the low-priority process just brought in this page looks like a perfect replacement  it is clean and will not need to be written out  and it apparently has not been used for a long time  whether the high-priority process should be able to replace the low-priority process is a policy decision after all  we are simply delaying the low-priority process for the benefit of the high-priority process however  we are wasting the effort spent to bring in the page for the low-priority process if we decide to prevent replacement of a newly brought-in page until it can be used at least once  then we can use the lock bit to implement this mechanism when a page is selected for replacement  its lock bit is turned on ; it remains on until the faulting process is again dispatched  using a lock bit can be dangerous  the lock bit may get turned on but never turned off should this situation occur  because of a bug in the operating system  for example   the locked frame becomes unusable on a single-user system  the overuse of locking would hurt only the user doing the locking  multiuser systems must be less trusting of users for instance  solaris allows locking hints  but it is free to disregard these hints if the free-frame pool becomes too small or if an individual process requests that too many pages be locked in memory  in this section  we describe how windows xp and solaris implement virtual memory  9.10.1 windows xp windows xp implements virtual memory using demand paging with clustering handles page faults by bringing in not only the faultil1.g page also several pages following the faulting page when a process is first created  it is assigned a working-set minimum and maximum the is the minimum number of pages the process is guaranteed to in memory if sufficient memory is available  a process may be assigned as many pages as its for most applications  the value of working-set minimum and working-set maximum is 50 and 345 pages  respectively  in some circumstances  a process may be allowed to exceed its working-set maximum  the virtual memory manager maintains a list of free page frames associated with this list is a threshold value that is used to 406 chapter 9 indicate whether sufficient free memory is available if a page fault occurs for a process that is below its working-set maximum  the virtual memory manager allocates a page from this list of free pages if a process that is at its working-set rnaximum incurs a page fault  it must select a page for replacement using a local page-replacement policy  when the amount of free memory falls below the threshold  the virtual memory manager uses a tactic known as to restore the value above the threshold automatic working-set trimming works by evaluating the number of pages allocated to processes if a process has been allocated more pages than its working-set minimum  the virtual memory manager removes pages until the process reaches its working-set minimum a process that is at its working-set minimum may be allocated pages from the free-page-frame list once sufficient free memory is available  the algorithm used to determine which page to remove from a working set depends on the type of processor on single-processor 80x86 systems  windows xp uses a variation of the clock algorithm discussed in section 9.4.5.2 on alpha and multiprocessor x86 systems  clearing the reference bit may require invalidatil g the entry in the translation look-aside buffer on other processors  rather than incurring this overhead  windows xp uses a variation on the fifo algorithm discussed in section 9.4.2  9.10.2 solaris in solaris  when a thread incurs a page fault  the kernel assigns a page to the faulting thread from the list of free pages it maintains therefore  it is imperative that the kernel keep a sufficient amount of free memory available  associated with this list of free pages is a parameter-zotsfree-that represents a threshold to begin paging the lotsfree parameter is typically set to 1/64 the size of the physical memory four times per second  the kernel checks whether the amount of free memory is less than lotsfree if the number of free pages falls below lotsfree  a process known as a pageout starts up the pageout process is similar to the second-chance algorithm described in section 9.4.5.2  except that it uses two hands while scanning pages  rather than one the pageout process works as follows  the front hand of the clock scans all pages in memory  setting the reference bit to 0 later  the back hand of the clock examines the reference bit for the pages in memory  appending each page whose reference bit is still set to 0 to the free list and writing to disk its contents if modified solaris maintains a cache list of pages that have been freed but have not yet been overwritten  the free list contains frames that have invalid contents pages can be reclaimed from the cache list if they are accessed before being moved to the free list  the pageout algorithm uses several parameters to control the rate at which pages are scam ed  known as the scanrate   the scanrate is expressed in pages per second and ranges from slowscan to fastscan when free memory falls below lotsfree  scanning occurs at slowscan pages per second and progresses to fastscan  depending on the amount of free memory available the default value of slowscan is 100 pages per second ; fasts can is typically set to the value  total physical pages  /2 pages per second  with a maximum of 8,192 pages per second this is shown in figure 9.30  withfastscan set to the maximum   the distance  in pages  between the hands of the clock is determil ed by a system parameter  handspread the amount of time between the front hand 's 9.11 8192 fastscan cll 7 c  1j u en 100 slowscan minfree desfree amount of free memory figure 9.30 solaris page scanner  9.11 407 lotsfree clearing a bit and the back hand 's investigating its value depends on the scanrate and the handspread if scam-ate is 100 pages per second and handspread is 1,024 pages  10 seconds can pass between the time a bit is set by the front hand and the time it is checked by the back hand however  because of the demands placed on the memory system  a scanrate of several thousand is not uncommon  this means that the amount of time between clearing and investigating a bit is often a few seconds  as mentioned above  the pageout process checks memory four times per second however  if free memory falls below desfree  figure 9.30   pageout will nm 100 times per second with the intention of keeping at least desfree free memory available if the pageout process is unable to keep the amount of free memory at desfree for a 30-second average  the kernel begins swapping processes  thereby freeing all pages allocated to swapped processes in general  the kernel looks for processes that have been idle for long periods of time if the system is unable to maintain the amount of free memory at minfree  the pageout process is called for every request for a new page  recent releases of the solaris kernel have provided enhancements of the paging algorithm one such enhancement involves recognizing pages from shared libraries pages belonging to libraries that are being shared by several processes-even if they are eligible to be claimed by the scannerare skipped during the page-scanning process another enhancement concerns distinguishing pages that have been allocated to processes from pages allocated to regularfiles this is known as and is covered in section 11.6.2  it is desirable to be able to execute a process whose logical address space is larger than the available physical address space virtual memory is a technique 408 chapter 9 that enables us to map a large logical address space onto a smaller physical menlory virtual memory allows us to run extremely large processes and to raise the degree of multiprogramming  increasing cpu utilization further  it frees application programmers from worrying about memory availability in addition  with virtual memory  several processes can share system libraries and memory virtual memory also enables us to use an efficient type of process creation known as copy-on-write  wherein parent and child processes share actual pages of memory  virtual memory is commonly implemented by demand paging pure demand paging never brings in a page until that page is referenced the first reference causes a page fault to the operating system the operating-system kernel consults an internal table to determine where the page is located on the backing store it then finds a free frame and reads the page in from the backing store the page table is updated to reflect this change  and the instruction that caused the page fault is restarted this approach allows a process to run even though its entire memory image is not in main memory at once as long as the page-fault rate is reasonably low  performance is acceptable  we can use demand paging to reduce the number of frames allocated to a process this arrangement can increase the degree of multiprogramming  allowing more processes to be available for execution at one time  and-in theory  at least-the cpu utilization of the system it also allows processes to be run even though their memory requirements exceed the total available physical memory such processes run in virtual memory  if total memory requirements exceed the capacity of physical memory  then it may be necessary to replace pages from memory to free frames for new pages various page-replacement algorithms are used fifo page replacement is easy to program but suffers from belady 's anomaly optimal page replacement requires future knowledge lru replacement is an approximation of optimal page replacement  but even it may be difficult to implement  most page-replacement algorithms  such as the second-chance algorithm  are approximations of lru replacement  in addition to a page-replacement algorithm  a frame-allocation policy is needed allocation can be fixed  suggesting local page replacement  or dynamic  suggesting global replacement the working-set model assumes that processes execute in localities the working set is the set of pages in the current locality accordingly  each process should be allocated enough frames for its current working set if a process does not have enough memory for its working set  it will thrash providing enough frames to each process to avoid thrashing may require process swapping and schedulil g  most operating systems provide features for memory mappil1g files  thus allowing file i/0 to be treated as routine memory access the win32 api implements shared memory through memory mappil1g files  kernel processes typically req1.1ire memory to be allocated using pages that are physically contiguous the buddy system allocates memory to kernel processes in units sized according to a power of 2  which often results in fragmentation slab allocators assign kernel data structures to caches associated with slabs  which are made up of one or more physically contiguous pages  with slab allocation  no memory is wasted due to fragmentation  and memory requests can be satisfied quickly  409 in addition to reqmnng that we solve the major problems of page replacement and frame allocation  the proper design of a paging systern requires that we consider prep aging  page size  tlb reach  inverted page tables  program structure  i/0 interlock  and other issues  9.1 assume there is a 1,024-kb segment where memory is allocated using the buddy system using figure 9.27 as a guide  draw a tree illustrating how the following memory requests are allocated  request 240 bytes request 120 bytes request 60 bytes request 130 bytes next modify the tree for the followilcg releases of memory perform coalescing whenever possible  release 240 bytes release 60 bytes release 120 bytes 9.2 consider the page table for a system with 12-bit virtual and physical addresses with 256-byte pages the list of free page frames is d  e  f  that is  dis at the head of the list e is second  and f is last   410 chapter 9 convert the following virtual addresses to their equivalent physical addresses in hexadecimal all numbers are given in hexadecimal  a dash for a page frame indicates that the page is not in memory  9ef 111 700 off 9.3 a page-replacement algorithm should minimize the number of page faults we can achieve this minimization by distributing heavily used pages evenly over all of memory  rather than having them compete for a small number of page frames we can associate with each page frame a counter of the number of pages associated with that frame then  to replace a page  we can search for the page frame with the smallest counter  a define a page-replacement algorithm using this basic idea specifically address these problems  i what is the initial value of the counters ii when are counters increased iii when are counters decreased 1v how is the page to be replaced selected b how many page faults occur for your algorithm for the following reference string with four page frames 1  2  3  4  5  3  4  1  6  7  8  7  8  9  7  8  9  5  4  5  4  2  c what is the minimum number of page faults for an optimal pagereplacement strategy for the reference string in part b with four page frames 9.4 consider a demand-paging system with the following time-measured utilizations  cpu utilization paging disk other i/0 devices 20 % 97.7 % 5 % for each of the following  say whether it will  or is likely to  improve cpu utilization explain your answers  a install a faster cpu  b install a bigger paging disk  c increase the degree of multiprogramming  d decrease the degree of multiprogramming  411 e install more main n1.enl0ry  f install a faster hard disk or multiple controllers with multiple hard disks  g add prepaging to the page-fetch algorithms  h increase the page size  9.5 consider a demand-paged computer system where the degree of multiprogramming is currently fixed at four the system was recently measured to determine utilization of the cpu and the paging disk  the results are one of the following alternatives for each case  what is happening can the degree of multiprogramming be increased to increase the cpu utilization is the paging helping a cpu utilization 13 percent ; disk utilization 97 percent b cpu utilization 87 percent ; disk utilization 3 percent c cpu utilization 13 percent ; disk utilization 3 percent 9.6 consider a demand-paging system with a paging disk that has an average access and transfer time of 20 milliseconds addresses are translated through a page table in main memory  with an access time of 1 microsecond per memory access thus  each memory reference through the page table takes two accesses to improve this time  we have added an associative memory that reduces access time to one memory reference if the page-table entry is in the associative memory  assume that 80 percent of the accesses are in the associative memory and that  of those remaining  10 percent  or 2 percent of the total  cause page faults what is the effective memory access time 9.7 a simplified view of thread states is ready  running  and blocked  where a thread is either ready and waiting to be scheduled  is running on the processor  or is blocked  i.e is waiting for i/0  this is illustrated in figure 9.31 assuming a thread is in the running state  answer the following questions   be sure to explain your answer  a will the thread change state if it incurs a page fault if so  to what new state figure 9.31 thread state diagram for exercise 9.7  412 chapter 9 b will the thread change state if it generates a tlb miss that is resolved in the page table if so  to what new state c will the thread change state if an address reference is resolved in the page table if so  to what new state 9.8 discuss the hardware support required to support demand paging  9.9 consider the following page reference string  1  2  3  4  2  1  5  6  2  1  2  3  7  6  3  2  1  2  3  6  how many page faults would occur for the following replacement algorithms  assuming one  two  three  four  five  six  and seven frames remember that all frames are initially empty  so your first unique pages will cost one fault each  lru replacement fifo replacement optimal replacement 9.10 consider a system that allocates pages of different sizes to its processes  what are the advantages of such a paging scheme what modifications to the virtual memory system provide this functionality 9.11 discuss situations in which the most frequently used page-replacement algorithm generates fewer page faults than the least recently used page-replacement algorithm also discuss under what circumstances the opposite holds  9.12 under what circumstances do page faults occur describe the actions taken by the operating system when a page fault occurs  9.13 suppose that a machine provides instructions that can access memory locations using the one-level indirect addressing scheme what sequence of page faults is ilccurred when all of the pages of a program are currently nonresident and the first instruction of the program is an indirect memory-load operation what happens when the operating system is using a per-process frame allocation technique and only two pages are allocated to this process 9.14 consider a system that provides support for user-level and kernellevel threads the mapping in this system is one to one  there is a corresponding kernel thread for each user thread   does a multithreaded process consist of  a  a working set for the entire process or  b  a working set for each thread explain  413 9.15 what is the copy-on-write feature  and under what circumstances is it beneficial to use this feature what hardware support is required to implement this feature 9.16 consider the two-dimensional array a  int a     = new int  100   100  ; where a  oj  oj is at location 200 in a paged memory system with pages of size 200 a small process that manipulates the matrix resides in page 0  locations 0 to 199   thus  every instruction fetch will be from page 0  for three page frames  how many page faults are generated by the following array-initialization loops  using lru replacement and assuming that page frame 1 contains the process and the other two are initially empty a for  int j = 0 ; j 100 ; j + +  for  int i = 0 ; i 100 ; i + +  a  i   j  = 0 ; b for  int i = 0 ; i 100 ; i + +  for  int j = 0 ; j 100 ; j + +  a  i   j  = 0 ; 9.17 discuss situations in which the least frequently used page-replacement algorithm generates fewer page faults than the least recently used page-replacement algorithm also discuss under what circumstances the opposite holds  9.18 what is the cause of thrashing how does the system detect thrashing once it detects thrashing  what can the system do to eliminate this problem 9.19 assume that you are monitoring the rate at which the pointer in the clock algorithm  which indicates the candidate page for replacement  moves what can you say about the system if you notice the following behavior  a pointer is moving fast  b pointer is moving slow  9.20 the vax/vms system uses a fifo replacement algorithm for resident pages and a free-frame pool of recently used pages assume that the free-frame pool is managed using the least recently used replacement policy answer the following questions  a if a page fault occurs and if the page does not exist in the free-frame pool  how is free space generated for the newly requested page 414 chapter 9 b if a page fault occurs and if the page exists in the free-frame pool  how is the resident page set and the free-france pool managed to make space for the requested page c what does the system degenerate to if the number of resident pages is set to one d what does the system degenerate to if the number of pages in the free-frame pool is zero 9.21 the slab-allocation algorithm uses a separate cache for each different object type assuming there is one cache per object type  explain why this scheme does n't scale well with multiple cpus what could be done to address this scalability issue 9.22 assume that we have a demand-paged memory the page table is held in registers it takes 8 milliseconds to service a page fault if an empty frame is available or if the replaced page is not modified and 20 milliseconds if the replaced page is modified memory-access time is 100 nanoseconds  assume that the page to be replaced is modified 70 percent of the time what is the maximum acceptable page-fault rate for an effective access time of no more than 200 nanoseconds 9.23 segmentation is similar to paging but uses variable-sized pages define two segment-replacement algorithms based on fifo and lru pagereplacement schemes remember that since segments are not the same size  the segment that is chosen to be replaced may not be big enough to leave enough consecutive locations for the needed segment consider strategies for systems where segments cam ot be relocated and strategies for systems where they can  9.24 which of the following programming techniques and structures are good for a demand-paged environment which are not good explain your answers  a stack b hashed symbol table c sequential search d binary search e pure code f vector operations a indirection b 9.25 when a page fault occurs  the process requesting the page must block while waiting for the page to be brought from disk into physical memory  assume that there exists a process with five user-level threads and that the mapping of user threads to kernel threads is many to one if one user thread incurs a page fault while accessing its stack  would the other user user threads belonging to the same process also be affected by the page fault-that is  would they also have to wait for the faulting page to be brought into memory explain  415 9.26 consider a system that uses pure demand paging  a when a process first starts execution  how would you characterize the page fault rate b once the working set for a process is loaded into memory  how would you characterize the page fault rate c assume that a process changes its locality and the size of the new working set is too large to be stored in available free memory  identify some options system designers could choose from to handle this situation  9.27 assume that a program has just referenced an address in virtual memory  describe a scenario in which each of the following can occur  if no such scenario can occur  explain why  tlb miss with no page fault tlb miss and page fault tlb hit and no page fault tlb hit and page fault 9.28 a certain computer provides its users with a virtual memory space of 232 bytes the computer has 218 bytes of physical memory the virtual memory is implemented by paging  and the page size is 4,096 bytes  a user process generates the virtual address 11123456 explain how the system establishes the corresponding physical location distinguish between software and hardware operations  9.29 when virtual memory is implemented in a computing system  there are certain costs associated with the technique and certain benefits list the costs and the benefits is it possible for the costs to exceed the benefits if it is  what measures can be taken to ensure that this does not happen 9.30 give an example that illustrates the problem with restarting the move character instruction  mvc  on the ibm 360/370 when the source and destination regions are overlapping  9.31 consider the parameter 6 used to define the working-set window in the working-set model what is the effect of setting 6 to a small value on the page-fault frequency and the number of active  nonsuspended  processes currently executing in the system what is the effect when 6  is set to a very high value 9.32 is it possible for a process to have two working sets  one representing data and another representing code explain  9.33 suppose that your replacement policy  in a paged system  is to examine each page regularly and to discard that page if it has not been used since the last examination what would you gain and what would you lose by using this policy rather than lru or second-chance replacement 416 chapter 9 9.34 write a program that implements the fifo and lru page-replacement algorithms presented in this chapter first  generate a random pagereference string where page numbers range from 0 to 9 apply the random page-reference string to each algorithm  and record the number of page faults incurred by each algorithm implement the replacement algorithms so that the number of page frames can vary from 1 to 7  assume that demand paging is used  9.35 the catalan numbers are an integer sequence c11 that appear in treeenumeration problems the first catalan numbers for n = 1  2  3   are 1  2  5  14  42  132   a formula generating c11 is 1  2n   2n  ! ell =  n + 1   ; ; =  n + 1  ! n ! design two programs that communicate with shared memory using the win32 api as outlined in section 9.7.2 the producer process will generate the catalan sequence and write it to a shared memory object  the consumer process will then read and output the sequence from shared memory  in this instance  the producer process will be passed an integer parameter on the command line specifying how many catalan numbers to produce  for example  providing 5 on the command line means the producer process will generate the first five catalan numbers   demand paging was first used iil the atlas system  implemented on the manchester university muse computer around 1960  kilburn et al  1961    another early demand-paging system was multics  implemented on the ge 645 system  organick  1972    belady et al  1969  were the first researchers to observe that the fifo replacement strategy may produce the anomaly that bears belady 's name  mattson et al  1970  demonstrated that stack algorithms are not subject to belady 's anomaly  the optimal replacement algorithm was presented by belady  1966  and was proved to be optimal by mattson et al  1970   belady ' s optimal algorithm is for a fixed allocation ; prieve and fabry  1976  presented an optimal algorithm for situations in which the allocation can vary  the enl lanced clock algorithm was discussed by carr and hennessy  1981   the working-set model was developed by denning  1968   discussions concerning the working-set model were presented by denning  1980   the scheme for monitoring the page-fault rate was developed by wulf  1969   who successfully applied this technique to the burroughs bssoo computer system  wilson et al  1995  presented several algoritluns for dynamic memory allocation  jolmstone and wilson  1998  described various memory-fragmentation 417 issues buddy system memory allocators were described in knowlton  1965l peterson and norman  1977   and purdom  jr and stigler  1970   bonwick  1994  discussed the slab allocator  and bonwick and adams  2001  extended the discussion to multiple processors other memory-fitting algorithms can be found in stephenson  1983   bays  1977   and brent  1989   a survey of memory-allocation strategies can be found in wilson et al  1995   solomon and russinovich  2000  and russinovich and solomon  2005  described how windows implements virtual memory mcdougall and mauro  2007  discussed virtual memory in solaris virtual memory techniques in linux and bsd were described by bovet and cesati  2002  and mckusick et al  1996   respectively ganapathy and schimmel  1998  and navarro et al   2002  discussed operating system support for multiple page sizes ortiz  2001  described virtual memory used in a real-time embedded operating system  jacob and mudge  1998b  compared implementations of virtual memory in the mips  powerpc  and pentium architectures a companion article  jacob and mudge  1998a   described the hardware support necessary for implementation of virtual memory in six different architectures  including the ultrasparc  part five since main memory is usually too small to accommodate all the data and programs permanently  the computer system must provide secondary storage to back up main memory modern computer systems use disks as the primary on-line storage medium for information  both programs and data   the file system provides the mechanism for on-line storage of and access to both data and programs residing on the disks a file is a collection of related information defined by its creator the files are mapped by the operating system onto physical devices files are normally organized into directories for ease of use  the devices that attach to a computer vary in many aspects some devices transfer a character or a block of characters at a time some can be accessed only sequentially  others randomly some transfer data synchronously  others asynchronously some are dedicated  some shared they can be read-only or read-write they vary greatly in speed  in many ways  they are also the slowest major component of the computer  because of all this device variation  the operating system needs to provide a wide range of functionality to applications  to allow them to control all aspects of the devices one key goal of an operating system 's 1/0 subsystem is to provide the simplest interface possible to the rest of the system because devices are a performance bottleneck  another key is to optimize 1/0 for maximum concurrency  10.1 r for most users  the file system is the most visible aspect of an operating system  it provides the mechanism for on-line storage of and access to both data and programs of the operating system and all the users of the computer system the file system consists of two distinct parts  a collection of files  each storing related data  and a directory structure  which organizes and provides information about all the files in the system file systems live on devices  which we explore fully irl the following chapters but touch upon here in this chapter  we consider the various aspects of files and the major directory structures we also discuss the semantics of sharing files among multiple processes  users  and computers  finally  we discuss ways to handle file protection  necessary when we have multiple users and we want to control who may access files and how files may be accessed  to explain the function of file systems  to describe the interfaces to file systems  to discuss file-system design tradeoffs  including access methods  file sharing  file locking  and directory structures  to explore file-system protection  computers can store information on various storage media  such as magnetic disks  magnetic tapes  and optical disks so that the computer system will be convenient to use  the operating system provides a uniform logical view of information storage the operating system abstracts from the physical properties of its storage devices to define a logical storage unit  the file files are mapped by the operating system onto physical devices these storage devices are usually nonvolatile  so the contents are persistent through power failures and system reboots  421 422 chapter 10 a file is a named collection of related information that is recorded on secondary storage from a user 's perspective  a file is the smallest allotment of logical secondary storage ; that is  data can not be written to secondary storage unless they are within a file commonly  files represent programs  both source and object forms  and data data files may be numeric  alphabetic  alphanumeric  or binary files may be free form  such as text files  or may be formatted rigidly in general  a file is a sequence of bits  bytes  lines  or records  the meaning of which is defined by the file 's creator and user the concept of a file is thus extremely general the information in a file is defined by its creator many different types of information may be stored in a file-source programs  object programs  executable programs  numeric data  text  payroll records  graphic images  sound recordings  and so on a file has a certain defined which depends on its type a text file is a sequence of characters organized into lines  and possibly pages   a source file is a sequence of subroutines and functions  each of which is further organized as declarations followed by executable statements an object file is a sequence of bytes organized in.to blocks nnderstandable by the system 's linker an executable file is a series of code sections that the loader can bring into memory and execute  10.1.1 file attributes a file is named  for the convenience of its human users  and is referred to by its name a name is usually a string of characters  such as example.c some systems differentiate between uppercase and lowercase characters in names  whereas other systems do not when a file is named  it becomes independent of the process  the user  and even the system that created it for instance  one user might create the file example.c  and another user might edit that file by specifying its name the file 's owner might write the file to a floppy disk  send it in an e-mail  or copy it across a network  and it could still be called example.c on the destination system  a file 's attributes vary from one operating system to another but typically consist of these  name the symbolic file name is the only information kept in humanreadable form  identifier this unique tag  usually a number  identifies the file within the file system ; it is the non-human-readable name for the file  type this information is needed for systems that support different types of files  location this information is a pointer to a device and to the location of the file on that device  size the current size of the file  in bytes  words  or blocks  and possibly the maximum allowed size are included in this attribute  protection access-control information determines who can do reading  writing  executing  and so on  10.1 423 time  date  and user identification this information may be kept for creation  last modification  and last use these data can be useful for protection  security  and usage monitoring  the information about all files is kept in the directory structure  which also resides on secondary storage typically  a directory entry consists of the file 's name and its unique identifier the identifier in turn locates the other file attributes it may take more than a kilobyte to record this information for each file in a system with many files  the size of the directory itself may be megabytes because directories  like files  must be nonvolatile  they must be stored on the device and brought into memory piecemeal  as needed  10.1.2 file operations a file is an to define a file properly  we need to consider the operations that can be performed on files the operating system can provide system calls to create  write  read  reposition  delete  and truncate files let 's examine what the operating system must do to perform each of these six basic file operations it should then be easy to see how other similar operations  such as renaming a file  can be implemented  creating a file two steps are necessary to create a file first  space in the file system must be found for the file we discuss how to allocate space for the file in chapter 11 second  an entry for the new file must be made in the directory  writing a file to write a file  we make a system call specifying both the name of the file and the information to be written to the file given the name of the file  the system searches the directory to find the file 's location  the system must keep a write pointer to the location in the file where the next write is to take place the write pointer must be updated whenever a write occurs  reading a file to read from a file  we use a system call that specifies the name of the file and where  in memory  the next block of the file should be put again  the directory is searched for the associated entry  and the system needs to keep a read pointer to the location in the file where the next read is to take place once the read has taken place  the read pointer is updated because a process is usually either reading from or writing to a file  the current operation location can be kept as a per-process  both the read and write operations use this same pointer  saving space and reducing system complexity  repositioning within a file the directory is searched for the appropriate entry  and the current-file-position pointer is repositioned to a given value  repositioning within a file need not involve any actual i/0 this file operation is also kn.own as a file seek  deleting a file to delete a file  we search the directory for the named file  having found the associated directory entry  we release all file space  so that it can be reused by other files  and erase the directory entry  424 chapter 10 truncating a file the user may want to erase the contents of a file but keep its attributes rather than forcing the user to delete the file and then recreate it  this function allows all attributes to remain unchanged -except for file length-but lets the file be reset to length zero and its file space released  these six basic operations comprise the minimal set of required file operations other common operations include appending new information to the end of an existing file and renaming an existing file these primitive operations can then be combined to perform other file operations for instance  we can create a copy of a file  or copy the file to another i/o device  such as a printer or a display  by creating a new file and then reading from the old and writing to the new we also want to have operations that allow a user to get and set the various attributes of a file for example  we may want to have operations that allow a user to determine the status of a file  such as the file 's length  and to set file attributes  such as the file 's owner  most of the file operations mentioned involve searching the directory for the entry associated with the named file to avoid this constant searching  many systems require that an open   system call be made before a file is first used actively the operating system keeps a small table  called the containing information about all open files when a file operation is requested  the file is specified via an index into this table  so no searching is required  when the file is no longer being actively used  it is closed by the process  and the operating system removes its entry from the open-file table create and delete are system calls that work with closed rather than open files  some systems implicitly open a file when the first reference to it is made  the file is automatically closed when the job or program that opened the file terminates most systems  however  require that the programmer open a file explicitly with the open   system call before that file can be used the open   operation takes a file name and searches the directory  copying the directory entry into the open-file table the open   call can also accept accessmode information-create  read-only  read-write  append-only  and so on  this mode is checked against the file 's permissions if the request mode is allowed  the file is opened for the process the open   system call typically returns a pointer to the entry in the open-file table this pointer  not the actual file name  is used in all i/0 operations  avoiding any further searching and simplifying the system-call interface  the implementation of the open   and close   operations is more complicated in an environment where several processes may open the file simultaneously this may occur in a system ~ where several different applications open the same file at the same time typically  the operating system uses two levels of internal tables  a per-process table and a system-wide table the perprocess table tracks all files that a process has open stored in this table is information regarding the use of the file by the process for instance  the current file pointer for each file is found here access rights to the file and accounting information can also be included  each entry in the per-process table in turn points to a system-wide open-file table the system-wide table contains process-independent information  such as the location of the file on disk  access dates  and file size once a file has been opened by one process  the system-wide table includes an entry for the file  10.1 425 when another process executes an open   calt a new entry is simply added to the process 's open-file table pointing to the appropriate entry in the systemwide table typically  the open-file table also has an open count associated with each file to indicate how ncany processes have the file open each close   decreases this open count  and when the open count reaches zero  the file is no longer in use  and the file 's entry is removed from the open-file table  in summary  several pieces of information are associated with an open file  file pointer on systems that do not include a file offset as part of the read   and write   system calls  the systein must track the last readwrite location as a current-file-position pointer this pointer is unique to each process operating on the file and therefore must be kept separate from the on-disk file attributes  file-open count as files are closed  the operating system must reuse its open-file table entries  or it could run out of space in the table because multiple processes may have opened a file  the system must wait for the last file to close before removing the open-file table entry the file-open counter tracks the number of opens and closes and reaches zero on the last close the system can then remove the entry  disk location of the file most file operations require the system to modify data within the file the information needed to locate the file on disk is kept in memory so that the system does not have to read it from disk for each operation  access rights each process opens a file in an access mode this information is stored on the per-process table so the operating system can allow or deny subsequent i/0 requests  some operating systems provide facilities for locking an open file  or sections of a file   file locks allow one process to lock a file and prevent other processes from gaining access to it file locks are useful for files that are shared by several processes-for example  a system log file that can be accessed and modified by a number of processes in the system  file locking in java in the java api  acquiring a lock requires firstobtaini  ng the f  i..lechannel fbr thefile to be locked the loc ; k   method of the filechannel is used to acquir  o the lock the api of the lock   method is filelock lock  l.ong begin  long end  l ; ooleqn shared  where begin and end are the h  ~ gi1iningand ending positions of the region being locked settingshared to true isfb ~ shared locks ; setting shared to false acquires the lock exclusively tice lock is released by invoking the release   of the filelock returned by the lock   operati n  the program in figure 10.1 illusttates file locking in java  this program acquires two locks on thefilefile  txt the first half of.the file is acquired as an exclusive lock ~ the lock for the second half is a shared lock  426 chapter 10 file locks provide functionality similar to reader-writer locks  covered in section 6.6.2 a shared lock is akin to a reader lock in that several processes can acquire the lock concurrently an exclusive lock behaves like a writer lock ; only one process at a time can acquire such a lock it is important to note 10.1 427 that not au operating systems provide both types of locks ; some systems only provide exclusive file locking  furthermore  operating systems may provide either mandatory or advisory file-locking mechanisms if a lock is n1.andatory  then once a process acquires an exclusive lock  the operating system will prevent any other process from accessing the locked file for example  assume a process acquires an exclusive lock on the file system .log if we attempt to open system .log from another process-for example  a text editor-the operating system will prevent access until the exclusive lock is released this occurs even if the text editor is not written explicitly to acquire the lock alternatively  if the lock is advisory  then the operating system will not prevent the text editor from acquiring access to system .log rather  the text editor must be written so that it manually acquires the lock before accessing the file in other words  if the locking scheme is mandatory  the operating system ensures locking integrity  for advisory locking  it is up to software developers to ensure that locks are appropriately acquired and released as a general rule  windows operating systems adopt mandatory locking  and unix systems employ advisory locks  the use of file locks requires the same precautions as ordinary process synchronization for example  programmers developing on systems with mandatory locking must be careful to hold exclusive file locks only while they are accessing the file ; otherwise  they will prevent other processes from accessing the file as well furthermore  some measures must be taken to ensure that two or more processes do not become involved in a deadlock while trying to acquire file locks  10.1.3 file types when we design a file system-indeed  an entire operating system-we always consider whether the operating system should recognize and support file types if an operating system recognizes the type of a file  it can then operate on the file in reasonable ways for example  a common mistake occurs when a user tries to print the binary-object form of a program this attempt normally produces garbage ; however  the attempt can succeed if the operating system has been told that the file is a binary-object program  a common technique for implementing file types is to include the type as part of the file name the name is split into two parts-a name and an extension  usually separated by a period character  figure 10.2   in this way  the user and the operating system can tell from the name alone what the type of a file is  for example  most operating systems allow users to specify a file name as a sequence of characters followed by a period and terminated by an extension of additional characters file name examples include resume.doc  server.java  and readerthread c  the system uses the extension to indicate the type of the file and the type of operations that can be done on that file only a file with a .com  .exe  or .bat extension can be executed  for instance the .com and .exe files are two forms of binary executable files  whereas a .bat file is a containing  in ascii format  commands to the operating system ms-dos recognizes only a few extensions  but application programs also use extensions to indicate file types in which they are interested for example  assemblers expect source files to have an .asm extension  and the microsoft word word processor expects its files to 428 chapter 10 !   isnl ~ 1  f '  ~   j \  ir ~ i  tji ~  ~   '' ' r  ~ ~  r    ~ ; ,'u  ~ rt ~ tt ~ ~ ~   ~ \        ' ' ~   '   c executable exe  com  bin ready ~ to-run machineor none language program object obj  o compiled  machine language  not linked source code c  cc  java  pas  source code in various asm  a languages batch bat  sh commands to the command interpreter text txt  doc textual data  documents wo rdprocessor wp,tex  rtf  various wordcprocessor doc formats library lib  a  so  dll libraries o.troutines for .programmers print or view ps  pdf  jpg ascii or binary file in a format for printing or viewing archive arc  zip  .tar 1 related files grouped into .one file,sometimes compressed  for archiving or storage multimedia mpeg  mov  rm  binary file containing mp3  avi audio or a/v information figure 10.2 common file types  end with a .doc extension these extensions are not required  so a user may specify a file without the extension  to save typing   and the application will look for a file with the given name and the extension it expects because these extensions are not supported by the operating system  they can be considered as hints to the applications that operate on them  another example of the utility of file types comes from the tops-20 operating system if the user tries to execute an object program whose source file has been modified  or edited  since the object file was produced  the source file will be recompiled automatically this function ensures that the user always runs an up-to-date object file otherwise  the user could waste a significant amount of time executing the old object file for this function to be possible  the operating system must be able to discriminate the source file from the object file  to check the time that each file was created or last modified  and to determine the language of the source program  in order to use the correct compiler   consider  too  the mac os x operating system in this system  each file has a type  such as text  for text file  or appl  for application   each file also has a creator attribute containing the name of the program that created it this attribute is set by the operating system during the create   call  so its use is enforced and supported by the system for instance  a file produced by a word processor has the word processor 's name as its creator when the user opens that file  by double-clicking the mouse on the icon representing the file  10.1 429 the word processor is invoked automatically  and the file is loaded  ready to be edited  the unix system uses a crude stored at the beginning of some files to indicate roughly the type of the file-executable program  batch file  or postscript file  and so on not all files have magic numbers  so system features can not be based solely on this information unix does not record the name of the creating program  either unix does allow file-nameextension hints  but these extensions are neither enforced nor depended on by the operating system ; they are meant mostly to aid users in determining what type of contents the file contains extensions can be used or ignored by a given application  but that is up to the application 's programmer  10.1.4 file structure file types also can be used to indicate the internal structure of the file as mentioned in section 10.1.3  source and object files have structures that match the expectations of the programs that read them further  certain files must conform to a required structure that is understood by the operating system for example  the operating system requires that an executable file have a specific structure so that it can determine where in memory to load the file and what the location of the first instruction is some operating systems extend this idea into a set of system-supported file structures  with sets of special operations for manipulating files with those structures for instance  dec 's vms operating system has a file system that supports three defined file structures  this point brings us to one of the disadvantages of having the operating system support multiple file structures  the resulting size of the operating system is cumbersome if the operating system defines five different file structures  it needs to contain the code to support these file structures  in addition  it may be necessary to define every file as one of the file types supported by the operating system when new applications require information structured in ways not supported by the operating system  severe problems may result  for example  assume that a system supports two types of files  text files  composed of ascii characters separated by a carriage return and line feed  and executable binary files now  if we  as users  want to define an encrypted file to protect the contents from being read by unauthorized people  we may find neither file type to be appropriate the encrypted file is not ascii text lines but rather is  apparently  random bits although it may appear to be a binary file  it is not executable as a result  we may have to circumvent or misuse the operating system 's file-type mechanism or abandon our encryption scheme  some operating systems impose  and support  a minimal number of file structures this approach has been adopted in unix  ms-dos  and others un1x considers each file to be a sequence of 8-bit bytes ; no interpretation of these bits is made by the operating systen'l this scheme provides maximum flexibility but little support each application program must include its own code to interpret an input file as to the appropriate structure however  all operating systems must support at least one structure-that of an executable file-so that the system is able to load and run programs  the macintosh operating system also supports a minimal number of file structures it expects files to contain two parts  a and a 430 chapter 10 10.2 the resource fork contains information of interest to the user  for instance  it holds the labels of any buttons displayed by the program  a foreign user may want to re-label these buttons in his own language  and the macintosh operating system provides tools to allow modification of the data in the resource fork the data fork contains program code or data-the traditional file contents to accomplish the same task on a unix or ms-dos system  the programmer would need to change and recompile the source code  unless she created her own user-changeable data file clearly  it is useful for an operating system to support structures that will be used frequently and that will save the programmer substantial effort too few structures make programming inconvenient  whereas too many cause operating-system bloat and programmer confusion  10.1.5 internal file structure internally  locating an offset within a file can be complicated for the operating system disk systems typically have a well-defined block size determined by the size of a sector all disk i/0 is performed in units of one block  physical record   and all blocks are the same size it is unlikely that the physical record size will exactly match the length of the desired logical record logical records may even vary in length paddng a number of logical records into physical blocks is a common solution to this problem  for example  the unix operating system defines all files to be simply streams of bytes each byte is individually addressable by its offset from the begi1ming  or end  of the file in this case  the logical record size is 1 byte the file system automatically packs and unpacks bytes into physical disk blockssay  512 bytes per block-as necessary  the logical record size  physical block size  and packing technique determine how many logical records are in each physical block the packing can be done either by the user 's application program or by the operating system in either case  the file may be considered a sequence of blocks all the basic i/o functions operate in terms of blocks the conversion from logical records to physical blocks is a relatively simple software problem  because disk space is always allocated in blocks  some portion of the last block of each file is generally wasted if each block were 512 bytes  for example  then a file of 1,949 bytes would be allocated four blocks  2,048 bytes  ; the last 99 bytes would be wasted the waste incurred to keep everything in units of blocks  instead of bytes  is all file systems suffer from internal fragmentation ; the larger the block size  the greater the internal fragmentation  files store information when it is used  this information must be accessed and read into computer memory the information in the file can be accessed in several ways some systems provide only one access method for files other systems  such as those of ibm  support many access methods  and choosing the right one for a particular application is a major design problem  10.2 431 beginning current position end  ;        = = =  rewind ~ read or write ~ figure 10.3 sequential-access file  10.2.1 sequential access the simplest access method is  information in the file is processed in order  one record after the other this mode of access is by far the most common ; for example  editors and compilers usually access files in this fashion  reads and writes make up the bulk of the operations on a file a read operation-read next-reads the next portion of the file and automatically advances a file pointer  which tracks the i/o location similarly  the write operation-write next-appends to the end of the file and advances to the end of the newly written material  the new end of file   such a file can be reset to the beginning ; and on some systems  a program may be able to skip forward or backward n records for some integer n-perhaps only for n = 1 sequential access  which is depicted in figure 10.3  is based on a tape model of a file and works as well on sequential-access devices as it does on random-access ones  10.2.2 direct access  or a file is made up of fixedlength that allow programs to read and write records rapidly in no particular order the direct-access method is based on a disk model of a file  since disks allow random access to any file block for direct access  the file is viewed as a numbered sequence of blocks or records thus  we may read block 14  then read block 53  and then write block 7 there are no restrictions on the order of reading or writing for a direct-access file  direct-access files are of great use for immediate access to large amounts of information databases are often of this type when a query concerning a particular subject arrives  we compute which block contains the answer and then read that block directly to provide the desired information  as a simple example  on an airline-reservation system  we might store all the information about a particular flight  for example  flight 713  in the block identified by the flight number thus  the number of available seats for flight 713 is stored in block 713 of the reservation file to store il1formation about a larger set such as people  we might compute a hash function on the people 's names or search a small in-ncemory index to determine a block to read and search  for the direct-access method  the file operations must be modified to include the block number as a parameter thus  we have read n  where n is the block number  rather than read next  and write n rather than write next an alternative approach is to retain read next and write next  as with sequential 432 chapter 10 figure 10.4 simulation of sequential access on a direct-access file  access  and to add an operation position file to n  where n is the block number  then  to effect a read n  we would position to n and then read next  the block number by the user to the operating system is normally a a relative block number is an index relative to the begirm.ing of the file thus  the first relative block of the file is 0  the next is 1  and so on  even though the absolute disk address may be 14703 for the first block and 3192 for the second the use of relative block numbers allows the operating system to decide where the file should be placed  called the allocation problem  as discussed in chapter 11  and helps to prevent the user from accessing portions of the file system that may not be part of her file some systems start their relative block numbers at 0 ; others start at 1  how  then  does the system satisfy a request for record nina file assuming we have a logical record length l  the request for record n is turned into an i/0 request for l bytes starting at location l  n  within the file  assuming the first record is n = 0   since logical records are of a fixed size  it is also easy to read  write  or delete a record  not all operating systems support both sequential and direct access for files some systems allow only sequential file access ; others allow only direct access some systems require that a file be defined as sequential or direct when it is created ; such a file can be accessed only in a manner consistent with its declaration we can easily simulate sequential access on a direct-access file by simply keeping a variable cp that defines our current position  as shown in figure 10.4 simulating a direct-access file on a sequential-access file  however  is extremely inefficient and clumsy  10.2.3 other access methods other access methods can be built on top of a direct-access method these methods generally involve the construction of an index for the file the like an index in the back of a contains pointers to the various blocks to find a record in the file  we first search the index and then use the to access the file directly and to find the desired record  for example  a retail-price file might list the universal codes  upcs  items  with the associated prices each record consists a 10-digit upc and a 6-digit price  a 16-byte record if our disk has 1,024 bytes per we can store 64 records per block a file of 120,000 records would occupy about 2,000 blocks  2 million bytes   by keeping the file sorted by upc  we can define an index consisting of the first upc in each block this index would have entries of 10 digits each  or 20,000 bytes  and thus could be kept in memory to 10.3 10.3 433 logical record last name number adams arthur asher sm ! th,jol  ir ! social ~ security  age  / e  smith .' '  / index file relative file figure 10.5 example of irdex and relative files  find the price of a particular item  we can make a binary search of the index  from this search  we learn exactly which block contains the desired record and access that block this structure allows us to search a large file doing little i/0  with large files  the index file itself may become too large to be kept in memory one solution is to create an index for the index file the primary index file would contain pointers to secondary index files  which would point to the actual data items  for example  ibm 's indexed sequential-access method  isam  uses a small master index that points to disk blocks of a secondary index the secondary index blocks point to the actual file blocks the file is kept sorted on a defined key to find a particular item  we first make a binary search of the master index  which provides the block number of the secondary index this block is read in  and again a binary search is used to find the block containing the desired record finally  this block is searched sequentially in this way  any record can be located from its key by at most two direct-access reads figure 10.5 shows a similar situation as implemented by vms index and relative files  next  we consider how to store files certainly  no general-purpose computer stores just one file there are typically thousand  millions  and even billions of files within a computer files are stored on random-access storage devices  including hard disks  optical disks  and solid state  memory-based  disks  a storage device can be used in its entirety for a file system it can also be subdivided for finer-grained control for example  a disk can be into quarters  and each quarter can hold a file system storage devices can also be collected together into raid sets that provide protection from the failure of a single disk  as described in section 12.7   sometimes  disks are subdivided and also collected into raid sets  partitioning is useful for limiting the sizes of individual file systems  putting multiple file-system types on the same device  or leaving part of the device available for other uses  such as swap space or unformatted  rz ; c  .v  disk 434 chapter 10 directory  directory partition a files disk 2 1-7 ~ ~ ~ disk 1 directory partition c files partition b files disk 3 figure 10.6 a typical file-system organization  space partitions are also known as or  in the ibm world  a file system can be created on each of these parts of the disk any entity containing a file system is generally known as a the volume may be a subset of a device  a whole device  or multiple devices linked together into a raid set each volume can be thought of as a virtual disk volumes can also store multiple operating systems  allowing a system to boot and run more than one operating system  each volume that contains a file system must also contain information about the files in the system this information is kept in entries in a or ~ the device directory  more commonly known simply as that records information -such as name  location  size  and type-for all files on that volume figure 10.6 shows a typical file-system organization  10.3.1 storage structure as we have just seen  a general-purpose computer system has multiple storage devices  and those devices can be sliced up into volumes that hold file systems  computer systems may have zero or more file systems  and the file systems may be of varying types for example  a typical solaris system may have dozens of file systems of a dozen different types  as shown in the file system list in fig1-1re 10.7  in this book  we consider only general-purpose file systems it is worth noting  though  that there are many special-purpose file systems consider the types of file systems in the solaris example mentioned above  tmpfs-a temporary file system that is created in volatile main memory and has its contents erased if the system reboots or crashes objfs-a virtual file system  essentially an interface to the kernel that looks like a file system  that gives debuggers access to kernel symbols dfs-a virtual file system that maintains contract information to manage which processes start when the system boots and must continue to run during operation 10.3 435 i ufs /devices devfs /dev dev i system/ contract ctfs /proc proc /etc/mnttab mntfs i etc/ svc/volatile tmpfs i system/ object objfs /lib /libc.so.l lofs /dev/fd fd /var ufs /tmp tmpfs /var/run tmpfs /opt ufs /zpbge zfs i zpbge/backup zfs i export/home zfs /var/mail zfs /var/spool/inqueue zfs /zpbg zfs /zpbg/zones zfs figure 10.7 solaris file system  lofs-a loop back file system that allows one file system to be accessed in place of another one prods-a virtual file system that presents information on all processes as a file system ufs  zfs-general-purpose file systems the file systems of computers  then  can be extensive even within a file system  it is useful to segregate files into groups and manage and act on those groups this organization involves the use of directories in the remainder of this section  we explore the topic of directory structure  10.3.2 directory overview the directory can be viewed as a symbol table that translates file names into their directory entries if we take such a view  we see that the directory itself can be organized in many ways we want to be able to insert entries  to delete entries  to search for a named entry  and to list all the entries in the directory  in this section  we examine several schemes for defining the logical structure of the directory system  when considering a particular directory structure  we need to keep in mind the operations that are to be performed on a directory  search for a file we need to be able to search a directory structure to find the entry for a particular file since files have symbolic names  and similar 436 chapter 10 names may indicate a relationship between files  we may want to be able to find all files whose names match a particular pattern  create a file new files need to be created and added to the directory  delete a file when a file is no longer needed  we want to be able to remove it from the directory  list a directory we need to be able to list the files in a directory and the contents of the directory entry for each file in the list  rename a file because the name of a file represents its contents to its users  we must be able to change the name when the contents or use of the file changes renaming a file may also allow its position within the directory structure to be changed  traverse the file system we may wish to access every directory and every file within a directory structure for reliability  it is a good idea to save the contents and structure of the entire file system at regular intervals often  we do this by copyin.g all files to magn.etic tape this technique provides a backup copy in case of system failure in addition  if a file is no longer in use  the file can be copied to tape and the disk space of that file released for reuse by another file  in the following sections  we describe the most common schemes for defining the logical structure of a directory  10.3.3 single-level directory the simplest directory structure is the single-level directory all files are contained in the same directory  which is easy to support and understand  figure 10.8   a single-level directory has significant limitations  however  when the number of files increases or when the system has more than one user since all files are in the same directory  they must have unique names if two users call their data file test  then the unique-name rule is violated for example  in one programming class  23 students called the program for their second assignment prog2 ; another 11 called it assign2 although file names are generally selected to reflect the content of the file  they are often limited in length  complicating the task of making file names unique the ms-dos operating system allows only 11-character file names ; unix  in contrast  allows 255 characters  even a single user on a single-level directory may find it difficult to remember the names of all the files as the number of files increases it is not directory files figure 10.8 single-level directory  10.3 437 uncommon for a user to have hundreds of files on one computer system and an equal number of additional files on another system keeping track of so many files is a daunting task  10.3.4 two-level directory as we have seen  a single-level directory often leads to confusion of file names among different users the standard solution is to create a separate directory for each user  in the two-level directory structure  each user has his own the ufds have similar structures  but each lists only the files of a single user w11en a user job starts or a user logs in  the system 's is searched the mfd is indexed by user name or account number  and each entry points to the ufd for that user  figure 10.9   when a user refers to a particular file  only his own ufd is searched thus  different users may have files with the same name  as long as all the file names within each ufd are unique to create a file for a user  the operating system searches only that user 's ufd to ascertain whether another file of that name exists to delete a file  the operating system confines its search to the local ufd ; thus  it can not accidentally delete another user 's file that has the same name  the user directories themselves must be created and deleted as necessary  a special system program is run with the appropriate user name and account information the program creates a new ufd and adds an entry for it to the mfd  the execution of this program might be restricted to system administrators the allocation of disk space for user directories can be handled with the teduciques discussed in chapter 11 for files themselves  although the two-level directory structure solves the name-collision problem  it still has disadvantages this structure effectively isolates one user from another isolation is an advantage when the users are completely independent but is a disadvantage when the users want to cooperate on some task and to access one another 's files some systems simply do not allow local user files to be accessed by other users  if access is to be pennitted  one user must have the ability to name a file in another user 's directory to name a particular file lmiquely in a two-level directory  we must give both the user name and the file name a two-level directory can be thought of as a tree  or an inverted tree  of height 2 the root of the tree is the mfd its direct descendants are the ufds the descendants of user file directory figure i 0.9 two-level directory structure  438 chapter 10 the ufds are the files themselves the files are the leaves of the tree specifying a user name and a file name defines a path in the tree from the root  the mfd  to a leaf  the specified file   thus  a user name and a file name define a path name every file in the system has a path name to name a file uniquely  a user must know the path name of the file desired  for example  if user a wishes to access her own test file named test  she can simply refer to test to access the file named test of user b  with directory-entry name userb   however  she might have to refer to /userb/test every system has its own syntax for naming files in directories other than the user 's own  additional syntax is needed to specify the volume of a file for instance  in ms-dos a volume is specified by a letter followed by a colon thus  a file specification might be c  \ userb \ fest some systems go even further and separate the volume  directory name  and file name parts of the specification for instance  in vms  the file login.com might be specified as  u   sst.jdeck  login.com ; l  where u is the name of the volume  sst is the name of the directory  jdeck is the name of the subdirectory  and 1 is the version number other systems simply treat the volume name as part of the directory name the first name given is that of the volume  and the rest is the directory and file for instance  /u/pbg/test might specify volume u  directory pbg  and file test  a special case of this situation occurs with the system files programs provided as part of the system -loaders  assemblers  compilers  utility routines  libraries  and so on-are generally defined as files when the appropriate commands are given to the operating system  these files are read by the loader and executed many command interpreters simply treat such a command as the name of a file to load and execute as the directory system is defined presently  this file name would be searched for in the current ufd one solution would be to copy the system files into each ufd however  copying all the system files would waste an enormous amount of space  if the system files require 5 mb  then supporting 12 users would require 5 x 12 = = 60 mb just for copies of the system files  the standard solution is to complicate the search procedure slightly a special user directory is defined to contain the system files  for example  user 0   whenever a file name is given to be loaded  the operating system first searches the local ufd if the file is found  it is used if it is not found  the system automatically searches the special user directory that contains the system files  the sequence of directories searched when a file is named is called the  the search path can be extended to contain an unlimited list of directories to search when a command name is given this method is the one most used in unix and ms-dos systems can also be designed so that each user has his own search path  10.3.5 tree-structured directories once we have seen how to view a two-level directory as a two-level tree  the natural generalization is to extend the directory structure to a tree of arbitrary height  figure 10.10   this generalization allows users to create their own subdirectories and to organize their files accordingly a tree is the most common directory structure the tree has a root directory  and every file in the system has a unique path name  10.3 439 root ititi 0 0 figure i 0.10 tree-structured directory structure  a directory  or subdirectory  contains a set of files or subdirectories a directory is simply another file  but it is treated in a special way all directories have the same internal format one bit in each directory entry defines the entry as a file  0  or as a subdirectory  1   special system calls are used to create and delete directories  in normal use  each process has a current directory the should contain most of the files that are of current interest to the process  when reference is made to a file  the current directory is searched if a file is needed that is not in the current directory  then the user usually must either specify a path name or change the current directory to be the directory holding that file to change directories  a system call is provided that takes a directory name as a parameter and uses it to redefine the current directory thus  the user can change his current directory whenever he desires from one change directory system call to the next  all open system calls search the current directory for the specified file note that the search path may or may not contain a special entry that stands for the current directory  the initial current directory of the login shell of a user is designated when the user job starts or the user logs in the operating system searches the accounting file  or some other predefined location  to find an entry for this user  for accounting purposes   in the accounting file is a pointer to  or the name of  the user 's initial directory this pointer is copied to a local variable for this user that specifies the user 's initial current directory from that shell  other processes can be spawned the current directory of any subprocess is usually the current directory of the parent when it was spawned  path names can be of two types  absolute and relative an begins at the root and follows a down to the specified file  giving the directory names on the path a defi11es a path from the current directory for example  in the tree-structured file system of figure 10.10  440 chapter 10 if the current directory is root/spell/mail  then the relative path nan e prt/jirst refers to the same file as does the absolute path name root/spell/mail/prt/jirst  allowing a user to define her own subdirectories permits her to impose a structure on her files this structure might result in separate directories for files associated with different topics  for example  a subdirectory was created to hold the text of this book  or different forms of information  for example  the directory programs may contain source programs ; the directory bin may store all the binaries   an interesting policy decision in a tree-structured directory concerns how to handle the deletion of a directory if a directory is empty  its entry in the directory that contains it can simply be deleted however  suppose the directory to be deleted is not ernpty but contains several files or subdirectories one of two approaches can be taken some systems  such as ms-dos  will not delete a directory unless it is empty thus  to delete a directory  the user must first delete all the files in that directory if any subdirectories exist this procedure must be applied recursively to them  so that they can be deleted also this approach can result in a substantial amount of work an alternative approach  such as that taken by the unix rm command  is to provide an option  when a request is made to delete a directory  all that directory 's files and subdirectories are also to be deleted either approach is fairly easy to implement ; the choice is one of policy the latter policy is more convenient  but it is also more dangerous  because an entire directory structure can be removed with one command if that command is issued in error  a large number of files and directories will need to be restored  assuming a backup exists   with a tree-structured directory system  users can be allowed to access  in addition to their files  the files of other users for example  user b can access a file of user a by specifying its path names user b can specify either an absolute or a relative path name alternatively  user b can change her current directory to be user a 's directory and access the file by its file names  a path to a file in a tree-struch1red directory can be longer than a path in a two-level directory to allow users to access programs without having to remember these long paths  the macintosh operating system automates the search for executable programs one method it uses is to maintain a file  called the desktop file  containing the metadata code and the name and location of all executable programs it has seen when a new hard disk is added to the system  or the network is accessed  the operating system traverses the directory structure  searching for executable programs on the device and recording the pertinent information this mechanism supports the double-dick execution functionality described previously a double-dick on a file causes its creatorattribute data to be read and the desktop file to be searched for a match once the match is found  the appropriate executable program is started with the clicked-on file as its input  10.3.6 acyclic-graph directories consider two programmers who are working on a joint project the files associated with that project can be stored in a subdirectory  separating them from other projects and files of the two programmers but since both programmers are equally responsible for the project  both want the subdirectory to be in 10.3 directory and disk structure 441 figure 10.11 acyclic-graph directory structure  their own directories the common subdirectory should be shared a shared directory or file will exist in the file system in two  or more  places at once  a tree structure prohibits the sharing of files or directories an acyclic graph -that is  a graph with no cycles-allows directories to share subdirectories and files  figure 10.11   the same file or subdirectory may be in two different directories the acyclic graph is a natural generalization of the tree-structured directory scheme  it is important to note that a shared file  or directory  is not the same as two copies of the file with two copies  each programmer can view the copy rather than the original  but if one programmer changes the file  the changes will not appear in the other 's copy with a shared file  only one actual file exists  so any changes made by one person are immediately visible to the other sharing is particularly important for subdirectories ; a new file created by one person will automatically appear in all the shared subdirectories  when people are working as a team  all the files they want to share can be put into one directory the ufd of each team member will contain this directory of shared files as a subdirectory even in the case of a single user  the user 's file organization may require that some file be placed in different subdirectories  for example  a program written for a particular project should be both in the directory of all programs and in the directory for that project  shared files and subdirectories can be implemented in several ways a common way  exemplified by many of the unix systems  is to create a new directory entry called a link a link is effectively a pointer to another file or subdirectory for example  a link may be implemented as an absolute or a relative path name when a reference to a file is made  we search the directory if the directory entry is marked as a link  then the name of the real file is included in the link information we resolve the link by using that path name to locate the real file links are easily identified by their format in the directory entry  or by having a special type on systems that support types  and are effectively 442 chapter 10 indirect pointers the operating system ignores these links when traversing directory trees to preserve the acyclic structure of the system  another common approach to implementing shared files is simply to duplicate all information about them in both sharing directories thus  both entries are identical and equal consider the difference between this approach and the creation of a link the link is clearly different from the original directory entry ; thus  the two are not equal duplicate directory entries  however  make the original and the copy indistinguishable a major problem with duplicate directory entries is maintaining consistency when a file is modified  an acyclic-graph directory structure is more flexible than is a simple tree structure  but it is also more complex several problems must be considered carefully a file may now have multiple absolute path names consequently  distinct file names may refer to the same file this situation is similar to the aliasing problem for programming languages if we are trying to traverse the entire file system-to find a file  to accumulate statistics on all files  or to copy all files to backup storage-this problem becomes significant  since we do not want to traverse shared structures more than once  another problem involves deletion when can the space allocated to a shared file be deallocated and reused one possibility is to remove the file whenever anyone deletes it  but this action may leave dangling pointers to the now-nonexistent file worse  if the remaining file pointers contain actual disk addresses  and the space is subsequently reused for other files  these dangling pointers may point into the middle of other files  in a system where sharing is implemented by symbolic links  this situation is somewhat easier to handle the deletion of a link need not affect the original file ; only the link is removed if the file entry itself is deleted  the space for the file is deallocated  leaving the links dangling we can search for these links and remove them as well  but unless a list of the associated links is kept with each file  this search can be expensive alternatively  we can leave the links until an attempt is made to use them at that time  we can determine that the file of the name given by the link does not exist and can fail to resolve the link name ; the access is treated just as with any other illegal file name  in this case  the system designer should consider carefully what to do when a file is deleted and another file of the same name is created  before a symbolic link to the original file is used  in the case of unix  symbolic links are left when a file is deleted  and it is up to the user to realize that the orig  llcal file is gone or has been replaced microsoft windows  all flavors  uses the same approach  another approach to deletion is to preserve the file until all references to it are deleted to implement this approach  we must have some mechanism for determining that the last reference to the file has been deleted we could keep a list of all references to a file  directory entries or symbolic links   when a link or a copy of the directory entry is established  a new entry is added to the file-reference list when a link or directory entry is deleted  we remove its entry on the list the file is deleted when its file-reference list is empty  the trouble with this approach is the variable and potentially large size of the file-reference list however  we really do not need to keep the entire list -we need to keep only a count of the number of references adding a new link or directory entry increments the reference count ; deleting a link or entry decrements the count when the count is 0  the file can be deleted ; there are no remaining references to it the unix operating system uses this approach 10.3 443 for nonsymbolic links  or keeping a reference count in the file information block  or inode ; see appendix a.7.2   by effectively prohibiting multiple references to directories  we maintain an acyclic-graph structure  to avoid problems such as the ones just discussed  some systems do not allow shared directories or links for example  in ms-dos  the directory structure is a tree structure rather than an acyclic graph  10.3.7 general graph directory a serious problem with using an acyclic-graph structure is ensuring that there are no cycles if we start with a two-level directory and allow users to create subdirectories  a tree-structured directory results it should be fairly easy to see that simply adding new files and subdirectories to an existing tree-structured directory preserves the tree-structured nature howeve1 ~ when we add links  the tree structure is destroyed  resulting in a simple graph structure  figure 10.12   the primary advantage of an acyclic graph is the relative simplicity of the algorithms to traverse the graph and to determine when there are no more references to a file we want to avoid traversing shared sections of an acyclic graph twice  mainly for performance reasons if we have just searched a major shared subdirectory for a particular file without finding it  we want to avoid searching that subdirectory again ; the second search would be a waste of time  if cycles are allowed to exist in the directory  we likewise want to avoid searching any component twice  for reasons of correctness as well as performance a poorly designed algorithm might result in an infinite loop continually searching through the cycle and never terminating one solution is to limit arbitrarily the number of directories that will be accessed during a search  a similar problem exists when we are trying to determine when a file can be deleted with acyclic-graph directory structures  a value of 0 in the reference count means that there are no more references to the file or directory  figure 10.12 general graph directory  444 chapter 10 10.4 and the file can be deleted however  when cycles exist  the reference count may not be 0 even when it is no longer possible to refer to a directory or file  this anomaly results from the possibility of self-referencing  or a cycle  in the directory structure in this case  we generally need to use a garbage-collection scheme to determine when the last reference has been deleted and the disk space can be reallocated garbage collection involves traversing the entire file system  marking everything that can be accessed then  a second pass collects everything that is not marked onto a list of free space  a similar marking procedure can be used to ensure that a traversal or search will cover everything in the file system once and only once  garbage collection for a disk-based file system  however  is extremely time consuming and is thus seldom attempted  garbage collection is necessary only because of possible cycles in the graph  thus  an acyclic-graph structure is much easier to work with the difficulty is to avoid cycles as new links are added to the structure how do we know when a new lir1k will complete a cycle there are algorithms to detect cycles in graphs ; however  they are computationally expensive  especially when the graph is on disk storage a simpler algorithm in the special case of directories and links is to bypass links during directory traversal cycles are avoided  and no extra overhead is incurred  just as a file must be opened before it is used  a file system must be mounted before it can be available to processes on the system more specifically  the directory structure may be built out of multiple volumes  which must be mounted to make them available within the file-system name space  the mount procedure is straightforward the operating system is given the name of the device and the location within the file structure where the file system is to be attached some operating systems require that a file system type be provided  while others inspect the structures of the device and determine the type of file system typically  a mount point is an empty directory for instance  on a unix system  a file system containing a user 's home directories might be mounted as /home ; then  to access the directory structure within that file system  we could precede the directory names with /home  as in /home/jane motmting that file system under /users would result in the path name /users/jane  which we could use to reach the same directory  next  the operating system verifies that the device contains a valid file system it does so by asking the device driver to read the device directory and verifying that the directory has the expected format finally  the operating system notes in its directory structure that a file system is n1.ounted at the specified mount point this scheme enables the operating system to traverse its directory structure  switching among file systems  and even file systems of varying types  as appropriate  to illustrate file mounting  consider the file system depicted in figure 10.13  where the triangles represent subtrees of directories that are of interest  figure 10.13  a  shows an existing file system  while figure 10.13  b  shows an unmounted volume residing on /device/ds ! c at this point  only the files on the existing file system can be accessed figure 10.14 shows the effects of mounting 10.4 file-system mounting 445 bill  a   b  figure 10.13 file system  a  existing system  b  unmounted volume  the volume residing on /device/dsk over /users if the volume is unmounted  the file system is restored to the situation depicted in figure 10.13  systems impose semantics to clarify functionality for example  a system may disallow a mount over a directory that contains files ; or it may make the mounted file system available at that directory and obscure the directory 's existing files until the file system is unmounted  terminating the use of the file system and allowing access to the original files in that directory as another example  a system may allow the same file system to be mounted repeatedly  at different mount points ; or it may only allow one mount per file system  consider the actions of the classic macintosh operating system whenever the system encounters a disk for the first time  hard disks are found at boot time  and optical disks are seen when they are inserted into the drive   the macintosh operating system searches for a file system on the device if it finds one  it automatically mounts the file system at the root level  adding a folder icon on the screen labeled with the name of the file system  as stored in the i figure 10.14 mount point  446 chapter 10 10.5 device directory   the user is then able to click on the icon and thus display the newly mounted file system mac os x behaves much like bsd unix  on which it is based all file systems are mounted under the /volumes directory the mac os x gui hides this fact and shows the file systems as if they were all mounted at the root level  the microsoft windows family of operating systems  95  98  nt  small 2000  2003  xp  vista  maintains an extended two-level directory structure  with devices and volumes assigned drive letters volumes have a general graph directory structure associated with the drive letter the path to a specific file takes the form of drive-letter  \ path \ to \ file the more recent versions of windows allow a file system to be mounted anywhere in the directory tree  just as unix does windows operating systems automatically discover all devices and mount all located file systems at boot time in some systems  like unix  the mount commands are explicit a system configuration file contains a list of devices and mount points for automatic mounting at boot time  but other mounts may be executed manually  issues concerning file system mounting are further discussed in section 11.2.2 and in appendix a.7.5  in the previous sections  we explored the motivation for file sharing and some of the difficulties involved in allowing users to share files such file sharing is very desirable for users who want to collaborate and to reduce the effort required to achieve a computing goal therefore  user-oriented operating systems must accommodate the need to share files in spite of the inherent difficulties  in this section  we examine more aspects of file sharing we begin by discussing general issues that arise when multiple users share files once multiple users are allowed to share files  the challenge is to extend sharing to multiple file systems  including remote file systems ; we discuss that challenge as well finally  we consider what to do about conflicting actions occurring on shared files for instance  if multiple users are writing to a file  should all the writes be allowed to occurf or should the operating system protect the users ' actions from one another 10.5.1 multiple users when an operating system accommodates multiple users  the issues of file sharing  file naming  and file protection become preeminent given a directory structure that allows files to be shared by users  the system must mediate the file sharing the system can either allow a user to access the files of other users by default or require that a user specifically grant access to the files these are the issues of access control and protection  which are covered in section 10.6  to implement sharing and protection  the system must maintain more file and directory attributes than are needed on a single-user system although many approaches have been taken to meet this requirement  most systems have evolved to use the concepts of file  or directory  owner  or user  and group  the owner is the user who can change attributes and grant access and who has the most control over the file the group attribute defines a subset of users who 10.5 447 can share access to the file for example  the owner of a file on a unix system can issue all operations on a file  while members of the file 's group can execute one subset of those operations  and all other users can execute another subset of operations exactly which operations can be executed by group members and other users is definable by the file 's owner more details on permission attributes are included in the next section  the owner and group ids of a given file  or directory  are stored with the other file attributes when a user requests an operation on a file  the user id can be compared with the owner attribute to determine if the requesting user is the owner of the file likewise  the group ids can be compared the result indicates which permissions are applicable the system then applies those permissions to the requested operation and allows or denies it  many systems have multiple local file systems  including volumes of a single disk or multiple volumes on multiple attached disks in these cases  the id checking and permission matching are straightforward  once the file systems are mounted  10.5.2 remote file systems with the advent of networks  chapter 16   communication among remote computers became possible networking allows the sharing of resources spread across a campus or even around the world one obvious resource to share is data in the form of files  through the evolution of network and file technology  remote file-sharing methods have changed the first implemented method involves manually transferring files between machines via programs like ftp the second major method uses a  dfs  in which remote directories are visible from a local machine in some ways  the third method  the is a reversion to the first a browser is needed to gain access to the remote files  and separate operations  essentially a wrapper for ftp  are used to transfer files  ftp is used for both anonymous and authenticated access  allows a user to transfer files without having an account on the remote system the world wide web uses anonymous file exchange almost exclusively  dfs involves a much tighter integration between the machine that is accessing the remote files and the machine providing the files this integration adds complexity  which we describe in this section  10.5.2.1 the client-server model remote file systems allow a computer to mom1.t one or more file systems from one or more remote machines in this case  the machine containing the files is the server  and the machine seeking access to the files is the client the client-server relationship is common with networked machines generally  the server declares that a resource is available to clients and specifies exactly which resource  in this case  which files  and exactly which clients a server can serve multiple clients  and a client can use multiple servers  depending on the implementation details of a given client-server facility  the server usually specifies the available files on a volume or directory level client identification is more difficult a client can be specified network name or other identifier  such as an ip address  but these can be 448 chapter 10 or imitated as a result of spoofing  an unauthorized client could be allowed access to the server more secure solutions include secure authentication of the client via encrypted keys unfortunately  with security come many challenges  including ensuring compatibility of the client and server  they must use the same encryption algorithms  and security of key exchanges  intercepted keys could again allow unauthorized access   because of the difficulty of solving these problems  unsecure authentication methods are most commonly used  in the case of unix and its network file system  nfs   authentication takes place via the client networking information  by default in this scheme  the user 's ids on the client and server must match lf they do not  the server will be unable to determine access rights to files consider the example of a user who has an id of 1000 on the client and 2000 on the server a request from the client to the server for a specific file will not be handled appropriately  as the server will determine if user 1000 has access to the file rather than basing the determination on the real user id of 2000 access is thus granted or denied based on incorrect authentication information the server must trust the client to present the correct user id note that the nfs protocols allow many-to-many relationships that is  many servers can provide files to many clients in fact a given machine can be both a server to some nfs clients and a client of other nfs servers  once the remote file system is mounted  file operation requests are sent on behalf of the user across the network to the server via the dfs protocol  typically  a file-open request is sent along with the id of the requesting user  the server then applies the standard access checks to determine if the user has credentials to access the file in the mode requested the request is either allowed or denied if it is allowed  a file handle is returned to the client application  and the application then can perform read  write  and other operations on the file the client closes the file when access is completed the operating system may apply semantics similar to those for a local file-system mount or may use different semantics  10.5.2.2 distributed information systems to make client-server systems easier to manage  also known as provide unified access to the information needed for remote computing the provides host-name-to-network-address translations for the entire internet  including the world wide web   before dns became widespread  files containing the same information were sent via e-mail or ftp between all networked hosts this methodology was not scalable dns is further discussed in section 16.5.1  other distributed information systems provide user name/password/user id/group id space for a distributed facility unix systems have employed a wide variety of distributed-information methods sun microsystems introduced yellow pages  since renamed or and most of the industry adopted its use it centralizes storage of user names  host names  printer information  and the like unfortunately  it uses unsecure authentication methods  including sending user passwords unencrypted  in clear text  and identifying hosts by ip address sun 's nis + is a much more secure replacement for nis but is also much more complicated and has not been widely adopted  10.5 449 network information is used in conjunction with user authentication  user name and password  to create a that the server uses to decide whether to allow or deny access to a requested file system for this authentication to be valid  the user names m.u.st match from machine to machine  as with nfs   microsoft uses two distributed naming structures to provide a single name space for users the older naming technology is the newer technology  available in windows xp and windows 2000  is once established  the distributed naming facility is used by all clients servers to authenticate users  the industry is moving toward use of the as a secure distributed naming mechanism in fact  active is based on ldap sun microsystems includes ldap with the operating system and allows it to be employed for user authentication as well as system-wide retrieval of information  such as availability of printers  conceivably  one distributed ldap directory could be used by an organization to store all user and resource information for all the organization 's computers  the result would be for users  who would enter their authentication information once for access to all computers within the organization it would also ease system-administration efforts by combining  in one location  information that is currently scattered in various files on each system or in different distributed information services  10.5.2.3 failure modes local file systems can fail for a variety of reasons  including failure of the disk containing the file system  corruption of the directory structure or other disk-management information  collectively called disk-controller failure  cable failure  and host-adapter failure user or system-administrator failure can also cause files to be lost or entire directories or volumes to be deleted many of these failures will cause a host to crash and an error condition to be displayed  and human intervention will be required to repair the damage  remote file systems have even more failure modes because of the complexity of network systems and the required interactions between remote machines  many more problems can interfere with the proper operation of remote file systems in the case of networks  the network can be interrupted between two hosts such interruptions can result from hardware failure  poor hardware configuration  or networking implementation issues although some networks have built-in resiliency  including multiple paths between hosts  many do not any single failure can thus interrupt the flow of dfs commands  consider a client in the midst of using a remote file system it has files open from the remote host ; among other activities  it may be performing directory lookups to open files  reading or writing data to files  and closing files now consider a partitioning of the network  a crash of the server  or even a scheduled shutdown of the server suddenly  the remote file system is no longer reachable  this scenario is rather common  so it would not be appropriate for the client system to act as it would if a local file system were lost rather  the system can either terminate all operations to the lost server or delay operations until the server is again reachable these failure semantics are defined and in plemented as part of the remote-file-system protocol termination of all operations can 450 chapter 10 result in users ' losing data-and patience thus  most dfs protocols either enforce or allow delaying of file-system operations to rencote hosts  with the hope that the remote host will become available again  to implement this kind of recovery from failure  some kind of may be maintained on both the client and the server if both server and client maintain knowledge of their current activities and open files  then they can seamlessly recover from a failure in the situation where the server crashes but must recognize that it has remotely rnounted exported file systems and opened files  nfs takes a simple approach  implementing a dfs  in essence  it assumes that a client request for a file read or write would not have occurred unless the file system had been remotely mounted and the file had been previously open the nfs protocol carries all the information needed to locate the appropriate file and perform the requested operation similarly  it does not track which clients have the exported volumes mounted  again assuming that if a request comes in  it must be legitimate while this stateless approach makes nfs resilient and rather easy to implement  it also makes it unsecure for example  forged read or write requests could be allowed by an nfs server even though the requisite mount request and permission check had not taken place these issues are addressed in the industry standard nfs version 4  in which nfs is made stateful to improve its security  performance  and functionality  10.5.3 consistency semantics represent an important criterion for evaluating any file system that supports file sharing these semantics specify how multiple users of a system are to access a shared file simultaneously in particular  they specify when modifications of data by one user will be observable by other users these semantics are typically implemented as code with the file system  consistency semantics are directly related to the process-synchronization algorithms of chapter 6 however  the complex algorithms of that chapter tend not to be implemented in the case of file i/0 because of the great latencies and slow transfer rates of disks and networks for example  performing an atomic transaction to a remote disk could involve several network communications  several disk reads and writes  or both systems that attempt such a full set of functionalities tend to perform poorly a successful implementation of complex sharing semantics can be found in the andrew file system  for the following discussion  we assume that a series of file accesses  that is  reads and writes  attempted by a user to the same file is always enclosed between the open   and close   operations the series of accesses between the open   and close   operations makes up a to illustrate the concept  we sketch several prominent examples of consistency semantics  10.5.3.1 unix semantics the unix file system  chapter 17  uses the following consistency semantics  writes to an open file by a user are visible immediately to other users who have this file open  one mode of sharing allows users to share the pointer of current location into the file thus  the advancing of the pointer by one user affects all 10.6 10.6 451 sharing users here  a file has a single image that interleaves all accesses  regardless of their origin  in the unix semantics  a file is associated with a single physical image that is accessed as an exclusive resource contention for this single image causes delays in user processes  10.5.3.2 session semantics the andrew file system  afs   chapter 17  uses the following consistency semantics  writes to an open file by a user are not visible immediately to other users that have the same file open  once a file is closed  the changes made to it are visible only in sessions starting later already open instances of the file do not reflect these changes  according to these semantics  a file may be associated temporarily with several  possibly different  images at the same time consequently  multiple users are allowed to perform both read and write accesses concurrently on their images of the file  without delay almost no constraints are enforced on scheduling accesses  10.5.3.3 immutable-shared-files semantics a unique approach is that of once a file is declared as shared by its creator  it cam1ot be modified an immutable ile has two key properties  its name may not be reused  and its contents may not be altered  thus  the name of an immutable file signifies that the contents of the file are fixed the implementation of these semantics in a distributed system  chapter 17  is simple  because the sharing is disciplined  read-only   when information is stored in a computer system  we want to keep it safe from physical damage  the issue of reliability  and improper access  the issue of protection   reliability is generally provided by duplicate copies of files many computers have systems programs that automatically  or through computer-operator intervention  copy disk files to tape at regular intervals  once per day or week or month  to maintain a copy should a file system be accidentally destroyed  file systems can be damaged by hardware problems  such as errors in reading or writing   power surges or failures  head crashes  dirt  temperature extremes  and vandalism files may be deleted accidentally bugs in the file-system software can also cause file contents to be lost reliability is covered in more detail in chapter 12  protection can be provided in many ways for a small single-user system  we might provide protection by physically removing the floppy disks and locking them in a desk drawer or file cabinet in a multiuser system  however  other mechanisms are needed  452 chapter 10 10.6.1 types of access the need to protect files is a direct result of the ability to access files systems that do not permit access to the files of other users do not need protection thus  we could provide complete protection by prohibiting access alternatively  we could provide free access with no protection both approaches are too extreme for general use what is needed is protection mechanisms provide controlled access by limitin.g the types of file access that can be made access is permitted or denied depending on several factors  one of which is the type of access requested several different types of operations may be controlled  read read from the file  write write or rewrite the file  execute load the file into memory and execute it  append write new information at the end of the file  delete delete the file and free its space for possible reuse  list list the name and attributes of the file  other operations  such as renaming  copying  and editing the file  may also be controlled for many systems  however  these higher-level fm1ctions may be implemented by a system program that makes lower-level system calls  protection is provided at only the lower level for instance  copying a file may be implemented simply by a sequence of read requests in this case  a user with read access can also cause the file to be copied  printed  and so on  many protection mechanisms have been proposed each has advantages and disadvantages and must be appropriate for its intended application a small computer system that is used by only a few members of a research group  for example  may not need the same types of protection as a large corporate computer that is used for research  finance  and personnel operations we discuss some approaches to protection in the following sections and present a more complete treatment in chapter 14  10.6.2 access control the most common approach to the protection problem is to make access dependent on the identity of the user different users may need different types of access to a file or directory the most general scheme to implement dependent access is to associate with each file and directory an  acju specifying user names and the types of access allowed for each user  when a user requests access to a particular file  the operating system checks the access list associated with that file if that user is listed for the requested access  the access is allowed otherwise  a protection violation occurs  and the user job is denied access to the file  this approach has the advantage of enabling complex access methodologies  the main problem with access lists is their length if we want to allow everyone to read a file  we must list all users with read access this technique has two undesirable consequences  10.6 453 constructing such a list may be a tedious and unrewarding task  especially if we do not know in advance the list of users in the system  the directory entry  previously of fixed size  now must be of variable size  resulting in more complicated space management  these problems can be resolved by use of a condensed version of the access list  to condense the length of the access-control list  many systems recognize three classifications of users in connection with each file  owner the user who created the file is the owner  group a set of users who are sharing the file and need similar access is a group  or work group  universe all other users in the system constitute the universe  the most common recent approach is to combine access-control lists with the more general  and easier to implement  owner  group  and universe accesscontrol scheme just described for example  solaris 2.6 and beyond use the three categories of access by default but allow access-control lists to be added to specific files and directories when more fine-grained access control is desired  to illustrate  consider a person  sara  who is writing a new book she has hired three graduate students  jim  dawn  and jill  to help with the project  the text of the book is kept in a file named book the protection associated with this file is as follows  sara should be able to invoke all operations on the file  jim  dawn  and jill should be able only to read and write the file ; they should not be allowed to delete the file  all other users should be able to read  but not write  the file  sara is interested in letting as many people as possible read the text so that she can obtain feedback  to achieve such protection  we must create a new group-say  textwith members jim  dawn  and jill the name of the group  text  must then be associated with the file book  and the access rights must be set in accordance with the policy we have outlined  now consider a visitor to whom sara would like to grant temporary access to chapter 1 the visitor can not be added to the text group because that would give him access to all chapters because a file can only be in one group  sara can not add another group to chapter 1 \ nith the addition of access-control-list functionality  though  the visitor can be added to the access control list of chapter 1  for this scheme to work properly  permissions and access lists must be controlled tightly this control can be accomplished in several ways for example  in the unix system  groups can be created and modified only by the manager of the facility  or by any superuser   thus  control is achieved through human interaction in the vms system  the owner of the file can create 454 chapter 10 and modify the access-control list access lists are discussed further in section 14.5.2  with the more limited protection classification  only three fields are needed to define protection often  each field is a collection of bits  and each bit either allows or prevents the access associated with it for example  the unix system defines three fields of 3 bits each -rwx  where r controls read access  w controls write access  and x controls execution a separate field is kept for the file owner  for the file 's group  and for all other users in this scheme  9 bits per file are needed to record protection information thus  for our example  the protection fields for the file book are as follows  for the owner sara  all bits are set ; for the group text  the rand w bits are set ; and for the universe  only the r bit is set  one difficulty in combining approaches comes in the user interface users must be able to tell when the optional acl permissions are set on a file in the solaris example  a + appends the regular permissions  as in  f1l s'/stetvl  ji users  pbg-la.ptof \ users  permissions for gue  ; t full contml h-1odi ~ ,. f ; _e a.d g execute r.ead 'vi/rite spec  ia.l permissions a.llo w for specia.l permissions orfor advanced settings  click .a.dva.nced  .a.dva.nced figure 10.15 windows xp access-control list management  10.6 455 19 -rw-r--r + 1 jim staff 130 may 25 22  13 file1 a separate set of commands  setfacl and getfacl  is used to manage the acls  windows xp users typically manage access-control lists via the cui figure 10.15 shows a file-permission window on windows xp 's ntfs file system in this example  user guest is specifically denied access to the file lo.tex  another difficulty is assigning precedence when permission and acls conflict for example  if joe is in a file 's group  which has read permission  but the file has an acl granting joe read and write permission  should a write by joe be granted or denied solaris gives acls precedence  as they are more fine-grained and are not assigned by default   this follows the general rule that specificity should have priority  10.6.3 other protection approaches another approach to the protection problem is to associate a password with each file just as access to the computer system is often controlled by a password  access to each file can be controlled in the same way if the passwords are chosen randomly and changed often  this scheme may be effective in limiting access to a file the use of passwords has a few disadvantages  however first  the number of passwords that a user needs to remember may permissions in a unix system in the unix system  directory protection and file protection are handled similarly associated with each subdirectory are three fields-owner  group  and universe-each consisting of the three bits rwx thus  a user can list the content of a subdirectory only if the r bit is set in the appropriate field  similarly  a user can change his current directory to another current directory  say  faa  only if the x bit associated with the faa subdirectory is set in the appropriate field  a sample directory listing from a unix environment is shown in figure 10.16 the first field describes the protecti.on of the file or directory ad as the first character indicates a s11bdirectory also shown are the number of links to the file  the owner 's name  the group 's name  the size of the file in bytes  the date of last modification  and finally the file 's name  with optional extension   -rw-rw-r l pbg staff 31200 sep 30l  uo intro.ps drwx 5 pbg staff 512 jul 8 09.33 private/ drwxrwxr-x 2 pbg staff 512 jul8 09  35 doc/ drwxrwx 2 pbg student 512 aug 3 14  13 student-proj/ -rw-r--r 1 pbg staff 9423 feb 24 2003 program.c -rwxr-xr-x l pbg staff 20471 feb 24 2003 program drwx ~ -x--x 4 pbg faculty 512 jul 31 10  31 lib/ drwx 3 pbg staff 1024 aug 29 06  52 mail/ drwxrwxrwx 3 pbg staff 512 jul 8 09  35 test/ figure 10.16 a sample directory listing  456 chapter 10 10.7 become large  making the scheme impractical second  if only one password is used for all the files  then once it is discovered  all files are accessible ; protection is on an all-or-none basis some systems  for example  tops-20  allow a user to associate a password with a subdirectory  rather than with an individual file  to deal with this problem the ibmvm/cms operating system allows three passwords for a minidisk-one each for read  write  and nrultiwrite access  some single-user operating systencs-such as ms-dos and versions of the macintosh operating system prior to mac os x-provide little in terms of file protection in scenarios where these older systems are now being placed on networks file sharing and communication  protection mechanisms must be into them designing a feature for a new operating system is almost always easier than adding a feature to an existing one such updates are usually less effective and are not seamless  in a multilevel directory structure  we need to protect not only individual files but also collections of files in subdirectories ; that is  we need to provide a mechanism for directory protection the directory operations that must be protected are somewhat different from the file operations we want to control the creation and deletion of files in a directory in addition  we probably want to control whether a user can determine the existence of a file in a directory  sometimes  knowledge of the existence and name of a file is significant in itself  thus  listing the contents of a directory must be a protected operation similarly  if a path name refers to a file in a directory  the user must be allowed access to both the directory and the file in systems where files may have numerous path names  such as acyclic or general graphs   a given user may have different access rights to a particular file  depending on the path name used  a file is an abstract data type defined and implemented by the operating system it is a sequence of logical records a logical record may be a byte  a line  of fixed or variable length   or a more complex data item the operating system may specifically support various record types or may leave that support to the application program  the major task for the operating system is to map the logical file concept onto physical storage devices such as magnetic tape or disk since the physical record size of the device may not be the same as the logical record size  it may be necessary to order logical records into physical records again  this task may be supported by the operating system or left for the application program  each device in a file system keeps a volume table of contents or a device directory listing the location of the files on the device in addition  it is useful to create directories to allow files to be organized a single-level directory in a multiuser system causes naming problems  since each must have a unique name a two-level directory solves this creating a separate directory for each users files the directory lists name and includes the file 's location on the disk  length  type  owner  time creation  time of last use  and so on  the natural generalization of a two-level directory is a tree-structured directory a tree-structured directory allows a user to create subdirectories to organize files acyclic-graph directory structures enable users to share 457 subdirectories and files but complicate searching and deletion a general graph structure allows complete flexibility in the sharing of files and directories but sometimes requires garbage collection to recover unused disk space  disks are segmented into one or more volumes/ each containing a file system or left raw file systems may be mounted into the system 's naming structures to make them available the naming scheme varies by operating system once mounted  the files within the volume are available for use file systems may be unmounted to disable access or for maintenance  file sharing depends on the semantics provided by the system files may have multiple readers  multiple writers  or limits on sharing distributed file systems allow client hosts to mount volumes or directories from servers  as long as they can access each other across a network remote file systems present challenges in reliability  performance  and security distributed information systems maintain user/ host/ and access information so that clients and servers can share state information to ncanage use and access  since files are the main information-storage mechanism in most computer systems  file protection is needed access to files can be controlled separately for each type of access-read  write  execute  append  delete  list directory  and so on file protection can be provided by access lists  passwords  or other techniques  10.1 some systems provide file sharing by maintaining a single copy of a file ; other systems maintain several copies  one for each of the users sharing the file discuss the relative merits of each approach  10.2 some systems automatically open a file when it is referenced for the first time and close the file when the job terminates discuss the advantages and disadvantages of this scheme compared with the more traditional one  where the user has to open and close the file explicitly  10.3 in some systems  a subdirectory can be read and written by an authorized user  just as ordinary files can be  a describe the protection problems that could arise  b suggest a scheme for dealing with each of these protection problems  10.4 do some systems keep track of the type of a file  while others leave it to the user and others simply do not implement multiple file types which system is better 10.5 consider a system that supports 5,000 users suppose that you want to allow 4,990 of these users to be able to access one file  a howwould specify this protection scheme in unix b can you suggest another protection scheme that can be used more effectively for this purpose than the scheme provided by unix 458 chapter 10 10.6 what are the advantages and disadvantages of providing ncandatory locks instead of advisory locks whose usage is left to users ' discretion 10.7 explain the purpose of the open   and close   operations  10.8 the open-file table is used to maintain information about files that are currently open should the operating system maintain a separate table for each user or just maintain one table that contains references to files that are currently being accessed by all users if the same file is being accessed by two different programs or users  should there be separate entries in the open-file table 10.9 give an example of an application that could benefit from operatingsystem support for random access to indexed files  10.10 discuss the advantages and disadvantages of associating with remote file systems  stored on file servers  a set of failure semantics different from that associated with local file systems  10.11 could you simulate a multilevel directory structure with a single-level directory structure in which arbitrarily long names can be used if your answer is yes  explain how you can do so  and contrast this scheme with the multilevel directory scheme if your answer is no  explain what prevents your simulation 's success how would your answer change if file names were limited to seven characters 10.12 what are the implications of supporting unix consistency semantics for shared access for files stored on remote file systems 10.13 if the operating system knew that a certain application was going to access file data in a sequential manner  how could it exploit this information to improve performance 10.14 consider a file system in which a file can be deleted and its disk space reclaimed while links to that file still exist what problems may occur if a new file is created in the same storage area or with the same absolute path name how can these problems be avoided 10.15 discuss the advantages and disadvantages of supporting links to files that cross mount points  that is  the file link refers to a file that is stored in a different volume   10.16 what are the advantages and disadvantages of recording the name of the creating program with the file 's attributes  as is done in the macintosh operating system  general discussions concerning file systems are offered by grosshans  1986   golden and pechura  1986  describe the structure of microcomputer file systems database systems and their file structures are described in full in silberschatz et al  2001   a multilevel directory structure was first implemented on the multics system  organick  1972    most operating systems now implement multilevel 459 directory structures these include linux  bovet and cesati  2002    mac os x  http  / /www.apple.com/macosx/   solaris  mcdougall and mauro  2007    and all versions of windows  russinovich and solomon  2005    the network file system  nfs   designed by sun microsystems  allows directory structures to be spread across networked computer systems nfs is fully described in chapter 17 nfs version 4 is described in rfc3505  http  / /www.ietf.org/rfc/rfc3530.txt   general discussion of solaris file systems is found in the sun system administration guide  devices and file systems  http  / i docs sun com/ app i docs/ doc/817-5093   dns was first proposed by su  1982  and has gone through several revisions since  with mockapetris  1987  adding several major features eastlake  1999  has proposed security extensions to let dns hold security keys  ldap  also known as x.509  is a derivative subset of the x.soo distributed directory protocol it was defined by yeong et al  1995  and has been implemented on many operating systems  interesting research is ongoing in the area of file-system interfaces-in particular  on issues relating to file naming and attributes for example  the plan 9 operating system from bell laboratories  lucent technology  makes all objects look like file systems thus  to display a list of processes on a system  a user simply lists the contents of the /proc directory similarly  to display the time of day  a user need only type the file i dev i time  11.1 c as we saw in chapter 10  the file system provides the mechanism for on-line storage and access to file contents  including data and programs the file system resides permanently on secondary storage  which is designed to hold a large amount of data permanently this chapter is primarily concerned with issues surrounding file storage and access on the most common secondary-storage medium  the disk we explore ways to structure file use  to allocate disk space  to recover freed space  to track the locations of data  and to interface other parts of the operating system to secondary storage performance issues are considered throughout the chapter  to describe the details of implementing local file systems and directory structures  to describe the implementation of remote file systems  to discuss block allocation and free-block algorithms and trade-offs  disks provide the bulk of secondary storage on which a file system is maintained they have two characteristics that make them a convenient medium for storing multiple files  a disk can be rewritten in place ; it is possible to read a block from the disk  modify the block  and write it back into the sance place  a disk can access directly any block of information it contains thus  it is simple to access any file either sequentially or randomly  and switching from one file to another requires only moving the read-write heads and waiting for the disk to rotate  we discuss disk structure in great detail in chapter 12  461 462 chapter 11 to improve i/0 efficiency  i/0 transfers between memory and disk are performed in units of blocks each block has one or more sectors depending on the disk drive  sector size varies from 32 bytes to 4,096 bytes ; the usual size is 512 bytes  provide efficient and convenient access to the disk by allowing data to be stored  located  and retrieved easily a file system poses two quite different design problems the first problem is defining how the file system should look to the user this task involves defining a file and its attributes  the operations allowed on a file  and the directory structure for organizing files the second problem is creating algorithms and data structures to map the logical file system onto the physical secondary-storage devices  the file system itself is generally composed of many different levels the structure shown in figure 11.1 is an example of a layered design each level in the design uses the features of lower levels to create new features for use by higher levels  the lowest level  the i/o control  consists of and interrupt handlers to transfer information between the main memory and the disk system a device driver can be thought of as a translator its input consists of high-level commands such as retrieve block 123 its output consists of lowlevel  hardware-specific instructions that are used by the hardware controller  which interfaces the i/0 device to the rest of the system the device driver usually writes specific bit patterns to special locations in the i/0 controller 's memory to tell the controller which device location to act on and what actions to take the details of device drivers and the i/o infrastructure are covered in chapter 13  the needs only to issue generic commands to the appropriate device driver to read and write physical blocks on the disk each physical block is identified by its numeric disk address  for example  drive 1  cylilcder 73  track 2  sector 10   this layer also manages the memory buffers and caches that hold various file-system  directory  and data blocks a block application programs ~ logical file system ~ file-organization module ~ basic file system ~ 1/0 control devices figure 11.1 layered file system  11.1 463 in the buffer is allocated before the transfer of a disk block can occur when the buffer is full  the buffer m ~ anager must find more buffer ncemory or free up buffer space to allow a requested i/o to complete caches are used to hold frequently used file-system metadata to improve performance  so managing their contents is critical for optimum system performance  the knows about files and their logical blocks  as well as physical blocks by knowing the type of file allocation used and the location of the file  the file-organization module can translate logical block addresses to physical block addresses for the basic file system to transfer  each file 's logical blocks are numbered from 0  or 1  through n since the physical blocks containing the data usually do not match the logical numbers  a translation is needed to locate each block the file-organization module also includes the free-space manager  which tracks unallocated blocks and provides these blocks to the file-organization module when requested  finally  the f ! je manages metadata information metadata includes all of the file-system structure except the actual data  or contents of the files   the logical file system manages the directory structure to provide the fileorganization module with the information the latter needs  given a symbolic file name it maintains file structure via file-control blocks a flle-corttml  an in most unix file systems  contains information about the file  including ownership  permissions  and location of the file contents the logical file system is also responsible for protection and security  as discussed in chapters 10 and 14  when a layered structure is used for file-system implementation  duplication of code is minimized the i/o control and sometimes the basic file-system code can be used by multiple file systems each file system can then have its own logical file-system and file-organization modules unfortunately  layering can introduce more operating system overhead  which may result in decreased performance the use of layering  including the decision about how many layers to use and what each layer should do  is a major challenge in designing new systems  many file systems are in use today most operating systems support more than one for example  most cd-roms are written in the iso 9660 format  a standard format agreed on by cd-rom manufacturers in addition to removable-media file systems  each operating system has one or more diskbased file systems unix uses the fee which is based on the berkeley fast file system  ffs   windows nt  2000  and xp support disk file-system formats of fat  fat32  and ntfs  or windows nt file system   as well as cd-rom  dvd  and floppy-disk file-system formats although linux supports over forty different file systerns  the standard linux file system is known as the with the most common versions being ext2 and ext3 there are also distributed file systems in which a file system on a server is mounted by one or more client computers across a network  file-system research continues to be an active area of operating-system design and implementation coogle created its own file system to meet the company 's specific storage and retrieval needs another interesting project is the fuse file-system  which provides flexibility in file-system use by implementing and executing file systems as user-level rather than kernel-level code using fuse  a user can add a new file system to a variety of operating systems and can use that file system to manage her files  464 chapter 11 11.2 as was described in section 10.1.2  operating systems implement open   and close   systems calls for processes to request access to file contents  in this section  we delve into the structures and operations used to implement file-system operations  11.2.1 overview several on-disk and in-memory structures are used to implement a file system  these structures vary depending on the operating system and the file system  but some general principles apply  on disk  the file system may contain information about how to boot an operating system stored there  the total number of blocks  the number and location of free blocks  the directory structure  and individual files many of these structures are detailed throughout the remainder of this chapter ; here  we describe them briefly  a  per volume  can contain information needed by the system to boot an operating system from that volume if the disk does not contain an operating system  this block can be empty it is typically the first block of a volume in ufs  it is called the b,jsck ; in ntfs  it is the  per volume  contains volume  or partition  details  such as the number of blocks in the partition  the size of the blocks  a free-block count and free-block pointers  and a free-fcb count and fcb pointers in ufs  this is called a in ntfs  it is stored in the a directory structure  per file system  is used to organize the files in ufs  this includes file names and associated inode numbers in ntfs  it is stored in the master file table  a per-file fcb contains many details about the file it has a unique identifier number to allow association with a directory entry in ntfs  this information is actually stored within the master file table  which uses a relational database structure  with a row per file  the in-memory in.formation is used for both file-system management and performance improvement via caching the data are loaded at mount time  updated during file-system operations  and discarded at dismount several types of structures may be included  an in-memory volume  contains information about each mounted an in-memory directory-structure cache holds the directory information of recently accessed directories  for directories at which volumes are mounted  it can contain a pointer to the volume table  the contains a copy of the fcb of each open file  as well as other information  11.2 465 file dates  create  access  write  file owner group  acl file data blocks or pointers to file data blocks figure 11.2 a typical file-control block  the contains a pointer to the appropriate entry in the system-wide open-file table  as well as other information  buffers hold file-system blocks when they are being read from disk or written to disk  to create a new file  an application program calls the logical file system  the logical file system knows the format of the directory structures to create a new file  it allocates a new fcb  alternatively  if the file-system implementation creates all fcbs at file-system creation time  an fcb is allocated from the set of free fcbs  the system then reads the appropriate directory into memory  updates it with the new file name and fcb  and writes it back to the disk a typical fcb is shown in figure 11.2  some operating systems  including unix  treat a directory exactly the same as a file-one with a type field indicating that it is a directory other operating systems  includii g windows nt  implement separate system calls for files and directories and treat directories as entities separate from files whatever the larger structural issues  the logical file system can call the file-organization module to map the directory i/0 into disk-block numbers  which are passed on to the basic file system and i/o control system  now that a file has been created  it can be used for i/0 first  though  it must be opened the open   call passes a file name to the logical file system  the open   system call first searches the system-wide open-file table to see if the file is already in use by another process if it is  a per-process open-file table entry is created pointing to the existing system-wide open-file table this algorithm can save substantial overhead if the file is not already open  the directory structure is searched for the given file name parts of the directory structure are usually cached in memory to speed directory operations once the file is found  the fcb is copied into a system-wide open-file table in memory  this table not only stores the fcb but also tracks the number of processes that have the file open  next  an entry is made in the per-process open-file table  with a pointer to the entry in the system-wide open-file table and some other fields these other fields may include a pointer to the current location in the file  for the next read   or write   operation  and the access mode in which the file is open  the open   call returns a pointer to the appropriate entry in the per-process 466 chapter 11 user space user space kernel memory  a  kernel memory  b  ,  ---..,...  + -t-ilej d do secondary storage secondary storage figure 11.3 in-memory file-system structures  a  file open  b  file read  file-system table all file operations are then performed via this pointer the file name may not be part of the open-file table  as the system has no use for it once the appropriate fcb is located on disk it could be cached  though  to save time on subsequent opens of the same file the name given to the entry varies unix systems refer to it as a windows refers to it as a when a process closes the file  the per-process table entry is removed  and the system-wide entry 's open count is decremented when all users that have opened the file close it  any updated metadata is copied back to the disk-based directory structure  and the system-wide open-file table entry is removed  some systems complicate this scheme further by using the file system as an interface to other system aspects  such as networking for example  in ufs  the system-wide open-file table holds the inodes and other information for files and directories it also holds similar information for network connections and devices in this way  one mechanism can be used for multiple purposes  the caching aspects of file-system structures should not be overlooked  most systems keep all information about an open file  except for its actual data blocks  in memory the bsd unix system is typical in its use of caches wherever disk i/0 can be saved its average cache hit rate of 85 percent shows that these techniques are well worth implementing the bsd unix system is described fully in appendix a  the operating structures of a file-system implementation are summarized in figure 11.3  11.2 467 11.2.2 partitions and mounting the layout of a disk can have many variations  depending on the operating system a disk can be sliced into multiple partitions  or a volume can span multiple partitions on multiple disks the former layout is discussed here  while the latter  which is more appropriately considered a form of raid  is covered in section 12.7  each partition can be either raw  containing no file system  or cooked  containing a file system is used where no file system is appropriate  unix swap space can use a raw partition  for example  as it uses its own format on disk and does not use a file system likewise  some databases use raw disk and format the data to suit their needs raw disk can also hold information needed by disk raid systems  such as bit maps indicating which blocks are mirrored and which have changed and need to be mirrored similarly  raw disk can contain a miniature database holding raid configuration information  such as which disks are members of each raid set raw disk use is further discussed in section 12.5.1  boot information can be stored in a separate partition again  it has its own format  because at boot time the system does not have the file-system code loaded and therefore can not interpret the file-system format rather  boot information is usually a sequential series of blocks  loaded as an image into memory execution of the image starts at a predefined location  such as the first byte this in turn knows enough about the file-system structure to be able to find and load the kernel and start it executing it can contain more than the instructions for how to boot a specific operating system for instance  pcs and other systems can be multiple operating systems can be installed on such a system how does the system know which one to boot a boot loader that understands multiple file systems and multiple operating systems can occupy the boot space once loaded  it can boot one of the operating systems available on the disk the disk can have multiple partitions  each containing a different type of file system and a different operating system  the which contains the operating-system kernel and sometimes other system files  is mounted at boot time other volumes can be automatically mounted at boot or manually mounted later  depending on the operating system as part of a successful mount operation  the operating system verifies that the device contains a valid file system it does so by asking the device driver to read the device directory and verifying that the directory has the expected format if the format is invalid  the partition must have its consistency checked and possibly corrected  either with or without user intervention finally  the operating system notes in its in-memory mount table that a file system is mounted  along with the type of the file system the details of this function depend on the operating system microsoft windows-based systems mount each volume in a separate name space  denoted by a letter and a colon to record that a file system is mounted at f   for example  the operating system places a pointer to the file system in a field of the device structure corresponding to f   when a process specifies the driver letter  the operating system finds the appropriate file-system pointer and traverses the directory structures on that device to find the specified file or directory  later versions of windows can mount a file system at any point within the existing directory structure  468 chapter 11 on unix  file systems can be mounted at any directory mounting is implemented by setting a flag in the in-memory copy of the inode for that directory the flag indicates that the directory is a mount point a field then points to an entry in the mount table  indicating which device is mounted there  the mount table entry contains a pointer to the superblock of the file system on that device this scheme enables the operating system to traverse its directory structure  switching seamlessly among file systems of varying types  11.2.3 virtual file systems the previous section m.akes it clear that modern operating systems must concurrently support multiple types of file systems but how does an operating system allow multiple types of file systems to be integrated into a directory structure and how can users seamlessly move between file-system types as they navigate the file-system space we now discuss some of these implementation details  an obvious but suboptimal method of implementing multiple types of file systems is to write directory and file routines for each type instead  however  most operating systems  including unix  use object-oriented techniques to simplify  organize  and modularize the implementation the use of these methods allows very dissimilar file-system types to be implemented within the same structure  including network file systems  such as nfs users can access files that are contained within multiple file systems on the local disk or even on file systems available across the network  data structures and procedures are used to isolate the basic systemcall functionality from the implementation details thus  the file-system implementation consists of three major layers  as depicted schematically in figure 11.4 the first layer is the file-system interface  based on the open    read    write    and close   calls and on file descriptors  the second layer is called the layer the vfs layer serves two important functions  it separates file-system-generic operations from their implementation by defining a clean vfs interface several implementations for the vfs interface may coexist on the same machine  allowing transparent access to different types of file systems mounted locally  it provides a mechanism for uniquely representing a file throughout a network the vfs is based on a file-representation structure  called a that contains a numerical designator for a network-wide unique file  unix inodes are unique within only a single file system  this network-wide uniqueness is required for support of network file systems  the kernel maintains one vnode structure for each active node  file or directory   thus  the vfs distinguishes local files from remote ones  and local files are further distinguished according to their file-system types  the vfs activates file-system-specific operations to handle local requests according to their file-system types and calls the nfs protocol procedures for remote requests file handles are constructed from the relevant vnodes and are passed as arguments to these procedures the layer implementing the 11.2 469 network figure 11.4 schematic view of a virtual file system  file-system type or the remote-file-system protocol is the third layer of the architecture  let 's briefly examine the vfs architecture in linux the four main object types defined by the linux vfs are  the inode object  which represents an individual file the file object  which represents an open file the superblock object  which represents an entire file system the dentry object  which represents an individual directory entry for each of these four object types  the vfs defines a set of operations that must be implemented every object of one of these types contains a pointer to a f1.mction table the function table lists the addresses of the actual functions that implement the defined operations for that particular object for example  an abbreviated api for some of the operations for the file object include  int open      -open a file  ssize_t read      -read from a file  ssize_t write      -write to a file  int mmap    -memory-map a file  an implementation of the file object for a specific file type is required to implement each function specified in the definition of the file object  the complete definition ofthe file object is specified in the struct f ile_operat ions  which is located in the file /usr/include/linux/fs .h  470 chapter 11 11.3 thus  the vfs software layer can perform an operation on one of these objects by calling the appropriate function from the object 's function table  without having to know in advance exactly what kind of object it is dealing with the vfs does not know  or care  whether an inode represents a disk file  a directory file  or a remote file the appropriate function for that file 's read   operation will always be at the same place in its function table  and the vfs software layer will call that function without caring how the data are actually read  the selection of directory-allocation and directory-management algorithms significantly affects the efficiency  performance  and reliability of the file system in this section  we discuss the trade-offs involved in choosing one of these algorithms  11.3.1 linear list the simplest method of implementing a directory is to use a linear list of file names with pointers to the data blocks this method is simple to program but time-consuming to execute to create a new file  we must first search the directory to be sure that no existing file has the same name then  we add a new entry at the end of the directory to delete a file  we search the directory for the named file and then release the space allocated to it to reuse the directory entry  we can do one of several things we can mark the entry as unused  by assigning it a special name  such as an all-blank name  or with a used -unused bit in each entry   or we can attach it to a list of free directory entries a third alternative is to copy the last entry in the directory into the freed location and to decrease the length of the directory a linked list can also be used to decrease the time required to delete a file  the real disadvantage of a linear list of directory entries is that finding a file requires a linear search directory information is used frequently  and users will notice if access to it is slow in fact  many operating systems implement a software cache to store the most recently used directory information a cache hit avoids the need to constantly reread the information from disk a sorted list allows a binary search and decreases the average search time however  the requirement that the list be kept sorted may complicate creating and deleting files  since we may have to move substantial amounts of directory information to maintain a sorted directory a more sophisticated tree data structure  such as a b-h ee  might help here an advantage of the sorted list is that a sorted directory listing can be produced without a separate sort step  11.3.2 hash table another data structure used for a file directory is a with this method  a linear list stores the directory entries  but a hash data structure is also used the hash table takes a value computed from the file name and returns a pointer to the file name in the linear list therefore  it can greatly decrease the directory search time insertion and deletion are also fairly straightforward  although some provision must be made for collisions-situations in which two file names hash to the same location  11.4 11.4 471 the major difficulties with a hash table are its generally fixed size and the dependence of the hash function on that size for example  assume that we make a linear-probing hash table that holds 64 entries the hash function converts file names into integers from 0 to 63  for instance  by using the remainder of a division by 64 if we later try to create a 65th file  we must enlarge the directory hash table-say  to 128 entries as a result  we need a new hash function that must map file narnes to the range 0 to 127  and we must reorganize the existing directory entries to reflect their new hash-function values  alternatively  a chained-overflow hash table can be used each hash entry can be a linked list instead of an individual value  and we can resolve collisions by adding the new entry to the linked list lookups may be somewhat slowed  because searching for a name might require stepping through a linked list of colliding table entries still  this method is likely to be much faster than a linear search through the entire directory  the direct-access nature of disks allows us flexibility in the implementation of files in almost every case  many files are stored on the same disk the main problem is how to allocate space to these files so that disk space is utilized effectively and files can be accessed quickly three major methods of allocating disk space are in wide use  contiguous  linked  and indexed each method has advantages and disadvantages some systems  such as data general 's rdos for its nova line of computers  support all three more commonly  a system uses one method for all files within a file-system type  11.4.1 contiguous allocation requires that each file occupy a set of contiguous blocks on disk disk addresses define a linear ordering on the disk with this ordering  assuming that only one job is accessil1.g the disk  accessing block b + 1 after block b normally requires no head movement when head movement is needed  from the last sector of one cylil1.der to the first sector of the next cylinder   the head need only move from one track to the next thus  the number of disk seeks required for accessing contiguously allocated files is minimal  as is seek time when a seek is finally needed the ibm vm/cms operatil1.g system uses contiguous allocation because it provides such good performance  contiguous allocation of a file is defined by the disk address and length  in block units  of the first block if the file is n blocks long and starts at location b  then it occupies blocks b  b + 1  b + 2    b + n  1 the directory entry for each file indicates the address of the starting block and the length of the area allocated for this file  figure 11.5   accessing a file that has been allocated contiguously is easy for sequential access  the file system remembers the disk address of the last block referenced and  when necessary  reads the next block for direct access to block i of a file that starts at block b  we can immediately access block b + i thus  both sequential and direct access can be supported by contiguous allocation  472 chapter 11 directory file start length count 0 2 tr 14 3 mail 19 6 list 28 4 f 6 2 figure 1 i .5 contiguous allocation of disk space  contiguous allocation has some problems  however one difficulty is finding space for a new file the system chosen to manage free space determines how this task is accomplished ; these management systems are discussed in section 11.5 any management system can be used  but some are slower than others  the contiguous-allocation problem can be seen as a particular application of the general problem discussed in section 8.3  which involves to satisfy a request of size n from a list of free holes first fit and best fit are the most common strategies used to select a free hole from the set of available holes simulations have shown that both first fit and best fit are more efficient than worst fit in terms of both time and storage utilization  neither first fit nor best fit is clearly best in terms of storage utilization  but first fit is generally faster  all these algorithms suffer from the problem of as files are allocated and deleted  the free disk space is broken into pieces  external fragmentation exists whenever free space is broken into chunks it becomes a problem when the largest contiguous chunk is insufficient for a request ; storage is fragncented into a number of holes  none of which is large enough to store the data depending on the total amount of disk storage and the average file size  external fragmentation may be a minor or a major problem  one strategy for preventing loss of significant amounts of disk space to external fragmentation is to copy an entire file system onto another disk or tape the original disk is then freed completely  creating one large contiguous free space we then copy the files back onto the original disk by allocating contiguous space from this one large hole this scheme effectively all free space into one contiguous space  solving the fragmentation however  the cost of this compaction is time and it can be particularly severe for large hard disks that use contiguous allocation  where compacting all the space 11.4 473 may take hours and may be necessary on a weekly basis some systems require that this function be done with the file system unmounted during this normal system operation generally can not be permitted  so such compaction is avoided at all costs on production machines most modern systems that need defragmentation can perform it during normal system operations  but the performance penalty can be substantial  another problem with contiguous allocation is determining how much space is needed for a file when the file is created  the total amount of space it will need must be found and allocated how does the creator  program or person  know the size of the file to be created in some cases  this detennination may be fairly simple  copying an existing file  for example  ; in general  however  the size of an output file may be difficult to estimate  if we allocate too little space to a file  we may find that the file can not be extended especially with a best-fit allocation strategy  the space on both sides of the file may be in use hence  we can not make the file larger in place  two possibilities then exist first  the user program can be terminated  with an appropriate error message the user must then allocate more space and run the program again these repeated runs may be costly to prevent them  the user will normally overestimate the amount of space needed  resulting in considerable wasted space the other possibility is to find a larger hole  copy the contents of the file to the new space  and release the previous space this series of actions can be repeated as long as space exists  although it can be time consuming however  the user need never be informed explicitly about what is happening ; the system continues despite the problem  although more and more slowly  even if the total amount of space needed for a file is known in advance  preallocation may be inefficient a file that will grow slowly over a long period  months or years  must be allocated enough space for its final size  even though much of that space will be unused for a long time the file therefore has a large amount of internal fragmentation  to minimize these drawbacks  some operating systems use a modified contiguous-allocation scheme here  a contiguous chunk of space is allocated initially ; then  if that amount proves not to be large enough  another chunk of contiguous space  known as an is added the location of a file 's blocks is then recorded as a location and a block count  plus a link to the first block of the next extent on some systems  the owner of the file can set the extent size  but this setting results in inefficiencies if the owner is incorrect internal fragm.entation can still be a problem if the extents are too large  and external fragmentation can become a problem as extents of varying sizes are allocated and deallocated the commercial veritas file system uses extents to optimize performance it is a high-performance replacement for the standard unix ufs  11.4.2 linked allocation solves all problems of contiguous allocation with linked allocation  each file is a linked list of disk blocks ; the disk blocks may be scattered anywhere on the disk the directory contains a pointer to the first and last blocks of the file for example  a file of five blocks might start at block 9 and continue at block 16  then block 1  then block 10  and finally block 25  figure 11.6   each block contains a pointer to the next block these pointers 474 chapter 11 directory 12 16 170180190 20021 ~ _20_ ~ 23_0-4 ~ 2402sc.51  260270 280290300310 figure i 1.6 linked allocation of disk space  are not made available to the user thus  if each block is 512 bytes in size  and a disk address  the poileter  requires 4 bytes  then the user sees blocks of 508 bytes  to create a new file  we simply create a new entry ile the directory with linked allocation  each directory entry has a pointer to the first disk block of the file this pointer is initialized to nil  the end-of-list pointer value  to signify an empty file the size field is also set to 0 a write to the file causes the free-space management system to filed a free block  and this new block is written to and is linked to the end of the file to read a file  we simply read blocks by following the pointers from block to block there is no external fragmentation with linked allocation  and any free block on the free-space list can be used to satisfy a request the size of a file need not be declared when that file is created  a file can continue to grow as long as free blocks are available consequently  it is never necessary to compact disk space  linked allocation does have disadvantages  however the major problem is that it can be used effectively only for sequential-access files to filed the ith block of a file  we must start at the begirueing of that file and follow the pointers rnetil we get to the ith block each access to a pointer requires a disk read  and some require a disk seek consequently  it is inefficient to support a direct-access capability for linked-allocation files  another disadvantage is the space required for the pointers if a pointer requires 4 bytes out of a 512-byte block  then 0.78 percent of the disk is being used for pointers  rather than for information each file requires slightly more space than it would otherwise  the usual solution to this problem is to collect blocks into multiples  called and to allocate clusters rather than blocks for instance  the file system may define a cluster as four blocks and operate on the disk only in cluster units pointers then use a much smaller percentage of the file 's disk space  this method allows the logical-to-physical block mapping to remain simple 11.4 475 but improves disk throughput  because fewer disk-head seeks are required  and decreases the space needed for block allocation and free-list management  the cost of this approach is an increase in internal fragmentation  because more space is wasted when a cluster is partially full than when a block is partially full clusters can be used to improve the disk-access time for many other algorithms as welt so they are used in most file systems  yet another problem of linked allocation is reliability recall that the files are linked together by pointers scattered all over the disk  and consider what would happen if a pointer were lost or damaged a bug in the operating-system software or a disk hardware failure might result in picking up the wrong pointer this error could in turn result in linking into the free-space list or into another file one partial solution is to use doubly linked lists  and another is to store the file name and relative block number in each block ; however  these schemes require even more overhead for each file  an important variation on linked allocation is the use of a  fat !  this simple but efficient method of disk-space allocation is used by the ms-dos and os/2 operating systems a section of disk at the beginning of each volume is set aside to contain the table the table has one entry for each disk block and is indexed by block number the fat is used in much the same way as a linked list the directory entry contains the block number of the first block of the file the table entry indexed by that block number contains the block number of the next block in the file this chain continues until it reaches the last block  which has a special end-of-file value as the table entry  an unused block is indicated by a table value of 0 allocating a new block to a file is a simple matter of finding the first 0-valued table entry and replacing the previous end-of-file value with the address of the new block the 0 is then replaced with the end-of-file value an illustrative example is the fat structure shown in figure 11.7 for a file consisting of disk blocks 217  618  and 339  directory entry name start block 0 217 618 339  618 339 number of disk blocks -1 fat figure 11.7 file-allocation table  476 chapter 11 the fat allocation scheme can result in a significant number of disk head seeks  unless the fat is cached the disk head must move to the start of the volume to read the fat and find the location of the block in question  then move to the location of the block itself in the worst case  both moves occur for each of the blocks a benefit is that random-access time is improved  because the disk head can find the location of any block by reading the information in the fat  11.4.3 indexed allocation linked allocation solves the external-fragmentation and size-declaration problems of contiguous allocation however  in the absence of a fat  linked allocation can not support efficient direct access  since the pointers to the blocks are scattered with the blocks themselves all over the disk and must be retrieved in order solves this problem by bringil1.g all the pointers together into one location  the blo ; ct   each file has its own index block  which is an array of disk-block addresses  the i th entry in the index block points to the i 111 block of the file the directory contains the address of the index block  figure 11.8   to find and read the i 1jz block  we use the pointer in the i 1lz index-block entry this scheme is similar to the paging scheme described il1 section 8.4  when the file is created  all pointers in the index block are set to nil when the ith block is first written  a block is obtained from the free-space manage1 ~ and its address is put in the ith index-block entry  indexed allocation supports direct access  without suffering from external fragmentation  because any free block on the disk can satisfy a request for more space indexed allocation does suffer from wasted space  however the pointer overhead of the index block is generally greater than the pointer overhead of linked allocation consider a common case in which we have a file of only one or two blocks with linked allocation  we lose the space of only one pointer per directory file jeep 16 figure 11.8 indexed allocation of disk space  11.4 allocation methods 477 block with indexed allocation  an entire index block must be allocated  even if only one or two pointers will be non-nil  this point raises the question of how large the index block should be every file must have an index block  so we want the index block to be as small as possible if the index block is too small  however  it will not be able to hold enough pointers for a large file  and a mechanism will have to be available to deal with this issue mechanisms for this purpose include the following  c linked scheme an index block is normally one disk block thus  it can be read and written directly by itself to allow for large files  we can link together several index blocks for example  an index block might contain a small header giving the name of the file and a set of the first 100 disk-block addresses the next address  the last word in the index block  is nil  for a small file  or is a pointer to another index block  for a large file   multilevel index a variant of linked representation uses a first-level index block to point to a set of second-level index blocks  which in tum point to the file blocks to access a block  the operating system uses the first-level index to find a second-level index block and then uses that block to find the desired data block this approach could be continued to a third or fourth level  depending on the desired maximum file size with 4,096-byte blocks  we could store 1,024 four-byte pointers in an index block two levels of indexes allow 1,048,576 data blocks and a file size of up to 4gb  combined scheme another alternative  used in the ufs  is to keep the first  say  15 pointers of the index block in the file 's inode the first 12 of these pointers point to direct blocks ; that is  they contain addresses of blocks that contain data of the file thus  the data for small files  of no more than 12 blocks  do not need a separate index block if the block size is 4 kb  then up to 48 kb of data can be accessed directly the next three pointers point to indirect blocks the first points to a single indirect block  which is an index block containing not data but the addresses of blocks that do contain data the second points to a double indirect block  which contains the address of a block that contains the addresses of blocks that contain pointers to the actual data blocks the last pointer contains the address of a triple indirect block under this method  the number of blocks that can be allocated to a file exceeds the amount of space addressable by the four-byte file pointers used by many operating systems a 32-bit file pointer reaches only 232 bytes  or 4gb many unix implementations  including solaris and ibm 's aix  now support up to 64-bit file pointers pointers of this size allow files and file systems to be terabytes in size a unix inode is shown in figure 11.9  indexed-allocation schemes suffer from some of the same performance problems as does linked allocation specifically  the index blocks can be cached in memory  but the data blocks may be spread all over a volume  11.4.4 performance the allocation methods that we have discussed vary in their storage efficiency and data-block access times both are important criteria in selecting the proper method or methods for an operating system to implement  478 chapter 11 implementing file systems figure 11.9 the unix inode  before selecting an allocation method  we need to determine how the systems will be used a system with mostly sequential access should not use the same method as a system with mostly random access  for any type of access  contiguous allocation requires only one access to get a disk block since we can easily keep the initial address of the file in memory  we can calculate immediately the disk address of the ith block  or the next block  and read it directly  for linked allocation  we can also keep the address of the next block in memory and read it directly this method is fine for sequential access ; for direct access  however  an access to the ith block might require i disk reads this problem indicates why linked allocation should not be used for an application requiring direct access  as a result  some systems support direct-access files by using contiguous allocation and sequential-access files by using linked allocation for these systems  the type of access to be made must be declared when the file is created a file created for sequential access will be linked and can not be used for direct access a file created for direct access will be contiguous and can support both direct access and sequential access  but its maximum length must be declared when it is created in this case  the operating system must have appropriate data structures and algorithms to support both allocation methods  files can be converted from one type to another by the creation of a new file of the desired type  into which the contents of the old file are copied the old file may then be deleted and the new file renamed  indexed allocation is more complex if the index block is already in memory  then the access can be made directly however  keeping the index block in memory requires considerable space if this memory space is not available  then we may have to read first the index block and then the desired data block for a two-level index  two index-block reads might be necessary for an 11.5 11.5 479 extremely large file  accessing a block near the end of the file would require reading in all the index blocks before the needed data block finally could be read thus  the performance of indexed allocation depends on the index structure  on the size of the file  and on the position of the block desired  some systems combine contiguous allocation with indexed allocation by using contiguous allocation for small files  up to three or four blocks  and automatically switching to an indexed allocation if the file grows large since most files are small  and contiguous allocation is efficient for small files  average performance can be quite good  for instance  the version of the unix operating system from sun microsystems was changed in 1991 to improve performance in the file-system allocation algorithm the performance measurements indicated that the maximum disk throughput on a typical workstation  a 12-mips sparcstation1  took 50 percent of the cpu and produced a disk bandwidth of only 1.5 me per second to improve performance  sun made changes to allocate space in clusters of 56 kb whenever possible  56 kb was the maximum size of a dma transfer on sun systems at that time   this allocation reduced external fragmentation  and thus seek and latency times in addition  the disk-reading routines were optimized to read in these large clusters the inode structure was left unchanged as a result of these changes  plus the use of read-ahead and free-behind  discussed in section 11.6.2   25 percent less cpu was used  and throughput substantially improved  many other optimizations are in use given the disparity between cpu speed and disk speed  it is not unreasonable to add thousands of extra instructions to the operating system to save just a few disk-head movements  furthermore  this disparity is increasing over time  to the point where hundreds of thousands of instructions reasonably could be used to optimize head movements  since disk space is limited  we need to reuse the space from deleted files for new files  if possible  write-once optical disks only allow one write to any given sector  and thus such reuse is not physically possible  to keep track of free disk space  the system maintains a the free-space list records all free disk blocks-those not allocated to some file or directory to create a file  we search the free-space list for the required amount of space and allocate that space to the new file this space is then removed from the free-space list when a file is deleted  its disk space is added to the free-space list the free-space list  despite its name  might not be implemented as a list  as we discuss next  11.5.1 bit vector frequently  the free-space list is implemented as a or each block is represented by 1 bit if the block is free  the bit is 1 ; if the block is allocated  the bit is 0  for example  consider a disk where blocks 2  3  4  5  8  9  10  11  12  13  17  18  25  26  and 27 are free and the rest of the blocks are allocated the free-space bit map would be 480 chapter 11 001111001111110001100000011100000   the main advantage of this approach is its relative simplicity and its efficiency in finding the first free block or n consecutive free blocks on the disk indeed  many computers supply bit-manipulation instructions that can be used effectively for that purpose for example  the intel family starting with the 80386 and the motorola family starting with the 68020 have instructions that return the offset in a word of the first bit with the value 1  these processors have powered pcs and macintosh systems  respectively   one technique for finding the first free block on a system that uses a bit-vector to allocate disk space is to sequentially check each word in the bit map to see whether that value is not 0  since a 0-valued word contains only 0 bits and represents a set of allocated blocks the first non-0 word is scanned for the first 1 bit  which is the location of the first free block the calculation of the block number is  number of bits per word  x  number of 0-value words  + offset of first 1 bit  again  we see hardware features driving software functionality unfortunately  bit vectors are inefficient unless the entire vector is kept in main memory  and is written to disk occasionally for recovery needs   keeping it in main memory is possible for smaller disks but not necessarily for larger ones  a 1.3-gb disk with 512-byte blocks would need a bit map of over 332 kb to track its free blocks  although clustering the blocks in groups of four reduces this number to around 83 kb per disk a 1-tb disk with 4-kb blocks requires 32 mb to store its bit map given that disk size constantly increases  the problem with bit vectors will continue to escalate a 1-pb file system would take a 32-gb bitmap just to manage its free space  11.5.2 linked list another approach to free-space management is to link together all the free disk blocks  keeping a pointer to the first free block in a special location on the disk and caching it in memory this first block contains a pointer to the next free disk block  and so on recall our earlier example  section 11.5.1   in which blocks 2  3  4  5  8  9  10  11  12  13  17  18  25  26  and 27 were free and the rest of the blocks were allocated in this situation  we would keep a pointer to block 2 as the first free block block 2 would contain a pointer to block 3  which would point to block 4  which would point to block 5  which would point to block 8  and so on  figure 11.10   this scheme is not efficient ; to traverse the list  we must read each block  which requires substantial i/0 time fortunately  however  traversing the free list is not a frequent action usually  the operating system simply needs a free block so that it can allocate that block to a file  so the first block in the free list is used the fat method incorporates free-block accounting into the allocation data structure no separate method is needed  11.5.3 grouping a modification of the free-list approach stores the addresses of n free blocks in the first free block the first n-1 of these blocks are actually free the last block contains the addresses of another n free blocks  and so on the addresses 11.5 481 figure 11.10 linked free-space list on disk  of a large number of free blocks can now be found quickly  unlike the situation when the standard linked-list approach is used  11.5.4 counting another approach takes advantage of the fact that  generally  several contiguous blocks may be allocated or freed simultaneously  particularly when space is allocated with the contiguous-allocation algorithm or through clustering thus  rather than keeping a list of n free disk addresses  we can keep the address of the first free block and the number  n  of free contiguous blocks that follow the first block each entry in the free-space list then consists of a disk address and a count although each entry requires more space than would a simple disk address  the overall list is shorter  as long as the count is generally greater than 1 note that this method of tracking free space is similar to the extent method of allocating blocks these entries can be stored in a b-tree  rather than a linked list for efficient lookup  insertion  and deletion  11.5.5 space maps sun 's zfs file system was designed to encompass huge numbers of files  directories  and even file systems  in zfs  we can create file-system hierarchies   the resulting data structures could have been large and inefficient if they had not been designed and implemented properly on these scales  metadata i/0 can have a large performance impact conside1 ~ for example  that if the freespace list is implemented as a bit map  bit maps must be modified both when blocks are allocated and when they are freed freeing 1gb of data on a 1-tb disk could cause thousands of blocks of bit maps to be updated  because those data blocks could be scattered over the entire disk  482 chapter 11 11.6 zfs uses a combination of techniques in its free-space managem.ent algorithm to control the size of data structures and minimize the i/0 needed to manage those structures first  zfs creates to divide the space on the device into chucks of manageable size a given volume may contain hundreds of metaslabs each metaslab has an associated space map zfs uses the counting algorithm to store information about free blocks rather than write count structures to disk  it uses log-structured file system techniques to record them the space map is a log of all block activity  allocatil g and freemg   in time order  in countil g format when zfs decides to allocate or free space from a metaslab  it loads the associated space map into memory in a balanced-tree structure  for very efficient operation   indexed by offset  and replays the log into that structure the in-memory space map is then an accurate representation of the allocated and free space in the metaslab zfs also condenses the map as much as possible by combining contiguous free blocks into a sil gle entry finally  the free-space list is updated on disk as part of the transaction-oriented operations of zfs during the collection and sortmg phase  block requests can still occur  and zfs satisfies these requests from the log in essence  the log plus the balanced tree is the free list  now that we have discussed various block-allocation and directorymanagement options  we can further consider their effect on performance and efficient disk use disks tend to represent a major bottleneck in system performance  since they are the slowest main computer component in this section  we discuss a variety of techniques used to improve the efficiency and performance of secondary storage  11.6.1 efficiency the efficient use of disk space depends heavily on the disk allocation and directory algorithms in use for instance  unix inodes are preallocated on a volume even an empty disk has a percentage of its space lost to inodes  however  by preallocating the inodes and spreading them across the volume  we improve the file system 's performance this improved performance results from the unix allocation and free-space algorithms  which try to keep a file 's data blocks near that file 's inode block to reduce seek time  as another example  let 's reconsider the clustermg scheme discussed in section 11.4  which aids in file-seek and file-transfer performance at the cost of internal fragmentation to reduce this fragmentation  bsd unix varies the cluster size as a file grows large clusters are used where they can be filled  and small clusters are used for small files and the last cluster of a file this system is described in appendix a  the types of data normally kept in a file 's directory  or inode  entry also require consideration commonly  a last write date is recorded to supply information to the user and to determine whether the file needs to be backed up some systems also keep a last access date  so that a user can determine when the file was last read the result of keeping this information is that  whenever the file is read  a field in the directory structure must be written 11.6 483 to that means the block must be read into memory  a section changed  and the block written back out to disk  because operations on disks occur only in block  or cluster  chunks so any time a file is opened for reading  its directory entry must be read and written as well this requirement can be inefficient for frequently accessed files  so we must weigh its benefit against its performance cost when designing a file system generally  every data item associated with a file needs to be considered for its effect on efficiency and performance  as an example  consider how efficiency is affected by the size of the pointers used to access data most systems use either 16 or 32-bit pointers throughout the operating system these pointer sizes limit the length of a file to either 216  64 kb  or 232 bytes  4 gb   some systems implement 64-bit pointers to increase this limit to 264 bytes  which is a very large number indeed however  64-bit pointers take more space to store and in turn make the allocation and free-space-management methods  linked lists  indexes  and so on  use more disk space  one of the difficulties in choosing a pointer size  or indeed any fixed allocation size within an operating system  is planning for the effects of changing technology consider that the ibm pc xt had a 10-mb hard drive and an ms-dos file system that could support only 32 mb  each fat entry was 12 bits  pointing to an 8-kb cluster  as disk capacities increased  larger disks had to be split into 32-mb partitions  because the file system could not track blocks beyond 32mb as hard disks with capacities of over 100mb became common  the disk data structures and algorithms in ms-dos had to be modified to allow larger file systems  each fat entry was expanded to 16 bits and later to 32 bits  the initial file-system decisions were made for efficiency reasons ; however  with the advent of ms-dos version 4  millions of computer users were inconvenienced when they had to switch to the new  larger file system sun 's zfs file system uses 128-bit pointers  which theoretically should never need to be extended  the minimum mass of a device capable of storing 2128 bytes using atomic-level storage would be about 272 trillion kilograms  as another example  consider the evolution of sun 's solaris operating system originally  many data structures were of fixed length  allocated at system startup these structures included the process table and the open-file table when the process table became full  no more processes could be created  when the file table became full  no more files could be opened the system would fail to provide services to users table sizes could be increased only by recompiling the kernel and rebooting the system since the release of solaris 2  almost all kernel structures have been allocated dynamically  eliminating these artificial limits on system performance of course  the algorithms that manipulate these tables are more complicated  and the operating system is a little slower because it must dynamically allocate and deallocate table entries ; but that price is the usual one for more general functionality  11.6.2 performance even after the basic file-system algorithms have been selected  we can still improve performance in several ways as will be discussed in chapter 13  most disk controllers include local memory to form an on-board cache that is large enough to store entire tracks at a time once a seek is performed  the track is read into the disk cache starting at the sector under the disk head 484 chapter 11 1/0 using read   and write   tile system figure 11.11 1/0 without a unified buffer cache   reducing latency time   the disk controller then transfers any sector requests to the operating system once blocks make it from the disk controller into main memory  the operating system may cache the blocks there  some systems maintain a separate section of main memory for a where blocks are kept under the assumption that will be used again shortly other systems cache file data using a the page cache uses virtual memory techniques to cache file data as pages rather than as file-system-oriented blocks cachii lg file data using virtual addresses is far more efficient than caching through physical disk blocks  as accesses interface with virtual memory rather than the file system several systems-including solaris  linux  and windows nt  2000  and xp-use caching to cache both process pages and file data this is known as some versions of unix and linux provide a to illustrate the benefits of the unified buffer cache  consider the two alternatives for opening and accessing a file one approach is to use memory mapping  section 9.7  ; the second is to use the standard system calls read   and write    without a unified buffer cache  we have a situation similar to figure 11.11 here  the read   and write   system calls go through the buffer cache  the memory-mapping call  however  requires using two caches-the page cache and the buffer cache a memory mapping proceeds by reading in disk blocks from the file system and storing them in the buffer cache because the virtual memory system does not interface with the buffer cache  the contents of the file in the buffer cache must be copied into the page cache this situation is known as and requires caching file-system data twice not only does it waste memory but it also wastes significant cpu and i/o cycles due to the extra data movement within system memory in addition  inconsistencies between the two caches can result in corrupt files in contrast  when a unified buffer cache is provided  both memory mapping and the read   and write   system calls use the same page cache this has the benefit of a voiding double 11.6 485 memory-mapped 1/0 buffer cache file system figure 11.12 1/0 using a unified buffer cache  caching  and it allows the virtual memory system to manage file-system data  the unified buffer cache is shown in figure 11.12  regardless of whether we are caching disk blocks or pages  or both   lru  section 9.4.4  seems a reasonable general-purpose algorithm for block or page replacement however  the evolution of the solaris page-caching algorithms reveals the difficulty in choosil1.g an algorithm solaris allows processes and the page cache to share unused memory versions earlier than solaris 2.5.1 made no distmction between allocatmg pages to a process and allocating them to the page cache as a result  a system performing many i/0 operations used most of the available memory for caching pages because of the high rates of i/0  the page scanner  section 9.10.2  reclaimed pages from processesrather than from the page cache-when free memory ran low solaris 2.6 and solaris 7 optionally implemented priority paging  in which the page scanner gives priority to process pages over the page cache solaris 8 applied a fixed limit to process pages and the file-system page cache  preventing either from forcing the other out of memory solaris 9 and 10 again changed the algorithms to maximize memory use and mmimize thrashing  another issue that can affect the performance of i/0 is whether writes to the file system occur synchronously or asynchronously  occur in the order in which the disk subsystem receives and the writes are not buffered thus  the calling routine must wait for the data to reach the disk drive before it can proceed in an the data are stored in the cache  and control returns to the caller asynchronous writes are done the majority of the time however  metadata writes  among others  can be synchronous operating systems frequently include a flag in the open system call to allow a process to request that writes be performed synchxonously for example  databases use this feature for atomic transactions  to assure that data reach stable storage in the required order  some systems optimize their page cache by using different replacement algorithms  depending on the access type of the file a file being read or written sequentially should not have its pages replaced in lru order  because the most recently used page will be used last  or perhaps never again instead  sequential access can be optimized by techniques known as free-behind and read-ahead removes a page from the buffer as soon as the next 486 chapter 11 11.7 page is requested the previous are not likely to be used again and waste buffer space with a requested page and several subsequent pages are read and cached these pages are likely to be requested after the current page is processed retrieving these data from the disk in one transfer and caching them saves a considerable ancount of time one might think that a track cache on the controller would elincinate the need for read-ahead on a multiprogrammed system however  because of the high latency and overhead involved in making many small transfers from the track cache to main memory  performing a read-ahead remains beneficial  the page cache  the file system  and the disk drivers have some interesting interactions when data are written to a disk file  the pages are buffered in the cache  and the disk driver sorts its output queue according to disk address  these two actions allow the disk driver to minimize disk-head seeks and to write data at times optimized for disk rotation unless synchronous writes are required  a process writing to disk simply writes into the cache  and the system asynchronously writes the data to disk when convenient the user process sees very fast writes when data are read from a disk file  the block i/0 system does some read-ahead ; however  writes are much more nearly asynchronous than are reads thus  output to the disk through the file system is often faster than is input for large transfers  counter to intuition  files and directories are kept both in main memory and on disk  and care must be taken to ensure that a system failure does not result in loss of data or in data inconsistency we deal with these issues in this section as well as how a system can recover from such a failure  a system crash can cause inconsistencies among on-disk file-system data structures  such as directory structures  free-block pointers  and free fcb pointers many file systems apply changes to these structures in place a typical operation  such as creating a file  can involve many structural changes within the file system on the disk directory structures are modified  fcbs are allocated  data blocks are allocated  and the free counts for all of these blocks are decreased these changes can be interrupted by a crash  and inconsistencies among the structures can result for example  the free fcb count might indicate that an fcb had been allocated  but the directory structure might not point to the fcb compounding this problem is the caching that operating systems do to optimize i/0 performance some changes may go directly to disk  while others may be cached if the cached changes do not reach disk before a crash occurs  more corruption is possible  in addition to crashes  bugs in file-system implementation  disk controllers  and even user applications can corrupt a file system file systems have varying methods to deal with corruption  depending on the file-system data structures and algorithms we deal with these issues next  11.7.1 consistency checking whatever the cause of corruption  a file system must first detect the problems and then correct them for detection  a scan of all the metadata on each file 11.7 487 system can confirm or deny the consistency of the systenl unfortunately  this scan can take minutes or hours and should occur every time the system boots  alternatively  a file system can record its state within the file-system metadata  at the start of any metadata change  a status bit is set to indicate that the metadata is in flux if all updates to the metadata complete successfully  the file system can clear that bit it however  the status bit remains set  a consistency checker is run  the systems program such as f s ck in unix or chkdsk in windows-compares the data in the directory structure with the data blocks on disk and tries to fix any inconsistencies it finds the allocation and free-space-management algorithms dictate what types of problems the checker can find and how successful it will be in fixing them for instance  if linked allocation is used and there is a link from any block to its next block  then the entire file can be reconstructed from the data blocks  and the directory structure can be recreated in contrast the loss of a directory entry on an indexed allocation system can be disastrous  because the data blocks have no knowledge of one another for this reason  unix caches directory entries for reads ; but any write that results in space allocation  or other metadata changes  is done synchronously  before the corresponding data blocks are written of course  problems can still occur if a synchronous write is interrupted by a crash  11.7.2 log-structured file systems computer scientists often fin.d that algorithms and technologies origil1.ally used in one area are equally useful in other areas such is the case with the database log-based recovery algorithms described in section 6.9.2 these logging algorithms have been applied successfully to the of consistency '-.il'c' .ll the resulting implementations are known as  or file systems  note that with the consistency-checking approach discussed in the preceding section  we essentially allow structures to break and repair them on recovery however  there are several problems with this approach one is that the inconsistency may be irreparable the consistency check may not be able to recover the structures  resulting in loss of files and even entire directories  consistency checking can require human intervention to resolve conflicts  and that is inconvenient if no human is available the system can remain unavailable until the human tells it how to proceed consistency checking also takes system and clock time to check terabytes of data  hours of clock time may be required  the solution to this problem is to apply log-based recovery techniques to file-system metadata updates both ntfs and the veritas file system use this method  and it is included in recent versions of ufs on solaris in fact it is becoming common on many operating systems  fundamentally  all metadata changes are written each set of operations for performing a specific task is a the changes are written to this log  they are considered to be committed  and the system call can return to the user process  allowing it to continue execution meanwhile  these log entries are replayed across the actual filesystem structures as the changes are made  a pointer is updated to indicate 488 chapter 11 which actions have completed and which are still incomplete when an entire committed transaction is completed  it is removed from the log file  which is actually a circular buffer a cb  ul ; n writes to the end of its space and then continues at the beginning  overwriting older values as it goes we would not want the buffer to write over data that had not yet been saved  so that scenario is avoided the log may be in a separate section of the file system or even on a separate disk spindle it is more efficient  but more complex  to have it under separate read and write heads  thereby decreasing head contention and seek times  if the system crashes  the log file will contain zero or more transactions  any transactions it contains were not completed to the file system  even though they were committed by the operating system  so they must now be completed  the transactions can be executed from the pointer until the work is complete so that the file-system structures remain consistent the only problem occurs when a transaction was aborted -that is  was not committed before the system crashed any changes from such a transaction that were applied to the file system must be undone  again preserving the consistency of the file system  this recovery is all that is needed after a crash  elimil ating any problems with consistency checking  a side benefit of using logging on disk metadata updates is that those updates proceed much faster than when they are applied directly to the on-disk data structures the reason for this improvement is found in the performance advantage of sequential i/0 over random i/0 the costly synchronous random meta data writes are turned into much less costly synchronous sequential writes to the log-structured file system 's loggil g area those changes in turn are replayed asynchronously via random writes to the appropriate structures  the overall result is a significant gain in performance of metadata-oriented operations  such as file creation and deletion  11.7.3 other solutions another alternative to consistency checking is employed by network appliance 's wafl file system and sun 's zfs file system these systems never overwrite blocks with new data rather  a transaction writes all data and metadata changes to new blocks when the transaction is complete  the metadata structures that pointed to the old versions of these blocks are updated to point to the new blocks the file system can then remove the old pointers and the old blocks and make them available for reuse if the old pointers and blocks are kept  a is created ; the snapshot is a view of the file system before the last update took place this solution should require no consistency checking if the pointer update is done atomically wafl does have a consistency checke1 ~ however  so some failure scenarios can still cause metadata corruption  see 11.9 for details of the wafl file system  sun 's zfs takes an even more im ovative approach to disk consistency  it never overwrites blocks  just as is the case with wafl however  zfs goes further and provides check-summing of all metadata and data blocks this solution  when combined with raid  assures that data are always correct zfs therefore has no consistency checker  more details on zfs are found in section 12.7.6  11.7 489 11.7.4 backup and restore magnetic disks sometimes fail  and care must be taken to ensure that the data lost in such a failure are not lost forever to this end  system programs can be used to data from disk to another storage device  such as a floppy disk  magnetic tape  optical disk  or other hard disk recovery from the loss of an individual file  or of an entire disk  may then be a matter of the data from backup  to minimize the copying needed  we can use information from each file 's directory entry for instance  if the backup program knows when the last backup of a file was done  and the file 's last write date in the directory indicates that the file has not changed since that date  then the file does not need to be copied again a typical backup schedule may then be as follows  1 copy to a backup medium all files from the disk this is called a to another medium all files changed since day 1 this is an day 3 copy to another medium all files changed since day 2  day n copy to another medium all files changed since day n-1 then go back to day 1  the new cycle can have its backup written over the previous set or onto a new set of backup media in this manner  we can restore an entire disk by starting restores with the full backup and continuing through each of the incremental backups of course  the larger the value of n  the greater the number of media that must be read for a complete restore an added advantage of this backup cycle is that we can restore any file accidentally deleted during the cycle by retrieving the deleted file from the backup of the previous day the length of the cycle is a compromise between the amount of backup medium needed and the number of days back from which a restore can be done to decrease the number of tapes that must be read to do a restore  an option is to perform a full backup and then each day back up all files that have changed since the full backup in this way  a restore can be done via the most recent incremental backup and the full backup  with no other incremental backups needed the trade-off is that more files will be modified each day  so each successive incremental backup involves more files and more backup media  a user ncay notice that a particular file is missing or corrupted long after the damage was done for this reason  we usually plan to take a full backup from time to time that will be saved forever it is a good idea to store these permanent backups far away from the regular backups to protect against hazard  such as a fire that destroys the computer and all the backups too  and if the backup cycle reuses media  we must take care not to reuse the 490 chapter 11 11.8 media too many times-if the media wear out  it might not be possible to restore any data from the backups  network file systems are commonplace they are typically integrated with the overall directory structure and interface of the client system nfs is a good example of a widely used  well-implemented client-server network file system here  we use it as an example to explore the implementation details of network file systems  nfs is both an implementation and a specification of a software system for accessing remote files across lans  or even wans   nfs is part of onc +  which most unix vendors and some pc operating systems support the implementation described here is part of the solaris operating system  which is a modified version of unix svr4 running on sun workstations and other hardware it uses either the tcp or udp /ip protocol  depending on the interconnecting network   the specification and the implementation are intertwined in our description of nfs whenever detail is needed  we refer to the sun implementation ; whenever the description is general  it applies to the specification also  there are multiple versions of nfs  with the latest being version 4 here  we describe version 3  as that is the one most commonly deployed  11.8.1 overview nfs views a set of interconnected workstations as a set of independent machines with independent file systems the goal is to allow some degree of sharing among these file systems  on explicit request  in a transparent manner sharing is based on a client-server relationship a machine may be  and often is  both a client and a server sharing is allowed between any pair of machines to ensure machine independence  sharing of a remote file system affects only the client machine and no other machine  so that a remote directory will be accessible in a transparent manner from a particular machine-say  from ml-a client of that machine must first carry out a mount operation the semantics of the operation involve mounting a remote directory over a directory of a local file system once the mount operation is completed  the mounted directory looks like an integral subtree of the local file system  replacing the subtree descending from the local directory the local directory becomes the name of the root of the newly mounted directory specification of the remote directory as an argument for the mount operation is not done transparently ; the location  or host name  of the remote directory has to be provided however  fron l then on  users on machine ml can access files in the remote directory in a totally transparent manner  to illustrate file mounting  consider the file system depicted in figure 11.13  where the triangles represent subtrees of directories that are of interest the figure shows three independent file systems of machines named u  51  and 52 at this point  on each machine  only the local files can be accessed figure 11.14  a  shows the effects of mounting 81  /usr/shared over u  /usr/local  this figure depicts the view users on u have of their file system notice that after the mount is complete  they can access any file within the dirl directory 11.8 491 u  s1  s2  usr usr usr figure 11.13 three independent file systems  using the prefix /usr /local/ dir1 the original directory /usr /local on that machine is no longer visible  subject to access-rights accreditation  any file system  or any directory within a file system  can be mounted remotely on top of any local directory  diskless workstations can even mount their own roots from servers cascading mounts are also permitted in some nfs implementations that is  a file system can be mounted over another file system that is remotely mounted  not local a machine is affected by only those mounts that it has itself invoked mounting a remote file system does not give the client access to other file systems that were  by chance  mounted over the former file system thus  the mount mechanism does not exhibit a transitivity property  in figure 11.14  b   we illustrate cascading mounts the figure shows the result of mounting s2  /usr /dir2 over u  /usr/local/dir1  which is already remotely mounted from 51 users can access files within dir2 on u using the u  u   a   b  figure 11.14 mounting in nfs  a  mounts  b  cascading mounts  492 chapter 11 prefix /usr/local/dir1 if a shared file system is mounted over a user 's home directories on all machines in a network  the user can log into any workstation and get his honce environment this property permits one of the design goals of nfs was to operate in a heterogeneous environment of different machines  operating systems  and network architectures  the nfs specification is independent of these media and thus encourages other implementations this independence is achieved through the use of rpc primitives built on top of an external data representation  xdr  protocol used between two implementation-independent interfaces hence  if the system consists of heterogeneous machines and file systems that are properly interfaced to nfs  file systems of different types can be mounted both locally and remotely  the nfs specification distinguishes between the services provided by a mount mechanism and the actual remote-file-access services accordingly  two separate protocols are specified for these services  a mount protocol and a protocol for remote file accesses  the the protocols are specified as sets of rpcs these rpcs are the building blocks used to implement transparent remote file access  11.8.2 the mount protocol the establishes the initial logical connection between a server and a client in sun 's implementation  each machine has a server process  outside the kernel  performing the protocol functions  a mount operation includes the name of the remote directory to be mounted and the name of the server machine storing it the mount request is mapped to the corresponding rpc and is forwarded to the mount server running on the specific server machine the server maintains an that specifies local file systems that it exports for mounting  along with names of machines that are permitted to mount them  in solaris  this list is the i etc/dfs/dfstab  which can be edited only by a superuser  the specification can also include access rights  such as read only to simplify the maintenance of export lists and mount tables  a distributed naming scheme can be used to hold this information and make it available to appropriate clients  recall that any directory within an exported file system can be mounted remotely by an accredited machine a component unit is such a directory when the server receives a mount request that conforms to its export list  it returns to the client a file handle that serves as the key for further accesses to files within the mounted file system the file handle contains all the information that the server needs to distinguish an individual file it stores in unix terms  the file handle consists of a file-system identifier and an inode number to identify the exact mounted directory within the exported file system  the server also maintains a list of the client machines and the corresponding currently mounted directories this list is used mainly for administrative purposes-for instance  for notifying all clients that the server is going down  only through addition and deletion of entries in this list can the server state be affected by the mount protocol  usually  a system has a static mounting preconfiguration that is established at boot time  i etc/vfstab in solaris  ; however  this layout can be modified in 11.8 493 addition to the actual mount procedure  the mount protocol includes several other procedures  such as unmount and return export list  11.8.3 the nfs protocol the nfs protocol provides a set of rpcs for remote file operations the procedures support the following operations  searching for a file within a directory reading a set of directory entries manipulating links and directories accessing file attributes reading and writing files these procedures can be invoked only after a file handle for the remotely mounted directory has been established  the omission of open   and close   operations is intentional a prominent feature of nfs servers is that they are stateless servers do not maintain information about their clients from one access to another no parallels to unix 's open-files table or file structures exist on the server side consequently  each request has to provide a full set of arguments  including a unique file identifier and an absolute offset inside the file for the appropriate operations  the resulting design is robust ; no special measures need be taken to recover a server after a crash file operations must be idempotent for this purpose  every nfs request has a sequence number  allowing the server to determine if a request is duplicated or if any are missing  maintaining the list of clients that we mentioned seems to violate the statelessness of the server howeve1 ~ this list is not essential for the correct operation of the client or the server  and hence it does not need to be restored after a server crash consequently  it might include inconsistent data and is treated as only a hint  a further implication of the stateless-server philosophy and a result of the synchrony of an rpc is that modified data  including indirection and status blocks  must be committed to the server 's disk before results are returned to the client that is  a client can cache write blocks  but when it flushes them to the server  it assumes that they have reached the server 's disks the server must write all nfs data synchronously thus  a server crash and recovery will be invisible to a client ; all blocks that the server is managing for the client will be intact the consequent performance penalty can be large  because the advantages of caching are lost performance can be increased using storage with its own nonvolatile cache  usually battery-backed-up memory   the disk controller ackiwwledges the disk write when the write is stored in the nonvolatile cache in essence  the host sees a very fast synchronous write  these blocks remain intact even after system crash and are written from this stable storage to disk periodically  a single nfs write procedure call is guaranteed to be atomic and is not intermixed with other write calls to the same file the nfs protocol  however  does not provide concurrency-control mechanisms a write   system call may 494 chapter 11 client server figure 11.15 schematic view of the nfs architecture  be broken down into several rpc writes  because each nfs write or read call can contain up to 8 kb of data and udp packets are limited to 1,500 bytes as a result  two users writing to the same remote file may get their data intermixed  the claim is that  because lock management is inherently stateful  a service outside the nfs should provide locking  and solaris does   users are advised to coordinate access to shared files using mechanisms outside the scope of nfs  nfs is integrated into the operating system via a vfs as an illustration of the architecture  let 's trace how an operation on an already open remote file is handled  follow the example in figure 11.15   the client initiates the operation with a regular system call the operating-system layer maps this call to a vfs operation on the appropriate vnode the vfs layer identifies the file as a remote one and invokes the appropriate nfs procedure an rpc call is made to the nfs service layer at the remote server this call is reinjected to the vfs layer on the remote system  which finds that it is local and invokes the appropriate file-system operation this path is retraced to return the result  an advantage of this architecture is that the client and the server are identical ; thus  a machine may be a client  or a server  or both the actual service on each server is performed by kernel threads  11.8.4 path-name translation in nfs involves the parsing of a path name such as /usr/local/dir1/file txt into separate directory entries  or components   1  usr   2  local  and  3  dir1 path-name translation is done by breaking the path into component names and perform.ing a separate nfs lookup call for every pair of component name and directory vnode once a n10unt point is crossed  every component lookup causes a separate rpc to the server this 11.8 495 expensive path-name-traversal scheme is needed  since the layout of each client 's logical name space is unique  dictated by the mounts the client has performed it would be itluch more efficient to hand a server a path name and receive a target vnode once a mount point is encountered at any point  however  there might be another mount point for the particular client of whicb the stateless server is unaware  so that lookup is fast  a directory-name-lookup cache on the client side holds the vnodes for remote directory names this cache speeds up references to files with the same initial path name the directory cache is discarded when attributes returned from the server do not match the attributes of the cached vnode  recall that mounting a remote file system on top of another already mounted remote file system  a cascading mount  is allowed in some implementations of nfs however  a server can not act as an intermediary between a client and another server instead  a client must establish a direct client-server com1ection with the second server by directly mounting the desired directory  when a client has a cascading mount  more than one server can be involved in a path-name traversal however  each component lookup is performed between the original client and some server therefore  when a client does a lookup on a directory on which the server has mounted a file system  the client sees the underlying directory instead of the mounted directory  11.8.5 remote operations with the exception of opening and closing files  there is almost a one-to-one correspondence between the regular unix system calls for file operations and the nfs protocol rpcs thus  a remote file operation can be translated directly to the corresponding rpc conceptually  nfs adheres to the remote-service paradigm ; but in practice  buffering and caching techniques are employed for the sake of performance no direct correspondence exists between a remote operation and an rpc instead  file blocks and file attributes are fetched by the rpcs and are cached locally future remote operations use the cached data  subject to consistency constraints  there are two caches  the file-attribute  inode-infonnation  cache and the file-blocks cache when a file is opened  the kernel checks with the remote server to determine whether to fetch or revalidate the cached attributes the cached file blocks are used only if the corresponding cached attributes are up to date the attribute cache is updated whenever new attributes arrive from the server cached attributes are  by default  discarded after 60 seconds both read-ahead and delayed-write techniques are used between the server and the client clients do not free delayed-write blocks until the server confirms that the data have been written to disk delayed-write is retained even when a file is opened concurrently  in conflicting modes hence  unix semantics  section 10.5.3.1  are not preserved  tuning the system for performance makes it difficult to characterize the consistency semantics of nfs new files created on a machine may not be visible elsewhere for 30 seconds furthermore  writes to a file at one site may or may not be visible at other sites that have this file open for reading new opens of a file observe only the changes that have already been flushed to the server thus  nfs provides neither strict emulation of unix semantics nor the 496 chapter 11 11.9 session sen antics of andrew  section 10.5.3.2  .ln spite of these drawbacks  the utility and good performance of the mechanism make it the most widely used multi-vendor-distributed system in operation  disk i/o has a huge impact on system performance as a result  file-system design and implementation command quite a lot of attention from system designers some file systems are general purpose  in that they can provide reasonable performance and functionality for a wide variety of file sizes  file types  and i/0 loads others are optimized for specific tasks in an attempt to provide better performance in those areas than general-purpose file systems  the wafl file system from network appliance is an example of this sort of optimization wafl  the write-anywhere file layout  is a powerful  elegant file system optimized for random writes  wafl is used exclusively on network file servers produced by network appliance and so is meant for use as a distributed file system it can provide files to clients via the nfs  cifs  ftp  and http protocols  although it was designed just for nfs and cifs when many clients use these protocols to talk to a file server  the server may see a very large demand for random reads and an even larger demand for random writes the nfs and cifs protocols cache data from read operations  so writes are of the greatest concern to file-server creators  wafl is used on file servers that include an nvram cache for writes  the wafl designers took advantage of running on a specific architecture to optimize the file system for random i/0  with a stable-storage cache in front  ease of use is one of the guiding principles of wafl  because it is designed to be used in an appliance its creators also designed it to include a new snapshot functionality that creates multiple read-only copies of the file system at different points in time  as we shall see  the file system is similar to the berkeley fast file system  with many modifications it is block-based and uses inodes to describe files each inode contains 16 pointers to blocks  or indirect blocks  belonging to the file described by the inode each file system has a root inode all of the metadata lives in files  all inodes are in one file  the free-block map in another  and the free-inode root inode 1 free blotk map i figure 11.16 the wafl file layout 11.9 497 map in a third  as shown in figure 11.16 because these are standard files  the data blocks are not limited in location and can be placed anywhere if a file system is expanded by addition of disks  the lengths of the metadata files are automatically expanded by the file systen  thus  a wafl file system is a tree of blocks with the root inode as its base to take a snapshot  wafl creates a copy of the root inode any file or metadata updates after that go to new blocks rather than overwriting their existing blocks the new root inode points to metadata and data changed as a result of these writes meanwhile  the snapshot  the old root inode  still points to the old blocks  which have not been updated it therefore provides access to the file system just as it was at the instant the snapshot was made-and takes very little disk space to do so ! in essence  the extra disk space occupied by a snapshot consists of just the blocks that have been modified since the snapshot was taken  an important change from more standard file systems is that the free-block map has more than one bit per block it is a bitmap with a bit set for each snapshot that is using the block when all snapshots that have been using the block are deleted  the bit map for that block is all zeros  and the block is free to be reused used blocks are never overwritten  so writes are very fast  because a write can occur at the free block nearest the current head location there are many other performance optimizations in wafl as well  many snapshots can exist simultaneously  so one can be taken each hour of the day and each day of the month a user with access to these snapshots can access files as they were at any of the times the snapshots were taken  the snapshot facility is also useful for backups  testing  versioning  and so on  wafl 's snapshot facility is very efficient in that it does not even require that copy-on-write copies of each data block be taken before the block is modified  other file systems provide snapshots  but frequently with less efficiency wafl snapshots are depicted in figure 11.17  newer versions of wafl actually allow read-write snapshots  known as ,.hj ' ' ' clones are also efficient  using the same techniques as shapshots in this case  a read-only snapshot captures the state of the file system  and a clone refers back to that read-only snapshot any writes to the clone are stored in new blocks  and the clone 's pointers are updated to refer to the new blocks  the original snapshot is unmodified  still giving a view into the file system as it was before the clone was updated clones can also be promoted to replace the original file system ; this involves throwing out all of the old pointers and any associated old blocks clones are useful for testing and upgrades  as the original version is left untouched and the clone deleted when the test is done or if the upgrade fails  another feature that naturally falls from the wafl file system implementation is the duplication and synchronization of a set of data over a network to another system first  a snapshot of a wafl file system is duplicated to another system when another snapshot is taken on the source system  it is relatively easy to update the remote system just by sending over all blocks contained in the new snapshot these blocks are the ones that have changed between the times the two snapshots were taken the remote system adds these blocks to the file system and updates its pointers  and the new system then is a duplicate of the source system as of the time of the second snapshot repeating this process maintains the remote system as a nearly up-to-date copy of the first 498 chapter 11 11.10  a  before a snapshot   b  after a snapshot  before any blocks change   c  after block d has changed to o  figure 11.17 snapshots in wafl  system such replication is used for disaster recovery should the first system be destroyed  most of its data are available for use on the remote system  finally  we should note that sun 's zfs file system supports similarly efficient snapshots  clones  and replication  the file system resides permanently on secondary storage  which is designed to hold a large amount of data permanently the most common secondary-storage medium is the disk  physical disks may be segmented into partitions to control media use and to allow multiple  possibly varying  file systems on a single spindle  these file systems are mounted onto a logical file system architecture to make then available for use file systems are often implemented in a layered or modular structure the lower levels deal with the physical properties of storage devices upper levels deal with symbolic file names and logical properties of files intermediate levels map the logical file concepts into physical device properties  any file-system type can have different structures and algorithms a vfs layer allows the upper layers to deal with each file-system type uniformly even 499 remote file systems can be integrated into the system 's directory structure and acted on by standard system calls via the vfs interface  the various files can be allocated space on the disk in three ways  through contiguous  linked  or indexed allocation contiguous allocation can suffer from external fragmentation direct access is very inefficient with linked allocation indexed allocation may require substantial overhead for its index block these algorithms can be optimized in many ways contiguous space can be enlarged through extents to increase flexibility and to decrease external fragmentation indexed allocation can be done in clusters of multiple blocks to increase throughput and to reduce the number of index entries needed indexing in large clusters is similar to contiguous allocation with extents  free-space allocation methods also influence the efficiency of disk-space use  the performance of the file system  and the reliability of secondary storage  the methods used include bit vectors and linked lists optimizations include grouping  countilcg  and the fat  which places the linked list in one contiguous area  directory-management routines must consider efficiency  performance  and reliability a hash table is a commonly used method  as it is fast and efficient unfortunately  damage to the table or a system crash can result in inconsistency between the directory information and the disk 's contents  a consistency checker can be used to repair the damage operating-system backup tools allow disk data to be copied to tape  enabling the user to recover from data or even disk loss due to hardware failure  operating system bug  or user error  network file systems  such as nfs  use client-server methodology to allow users to access files and directories from remote machines as if they were on local file systems system calls on the client are translated into network protocols and retranslated into file-system operations on the server  networking and multiple-client access create challenges in the areas of data consistency and performance  due to the fundamental role that file systems play in system operation  their performance and reliability are crucial techniques such as log structures and cachirtg help improve performance  while log structures and raid improve reliability the wafl file system is an example of optimization of performance to match a specific i/o load  11.1 in what situations would using memory as a ram disk be more useful than using it as a disk cache 11.2 consider a file systenc that uses a modifed contiguous-allocation scheme with support for extents a file is a collection of extents  with each extent corresponding to a contiguous set of blocks a key issue in such systems is the degree of variability in the size of the 500 chapter 11 extents what are the advantages and disadvantages of the following schemes a all extents are of the same size  and the size is predetermined  b extents can be of any size and are allocated dynamically  c extents can be of a few fixed sizes  and these sizes are predetermined  11.3 some file systems allow disk storage to be allocated at different levels of granularity for instance  a file system could allocate 4 kb of disk space as a single 4-kb block or as eight 512-byte blocks how could we take advantage of this flexibility to improve performance what modifications would have to be made to the free-space management scheme in order to support this feature 11.4 what are the advantages of the variant of linked allocation that uses a fat to chain together the blocks of a file 11.5 consider a file currently consisting of 100 blocks assume that the filecontrol block  and the index block  in the case of indexed allocation  is already in memory calculate how many disk i/0 operations are required for contiguous  linked  and indexed  single-level  allocation strategies  if  for one block  the following conditions hold in the contiguous-allocation case  assume that there is no room to grow at the beginning but there is room to grow at the end also assume that the block information to be added is stored in memory  a the block is added at the beginning  b the block is added in the middle  c the block is added at the end  d the block is removed from the beginning  e the block is removed from the middle  f the block is removed from the end  11.6 consider a file system that uses inodes to represent files disk blocks are 8 kb in size  and a pointer to a disk block requires 4 bytes this file system has 12 direct disk blocks  as well as single  double  and triple indirect disk blocks what is the maximum size of a file that can be stored in this file system 11.7 assume that in a particular augmentation of a reinote-file-access protocol  each client maintains a name cache that caches translations from file names to corresponding file handles what issues should we take into account in implementing the name cache 11.8 consider the following backup scheme  day 1 copy to a backup medium all files from the disk  day 2 copy to another m.edium all files changed since day 1  day 3 copy to another medium all files changed since day 1  501 this differs from the schedule given in section 11.7.4 by having all subsequent backups copy all files modified since the first full backup  what are the benefits of this system over the one in section 11.7.4 what are the drawbacks are restore operations made easier or more difficult explain your answer  11.9 why must the bit map for file allocation be kept on mass storage  rather than in main memory 11.10 consider a file system on a disk that has both logical and physical block sizes of 512 bytes assume that the information about each file is already in memory for each of the three allocation strategies  contiguous  linked  and indexed   answer these questions  a how is the logical-to-physical address mapping accomplished in this system  for the indexed allocation  assume that a file is always less than 512 blocks long  b if we are currently at logical block 10  the last block accessed was block 10  and want to access logical block 4  how many physical blocks must be read from the disk 11.11 why is it advantageous to the user for an operating system to dynamically allocate its internal tables what are the penalties to the operating system for doing so 11.12 explain why logging metadata updates ensures recovery of a file system after a file-system crash  11.13 fragmentation on a storage device can be eliminated by recompaction of the information typical disk devices do not have relocation or base registers  such as those used when memory is to be compacted   so how can we relocate files give three reasons why recompacting and relocation of files are often avoided  11.14 consider a system where free space is kept in a free-space list  a suppose that the pointer to the free-space list is lost can the system reconstruct the free-space list explain your answer  b consider a file system similar to the one used by unix with indexed allocation how many disk i/0 operations might be 502 chapter 11 required to read the contents of a small local file at /a/b/c assume that none of the disk blocks is currently being cached  c suggest a scheme to ensure that the pointer is never lost as a result of memory failure  11.15 one problem with contiguous allocation is that the user must preallocate enough space for each file if the file grows to be larger than the space allocated for it  special actions must be taken one solution to this problem is to define a file structure consisting of an initial contiguous area  of a specified size   if this area is filled  the operating system automatically defines an overflow area that is linked to the initial contiguous area if the overflow area is filled  another overflow area is allocated compare this implementation of a file with the standard contiguous and linked implementations  11.16 discuss how performance optimizations for file systems might result in difficulties in maintaining the consistency of the systems in the event of com.puter crashes  the ms-dos fat system is explained in norton and wilton  1988   and the os/2 description can be found in iacobucci  1988   these operating systems use the intel 8086 cpus  intel  1985b   intel  1985a   intel  1986   and intel  1990    ibm allocation methods are described in deitel  1990   the internals of the bsd unl ' system are covered in full in mckusick et al  1996   mcvoy and kleiman  1991  discusses optimizations of these methods made in solaris the coogle file system is described in ghemawat et al  2003   fuse can be found at http  / /fuse.sourceforge.net/  disk file allocation based on the buddy system is covered in koch  1987   a file-organization scheme that guarantees retrieval in one access is described by larson and kajla  1984   log-structured file organizations for enhancing both performance and consistency are discussed in rosenblum and ousterhout  1991   seltzer et al  1993   and seltzer et al  1995   algorithms such as balanced trees  and much more  are covered by knuth  1998  and carmen et al  2001   the zfs source code for space maps can be found at http  //src.opensolaris.org/source/xref/onnv/onnvgate/ usr i src/uts/ common/ fs/ zfs/ space_map.c  disk caching is discussed by mckeon  1985  and smith  1985   caching in the experimental sprite operating system is described in nelson et al  1988   general discussions concerning mass-storage technology are offered by chi  1982  and hoagland  1985   folk and zoellick  1987  covers the gamut of file structures silvers  2000  discusses implementing the page cache in the netbsd operating system  the network file system  nfs  is discussed in sandberg et al  1985   sandberg  1987   sun  1990   and callaghan  2000   nfs version 4 is a standard described at http  / /www.ietf.org/rfc/rfc3530.txt the characteristics of 503 workloads in distributed file systems are examined in baker et al  1991   ousterhout  1991  discusses the role of distributed state in networked file systems log-structured designs for networked file systems are proposed in hartman and ousterhout  1995  and thekkath et al  1997   nfs and the unix file system  ufs  are described in vahalia  1996  and mauro and mcdougall  2007   the windows nt file system  ntfs  is explained in solomon  1998   the ext2 file system used in linux is described in bovet and cesati  2002  and the wafl file system in hitz et al  1995   zfs documentation can be found at http  / /www.opensolaris.org/ os/ community /zfs/ docs  12.1 the file system can be viewed logically as consisting of three parts in chapter 10  we examined the user and programmer interface to the file system in chapter 11  we described the internal data structures and algorithms used by the operating system to implement this interface in this chapter  we discuss the lowest level of the file system  the secondary and tertiary storage structures we first describe the physical structure of magenetic disks and magnetic tapes we then describe disk-scheduling algorithms  which schedule the order of disk i/ os to improve performance next  we discuss disk formatting and management of boot blocks  damaged blocks  and swap space we then examine secondary storage structure  covering disk reliability and stablestorage implementation we conclude with a brief description of tertiary storage devices and the problems that arise when an operating system uses tertiary storage  to describe the physical structure of secondary and tertiary storage devices and its effects on the uses of the devices  to explain the performance characteristics of mass-storage devices  to discuss operating-system services provided for mass storage  including raid and hsm  in this section  we present a general overview of the physical structure of secondary and tertiary storage devices  12.1.1 magnetic disks provide the bulk of secondary storage for modern computer systems conceptually  disks are relatively simple  figure 12.1   each disk platter has a flat circular shape  like a cd common platter diameters range 505 506 chapter 12 arm assembly rotation figure 12.1 moving-head disk mechanism  from 1.8 to 5.25 inches the two surfaces of a platter are covered with a magnetic material we store information by recording it magnetically on the platters  a read -write head flies just above each surface of every platter the heads are attached to a that moves all the heads as a unit the surface of a platter is logically divided into circular which are subdivided into the set of tracks that are at one arm position makes up a there may be thousands of concentric cylinders in a disk drive  and each track may contain hundreds of sectors the storage capacity of common disk drives is measured iil gigabytes  when the disk is in use  a drive motor spins it at high speed most drives rotate 60 to 200 times per second disk speed has two parts the is the rate at which data flow between the drive and the computer the sometimes called the consists of the time necessary to move the disk arm to the desired cylinder  called the and the time necessary for the desired sector to rotate to the disk head  called the typical disks can transfer several megabytes of data per second  and they seek times and rotational latencies of several milliseconds  because the disk head flies on an extremely thin cushion of air  measured in microns   there is a danger that the head will make contact with the disk surface although the disk platters are coated with a thin protective laye1 ~ the head will sometimes damage the magnetic surface this accident is called a a head crash normally can not be repaired ; the entire disk must be replaced  a disk can be allowing different disks to be mounted as needed  removable magnetic disks generally consist of one platter  held in a plastic case to prevent damage while not in the disk drive are inexpensive removable magnetic disks that have a soft plastic case containing a flexible platter the head of a floppy-disk drive generally sits directly on the disk 12.1 507 disk transfer rates as with many aspects of computingf published performance numbers for disks are not the same as real-world performance numbers stated transfer rates are always lower than for example the transfer rate may be the rate at which bits can be read from the magnetic media by the disk head  but that is different from the rate at which blocks are delivered to the operating system  surface  so the drive is designed to rotate more slowly than a hard-disk drive to reduce the wear on the disk surface the storage capacity of a floppy disk is typically only 1.44mb or so removable disks are available that work much like normal hard disks and have capacities measured in gigabytes  a disk drive is attached to a computer by a set of wires called an several kinds of buses are available  including buses the data transfers on a bus are carried out by special electronic processors called the is the controller at the computer end of the bus a is built into each disk drive to perform a disk i/0 operation  the computer places a command into the host controller  typically using memory-mapped i/0 portsf as described in section 9.7.3 the host controller then sends the command via messages to the disk controller  and the disk controller operates the disk-drive hardware to carry out the command disk controllers usually have a built-in cache data transfer at the disk drive happens between the cache and the disk surface  and data transfer to the host  at fast electronic speeds  occurs between the cache and the host controller  12.1.2 magnetic tapes was used as an early secondary-storage medium although it is relatively permanent and can hold large quantities of dataf its access time is slow compared with that of main memory and magnetic disk in addition  random access to magnetic tape is about a thousand times slower than random access to magnetic disk  so tapes are not very useful for secondary storage  tapes are used mainly for backup  for storage of infrequently used information  and as a medium for transferring information from one system to another  a tape is kept in a spool and is wound or rewound past a read-write head  moving to the correct spot on a tape can take minutes  but once positioned  tape drives can write data at speeds comparable to disk drives tape capacities vary greatly  depending on the particular kind of tape drive typically  they store from 20gb to 200gb some have built-in compression that can more than double the effective storage tapes and their drivers are usually categorized by width  includil1.g 4  8f and 19 millimeters and 1/4 and 1/2 inch some are named according to technology  such as lt0-2 and sdlt tape storage is further described in section 12.9  508 chapter 12 12.2 fire wire refers to an interface designed for connecting peripheral devices such as hard drives  dvd drives  and digital video cameras to a computer system fire wire was first developed by apple computer and became the ieee 1394 standard in 1995 the originalfirewire standard provided bandwidth up to 400 megabits per second recently  a new standardfirewire 2-has emerged and is identified by the ieee 1394b standard  firewire 2 provides double the data rate of the original firewire-800 megabits per second  modern disk drives are addressed as large one-dimensional arrays of where the logical block is the smallest unit of transfer the size of a logical block is usually 512 bytes  although some disks can be to have a different logical block size  such as 1,024 bytes this option is described in section 12.5.1 the one-dimensional array of logical blocks is mapped onto the sectors of the disk sequentially sector 0 is the first sector of the first track on the outermost cylinder the mapping proceeds in order through that track  then through the rest of the tracks in that cylinder  and then through the rest of the cylinders from outermost to innermost  by using this mapping  we can -at least in theory-convert a logical block number into an old-style disk address that consists of a cylinder number  a track number within that cylinder  and a sector number within that track in practice  it is difficult to perform this translation  for two reasons first  most disks have some defective sectors  but the mapping hides this by substituting spare sectors from elsewhere on the disk second  the number of sectors per track is not a constant on smne drives  let 's look more closely at the second reason on media that use the density of bits per track is uniform the farther a track is from the center of the disk  the greater its length  so the more sectors it can hold as we move from outer zones to inner zones  the number of sectors per track decreases tracks in the outermost zone typically hold 40 percent more sectors than do tracks in the innermost zone the drive increases its rotation speed as the head moves from the outer to the inner tracks to keep the same rate of data moving under the head this method is used in cd-rom and dvd-rom drives alternatively  the disk rotation speed can stay constant ; in this case  the density of bits decreases from inner tracks to outer tracks to keep the data rate constant this method is used in hard disks and is known as the number of sectors per track has been increasing as disk technology improves  and the outer zone of a disk usually has several hundred sectors per track similarly  the number of cylinders per disk has been increasing ; large disks have tens of thousands of cylinders  12.3 12.3 509 computers access disk storage in two ways one way is via i/o ports  or this is common on small systems the other way is via a remote host in a distributed file system ; this is referred to as 12.3.1 host-attached storage host-attached storage is storage accessed through local i/0 ports these ports use several technologies the typical desktop pc uses an i/0 bus architecture called ide or ata this architecture supports a maximum of two drives per i/0 bus a newer  similar protocol that has simplified cabling is sata high-end workstations and servers generally use more sophisticated i/0 architectures  such as scsi and fiber charmel  fc   scsi is a bus architecture its physical medium is usually a ribbon cable with a large number of conductors  typically 50 or 68   the scsi protocol supports a maximum of 16 devices per bus generally  the devices include one controller card in the host  the and up to 15 storage devices  the to.rgr    ts   a scsi disk is a common scsi target  but the protocol provides the ability to address up to 8 in each scsi target a typical use of logical unit addressing is to commands to components of a raid array or components of a removable media library  such as a cd jukebox sendil g commands to the media-changer mechanism or to one of the drives   fc is a high-speed serial architecture that can operate over optical fiber or over a four-conductor copper cable it has two variants one is a large switched fabric having a 24-bit address space this variant is expected to dominate in the future and is the basis of  sjld '  ; s   discussed in section 12.3.3 because of the large space and the switched nature of the communication  multiple hosts and storage devices can attach to the fabric  allowing great flexibility in i/0 communication the other fc variant is an that can address 126 devices  drives and controllers   a wide variety of storage devices are suitable for use as host-attached storage among these are hard disk drives  raid arrays  and cd  dvd  and tape drives the i/0 commands that initiate data transfers to a host-attached storage device are reads and writes of logical data blocks directed to specifically identified storage units  such as bus id  scsi id  and target logical unit   12.3.2 network-attached storage a network-attached storage  nas  device is a special-purpose storage system that is accessed remotely over a data network  figure 12.2   clients access network-attached storage via a remote-procedure-call interface such as nfs for unix systems or cifs for windows machines the remote procedure calls  rpcs  are carried via tcp or udp over an ip network-usually the same local-area network  lan  that carries all data traffic to the clients the networkattached storage unit is usually implemented as a raid array with software that implements the rpc interface it is easiest to thil k of nas as simply another storage-access protocol for example  rather than using a scsi device driver and scsi protocols to access storage  a system using nas would use rpc over tcp /ip  510 chapter 12 12.4 lan/wan figure 12.2 network-attached storage  network-attached storage provides a convenient way for all the computers on a lan to share a pool of storage with the same ease of naming and access enjoyed with local host-attached storage however  it tends to be less efficient and have lower performance than some direct-attached storage options  is the latest network-attached storage protocol in essence  it uses the ip network protocol to carry the scsi protocol thus  networks-rather than scsi cables-can be used as the interconnects between hosts and their storage  as a result  hosts can treat their storage as if it were directly attached  even if the storage is distant from the host  12.3.3 storage-area network one drawback of network-attached storage systems is that the storage i/o operations consume bandwidth on the data network  thereby increasing the latency of network communication this problem can be particularly acute in large client-server installations-the communication between servers and clients competes for bandwidth with the communication among servers and storage devices  a storage-area network  san  is a private network  using storage protocols rather than networking protocols  connecting servers and storage units  as shown in figure 12.3 the power of a san lies in its flexibility multiple hosts and multiple storage arrays can attach to the same san  and storage can be dynamically allocated to hosts a san switch allows or prohibits access between the hosts and the storage as one example  if a host is running low on disk space  the san can be configured to allocate more storage to that host  sans make it possible for clusters of servers to share the same storage and for storage arrays to include multiple direct host com1.ections sans typically have more ports  and less expensive ports  than storage arrays  fc is the most common san interconnect  although the simplicity of iscsi is increasing its use an emerging alternative is a special-purpose bus architecture named infiniband  which provides hardware and software support for highspeed interconnection networks for servers and storage units  one of the responsibilities of the operating system is to use the hardware efficiently for the disk drives  meeting this responsibility entails having 12.4 511 figure 12.3 storage-area network  fast access time and large disk bandwidth the access time has two major components  also see section 12.1.1   the is the time for the disk arm to move the heads to the cylinder containing the desired sector the is the additional time for the disk to rotate the desired sector to the disk head the disk is the total number of bytes transferred  divided by the total time between the first request for service and the completion of the last transfer we can improve both the access time and the bandwidth by managing the order in which disk i/o requests are serviced  whenever a process needs i/0 to or from the disk  it issues a system call to the operating system the request specifies several pieces of information  whether this operation is input or output what the disk address for the transfer is what the memory address for the transfer is what the number of sectors to be transferred is if the desired disk drive and controller are available  the request can be serviced immediately if the drive or controller is busy  any new requests for service will be placed in the queue of pending requests for that drive  for a multiprogramming system with many processes  the disk queue may often have several pending requests thus  when one request is completed  the operating system chooses which pending request to service next how does the operating system make this choice any one of several disk-scheduling algorithms can be used  and we discuss them next  12.4.1 fcfs scheduling the simplest form of disk scheduling is  of course  the first-come  first-served  fcfs  algorithm this algorithm is intrinsically fair  but it generally does not provide the fastest service consider  for example  a disk queue with requests for i/0 to blocks on cylinders 98  183  37  122  14  124  65  67  512 chapter 12 queue = 98  183,37,122  14,124,65,67 head starts at 53 0 14 37 536567 98 122124 figure 12.4 fcfs disk scheduling  183199 in that order if the disk head is initially at cylinder 53  it will first move from 53 to 98  then to 183  37  122  14  124  65  and finally to 67  for a total head movement of 640 cylinders this schedule is diagrammed in figure 12.4  the wild swing from 122 to 14 and then back to 124 illustrates the problem with this schedule if the requests for cylinders 37 and 14 could be serviced together  before or after the requests for 122 and 124  the total head movement could be decreased substantially  and performance could be thereby improved  12.4.2 sstf scheduling it seems reasonable to service all the requests close to the current head position before moving the head far to service other this assumption is the basis for the the sstf algorithm selects the request with the least seek time from the current head position  since seek time increases with the number of cylinders traversed by the head  sstf chooses the pending request closest to the current head position  for our example request queue  the closest request to the initial head position  53  is at cylinder 65 once we are at cylinder 65  the next closest request is at cylinder 67 from there  the request at cylinder 37 is closer than the one at 98  so 37 is served next continuing  we service the request at cylinder 14  then 98  122  124  and finally 183  figure 12.5   this scheduling method results in a total head movement of only 236 cylinders-little more than one-third of the distance needed for fcfs scheduling of this request queue clearly  this algorithm gives a substantial improvement in performance  sstf scheduling is essentially a form of shortest-job-first  sjf  scheduling ; and like sjf scheduling  it may cause starvation of some requests remember that requests may arrive at any time suppose that we have two requests in the queue  for cylinders 14 and 186  and while the request from 14 is being serviced  a new request near 14 arrives this new request will be serviced next  making the request at 186 wait while this request is being serviced  another request close to 14 could arrive in theory  a continual stream of requests near one another could cause the request for cylinder 186 to wait indefinitely  12.4 queue = 98  183  37  122  14  124  65  67 head starts at 53 0 14 37 536567 98 122124 figure 12.5 sstf disk scheduling  513 183199 this scenario becomes increasingly likely as the pending-request queue grows longer  although the sstf algorithm is a substantial improvement over the fcfs algorithm  it is not optimal in the example  we can do better by moving the head from 53 to 37  even though the latter is not closest  and then to 14  before turning around to service 65  67  98  122  124  and 183 this strategy reduces the total head movement to 208 cylinders  12.4.3 scan scheduling in the toward the end  servicing requests as it reaches each cylinder  until it gets to the other end of the disk at the other end  the direction of head movement is reversed  and servicing continues the head continuously scans back and forth across the disk the scan algorithm is sometimes called the since the disk arm behaves just like an elevator in a building  first servicing all the requests going up and then reversing to service requests the other way  let 's return to our example to illustrate before applying scan to schedule the requests on cylinders 98  183,37  122  14  124  65  and 67  we need to know the direction of head movement in addition to the head 's current position  assuming that the disk arm is moving toward 0 and that the initial head position is again 53  the head will next service 37 and then 14 at cylinder 0  the arm will reverse and will move toward the other end of the disk  servicil lg the requests at 65  67  98  122  124  and 183  figure 12.6   if a request arrives in the queue just in front of the head  it will be serviced almost immediately ; a request arriving just behind the head will have to wait until the arm moves to the end of the disk  reverses direction  and comes back  assuming a uniform distribution of requests for cylinders  consider the density of requests when the head reaches one end and reverses direction at this point  relatively few requests are immediately in front of the head  since these cylinders have recently been serviced the heaviest density of requests 514 chapter 12 queue = 98  183,37,122  14,124,65,67 head starts at 53 0 14 37 536567 98 122124 figure 12.6 scan disk scheduling  183199 is at the other end of the disk these requests have also waited the longest so why not go there first that is the idea of the next algorithm  12.4.4 c-scan scheduling is a variant of scan designed to provide a more uniform wait time like scan  c-scan moves the head from one end of the disk to the other  servicing requests along the way when the head reaches the other end  however  it immediately returns to the beginning of the disk without servicing any requests on the return trip  figure 12.7   the c-scan scheduling algorithm essentially treats the cylinders as a circular list that wraps around from the final cylinder to the first one  queue = 98  183  37  122  14  124  65  67 head starts at 53 0 1 4 37 53 65 67 98 1 22 1 24 figure 12.7 c-scan disk scheduling  183199 12.4 queue = 98  183  37  122  14  124  65  67 head starts at 53 0 14 37 536567 98 122124 figure 12.8 c-look disk scheduling  12.4.5 look scheduling 515 183199 as we described themf both scan and c-scan move the disk arm across the full width of the disk in practicef neither algorithm is often implemented this way more commonlyf the arm goes only as far as the final request in each direction then  it reverses direction immediatelyf without going all the way to the end of the disk versions of scan and c-scan that follow this pattern are called and because they look for a request before continuing to move in a given direction  figure 12.8   12.4.6 selection of a disk-scheduling algorithm given so many disk-scheduling algorithmsf how do we choose the best one sstf is common and has a natural appeal because it increases performance over fcfs scan and c-scan perform better for systems that place a heavy load on the diskf because they are less likely to cause a starvation problem for any particular list of requestsf we can define an optimal order of retrievat but the computation needed to find an optimal schedule may not justify the savings over sstf or scan with any scheduling algoritlunf howeverf performance depends heavily on the number and types of requests for instance  suppose that the queue usually has just one outstanding request thenf all scheduling algorithms behave the samef because they have only one choice of where to move the disk head  they all behave like fcfs scheduling  requests for disk service can be greatly influenced by the file-allocation method a program reading a contiguously allocated file will generate several requests that are close together on the disk  resulting in limited head movement  a linked or indexed fik in contrastf may include blocks that are widely scattered on the diskf resulting in greater head movement  the location of directories and index blocks is also important since every file must be opened to be usedf and opening a file requires searching the directory structuref the directories will be accessed frequently suppose that a directory entry is on the first cylinder and a filef s data are on the final cylinder in this casef the disk head has to move the entire width of the disk if the directory 516 chapter 12 12.5 entry were on the middle cylinder  the head would have to move only one-half the width caching the directories and index blocks in main memory can also help to reduce disk-arm movement particularly for read requests  because of these complexities  the disk-scheduling algorithm should be written as a separate module of the operating system  so that it can be replaced with a different algorithm if necessary either sstf or look is a reasonable choice for the default algorithm  the scheduling algorithms described here consider only the seek distances  for modern disks  the rotational latency can be nearly as large as the average seek time it is difficult for the operating system to schedule for improved rotational latency  though  because modern disks do not disclose the physical location of logical blocks disk manufacturers have been alleviating this problem by implementing disk-scheduling algorithms in the controller hardware built into the disk drive if the operating system sends a batch of requests to the controller  the controller can queue them and then schedule them to improve both the seek time and the rotational latency  if i/o performance were the only consideration  the operating system would gladly turn over the responsibility of disk scheduling to the disk hardware  in practice  however  the operating system may have other constraints on the service order for requests for instance  demand paging may take priority over application i/0  and writes are more urgent than reads if the cache is running out of free pages also  it may be desirable to guarantee the order of a set of disk writes to make the file system robust in the face of system crashes  consider what could happen if the operating system allocated a disk page to a file and the application wrote data into that page before the operating system had a chance to flush the modified inode and free-space list back to disk to accommodate such requirements  an operating system may choose to do its own disk scheduling and to spoon-feed the requests to the disk controller  one by one  for some types of i/0  the operating system is responsible for several other aspects of disk management  too here we discuss disk initialization  booting from disk  and bad-block recovery  12.5.1 disk formatting a new magnetic disk is a blank slate  it is just a platter of a magnetic recording material before a disk can store data  it must be divided into sectors that the disk controller can read and write this process is called or low-level formatting fills the disk with a special data structure for each sector the data structure for a sector typically consists of a header  a data area  usually 512 bytes in size   and a trailer the header and trailer contain information used by the disk controller  such as a sector number and an  when the controller writes a sector of data during normal i/0  the ecc is updated with a value calculated from all the bytes in the data area when the sector is read  the ecc is recalculated and compared with the stored value if the stored and calculated numbers are different  this 12.5 517 mismatch indicates that the data area of the sector has become corrupted and that the disk sector may be bad  section 12.5.3   the ecc is an error-correcting code because it contains enough information  if only a few bits of data have been corrupted  to enable the controller to identify which bits have changed and calculate what their correct values should be it then reports a recoverable  the controller automatically does the ecc processing whenever a sector is read or written  most hard disks are low-level-formatted at the factory as a part of the manufacturing process this formatting enables the manufacturer to test the disk and to initialize the mapping from logical block numbers to defect-free sectors on the disk for many hard disks  when the disk controller is instructed to low-level-format the disk  it can also be told how many bytes of data space to leave between the header and trailer of all sectors it is usually possible to choose among a few sizes  such as 256,512  and 1,024 bytes formatting a disk with a larger sector size means that fewer sectors can fit on each track ; but it also means that fewer headers and trailers are written on each track and more space is available for user data some operating systems can handle only a sector size of 512 bytes  before it can use a disk to hold files  the operating system still needs to record its own data structures on the disk it does so in two steps the first step is to the disk into one or more groups of cylinders the operatiltg system can treat each partition as though it were a separate disk for instance  one partition can hold a copy of the operating system 's executable code  while another holds user files the second step is icgicz ; i or creation of a file system in this step  the operating system stores the iltitial file-system data structures onto the disk these data structures may include maps of free and allocated space  a fat or inodes  and an initial empty directory  to increase efficiency  most file systems group blocks together into larger chunks  frequently called disk i/0 is done via blocks  but file system ii 0 is done via clusters  effectively assuring that ii 0 has more sequential-access and fewer random-access characteristics  some operating systems give special programs the ability to use a disk partition as a large sequential array of logical blocks  without any file-system data structures this array is sometimes called the raw disk  and ii 0 to this array is termed raw l/0 for example  some database systems prefer raw iio because it enables them to control the exact disk location where each database record is stored raw l/0 bypasses all the file-system services  such as the buffer cache  file locking  prefetching  space allocation  file names  and directories we can make certain applications more efficient by allowing them to implement their own special-purpose storage services on a raw partition  but most applications perform better when they use the regular file-system services  12.5.2 boot block for a computer to start running-for instance  when it is powered up or rebooted -it must have an initial program to run this initial bootstrap program tends to be simple it initializes all aspects of the system  from cpu registers to device controllers and the contents of main memory  and then starts the operating system to do its job  the bootstrap program finds the 518 chapter 12 operating-system kernel on disk  loads that kernel into memory  and jumps to an initial address to begin the operating-system execution  for most computers  the bootstrap is stored in this location is convenient  because rom needs no initialization and is at a fixed location that the processor can start executing when powered up or reset and  since rom is read only  it can not be infected by a computer virus the problem is that changing this bootstrap code requires changing the rom hardware chips  for this reason  most systems store a tiny bootstrap loader program in the boot rom whose only job is to bring in a full bootstrap program from disk the full bootstrap program can be changed easily  a new version is simply written onto the disk the full bootstrap program is stored in the boot blocks at a fixed location on the disk a disk that has a boot partition is called a or the code in the boot rom instructs the disk controller to read the boot blocks into memory  no device drivers are loaded at this point  and then starts executing that code the full bootstrap program is more sophisticated than the bootstrap loader in the boot rom ; it is able to load the entire operating system from a non-fixed location on disk and to start the operating system ruru1ing  even so  the full bootstrap code may be small  let 's consider as an example the boot process in windows 2000 the windows 2000 system places its boot code in the first sector on the hard disk  which it terms the or furthermore  windows 2000 allows a hard disk to be divided into one or more partitions ; one partition  identified as the contains the operating system and device drivers bootil1g begins in a windows 2000 system by running code that is resident in the system 's rom memory this code directs the system to read the boot code from the mbr in addition to containing boot code  the mbr contains a table listing the partitions for the hard disk and a flag indicating which partition the system is to be booted from  as illustrated in figure 12.9  once the system identifies the boot partition  it reads the first sector from that partition  which is called the and contilmes with the remainder of the boot process  which includes loading the various subsystems and system services  mbr partition 1 partition 2 partition 3 partition 4 boot code partition table boot partition figure 12.9 booting from disk in windows 2000  12.5 519 12.5.3 bad blocks because disks have moving parts and small tolerances  recall that the disk head flies just above the disk surface   they are prone to failure sometimes the failure is complete ; in this case  the disk needs to be replaced and its contents restored from backup media to the new disk more frequently  one or more sectors become defective most disks even con'le from the factory with depending on the disk and controller in use  these blocks are handled in a variety of ways  on simple disks  such as some disks with ide controllers  bad blocks are handled manually for instance  the ms-dos format command performs logical formatting and  as a part of the process  scans the disk to find bad blocks if format finds a bad block  it writes a special value into the corresponding fat entry to tell the allocation routines not to use that block if blocks go bad during normal operation  a special program  such as chkdsk  must be run manually to search for the bad blocks and to lock them away data that resided on the bad blocks usually are lost  more sophisticated disks  such as the scsi disks used in high-end pcs and most workstations and servers  are smarter about bad-block recovery the controller maintains a list of bad blocks on the disk the list is initialized during the low-level formatting at the factory and is updated over the life of the disk  low-level formatting also sets aside spare sectors not visible to the operating system the controller can be told to replace each bad sector logically with one of the spare sectors this scheme is known as or a typical bad-sector transaction might be as follows  the operating system tries to read logical block 87  the controller calculates the ecc and finds that the sector is bad it reports this finding to the operating system  the next time the system is rebooted  a special command is run to tell the scsi controller to replace the bad sector with a spare  after that  whenever the system requests logical block 87  the request is translated into the replacement sector 's address by the controller  note that such a redirection by the controller could invalidate any optimization by the operating system 's disk-scheduling algorithm ! for this reason  most disks are formatted to provide a few spare sectors in each cylinder and a spare cylinder as well when a bad block is remapped  the controller uses a spare sector from the same cylinder  if possible  as an alternative to sector some controllers can be instructed to replace a bad block by here is an example  suppose that logical block 17 becomes defective and the first available spare follows sector 202 then  sector slipping remaps all the sectors front 17 to 202  moving them all down one spot that is  sector 202 is copied into the spare  then sector 201 into 202  then 200 into 201  and so on  until sector 18 is copied into sector 19  slipping the sectors in this way frees up the space of sector 18  so sector 17 can be mapped to it  the replacement of a bad block generally is not totally automatic because the data in the bad block are usually lost soft errors may trigger a process in chapter 18 o my children  lemar  sivan  and aaron and my nicolette avi silberschatz to my wife  carla  and my children  gwen  owen  and maddie peter baer galvin to my wife  pat  and our sons  tom and jay greg gagne abraham silberschatz is the sidney j weinberg professor & chair of computer science at yale university prior to joining yale  he was the vice president of the information sciences research center at bell laboratories prior to that  he held a chaired professorship in the department of computer sciences at the university of texas at austin  professor silberschatz is an acm fellow and an ieee fellow he received the 2002 ieee taylor l booth education award  the 1998 acm karl v karlstrom outstanding educator award  and the 1997 acm sigmod contribution award in recognition of his outstanding level of innovation and technical excellence  he was awarded the bell laboratories president 's award for three different projects-the qtm project  1998   the datablitz project  1999   and the netlnventory project  2004   professor silberschatz ' writings have appeared in numerous acm and ieee publications and other professional conferences and journals he is a coauthor of the textbook database system concepts he has also written op-ed articles for the new york times  the boston globe  and the hartford courant  among others  peter baer galvin is the chief technologist for corporate technologies  www.cptech.com   a computer facility reseller and integrator before that  mr  galvin was the systems manager for brown university 's computer science department he is also sun columnist for ; login  magazine mr galvin has written articles for byte and other magazines  and has written columns for sun world and sysadmin magazines as a consultant and trainer  he has given talks and taught tutorials on security and system administration worldwide  greg gagne is chair of the computer science department at westminster college in salt lake city where he has been teaching since 1990 in addition to teaching operating systems  he also teaches computer networks  distributed systems  and software engineering he also provides workshops to computer science educators and industry professionals  operating systems are an essential part of any computer system similarly  a course on operating systems is an essential part of any computer-science education this field is undergoing rapid change  as computers are now prevalent in virtually every application  from games for children through the most sophisticated planning tools for governments and multinational firms  yet the fundamental concepts remain fairly clear  and it is on these that we base this book  we wrote this book as a text for an introductory course in operating systems at the junior or senior undergraduate level or at the first-year graduate level  we hope that practitioners will also find it useful it provides a clear description of the concepts that underlie operating systems as prerequisites  we assume that the reader is familiar with basic data struchues  computer organization  and a high-level language  such as c or java the hardware topics required for an understanding of operating systems are included in chapter 1 for code examples  we use predominantly c  with some java  but the reader can still understand the algorithms without a thorough knowledge of these languages  concepts are presented using intuitive descriptions important theoretical results are covered  but formal proofs are omitted the bibliographical notes at the end of each chapter contain pointers to research papers in which results were first presented and proved  as well as references to material for further reading in place of proofs  figures and examples are used to suggest why we should expect the result in question to be true  the fundamental concepts and algorithms covered in the book are often based on those used in existing conunercial operating systems our aim is to present these concepts and algorithms in a general setting that is not tied to one particular operating system we present a large number of examples that pertain to the most popular and the most im1.ovative operating systems  including sun microsystems ' solaris ; linux ; microsoft windows vista  windows 2000  and windows xp ; and apple mac os x when we refer to windows xp as an example operating system  we are implying windows vista  windows xp  and windows 2000 if a feature exists in a specific release  we state this explicitly  vii viii the organization of this text reflects our many years of teaching courses on operating systems consideration was also given to the feedback provided by the reviewers of the text  as well as comments submitted by readers of earlier editions in addition  the content of the text corresponds to the suggestions from computing curricula 2005 for teaching operating systems  published by the joint task force of the ieee computing society and the association for computing machinery  acm   on the supporting web site for this text  we provide several sample syllabi that suggest various approaches for using the text in both introductory and advanced courses as a general rule  we encourage readers to progress sequentially through the chapters  as this strategy provides the most thorough study of operating systems however  by using the sample syllabi  a reader can select a different ordering of chapters  or subsections of chapters   on-line support for the text is provided by wileyplus on this site  students can find sample exercises and programming problems  and instructors can assign and grade problems in addition  in wileyplus  students can access new operating-system simulators  which are used to work through exercises and hands-on lab activities references to the simulators and associated activities appear at the ends of several chapters in the text  the text is organized in nine major parts  overview chapters 1 and 2 explain what operating systems are  what they do  and how they are designed and constructed these chapters discuss what the common features of an operating system are  what an operating system does for the user  and what it does for the computer-system operator the presentation is motivational and explanatory in nature we have avoided a discussion of how things are done internally in these chapters therefore  they are suitable for individual readers or for students in lower-level classes who want to learn what an operating system is without getting into the details of the internal algorithms  process management and process coordination chapters 3 through 7 describe the process concept and concurrency as the heart of modern operating systems a process is the unit of work in a system  such a system consists of a collection of concurrently executing processes  some of which are operating-system processes  those that execute system code  and the rest of which are user processes  those that execute user code   these chapters cover n1.ethods for process scheduling  interprocess communication  process synchronization  and deadlock handling also included is a discussion of threads  as well as an examination of issues related to multicore systems  memory management chapters 8 and 9 deal with the management of main memory during the execution of a process to improve both the utilization of the cpu and the speed of its response to its users  the computer must keep several processes in memory there are many different ix management  and the effectiveness of a particular algorithm depends on the situation  storage management chapters 10 through 13 describe how the file system  mass storage  and i/0 are handled in a modern computer system the file system provides the mechanism for on-line storage of and access to both data and programs we describe the classic internal algorithms and structures of storage management and provide a firm practical understanding of the algorithms used -their properties  advantages  and disadvantages our discussion of storage also includes matters related to secondary and tertiary storage since the i/0 devices that attach to a computer vary widely  the operating system needs to provide a wide range of functionality to applications to allow them to control all aspects of these devices we discuss system i/o in depth  including i/o system design  interfaces  and internal system structures and functions in many ways  i/o devices are the slowest major components of the computer because they represent a performance bottleneck  we also examine performance issues associated with i/0 devices  protection and security chapters 14 and 15 discuss the mechanisms necessary for the protection and security of computer systems the processes in an operating system must be protected from one another 's activities  and to provide such protection  we must ensure that only processes that have gained proper authorization from the operating system can operate on the files  memory  cpu  and other resources of the system  protection is a mechanism for controlling the access of programs  processes  or users to the resources defined by a computer system this mechanism must provide a means of specifying the controls to be imposed  as well as a means of enforcement security protects the integrity of the information stored in the system  both data and code   as well as the physical resources of the system  from 1.mauthorized access  malicious destruction or alteration  and accidental introduction of inconsistency  distributed systems chapters 16 through 18 deal with a collection of processors that do not share memory or a clock-a distributed system by providing the user with access to the various resources that it maintains  a distributed system can improve computation speed and data availability and reliability such a system also provides the user with a distributed file system  which is a file-service system whose users  servers  and storage devices are dispersed among the sites of a distributed system a distributed system must provide various mechanisms for process synchronization and communication  as well as for dealing with deadlock problems and a variety of failures that are not encountered in a centralized system  special-purpose systems chapters 19 and 20 deal with systems used for specific purposes  including real-time systems and multimedia systems  these systems have specific requirements that differ from those of the general-purpose systems that are the focus of the remainder of the text  real-time systems may require not only that computed results be correct but also that the results be produced within a specified deadline period  multimedia systems require quality-of-service guarantees ensuring that the multimedia data are delivered to clients within a specific time frame  x case studies chapters 21 through 23 in the book  and appendices a through c  which are available on www.wiley.comj go i global/ silberschatz and in wileyplus   integrate the concepts described in the earlier chapters by describing real operating systems these systems include linux  windows xp  freebsd  mach  and windows 2000 we chose linux and freebsd because unix-at one time-was almost small enough to understand yet was not a toy operating system most of its internal algorithms were selected for simplicity  rather than for speed or sophistication both linux and freebsd are readily available to computer-science departments  so many students have access to these systems we chose windows xp and windows 2000 because they provide an opporhmity for us to study a modern operating system with a design and implementation drastically different from those of unix chapter 23 briefly describes a few other influential operating systems  this book uses examples of many real-world operating systems to illustrate fundamental operating-system concepts however  particular attention is paid to the microsoft family of operating systems  including windows vista  windows 2000  and windows xp  and various versions of unix  including solaris  bsd  and mac os x   we also provide a significant amount of coverage of the linux operating system reflecting the most recent version of the kernel -version 2.6-at the time this book was written  the text also provides several example programs written in c and java these programs are intended to run in the following programming environments  windows systems the primary programming environment for windows systems is the win32 api  application programming interface   which provides a comprehensive set of functions for managing processes  threads  memory  and peripheral devices we provide several c programs illustrating the use of the win32 api example programs were tested on systems rum1.ing windows vista  windows 2000  and windows xp  posix posix  which stands for portable operating system inte1jace  represents a set of standards implemented primarily for unix-based operating systems although windows vista  windows xp  and windows 2000 systems can also run certain posix programs  our coverage of posix focuses primarily on unix and linux systems posix-compliant systems must implement the posix core standard  posix.1   linux  solaris  and mac os x are examples of posix-compliant systems posix also defines several extensions to the standards  including real-time extensions  posixl.b  and an extension for a threads library  posix1.c  better known as pthreads   we provide several programn1.ing examples written inc illustrating the posix base api  as well as pthreads and the extensions for real-time programming  these example programs were tested on debian linux 2.4 and 2.6 systems  mac os x 10.5  and solaris 10 using the gee 3.3 and 4.0 compilers  java java is a widely used programming language with a rich api and built-in language support for thread creation and management java xi programs run on any operating system supporting a java virtual machine  or jvm   we illustrate various operating system and networking concepts with several java programs tested using the java 1.5 jvm  we have chosen these three programming environments because it is our opinion that they best represent the two most popular models of operating systems  windows and unix/linux  along with the widely used java environment  most programming examples are written in c  and we expect readers to be comfortable with this language ; readers familiar with both the c and java languages should easily understand most programs provided in this text  in some instances-such as thread creation-we illustrate a specific concept using all three programming environments  allowing the reader to contrast the three different libraries as they address the same task in other situations  we may use just one of the apis to demonstrate a concept  for example  we illustrate shared memory using just the posix api ; socket programming in tcp /ip is highlighted using the java api  as we wrote the eighth edition of operating system concepts  we were guided by the many comments and suggestions we received from readers of our previous editions  as well as by our own observations about the rapidly changing fields of operating systems and networking we have rewritten material in most of the chapters by bringing older material up to date and removing material that was no longer of interest or relevance  we have made substantive revisions and organizational changes in many of the chapters most importantly  we have added coverage of open-source operating systems in chapter 1 we have also added more practice exercises for students and included solutions in wileyplus  which also includes new simulators to provide demonstrations of operating-system operation below  we provide a brief outline of the major changes to the various chapters  chapter 1  introduction  has been expanded to include multicore cpus  clustered computers  and open-source operating systems  chapter 2  system structures  provides significantly updated coverage of virtual machines  as well as multicore cpus  the grub boot loader  and operating-system debugging  chapter 3  process concept  provides new coverage of pipes as a form of interprocess communication  chapter 4  multithreaded programming  adds new coverage of programming for multicore systems  chapter 5  process scheduling  adds coverage of virtual machine scheduling and multithreaded  multicore architectures  chapter 6  synchronization  adds a discussion of mutual exclusion locks  priority inversion  and transactional memory  chapter 8  memory-management strategies  includes discussion of numa  xii chapter 9  virtual-memory management  updates the solaris example to include solaris 10 memory managernent  chapter 10  file system  is updated with current technologies and capacities  chapter 11  implementing file systems  includes a full description of sun 's zfs file system and expands the coverage of volumes and directories  chapter 12  secondary-storage structure  adds coverage of iscsi  volumes  and zfs pools  chapter 13  i/0 systems  adds coverage of pcix pci express  and hypertransport  chapter 16  distributed operating systems  adds coverage of 802.11 wireless networks  chapter 21  the limix system  has been updated to cover the latest version of the limix kernel  chapter 23  influential operating systems  increases coverage of very early computers as well as tops-20  cp/m  ms-dos  windows  and the original mac os  to emphasize the concepts presented in the text  we have added several programming problems and projects that use the posix and win32 apis  as well as java we have added more than 15 new programming problems  which emphasize processes  threads  shared memory  process synchronization  and networking in addition  we have added or modified several programming projects that are more involved than standard programming exercises these projects include adding a system call to the linux kernel  using pipes on both unix and windows systems  using unix message queues  creating multithreaded applications  and solving the producer-consumer problem using shared memory  the eighth edition also incorporates a set of operating-system simulators designed by steven robbins of the university of texas at san antonio the simulators are intended to model the behavior of an operating system as it performs various tasks  such as cpu and disk-head schedulil1.g  process creation and interprocess communication  starvation  and address translation these simulators are written in java and will run on any computer systern with java 1.4 students can download the simulators from wileyplus and observe the behavior of several operating system concepts in various scenarios in addition  each simulator includes several exercises that ask students to set certain parameters of the simulator  observe how the system behaves  and then explain this behavior these exercises can be assigned through wileyplus the wileyplus course also includes algorithmic problems and tutorials developed by scott m pike of texas a&m university  xiii the following teaching supplencents are available in wileyplus and on www.wiley.coml go i global/ silberschatz  a set of slides to accompany the book  model course syllabi  all c and java source code  up-to-date errata  three case study appendices and the distributed communication appendix  the wileyplus course also contains the simulators and associated exercises  additional practice exercises  with solutions  not found in the text  and a testbank of additional problems students are encouraged to solve the practice exercises on their own and then use the provided solutions to check their own answers  to obtain restricted supplements  such as the solution guide to the exercises in the text  contact your local j orne wiley & sons sales representative note that these supplements are available only to faculty who use this text  we use the mailman system for communication among the users of operating system concepts if you wish to use this facility  please visit the following url and follow the instructions there to subscribe  http  i i mailman.cs.yale.edul mailmanllistinfo i os-book the mailman mailing-list system provides many benefits  such as an archive of postings  as well as several subscription options  including digest and web only to send messages to the list  send e-mail to  os-book @ cs.yale.edu depending on the message  we will either reply to you personally or forward the message to everyone on the mailing list the list is moderated  so you will receive no inappropriate mail  students who are using this book as a text for class should not use the list to ask for answers to the exercises they will not be provided  we have attempted to clean up every error in this new edition  but-as happens with operating systems-a few obscure bugs may remain we would appreciate hearing from you about any textual errors or omissions that you identify  if you would like to suggest improvements or to contribute exercises  we would also be glad to hear from you please send correspondence to os-book-authors @ cs.yale.edu  this book is derived from the previous editions  the first three of which were coauthored by james peterson others who helped us with previous editions include hamid arabnia  rida bazzi  randy bentson  david black  xiv joseph boykin  jeff brumfield  gael buckley  roy campbell  p c capon  john carpenter  gil carrick  thomas casavant  bart childs  ajoy kum.ar datta  joe deck  sudarshan k dhall  thomas doeppner  caleb drake  m racsit eskicioglu  hans flack  robert fowler  g scott graham  richard guy  max hailperin  rebecca i-iartncan  wayne hathaway  christopher haynes  don heller  bruce hillyer  mark holliday  dean hougen  michael huangs  ahmed kamet marty kewstet richard kieburtz  carol kroll  marty k westet thomas leblanc  john leggett  jerrold leichter  ted leung  gary lippman  carolyn miller  michael molloy  euripides montagne  yoichi muraoka  jim m ng  banu ozden  ed posnak  boris putanec  charles qualline  john quarterman  mike reiter  gustavo rodriguez-rivera  carolyn j c schauble  thomas p  skimcer  yannis smaragdakis  jesse st laurent  john stankovic  adam stauffer  steven stepanek  john sterling  hal stern  louis stevens  pete thomas  david umbaugh  steve vinoski  tommy wagner  larry l wear  jolm werth  james m westall  j s weston  and yang xiang parts of chapter 12 were derived from a paper by hillyer and silberschatz  1996   parts of chapter 17 were derived from a paper by levy and silberschatz  1990   chapter 21 was derived from an unpublished manuscript by stephen tweedie chapter 22 was derived from an unpublished manuscript by dave probert  cliff martin  and avi silberschatz appendix c was derived from an unpublished manuscript by cliff martin cliff martin also helped with updating the unix appendix to cover freebsd some of the exercises and accompanying solutions were supplied by arvind krishnamurthy  mike shapiro  bryan cantrill  and jim mauro answered several solarisrelated questions bryan cantrill from sun microsystems helped with the zfs coverage steve robbins of the university of texas at san antonio designed the set of simulators that we incorporate in wileyplus reece newman of westminster college initially explored this set of simulators and their appropriateness for this text josh dees and rob reynolds contributed coverage of microsoft 's .net the project for posix message queues was contributed by john trona of saint michael 's college in colchester  vermont  marilyn turnamian helped generate figures and presentation slides mark wogahn has made sure that the software to produce the book  e.g  latex macros  fonts  works properly  our associate publisher  dan sayre  provided expert guidance as we prepared this edition he was assisted by carolyn weisman  who managed many details of this project smoothly the senior production editor ken santor  was instrumental in handling all the production details lauren sapira and cindy jolmson have been very helpful with getting material ready and available for wileyplus  beverly peavler copy-edited the manuscript the freelance proofreader was katrina avery ; the freelance indexer was word co  inc  abraham silberschatz  new haven  ct  2008 peter baer galvin  burlington  ma 2008 greg gagne  salt lake city  ut  2008 part one overview chapter 1 introduction 1.1 what operating systems do 3 1.2 computer-system organization 6 1.3 computer-system architecture 12 1.4 operating-system sh ucture 18 1.5 operating-system operations 20 1.6 process management 23 1.7 memory management 24 1.8 storage management 25 chapter 2 system structures 2.1 operating-system services 49 2.2 user operating-system interface 52 2.3 system calls 55 2.4 types of system calls 58 2.5 system programs 66 2.6 operating-system design and implementation 68 2.7 operating-system structure 70 1.9 protection and security 29 1.10 distributed systems 30 1.11 special-purpose systems 32 1.12 computing environments 34 1.13 open-source operating systems 37 1.14 summary 40 exercises 42 bibliographical notes 46 2.8 virtual machines 76 2.9 operating-system debugging 84 2.10 operating-system generation 88 2.11 system boot 89 2.12 summary 90 exercises 91 bibliographical notes 97 part two process management chapter 3 process concept 3.1 process concept 101 3.2 process scheduling 105 3.3 operations on processes 110 3.4 interprocess communication 116 3.5 examples of ipc systems 123 3.6 communication in clientserver systems 128 3.7 summary 140 exercises 141 bibliographical notes 152 xv xvi chapter 4 multithreaded programming 4.1 overview 153 4.2 multithreading models 157 4.3 thread libraries 159 4.4 threading issues 165 chapter 5 process scheduling 5.1 basic concepts 183 5.2 scheduling criteria 187 5.3 scheduling algorithms 188 5.4 thread scheduling 199 5.5 multiple-processor scheduling 200 4.5 operating-system examples 171 4.6 summary 174 exercises 174 bibliographical notes 181 5.6 operating system examples 206 5.7 algorithm evaluation 213 5.8 summary 217 exercises 218 bibliographical notes 222 part three process coordination chapter 6 synchronization 6.1 backgrmmd 225 6.2 the critical-section problem 227 6.3 peterson 's solution 229 6.4 synchronization hardware 231 6.5 semaphores 234 6.6 classic problems of synchronization 239 chapter 7 deadlocks 7.1 system model 283 7.2 deadlock characterization 285 7.3 methods for handling deadlocks 290 7.4 deadlock prevention 291 7.5 deadlock avoidance 294 6.7 monitors 244 6.8 synchronization examples 252 6.9 atomic transactions 257 6.10 summary 267 exercises 267 bibliographical notes 280 7.6 deadlock detection 301 7.7 recovery from deadlock 304 7.8 summary 306 exercises 307 bibliographical notes 310 part four memory management chapter 8 memory-management strategies 8.1 background 315 8.2 swapping 322 8.3 contiguous memory allocation 324 8.4 paging 328 8.5 structure of the page table 337 8.6 segmentation 342 8.7 example  the intel pentium 345 8.8 summary 349 exercises 350 bibliographical notes 354 xvii chapter 9 virtual-memory management 9.1 background 357 9.2 demand paging 361 9.3 copy-on-write 367 9.4 page replacement 369 9.5 allocation of frames 382 9.6 thrashing 386 9.7 memory-mapped files 390 9.8 allocating kernel memory 396 9.9 other considerations 399 9.10 operating-system examples 405 9.11 summary 407 exercises 409 bibliographical notes 416 part five storage management chapter 10 file system 10.1 file concept 421 10.2 access methods 430 10.3 directory and disk structure 433 10.4 file-system mounting 444 10.5 file sharing 446 10.6 protection 451 10.7 summary 456 exercises 457 bibliographical notes 458 chapter 11 implementing file systems 11.1 file-system structure 461 11.2 file-system implementation 464 11.3 directory implementation 470 11.4 allocation methods 471 11.5 free-space management 479 11.6 efficiency and performance 482 11.7 recovery 486 11.8 nfs 490 11.9 example  the wafl file system 496 11.10 summary 498 exercises 499 bibliographical notes 502 chapter 12 secondary-storage structure 12.1 overview of mass-storage structure 505 12.2 disk structure 508 12.3 disk attachment 509 12.4 disk scheduling 510 12.5 disk man.agement 516 12.6 swap-space management 520 chapter 13 i/0 systems 13.1 overview 555 13.2 i/0 hardware 556 13.3 application i/0 interface 565 13.4 kernel i/0 subsystem 571 13.5 transforming i/0 requests to hardware operations 578 12.7 raid structure 522 12.8 stable-storage implementation 533 12.9 tertiary-storage struchue 534 12.10 summary 543 exercises 545 bibliographical notes 552 13.6 streams 580 13.7 performance 582 13.8 summary 585 exercises 586 bibliographical notes 588 xviii part six protection and security chapter 14 system protection 14.1 goals of protection 591 14.2 principles of protection 592 14.3 domain of protection 593 14.4 access matrix 598 14.5 implementation of access matrix 602 14.6 access control 605 chapter 15 system security 15.1 the security problem 621 15.2 program threats 625 15.3 system and network threats 633 15.4 cryptography as a security tool 638 15.5 user authentication 649 15.6 implementing security defenses 654 15.7 firewalling to protect systems and networks 661 14.7 revocation of access rights 606 14.8 capability-based systems 607 14.9 language-based protection 610 14.10 surnmary 615 exercises 616 bibliographical notes 618 15.8 computer-security classifications 662 15.9 an example  windows xp 664 15.10 summary 665 exercises 666 bibliographical notes 667 part seven distributed systems chapter 16 distributed operating systems 16.1 motivation 673 16.2 types of networkbased operating systems 675 16.3 network structure 679 16.4 network topology 683 16.5 communication structure 684 16.6 communication protocols 690 16.7 robustness 694 16.8 design issues 697 16.9 an example  networking 699 16.10 summary 701 exercises 701 bibliographical notes 703 chapter 17 distributed file systems 17.1 background 705 17.2 naming and transparency 707 17.3 remote file access 710 17.4 stateful versus stateless service 715 17.5 file replication 716 17.6 an example  afs 718 17.7 summary 723 exercises 724 bibliographical notes 725 chapter 18 distributed synchronization 18.1 event ordering 727 18.2 mutual exclusion 730 18.3 atomicity 733 18.4 concurrency control 736 18.5 deadlock handling 740 18.6 election algorithms 747 18.7 reaching agreement 750 18.8 summary 752 exercises 753 bibliographical notes 754 part eight special purpose systems chapter 19 real-time systems 19.1 overview 759 19.2 system characteristics 760 19.3 features of real-time kernels 762 19.4 implementing real-time operating systems 764 19.5 real-time cpu scheduling 768 19.6 an example  vxworks 5.x 774 19.7 summary 776 exercises 777 bibliographical notes 777 chapter 20 multimedia systems 20.1 what is multimedia 779 20.2 compression 782 20.3 requirements of multimedia kernels 784 20.4 cpu scheduling 786 20.5 disk scheduling 787 20.6 network management 789 20.7 an example  cineblitz 792 20.8 summary 795 exercises 795 bibliographical notes 797 part nine case studies chapter 21 the linux system 21.1 linux history 801 21.2 design principles 806 21.3 kernel modules 809 21.4 process management 812 21.5 scheduling 815 21.6 memory management 820 21.7 file systems 828 chapter 22 windows xp 22.1 history 847 22.2 design principles 849 22.3 system components 851 22.4 environmental subsystems 874 22.5 file system 878 21.8 input and output 834 21.9 interprocess communication 837 21.10 network structure 838 21.11 security 840 21.12 summary 843 exercises 844 bibliographical notes 845 22.6 networking 886 22.7 programmer interface 892 22.8 sum.mary 900 exercises 900 bibliographical notes 901 chapter 23 influential operating systems 23.1 feature migration 903 23.2 early systems 904 23.3 atlas 911 23.4 xds-940 912 23.5 the 913 23.6 rc 4000 913 23.7 ctss 914 23.8 multics 915 23.9 ibm os/360 915 23.10 tops-20 917 23.11 cp/m and ms/dos 917 23.12 macintosh operating system and windows 918 23.13 mach 919 23.14 other systems 920 exercises 921 xix xx chapter a bsd unix a1 unix history 1 a2 design principles 6 a3 programmer interface 8 a.4 user interface 15 as process management 18 a6 memory management 22 appendix b the mach system b.l history of the mach system 1 b.2 design principles 3 b.3 system components 4 b.4 process management 7 b.s interprocess conununication 13 b.6 memory management 18 appendix c windows 2000 c.1 history 1 c.2 design principles 2 c.3 system components 3 c.4 enviromnental subsystems 19 c.s file system 22 bibliography 923 credits 941 index 943 a7 file system 25 as i/0 system 32 a9 interprocess communication 35 alo summary 40 exercises 41 bibliographical notes 42 b.7 programmer interface 23 b.s summary 24 exercises 25 bibliographical notes 26 credits 27 c.6 networking 28 c.7 programmer interface 33 c.s summary 40 exercises 40 bibliographical notes 41 part one an operating system acts as an intermediary between the user of a computer and the computer hardware the purpose of an operating system is to provide an environment in which a user can execute programs in a convenient and efficient manner  an operating system is software that manages the computer hardware  the hardware must provide appropriate mechanisms to ensure the correct operation of the computer system and to prevent user programs from interfering with the proper operation of the system  internally  operating systems vary greatly in their makeup  since they are organized along many different lines the design of a new operating system is a major task it is impmtant that the goals of the system be well defined before the design begins these goals form the basis for choices among various algorithms and strategies  because an operating system is large and complex  it must be created piece by piece each of these pieces should be a well delineated portion of the system  with carefully defined inputs  outputs  and functions  1.1 ch er an is a program that manages the computer hardware it also provides a basis for application programs and acts as an intermediary between the computer user and the computer hardware an amazing aspect of operating systems is how varied they are in accomplishing these tasks  mainframe operating systems are designed primarily to optimize utilization of hardware personal computer  pc  operating systems support complex games  business applications  and everything in between operating systems for handheld computers are designed to provide an environment in which a user can easily interface with the computer to execute programs thus  some operating systems are designed to be convenient  others to be efficient  and others some combination of the two  before we can explore the details of computer system operation  we need to know something about system structure we begin by discussing the basic functions of system startup  i/0  and storage we also describe the basic computer architecture that makes it possible to write a functional operating system  because an operating system is large and complex  it must be created piece by piece each of these pieces should be a well-delineated portion of the system  with carefully defined inputs  outputs  and functions in this chapter  we provide a general overview of the major components of an operating system  to provide a grand tour of the major components of operating systems  to describe the basic organization of computer systems  we begin our discussion by looking at the operating system 's role in the overall computer system a computer system can be divided roughly into 3 4 chapter 1 compiler assembler text editor operating system database system figure 1.1 abstract view of the components of a computer system  four components  the hardware/ the operating system  the application programs/ and the users  figure 1.1   the hardwa.te-the the and the ievices-provides the basic computing resources for the system the as word processors/ spreadsheets/ compilers  and web browsers-define the ways in which these resources are used to solve users ' computing problems the operating system controls the hardware and coordinates its use among the various application programs for the various users  we can also view a computer system as consisting of hardware/ software/ and data the operating system provides the means for proper use of these resources in the operation of the computer system an operating system is similar to a government like a government  it performs no useful function by itself it simply provides an environment within which other programs can do useful work  to understand more fully the operating systemfs role  we next explore operating systems from two viewpoints  that of the user and that of the system  1.1.1 user view the user 's view of the computer varies according to the interface being used most computer users sit in front of a pc  consisting of a monitor/ keyboard/ mouse  and system unit such a system is designed for one user to monopolize its resources the goal is to maximize the work  or play  that the user is performing in this case/ the operating system is designed mostly for with some attention paid to performance and none paid to various hardware and software resources are shared performance is  of course  important to the user ; but such systems 1.1 5 are optimized for the single-user experience rather than the requirements of multiple users  in other cases  a user sits at a terminal connected to a or a other users are accessing the sance computer through other terminals these users share resources and may exchange information the operating system in s llclc cases is designed to maximize resource utilizationto assure that all available cpu time  memory  and i/0 are used efficiently and tbat no individual user takes more than her fair share  in still otber cases  users sit at connected to networks of other workstations and these users have dedicated resources at their disposal  but they also share resources such as networking and servers-file  compute  and print servers therefore  their operating system is designed to compromise between individual usability and resource utilization  recently  many varieties of handheld computers have come into fashion  most of these devices are standalone units for individual users some are connected to networks  either directly by wire or  more often  through wireless modems and networking because of power  speed  and interface limitations  they perform relatively few remote operations their operating systems are designed mostly for individual usability  but performance per unit of battery life is important as well  some computers have little or no user view for example  embedded computers in home devices and automobiles may have numeric keypads and may turn indicator lights on or off to show status  but they and their operating systems are designed primarily to run without user intervention  1.1.2 system view from the computer 's point of view  the operating system is the program most intimately involved with the hardware in this context  we can view an operating system as a  a computer system has many resources that may be required to solve a problem  cpu time  memory space  file-storage space  i/0 devices  and so on the operating system acts as the manager of these resources facing numerous and possibly conflicting requests for resources  the operating system must decide how to allocate them to specific programs and users so that it can operate the computer system efficiently and fairly as we have seen  resource allocation is especially important where many users access the same mainframe or minicomputer  a slightly different view of an operating system emphasizes the need to control the various i/0 devices and user programs an operating system is a control program a manages the execution of user programs to prevent errors and improper use of the computer it is especially concerned with the operation and control of i/o devices  1.1.3 defining operating systems we have looked at the operating system 's role from the views of the user and of the system how  though  can we define what an operating system is in general  we have no completely adequate definition of an operating system operating systems exist because they offer a reasonable way to solve the problem of creating a usable computing system the fundamental goal of computer systems is to execute user programs and to make solving user 6 chapter 1 1.2 storage definitions and notation a is the basic unit of computer storage it can contain one of two values  zero and one all other storage in a computer is based on collections of bits  given enough bits  it is amazing how many things a computer can represent  numbers  letters  images  movies  sounds  documents  and programs  to name a few a is 8 bits  and on most computers it is the smallest convenient chunk of storage for example  most computers do n't have an instruction to move a bit but do have one to move a byte a less common term is which is a given computer architecture 's native storage unit a word is generally made up of one or more bytes for example  a computer may have instructions to move 64-bit  8-byte  words  a kilobyte  or kb  is 1,024 bytes ; a megabyte  or mb  is 1,0242 bytes ; and a gigabyte  or gb  ! s 1,0243 bytes computer manufacturers often round off these numbers and say that a megabyte is 1 million bytes and a gigabyte is 1 billion bytes  problems easier toward this goal  computer hardware is constructed since bare hardware alone is not particularly easy to use  application programs are developed these programs require certain common operations  such as those controlling the ii 0 devices the common functions of controlling and allocating resources are then brought together into one piece of software  the operating system  in addition  we have no universally accepted definition of what is part of the operating system a simple viewpoint is that it includes everything a vendor ships when you order the operating system the features included  however  vary greatly across systems some systems take up less than 1 megabyte of space and lack even a full-screen editor  whereas others require gigabytes of space and are entirely based on graphical windowing systems a more common definition  and the one that we usually follow  is that the operating system is the one program running at all times on the computer-usually called the   along with the kernel  there are two other types of programs  which are associated with the operating system but are not part of the kernel  and which include all programs not associated with the operation of the system  the matter of what constitutes an operating system has become increasingly important in 1998  the united states deparhnent of justice filed suit against microsoft  in essence claiming that microsoft included too much functionality in its operating systems and thus prevented application vendors from competing for example  a web browser was an integral part of the operating systems as a result  microsoft was found guilty of using its operating-system monopoly to limit competition  before we can explore the details of how computer systems operate  we need general knowledge of the structure of a computer system in this section  we look at several parts of this structure the section is mostly concerned 1.2 the study ofoperating systems there has neverbeenarnore interestirighnwtostud yoperating systems  and it has neverb.een.e ~ sier.theopen-sourc ; e movernent has overtaken .operating systems  caj.tsing marly ofthenctobemadeavailable in both source and binary  e ~ ecuta  jle  fonnat .this iistindud ~ ~ linu    bsdunix/solat is,and part of  \ ii ~ cos.x th ~ availa ~ ilityqf source.code.q,llowsus.tostudyoperq,til  .gsy tems frorrt theinsid,eout '  questionsthat previo  1sly could onlyb ~ answerecl ~ y looking atdocumentaticmor thebehayior.ofan op ~ rating system c.annow be answered by examining the code itself  in additi n the rise of virtualization as a ll  .ainsfreafll  andfrequelltly free  cmnp  1ter ftmctionmakesitpos ; ~ i1jlet   runnmnyoperqtingsystems.ontop.of onecoresystem  forexample,vmware  j  lttp  //www  vmware   .com   provides afree ''player' ' on which hundreds.of free .''virtualappliilnces' ' cann.m.using this method,students call tryolit hundreds ofoperatingsystems.withintheir existing operatingsystems .atno cost          operating .sy ~ temsthat are no lortge ~ ~ ofllmerci ~ lly viableltave been opell ~ o  lrced asvvell  enablirtg .usto study how system ~ pperated i ~ time.of  f ~ v.r ~ r cpu  ll  .emory  etnd.storcrge .resoj.trces  .an  exten ~ iye.b  .it not complete   list   f 9pen'-sourct operafirtg system pr j ~ ts is  availa ~ le rom ht ~ p  // dm   ~ ' org/ c  omp  1ters/softp  lre /operati g  -systems/p ~ ~ m._sourc ~ / s i..m   .u l a t .o r s  o .f s  p e  c i ..f i c  ....h a  ...r ....d w  .a  r e   ar..e  .a l s .o   a   v  a.i.l 1 b  .le  .i n    s om   .e   .c  a  s e s '  al i ....o w   m ...g th ~ operat ~ g systell  .to.runon.''na ~ ve''.hardware   all ~ ithrrtthec l  .fines of a modem co ! tipj-iter and moderj1 opf/'atirtg ~ ystem for  example  a decsystemc20 simulator running on mac os x can boot tops-20  loa ~  the ~ ource.tages ;  and modify al'ld comp ~ le l .j  t.evvtops-20 .k ~ rnel art interested stltdent ~ ar search theint ~ rnet to find the origillal papers that de ~ cribe the operating systemand  the.origipa ~ manuals  tl e adve ~ t fogen-source operafirtg sy ~ te1tis also l   lal es it easy t .make the move fromstu ~ enttooper  lting ~ systemdeveloper.with some knov.rledge  som ~ effo1't  a11d an internet connection,a student c ; al'leven create a new operating-systemdistribution ! justa fev.r years  ~ go itwas diffic  _llt or if1lpossible to get acce ~ s  to source co e   n v.r  that access is lijnited only bylt   wmuchtimeand disk space a student has  7 with computer-system organization  so you can skim or skip it if you already understand the concepts  1.2.1 computer-system operation a modern general-purpose computer system consists of one or more cpus and a number of device controllers connected through a common bus that provides access to shared memory  figure 1.2   each device controller is in charge of a specific type of device  for example  disk drives  audio devices  and video displays   the cpu and the device controllers can execute concurrently  competing for memory cycles to ensure orderly access to the shared memory  a memory controller is provided whose function is to synchronize access to the memory  for a computer to start rum ing-for instance  when it is powered up or rebooted-it needs to have an initial program to run this initial 8 chapter 1 mouse keyboard printer monitor o ~ ~ ~  _rlo i-nneh b figure 1.2 a modern computer system  program  or tends to be simple typically  it is stored in read-only memory or electrically erasable programmable read-only memory known by the general term within the computer hardware it initializes all aspects of the system  from cpu registers to device controllers to memory contents the bootstrap program must know how to load the operating system and how to start executing that system to accomplish this goal  the bootstrap program must locate and load into memory the operatingsystem kernel the operating system then starts executing the first process  such as init  and waits for some event to occur  the occurrence of an event is usually signaled by an from either the hardware or the software hardware may trigger an interrupt at any time by sending a signal to the cpu  usually by way of the system bus software may trigger an interrupt executing a special operation called a  also called a when the cpu is interrupted  it stops what it is doing and immediately transfers execution to a fixed location the fixed location usually contains the starting address where the service routine for the interrupt is located  the interrupt service routine executes ; on completion  the cpu resumes the interrupted computation a time line of this operation is shown in figure 1.3  interrupts are an important part of a computer architecture each computer design has its own interrupt mechanism  but several functions are common  the interrupt must transfer control to the appropriate interrupt service routine  the straightforward method for handling this transfer would be to invoke a generic routine to examine the interrupt information ; the routine  in turn  would call the interrupt-specific handler however  interrupts must be handled quickly since only a predefined number of interrupts is possible  a table of pointers to interrupt routines can be used instead to provide the necessary speed the interrupt routine is called indirectly through the table  with no intermediate routine needed generally  the table of pointers is stored in low memory  the first hundred or so locations   these locations hold the addresses of the interrupt service routines for the various devices this array  or of addresses is then indexed by a unique device number  given with the interrupt request  to provide the address of the interrupt service routine for cpu user 1/0 device process executing 1/0 interrupt processing idle ~ ~  ~  tmcefeniog i l  1/0 request 1.2 ll v  ~ ~ ' ''''' ' ~ ' ' '  ~ ~  ~ ~   t ~  ' 'm '  l  ~ ~ ~ ~ transfer done 1/0 transfer request done figure 1.3 interrupt time line for a single process doing output  9 the interrupting device operating systems as different as windows and unix dispatch interrupts in this manner  the interrupt architecture must also save the address of the interrupted instruction many old designs simply stored the interrupt address in a fixed location or in a location indexed by the device number more recent architectures store the return address on the system stack if the interrupt routine needs to modify the processor state-for instance  by modifying register values-it must explicitly save the current state and then restore that state before returning after the interrupt is serviced  the saved return address is loaded into the program counter  and the interrupted computation resumes as though the interrupt had not occurred  1.2.2 storage structure the cpu can load instructions only from memory  so any programs to run must be stored there general-purpose computers run most of their programs from rewriteable memory  called main memory  also called or ram   main commonly is implemented in a semiconductor technology called computers use other forms of memory as well because the read-only memory  rom  camwt be changed  only static programs are stored there the immutability of rom is of use in game cartridges eeprom camwt be changed frequently and so contains mostly static programs for example  smartphones have eeprom to store their factory-il stalled programs  all forms of memory provide an array of words each word has its own address interaction is achieved through a sequence of load or store instructions to specific memory addresses the load instruction moves a word from main memory to an internal register within the cpu  whereas the store instruction moves the content of a register to main memory aside from explicit loads and stores  the cpu automatically loads instructions from main memory for execution  a typical instruction-execution cycle  as executed on a system with a architecture  first fetches an il1struction from memory and stores that instruction in the  the instruction is then decoded and may cause operands to be fetched from memory and stored in some 10 chapter 1 internal register after the instruction on the operands has been executed  the result may be stored back in memory notice that the memory unit sees only a stream of memory addresses ; it does not know how they are generated  by the instruction counter  indexing  indirection  literal addresses  or some other means  or what they are for  instructions or data   accordingly  we can ignore how a memory address is generated by a program we are interested only in the sequence of memory addresses generated by the running program  ideally  we want the programs and data to reside in main ncemory permanently this arrangement usually is not possible for the following two reasons  main memory is usually too small to store all needed programs and data permanently  main memory is a volatile storage device that loses its contents when power is turned off or otherwise lost  thus  most computer systems provide as an extension of main memory the main requirement for secondary storage is that it be able to hold large quantities of data permanently  the most common secondary-storage device is a which provides storage for both programs and data most programs  system and application  are stored on a disk until they are loaded into memory many programs then use the disk as both the source and the destination of their processing hence  the proper management of disk storage is of central importance to a computer system  as we discuss in chapter 12  in a larger sense  however  the storage structure that we have describedconsisting of registers  main memory  and magnetic disks-is only one of many possible storage systems others include cache memory  cd-rom  magnetic tapes  and so on each storage system provides the basic functions of storing a datum and holding that datum until it is retrieved at a later time the main differences among the various storage systems lie in speed  cost  size  and volatility  the wide variety of storage systems in a computer system can be organized in a hierarchy  figure 1.4  according to speed and cost the higher levels are expensive  but they are fast as we move down the hierarchy  the cost per bit generally decreases  whereas the access time generally increases this trade-off is reasonable ; if a given storage system were both faster and less expensive than another-other properties being the same-then there would be no reason to use the slower  more expensive memory in fact  many early storage devices  including paper tape and core memories  are relegated to museums now that magnetic tape and have become faster and cheaper the top four levels of memory in figure 1.4 may be constructed using semiconductor memory  in addition to differing in speed and cost  the various storage systems are either volatile or nonvolatile as mentioned earlier  loses its contents when the power to the device is removed in the absence of expensive battery and generator backup systems  data must be written to for safekeeping in the hierarchy shown in figure 1.4  the the electronic disk are volatile  whereas those below 1.3 15 figure 1.6 symmetric multiprocessing architecture  solaris the benefit of this model is that many processes can run simultaneously -n processes can run if there are n cpus-without causing a significant deterioration of performance however  we must carefully control i/0 to ensure that the data reach the appropriate processor also  since the cpus are separate  one may be sitting idle while another is overloaded  resulting in inefficiencies these inefficiencies can be avoided if the processors share certain data structures a multiprocessor system of this form will allow processes and resources-such as memory-to be shared dynamically among the various processors and can lower the variance among the processors such a system must be written carefully  as we shall see in chapter 6 virtually all modern operating systems-including windows  windows xp  mac os x  and linux -now provide support for smp  the difference between symmetric and asymmetric multiprocessing may result from either hardware or software special hardware can differentiate the multiple processors  or the software can be written to allow only one master and multiple slaves for instance  sun 's operating system sunos version 4 provided asymmetric multiprocessing  whereas version 5  solaris  is symmetric on the same hardware  multiprocessing adds cpus to increase computing power if the cpu has an integrated memory controller  then adding cpus can also increase the amount of memory addressable in the system either way  multiprocessing can cause a system to change its memory access model from uniform memory access to non-uniform memory access uma is defined as the situation in which access to any ram from any cpu takes the same amount of time with numa  some parts of memory may take longer to access than other parts  creating a performance penalty operating systems can minimize the numa penalty through resource management_  as discussed in section 9.5.4  a recent trend in cpu design is to in.clude multiple computing on a single chip in essence  these are multiprocessor chips they can be more efficient than multiple chips with single cores because on-chip communication is faster than between-chip communication in addition  one chip with multiple cores uses significantly less power than multiple single-core chips as a result  multicore systems are especially well suited for server systems such as database and web servers  16 chapter 1 figure 1.7 a dual-core design with two cores placed on the same chip  in figure 1.7  we show a dual-core design with two cores on the same chip in this design  each core has its own register set as well as its own local cache ; other designs might use a shared cache or a combination of local and shared caches aside from architectural considerations  such as cache  memory  and bus contention  these multicore cpus appear to the operating system as n standard processors this tendency puts pressure on operating system designers-and application programmers-to make use of those cpus  finally  are a recent development in which multiple processor boards  i/0 boards  and networking boards are placed in the same chassis  the difference between these and traditional multiprocessor systems is that each blade-processor board boots independently and runs its own operating system some blade-server boards are n1.ultiprocessor as well  which blurs the lines between types of computers in essence  these servers consist of multiple independent multiprocessor systems  1.3.3 clustered systems another type of multiple-cpu system is the like multiprocessor systems  clustered systems gather together multiple cpus to accomplish computational work clustered systems differ from multiprocessor systems  however  in that they are composed of two or more individual systems-or nodes-joined together the definition of the term clustered is not concrete ; many commercial packages wrestle with what a clustered system is and why one form is better than another the generally accepted definition is that clustered computers share storage and are closely linked via a jc'.h.a  o.x  as described in section 1.10  or a faster interconnect  such as infiniband  clustering is usually used to provide service ; that is  service will continue even if one or more systems in the cluster fail high availability is generally obtained by adding a level of redundancy in the system a layer of cluster software runs on the cluster nodes each node can monitor one or more of the others  over the lan   if the monitored machine fails  the monitoring machine can take ownership of its storage and restart the applications that were running on the failed machine the users and clients of the applications see only a brief interruption of service  1.3 beowulf clusters beowulf clusters are designed for solving high-performance computing tasks these clusters are built using comm.odi ty hard ware-such as personal computers-that are connected via a simple local area network interestingly  a beowulf duster uses no one specific software package but rather consists of a set of open-source software libraries that allow the con1puting nodes in the cluster to communicate with one another  thus,.there are a variety of approaches for constructing a beowulf cluster  although beowulf computing nodes typically run the linux operating system since beowulf clusters require no special hardware and operate using open ~ source software that is freely available  they offer a low-cost strategy for building a high ~ performance computing cluster in fact  some beowulf clusters built from collections of discarded personal computers are using ht.mdreds of cornputing nodes to solve computationally expensive problems in scientific computing  clusterin.g can be structured or symmetrically in 17 one machine is in while the other is rmming the applications the hot-standby host machine does nothing but monitor the active server if that server fails  the hot-standby host becomes the active server in two or more hosts are rmming applications and are monitoring each other this mode is obviously more efficient  as it uses all of the available hardware it does require that more than one application be available to run  as a cluster consists of several clusters may also be used to provide environments  such systems can supply significantly greater computational power than single-processor or even smp systems because they are capable of running an application concurrently on all computers in the cluster however  applications must be written to take advantage of the cluster by using a technique known as which consists of dividing a program into separate components that run in parallel on individual computers in the cluster typically  these applications are designed so that once each computing node in the cluster has solved its portion of the problem  the results from all the nodes are combined into a final solution  other forms of clusters include parallel clusters and clustering over a wide-area network  wan   as described in section 1.10   parallel clusters allow multiple hosts to access the same data on the shared storage because most operating systems lack support for simultaneous data access by multiple hosts  parallel clusters are usually accomplished by use of special versions of software and special releases of applications for example  oracle real application cluster is a version of oracle 's database that has been designed to run on a parallel cluster each machine runs oracle  and a layer of software tracks access to the shared disk each machine has full access to all data in the database to provide this shared access to data  the system must also supply access control and locking to ensure that no conflicting operations occur this function  commonly known as a is included in some cluster technology  18 chapter 1 1.4 interconnect interconnect computer computer computer figure 1.8 general structure of a clustered system  cluster technology is changing rapidly some cluster products support dozens of systems in a cluster  as well as clustered nodes that are separated by miles many of these improvements are made possible by  saj ~ is   as described in section 12.3.3  which allow many systems to attach to a pool of storage if the applications and their data are stored on the san  then the cluster software can assign the application to run on any host that is attached to the san if the host fails  then any other host can take over in a database cluster  dozens of hosts can share the same database  greatly increasing performance and reliability figure 1.8 depicts the general structure of a clustered system  now that we have discussed basic information about computer-system organization and architecture  we are ready to talk about operating systems  an operating system provides the envirorunent within which programs are executed internally  operating systems vary greatly in their makeup  since they are organized along many different lines there are  however  many commonalities  which we consider in this section  one of the most important aspects of operating systems is the ability to multiprogram a single program can not  in generat k ~ ~ p ~ ith_er thg cpu ortbt j/qgey  ic  es 1jusy_c1t all times  single users frequently have multiple programs running il.ul increases cpu utilization byorganizing jobs  codeand datafso      _ hasoi1  0to execl1te   fhe idea is as follows  the op-ei  atlng system keeps several jobs in memory simultaneously  figure 1.9   since  in generat main memory is too small to accommodate all jobs  the jobs are kept initially on the disk in the this pool consists of all processes residing on disk awaiting allocation of main memory  ih ~ setofjobs inmemg_ry_canbe asubt  ; et of the jobs kept in thejql  jpoo1  the operating system picks and begins to execute one of the jobs in memory  eventually  the job may have to wait for some task  such as an i/o operation  1.4 19 figure 1.9 memory layout for a multiprogramming system  !   _c   _tnpl ~ te  in a non-multiprogrammed system  the cpu would sit idle in a multiprogrammed system  the operatilcg system simply switches to  and executes  another job when that job needs to wait  the cpu is switched to another job  and so on eventually the first job finishes waiting and gets the cpu back as long as at least one job needs to execute  the cpu is never idle  this idea is common in other life situations a lawyer does not work for only one client at a time  for example while one case is waiting to go to trial or have papers typed  the lawyer can work on another case if he has enough clients  the lawyer will never be idle for lack of work  idle lawyers tend to become politicians  so there is a certain social value in keeping lawyers busy  multiprogrammed systems provide an environment in which the various system resources  for example  cpu  memory  and peripheral devices  are utilized effectively  but they do not provide for user interaction with the computer system is_ ~ l   gi ~ alex_tension of multiprogramming ~ ' time-s ! caring syste ~ s,the cpl  execu ~ eslnl1ltiplejobs by switcll.ing ~ ainong them  but the switches occur so frequently that the ~ 1sers canh ~ teract with eachprograffi ~ v ere l.t1sil.mning. -ti1ne shar  il ~ g requi.i-es an    or  which provides direct communication between the user and the system the user gives instructions to the operating system or to a program directly  using a input device such as a keyboard or a mouse  and waits for immediate results on an output device accordingly  ! ! 'te sho ~ 1ld be sh   rt = typically less than one second  a time-shared operating system allows many users to share the computer simultaneously since each action or command in a time-shared system tends to be short  only a little cpu time is needed for each user as the system switches rapidly from one user to the next  each user is given the impression that the entire computer system is dedicated to his use  even though it is being shared among many users  a time-shared operating system 11ses cpu scheduling and multiprogramming to provide each user with a small portion of a time-shared computer  eachuserhas atleast or  t_e s parateprogra111inmemory a program loaded into 20 1.5 chapter 1 memory and executing is called a when a process executes  it typically executes for only a short tirne it either finishes or needs to perform i/0  i/0 may be interactive ; that is  output goes to a display for the user  and input comes from a user keyboard  mouse  or other device since interactive i/0 typically runs at people speeds  it may take a long time to complete input  for example  may be bounded by the user 's typing speed ; seven characters per second is fast for people but incredibly slow for computers rather than let the cpu sit idle as this interactive input takes place  the operating system will rapidly switch the cpu to the program of some other user  time sharing and multiprogramming require that several jobs be kept simultaneously in memory if several jobs are ready to be brought into memory  and if there is not enough room for all of them  then the system must choose among them making this decision is which is discussed in chapter 5 when the operating system selects a job from the job pool  it loads that job into memory for execution having several programs in memory at the same time requires some form of memory management  which is covered in chapters 8 and 9 in addition  ! f_s ~ verajjq  jsaxere  lcly to rw ~ at the same time  the system must choose among them making this decision i ~ _ _ sd1,2dviii lg  which is discussed in chapter 5 finally  running multiple jobscoi ~ cl.lrl  ei1hy requires that their ability to affect one another be limited in all phases of the operating system  including process scheduling  disk storage  and memory management these considerations are discussed throughout the text  in a time-sharing system  the operating system must ensure reasonable response time  which is sometimes accomplished through where processes are swapped in and out of main memory to the disk a more common method for achieving this goal tec  hdiql1 ~ _fuc ! t _ cillqws._ the execution of aprocessthat isnot completely inl1le1yl_cld ~   chapter 9   the main advai1tage of the virtual-memory scheme is that it enables users to run programs that are larger than actual  further  it abstracts main memory into a large  uniform array of storage  separating logical as viewed by the user from physical memory this arrangement frees programmers from concern over memory-storage limitations  time-sharing systems must also provide a file system  chapters 10 and 11   the file system resides on a collection of disks ; hence  disk management must be provided  chapter 12   also  time-sharing systems provide a mechanism for protecting resources from inappropriate use  chapter 14   to ensure orderly execution  the system must provide mechanisms for job synchronization and communication  chapter 6   and it may ensure that jobs do not get stuck in a deadlock  forever waiting for one another  chapter 7    \ si1  e11tio11ecl ~ arlier  rn   cletnopexatli1ksystems_m ~ e _ if there are no processes to execute  no i/0 devices to service  and no users to whom to respond  an operating system will sit quietly waiting for something to happen events are almost always signaled by the occurrence of an interrupt or a trap  or an is_ a software ~ generated interruptca ~ seci ~ it  ler byan error  for division byzero or invalid memory acc ~ ss_  or by a specific request from a user program that an operating-system service 1.5 21 be performed the interrupt-driven nature of an operating system defines that system 's general structure for each type of interrupt  separate segments of code in the operating system determine what action should be taken an interrupt service routine is provided that is responsible for dealing with the interrupt  since the operating system and the users share the hardware and software resources of the computer system  we need to make sure that an error in a user program could cause problems only for the one program running with sharing  many processes could be adversely affected by a bug in one program  for example  if a process gets stuck in an infinite loop  this loop could prev.ent the correct operation of many other processes more subtle errors can occur in a multiprogramming system  where one erroneous program might modify another program  the data of another program  or even the operating system itself  without protection against these sorts of errors  either the computer must execute only one process at a time or all output must be suspect a properly designed operating system must ensure that an incorrect  or malicious  program can not cause other program ~ to  ~ x.t ; cute incorrectly  ~ ~  ; ~ ,_c  ; ..c ~ 1.5.1 dual-mode operation in order to ensure the proper execution of the operating system  we must be able to distinguish between the execution of operating-system code and userdefined code the approach taken by most computer systems is to provide hardware support that allows us to differentiate among various modes of execution  at the very least we need two and  also called or a bit  called the is added to the hardware of the computer to indicate the current mode  kernel  0  or user  1   \ ! viththeplode1  jit \ ! ve2lrea  jle to distinguishbetween a task that is executed onbehalf of the operating system aicd one that is executeci on behalfofthejjser  when tl ~ e computer systel.n1s executing on behalf of a user application  the system is in user mode however  when a user application requests a service from the operating system  via a  system call   it must transition from user to kernel mode to fulfill the request  / this is shown in figure 1.10 as we shall see  this architectural enhancement is useful for many other aspects of system operation as well  execute system call figure 1 i 0 transition from user to kernel mode  user mode  mode bit = i  kernel mode  mode bit = 0  22 chapter 1 at system boot time  the hardware starts in kernel mode the operating system is then loaded and starts user applications in user mode whenever a trap or interrupt occurs  the hardware switches from user mode to kernel mode  that is  changes the state of the mode bit to 0   thus  whenever the operating system gains control of the computer  it is in kernel mode the system always switches to user mode  by setting the mode bit to 1  before passing control to a user program  the dual mode of operation provides us with the means for protecting the operating system from errant users-and errant users from one another  ye _  ! cc011lplishthis protection by designating some ofthe machineine ; tructions ~ ha !  trlijjt cal1_sej ~ i  i ~ l11 ins trucrci \   l  il1e hardware all ~ \ \ 'spl iyileg ~ d instrl  ctionsto be o11ly inkern ~ ll11qq_ ~  if an attempt is made to execute a privileged instruction in user mode  the hardware does not execute the instruction but rather treats it as illegal and traps it to the operating system  the instruction to switch to kernel mode is an example of a privileged instruction some other examples include i/0 controt timer management and interrupt management as we shall see throughout the text  there are many additional privileged instructions  we can now see the life cycle of instruction execution in a computer system  initial control resides in the operating system  where instructions are executed in kernel mode when control is given to a user application  the mode is set to user mode eventually  control is switched back to the operating system via an interrupt  a trap  or a system call  _5ysiemcalls proyide the means for auser program to ask the operating 2  'st ~ m to perforp  t tasks re_ erved forjhe operating syst ~ m gr1 the 1.lser .12l  qgra1ll'sbeha,lf a system call is invoked in a variety of ways  depending on the functionality provided by the underlying processor in all forms  it is the method used by a process to request action by the operating system a system call usually takes the form of a trap to a specific location in the interrupt vector  this trap can be executed by a generic trap instruction  although some systems  such as the mips r2000 family  have a specific syscall instruction  when asystep1 calljs e   ecutect it is treated by the hardware as a software -i  rlt ~ rr  l.l   if  c   iltrol passes through the interrupt vector to a service routine in the operating system/ and the m   de bit is set to kernel mode the systemcaflserv1ce routine is a part of the operating system the-kernel examines the interrupting instruction to determine what system call has occurred ; a ~ parameter indicates what type of service the user program is requesting  additional information needed for the r ~ quest_may be passed in registers  on the stack/ or in memory  with pointers to the memory locations passed in registers   the kernel vedfies that the parameters are correct and legat executes ti1erequest  and returns control to the instruction following the system call we describe system calls more fully in section 2.3  the lack of a hardware-supported dual mode can cause serious shortcomings in an operating system for instance  ms-dos was written for the intel 8088 architecture  which has no mode bit and therefore no dual mode a user program rum1ing awry can wipe out the operating system by writing over it with data ; and multiple programs are able to write to a device at the same time  with potentially disastrous results recent versions of the intel cpu do provide dual-mode operation accordingly  most contemporary operating systemssuch as microsoft vista and windows xp  as well as unix  linux  and solaris 1.6 1.6 23 -take advantage of this dual-mode feature and provide greater protection for the operating system  once hardware protection is in place  it detects errors that violate modes  these errors are normally handled by the operating system if a user program fails in some way-such as by making an attempt either to execute an illegal instruction or to access memory that is not in the user 's address space-then the hardware traps to the operating system the trap transfers control through the interrupt vector to the operating system  just as an interrupt does when a program error occurs  the operating system must terminate the program abnormally this situation is handled by the same code as a user-requested abnormal termination an appropriate error message is given  and the memory of the program may be dumped the memory dump is usually written to a file so that the user or programmer can examine it and perhaps correct it and restart the program  1.5.2 timer wer  r1,_ust ensure th t ! the ope  j ; atil  gsystemij  taintains t  ontrol overthe c  j_ !  l_ ~ we cam1.ot allow a userp ~ ogram to_ get stuc  kin e1ninfinite loop or to fail to call syste1n seryices and never retltrn control to the c  perating system to ~ c  9 ! ll 1i  s ~ tl1.1s = g ~ at we_can usea _a_tirn ~ r_can beset to interrupt th ~ c  c  mp_ut ~ r af_t ~ ril p ~ c  ified peri   d the period may be fixed  for example  1/60 second  or variable  for example  from 1 millisecond to 1 second   a is generally implemented by a fixed-rate clock and a counter  the operating system sets the counter every time the clock ticks  the counter is decremented when the counter reaches 0  an interrupt occurs for instance  a 10-bit counter with a 1-millisecond clock allows interrupts at intervals from 1 millisecond to 1,024 milliseconds  in steps of 1 millisecond  before turning over control to the user  the operating system ensures that the timer is set to interrupt ll ~ ll ~ __ tij11e_ _il1t ~ rrl1pts/control transfers automatically totll.e   pel  9  t ~ ~ y ! epl,_ \ thicfl__ ! l-1  1ytreat the interrupt as a faiaf error or n  taygi-y_etll.ep_rograrn rnc  r ~ ! i  rn ~   clearly,il ~ structions that modify the content of the timer are privileged  thus  we can use the timer to prevent a user program from running too long a simple technique is to il1.itialize a counter with the amount of time that a program is allowed to run a program with a 7-minute time limit  for example  would have its counter initialized to 420 every second  the timer interrupts and the counter is decremented by 1 as long as the counter is positive  control is returned to the user program when the counter becomes negative  the operating system terminates the program for exceeding the assigned time limit  a program does nothing unless its instructions are executed by a cpu a program in execution  as mentioned  is a process a time-shared user program such as a compiler is a process a word-processing program being run by an individual user on a pc is a process a system task  such as sending output to a printer  can also be a process  or at least part of one   for now  you can consider a process to be a job or a time-shared program  but later you will learn 24 chapter 1 1.7 that the concept is more general as we shall see in chapter 3  it is possible to provide system calls that allow processes to create subprocesses to execute concurrent ! y  a process needs certain resources---including cpu time  me111ory  files  and-i ; o devices    _  _ to accomplish its  task these i esources are e ! tl1er given to the process when it is created or allocated to it while it is running in addition to the various physical and logical resources that a process obtains when it is created  various initialization data  input  may be passed along for example  consider a process whose function is to display the status of a file on the screen of a terminal the process will be given as an input the name of the file and will execute the appropriate instructions and system calls to obtain and display on the terminal the desired information when the process terminates  the operating system will reclaim any reusable resources  l ve ~ _111pl  t21size that a program by itselfis nota process ; a program is a y_assive er ~ ! ~ ty  likt  tl1e c   i1terltsof a fil  storecl_m1 c ! iskl ~ a.thereasc \ _pr  jce ~ ~ s_1s 21 ~ 1 active entity a si-dgl ~   1hr  eaded proc ~ ss has on ~ _pr_ogra111 cou11 ! er s  eecifying the nexf1il ~ r  uc_tiogt   _ex ~ cljte  threads are covered in chapter 4  the -execi.rtioil  of such a process must be sequential the cpu executes one instruction of the process after another  until the process completes further  at any time  one instruction at most is executed on behalf of the process thus  although two processes may be associated with the same program  they are nevertheless considered two separate execution sequences a multithreaded process has multiple program counters  each pointing to the next instruction to execute for a given thread  a process is the unit of work in a system such a system consists of a collection of processes  some of which are operating-system processes  those that execute system code  and the rest of which are user processes  those that execute user code   al  jheseprocesses canp   t ~ ! ltially execute concurrently _lly.ij  lli  ! p_l ~   _i ! lg   i ' \ a sir1gle _c  pl  ,for_ ~   ample      the operating system is responsible for the following activities in connection with process management  scheduling processes and threads on the cpus creating and deleting both user and system processes suspending and resuming processes providing mechanisms for process synchronization providing mechanisms for process communication we discuss process-management techniques in chapters 3 through 6  as we discussed in section 1.2.2  the main memory is central to the operation of a modern computer system main memory is a large array of words or bytes  ranging in size from hundreds of thousands to billions each word or byte has its own address main memory is a repository of quickly accessible data shared by the cpu and i/0 devices the central processor reads instructions from main 1.8 1.8 25 memory during the instruction-fetch cycle and both reads and writes data from main memory during the data-fetch cycle  on a von neumann architecture   as noted earlier  the main memory is generallythe only large storage device that the cpu is able to address and access directly for example  for the cpu to process data from disk  those data mu.st first be transferred to main n lemory by cpu-generated i/0 calls in the same way  instructions must be in memory for the cpu to execute them  for a program to be executed  it must be mapped to absolute addresses and loaded into memory as the program executes  it accesses program instructions and data from memory by generating these absolute addresses eventually  the program terminates  its memory space is declared available  and the next program can be loaded and executed  to improve both the utilization of the cpu and the speed of the computer 's response to its users  general-purpose computers must keep several programs in memory  creating a need for memory management many different memorymanagement schemes are used these schemes reflect various approaches  and the effectiveness of any given algorithm depends on the situation in selecting a memory-management scheme for a specific system  we must take into account many factors-especially the hardware design of the system each algorithm requires its own hardware support  the operating system is responsible for the following activities in connection with memory management  keeping track of which parts of memory are currently being used and by whom deciding which processes  or parts thereof  and data to move into and out of memory allocating and deallocating memory space as needed memory-management techniques are discussed il1 chapters 8 and 9  to make the computer system convenient for users  the operating system provides a uniform  logical view of information storage the operating system abstracts from the physical properties of its storage devices to define a logical storage unit  the file the operating system maps files onto physical media and accesses these files via the storage devices  1.8.1 file-system management pile management is one of the most visible components of an operating system  computers can store information on several different types of physical media  magnetic disk  optical disk  and magnetic tape are the most common each of these media has its own characteristics and physical organization each medium is controlled by a device  such as a disk drive or tape drive  that also has its own unique characteristics these properties include access speed  capacity  data-transfer rate  and access method  sequential or randmn   26 chapter 1 a file is a collection of related information defined by its creator commonly  files represent programs  both source and object forms  and data data files may be numeric  alphabetic  alphanumeric  or binary files may be free-form  for example  text files   or they may be formatted rigidly  for example  fixed fields   clearly  the concept of a file is an extremely general one  the operating system implements the abstract concept of a file by managing mass-storage media  such as tapes and disks  and the devices that control them  also  files are normally organized into directories to make them easier to use  finally  when multiple users have access to files  it may be desirable to control by whom and in what ways  for example  read  write  append  files may be accessed  the operating system is responsible for the following activities in connection with file management  creating and deleting files creating and deleting directories to organize files supporting primitives for manipulating files and directories mapping files onto secondary storage backing up files on stable  nonvolatile  storage media file-management teclmiques are discussed in chapters 10 and 11  1.8.2 mass-storage management as we have already seen  because main memory is too small to accommodate all data and programs  and because the data that it holds are lost when power is lost  the computer system must provide secondary storage to back up main memory most modern computer systems use disks as the principal on-line storage medium for both programs and data most programs-including compilers  assemblers  word processors  editors  and formatters-are stored on a disk until loaded into memory and then use the disk as both the source and destination of their processing hence  the proper management of disk storage is of central importance to a computer system the operating system is responsible for the following activities in connection with disk management  free-space management storage allocation disk scheduling because secondary storage is used frequently  it must be used efficiently the entire speed of operation of a computer may hinge on the speeds of the disk subsystem and the algorithms that manipulate that subsystem  there are  however  many uses for storage that is slower and lower in cost  and sometimes of higher capacity  than secondary storage backups of disk data  seldom-used data  and long-term archival storage are some examples  magnetic drives and their tapes and cd and dvd drives and platters are typical devices the media  tapes and optical platters  vary between  write-once  read-many-times  and  read-write  formats  1.8 27 tertiary storage is not crucial to systern performance  but it still must be managed some operating systems take on this task  while others leave tertiary-storage management to application progran1s some of the functions that operating systerns can provide include mounting and unmounting rnedia in devices  allocating and freeing the devices for exclusive use by processes  and migrating data from secondary to tertiary storage  techniques for secondary and tertiary storage management are discussed in chapter 12  1.8.3 caching is an important principle of computer systems information is normally kept in some storage system  such as main memory   as it is used  it is copied into a faster storage system-the cache-on a temporary basis  when we need a particular piece of information  we first check whether it is in the cache if it is  we use the information directly from the cache ; if it is not  we use the information from the source  putting a copy in the cache under the assumption that we will need it again soon  in addition  internal programmable registers   such as index registers  provide a high-speed cache for main memory the programmer  or compiler  implements the register-allocation and register-replacement algorithms to decide which information to keep in registers and which to keep in main memory there are also caches that are implemented totally in hardware  for instance  most systems have an instruction cache to hold the instructions expected to be executed next without this cache  the cpu would have to wait several cycles while an instruction was fetched from main memory for similar reasons  most systems have one or more high-speed data caches in the memory hierarchy we are not concerned with these hardware-only caches in this text  since they are outside the control of the operating system  because caches have limited size  is an important design problem careful selection of the cache size and of a replacement policy can result in greatly increased performance figure 1.11 compares storage performance in large workstations and small servers various replacement algorithms for software-controlled caches are discussed in chapter 9  typical size 16mb 64gb 100gb implementation custom memory with on-chip or off-chip cmos dram magnetic disk technology multiple ports  cmos cmossram access time  ns  0.25-0.5 0.5-25 80-250 5,000.000 bandwidth  mb/sec  20,000 ~ 100,000 5000 10,000 1000-5000 20-150 managed by compiler hardware operating system operating system backed by cache main memory disk cd or tape figure 1.11 performance of various levels of storage  28 chapter 1 main memory can be viewed as a fast cache for secondary storage  since data in secondary storage must be copied into main memory for use  and data must be in main memory before being moved to secondary storage for safekeeping the file-system data  which resides permanently on secondary storage  may appear on several levels in the storage hierarchy at the highest level  the operating system may maintain a cache of file-system data in main memory in addition  electronic ram disks  also known as may be used for high-speed storage that is accessed through the file-system interface the bulk of secondary storage is on magnetic disks the magneticdisk storage  in turn  is often backed up onto magnetic tapes or removable disks to protect against data loss in case of a hard-disk failure some systems autoinatically archive old file data from secondary storage to tertiary storage  such as tape jukeboxes  to lower the storage cost  see chapter 12   the movement of information between levels of a storage hierarchy may be either explicit or implicit  depending on the hardware design and the controlling operating-system software f_o   '  ! lstilnce,datatransfe ~ from cache _l ~ cpu ~ '11 ~ cl_ ! ~ g ~ ~ ! ~  r-_s _is_ _ ~ 1suall y ahardvvare function  with no op-era t  ii.g = sy-s tern intervention in contrast  transfer of data-from aisk to memory is usually controlledby the-op ~ ra-t !  ri.g system   fn a 11ier2rrchical storage structure  the same data may appear in different levels of the storage system for example  suppose that an integer a that is to be incremented by 1 is located in file b  and file b resides on magnetic disk  the increment operation proceeds by first issuing an i/o operation to copy the disk block on which a resides to main memory this operation is followed by copying a to the cache and to an internal register thus  the copy of a appears in several places  on the magnetic disk  in main memory  in the cache  and in an internal register  see figure 1.12   once the increment takes place in the internal register  the value of a differs in the various storage systems the value of a becomes the same only after the new value of a is written from the internal register back to the magnetic disk  in a computing environment where only one process executes at a tim.e  this arrangement poses no difficulties  since an access to integer a will always be to the copy at the highest level of the hierarchy however  in a multitasking environment  where the cpu is switched back and -forth-among var1ous processes ~ extreme care must be taken to ensure that  if several processe ~ vv  is  l i  o-accessa  then each of these processes will obtain the most recently updated ___ c_ = --.c of a     the situation becomes more complicated in a multiprocessor environment where  in addition to maintaining internal registers  each of the cpus also contains a local cache  figure 1.6   ~ ' 1_ su ~ bc1  .1l_  _n ~ i o ! _l  _il'l_ ~ 1lt  ~ s 2ey   f_a ij.t ~ y exist simultaneouslyinseyeral caches since the variouscpus can all execute .s  2 ~ 1c ~ r ~ ~ l ~ tly  \ ,ve-must1nake surethat an to the value ofa in one cache figure 1.12 migration of integer a from disk to register  1.9 1.9 29 1.8.4 1/0 systems one of the purposes of a11 operating system is to hide the peculiarities ofspecific hardware d ~ ~ ic  ~ jro1n th ~ l1s ~ j   for example  in unix  the peculiarities of i/o devices are hidden from the bulk of the operating system itself by the i/0 subsystem the i/o subsystem consists of several components  a memory-management component that includes buffering  caching  and spooling a general device-driver interface drivers for specific hardware devices only the device driver knows the peculiarities of the specific device to which it is assigned  we discussed in section 1.2.3 how interrupt handlers and device drivers are used in the construction of efficient i/o subsystems in chapter 13  we discuss how the i/o subsystem interfaces to the other system components  manages devices  transfers data  and detects i/0 completion  if a computer system has multiple users and allows the concurrent execution of multiple processes  then access to data must be regulated for that purpose  mechanisms ensure that files  memory segments  cpu  and other resources can be operated on by only those processes that have gained proper authorization from the operating system for example  memory-addressing hardware ensures that a process can execute only within its own address space the timer ensures that no process can gain control of the cpu without eventually relinquishing control device-control registers are not accessible to users  so the integrity of the various peripheral devices is protected  protection  then  is any mechanism for controlling the access of processes or users-to the resourcesdefined by a computer system this mechanism rni1st provide means to specify the confrols to be imposed and means to enforce the controls  protection can improve reliability by detecting latent errors at the interfaces between component subsystems early detection of interface errors can often prevent contamination of a healthy subsystem by another subsystem that is 30 chapter 1 1.10 malfunctioning furthermore  an unprotected resource can not defend against use  or n isuse  by an unauthorized or incompetent user a protection-oriented system provides a means to distinguish between authorized and unauthorized usage  as we discuss in chapter 14  6 yt  ; terl _ca1lhave adequateprotection but still be prone to failure and ado_w inappr   priat ~ acs ~ s ~  consider a user whose authentication information  her means of identifying herself to the system  is stolen her data could be copied or deleted  even though file and memory protection are working it is the job of to defend a system from external and internal attacks such attacks spread across a huge range and include viruses and worms  denial-ofservice attacks  which use all of a system 's resources and so keep legitimate users out of the system   identity theft  and theft of service  unauthorized use of a system   prevention of some of these attacks is considered an operatingsystem function on some systems  while other systems leave the prevention to policy or additional software due to the alarming rise in security incidents  operating-system security features represent a fast-growing area of research and implementation security is discussed in chapter 15  protection and security require the system to be able to distinguish among all its users most maintain a list of user names and    in windows vista parlance  this is _ 1_ these numerical ids are unique  one per user when a user logs in system  the authentication stage determines the appropriate user id for the user that user id is associated with all of the user 's processes and threads when an id needs to be user readable  it is translated back to the user name via the user name list  in some circumstances  we wish to distinguish among sets of users rather than individual users for example  the owner of a file on a unix system may be allowed to issue all operations on that file  whereas a selected set of users may only be allowed to read the file to accomplish this  we need to define a group name and the set of users belonging to that group group functionality can be implemented as a system-wide list of group names and ic'1entifiers  a user can be in one or more groups  depending on operating-system design decisions the user 's group ids are also included in every associated process and thread  in the course of normal use of a system  the user id and are s-l.iffici.e11t hov \ tever ; a user sometimes needs to to gain extra permissions for an activity the user may need access to a fhatis resh ; icted,for examp1e.operatmg systems provide various methods to allow privilege escalation on unix  for example  the setuid attribute on a program causes that program to run with the user id of the owner of the file  rather than the current user 's id the process runs with this until it turns off the extra privileges or terminates  a distributed system is a collection of physically separate  possibly heterogeneous  computer systems that are networked to provide the users with access to the various resources that the system maintains access to a shared resource 1.10 31 increases computation speed  functionality  data availability  and reliability  some operating systems generalize network access as a form of file access  with the details of networking contained in the network interface 's device driver  others make users specifically invoke network functions generally  systems contain a mix of the two modes-for example ftp and nfs the protocols that create a distributed system can greatly affect that system 's utility and popularity  a in the simplest terms  is a communication path between two or more systems distributed systems depend on networking for their functionality networks vary by the protocols used  the distances between nodes  and the transport media tcp /ip is the most common network protocol  although atm and other protocols are in widespread use likewise  operatingsystem support of protocols varies most operating systems support tcp /ip  including the windows and unix operating systems some systems support proprietary protocols to suit their needs to an operating system  a network protocol simply needs an interface device-a network adapter  for examplewith a device driver to manage it  as well as software to handle data these concepts are discussed throughout this book  networks are characterized based on the distances between their nodes  a computers within a room  a floor  or a building a n  usually links buildings  cities  or countries a global company may have a wan to com1ect its offices worldwide these networks may run one protocol or several protocols the continuing advent of new technologies brings about new forms of networks  for example  a   '/ ! al i  could link buildings within '   a city bluetooth and 802.11 devices use wireless technology to commt.micate over a distance of several feet  in essence creating a such as might be found in a home  the media to carry networks are equally varied they include copper wires  fiber strands  and wireless transmissions between satellites  microwave dishes  and radios when computing devices are connected to cellular phones  they create a network even very short-range infrared communication can be used for networking at a rudimentary level  whenever computers communicate  they use or create a network these networks also vary in their performance and reliability  some operating systems have taken the concept of networks and distributed systems further than the notion of providing network connectivity a is an operating system that provides features such as file sharing across the network and that includes a communication scheme that allows different processes on different computers to exchange messages  a computer rmming a network operating system acts autonomously from all other computers on the network  although it is aware of the network and is able to communicate with other networked computers a distributed operating system provides a less autonomous envirorunent  the different operating systems comm lmicate closely enough to provide the illusion that only a single operating system controls the network  we cover computer networks and distributed systems in chapters 16 through 18  32 chapter 1 1.11 the discussion thus far has focused on the general-purpose computer systems that we are all familiar with there are  however  other classes of computer systems whose functions are more limited and whose objective is to deal with limited computation domains  1.11.1 real-time embedded systems embedded computers are the most prevalent form of computers in existence  these devices are found everywhere  from car engines and manufacturing robots to dvds and microwave ovens they tend to have very specific tasks  the systencs they run on are usually primitive  and so the operating systems provide limited features usually  they have little or no user interface  preferring to spend their time monitoring and managing hardware devices  such as automobile engines and robotic arms  these embedded systems vary considerably some are general-purpose computers  running standard operating systems-such as unix-with special-purpose applications to implement the functionality others are hardware devices with a special-purpose embedded operating system providing just the functionality desired yet others are hardware devices with application-specific integrated circuits that perform their tasks without an operating system  the use of embedded systems continues to expand the power of these devices  both as standalone units and as elements of networks and the web  is sure to increase as well even now  entire houses can be computerized  so that a central computer-either a general-purpose computer or an embedded system-can control heating and lighting  alarm systems  and even coffee makers web access can enable a home owner to tell the house to heat up before she arrives home someday  the refrigerator may call the grocery store when it notices the milk is gone  embedded systems almost always run a real-time system is used when rigid time requirements been placed on the operation of a processor or the flow of data ; thus  it is often used as a control device in a dedicated application sensors bring data to the computer  the computer must analyze the data and possibly adjust controls to modify the sensor inputs systems that control scientific experiments  medical imaging systems  industrial control systems  and certain display systems are realtime systems some automobile-engine fuel-injection systems  home-appliance controllers  and weapon systems are also real-time systems  a real-time system has well-defined  fixed time constraints processing must be done within the defined constraints  or the system will fail for instance  it would not do for a robot arm to be instructed to halt after it had smashed into the car it was building a real-time system functions correctly only if it returns the correct result within its time constraints contrast this system with a time-sharing system  where it is desirable  but not mandatory  to respond quickly or a batch system  which may have no time constraints at all  in chapter 19  we cover real-time embedded systems in great detail in chapter 5  we consider the scheduling facility needed to implement real-time functionality in an operating system in chapter 9  we describe the design 1.11 33 of memory management for real-time computing finally  in chapter 22  we describe the real-time components of the windows xp operating system  1.11.2 multimedia systems most operating systems are designed to handle conventional data such as text files  progran'ls  word-processing documents  and spreadsheets however  a recent trend in technology is the incorporation of multimedia data into computer systems multimedia data consist of audio and video files as well as conventional files these data differ from conventional data in that multimedia data-such as frames of video-must be delivered  streamed  according to certain time restrictions  for example  30 frames per second   multimedia describes a wide range of applications in popular use today  these include audio files such as mp3  dvd movies  video conferencing  and short video clips of movie previews or news stories downloaded over the internet multimedia applications may also include live webcasts  broadcasting over the world wide web  of speeches or sporting events and even live webcams that allow a viewer in manhattan to observe customers at a cafe in paris multimedia applications need not be either audio or video ; rather  a multimedia application often includes a combination of both for example  a movie may consist of separate audio and video tracks nor must multimedia applications be delivered only to desktop personal computers increasingly  they are being directed toward smaller devices  including pdas and cellular telephones for example  a stock trader may have stock quotes delivered wirelessly and in real time to his pda  in chapter 20  we explore the demands of multimedia applications  describe how multimedia data differ from conventional data  and explain how the nature of these data affects the design of operating systems that support the requirements of multimedia systems  1.11.3 handheld systems include personal digital assistants  pdas   such as palm and pocket-pes  and cellular telephones  many of which use special-purpose embedded operating systems developers of handheld systems and applications face many challenges  most of which are due to the limited size of such devices for example  a pda is typically about 5 inches in height and 3 inches in width  and it weighs less than one-half pound because of their size  most handheld devices have small amounts of memory  slow processors  and small display screens we take a look now at each of these limitations  the amount of physical memory in a handheld depends on the device  but typically it is somewhere between 1 mb and 1 gb  contrast this with a typical pc or workstation  which may have several gigabytes of memory  as a result  the operating system and applications must manage memory efficiently this includes returning all allocated memory to the memory manager when the memory is not being used in chapter 9  we explore virtual memory  which allows developers to write programs that behave as if the system has more memory than is physically available currently  not many handheld devices use virtual memory techniques  so program developers must work within the confines of limited physical memory  34 chapter 1 1.12 a second issue of concern to developers of handheld devices is the speed of the processor used in the devices processors for most handheld devices run at a fraction of the speed of a processor in a pc faster processors require more power to include a faster processor in a handheld device would require a larger battery  which would take up more space and would have to be replaced  or recharged  more frequently most handheld devices use smaller  slower processors that consume less power therefore  the operating system and applications must be designed not to tax the processor  the last issue confronting program designers for handheld devices is l/0  a lack of physical space limits input methods to small keyboards  handwriting recognition  or small screen-based keyboards the small display screens limit output options whereas a monitor for a home computer may measure up to 30 inches  the display for a handheld device is often no more than 3 inches square familiar tasks  such as reading e-mail and browsing web pages  must be condensed into smaller displays one approach for displaying the content in web pages is where only a small subset of a web page is delivered and displayed on the handheld device  some handheld devices use wireless technology  such as bluetooth or 802.11  allowing remote access to e-mail and web browsing cellular telephones with connectivity to the internet fall into this category however  for pdas that do not provide wireless access  downloading data typically requires the user first to download the data to a pc or workstation and then download the data to the pda some pdas allow data to be directly copied from one device to another using an infrared link generally  the limitations in the functionality of pdas are balanced by their convenience and portability their use continues to expand as network com1ections become more available and other options  such as digital cameras and mp3 players  expand their utility  so far  we have provided an overview of computer-system organization and major operating-system components we conclude with a brief overview of how these are used in a variety of computing environments  1.12.1 traditional computing as computing matures  the lines separating many of the traditional computing environments are blurring consider the typical office environment just a few years ago  this environment consisted of pcs connected to a network  with servers providing file and print services remote access was awkward  and portability was achieved by use of laptop computers terminals attached to mainframes were prevalent at many companies as well  with even fewer remote access and portability options  the current trend is toward providing more ways to access these computing environments web technologies are stretching the boundaries of traditional computing companies establish which provide web accessibility to their internal servers ccxepu1as are essentially terminals that understand web-based computing handheld computers can synchronize with 1.12 35 pcs to allow very portable use of con1pany information handheld pdas can also connect to to use the company 's web portal  as well as the myriad other web resources   at home  most users had a single computer with a slow modem connection to the office  the internet  or both today  network-connection speeds once available only at great cost are relatively inexpensive  giving home users more access to more data these fast data connections are allowing home computers to serve up web pages and to run networks that include printers  client pcs  and servers some homes even have to protect their networks from security breaches those firewalls cost thousands of dollars a few years ago and did not even exist a decade ago  in the latter half of the previous century  computing resources were scarce   before that  they were nonexistent !  for a period of time  systems were either batch or interactive batch systems processed jobs in bulk  with predetermined input  from files or other sources of data   interactive systems waited for input from users to optimize the use of the computing resources  multiple users shared time on these systems time-sharing systems used a timer and scheduling algorithms to rapidly cycle processes through the cpu  giving each user a share of the resources  today  traditional time-sharing systems are uncommon the same scheduling technique is still in use on workstations and servers  but frequently the processes are all owned by the same user  or a single user and the operating system   user processes  and system processes that provide services to the user  are managed so that each frequently gets a slice of computer time consider the windows created while a user is working on a pc  for example  and the fact that they may be performing different tasks at the same time  1.12.2 client-server computing as pcs have become faste1 ~ more powerful  and cheaper  designers have shifted away from centralized system architecture terminals connected to centralized systems are now being supplanted by pcs correspondingly  user-interface functionality once handled directly by centralized systems is increasingly being handled by pcs as a result  many of today ' s systems act as to satisfy requests generated by this form of specialized distributed system  called a system  has the general structure depicted in figure 1.13  server systems can be broadly categorized as compute servers and file servers  figure 1.13 general structure of a client-server system  36 chapter 1 the provides an interface to which a client can send a request to perform an action  for example  read data  ; in response  the server executes the action and sends back results to the client a server running a database that responds to client requests for data is an example of such a system  the provides a file-system interface where clients can create  update  read  and delete files an example of such a system is a web server that delivers files to clients running web browsers  1.12.3 peer-to-peer computing another structure for a distributed system is the peer-to-peer  p2p  system model in this model  clients and servers are not distinguished from one another ; instead  all nodes within the system are considered peers  and each ncay act as either a client or a server  depending on whether it is requesting or providing a service peer-to-peer systems offer an advantage over traditional client-server systems in a client-server system  the server is a bottleneck ; but in a peer-to-peer system  services can be provided by several nodes distributed throughout the network  to participate in a peer-to-peer system  a node must first join the network of peers once a node has joined the network  it can begin providing services to-and requesting services from -other nodes in the network determining what services are available is accomplished in one of two general ways  when a node joins a network  it registers its service with a centralized lookup service on the network any node desiring a specific service first contacts this centralized lookup service to determine which node provides the service the remainder of the communication takes place between the client and the service provider  a peer acting as a client must first discover what node provides a desired service by broadcasting a request for the service to all other nodes in the network the node  or nodes  providing that service responds to the peer making the request to support this approach  a discovery protocol must be provided that allows peers to discover services provided by other peers in the network  peer-to-peer networks gained widespread popularity in the late 1990s with several file-sharing services  such as napster and gnutella  that enable peers to exchange files with one another the napster system uses an approach similar to the first type described above  a centralized server maintains an index of all files stored on peer nodes in the napster network  and the actual exchanging of files takes place between the peer nodes the gnutella system uses a technique similar to the second type  a client broadcasts file requests to other nodes in the system  and nodes that can service the request respond directly to the client the future of exchanging files remains uncertain because many of the files are copyrighted  music  for example   and there are laws governing the distribution of copyrighted material in any case  though  peerto peer technology undoubtedly will play a role in the future of many services  such as searching  file exchange  and e-mail  1.13 1.13 37 1.12.4 web-based computing the web has become ubiquitous/ leading to more access by a wider variety of devices than was dreamt of a few years ago pcs are still the most prevalent access devices/ with workstations/ handheld pdas1 and even cell phones also providing access  web computing has increased the emphasis on networking devices that were not previously networked now include wired or wireless access devices that were networked now have faster network connectivity/ provided by either improved networking technology optimized network implementation code/ or both  the implementation of web-based computing has given rise to new categories of devices/ such as which distribute network connections an1.ong a pool of similar servers operating systems like windows 951 which acted as web clients/ have evolved into linux and windows xp 1 which can act as web servers as well as clients generally/ the web has increased the complexity of devices because their users require them to be web-enabled  the study of operating systems/ as noted earlier/ is made easier by the availability of a vast number of open-source releases  are those made available in source-code format rather than as compiled binary code linux is the most famous open source operating system  while microsoft windows is a well-known example of the opposite dosedapproach  starting with the source code allows the programmer to produce binary code that can be executed on a system doing the oppositethe source code from the binaries-is quite a lot of work1 and useful items such as comments are never recovered learning operating systems by examining the actual source code1 rather than reading summaries of that code/ can be extremely useful with the source code in hand/ a student can modify the operating system and then compile and nm the code to try out those changes1 which is another excellent learning tool this text indudes projects that involve modifying operating system source code/ while also describing algorithms at a high level to be sure all important operating system topics are covered throughout the text1 we provide pointers to examples of open-source code for deeper study  there are many benefits to open-source operating systems/ including a commtmity of interested  and usually unpaid  programmers who contribute to the code by helping to debug it analyze it/ provide support/ and suggest changes arguably/ open-source code is more secure than closed-source code because many more eyes are viewing the code certainly open-source code has bugs/ but open-source advocates argue that bugs tend to be found and fixed faster owing to the number of people using and viewing the code companies that earn revenue from selling their programs tend to be hesitant to open-source their code/ but red hat/ suse1 sun/ and a myriad of other companies are doing just that and showing that commercial companies benefit/ rather than suffer/ when they open-source their code revenue can be generated through support contracts and the sale of hardware on which the software runs/ for example  38 chapter 1 1.13.1 history in the early days of modern computing  that is  the 1950s   a great deal of software was available in open-source format the original hackers  computer enthusiasts  at mit 's tech model railroad club left their programs in drawers for others to work on homebrew user groups exchanged code during their meetings later  company-specific user groups  such as digital equipment corporation 's dec  accepted contributions of source-code programs  collected them onto tapes  and distributed the tapes to interested ncembers  computer and software companies eventually sought to limit the use of their software to authorized computers and paying customers releasing only the binary files compiled from the source code  rather than the source code itself  helped them to achieve this goal  as well as protecting their code and their ideas from their competitors another issue involved copyrighted material operating systems and other programs can limit the ability to play back movies and music or display electronic books to authorized computers  such or digital would not be effective if the source code that implemented these limits were published  laws in many countries  including the u.s digital millennium copyright act  dmca   make it illegal to reverse-engineer drm code or otherwise try to circumvent copy protection  to counter the move to limit software use and redistribution  richard stallman in 1983 started the gnu project to create a free  open-source unixcompatible operating system in 1985  he published the gnu manifesto  which argues that all software should be free and open-sourced he also formed the with the goal of encouraging the free exchange of software source code and the free use of that software rather than copyright its software  the fsf copylefts the software to encourage sharing and improvement the gercera  ! codifies copylefting and is a common license under which free software is released ftmdamentally  gpl requires that the source code be distributed with any binaries and that any changes made to the source code be released under the same gpl license  1.13.2 linux as an example of an open-source operating system  consider the gnu project produced many unix-compatible tools  including compilers  editors  and utilities  but never released a kernel in 1991  a student in finland  linus torvalds  released a rudimentary unix-like kernel using the gnu compilers and tools and invited contributions worldwide the advent of the internet meant that anyone interested could download the source code  modify it  and submit changes to torvalds releasing updates once a week allowed this so-called linux operating system to grow rapidly  enhanced by several thousand programmers  the gnu /linux operating system has spawned hundreds of unique or custom builds  of the system major distributions include redhat  suse  fedora  debian  slackware  and ubuntu distributions vary in function  utility  installed applications  hardware support  user interface  and purpose for example  redhat enterprise lim1x is geared to large commercial use pclinuxos is a  jvc  cd-an operating system that can be booted and run from a cd-rom without being installed on a system 's hard 1.13 39 disk one variant of pclinuxos  pclinuxos supergamer dvd  is a that includes graphics drivers and games a gamer can run it on any compatible system simply by booting from the dvd when the gamer is finished  a reboot of the system resets it to its installed operating system  access to the linux source code varies by release here  we consider ubuntu linux ubuntu is a popular linux distribution that comes in a variety of types  including those tuned for desktops  servers  and students its founder pays for the printing and mailing of dvds containing the binary and source code  which helps to make it popular   the following steps outline a way to explore the ubuntu kernel source code on systems that support the free vmware player tool  download the player from http  i /www vrnware com/ download/player i and install it on your system  download a virtual machine containing ubuntu hundreds of appliances  or virtual machirte images  pre-installed with operating systems and applications  are available from vmware at http  //www.vmware.com/appliances/  boot the virtual machine within vmware player  get the source code of the kernel release of interest  such as 2.6  by executing wget http  //www.kernel.org/pub/linux/kernel/v2.6/linux 2 6 18 1 tar bz2 within the ubuntu virtual machine  uncompress and untar the downloaded file via tar xj f linux 2.6.18.1.tar.bz2  explore the source code of the ubuntu kernel  which is now in  /linux 2 6.18 .1  for more about linux  see chapter 21 for more about virtual machines  see section 2.8  1.13.3 bsd unix has a longer and more complicated history than linux it started in 1978 as a derivative of at&t 's unix releases from the university of california at berkeley  ucb  came in source and binary form  but they were not opensource because a license from at&t was required bsd unix 's development was slowed by a lawsuit by at&t  but eventually a fully functional  open-source version  4.4bsd-lite  was released in 1994  just as with lim.ix  there are many distributions of bsd unix  including freebsd  netbsd  openbsd  and dragonflybsd to explore the source code of freebsd  simply download the virtual machine image of the version of interest and boot it within vmware  as described above for ubuntu linux the source code comes with the distribution and is stored in /usr i src/ the kernel source code is in /usr/src/sys for example  to examine the virtual-memory implementation code in the freebsd kernel  see the files in /usr/src/sys/vrn  darwin  the core kernel component of mac  is based on bsd unix and is open-sourced as well that source code is available from http  i /www opensource apple corn/ darwinsource/ every mac release 40 chapter 1 1.14 has its open-source components posted at that site the name of the package that contains the kernel is xnu the source code for mac kernel revision 1228  the source code to mac leopard  can be found at www.opensource.apple.coml darwinsource i tar balls i apsll xnu-1228 tar.gz  apple also provides extensive developer tools  documentation  and support at http  i i connect apple com for more information  see appendix a  1.13.4 solaris is the commercial unix-based operating system of sun microsystems  originally  sun 's operating system was based on bsd unix sun moved to at&t 's system v unix as its base in 1991 in 2005  sun open-sourced some of the solaris code  and over time  the company has added more and more to that open-source code base unfortunately  not all of solaris is open-sourced  because some of the code is still owned by at&t and other companies however  solaris can be compiled from the open source and linked with binaries of the close-sourced components  so it can still be explored  modified  compiled  and tested  the source code is available from http  i i opensolaris org/ os/ downloads/  also available there are pre-compiled distributions based on the source code  docun1.entation  and discussion groups it is not necessary to download the entire source-code bundle from the site  because sun allows visitors to explore the source code on-line via a source code browser  1.13.5 utility the free software movement is driving legions of programmers to create thousands of open-source projects  including operating systems sites like http  i /freshmeat net/ and http  i i distrowatch com/ provide portals to many of these projects open-source projects enable students to use source code as a learning tool they can modify programs and test them  help find and fix bugs  and otherwise explore mature  full-featured operating systems  compilers  tools  user interfaces  and other types of programs the availability of source code for historic projects  such as multics  can help students to understand those projects and to build knowledge that will help in the implementation of new projects  gnu ilinux  bsd unix  and solaris are all open-source operating systems  but each has its own goals  utility  licensing  and purpose sometimes licenses are not mutually exclusive and cross-pollination occurs  allowing rapid improvements in operating-system projects for example  several major components of solaris have been ported to bsd unix the advantages of free software and open sourcing are likely to increase the number and quality of open-source projects  leading to an increase in the number of individuals and companies that use these projects  an operating system is software that manages the cornputer hardware  as well as providing an environment for application programs to run perhaps the 1.14 41 most visible aspect of an operating system is the interface to the computer system it provides to the human user  for a computer to do its job of executing programs  the program.s must be in main memory main memory is the only large storage area that the processor can access directly it is an array of words or bytes  ranging in size from millions to billions each word in memory has its own address the main mem.ory is usually a volatile storage device that loses its contents when power is turned off or lost most computer systems provide secondary storage as an extension of main memory secondary storage provides a form of nonvolatile storage that is capable of holding large quantities of data permanently the most common secondary-storage device is a magnetic disk  which provides storage of both programs and data  the wide variety of storage systems in a computer system can be organized in a hierarchy according to speed and cost the higher levels are expensive  but they are fast as we move down the hierarchy  the cost per bit generally decreases  whereas the access time generally increases  there are several different strategies for designing a computer system  uniprocessor systems have only a single processor  while multiprocessor systems contain two or more processors that share physical memory and peripheral devices the most common multiprocessor design is symmetric multiprocessing  or smp   where all processors are considered peers and run independently of one another clustered systems are a specialized form of multiprocessor systems and consist of multiple computer systems connected by a local area network  to best utilize the cpu  modern operating systems employ multiprogramming  which allows several jobs to be in memory at the same time  thus ensuring that the cpu always has a job to execute time-sharing systems are an extension of multiprogramming wherein cpu scheduling algorithms rapidly switch between jobs  thus providing the illusion that each job is nmning concurrently  the operating system must ensure correct operation of the computer system to prevent user programs from interfering with the proper operation of the system  the hardware has two modes  user mode and kernel mode various instructions  such as i/0 instructions and halt instructions  are privileged and can be executed only in kernel mode the memory in which the operating system resides must also be protected from modification by the user a tin1.er prevents infinite loops these facilities  dual mode  privileged instructions  memory protection  and timer interrupt  are basic building blocks used by operating systems to achieve correct operation  a process  or job  is the fundamental unit of work in an operating system  process management includes creating and deleting processes and providing mechanisms for processes to communicate and synchronize with each other  an operating system manages memory by keeping track of what parts of memory are being used and by whom the operating system is also responsible for dynamically allocating and freeing memory space storage space is also managed by the operating system ; this includes providing file systems for representing files and directories and managing space on mass-storage devices  operating systems must also be concerned with protecting and securing the operating system and users protection measures are mechanisms that control the access of processes or users to the resources made available by the 42 chapter 1 computer system security measures are responsible for defending a computer system from external or internal attacks  distributed systems allow users to share resources on geographically dispersed hosts connected via a computer network services may be provided through either the client-server model or the peer-to-peer n10del in a clustered system  multiple machines can perform computations on data residing on shared storage  and computing can continue even when some subset of cluster members fails  lans and wans are the two basic types of networks lans enable processors distributed over a small geographical area to communicate  whereas wans allow processors distributed over a larger area to communicate lans typically are faster than wans  there are several computer systems that serve specific purposes these include real-time operating systems designed for embedded environments such as consumer devices  automobiles  and robotics real-time operating systems have well-defined  fixed-time constraints processing must be done within the defined constraints  or the system will fail multimedia systems involve the delivery of multimedia data and often have special requirements of displaying or playing audio  video  or synchronized audio and video streams  recently  the influence of the internet and the world wide web has encouraged the development of operating systems that include web browsers and networking and communication software as integral features  the free software movement has created thousands of open-source projects  including operating systems because of these projects  students are able to use source code as a learning tool they can modify programs and test them  help find and fix bugs  and otherwise explore mature  full-featured operating systems  compilers  tools  user interfaces  and other types of programs  gnu /linux  bsd unix  and solaris are all open-source operating systems  the advantages of free software and open sourcing are likely to increase the number and quality of open-source projects  leadi.j.1.g to an increase in the number of individuals and companies that use these projects  1.1 how are network computers different from traditional personal computers describe some usage scenarios in which it is advantageous to use network computers  1.2 what network configuration would best suit the following environments a a dormitory floor b a university campus c a state d a nation 43 1.3 give two reasons why caches are useful what problems do they solve vvbat problems do they cause if a cache can be made as large as the device for which it is caching  for instance  a cache as large as a disk   why not make it that large and eliminate the device 1.4 under what circumstances would a user be better off using a timesharing system rather than a pc or a single-user workstation 1.5 list the four steps that are necessary to run a program on a completely dedicated machine-a computer that is running only that program  1.6 how does the distinction between kernel mode and user mode function as a rudimentary form of protection  security  system 1.7 in a multiprogramming and time-sharing environment  several users share the system simultaneously this situation can result in various security problems  a what are two such problems b can we ensure the same degree of security in a time-shared machine as in a dedicated machine explain your answer  1.8 describe a mechanism for enforcing memory protection in order to prevent a program from modifying the memory associated with other programs  1.9 what are the tradeoffs inherent in handheld computers 1.10 distinguish between the client-server and peer-to-peer models of distributed systems  1.11 some computer systems do not provide a privileged mode of operation in hardware is it possible to construct a secure operating system for these computer systems give arguments both that it is and that it is not possible  1.12 what are the main differences between operating systems for mainframe computers and personal computers 1.13 which of the following instructions should be privileged a set value of timer  b read the clock  44 chapter 1 c clear memory  d issue a trap instruction  e turn off interrupts  f modify entries in device-status table  g switch from user to kernel mode  h access i/o device  1.14 discuss  with examples  how the problem of maintaining coherence of cached data manifests itself in the following processing environments  a single-processor systems b multiprocessor systems c distributed systems 1.15 identify several advantages and several disadvantages of open-source operating systems include the types of people who would find each aspect to be an advantage or a disadvantage  1.16 how do clustered systems differ from multiprocessor systems what is required for two machines belonging to a cluster to cooperate to provide a highly available service 1.17 what is the main difficulty that a programmer must overcome in writing an operating system for a real-time environment 1.18 direct memory access is used for high-speed i/o devices in order to avoid increasing the cpu 's execution load  a how does the cpu interface with the device to coordinate the transfer b how does the cpu know when the memory operations are complete c the cpu is allowed to execute other programs while the dma controller is transferring data does this process interfere with the execution of the user programs if so  describe what forms of interference are caused  1.19 identify which of the functionalities listed below need to be supported by the operating system for  a  handheld devices and  b  real-time systems  a batch programming b virtual memory c time sharing 45 1.20 some cpus provide for more than two modes of operation what are two possible uses of these multiple modes 1.21 define the essential properties of the following types of operating systems  a batch b interactive c time sharing d real time e network f parallel a distributed b h clustered 1 handheld 1.22 describe the differences between symmetric and asymmetric multiprocessing  what are three advantages and one disadvantage of multiprocessor systems 1.23 the issue of resource utilization shows up in different forms in different types of operating systems list what resources must be managed carefully in the following settings  a mainframe or minicomputer systems b workstations connected to servers c handheld computers 1.24 what is the purpose of interrupts what are the differences between a trap and an interrupt can traps be generated intentionally by a user program if so  for what purpose 1.25 consider an smp system sincilar to what is shown in figure 1.6 illustrate with an example how data residing in memory could in fact have two different values in each of the local caches  1.26 consider a computing cluster consisting of two nodes running a database describe two ways in which the cluster software can manage access to the data on the disk discuss the benefits and disadvantages of each  46 chapter 1 brookshear  2003  provides an overview of computer science in general an overview of the linux operating system is presented in bovet and cesati  2006   solomon and russinovich  2000  give an overview of microsoft windows and considerable technical detail abmrt the systern internals and components russinovich and solomon  2005  update this information to windows server 2003 and windows xp mcdougall and mauro  2007  cover the internals of the solaris operating system mac os x is presented at http  i /www apple com/macosx mac os x internals are discussed in singh  2007   coverage of peer-to-peer systems includes parameswaran et al  2001   gong  2002   ripeanu et al  2002   agre  2003   balakrishnan et al  2003   and loo  2003   a discussion of peer-to-peer file-sharing systems can be found in lee  2003   good coverage of cluster computing is provided by buyya  1999   recent advances in cluster computing are described by ahmed  2000   a survey of issues relating to operating-system support for distributed systems can be found in tanenbaum and van renesse  1985   many general textbooks cover operating systems  including stallings  2000b   nutt  2004   and tanenbaum  2001   hamacher et al  2002  describe cmnputer organization  and mcdougall and laudon  2006  discuss multicore processors hennessy and patterson  2007  provide coverage of i/o systems and buses  and of system architecture in general blaauw and brooks  1997  describe details of the architecture of many computer systems  including several from ibm stokes  2007  provides an illustrated introduction to microprocessors and computer architecture  cache memories  including associative memory  are described and analyzed by smith  1982   that paper also includes an extensive bibliography on the subject  discussions concerning magnetic-disk technology are presented by freedman  1983  and by harker et al  1981   optical disks are covered by kenville  1982   fujitani  1984   o'leary and kitts  1985   gait  1988   and olsen and kenley  1989   discussions of floppy disks are offered by pechura and schoeffler  1983  and by sarisky  1983   general discussions concerning mass-storage technology are offered by chi  1982  and by hoagland  1985   kurose and ross  2005  and tanenbaum  2003  provide general overviews of computer networks fortier  1989  presents a detailed discussion of networking hardware and software kozierok  2005  discuss tcp in detail mullender  1993  provides an overview of distributed systems  2003  discusses recent developments in developing embedded systems issues related to handheld devices can be found in myers and beigl  2003  and dipietro and mancini  2003   a full discussion of the history of open sourcing and its benefits and challenges is found in raymond  1999   the history of hacking is discussed in levy  1994   the free software foundation has published its philosophy on its web site  http  //www.gnu.org/philosophy/free-software-for-freedom.html  detailed instructions on how to build the ubuntu linux kernel are on 47 http  i /www howtof orge com/kernelcompilation_ubuntu the open-source components of mac are available from http  i i developer apple com/ opensource/ index.html  wikipedia  http  i i en wikipedia org/wiki/richard_stallman  has an informative entry about richard stallman  the source code of multics is available at http  i /web .mit edu/multicshistory/ source/multics_internet_server/multics_sources.html  2.1 an operating system provides the environment within which programs are executed internally  operating systems vary greatly in their makeup  since they are organized along many different lines the design of a new operating system is a major task it is important that the goals of the system be well defined before the design begins these goals form the basis for choices among various algorithms and strategies  we can view an operating system from several vantage points one view focuses on the services that the system provides ; another  on the interface that it makes available to users and programmers ; a third  on its components and their interconnections in this chapter  we explore all three aspects of operating systems  showin.g the viewpoints of users  programmers  and operating-system designers we consider what services an operating system provides  how they are provided  how they are debugged  and what the various methodologies are for designing such systems finally  we describe how operating systems are created and how a computer starts its operating system  to describe the services an operating system provides to users  processes  and other systems  to discuss the various ways of structuring an operating system  to explain how operating systems are installed and customized and how they boot  an operating system provides an environment for the execution of programs  it provides certain services to programs and to the users of those programs  the specific services provided  of course  differ from one operating system to another  but we can identify common classes these operating-system services are provided for the convenience of the programmer  to n1.ake the programming 49 50 chapter 2 user and other system programs hardware figure 2 i a view of operating system services  task easier figure 2.1 shows one view of the various operating-system services and how they interrelate  one set of operating-system services provides functions that are helpfuj to the user ~ ~ user interface almost all operating systems have a this interface can take several forms one is a dcfr '  c ; ~  which uses text commands and a method for entering them  say  a program to allow entering and editing of commands   another is a batch in which commands and directives to control those commands are entered into files  and those files are executed most commonly  a is used here  the interface is a window system with a pointing device to direct i/0  choose from menus  and make selections and a keyboard to enter text some systems provide two or all three of these variations  program execution the system must be able to load a program into memory and to run that program the program must be able to end its execution  either normally or abnormally  indicating error   i/o operations a running program may require i/0  which may involve a file or an i/0 device for specific devices  special functions may be desired  such as recording to a cd or dvd drive or blanking a display screen   for efficiency and protection  users usually can not control i/0 devices directly  therefore  the operating system must provide a means to do i/0  file-system manipulation the file system is of particular interest obviously  programs need to read and write files and directories they also need to create and delete them by name  search for a given file  and list file information finally  some programs include permissions management to allow or deny access to files or directories based on file ownership many operating systems provide a variety of file systems  sometimes to allow personal choice  and sometimes to provide specific features or performance characteristics  2.1 51 communications there are many circumstances in which one process needs to exchange information with another process such communication ncay occur between processes that are executing on the same computer or between processes that are executing on different computer systems tied together by a computer network communications may be implemented via shared rnenwry or through message passing  in which packets of information are moved between processes by the operating system  error detection the operating system needs to be constantly aware of possible errors errors may occur in the cpu and memory hardware  such as a memory error or a power failure   in i/0 devices  such as a parity error on tape  a connection failure on a network  or lack of paper in the printer   and in the user program  such as an arithmetic overflow  an attempt to access an illegal memory location  or a too-great use of cpu time   for each type of error  the operating system should take the appropriate action to ensure correct and consistent computing of course  there is variation in how operating systems react to and correct errors debugging facilities can greatly enhance the user 's and programmer 's abilities to use the system efficiently  another set of operating-system functions exists not for helping the user but rather for ensuring the efficient operation of the system itself systems with multiple users can gain efficiency by sharing the computer resources among the users  resource allocation when there are i  lultiple usersormultiple jobs rmuung at the sametime  resources must be allocated to each of them  many d1herent -types of resources are managed by the operating system  some  such as cpu cycles  main memory  and file storage  may have special allocation code  whereas others  such as i/0 devices  may have much more general request and release code for instance  in determining how best to use the cpu  operating systems have cpu-scheduling routines that take into account the speed of the cpu  the jobs that must be executed  the number of registers available  and other factors there may also be routines to allocate printers  modems  usb storage drives  and other peripheral devices  accounting vl  e want to_keeptrack of whichusers use  lovy rnl1c  hand what kindsofcomputer resources this record keeping may be used for accoun  tii1g  so thai  users can be billed  or simply for accumulating usage statistics usage statistics may be a valuable tool for researchers who wish to reconfigure the system to improve computing services  protection and security the owners of information stored in a multiuser or networked computer system may want to control use of that information  when several separate processes execute concurrently  it ~ hould not be possible for one process to interfere with the others or with the operating system itself protection iiwolves ensuring that all access to systerr1 resources 1s -controlled security of the system from outsiders is also important such security starts with requiring each user to authenticate himself or herself to the system  usually by means of a password  to gain access to system resources it extends to defending external i/0 devices  52 chapter 2 2.2 including modems and network adapters  from invalid access attempts and to recording all such connections for detection of break-ins if a system is to be protected and secure  precautions must be instituted throughout it a chain is only as strong as its weakest link  we mentioned earlier that there are several ways for users to interface with the operating system here  we discuss two fundamental approaches one provides a command-line interface  or that allows users to directly enter commands to be performed by the operating system the other allows users to interface with the operating system via a graphical user interface  or gui  2.2.1 command interpreter some operating systems include the command interpreter in the kernel others  such as windows xp and unix  treat the command interpreter as a special program that is rmming when a job is initiated or when a user first logs on  on interactive systems   on systems with multiple command interpreters to choose from  the interpreters are known as shells for example  on unix and linux systems  a user may choose among several different shells  including the bourne shell  c shell  bourne-again shell  korn shell  and others third-party shells and free user-written shells are also available most shells provide similar functionality  and a user 's choice of which shell to use is generally based on personal preference figure 2.2 shows the bourne shell command interpreter being used on solaris 10  the main function of the command interpreter is to get and execute the next user-specified command many of the commands given at this level manipulate files  create  delete  list  print  copy  execute  and so on the ms-dos and unix shells operate in this way these commands can be implemented in two general ways  in one approach  the command interpreter itself contains the code to execute the command for example  a command to delete a file may cause the command interpreter to jump to a section of its code that sets up the parameters and makes the appropriate system call in this case  the number of comn'lands that can be given determines the size of the command interpreter  since each command requires its own implementing code  an alternative approach -used by unix  among other operating systems -implements most commands through system programs in this case  the command interpreter does not understand the cmnmand in any way ; it merely uses the command to identify a file to be loaded into memory and executed  thus  the unix command to delete a file rm file.txt would search for a file called rm  load the file into memory  and execute it with the parameter file txt the function associated with the rm command would be defined completely by the code in the file rm in this way  programmers can add new commands to the system easily by creating new files with the proper 0.0 0.0 r/s 0.0 0.6 console 2.2 0.2 0.0 0.2 0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 extended device statistics w/s 0.0 0.0 kr./s klv/s 0.0 0.0 0.0 1 ogi nell idle 1sj un0718days wai t actv svc_t 9  tn 1i ~ b 0.0 0.0 0.0 0 0 0 0 0 load average  0.09  0.11  8.66 jcpu pcpu what 1 /usr/bin/ssh-agent  /usr/bi 18 4 w figure 2.2 the bourne shell command interpreter in solaris i 0  53 names the command-interpreter program  which can be small  does not have to be changed for new commands to be added  2.2.2 graphical user interfaces a second strategy for interfacing with the operating system is through a userfriendly graphical user interface  or cui here  rather than entering commands directly via a command-line interface  users employ a mouse-based windowand nl.enu system characterized by a metaphor the user moves the mouse to position its pointer on images  or on the screen  the desktop  that represent programs  files  directories  and system functions depending on the mouse pointer 's location  clicking a button on the mouse can invoke a program  select a file or directory-known as a folder-or pull down a menu that contains commands  graphical user interfaces first appeared due in part to research taking place in the early 1970s at xerox parc research facility the first cui appeared on the xerox alto computer in 1973 however  graphical interfaces became more widespread with the advent of apple macintosh computers in the 1980s the user interface for the macintosh operating system  mac os  has undergone various changes over the years  the most significant being the adoption of the aqua interface that appeared with mac os x microsoft 's first version of windows-version 1.0-was based on the addition of a cui interface to the ms-dos operating system later versions of windows have made cosmetic changes in the appearance of the cui along with several enhancements in its functionality  including windows explorer  54 chapter 2 traditionally  unix systencs have been dominated by command-line interfaces  various gul interfaces are available  however  including the common desktop environment  cde  and x-windows systems  which are common on commercial versions of unix  such as solaris and ibm 's aix system in addition  there has been significant development in gui designs from various projects  such as i desktop environment  or kde  and the gnome desktop by the gnu project both the kde and gnome desktops run on linux and various unix systems and are available under open-source licenses  which means their source code is readily available for reading and for modification under specific license terms  the choice of whether to use a command-line or gui interface is mostly one of personal preference as a very general rule  many unix users prefer command-line interfaces  as they often provide powerful shell interfaces  in contrast  most windows users are pleased to use the windows gui environment and almost never use the ms-dos shell interface the various changes undergone by the macintosh operating systems provide a nice study in contrast historically  mac os has not provided a command-line interface  always requiring its users to interface with the operating system using its gui  however  with the release of mac os x  which is in part implemented using a unix kernel   the operating system now provides both a new aqua interface and a command-line interface figure 2.3 is a screenshot of the mac os x gui  the user interface can vary from system to system and even from user to user within a system it typically is substantially removed from the actual system structure the design of a useful and friendly user interface is therefore figure 2.3 the mac os x gui  2.3 2.3 55 not a direct function of the operating systenc in this book  we concentrate on the fundamental problems of providing adequate service to user programs  from the point of view of the operating system  we do not distinguish between user programs and systern programs  system calls provide an interface to the services made available by an operating system these calls are generally available as routines written in c and c + +  although certain low-level tasks  for example  tasks where hardware must be accessed directly   may need to be written using assembly-language instructions  before we discuss how an operating system makes system calls available  let 's first use an example to illustrate how system calls are used  writing a simple program to read data from one file and copy them to another file the first input that the program will need is the names of the two files  the input file and the output file these names can be specified in many ways  depending on the operating-system design one approach is for the program to ask the user for the names of the two files in an interactive system  this approach will require a sequence of system calls  first to write a prompting message on the screen and then to read from the keyboard the characters that define the two files on mouse-based and icon-based systems  a menu of file names is usually displayed in a window the user can then use the mouse to select the source name  and a window can be opened for the destination name to be specified  this sequence requires many i/0 system calls  once the two file names are obtained  the program must open the input file and create the output file each of these operations requires another system call  there are also possible error conditions for each operation when the program tries to open the input file  it may find that there is no file of that name or that the file is protected against access in these cases  the program should print a message on the console  another sequence of system calls  and then terminate abnormally  another system call   if the input file exists  then we must create a new output file we may find that there is already an output file with the same name this situation may cause the program to abort  a system call   or we may delete the existing file  another system call  and create a new one  another system call   another option  in an interactive system  is to ask the user  via a sequence of system calls to output the prompting message and to read the response from the termin.al  whether to replace the existing file or to abort the program  now that both files are set up  we enter a loop that reads from the input file  a system call  and writes to the output file  another system call   each read and write must return status information regarding various possible error conditions on input  the program may find that the end of the file has been reached or that there was a hardware failure in the read  such as a parity error   the write operation may encounter various errors  depending on the output device  no more disk space  printer out of paper  and so on   finally  after the entire file is copied  the program may close both files  another system call   write a message to the console or window  more system calls   and finally terminate normally  the final system call   as we 56 chapter 2 can see1 even simple programs may make heavy use of the operating system  frequently/ systems execute thousands of system calls per second this systemcall sequence is shown in figure 2a  most programmers never see this level of detail however typically/ applicatiol1 developers design program.s accordir1g to an   ~ jl ~ j'i   tl1e aj'ispecifies a set of functions application programmer/ including the parameters that are passed to each function and the return values the programmer can expect three of the most common apis available to application programmers are the win32 api for windows systems  the posix api for posix-based systems  which include virtually all versions of unix  linux/ and mac os x   and the java api for designing programs that run on the java virtual machine note that-unless specified -the system-call names used throughout this text are generic examples each operating system has its own name for each system call  behind the scenes/ the functions that make up an api typically invoke the actual system calls on behalf of the application programmer for example  the win32 function createprocess    which unsurprisingly is used to create a new process  actually calls the ntcreateprocess   system call in the windows kernel why would an application programnl.er prefer programming according to an api rather than invoking actual system calls there are several reasons for doing so one benefit of programming according to an api concerns program portability  an application programmer designing a program using an api can expect her program to compile and run on any system that supports the same api  although in reality/ architectural differences often make this more difficult than it may appear   furthermore/ actual system calls can often be more detailed and difficult to work with than the api available to an application programmer regardless/ there often exists a strong correlation between a function in the api and its associated system call within the kernel  example system call sequence acquire input file name write prompt to screen accept input acquire output file name write prompt to screen accept input open the input file if file does n't exist  abort create output file if file exists  abort loop read from input file write to output file until read fails close output file write completion message to screen terminate normally figure 2.4 example of how system calls are used  2.3 example of standard api as an example of a standard apt  consider the readfile 0 unction in the win32 api-a function for reading rom a file the api for this function appears in figure 2.5    return value ~ bool readfile c t function name  handle lpvoid dword lpdword lpoverlapped file  ~ buffer  bytes to read  parameters bytes read  ovl  ; figure 2.5 the api for the readfile   function  a description of the parameters passed to readfile 0 is as follows  handle file-the file to be read lpvoid buffer-a buffer where the data will be read into and written from dword bytestoread-the number of bytes to be read into the buffer lpdword bytesread -the number of bytes read during the last read lpoverlapped ovl-indicates if overlapped i/0 is being used 57 in fact  many of the posix and win32 apis are similar to the native system calls provided by the unix  linux  and windows operating systems  the run-time support system  a set of functions built into libraries included with a compiler  for most programming languages provides a system-call interface that serves as the link to system calls made available by the operating system the system-call interface intercepts function calls in the api and invokes the necessary system calls within the operating system typically  a number is associated with each system call  and the system-call interface maintains a table indexed according to these nun'lbers the system call interface then invokes the intended system call in the operating-system kernel and returns the status of the system call and any return values  the caller need know nothing about how the system call is implemented or what it does during execution rathel ~ it need only obey the api and understand what the operating system will do as a result of the execution of that system call thus  most of the details of the operating-system interface are hidden from the programmer by the api and are managed by the run-time support library  the relationship between an api  the system-call interface  and the operating 58 chapter 2 2.4 user mode kernel mode user application opeo    j open   implementation of open   system call return figure 2.6 the handling of a user application invoking the open   system call  system is shown in figure 2.6  which illustrates how the operating system handles a user application invoking the open   system call  system calls occur in different ways  depending onthe coj  rlpl1te.rjjlll e  often  more information is required than simply the identity of the desired system call the exact type and ammmt of information vary according to the particular operating system and call for example  to get input  we may need to specify the file or device to use as the source  as well as the address and length of the memory buffer into which the input should be read of course  the device or file and length may be implicit in the call  three general methods are used to pass parameters to the operating system the simplest approach is to pass the param.eters in registers in some cases  however  there may be more parameters than registers in these cases  the parameters are generally stored in a block  or table  in memory  and the address of the block is passed as a parameter in a register  figure 2.7   this is the approach taken by linux and solaris parameters also can be placed  or pushed  onto the stack by the program and popped oh the stacl  by the operatirl  g ~ yste111  some operating syste1ns prefer the block or stack method because those approaches do not limit the number or length of parameters being passed  system calls can be grouped ~ oughly intc  six major categories  process control  file manipujation  device manipulation  information maintenance  coinmuiii ~ a1ioii ~ ;  lndpr   tediol   in seci  lo  ri.s 2.4.l ~ hi.i = o ~ l.gli 2.l  6 ~ we discllss briefly the types of system calls that may be provided by an operating system  most of these system calls support  or are supported by  concepts and functions x  parameters for call load address x system call 13 +  ~  user program 2.4 register operating system figure 2.7 passing of parameters as a table  59 that are discussed in later chapters figure 2.8 summarizes the types of system calls normally provided by an operating system  2.4.1 process control a running program needs to be able to halt its execution either normally  end  or abnormally  abort   if a system call is made to terminate the currently ruru1il1g program abnormally  or if the program runs into a problem and causes an error trap  a dump of memory is sometimes taken and an error message generated the dump is written to disk and may be examined by a system program designed to aid the programmer in finding and correcting bugs-to determine the cause of the problem under either normal or abnormal circumstances  the operating system must transfer control to the invoking command mterpreter the command interpreter then reads the next cominand in an interactive system  the command interpreter simply continues with the next command ; it is assumed that the user will issue an appropriate command to respond to any error in a gui system  a pop-up wmdow might alert the user to the error and ask for guidance in a batch system  the command interpreter usually terminates the entire job and continues with the next job  some systems allow control cards to indicate special recovery actions in case an error occurs a is a batch-system concept it is a command to manage the execution of a process if the program discovers an error in its input and wants to terminate abnormally  it may also want to define an error level  more severe errors can be indicated by a higher-level error parameter it is then possible to combi11e normal and abnormal termination by defining a normal termination as an error at level 0 the command interpreter or a following program can use this error level to determine the next action automatically  a process or jobexecuting one p !   gral11_11l  ly _ \ ; \ '  ll1tto joad andexecut ~ anoteer pro-gra1  n  . th1s feafl  11  e allows the cmnmand i11terpreter to execute a program as directed by  for example  a user command  the click of a mouse  or a batch command an interesting question is where to return control when the loaded program terminates this question is related to the problem of 60 chapter 2 process control o end  abort o load  execute o create process  terminate process o get process attributes  set process attributes o wait for time o wait event  signal event o allocate and free memory file management o create file  delete file o open  close o read  write  reposition o get file attributes  set file attributes e  device management o request device  release device o read  write  reposition o get device attributes  set device attributes o logically attach or detach devices information maintenance o get time or date  set time or date o get system data  set system data o get process  file  or device attributes o set process  file  or device attributes communications o create  delete communication connection o send  receive messages o transfer status information o attach or detach remote devices figure 2.8 types of system calls  whether the existing program is lost  saved  or allowed to continue execution concurrently with the new program  if control returns to the existing program when the new program terminates  we must save the memory image of the existing program ; thus  we have effectively created a mechanism for one program to call another program if both programs continue concurrently  we have created a new job or process to 2.4 61 examples of windows and unix system calls windows unix process createprocesso fork   control exi tprocess   exit   waitforsingleobject   wait   file createfile   open   manipulation readfile   read   writefile   write   closehandle   close   device setconsolemode   ioctl   manipulation readconsole   read   writeconsole   write   information getcurrentprocessid   getpid   maintenance settimero alarm   sleep   sleep   communication createpipe   pipe   createfilemapping   shmget   mapviewoffile   mmapo protection setfilesecurity   chmod   initlializesecuritydescriptor   umask   setsecuritydescriptorgroup   chown   be multi programmed often  there is a system call specifically for this purpose  create process or submit job   if we create a new job or process  or perhaps even a set of jobs or processes  we should be able to control its execution this control requires the ability to determine and reset the attributes of a job or process  including the job 's priority  its maximum allowable execution time  and so on  get process attributes and set process attributes   we may also want to terminate a job or process that we created  terminate process  if we find that it is incorrect or is no longer needed  having created new jobs or processes  we may need to wait for them to finish their execution we may want to wait for a certain amount of time to pass  wait time  ; more probably  we will want to wait for a specific event to occur  wait event   the jobs or processes should then signal when that event has occurred  signal event   quite often  two or more processes may share data to ensure the integrity of the data being shared  operating systems often provide system calls allowing a process to lock shared data  thus preventing another process from accessing the data while it is locked typically such system calls include acquire lock and release lock system calls of these 62 chapter 2 example of standard c library the standard c library provides a portion o the system-call interface for many versions of unix and linux as an example  let 's assume a c program invokes the printf   statement the c library intercepts this call and invokes the necessary system call  s  in the operating system-in this instance  the write   system call the c library takes the value returned by write   and passes it back to the user program this is shown in figure 2.9  user mode kernel mode i i # include stdio.h int main     printf  greetings  ; i + return 0 ; standard c library write   system call i i  figure 2.9 standard c library handling of write    types  dealilcg with the coordination of concurrent processes  are discussed in great detail in chapter 6  there are so many facets of and variations in process and job control that we next use two examples-one involving a single-tasking system and the other a multitasking system -to clarify these concepts the ms-dos operating system is an example of a single-tasking system it has a command interpreter that is invoked when the computer is started  figure 2.10  a    because ms-dos is single-tasking  it uses a sincple method to run a program and does not create a new process it loads the program into memory  writing over most of itself to give the program as much memory as possible  figure 2.10  b    next  it sets the instruction pointer to the first instruction of the program the program then runs  and either an error causes a trap  or the program executes a system call to terminate in either case  the error code is saved in the system memory for later use following this action  the small portion of the command interpreter that was not overwritten resumes execution its first task is to reload the rest free memory command interpreter  a  2.4 free memory process command interpreter  b  figure 2.10 ms-dos execution  a  at system startup  b  running a program  63 of the command interpreter from disk then the command interpreter makes the previous error code available to the user or to the next program  fre ~ _j _s_i   der_i_ \ ' ~ c ! jr   in b   j  ~ eley unix  is an example of a multitasking syst   ' ~ when a user logs on to the system ~ the shell otthe user 's-choiceis run this shell is similar to the ms-dos shell in that it accepts commands and executes programs that the user requests however  since freebsd is a multitasking system  the command interpreter may continue running while another program is executed  figure 2.11   io startanew  __process,_th_es1w1l execu ~ 2 \ _  for-k   sy ~ tem call then  the selected program is loaded into memory via an exec   system call  and the program is executed depending on the way the command was issued  the shell then either waits for the process to finish or runs the process in the background in the latter case  the shell immediately requests another command when a process is rmming in the background  it can not receive input directly fron1 the keyboard  because the process d free memory process c interpreter figure 2.11 freebsd running multiple programs  64 chapter 2 shell is using this resource i/o is therefore done through files or through a cui interface meanwhile  the user is free to ask the shell to run other programs  to monitor the progress of the running process  to change that program 's priority  and so on when the process is done  it executes an exit   system call to terminate  returning to the invoking process a status code of 0 or a nonzero error code this status or error code is then available to the shell or other programs processes are discussed in chapter 3 with a program example using thefork   and exec   systemcalls  2.4.2 file management the file system is discussed in more detail in chapters 10 and 11 we can  however  identify several common system calls dealing with files  we first need to be able to create and delete files either system call requires the name of the file and perhaps some of the file 's attributes once the file is created  we need to open it and to use it we may also read  write  or reposition  rewinding or skipping to the end of the file  for example   finally  we need to close the file  indicating that we are no longer using it  we may need these same sets of operations for directories if we have a directory structure for organizing files in the file system in addition  for either files or directories  we need to be able to determine the values of various attributes and perhaps to reset them if necessary file attributes include the file name  file type  protection codes  accounting information  and so on at least two system calls  get file attribute and set file attribute  are required for this function some operating systems provide many more calls  such as calls for file move and copy others might provide an api that performs those operations using code and other system calls  and others might just provide system programs to perform those tasks if the system programs are callable by other programs  then each can be considered an api by other system programs  2.4.3 device management a process may need several resources to execute-main memory  disk drives  access to files  and so on if the resources are available  they can be granted  and control can be returned to the user process otherwise  the process will have to wait until sufficient resources are available  the various resources controlled by the operating system can be thought of as devices some of these devices are physical devices  for example  disk drives   while others can be thought of as abstract or virtual devices  for example  files   a system with multiple users may require us to first request the device  to ensure exclusive use of it after we are finished with the device  we release it these functions are similar to the open and close system calls for files other operating systems allow llnmanaged access to devices  the hazard then is the potential for device contention and perhaps deadlock  which is described in chapter 7  once the device has been requested  and allocated to us   we can read  write  and  possibly  reposition the device  just as we can with files in fact  the similarity between i/0 devices and files is so great that many operating systems  including unix  merge the two into a combined file-device structure  in this case  a set of system calls is used on both files and devices sometimes  2.4 65 l/0 devices are identified by special file names  directory placement  or file attributes  the user interface can also ncake files and devices appear to be similar1 even though the underlying system calls are dissimilar this is another example of the many design decisions that go into building an operating system and user interface  2.4.4 information maintenance many system calls exist simply for the purpose of transferring information between the user program and the operating system for example  most systems have a system call to return the current time and date other system calls may return information about the system  such as the number of current users  the version number of the operating system  the amount of free memory or disk space  and so on  another set of system calls is helpful in debugging a program many systems provide system calls to dump memory this provision is useful for debugging a program trace lists each system call as it is executed even microprocessors provide a cpu mode known as single step  in which a trap is executed by the cpu after every instruction the trap is usually caught by a debugger  many operating systems provide a time profile of a program to indicate the amount of time that the program executes at a particular location or set of locations a time prof ~ ~ ~ ~  c_92  1i ! ~ ~ ~ i ! ! 'cee a t ~  lc ~ ki2l  ility_s e !  egl1lar tii ! '_ee interrupts at every occurrence of the timer interrupt  the value of the program c6l-i  i ~ te1 -ls recorded with sufficiently frequent timer interrupts  a statistical picture of the time spent on various parts of the program can be obtained  in addition  the operating system keeps information about all its processes  and system calls are used to access this information generally  calls are also used to reset the process information  get process attributes and set process attributes   in section 3.1.3  we discuss what information is normally kept  2.4.5 communication th ~ ~ e ~ e two c   ll1l ~ cj  ji1_ _m od_e_l ~ _   fi ! ' ! e_ ! el   _c ~ ss_col'rll ! ' ~ ~ nica tion  the  l ! ' ~ ssag_e   _ passing model and the shared-memory model ! nth ~ il ! ~ s ~ ~ g_e  pa,s ~ ij1gl ! '   'lel t_l  t_ ~ _c    rrtll12injfa_fii ~ gpr c ~ ~  ~ ~ -e   c lailg ~ il'l-es ~ ~ ges with one another to transfer i  tcfo_rillaji   j   messages can be exchanged between the processes either directly or indirectly through a common mailbox before communication can take place  a connection must be opened the name of the other communicator must be known  be it another process on the same system or a process on another computer comcected by a communications network each computer in a network has a host name by which it is commonly known a host also has a network identifier  such as an ip address similarly  each process has a process narne  and this name is translated into an identifier by which the operating systemcanrefertotheprocess the get hostidand get processid system calls do this translation the identifiers are then passed to the generalpurpose open and close calls provided by the file system or to specific open connection and close connection system calls  depending on the system 's model of communication the recipient process usually must give its 66 chapter 2 2.5 permission for comnmnication to take place with an accept connection call  most processes that will be receiving connections are special-purpose daemons  which are systems programs provided for that purpose they execute a wait for connection call and are awakened when a connection is rna de the source of the communication  known as the client  and the receiving daenwn  known as a server  then exchange messages by using read message and write message system calls the close connection call terminates the communication  _ ! 11 the shared-me_1llorytllodel,proc ~ sses use s  tlared memorycreate and shared memory attach system calls to create 2rt1d gain access toi egions ot n1emory owned by other processes recall that  normally  the operatinisystein hiesf   prevei1foiie process-from accessing another process 's memory shared memory requires that two or more processes agree to remove this restriction  they can then exchange information by reading and writing data in the shared areas the form of the data is determined by the processes and are not under the operating system 's control the processes are also responsible for ensuring that they are not writing to the same location sirnultaneously such mechanisms are discussed in chapter 6 in chapter 4  we look at a variation of the process scheme-threads-in which memory is shared by default  both of the models just discussed are common in operating systems  and most systems implement both message passing is useful for exchanging smaller amounts of data  because no conflicts need be avoided it is also easier to implement than is shared memory for intercomputer communication shared memory allows maximum speed and convenience of communication  since it can be done at memory transfer speeds when it takes place within a computer  problems exist  however  in the areas of protection and synchronization between the processes sharing memory  2.4.6 protection protection provides a mechanism for controlling access to the resources provided by a computer system historically  protection was a concern only on multiprogrammed computer systems with several users however  with the advent of networking and the internet  all computer systems  from servers to pdas  must be concerned with protection  typically  system calls providing protection include set permission and get permission  which manipulate the permission settings of resources such as files and disks the allow user and deny user system calls specify whether particular users can-or can not-be allowed access to certain resources  we cover protection in chapter 14 and the much larger issue of security in chapter 15  another aspect of a modern system is the collection of system programs recall figure 1.1  which depicted the logical computer hierarchy at the lowest level is hardware next is the operating system  then the system programs  and finally the application programs system programs  also known as system utilities  provide a convenient enviromnenf1orprograrn-aevelopmeiita1inexecuhon  2.5 67 some of them are simply user interfaces to system calls ; others are considerably more complex they can be divided into these categories  file management these programs create  delete  copy  rename  print  dump  list  and generally ncanipulate files and directories  status information some programs simply ask the system for the date  time  amount of available memory or disk space  number of users  or similar status information others are more complex  providing detailed performance  logging  and debugging information typically  these programs format and print the output to the terminal or other output devices or files or display it in a window of the gui some systems also support a which is used to store and retrieve configuration information  file modification several text editors may be available to create and modify the content of files stored on disk or other storage devices there may also be special commands to search contents of files or perform transformations of the text  programming-language support compilers  assemblers  debuggers  and interpreters for common programming languages  such as c  c + +  java  visual basic  and perl  are often provided to the user with the operating system  program loading and execution once a program is assembled or compiled  it must be loaded into memory to be executed the system may provide absolute loaders  relocatable loaders  linkage editors  and overlay loaders debugging systems for either higher-level languages or machine language are needed as well  communications these programs provide the mechanism for creating virtual comcections among processes  users  and computer systems they allow users to send rnessages to one another 's screens  to browse web pages  to send electronic-mail messages  to log in remotely  or to transfer files from one machine to another  in addition to systems programs  most operating systems are supplied with programs that are useful in solving common problems or performing common operations such application  jj  ogr ! lj1ls iitclllde'if \ t ~ l  l l  jrg_wsf2r ~  worg processors an i text f6-rinatteis,spreadsheets  database systems  compilers  plott1i1g ana s-tafistica  -analysis packages  ancl gan1es ~        ___ tne viewoclne opei ; ating-sysrerri-seen b  t inost users is defined by the application and system programs  rather than by the actual systern calls  consider a user 's pc when a user 's computer is rumcing the mac os x operating system  the user might see the gui  featuring a mouse-and-windows interface alternatively  or even in one of the windows  the user might have a command-line unix shell both use the same set of system calls  but the system calls look different and act in different ways further confusing the user view  consider the user dual-booting from mac os x into windows vista  now the same user on the same hardware has two entirely different interfaces and two sets of applications using the same physical resources on the same 68 chapter 2 2.6 hardware  then  a user can be exposed to multiple user interfaces sequentially or concurrently  in this section  we discuss problems we face in designing and implementing an operating system there are  of course  no complete solutions to such problems  but there are approaches that have proved successful  2.6.1 design goals the first problem in designing a system is to define goals and specifications  at the highest level  the design of the system will be affected by the choice of hardware and the type of system  batch  time shared  single user  multiuser  distributed  real time  or general purpose  beyond this highest design level  the requirements may be much harder to specify the requirements can  however  be divided into two basic groups  user goals and system goals  users desire certain obvious properties in a system the system should be convenient to use  easy to learn and to use  reliable  safe  and fast of course  these specifications are not particularly useful in the system design  since there is no general agreement on how to achieve them  a similar set of requirements can be defined by those people who must design  create  maintain  and operate the system the system should be easy to design  implement  and maintain ; and it should be flexible  reliable  error free  and efficient again  these requirements are vague and may be interpreted in various ways  there is  in short  no unique solution to the problem of defining the requirements for an operating system the wide range of systems in existence shows that different requirements can result in a large variety of solutions for different environments for example  the requirements for vxworks  a realtime operating system for embedded systems  must have been substantially different from those for mvs  a large multiuser  multiaccess operating system for ibm mainframes  specifying and designing an operating system is a highly creative task  although no textbook can tell you how to do it  general principles have been developed in the field of software engineering  and we turn now to a discussion of some of these principles c  2.6.2 mechanisms and policies   i one important principle is the separation of policy from mechanisiil ~ echa   1'lis ~ s   leter111il1e hcnu ! q_c  @ -son'l ~ tl-til1g ; p   lic  les  i ~ termir e  zul1dt wilcbe done  for example  the timer construct  see section 1.5.2  is a mechani.sril  -forensill1ng cpu protection  but deciding how long the timer is to be set for a particular user is a policy decision  _  'h ~ _s_ 122l ! cl_tig ! l    fp.qli_cy_an_ci ~ t1_echanism is imp   rtant for flexibility policies are likely to change across places o1  'over time 'rri tll'e worst case  each change in policy would require a change in the underlying mechanism a general mechanism insensitive to changes in policy would be more desirable a change 2.6 69 in policy would then require redefinition of only certain parameters of the system for instance  consider a mechanism for giving priority to certain types of programs over others if the mechanism is properly separated from policy  it can be used either to support a policy decision that i/o-intensive progran1.s should have priority over cpu-intensive ones or to support the opposite policy  microkernel = based operati1lg sy_sh  ~ ms  section 2 .3  take the separation of mechai ~ 1sinai ~ cfp hcyto one extreme byimplementing a basicset   j_pri111.iti_y ~ 1jiwding bfocks these blocks are almost policy free  allowing more advanced -1necharnsms and policies to be added via user-created kernel modules or via user programs themselves as an example  consider the history of unix at first  it had a time-sharing scheduler in the latest version of solaris  scheduling is controlled by loadable tables depending on the table currently loaded  the system can be time shared  batch processing  real time  fair share  or any combination making the scheduling mechanism general purpose allows vast policy changes to be made with a single load-new-table command at th_ ~   th ~ r extreme is_il_ ~ ~ ~ t ~ l il ~  ttc  l ~ -as _ \ 1 \ t'i_ ! l_t  l   ! yj'c_ ~ \ 1 \ t ~ ~ icj  l ~ qt ~ j1 lec  ! '.c  l ~ ~ ~ 1l ~ and_p   _1i_c_y__a_  r ~ _epc    ciec  lj ~ 1._ ! he sy ~ te ~ _ t  j_e_ilforce__ ~ gl   ~ ~ l   ok an_cl_ fe_el all applications have similar interfaces  because the interface itself is built into the kernel and system libraries the mac os x operating system has similar functionality  policy decisions are important for all resource allocation whenever it is necessary to decide whether or not to allocate a resource  a policy decision must be made whenever the question is how rather than what  it is a mechanism that must be determined  2.6.3 implementation once an operating system is designed  it must be implemented traditionally  operating systems have been written in assembly language now  however  they are most commonly written in higher-level languages such as cor c + +  the first system that was not written in assembly language was probably the master control program  mcp  for burroughs computers mcp was written in a variant of algol multics  developed at mit  was written mainly in pl/1 the linux and windows xp operating systems are written mostly in c  although there are some small sections of assembly code for device drivers and for saving and restoring the state of registers  the advantages of using a higher-level language  or at least a systemsimplementation language  for implementing operating systems are the same as those accrued when the language is used for application programs  the code can be written faster  is more compact  and is easier to understand and debug in addition  improvements in compiler technology will improve the generated code for the entire operating system by simple recompilation finally  an operating system is far easier to port-to move to some other hardware-if it is written in a higher-level language for example  ms-dos was written in intel 8088 assembly language consequently  it runs natively only on the intel x86 family of cpus  although ms-dos runs natively only on intel x86  emulators of the x86 instruction set allow the operating system to run non-nativelyslower  with more resource use-on other cpus are programs that duplicate the functionality of one system with another system  the linux 70 chapter 2 2.7 operating system  in contrast  is written mostly inc and is available natively on a number of different cpus  including intel x86  sun sparc  and ibmpowerpc  the only possible disadvantages of implementing an operating system in a higher-level language are reduced speed and increased storage requirements  this  howeve1 ~ is no longer a major issue in today 's systems although an expert assembly-language programmer can produce efficient small routines  for large programs a modern compiler can perform complex analysis and apply sophisticated optimizations that produce excellent code modern processors have deep pipelining and n1.ultiple functional units that can handle the details of complex dependencies much more easily than can the human mind  as is true in other systems  major performance improvements in operating systems are more likely to be the result of better data structures and algorithms than of excellent assembly-language code in addition  although operating systems are large  only a small amount of the code is critical to high performance ; the memory manager and the cpu scheduler are probably the most critical routines  after the system is written and is working correctly  bottleneck routines can be identified and can be replaced with assembly-language equivalents  a system as large and complex as a modern operating system must be engineered carefully if it is to function properly and be modified easily a common approach is to partition the task into small components rather than have one monolithic system each of these modules should be a well-defined portion of the system  with carefully defined inputs  outputs  and functions  we have already discussed briefly in chapter 1 the common components of operating systems in this section  we discuss how these components are interconnected and melded into a kernel  2.7.1 simple structure many commercial operating systen1.s do not have well-defined structures  frequently  such systems started as small  simple  and limited systems and then grew beyond their original scope ms-dos is an example of such a systen1  it was originally designed and implemented by a few people who had no idea that it would become so popular it was written to provide the most functionality in the least space  so it was not divided into modules carefully  figure 2.12 shows its structure  in ms-dos  the interfaces and levels of functionality are not wellseparated  for rnstai1.ce  appii.cat1on programs aie able to access the basic i  b 1  outiri.es to write directly to the display and disk drives such freedom leaves ms-dos vulnerable to errant  or malicio lls  programs  causing entire system crashes when user programs fail of course  ms-dos was also limited by the hardware of its era because the intel 8088 for which it was written provides no dual mode and no hardware protection  the designers of ms-dos had no choice but to leave the base hardware accessible  another example of limited structuring is the original unix operating systein like ms ~ dc5s  unix initially was limited by hard ware ft1il.cfionali.ty ft consistsoftwo separahlepai ; fs  thei  eril.el ai1d the system prograrns  thekei  nel 2.7 71 rom bios device drivers figure 2.12 ms-dos layer structure  is further separated into a series of interfaces and device drivers  which have been added and expanded over the years as unix has evolved we can view the traditional unix operating system as being layered  as shown in figure 2.13  everything below the system-call interface and above the physical hardware is the kernel tb ~ l  ol ll ~ lp  rgvides__i  h ~ _fil ~ syste  rn  c   p_l ! _s ~ h ~ dulijl,g  memory management  and other operating-system fm1ctions through system calls  taken i.n sum ~ thatl.sai1 enormous an1ol.lnt of functionality to be combined into one level this monolithic structure was difficult to implement and maintain  2.7.2 layered approach withproper j  tarc  l \ a ! lre support  operating systems can be brokeninto pieces that are smaller and more app1  opriate thar  t  hose allowed by the _ _2 ! i2 ; g ~ af  the users  shells and commands compilers and interpreters system libraries signals terminal handling character 1/0 system terminal drivers file system swapping block 1/0 system disk and tape drivers cpu scheduling page replacement demand paging virtual memory figure 2.13 traditional unix system structure  72 chapter 2 figure 2.14 a layered operating system  m ~   .qoi'ilncil  l'j_ix systeill ~ the operating system can then retain much greater control over the computer and over the applications that make use of that computer implementers have more freedom in changing the inner workin.gs of the system and in creating modular operating systems under a topdown approach  the overall functionality and features are determined and are separated into components information hiding is also important  because it leaves programmers free to implement the low-level routines as they see fit  provided that the external interface of the routine stays unchanged and that the routine itself performs the advertised task  a system can be made modular in many ways qne method is the layered approach  in which the operating system is broken ii1to a 1l.umberoflayers  lever8j.ti1eoottom.iiiyer  layer 0  .1sthetiarawai ; e ; the nig  ytesl   layern   1sfhe user interface this layering structure is depicted in figure 2.14  an operating-system layer is an implementation of an abstract object made up of data and the operations that can manipulate those data a typical operating-system layer-say  layer m -consists of data structures and a set of routines that can be invoked by higher-level layers layer m  in turn  can invoke operations on lower-level layers  the main advantage of the layered approach is simplicity of construction and debugging the layers are selected so that each uses functions  operations  and services of only lower-level layers this approach simplifies debugging and .system verification the first layer can be debugged without any concern for the rest of the system  because  by definition  it uses only the basic hardware  which is assumed correct  to implement its functions once the first layer is debugged  its correct functioning can be assumed while the second layer is debugged  and so on if an error is found during the debugging of a particular layer  the error must be on that layer  because the layers below it are already debugged thus  the design and implementation of the system are simplified  2.7 73 each layer is implemented with only those operations provided by lowerlevel layers a layer does not need to know how these operations are implemented ; it needs to know only what these operations do hence  each layer hides the existence of certain data structures  operations  and hardware from higher-level layers  the major difficulty with the layered approach involves appropriately defining the various layers because a layer can use only lower-level layers  careful planning is necessary for example  the device driver for the backing store  disk space used by virtual-memory algorithms  must be at a lower level than the memory-management routines  because memory management requires the ability to use the backing store  other requirements may not be so obvious the backing-store driver would normally be above the cpu scheduler  because the driver may need to wait for i/0 and the cpu can be rescheduled during this time however  on a large system  the cpu scheduler m.ay have more information about all the active processes than can fit in memory therefore  this u1.formation may need to be swapped u1 and out of memory  requiring the backu1.g-store driver routine to be below the cpu scheduler  a final problem with layered implementations is that they tend to be less efficient than other types for instance  when a user program executes an i/0 operation  it executes a system call that is trapped to the i/0 layer  which calls the memory-management laye1 ~ which in tum calls the cpu-scheduling layer  which is then passed to the hardware at each layer  the parameters may be modified  data may need to be passed  and so on each layer adds overhead to the system call ; the net result is a system call that takes longer than does one on a nonlayered system  these limitations have caused a small backlash against layering in recent years fewer layers with more functionality are beu1.g designed  providu1.g most of the advantages of modularized code while avoidu1.g the difficult problems of layer definition and interaction  2.7.3 microkernels we have already seen that as unix expanded  the kernel became large and difficult to manage in the mid-1980s  researchers at carnegie mellon university developed an operatu1.g system called mach that modularized the kernel using the ~ i ~ roke ~ ll  ~ _ ! _ ~ ee1 ~   2lc  ~ ~ i.b ~ ._gl ~ ! b_   _  ! _0ructl ~ ~ ~ ~ --t ! ~ e operatingsystem by removing all nonessential cornponentsfrom thekemel and 1mp ~ e_l  l ~ -ll  ! ~ ~ ~ itil ~ !   t ~ ~ s ~ ~ fe_l il ~ ~ ~ ~ rl.ls_ ~ l  ~ i ~ \ r ~   jr   greili ~ ~  the.reslin is-a smarrei  kernel there is little consensus regarding which services should remain u1 the kernel and which should be implemented in user space typically  however  microkernels provide minimal process and memory management  in addition to a communication facility  the main function of the micro kernel is to provide a communication facility between the client program and the various services that are also rum1.ing in user space communication is provided by message passing  which was described in section 2.4.5 for example  if the client program wishes to access a file  it must interact with the file server the client program and service never interact directly rathel ~ they communicate indirectly by exchanging messages with the microkemel  74 chapter 2 one benefit of the microkernel approach is ease of extending the operating system all new services are added to user space and consequently do not require modification of the kernel when the kernel does have to be modified  the changes tend to be fewer  because the microkernel is a smaller kernel  the resulting operating system is easier to port from one hardware design to another the microkernel also provides more security and reliability  since most services are running as user-rather than kernel-processes if a service fails  the rest of the operating system remains untouched  several contemporary operating systems have used the microkernel approach tru64 unix  formerly digital unix  provides a unix interface to the user  but it is implemented with a mach kernel the mach kernel maps unix system calls into messages to the appropriate user-level services the mac os x kernel  also known as darwin  is also based on the mach micro kernel  another example is qnx  a real-time operating system the qnx nl.icrokernel provides services for message passing and process scheduling it also handles low-level network communication and hardware interrupts all other services in qnx are provided by standard processes that run outside the kernel in user mode  unfortunately  microkernels can suffer from performance decreases due to increased system function overhead consider the history of windows nt  the first release had a layered microkernel organization however  this version delivered low performance compared with that of windows 95 windows nt 4.0 partially redressed the performance problem by moving layers from user space to kernel space and integrating them more closely by the time windows xp was designed  its architecture was more monolithic than microkernel  2.7.4 modules perhaps the best current methodology for operating-system design involves using object-oriented programming techniques to create a modular kernel  here  the kernel has a set of core components and links in additional services either during boot time or during run time such a strategy uses dynamically loadable modules and is common in modern implementations of unix  such as solaris  linux  and mac os x for example  the solaris operating system structure  shown in figure 2.15  is organized armmd a core kernel with seven types of loadable kernel modules  scheduling classes file systems loadable system calls executable formats streams modules miscellaneous device and bus drivers such a design allows the kernel to provide core services yet also allows certain features to be implemented dynamically for example  device and 2.7 file systems figure 2.15 solaris loadable modules  loadable system calls 75 bus drivers for specific hardware can be added to the kernel  and support for different file systems can be added as loadable modules the overall result resembles a layered system in that each kernel section has defined  protected interfaces ; but it is more flexible than a layered system in that any module can call any other module furthermore  the approach is like the microkernel approach in that the primary module has only core functions and knowledge of how to load and communicate with other modules ; but it is more efficient  because modules do not need to invoke message passing in order to communicate  the apple mac os x operating system uses a hybrid structure it is a layered system in which one layer consists of the mach microkernel the structure of mac os x appears in figure 2.16 the top layers include application environments and a set of services providing a graphical interface to applications  below these layers is the kernel environment  which consists primarily of the mach microkernel and the bsd kernel mach provides memory management ; support for remote procedure calls  rpcs  and interprocess communication  ipc  facilities  including message passing ; and thread scheduling the bsd component provides a bsd command line interface  support for networking and file systems  and an implementation of posix apis  including pthreads  kernel environment application environments and common services figure 2.16 the mac os x structure  76 chapter 2 2.8 in addition to mach and bsd  the kernel environment provides an i/0 kit for development of device drivers and dynamically loadable modules  which mac os x refers to as kernel extensions   as shown in the figure  applications and comn  10n services can make use of either the mach or bsd facilities directly  the layered approach described in section 2.7.2 is taken to its logical conclusion in the concept of a the fundamental idea behind a virtual machine is to abstract the hardware of a si11.gle computer  the cpu  memory  disk drives  network interface cards  and so forth  into several different execution environments  thereby creating the illusion that each separate execution environment is run.ning its own private computer  by using cpu scheduling  chapter 5  and virtual-memory techniques  chapter 9   an operating system can create the illusion that a process has its own processor with its own  virtual  memory the virtual machine provides an interface that is identical to the underlying bare hardware each process is provided with a  virtual  copy of the underlying computer  figure 2.17   usually  the guest process is in fact an operating system  and that is how a single physical machine can run multiple operating systems concurrently  each in its own virtual machine  2.8.1 history virtual machines first appeared commercially on ibm mainframes via the vm operating system in 1972 vm has evolved and is still available  and many of processes programming/ / interface 1 ~ -----1 kernel  a  processes processes processes kernel kernel kernel vm1 vm2 vm3 virtual-machine implementation  b  figure 2.17 system models  a  nonvirtual machine  b  virtual machine  2.8 77 the original concepts are found in other systems  making this facility worth exploring  ibm vm370 divided a mainframe into nmltiple virtual machines  each numing its own operating system a ncajor difficulty with the vm virtualmachine approach involved disk systems suppose that the physical machine had three disk drives but wanted to support seven virtual machines clearly  it could not allocate a disk drive to each virtual machine  because the virtualmachine software itself needed substantial disk space to provide virtual memory and spooling the solution was to provide virtual disks-termed minidislcs in ibm 's vm operating system -that are identical in all respects except size the system implemented each minidisk by allocating as many tracks on the physical disks as the minidisk needed  once these virtual machines were created  users could run any of the operating systems or software packages that were available on the underlying machine for the ibm vm system  a user normally ran cms-a single-user interactive operating system  2.8.2 benefits there are several reasons for creating a virtual machine most of them are fundarnentally related to being able to share the same hardware yet run several different execution environments  that is  different operating systems  concurrently  one important advantage is that the host system is protected from the virtual machines  just as the virtual machines are protected from each other a virus inside a guest operating system might damage that operating system but is unlikely to affect the host or the other guests because each virtual machine is completely isolated from all other virtual machines  there are no protection problems at the same time  however  there is no direct sharing of resources  two approaches to provide sharing have been implemented first  it is possible to share a file-system volume and thus to share files second  it is possible to define a network of virtual machines  each of which can send information over the virtual communications network the network is modeled after physical communication networks but is implemented in software  a virtual-machine system is a perfect vehicle for operating-systems research and development normally  changing an operating system is a difficult task operating systems are large and complex programs  and it is difficult to be sure that a change in one part will not cause obscure bugs to appear in some other part the power of the operating system makes changing it particularly dangerous because the operating system executes in kernel mode  a wrong change in a pointer could cause an error that would destroy the entire file system thus  it is necessary to test all changes to the operating system carefully  the operating system  however  runs on and controls the entire machine  therefore  tlle current system must be stopped and taken out of use while changes are made and tested this period is comnconly called systemdevelopment time since it makes the system unavailable to users  systemdevelopment time is often scheduled late at night or on weekends  when system load is low  78 chapter 2 a virtual-machine system can eliminate much of this problem system programmers are given their own virtual machine  and system development is done on the virtual machine instead of on a physical machine normal system operation seldom needs to be disrupted for system development  another advantage of virtual machines for developers is that multiple operating systems can be running on the developer 's workstation concurrently  this virtualized workstation allows for rapid porting and testing of programs in varying enviromnents sin'lilarly  quality-assurance engineers can test their applications in multiple environments without buying  powering  and maintaining a computer for each environment  a major advantage of virtual machines in production data-center use is system which involves taking two or more separate systems and running them in virtual machines on one system such physical-to-virtual conversions result in resource optimization  as many lightly used systems can be combined to create one more heavily used system  if the use of virtual machines continues to spread  application deployment will evolve accordingly if a system can easily add  remove  and move a virtual machine  then why install applications on that system directly instead  application developers would pre-install the application on a tuned and customized operating system in a virh1al machine that virtual environment would be the release mechanism for the application this method would be an improvement for application developers ; application management would become easier  less tuning would required  and technical support of the application would be more straightforward system administrators would find the environment easier to manage as well installation would be simple  and redeploying the application to another system would be much easier than the usual steps of uninstalling and reinstalling for widespread adoption of this methodology to occur  though  the format of virtual machines must be standardized so that any virtual machine will run on any virtualization platform the open virtual machine format is an attempt to do just that  and it could succeed in unifying virtual-machine formats  2.8.3 simulation system virtualization as discussed so far is just one of many system-emulation methodologies virtualization is the most common because it makes guest operating systems and applications believe they are running on native hardware because only the system 's resources need to be virtualized  these guests run at almost full speed  another methodology is in which the host system has one system architecture and the guest system was compiled for a different architecture  for example  suppose a company has replaced its outdated computer system with a new system but would like to continue to run certain important programs that were compiled for the old system the programs could be run in an e1nulator that translates each of the outdated system 's instructions into the native instruction set of the new system emulation can increase the life of programs and allow us to explore old architectures without having an actual old machine  but its major challenge is performance instruction-set emulation can run an order of magnitude slower than native instructions thus  unless the new machine is ten times faster than the old  the program running on 2.8 79 the new machine will run slower than it did on its native hardware another challenge is that it is difficult to create a correct emulator because  in essence  this involves writing an entire cpu in software  2.8.4 para-virtualization is another vanat10n on this theme rather than try to trick a guest operating system into believing it has a system to itself  paravirtualization presents the guest with a system that is similar but not identical to the guest 's preferred system the guest must be modified to run on the paravirtualized hardware the gain for this extra work is more efficient use of resources and a smaller virtualization layer  solaris 10 includes or that create a virtual layer between the operating system and the applications in this system  only one kernel is installed  and the hardware is not virtualized rather  the operating system and its devices are virtualized  providing processes within a container with the impression that they are the only processes on the system one or more containers can be created  and each can have its own applications  network stacks  network address and ports  user accounts  and so on cpu resources can be divided up among the containers and the systemwide processes figure 2.18 shows a solaris 10 system with two containers and the standard global user space  user programs system programs cpu resources memory resources global zone user programs system programs network addresses device access cpu resources user programs system programs network addresses device access cpu resources memory resources memory resources zone 1 zone 2 virtual platform device management figure 2.18 solaris i 0 with two containers  80 chapter 2 2.8.5 implementation although the virtual-machine concept is usefut it is difficult to implement  much work is required to provide an exact duplicate of the underlying machine  remember that the underlying machine typically has two modes  user mode and kernel mode the virtual-machine software can run in kernel mode  since it is the operating system the virtual machine itself can execute in only user mode just as the physical machine has two modes  however  so must the virtual machine consequently  we must have a virtual user mode and a virtual kernel mode  both of which run in a physical user mode those actions that cause a transfer from user mode to kernel mode on a real machine  such as a system call or an attempt to execute a privileged instruction  must also cause a transfer from virtual user mode to virtual kernel mode on a virtual machine  such a transfer can be accomplished as follows when a system calt for example  is made by a program running on a virtual machine in virtual user mode  it will cause a transfer to the virtual-machine monitor in the real machine  when the virtual-machine monitor gains controt it can change the register contents and program counter for the virtual machine to simulate the effect of the system call it can then restart the virtual machine  noting that it is now in virtual kernel mode  the major difference  of course  is time whereas the real i/o might have taken 100 milliseconds  the virtual i/o might take less time  because it is spooled  or more time  because it is interpreted   in addition  the cpu is being multi programmed among many virtual machines  further slowing down the virtual machines in unpredictable ways in the extreme case  it may be necessary to simulate all instructions to provide a true virtual machine vm  discussed earlier  works for ibm machines because normal instructions for the virtual machines can execute directly on the hardware only the privileged instructions  needed mainly for i/0  must be simulated and hence execute more slowly  without some level of hardware support  virtualization would be impossible  the more hardware support available within a system  the more feature rich  stable  and well performing the virtual machines can be all major generalpurpose cpus provide some amount of hardware support for virtualization  for example  amd virtualization technology is found in several amd processors  it defines two new modes of operation-host and guest virtual machine software can enable host mode  define the characteristics of each guest virtual machine  and then switch the system to guest mode  passing control of the system to the guest operating system that is running in the virtual machine  in guest mode  the virtualized operating system thinks it is rum1.ing on native hardware and sees certain devices  those included in the host 's definition of the guest   if the guest tries to access a virtualized resource  then control is passed to the host to manage that interaction  2.8.6 examples despite the advantages of virtual machines  they received little attention for a number of years after they were first developed today  however  virtual machines are coming into fashion as a means of solving system compatibility problems in this section  we explore two popular contemporary virtual machines  the vmware workstation and the java virtual machine as you 2.8 81 will see  these virtual machines can typically run on top of operating systems of any of the design types discussed earlier thus  operating system design methods-simple layers  microkernels  n  wdules  and virtual machines-are not mutually exclusive  2.8.6.1 vmware most of the virtualization techniques discussed in this section require virtualization to be supported by the kernel another method involves writing the virtualization tool to run in user mode as an application on top of the operating system virtual machines running within this tool believe they are rum ing on bare hardware but in fact are running inside a user-level application  is a popular commercial application that abstracts intel x86 and compatible hardware into isolated virtual machines vmware workstation runs as an application on a host operating system such as windows or linux and allows this host system to concurrently run several different guest operating systems as independent virtual machines  the architecture of such a system is shown in figure 2.19 in this scenario  linux is running as the host operating system ; and freebsd  windows nt  and windows xp are rum ing as guest operating systems the virtualization layer is the heart of vmware  as it abstracts the physical hardware into isolated virtual machines running as guest operating systems each virtual machine has its own virtual cpu  memory  disk drives  network interfaces  and so forth  the physical disk the guest owns and manages is really just a file within the file system of the host operating system to create an identical guest instance  we can simply copy the file copying the file to another location protects the guest instance against a disaster at the original site moving the file to another application application application application guest operating guest operating guest operating system system system  free bsd   windows nt   windows xp  virtual cpu virtual cpu virtual cpu virtual memory virtual memory virtual memory virtual devices virtual devices virtual devices virtualization layer hardware i qpu   i r ! jemgfy figure 2.19 vmware architecture  82 chapter 2 location moves the guest system these scenarios show how virtualization can improve the efficiency of system administration as well as system resource use  2.8.6.2 the java virtual machine java is a popular object-oriented programming language introduced by sun microsystems in 1995 in addition to a language specification and a large api library  java also provides a specification for a java virtual machine-or jvm  java objects are specified with the class construct ; a java program consists of one or more classes for each java class  the compiler produces an architecture-neutral bytecode output  .class  file that will run on any implementation of the jvm  the jvm is a specification for an abstract computer it consists of a class loader and a java interpreter that executes the architecture-neutral bytecodes  as diagrammed in figure 2.20 the class loader loads the compiled  class files from both the java program and the java api for execution by the java interpreter after a class is loaded  the verifier checks that the  class file is valid java bytecode and does not overflow or underflow the stack it also ensures that the bytecode does not perform pointer arithmetic  which could provide illegal memory access if the class passes verification  it is run by the java interpreter the jvm also automatically manages memory by performing garbage collection -the practice of reclaiming memory from objects no longer in use and returning it to the system much research focuses on garbage collection algorithms for increasing the performance of java programs in the virtual machine  the jvm may be implemented in software on top of a host operating system  such as windows  linux  or mac os x  or as part of a web browser  alternatively  the jvm may be implemented in hardware on a chip specifically designed to nm java programs if the jvm is implemented in software  the java interpreter interprets the bytecode operations one at a time a faster software technique is to use a just-in-time  jit  compiler here  the first time a java method is invoked  the bytecodes for the method are turned into native machine language for the host system these operations are then cached so that subsequent invocations of a method are performed using the native machine instructions and the bytecode operations need not be interpreted all over again  a technique that is potentially even faster is to nm the jvm in hardware on a java program .class files  i class loader 1 +  + i java i interpreter t host system  windows  linux  etc  figure 2.20 the java virtual machine  2.8 the .net framework the .net framework is a collection of technologies  including a set of class libraries  and an execution environment that come together to provide a platform for developing software this platform allows programs to be written to target the .net framework instead of a specific architecture a program written for the .net framework need not worry aboutthe specifics of the hardware or the operating system on which it will run thus  any architecture implementing .net will be able to successfully execute the program this is because the execution environment abstracts these details and provides a virtual machine as an intermediary between the executing program and the underlying architecture  at the core of the .net framework is the common language runtime  clr   the clr is the implementation of the .net virtual machine itprovides an environment for execution of programs written in any of the languages targeted at the .net framework programs written in languages such as c #  pronounced c-sharp  and vb.net are compiled into an intermediate  architecture-independent language called microsoft intermediate language  ms-il   these compiled files  called assemblies  include ms-il instructions and metadata they have file extensions of either .exe or .dll upon execution of a program  the clr loads assemblies into what .is known as the application domain as instructions are requested by the executing program  the clr converts the ms-il instructions inside the assemblies into native code that is specific to the underlying architecture using just-in-time compilation once instructions have been converted to native code  they are kept and will continue to run as native code for the cpu the architecture of the clr for the .net framework is shown in figure 2.21  compilation clr c + + source ms-il assembly vb.net source ms-il assembly host system figure 2.21 architecture ofthe.clr for the .net framework  83 84 chapter 2 2.9 special java chip that executes the java bytecode operations as native code  thus bypassing the need for either a software interpreter or a just-in-tim.e compiler  broadly  is the activity of finding and fixing errors  or in a system debugging seeks to find and fix errors in both hardware and software  performance problems are considered bugs  so debugging can also include which seeks to improve performance by removing  ' '  in the processing taking place within a system a discussion of hardware debugging is outside of the scope of this text in this section  we explore debugging kernel and process errors and performance problems  2.9.1 failure analysis if a process fails  most operating systems write the error information to a to alert system operators or users that the problem occurred the operating system can also take a capture of the memory  referred to as the core in the early days of computing  of the process this core image is stored in a file for later analysis running programs and core dumps can be probed by a a tool designed to allow a programmer to explore the code and memory a process  debugging user-level process code is a challenge operating system kernel debugging even more complex because of the size and complexity of the kernel  its control of the hardware  and the lack of user-level debugging tools a kernel failure is called a as with a process failure  error information is saved to a log file  and the memory state is saved to a operating system debugging frequently uses different tools and techniques than process debugging due to the very different nature of these two tasks  consider that a kernel failure in the file-system code would make it risky for the kernel to try to save its state to a file on the file system before rebooting  a common technique is to save the kernel 's memory state to a section of disk set aside for this purpose that contains no file system  if the kernel detects an unrecoverable error  it writes the entire contents of memory  or at least the kernel-owned parts of the system memory  to the disk area when the system reboots  a process runs to gather the data from that area and write it to a crash dump file within a file system for analysis  2.9.2 performance tuning to identify bottlenecks  we must be able to monitor system performance code must be added to compute and display measures of system behavior in a number of systems  the operating system does this task by producing trace listings of system behavior all interesting events are logged with their time and important parameters and are written to a file later  an analysis program can process the log file to determine system performance and to identify bottlenecks and inefficiencies these same traces can be run as input for a simulation of a suggested improved system traces also can help people to find errors in operating-system behavior  2.9 kernighan 's law debugging is twice as hard as writing the code in the first place therefore  if you write the code as cleverly as possible  you are  by definition  not smart enough to debug it  85 another approach to performance tuning is to include interactive tools with the system that allow users and administrators to question the state of various components of the system to look for bottlenecks the unix command top displays resources used on the system  as well as a sorted list of the top resource-using processes other tools display the state of disk i/0  memory allocation  and network traffic the authors of these single-purpose tools try to guess what a user would want to see while analyzing a system and to provide that information  making running operating systems easier to understand  debug  and tune is an active area of operating system research and implementation the cycle of enabling tracing as system problems occur and analyzing the traces later is being broken by a new generation of kernel-enabled performance analysis tools further  these tools are not single-purpose or merely for sections of code that were written to emit debugging data the solaris 10 dtrace dynamic tracing facility is a leading example of such a tool  2.9.3 dtrace is a facility that dynamically adds probes to a running system  both i11 user processes and in the kernel these probes can be queried via the d programming language to determine an astonishing amount about the kernel  the system state  and process activities for example  figure 2.22 follows an application as it executes a system call  ioctl  and further shows the functional calls within the kernel as they execute to perform the system call lines ending with u are executed in user mode  and lines ending in k in kernel mode  debugging the interactions between user-level and kernel code is nearly impossible without a toolset that understands both sets of code and can instrument the interactions for that toolset to be truly useful  it must be able to debug any area of a system  including areas that were not written with debugging in mind  and do so without affecting system reliability this tool must also have a minimum performance impact-ideally it should have no impact when not in use and a proportional impact during use the dtrace tool meets these requirements and provides a dynamic  safe  low-impact debugging environncent  until the dtrace framework and tools became available with solaris 10  kernel debugging was usually shrouded in mystery and accomplished via happenstance and archaic code and tools for example  cpus have a breakpoint feature that will halt execution and allow a debugger to examine the state of the system then execution can continue until the next breakpoint or termination  this method can not be used in a multiuser operating-system kernel without negatively affecting all of the users on the system pn  reen,g  which periodically samples the instruction pointer to determine which code is being executed  can show statistical trends but not individual activities code can be included in the kernel to emit specific data under specific circumstances  but that code 86 chapter 2 # ./all.d 'pgrep xclock ' xeventsqueued dtrace  script './all.d ' matched 52377 probes cpu function 0  xeventsqueued 0  _xeventsqueued u u 0  _xlltransbytesreadable u 0  _xlltransbytesreadable u 0  _xlltranssocketbytesreadable u 0  _xlltranssocketbytesreadable u 0  ioctl u 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0      ioctl   getf  set active fd  set active fd  getf  get udatamodel  get udatamodel  releasef  clear active   clear active  cv broadcast  cv broadcast  releasef ioctl ioctl xeventsqueued xeventsqueued fd fd k k k k k k k k k k k k k k u u u figure 2.22 solaris 10 dtrace follows a system call within the kernel  slows down the kernel and tends not to be included in the part of the kernel where the specific problem being debugged is occurring  in contrast  dtrace runs on production systems-systems that are running important or critical applications-and causes no harm to the system it slows activities while enabled  but after execution it resets the system to its pre-debugging state it is also a broad and deep tool it can broadly debug everything happening in the system  both at the user and kernel levels and between the user and kernel layers   dtrace can also delve deeply into code  showing individual cpu instructions or kernel subroutine activities  is composed of a compiler  a framework  of written within that framework  and of those probes dtrace providers create probes kernel structures exist to keep track of all probes that the providers have created the probes are stored in a hash table data structure that is hashed by name and indexed according to unique probe identifiers  when a probe is enabled  a bit of code in the area to be probed is rewritten to call dtrace_probe  probe identifier  and then continue with the code 's original operation different providers create different kinds of probes for example  a kernel system-call probe works differently from a user-process probe  and that is different from an i/o probe  dtrace features a compiler that generates a byte code that is run in the kernel this code is assured to be safe by the compiler for example  no 2.9 87 loops are allowed  and only specific kernel state modifications are allowed when specifically requested only users with the dtrace privileges  or root users  are allowed to use dt ! ace  as it can retrieve private kernel data  and modify data if requested   the generated code runs in the kernel and enables probes it also enables consumers in user mode and enables communications between the two  a dt ! ace consumer is code that is interested in a probe and its results  a consumer requests that the provider create one or more probes when a probe fires  it emits data that are managed by the kernel within the kernel  actions called or are performed when probes fire one probe can cause multiple ecbs to execute if more than one consumer is interested in that probe each ecb contains a predicate  if statement  that can filter out that ecb otherwise  the list of actions in the ecb is executed the most usual action is to capture some bit of data  such as a variable 's value at that point of the probe execution by gathering such data  a complete picture of a user or kernel action can be built further  probes firing from both user space and the kernel can show how a user-level action caused kernel-level reactions  such data are invaluable for performance monitoril1.g and code optimization  once the probe consumer tennil1.ates  its ecbs are removed if there are no ecbs consuming a probe  the probe is removed that involves rewriting the code to remove the dtrace_probe call and put back the original code thus  before a probe is created and after it is destroyed  the system is exactly the same  as if no probing occurred  dtrace takes care to assure that probes do not use too much memory or cpu capacity  which could harm the running system the buffers used to hold the probe results are monitored for exceeding default and maximum limits  cpu time for probe execution is monitored as well if limits are exceeded  the consumer is terminated  along with the offending probes buffers are allocated per cpu to avoid contention and data loss  an example ofd code and its output shows some of its utility the following program shows the dtrace code to enable scheduler probes and record the amount of cpu time of each process running with user id 101 while those probes are enabled  that is  while the program nms   sched    on-cpu uid = = 101  self ts timestamp ;  sched    off -cpu self ts   time  execname  self ts = 0 ; sum  timestamp self ts  ; the output of the program  showing the processes and how much time  in nanoseconds  they spend running on the cpus  is shown in figure 2.23  88 chapter 2 2.10 # dtrace -s sched.d dtrace  script 'sched.d ' matched 6 probes ac grwme-settings-d gnome-vfs-daemon dsdm wnck-applet gnome-panel clock-applet mapping-daemon xscreensaver meta city xorg gnome-terminal mixer applet2 java 142354 158243 189804 200030 277864 374916 385475 514177 539281 2579646 5007269 7388447 10769137 figure 2.23 output of the 0 code  because dtrace is part of the open-source solaris 10 operating system  it is being added to other operating systems when those systems do not have conflicting license agreements for example  dtrace has been added to mac os x 10.5 and freebsd and will likely spread further due to its unique capabilities other operating systems  especially the linux derivatives  are adding kernel-tracing functionality as well still other operating systems are beginning to include performance and tracing tools fostered by research at various institutions  including the paradyn project  it is possible to design  code  and implement an operating system specifically for one machine at one site more commonly  however  operating systems are designed to nm on any of a class of machines at a variety of sites with a variety of peripheral configurations the system must then be configured or generated for each specific computer site  a process sometimes known as system generation  sysgen   the operating system is normally distributed on disk  on cd-rom or dvd-rom  or as an iso image  which is a file in the format of a cd-rom or dvd-rom to generate a system  we use a special program this sysgen program reads from a given file  or asks the operator of the system for information concerning the specific configuration of the hardware systenc  or probes the hardware directly to determine what components are there the following kinds of information must be determined  what cpu is to be used what options  extended instruction sets  floatingpoint arithmetic  and so on  are installed for multiple cpu systems  each cpu may be described  2.11 2.11 89 how will the boot disk be formatted how many sections  or partitions  will it be separated into  and what will go into each partition how much memory is available some systems will determine this value themselves by referencing memory location after memory location until an illegal address fault is generated this procedure defines the final legal address and hence the amount of available memory  what devices are available the system will need to know how to address each device  the device number   the device interrupt number  the device 's type and model  and any special device characteristics  what operating-system options are desired  or what parameter values are to be used these options or values might include how many buffers of which sizes should be used  what type of cpu-scheduling algorithm is desired  what the maximum number of processes to be supported is  and so on  once this information is determined  it can be used in several ways at one extreme  a system administrator can use it to modify a copy of the source code of the operating system the operating system then is completely compiled data declarations  initializations  and constants  along with conditional compilation  produce an output-object version of the operating system that is tailored to the system described  at a slightly less tailored level  the system description can lead to the creation of tables and the selection of modules from a precompiled library  these modules are linked together to form the generated operating system  selection allows the library to contain the device drivers for all supported i/0 devices  but only those needed are linked into the operating system because the system is not recompiled  system generation is faster  but the resulting system may be overly general  at the other extreme  it is possible to construct a system that is completely table driven all the code is always part of the system  and selection occurs at execution time  rather than at compile or lil1.k time system generation involves simply creating the appropriate tables to describe the system  the major differences among these approaches are the size and generality of the generated system and the ease of modifying it as the hardware configuration changes consider the cost of modifying the system to support a newly acquired graphics termil1.al or another disk drive balanced against that cost  of course  is the frequency  or infrequency  of such changes  after an operating system is generated  it must be made available for use by the hardware but how does the hardware know where the kernel is or how to load that kernel the procedure of starting a computer by loading the kernel is known as booting the system on most computer systems  a small piece of code known as the bootstrap program or bootstrap loader locates the kernel  loads it into main memory  and starts its execution some computer systems  such as pcs  use a two-step process in which a simple bootstrap loader fetches a more complex boot program from disk  which in turn loads the kernel  90 chapter 2 2.12 when a cpu receives a reset event-for instance  when it is powered up or rebooted -the instruction register is loaded with a predefined memory location  and execution starts there at that location is the initial bootstrap program this program is in the form of read-only memory  rom   because the ram is in an unknown state at system startup rom is convenient because it needs no initialization and can not easily be infected by a computer virus  the bootstrap program can perform a variety of tasks usually  one task is to run diagnostics to determine the state of the machine if the diagnostics pass  the program can continue with the booting steps it can also initialize all aspects of the system  from cpu registers to device controllers and the contents of main memory sooner or later  it starts the operating system  some systems-such as cellular phones  pdas  and game consoles-store the entire operating system in rom storing the operating system in rom is suitable for small operating systems  simple supporting hardware  and rugged operation a problem with this approach is that changing the bootstrap code requires changing the rom hardware chips some systems resolve this problem by using erasable programmable read-only memory  eprom   which is readonly except when explicitly given a command to become writable all forms of rom are also known as firmware  since their characteristics fall somewhere between those of hardware and those of software a problem with firmware in general is that executing code there is slower thart executing code in ram  some systems store the operating system in firmware and copy it to ram for fast execution a final issue with firmware is that it is relatively expensive  so usually only small ammmts are available  for large operating systems  including most general-purpose operating systems like windows  mac os x  and unix  or for systems that change frequently  the bootstrap loader is stored in firmware  and the operating system is on disk in this case  the bootstrap nms diagnostics and has a bit of code that can read a single block at a fixed location  say block zero  from disk into memory and execute the code from that b ! ock the program stored in the boot block may be sophisticated enough to load the entire operating system into memory and begin its execution more typically  it is simple code  as it fits in a single disk block  and knows only the address on disk and length of the remainder of the bootstrap program is an example of an open-source bootstrap program for linux systems all of the disk-bound bootstrap  and the operating system itself  can be easily changed by writing new versions to disk  a disk that has a boot partition  more on that in section 12.5.1  is called a boot disk or system disk  now that the full bootsh ap program has been loaded  it can traverse the file system to find the operating system kernel  load it into memory  and start its execution it is only at this point that the system is said to be running  operating systems provide a number of services at the lowest level  system calls allow a running program to make requests from the operating system directly at a higher level  the command interpreter or shell provides a mechanism for a user to issue a request without writing a program commands may come from files during batch-mode execution or directly from a terminal 91 when in an interactive or time-shared mode system programs are provided to satisfy many common u.ser requests  the types of requests vary accord  ilcg to level the system-call level must provide the basic functions  such as process control and file and device manipulation higher-level requests  satisfied by the command interpreter or system programs  are translated into a sequence of system calls system services can be classified into several categories  program controt status requests  and i/0 requests program errors can be considered implicit requests for service  once the system services are defined  the structure of the operating system can be developed various tables are needed to record the information that defines the state of the computer system and the status of the system 's jobs  the design of a new operating system is a major task it is important that the goals of the system be well def  ilced before the design begins the type of system desired is the foundation for choices among various algorithms and strategies that will be needed  s  iilce an operating system is large  modularity is important designing a system as a sequence of layers or using a microkernel is considered a good technique the virtual-machine concept takes the layered approach and treats both the kernel of the operat  ilcg system and the hardware as though they were hardware even other operating systems may be loaded on top of this virtual machine  throughout the entire operating-system design cycle  we must be careful to separate policy decisions from implementation details  mechanisms   this separation allows maximum flexibility if policy decisions are to be changed later  operating systems are now almost always written in a systemsimplementation language or in a higher-level language this feature improves their implementation  maintenance  and portability to create an operating system for a particular machine configuration  we must perform system generation  debugging process and kernel failures can be accomplished through the use of de buggers and other tools that analyze core dumps tools such as dtrace analyze production systems to fucd bottlenecks and understand other system behavior  for a computer system to begin running  the cpu must initialize and start executing the bootstrap program in firmware the bootstrap can execute the operating system directly if the operating system is also in the firmware  or it can complete a sequence in which it loads progressively smarter programs from firmware and disk until the operating system itself is loaded into memory and executed  2.1 what are the five major activities of an operat  ilcg system with regard to file management 2.2 what are the three major activities of an operating system with regard to memory management 92 chapter 2 2.3 why is a just-in-time compiler useful for executing java programs 2.4 the services and functions provided by an operating system can be divided into two main categories briefly describe the two categories and discuss how they differ  2.5 why is the separation of mechanism and policy desirable 2.6 would it be possible for the user to develop a new command interpreter using the system-call interface provided by the operating system 2.7 what is the purpose of the command interpreter why is it usually separate from the kernel 2.8 what is the main advantage for an operating-system designer of using a virtual-machine architecture what is the main advantage for a user 2.9 it is sometimes difficult to achieve a layered approach if two components of the operating system are dependent on each other identify a scenario in which it is unclear how to layer two system components that require tight coupling of their functionalities  2.10 what is the main advantage of the layered approach to system design what are the disadvantages of using the layered approach 2.11 what is the relationship between a guest operating system and a host operating system in a system like vmware what factors need to be considered in choosing the host operating system 2.12 describe three general methods for passing parameters to the operating system  2.13 what is the main advantage of the microkemel approach to system design how do user programs and system services interact in a microkernel architecture what are the disadvantages of usil1.g the microkernel approach 2.14 what system calls have to be executed by a command interpreter or shell in order to start a new process 2.15 what are the two models of interprocess conununication what are the strengths and weaknesses of the two approaches 2.16 the experimental synthesis operating system has an assembler incorporated in the kernel to optimize system-call performance  the kernel assembles routines within kernel space to minimize the path that the system call must take through the kernel this approach is the antithesis of the layered approach  in which the path through the kernel is extended to make buildu1.g the operating system easier discuss the pros and cons of the synthesis approach to kernel design and system-performance optimization  2.17 in what ways is the modular kernel approach similar to the layered approach in what ways does it differ from the layered approach 2.18 how could a system be designed to allow a choice of operating systems from which to boot what would the bootstrap program need to do 93 2.19 what are the advantages and disadvantages of using the same systemcall interface for manipulating both files and devices 2.20 describe how you could obtain a statistical profile of the amount of time spent by a program executing different sections of its code discuss the importance of obtaining such a statistical profile  2.21 why do some systems store the operating system in firmware  while others store it on disk 2.22 in section 2.3  we described a program that copies the contents of one file to a destination file this program works by first prompting the user for the name of the source and destilcation files write this program using either the win32 or posix api be sure to include all necessary error checking  including ensuring that the source file exists  once you have correctly designed and tested the program  if you used a system that supports it  run the program using a utility that traces system calls linux systems provide the ptrace utility  and solaris systems use the truss or dtrace command on mac os x  the ktrace facility provides similar functionality as windows systems do not provide such features  you will have to trace through the win32 version of this program using a debugger  2.23 adding a system call to the linux kernel in this project you will study the system-call interface provided by the linux operating system and learn how user programs communicate with the operating system kernel via this interface your task is to i11corporate a new system call into the kernet thereby expanding the functionality of the operating system  part 1  getting started a user-mode procedure call is performed by passing arguments to the called procedure either on the stack or through registers  saving the current state and the value of the program counter  and jumping to the beginning of the code corresponding to the called procedure the process continues to have the same privileges as before  system calls appear as procedure calls to user programs but result i11 a change in execution context and privileges in linux on the intel386 architecture  a system call is accomplished by storing the system-call number into the eax register  storing arguments to the system call in other hardware registers  and executing a trap instruction  which is the 94 chapter 2 int ox80 assembly instruction   after the trap is executed  the systemcall number is used to index into a table of code pointers to obtain the starting address for the handler code implementing the system call the process then juxnps to this address  and the privileges of the process are switched from user to kernel mode with the expanded privileges  the process can now execute kernel code  which may include privileged instructions that can not be executed in user mode the kernel code can then carry out the requested services  such as interacting with i/o devices  and can perform process management and other activities that can not be performed in user mode  the system call numbers for recent versions of the linux kernel are listed in lusr i src/linux-2 xl include/ asm-i386/unistd h   for instance  __ nr_close corresponds to the system call close 0  which is invoked for closin.g a file descriptor  and is defined as value 6  the list of pointers to system-call handlers is typically stored in the file lusrlsrcllinux-2.x/arch/i386/kernel/entry.s under the heading entry  sys_calltable   notice that sys_close is stored at entry number 6 in the table to be consistent with the system-call number defined in the unistd h file  the keyword .long denotes that the entry will occupy the same number of bytes as a data value of type long  part 2  building a new kernel before adding a system call to the kernel  you must familiarize yourself with the task of building the binary for a kernel from its source code and booting the machine with the newly built kernel this activity comprises the following tasks  some of which depend on the particular installation of the linux operating system in use  obtain the kernel source code for the linux distribution if the source code package has already been installed on your machine  the corresponding files might be available under lusr i srcllinux or /usr i src/linux-2 x  where the suffix corresponds to the kernel version number   if the package has not yet been installed  it can be downloaded from the provider of your linux distribution or from http  l/www.kernel.org  learn how to configure  compile  and install the kernel binary this will vary among the different kernel distributions  but some typical commands for building the kernel  after entering the directory where the kernel source code is stored  include  o make xconfig o make dep o make bzimage add a new entry to the set of boatable kernels supported by the system the linux operating system typically uses utilities such as lilo and grub to maintain a list ofbootable kernels from which the 95 user can choose during machine boot-up if your system supports lilo  add an entry to lilo conf  such as  image = /boot/bzimage.mykernel label = mykernel root = /dev/hda5 read-only where lbootlbzimage my kernel is the kernel image and my kernel is the label associated with the new kernel this step will allow you to choose the new kernel during the boot-up process you will then have the option of either booting the new kernel or booting the unmodified kernel if the newly built kernel does not ftmction properly  part 3  extending the kernel source you can now experiment with adding a new file to the set of source files used for compiling the kernel typically  the source code is stored in the lusr i srcllinux-2 xlkernel directory  although that location may differ in your linux distribution there are two options for adding the system call the first is to add the system call to an existing source file in this directory the second is to create a new file in the source directory and modify lusr i srcllinux-2 xlkernelimakef ile to include the newly created file in the compilation process the advantage of the first approach is that when you modify an existing file that is already part of the compilation process  the makefile need not be modified  part 4  adding a system call to the kernel now that you are familiar with the various background tasks corresponding to building and booting linux kernels  you can begin the process of adding a new system call to the linux kernel in this project  the system call will have limited functionality ; it will simply transition from user mode to kernel mode  print a message that is logged with the kernel messages  and transition back to user mode we will call this the helloworld system call while it has only limited functionality  it illustrates the system-call mechanism and sheds light on the interaction between user programs and the kernel  create a new file called helloworld c to define your system call  include the header files linuxllinkage h and linuxlkernel h  add the following code to this file  # include linuxllinkage.h # include linuxlkernel.h asmlinkage int sysjhelloworld    printk  kern_emerg hello world !  ; return 1 ;  96 chapter 2 this creates a system call with the name sys_helloworld    if you choose to add this system call to an existing file in the source directory  all that is necessary is to add the sys_hellowor ld   function to the file you choose in the code  asmlinkage is a rellli ant from the days when linux used both c + + and c code and is used to indicate that the code is written in c the printk   function is used to print messages to a kernel log file and therefore may be called only from the kernel the kernel messages specified in the parameter to printk   are logged in the file /var/log/kernel/warnings the function prototype for the printk   call is defined in /usr /include/linux/kernel h  define a new system call number for __ nr_helloworld in /usr/src/linux-2.x/include/asm-i386/unistd.h a user program can use this number to identify the newly added system call also be sure to increment the value for __ nr_syscalls  which is stored in the same file this constant tracks the number of system calls currently defuced in the kernel  add an entry .long sys_helloworld to the sys_calltable definedinthe/usr/src/linux-2.x/arch/i386/kernel/entry.s file as discussed earlier  the system-call number is used to index into this table to find the position of the handler code for the invoked system call  add your file helloworld c to the makefile  if you created a new file for your system call  save a copy of your old kernel binary image  in case there are problems with your newly created kernel   you can now build the new kernet rename it to distinguish it from the unmodified kernet and add an entry to the loader configuration files  such as lilo conf   after completing these steps  you can boot either the old kernel or the new kernel that contains your system call  part 5  using the system call from a user program when you boot with the new kernet it will support the newly defined system call ; you now simply need to invoke this system call from a user program ordinarily  the standard c library supports an interface for system calls defined for the linux operating system as your new system call is not linked into the standard c library  however  invoking your system call will require manual intervention  as noted earlie1 ~ a system call is invoked by storing the appropriate value in a hardware register and performing a trap instruction unfortunately  these low-level operations can not be performed using c language statements and instead require assembly instructions fortunately  linux provides macros for instantiating wrapper functions that contain the appropriate assembly instructions for instance  the following c program uses the _syscallo   macro to invoke the newly defined system call  # include linux/errno.h # include sys/syscall.h # include linux/unistd.h _syscallo  int  helloworld  ; main    helloworld   ;  97 the _syscallo macro takes two arguments the first specifies the type of the value returned by the system call ; the second is the name of the system call the name is used to identify the systemcall number that is stored in the hardware register before the trap instruction is executed if your system call requires arguments  then a different macro  such as _syscallo  where the suffix indicates the number of arguments  could be used to instantiate the assembly code required for performing the system call  compile and execute the program with the newly built kernel  there should be a message hello world ! in the kernel log file /var/log/kernel/warnings to indicate that the system call has executed  as a next step  consider expanding the functionality of your system call  how would you pass an integer value or a character string to the system call and have it printed illto the kernel log file what are the implications of passing pointers to data stored in the user program 's address space as opposed to simply passing an integer value from the user program to the kernel using hardware registers dijkstra  1968  advocated the layered approach to operating-system desigll  brinch-hansen  1970  was an early proponent of constructing an operating system as a kernel  or nucleus  on which more complete systems can be built  system instrumentation and dynamic tracing are described in tamches and miller  1999   dtrace is discussed in cantrill et al  2004   the dtrace source code is available at http  i i src opensolaris org/ source/ cheung and loong  1995  explore issues of operating-system structure from microkernel to extensible systems  ms-dos  version 3.1  is described in microsoft  1986   windows nt and windows 2000 are described by solomon  1998  and solomon and russinovich  2000   windows 2003 and windows xp internals are described in russinovich and solomon  2005   hart  2005  covers windows system $ programming in detail bsd unix is described in mckusick et al  1996   bovet and cesati  2006  thoroughly discuss the linux kernel several unix systems-including mach-are treated in detail in vahalia  1996   mac os x is presented at 98 chapter 2 http  i lwww apple comlmacosx and in singh  2007   solaris is fully described in mcdougall and mauro  2007   the first operating system to provide a virtual machine was the cp i 67 on an ibm 360167 the commercially available ibm vmi370 operating system was derived from cp 167 details regarding mach  a microkernel-based operating system  can be found in young et al  1987   kaashoeket al  1997  present details regarding exokernel operating systems  wherein the architecture separates management issues from protection  thereby giving untrusted software the ability to exercise control over hardware and software resources  the specifications for the java language and the java virtual machine are presented by gosling et al  1996  and by lindholm and yellin  1999   respectively  the internal workings of the java virtual machine are fully described by ven11ers  1998   golm et al  2002  highlight the jx operating system ; back et al  2000  cover several issues in the design of java operating systems more information on java is available on the web at http  i lwww j a vas oft com  details about the implementation of vmware can be found in sugerman et al   2001   information about the open virh1al machine format can be found at http  llwww.vmware.comlappliancesllearnlovf.html  part two a process can be thought of as a program in execution a process will need certain resources-such as cpu time  memory  files  and 1/0 devices -to accomplish its task these resources are allocated to the process either when it is created or while it is executing  a process is the unit of work in most systems systems consist of a collection of processes  operating-system processes execute system code  and user processes execute user code all these processes may execute concurrently  although traditionally a process contained only a single thread of control as it ran  most modem operating systems now support processes that have multiple threads  the operating system is responsible for the following activities in connection with process and thread management  the creation and deletion of both user and system processes ; the scheduling of processes ; and the provision of mechanisms for synchronization  communication  and deadlock handling for processes  3.1 chapter early computer systems allowed only one program to be executed at a time this program had complete control of the system and had access to all the system 's resources in contrast  current-day computer systems allow multiple programs to be loaded into memory and executed concurrently  this evolution required firmer control and more compartmentalization of the various programs ; and these needs resulted in the notion of a process/ which is a program in execution a process is the unit of work in a modern time-sharing system  the more complex the operating system is  the more it is expected to do on behalf of its users although its main concern is the execution of user programs  it also needs to take care of various system tasks that are better left outside the kernel itself a system therefore consists of a collection of processes  operatingsystem processes executing system code and user processes executing user code potentially/ all these processes can execute concurrently/ with the cpu  or cpus  multiplexed among them by switching the cpu between processes  the operating system can make the computer more productive in this chapter/ you will read about what processes are and how they work  to introduce the notion of a process a program in execution  which forms the basis of all computation  to describe the various features of processes  including scheduling  creation and termination  and communication  to describe communication in client-server systems  a question that arises in discussing operating systems involves what to call all the cpu activities,_f \ _qcij  c  hj3ystem ~ xeq_l_ ~ _lqq.s_ ; .i ' \ 'b 'x  c9   _2l_ ! _i_ ! 11_e     _l  it ~ ds_ys ! ~  r  tl has user programs  or tas ~ ~  even on a single-user system such as microsoft 101 102 chapter 3 windows  a user may be able to run several programs at one time  a word processor  a web browse1 ~ and an e-mail package and even if the user can execute only one program at a time  the operating system may need to support its own internal programmed activities  such as memory management in many respects  all these activities are similar  so we call all of them processes  _the ten  ns ~ job i  ! dq pl ~ e ~ .s etif  '_lised almost interchangeably in this te   t  although we personally prefer the term process  much of operat1ng-system theory and terminology was developed during a time when the major activity of operating systems was job processing it would be misleading to avoid the use of commonly accepted terms that include the word job  such as job scheduling  simply because process has superseded job  3.1.1 the process informally  as mentioned earlier  a process is a program in execution a process is more than the program code  which is sometimes known as the text section  it also includes the current activity  as represented by the value of the program counter and the contents of the processor 's registers a process generally also includes the process stack  which contains temporary data  such as function parameters  return addresses  and local variables   and a data section  which contains global variables a process may also include a heap  which is memory thatis dynamically allocated during process run time the structure of a process in memory is shown in figure 3.1  we emphasize that a program by itself is not a process ; a program is a passive entity  such as a file containing a list of instructions stored on disk  often called an executable file   whereas a process is an active entity  with a program counter specifying the next instruction to execute and a set of associated resources a program becomes a process when an executable file is loaded into memory  two common techniques for loading executable files are double-clicking an icon representing the executable file and entering the name of the executable file on the command line  as in prog exe or a out  0 figure 3.1 process in memory  3.1 103 1/0 or event completion figure 3.2 diagram of process state  although two processes may be associated with the same program  they are nevertheless considered two separate execution sequences for instance  several users may be running different copies of the mail program  or the same user may invoke many copies of the web browser program each of these is a separate process ; and although the text sections are equivalent  the data  heap  and stack sections vary it is also common to have a process that spawns many processes as it runs we discuss such matters in section 3.4  3.1.2 process state as a proces   ; excr  utes  it changes state the state of a process is defil1.ed in part by the current activity of that process each process may be in one of the following states  new the process is being created  running instructions are being executed  waiting the process is waiting for some event to occur  such as an i/0 completion or reception of a signal   ready the process is waiting to be assigned to a processor  terminated the process has finished execution  these names are arbitrary  and they vary across operating systems the states that they represent are found on all systems  however certain operating systems also more finely delineate process states it is important to realize that only one process can be running on any processor at any instant many processes may be ready and waiting  however the state diagram corresponding to these states is presented in figure 3.2  3.1.3 process control block _  1cb pr   cess isrepreserlt ~ pjnthe operatir1,g system l  jy a process_ coptrol blo_ck _  pcb  -alsocalled a taskcontrozbloclc a pcb is shown in figure 3.3 it contains mi  my pieces of iil.format1o11assodated with a specific process  including these  104 chapter 3 figure 3.3 process control block  pcb   process state the state may be new  ready runnil g  waiting  halted  and so on  program counter the counter indicates the address of the next instruction to be executed for this process  cpu registers the registers vary in number and type  depending on the computer architecture they mclude accumulators  index registers  stack pointers  and general-purpose registers  plus any condition-code information along with the program counter  this state information must be saved when an mterrupt occurs  to allow the process to be continued correctly afterward  figure 3.4   cpu-scheduling information this information includes a process priority  pointers to scheduling queues  and any other scheduling parameters   chapter 5 describes process scheduling  memory-management information this information may include such information as the value of the base and limit registers  the page tables  or the segment tables  dependmg on the memory system used by the operating system  chapter 8   accounting information this mformation includes the amount of cpu and real time used  time limits  account numbers  job or process numbers  and so on  i/o status information this information includes the list of i/o devices allocated to the process  a list of open files  and so on  in briet the pcb simply serves as the repository for any information that may vary from process to process  3.1.4 threads the process model discussed so far has implied that a process is a program that performs a single thread of execution for example  when a process is running a word-processor program  a single thread of instructions is being executed this single thread of control allows the process to perform only one 3.2 process p0 idle 3.2 operating system interrupt or system call process p1 executing idle figure 3.4 diagram showing cpu switch from process to process  105 task at one time the user can not simultaneously type in characters and run the spell checker within the same process  for example many modern operatin.g systems have extended the process concept to allow a process to have multiple threads of execution and thus to perform more than one task at a time on a system that supports threads  the pcb is expanded to include information for each thread other changes throughout the system are also needed to support threads chapter 4 explores multithreaded processes in detail  the objective of multiprogramming is to have some process nnming at all times  to maximize cpu utilization the objective of time sharing is to switch the cpu among processes so frequently that users can interact with each program while it is run.ning to meet these objectives  the process scheduler selects an available process  possibly from a set of several available processes  for program execution on the cpu for a single-processor system  there will never be more than one running process if there are more processes  the rest will have to wait until the cpu is free and can be rescheduled  3.2.1 scheduling queues as processes enter the system  they are put into a job queue  which consists of all processes in the system the processes that are residing in main memory and are ready and waiting to execute are kept on a list called the ready queue  106 chapter 3 process representation in linux the process control block in the linux operating system is represented by the c struch1re task_struct this structure contains all the necessary information for representing a process  including the state of the process  scheduling and memory-management information  list of open files  and pointers to the process 's parent and any of its children  a process 's parent is the process that created it ; its children are any processes that it creates  some of these fields include  pid_t pid ; i process identifier i long state ; i state of the process i unsigned int time_slice i scheduling information i struct task_struct parent ; i this process 's parent i struct list__head children ; i this process 's children i struct files_struct files ; i list of open files i struct mm_struct mm ; i address space of this process i for example  the state of a process isrepresented by the field long state in this structure within the linux kernel  all active processes are represented using a doubly linked list of task_struct  and the kernel maintains a pointer -current -to the process currently executing on the system this is shown in figure 3.5  struct task_struct process information struct task_struct process information t current  currently executing proccess  figure 3.5 active processes in linux  struct task_struct process information as an illustration of how the kernel might manipulate one of the fields in the task_struct for a specified process  let 's assume the system would like to change the state of the process currently running to the value new_state  if currentis a pointer to the process currently executing  its state is changed with the following  current state = new_state ; this queue is generally stored as a linked list a ready-queue header contains pointers to the first and final pcbs in the list each pcb includes a pointer field that points to the next pcb in the ready queue  3.2 107 queue header mag tape ~ = 7  c 77 ~ unit 0 k  \ \ t_82_11 ~ ~ il = = figure 3.6 the ready queue and various 1/0 device queues  the system also includes other queues when a process is allocated the cpu  it executes for a while and eventually quits  is interrupted  or waits for the occurrence of a particular event  such as the completion of an i/0 request  suppose the process makes an i/o request to a shared device  such as a disk  since there are many processes in the system  the disk may be busy with the i/0 request of some other process the process therefore may have to wait for the disk the list of processes waiting for a particular i/0 device is called a device queue each device has its own device queue  figure 3.6   a common representation of process scheduling is a queueing diagram  such as that in figure 3.7 each rectangular box represents a queue two types of queues are present  the ready queue and a set of device queues the circles represent the resources that serve the queues  and the arrows indicate the flow of processes in the system  a new process is initially put in the ready queue it waits there until it is selected for execution  or is dispatched once the process is allocated the cpu and is executing  one of several events could occur  the process could issue an i/0 request and then be placed in an i/0 queue  the process could create a new subprocess and wait for the subprocess 's termination  the process could be removed forcibly from the cpu  as a result of an interrupt  and be put back in the ready queue  108 chapter 3 figure 3.7 queueing-diagram representation of process scheduling  in the first two cases  the process eventually switches from the waiting state to the ready state and is then put back in the ready queue a process continues this cycle until it terminates  at which time it is removed from all queues and has its pcb and resources deallocated  3.2.2 schedulers a process migrates among the various scheduling queues throughout its lifetime the operating system must select  for scheduling purposes  processes from these queues in some fashion the selection process is carried out by the appropriate scheduler  often  in a batch system  more processes are submitted than can be executed immediately these processes are spooled to a mass-storage device  typically a disk   where they are kept for later execution the long-term scheduler  or job scheduler  selects processes from this pool and loads them into memory for execution the short-term scheduler  or cpu scheduler  selects from among the processes that are ready to execute and allocates the cpu to one of them  the primary distinction between these two schedulers lies in frequency of execution the short-term scheduler must select a new process for the cpu frequently a process may execute for only a few milliseconds before waiting for an i/0 request often  the short-term scheduler executes at least once every 100 milliseconds because of the short time between executions  the short-term scheduler must be fast if it takes 10 milliseconds to decide to execute a process for 100 milliseconds  then 10 i  100 + 10  = 9 percent of the cpu is being used  wasted  simply for scheduling the work  the long-term scheduler executes much less freqvently ; minutes may separate the creation of one new process and the next the long-term scheduler controls the degree of multiprogramming  the number of processes in memory   if the degree of multiprogramming is stable  then the average rate of process creation must be equal to the average departure rate of processes leaving the system thus  the long-term scheduler may need to be invoked 3.2 109 only when a process leaves the system because of the longer interval between executions  the long-term scheduler can afford to take more tin e to decide which process should be selected for execution  it is important that the long-term scheduler make a careful selection in general  most processes can be described as either i/ 0 bound or cpu bound an i/o-bound process is one that spends more of its time doing i/o than it spends doing computations a cpu-bound process  in contrast  generates i/0 requests infrequently  using more of its time doing computations it is important that the long-term scheduler select a good process mix of i/o-bound and cpu-bound processes if all processes are i/0 bound  the ready queue will almost always be empty  and the short-term scheduler will have little to do if all processes are cpu bound  the i/0 waiting queue will almost always be empty  devices will go unused  and again the system will be unbalanced the system with the best performance will thus have a combination of cpu-bound and i/o-bound processes  on some systems  the long-term scheduler may be absent or minimal  for example  time-sharing systems such as unix and microsoft windows systems often have no long-term scheduler but simply put every new process in memory for the short-term scheduler the stability of these systems depends either on a physical limitation  such as the number of available terminals  or on the self-adjusting nature of human users if performance declines to m acceptable levels on a multiuser system  some users will simply quit  some operating systems  such as time-sharing systems  may introduce an additional  intermediate level of scheduling this medium-term scheduler is diagrammed in figure 3.8 the key idea behind a medium-term scheduler is that sometimes it can be advantageous to remove processes from memory  and from active contention for the cpu  and thus reduce the degree of multiprogramrning later  the process can be reintroduced into memory  and its execution can be continued where it left off this scheme is called swapping the process is swapped out  and is later swapped in  by the medium-term scheduler swapping may be necessary to improve the process mix or because a change in memory requirements has overcommitted available memory  requiring memory to be freed up swapping is discussed in chapter 8  swap in      partiaii  ' exec ~ t ~ d sw11pped-out processes  swap out figure 3.8 addition of medium-term scheduling to the queueing diagram  110 chapter 3 3.3 3.2.3 context switch as mentioned in section 1.2.1  interrupts cause the operating system to change a cpu from its current task and to run a kernel routine such operations happen frequently on general-purpose systems when an interrupt occurs  the system needs to save the current of the process running on the cpu so that it can restore that context when its processing is done  essentially suspending the process and then resuming it the context is represented in the pcb of the process ; it includes the value of the cpu registers  the process state  see figure 3.2   and memory-management information generically  we perform a of the current state of the cpu  be it in kernel or user mode  and then a to resu.me operations  switching the cpu to another process requires performing a state save of the current process and a state restore of a different process this task is known as a when a context switch occurs  the kernel saves the context of the old process in its pcb and loads the saved context of the new process scheduled to run context-switch time is pure overhead  because the system does no useful work while switching its speed varies from machine to machine  depending on the memory speed  the number of registers that must be copied  and the existence of special instructions  such as a single instruction to load or store all registers   typical speeds are a few milliseconds  context-switch times are highly dependent on hardware support for instance  some processors  such as the sun ultrasparc  provide multiple sets of registers a context switch here simply requires changing the pointer to the current register set of course  if there are more active processes than there are register sets  the system resorts to copying register data to and from memory  as before also  the more complex the operating system  the more work must be done during a context switch as we will see in chapter 8  advanced memory-management techniques may require extra data to be switched with each context for instance  the address space of the current process must be preserved as the space of the next task is prepared for use how the address space is preserved  and what amount of work is needed to preserve it  depend on the memory-management method of the operating system  the processes in most systems can execute concurrently  and they may be created and deleted dynamically thus  these systems must provide a mechanism for process creation and termination in this section  we explore the n1.echanisms involved in creating processes and illustrate process creation on unix and windows systems  3.3.1 process creation a process may create several new processes  via a create-process system call  during the course of execution the creating process is called a parent process  and the new processes are called the children of that process each of these new processes may in turn create other processes  forming a tree of processes  most operating systems  including unix and the windows family of operating systems  identify processes according to a unique process identifier 3.3 111  or pid   which is typically an integer number figure 3.9 illustrates a typical process tree for the solaris operating system  showing the name of each process and its pid in solaris  the process at the top of the tree is the sched process  with pid of 0 the sched process creates several children processes-including pageout and fsflush these processes are responsible for managing memory and file systems the sched process also creates the ini t process  which serves as the root parent process for all user processes in figure 3.9  we see two children of ini t-inetd and dtlogin inetd is responsible for networking services such as telnet and ftp ; dtlogin is the process representing a user login screen when a user logs in  dtlogin creates an x-windows session  xsession   which in turns creates the sdt_shel process below sdlshel  a user 's command-line shell-the c-shell or csh-is created in this commandline interface  the user can then invoke various child processes  such as the ls and cat commands we also see a csh process with pid of 7778 representing a user who has logged onto the system using telnet this user has started the netscape browser  pid of 7785  and the emacs editor  pid of 8105   on unix  we can obtain a listing of processes by using the ps command for example  the command ps -el will list complete information for all processes currently active in the system it is easy to construct a process tree similar to what is shown in figure 3.9 by recursively tracing parent processes all the way to the ini t process  in general  a process will need certain resources  cpu time  memory  files  i/0 devices  to accomplish its task when a process creates a subprocess  that inetd pid = 140 dtlogin pid = 251 figure 3.9 a tree of processes on a typical solaris system  112 chapter 3 subprocess may be able to obtain its resources directly from the operating system  or it may be constrained to a subset of the resources of the parent process the parent may have to partition its resources among its children  or it may be able to share some resources  such as ncemory or files  among several of its children restricting a child process to a subset of the parent 's resources prevents any process from overloading the system by creating too many subprocesses  in addition to the various physical and logical resources that a process obtains when it is created  initialization data  input  may be passed along by the parent process to the child process for example  consider a process whose function is to display the contents of a file-say  img.jpg-on the screen of a terminal when it is created  it will get  as an input from its parent process  the name of the file img.jpg  and it will use that file name  open the file  and write the contents out it may also get the name of the output device some operating systems pass resources to child processes on such a system  the new process may get two open files  img.jpg and the terminal device  and may simply transfer the datum between the two  when a process creates a new process  two possibilities exist in terms of execution  the parent continues to execute concurrently with its children  the parent waits until some or all of its children have terminated  there are also two possibilities in terms of the address space of the new process  the child process is a duplicate of the parent process  it has the same program and data as the parent   the child process has a new program loaded into it  to illustrate these differences  let 's first consider the unix operating system  in unix  as we 've seen  each process is identified by its process identifier  which is a tmique integer a new process is created by the fork   system call the new process consists of a copy of the address space of the original process this mechanism allows the parent process to communicate easily with its child process both processes  the parent and the child  continue execution at the instruction after the fork    with one difference  the return code for the fork   is zero for the new  child  process  whereas the  nonzero  process identifier of the child is returned to the parent  typically  the exec   system call is used after a fork   system call by one of the two processes to replace the process 's memory space with a new program the exec   system call loads a binary file into memory  destroying the memory image of the program containing the exec   system call  and starts its execution in this manner  the two processes are able to communicate and then go their separate ways the parent can then create more children ; or  if it has nothing else to do while the child runs  it can issue await   system call to move itself off the ready queue until the termination of the child  the c program shown in figure 3.10 illustrates the unix system calls previously described we now have two different processes running copies of the same program the only difference is that the value of pid  the process 3.3 # include sysltypes.h # include stdio.h # include unistd.h int main    pid_t pid ;  i fork a child process i pid = fork   ; if  pid 0   i error occurred i fprintf  stderr  fork failed  ; return 1 ;  else if  pid = = 0   i child process i execlp  lbinlls  ls ,null  ;  else  i parent process i  i parent will wait for the child to complete i wait  null  ; printf  child complete  ; return 0 ; figure 3.10 creating a separate process using the unix fork   system call  113 identifier  for the child process is zero  while that for the parent is an integer value greater than zero  in fact  it is the actual pid of the child process   the child process inherits privileges and scheduling attributes from the parent  as well certain resources  such as open files the child process then overlays its address space with the unix command lbin/ls  used to get a directory listing  using the execlp   system call  execlp   is a version of the exec   system call   the parent waits for the child process to complete with the wait   system call when the child process completes  by either implicitly or explicitly invoking exit    the parent process resumes from the call to wait   ,where it completes using the exit   system call this is also illustrated in figure 3.11  parent wait resumes child ~  +  exit   figure 3.11 process creation using fork   system call  114 chapter 3 # include stdio.h # include windows.h int main  void   startupinfo si ; process_information pi ;  ii allocate memory zeromemory  &si  sizeof  si   ; si.cb = sizeof  si  ; zeromemory  &pi  sizeof  pi   ; ii create child process if  ! createprocess  null  ii use command line c  \ \ windows \ \ system32 \ \ mspaint.exe  ii command line null  ii do n't inherit process handle   null  ii do n't inherit thread handle false  ii disable handle inheritance 0  ii no creation flags null  ii use parent 's environment block null  ii use parent 's existing directory &si  &pi   fprintf  stderr  create process failed  ; return -1 ; ii parent will wait for the child to complete waitforsingleobject  pi.hprocess  infinite  ; printf  child complete  ; ii close handles closehandle  pi.hprocess  ; closehandle  pi.hthread  ; figure 3.12 creating a separate process using the win32 api  as an alternative examplef we next consider process creation in windows  processes are created in the win32 api using the createprocess   functionf which is similar to fork   in that a parent creates a new child process howeverf whereas fork   has the child process inheriting the address space of its parent createprocess   requires loading a specified program into the address space of the child process at process creation furthermoref whereas fork   is passed no parametersf createprocess   expects no fewer than ten parameters  the c program shown in figure 3.12 illustrates the createprocess   functionf which creates a child process that loads the application mspaint ex e  we opt for many of the default values of the ten parameters passed to createprocess    readers interested in pursuing the details of process 3.3 115 creation and management in the win32 api are encouraged to consult the bibliographical notes at the end of this chapter  two parameters passed to createprocess   are instances of the startupinfo and process_information structures startupinfo specifies many properties of the new process  such as window size and appearance and handles to standard input and output files the process_information structure contains a handle and the identifiers to the newly created process and its thread  we invoke the zeromemory   function to allocate memory for each of these structures before proceeding with createprocess    the first two parameters passed to createprocess   are the application name and command-line parameters if the application name is null  as it is in this case   the command-line parameter specifies the application to load in this instance  we are loading the microsoft windows mspaint.exe application beyond these two initial parameters  we use the default parameters for inheriting process and thread handles as well as specifying no creation flags  we also use the parent 's existing environment block and starting directory  last  we provide two pointers to the startupinfo and process.lnformation structures created at the beginning of the program in figure 3.10  the parent process waits for the child to complete by invoking the wait   system call the equivalent of this in win32 is wai tforsingleobj ect    which is passed a handle of the child process-pi hprocess-and waits for this process to complete once the child process exits  control returns from the wai tforsingleobj ect   function in the parent process  3.3.2 process termination a process terminates when it finishes executing its final statement and asks the operating system to delete it by using the exit   system call at that point  the process may return a status value  typically an integer  to its parent process  via the wait   system call   all the resources of the process-including physical and virtual memory  open files  and i/0 buffers-are deallocated by the operating system  termination can occur in other circumstances as well a process can cause the termination of another process via an appropriate system call  for example  terminateprocess   in win32   usually  such a system call can be invoked only by the parent of the process that is to be terminated otherwise  users could arbitrarily kill each other 's jobs note that a parent needs to know the identities of its children thus  when one process creates a new process  the identity of the newly created process is passed to the parent  a parent may terminate the execution of one of its children for a variety of reasons  such as these  the child has exceeded its usage of some of the resources that it has been allocated  to determine whether this has occurred  the parent m.ust have a mechanism to inspect the state of its children  the task assigned to the child is no longer required  the parent is exiting  and the operating system does not allow a child to continue if its parent terminates  116 chapter 3 3.4 some systencs  including vms  do not allow a child to exist if its parent has terminated in such systems  if a process terminates  either normally or abnormally   then all its children must also be terminated this phenomenon  referred to as cascading termination  is normally initiated by the operating system  to illustrate process execution and termination  consider that  in unix  we can terminate a process by using the exit   system call ; its parent process may wait for the termination of a child process by using the wait   system call the wait   system call returns the process identifier of a terminated child so that the parent can tell which of its children has terminated if the parent terminates  however  all its children have assigned as their new parent the ini t process thus  the children still have a parent to collect their status and execution statistics  processes executing concurrently in the operating system may be either independent processes or cooperating processes a process is independent if it can not affect or be affected by the other processes executing in the system  any process that does not share data with any other process is independent a process is cooperating if it can affect or be affected by the other processes executing in the system clearly  any process that shares data with other processes is a cooperating process  there are several reasons for providing an environment that allows process cooperation  information sharing since several users may be interested in the same piece of information  for instance  a shared file   we must provide an environment to allow concurrent access to such information  computation speedup if we want a particular task to run faster  we must break it into subtasks  each of which will be executing in parallel with the others notice that such a speedup can be achieved only if the computer has multiple processing elements  such as cpus or i/o channels   modularity we may want to construct the system in a modular fashion  dividing the system functions into separate processes or threads  as we discussed in chapter 2  convenience even an individual user may work on many tasks at the same time for instance  a user may be editing  printing  and compiling in parallel  cooperating processes require an interprocess communication  ipc  mechanism that will allow them to exchange data and information there are two fundamental models of interprocess communication   1  shared memory and  2  message passing in the shared-memory model  a region of memory that is shared by cooperating processes is established processes can then exchange information by reading and writing data to the shared region in the messagepassing model  communication takes place by means of messages exchanged 3.4 117 process a process a 2 2 kernel  a   b  figure 3.13 communications models  a  message passing  b  shared memory  between the cooperating processes the two communications models are conh asted in figure 3.13  both of the models just discussed are common in operating systems  and many systems implement both message passing is useful for exchanging smaller ammmts of data  because no conflicts need be avoided message passing is also easier to implement than is shared memory for intercomputer communication shared memory allows maximum speed and convenience of communication shared memory is faster than message passing  as messagepassing system.s are typically implemented using system calls and thus require the more time-consuming task of kernel irttervention in contrast  in sharedmemory systems  system calls are required only to establish shared-memory regions once shared memory is established  all accesses are treated as routine memory accesses  and no assistance from the kernel is required in the ren1.ainder of this section  we explore each of these ipc models in more detail  3.4.1 shared-memory systems interprocess communication using shared memory requires communicating processes to establish a region of shared memory typically  a shared-memory region resides in the address space of the process creating the sharedmemory segment other processes that wish to communicate using this sharedmemory segment must attach it to their address space recall that  normally  the operating system tries to prevent one process from accessing another process 's memory shared memory requires that two or more processes agree to remove this restriction they can then excbange information by reading and writing data in the shared areas the form of the data and the location are determined by these processes and are not under the operating system 's control the processes are also responsible for ensuring that they are not writing to the same location simultaneously  118 chapter 3 to illustrate the concept of cooperating processes  let 's consider the producer-consumer problem  which is a common paradigm for cooperating processes a producer process produces information that is consumed by a consumer process for example  a compiler may produce assembly code  which is consumed by an assembler the assembler  in turn  ncay produce object modules  which are consumed by the loader the producer-consumer problem also provides a useful metaphor for the client-server paradigm we generally think of a server as a producer and a client as a consumer for example  a web server produces  that is  provides  html files and images  which are consumed  that is  read  by the client web browser requesting the resource  one solution to the producer-consumer problem uses shared memory to allow producer and consumer processes to run concurrently  we must have available a buffer of items that can be filled by the producer and emptied by the consumer this buffer will reside in a region of memory that is shared by the producer and consumer processes a producer can produce one item while the consumer is consuming another item the producer and consumer must be synchronized  so that the consumer does not try to consume an item that has not yet been produced  two types of buffers can be used the places no practical limit on the size of the buffer the consumer may have to wait for new items  but the producer can always produce new items the assumes a fixed buffer size in this case  the consumer must wait if the buffer is empty  and the producer must wait if the buffer is full  let 's look more closely at how the bounded buffer can be used to enable processes to share memory the following variables reside in a region of memory shared by the producer and consumer processes  # define buffer_size 10 typedef struct  item ; item buffer  buffer_size  ; int in = 0 ; int out = 0 ; the shared buffer is implemented as a circular array with two logical pointers  in and out the variable in points to the next free position in the buffer ; out points to the first full position in the buffer the buffer is empty when in = = out ; the buffer is full when   in + 1  % buffer_size  = = out  the code for the producer and consumer processes is shown in figures 3.14 and 3.15  respectively the producer process has a local variable nextproduced in which the new item to be produced is stored the consumer process has a local variable next consumed in which the item to be consumed is stored  this scheme allows at most buffer_size  1 items in the buffer at the same time we leave it as an exercise for you to provide a solution where buffer_size items can be in the buffer at the same time in section 3.5.1  we illustrate the posix api for shared memory  3.4 item nextproduced ; while  true    i produce an item in nextproduced i while    in + 1  % buffer_size  = = out  ; i do nothing i buffer  in  = nextproduced ; in =  in + 1  % buffer_size ; figure 3.'14 the producer process  119 one issue this illustration does not address concerns the situation in which both the producer process and the consumer process attempt to access the shared buffer concurrently in chapter 6  we discuss how synchronization among cooperating processes can be implemented effectively in a sharedmemory environment  3.4.2 message-passing systems lrt section 3.4.1  we showed how cooperating processes can communicate in a shared-memory environment the scheme requires that these processes share a region of memory and that the code for accessing and manipulating the shared memory be written explicitly by the application programmer another way to achieve the same effect is for the operating system to provide the means for cooperating processes to comm lmicate with each other via a message-passing facility  message passing provides a mechanism to allow processes to communicate and to synchronize their actions without sharing the same address space and is particularly useful in a distributed environment  where the communicating processes may reside on different computers connected by a network for example  a chat program used on the world wide web could be designed so that chat participants communicate with one another by exchanging messages  a message-passing facility provides at least two operations  send  message  and recei ve  message   messages sent by a process can be of either fixed or variable size if only fixed-sized messages can be sent  the system-level implementation is straightforward this restriction  however  makes the task item nextconsumed ; while  true    while  in = = out  ; ii do nothing nextconsumed = buffer  out  ; out =  out + 1  % buffer_size ; i consume the item in nextconsumed i figure 3.15 the consumer process  120 chapter 3 of programming more difficult conversely  variable-sized messages require a 1nore complex system-level implementation  but the programming task becomes simpler this is a coitlmon kind of tradeoff seen throughout operatingsystem design  if processes p and q want to communicate  they must send messages to and receive messages from each other ; a communication link must exist between them this link can be implemented in a variety of ways we are concerned here not with the link 's physical implementation  such as shared memory  hardware bus  or network  which are covered in chapter 16  but rather with its logical implementation here are several methods for logically implementing a link and the send 0 i receive   operations  direct or indirect communication synchronous or asynchronous communication automatic or explicit buffering we look at issues related to each of these features next  3.4.2.1 naming processes that want to communicate must have a way to refer to each other  they can use either direct or indirect communication  under direct communication  each process that wants to comm lmicate must explicitly name the recipient or sender of the communication in this scheme  the send   and receive   primitives are defined as  send  p  message  -send a message to process p  receive  q  message  -receive a message from process q  a communication link in this scheme has the following properties  a link is established automatically between every pair of processes that want to communicate the processes need to know only each other 's identity to communicate  a link is associated with exactly two processes  between each pair of processes  there exists exactly one link  this scheme exhibits symmetry in addressing ; that is  both the sender process and the receiver process must name the other to communicate a variant of this scheme employs asymmetry in addressing here  only the sender names the recipient ; the recipient is not required to name the sender in this scheme  the send   and receive   primitives are defined as follows  send  p  message  -send a message to process p  receive  id  message  -receive a message from any process ; the variable id is set to the name of the process with which communication has taken place  the disadvantage in both of these schemes  symmetric and asymmetric  is the limited modularity of the resulting process definitions changing the identifier of a process may necessitate examining all other process definitions  all references to the old identifier must be found  so that they can be modified 3.4 121 to the new identifier in general  any such hard-coding techniques  where identifiers must be explicitly stated  are less desirable than techniques involving indirection  as described next  with indirect communication  the messages are sent to and received from mailboxes  or ports a mailbox can be viewed abstractly as an object into which messages can be placed by processes and from which messages can be removed  each mailbox has a w1.ique identification for example  posix message queues use an integer value to identify a mailbox in this scheme  a process can communicate with some other process via a number of different mailboxes  two processes can communicate only if the processes have a shared mailbox  however the send   and receive 0 primitives are defined as follows  send  a  message  -send a message to mailbox a  receive  a  message  -receive a message from mailbox a  in this scheme  a communication link has the following properties  a link is established between a pair of processes only if both members of the pair have a shared mailbox  a link may be associated with more than two processes  between each pair of communicating processes  there may be a number of different links  with each link corresponding to one mailbox  now suppose that processes p1  p2  and p3 all share mailbox a process p1 sends a message to a  while both p2 and p3 execute a receive 0 from a  which process will receive the message sent by p1 the answer depends on which of the following methods we choose  allow a link to be associated with two processes at most  allow at most one process at a time to execute a receive 0 operation  allow the system to select arbitrarily which process will receive the message  that is  either p2 or p3  but not both  will receive the message   the system also may define an algorithm for selecting which process will receive the message  that is  round robin  where processes take turns receiving messages   the system may identify the receiver to the sender  a mailbox may be owned eith ~ r by a process or by the operating system  if the mailbox is owned by a process  that is  the mailbox is part of the address space of the process   then we distinguish between the owner  which can only receive messages through this mailbox  and the user  which can only send messages to the mailbox   since each mailbox has a unique owner  there can be no confusion about which process should receive a message sent to this mailbox when a process that owns a mailbox terminates  the mailbox disappears any process that subsequently sends a message to this mailbox must be notified that the mailbox no longer exists  in contrast  a mailbox that is owned by the operating system has an existence of its own it is independent and is not attached to any particular process the operating system then must provide a mechanism that allows a process to do the following  122 chapter 3 create a new mailbox  send and receive messages through the mailbox  delete a mailbox  the process that creates a new mailbox is that mailbox 's owner by default  initially  the owner is the only process that can receive messages through this n  tailbox however  the ownership and receiving privilege may be passed to other processes through appropriate system calls of course  this provision could result in multiple receivers for each mailbox  3.4.2.2 synchronization communication between processes takes place through calls to send   and receive   primitives there are different design options for implementing each primitive message passing may be either blocking or nonblockingalso known as synchronous and asynchronous  blocking send the sending process is blocked until the message is received by the receiving process or by the mailbox  nonblocking send the sending process sends the message and resumes operation  blocking receive the receiver blocks until a message is available  nonblocking receive the receiver retrieves either a valid message or a null  different combinations of send   and receive   are possible when both send   and receive   are blocking  we have a rendezvous between the sender and the receiver the solution to the producer-consumer problem becomes trivial when we use blocking send   and receive   statements  the producer merely invokes the blocking send   call and waits until the message is delivered to either the receiver or the mailbox likewise  when the consumer invokes receive    it blocks until a message is available  note that the concepts of synchronous and asynchronous occur frequently in operating-system i/0 algorithms  as you will see throughout this text  3.4.2.3 buffering whether communication is direct or indirect  messages exchanged by communicating processes reside in a temporary queue basically  such queues can be implemented in three ways  zero capacity the queue has a maximum length of zero ; thus  the link can not have any messages waiting in it in this case  the sender must block until the recipient receives the message  bounded capacity the que ~ ue has finite length n ; thus  at most n messages can reside in it if the queue is not full when a new message is sent  the message is placed in the queue  either the message is copied or a pointer to the message is kept   and the sender can continue execution without 3.5 3.5 123 waiting the link 's capacity is finite  however if the link is full  the sender must block until space is available in the quelie  unbounded capacity the queue 's length is potentially infinite ; thus  any number of messages can wait in it the sender never blocks  the zero-capacity case is sometimes referred to as a message system with no buffering ; the other cases are referred to as systems with automatic buffering  in this section  we explore three different ipc systems we first cover the posix api for shared memory and then discuss message passing in the mach operating system we conclude with windows xp  which interestingly uses shared memory as a mechanism for providing certain types of message passing  3.5.1 an example  posix shared memory several ipc mechanisms are available for posix systems  including shared memory and message passing here  we explore the posix api for shared memory  a process must first create a shared memory segment using the shmget   system call  shmget   is derived from shared memory get   the following example illustrates the use of shmget    segment_id = shmget  ipcprivate  size  s_lrusr i s_lwusr  ; this first parameter specifies the key  or identifier  of the shared-memory segment if this is set to ipcprivate  a new shared-memory segment is created  the second parameter specifies the size  in bytes  of the shared-memory segment finally  the third parameter identifies the mode  which indicates how the shared-memory segment is to be used-that is  for reading  writing  or both by setting the mode to s_lrusr 1 s_lwusr  we are indicating that the owner may read or write to the shared-memory segment a successful call to shmget   returns an integer identifier for the shared-memory segment other processes that want to use this region of shared memory must specify this identifier  processes that wish to access a shared-memory segment must attach it to their address space using the shmat    shared memory attach  system call  the call to shmat   expects three parameters as well the first is the integer identifier of the shared-memory segment being attached  and the second is a pointer location in memory indicating where the shared ncemory will be attached if we pass a value of null  the operating system selects the location on the user 's behalf the third parameter identifies a flag that allows the sharedmemory region to be attached in read-only or read-write mode ; by passing a parameter of 0  we allow both reads and writes to the shared region we attach a region of shared memory using shmat   as follows  shared_memory =  char  shmat  id  null  0  ; if successful  shmat   returns a pointer to the beginning location in memory where the shared-memory region has been attached  124 chapter 3 once the region of shared ncemory is attached to a process 's address space  the process can access the shared memory as a routine memory access using the pointer returned from shmat    in this example  shmat   returns a pointer to a character string thus  we could write to the shared-memory region as follows  sprintf  shared_memory  writing to shared memory  ; other processes sharing this segment would see the updates to the sharedmemory segment  typically  a process using an existing shared-memory segment first attaches the shared-memory region to its address space and then accesses  and possibly updates  the region of shared memory when a process no longer requires access to the shared-memory segment it detaches the segment from its address space to detach a region of shared memory  the process can pass the pointer of the shared-memory region to the shmdt   system call  as follows  shmdt  shared_memory  ; finally  a shared-memory segment can be removed from the system with the shmctl   system call  which is passed the identifier of the shared segrnent along with the flag ipcrmid  the program shown in figure 3.16 illustrates the posix shared-memory api just discussed this program creates a 4,096-byte shared-memory segment  once the region of shared memory is attached  the process writes the message hi there ! to shared memory after outputting the contents of the updated memory  it detaches and removes the shared-memory region we provide further exercises using the posix shared-memory api in the programming exercises at the end of this chapter  3.5.2 an example  mach as an example of a message-based operating system  we next consider the mach operating system  developed at carnegie mellon university we introduced mach in chapter 2 as part of the mac os x operating system the mach kernel supports the creation and destruction of multiple tasks  which are similar to processes but have multiple threads of control most communication in mach-including most of the system calls and all intertask informationis carried out by messages messages are sent to and received from mailboxes  called ports in mach  even system calls are made by messages when a task is created  two special n  tailboxes-the kernel mailbox and the notify mailbox-are also created the kernel mailbox is used by the kernel to communicate with the task the kernel sends notification of event occurrences to the notify port only three system calls are needed for message transfer the msg_send   call sends a message to a mailbox a message is received via msg_recei ve    remote procedure calls  rpcs  are executed via msg_rpc   ,which sends a message and waits for exactly one return message from the sender in this way  the rpc models a typical subroutine procedure call but can work between systems-hence the term remote  the porlallocate   system call creates a new mailbox and allocates space for its queue of messages the maximum size of the message queue # include stdio.h # include syslshm.h # include syslstat.h int main    3.5 i the identifier for the shared memory segment i int segmenlid ; i a pointer to the shared memory segment i char shared_memory ; i the size  in bytes  of the shared memory segment i canst int size = 4096 ; i allocate a shared memory segment i 125 segment_id = shmget  ipc_private  size  s_irusr i s_iwusr  ;  i attach the shared memory segment i shared_memory =  char  shmat  segment_id  null  0  ; i write a message to the shared memory segment i sprint  shared_memory  hi there !  ; i now print out the string from shared memory i printf  % s \ n  shared_memory  ; i now detach the shared memory segment i shmdt  shared_memory  ; i now remove the shared memory segment i shmctl  segment_id  ipc_rmid  null  ; return 0 ; figure 3.16 c program illustrating posix shared-memory api  defaults to eight messages the task that creates the mailbox is that mailbox 's owner the owner is also allowed to receive from the mailbox only one task at a time can either own or receive from a mailbox  but these rights can be sent to other tasks if desired  the mailbox 's message queue is initially empty as messages are sent to the mailbox  the messages are copied into the mailbox all messages have the same priority mach guarantees that multiple messages from the same sender are queued in first-in  first-out  fifo  order but does not guarantee an absolute ordering for instance  messages from two senders may be queued in any order  the messages themselves consist of a fixed-length header followed by a variable-length data portion the header indicates the length of the message and  indudes two mailbox names one mailbox name is the mailbox to which the message is being sent commonly  the sending thread expects a reply ; so 126 chapter 3 the mailbox name of the sender is passed on to the receiving task  which can use it as a return address  the variable part of a message is a list of typed data items each entry in the list has a type  size  and value the type of the objects specified in the message is important  since objects defined by the operating system-such as ownership or receive access rights  task states  and memory segments-n ay be sent in messages  the send and receive operations themselves are flexible for instance  when a message is sent to a mailbox  the mailbox may be full if the mailbox is not full  the message is copied to the mailbox  and the sending thread continues if the mailbox is full  the sending thread has four options  wait indefinitely until there is room in the mailbox  wait at most n milliseconds  do not wait at all but rather return immediately  temporarily cache a message one message can be given to the operating system to keep  even though the mailbox to which that message is being sent is full when the message can be put in the mailbox  a message is sent back to the sender ; only one such message to a full mailbox can be pending at any time for a given sending thread  the final option is meant for server tasks  such as a line-printer driver after finishing a request  such tasks may need to send a one-time reply to the task that had requested service ; but they must also continue with other service requests  even if the reply mailbox for a client is full  the receive operation must specify the mailbox or mailbox set from which a message is to be received a mailbox set is a collection of mailboxes  as declared by the task  which can be grouped together and treated as one mailbox for the purposes of the task threads in a task can receive only from a mailbox or mailbox set for which the task has receive access a porlstatus   system call returns the number of messages in a given mailbox the receive operation attempts to receive from  1  any mailbox in a mailbox set or  2  a specific  named  mailbox if no message is waiting to be received  the receiving thread can either wait at most n milliseconds or not wait at all  the mach system was especially designed for distributed systems  which we discuss in chapters 16 through 18  but mach is also suitable for singleprocessor systems  as evidenced by its inclusion in the mac os x system the major problem with message systems has generally been poor performance caused by double copying of messages ; the message is copied first from the sender to the mailbox and then from the mailbox to the receiver the mach message system attempts to avoid double-copy operations by using virtual-memory-management techniques  chapter 9   essentially  mach maps the address space containing the sender 's message into the receiver 's address space the message itself is never actually copied this message-management technique provides a large performance boost but works for only intrasystem messages the mach operating system is discussed in an extra chapter posted on our website  3.5 127 3.5.3 an example  windows xp the windows xp operating system is an example of modern design that employs modularity to increase functionality and decrease the time needed to implement new features windows xp provides support for multiple operating environments/ or subsystems/ with which application programs communicate via a n1.essage-passing mechanism the application programs can be considered clients of the windows xp subsystem server  the message-passing facility in windows xp is called the facility the lpc in windows xp communicates between two processes on the same machine it is similar to the standard rpc mechanism that is widely used/ but it is optimized for and specific to windows xp like mach/ windows xp uses a port object to establish and maintain a connection between two processes every client that calls a subsystem needs a communication channet which is provided by a port object and is never inherited windows xp uses two types of ports  connection ports and communication ports they are really the same but are given different names according to how they are used  cmmection ports are named objects and are visible to all processes ; they give applications a way to set up communication channels  chapter 22   the communication works as follows  the client opens a handle to the subsystem/ s connection port object  the client sends a cmmection request  the server creates two private conimunication ports and returns the handle to one of them to the client  the client and server use the corresponding port handle to send messages or callbacks and to listen for replies  windows xp uses two types of rnessage-passing techniques over a port that the client specifies when it establishes the channel the simplest/ which is used for small messages/ uses the port 's message queue as intermediate storage and copies the message from one process to the other under this method  messages of up to 256 bytes can be sent  if a client needs to send a larger message  it passes the message through a which sets up a region of shared memory the client has to decide when it sets up the channel whether or not it will need to send a large message if the client determines that it does want to send large messages/ it asks for a section object to be created similarly  if the server decides that replies will be large  it creates a section object so that the section object can be used  a small message is sent that contains a pointer and size information about the section object this method is more complicated than the first method  but it avoids data copying in both cases  a callback mechanism can be used when either the client or the server can not respond immediately to a request the callback mechanism allows them to perform asynchronous message handling  the structure of local procedure calls in windows xp is shown in figure 3.17  it is important to note that the lpc facility in windows xp is not part of the win32 api and hence is not visible to the application programmer rather  applications using the win32 api invoke standard remote procedure calls  128 chapter 3 3.6 client server connection request j connection i handle i port i handle i client i 1 communication port i ~ i server i handle communication port i shared section object  = 256 bytes  figure 3.17 local procedure calls in windows xp  when the rpc is being invoked on a process on the same system  the rpc is indirectly handled through a local procedure call lpcs are also used in a few other functions that are part of the win32 api  in section 3.4  we described how processes can communicate using shared memory and message passing these techniques can be used for communication in client-server systems  section 1.12.2  as well in this section  we explore three other strategies for communication ir1 client-server systems  sockets  remote procedure calls  rpcs   and pipes  3.6.1 sockets a is defined as an endpoint for communication a pair of processes communicating over a network employ a pair of sockets-one for each process  a socket is identified by an ip address concatenated with a port number in general  sockets use a client-server architecture the server waits for incoming client requests by listening to a specified port once a request is received  the server accepts a cmmection from the client socket to complete the com1ection  servers implementing specific services  such as telnet  ftp  and i-htp  listen to well-known ports  a telnet server listens to port 23 ; an ftp server listens to port 21 ; and a web  or http  server listens to port 80   all ports below 1024 are considered well known ; we can use them to implement standard services  when a client process initiates a request for a connection  it is assigned a port by its host computer this port is some arbitrary number greater than 1024 for example  if a client on host x with ip address 146.86.5.20 wishes to establish a connection with a web server  which is listening on port 80  at address 161.25.19.8  host x may be assigned port 1625 the connection will consist of a pair of sockets   146.86.5.20  1625  on host x and  161.25.19.8  80  on the web server this situation is illustrated in figure 3.18 the packets traveling between the hosts are delivered to the appropriate process based on the destination port number  3.6 129 host x  i 46.86.5.20  socket  i 46.86.5.20  1 625  web server  i 61 .25 i 9.8  socket  i 61 .25 i 9.8  80  figure 3.18 communication using sockets  all com1.ections must be unique therefore  if another process also on host x wished to establish another connection with the same web server  it would be assigned a port number greater than 1024 and not equal to 1625 this ensures that all com1.ections consist of a unique pair of sockets  although most program examples in this text use c  we will illustrate sockets using java  as it provides a much easier interface to sockets and has a rich library for networking utilities those interested in socket programming inc or c + + should consult the bibliographical notes at the end of the chapter  java provides three different types of sockets  are implemented with the socket class  use the datagram.socket class finally  the mul ticastsocket class is a subclass of the datagram.socket class a multicast socket allows data to be sent to multiple recipients  our example describes a date server that uses connection-oriented tcp sockets the operation allows clients to request the current date and time from the server the server listens to port 6013  although the port could have any arbitrary number greater than 1024 when a cmmection is received  the server returns the date and time to the client  the date server is shown in figure 3.19 the server creates a serversocket that specifies it will listen to port 6013 the server then begins listening to the port with the accept   method the server blocks on the accept   method waiting for a client to request a com1.ection when a connection request is received  accept   returns a socket that the server can use to communicate with the client the details of how the server communicates with the socket are as follows  the server first establishes a printwri ter objectthatitwill use to communicate with the client a printwri ter object allows the server to write to the socket using the routine print   and println   methods for output the server process sends the date to the client  calling the method println    once it has written the date to the socket  the server closes the socket to the client and resumes listening for more requests  a client communicates with the server by creating a socket and connecting to the port on which the server is listening we implement such a client in the 130 chapter 3 import java.net ; import java.io ; public class dateserver   public static void main  string   args   try    serversocket sock = new serversocket  6013  ; ii now listen for connections while  true    socket client = sock.accept   ; printwriter pout = new printwriter  client.getoutputstream    true  ; ii write the date to the socket pout.println  new java.util.date   .tostring    ; ii close the socket and resume ii listening for connections client close   ; catch  ioexception ioe   system.err.println  ioe  ;  figure 3.19 date server  java program shown in figure 3.20 the client creates a socket and requests a connection with the server at ip address 127.0.0.1 on port 6013 once the connection is madef the client can read from the socket using normal stream i/0 statements after it has received the date from the serverf the client closes the socket and exits the ip address 127.0.0.1 is a special ip address known as the when a computer refers to ip address 127.0.0.t it is referring to itself  this mechanism allows a client and server on the same host to communicate using the tcp /ip protocol the ip address 127.0.0.1 could be replaced with the ip address of another host running the date server in addition to an ip addressf an actual host namef such as www.westminstercollege.eduf can be used as well  communication using sockets-although common and efficient-is considered a low-level form of communication between distributed processes  one reason is that sockets allow only an unstructured stream of bytes to be exchanged between the communicating threads it is the responsibility of the client or server application to impose a structure on the data in the next two subsectionsf we look at two higher-level methods of communication  remote procedure calls  rpcs  and pipes  3.6 import java.net ; import java.io ; public class dateclient   public static void main  string   args   try    //make connection to server socket socket sock = new socket  127.0.0.1 ,6013  ; inputstream in = sock.getinputstream   ; bufferedreader bin = new bufferedreader  new inputstreamreader  in   ; ii read the date from the socket string line ; while   line = bin.readline    ! = null  system.out.println  line  ; ii close the socket connection sock close   ; catch  idexception ioe   system.err.println  ioe  ;  figure 3.20 date client  3.6.2 remote procedure calls 131 one of the most common forms of remote service is the rpc paradigm  which we discussed briefly in section 3.5.2 the rpc was designed as a way to abstract the procedure-call mechanism for use between systems with network connections it is similar in many respects to the ipc mechanism described in section 3.4  and it is usually built on top of such a system here  howeve1 ~ because we are dealing with an environment in which the processes are executing on separate systems  we must use a message-based communication scheme to provide remote service in contrast to the ipc facility  the messages exchanged in rpc communication are well structured and are thus no longer just packets of data each message is addressed to an rpc daemon listening to a port on the remote system  and each contains an identifier of the ftmction to execute and the parameters to pass to that function the function is then executed as requested  and any output is sent back to the requester in a separate message  a port is simply a number included at the start of a message packet whereas a system normally has one network address  it can have many ports within that address to differentiate the many network services it supports if a rencote process needs a service  it addresses a message to the proper port for instance  132 chapter 3 if a system wished to allow other systems to be able to list its current users  it would have a daemon supporting such an rpc attached to a port-say port 3027 any remote system could obtain the needed information  that is  the list of current users  by sending an rpc message to port 3027 oil the server ; the data would be received in a reply message  the semantics of rpcs allow a client to invoke a procedure on a remote host as it would invoke a procedure locally the rpc system hides the details that allow comnmnication to take place by providing a on the client side  typically  a separate stub exists for each separate remote procedure when the client invokes a remote procedure  the rpc system calls the appropriate stub  passing it the parameters provided to the remote procedure this stub locates the port on the server and marshals the parameters parameter marshalling involves packaging the parameters into a form that can be transmitted over a network the stub then transmits a message to the server using message passing a similar stub on the server side receives this message and invokes the procedure on the server if necessary  return values are passed back to the client using the same teclu1.ique  one issue that must be dealt with concerns differences in data representation on the client and server machines consider the representation of 32-bit integers some systems  known as big-endian  store the most significant byte first  while other systems  known as little-endian  store the least significant byte first neither order is better per se ; rather  the choice is arbitrary within a computer architecture to resolve differences like this  many rpc systems define a machine-independent representation of data one such representation is known as data on the client side  parameter marshalling involves converting the machine-dependent data into xdr before they are sent to the server on the server side  the xdr data are m1.marshalled and converted to the machine-dependent representation for the server  another important issue involves the semantics of a call whereas local procedure calls fail only under extreme circumstances  rpcs can fait or be duplicated and executed more than once  as a result of common network errors one way to address this problem is for the operating system to ensure that messages are acted on exactly once  rather than at most once most local procedure calls have the exactly once functionality  but it is more difficult to implement  first  consider at most once  this semantic can be implemented by attaching a timestamp to each message the server must keep a history of all the timestamps of messages it has already processed or a history large enough to ensure that repeated messages are detected incoming messages that have a timestamp already in the history are ignored the client can then send a message one or more times and be assured that it only executes once   generation of these timestamps is discussed in section 18.1  for exactly once/ ' we need to remove the risk that the server will never receive the reqliest to accomplish this  the server must implement the at most once protocol described above but must also acknowledge to the client that the rpc call was received and executed these ack messages are common throughout networking the client must resend each rpc call periodically until it receives the ack for that call  another important issue concerns the communication between a server and a client with standard procedure calls  some form of binding takes place 3.6 133 during link  load  or execution time  chapter 8  so that a procedure call 's name is replaced by the memory address of the procedure call the rpc scheme requires a similar binding of the client and the server port  but how does a client know the port numbers on the server neither system has full information about the other because they do not share memory  two approaches are common first  the binding information may be predetermined  in the form of fixed port addresses at compile time  an rpc call has a fixed port number associated with it once a program is compiled  the server can not change the port number of the requested service second  binding can be done dynamically by a rendezvous mechanism an operating system provides a rendezvous  also called a daemon on a fixed rpc port a client then sends a message containing the name of the rpc to the rendezvous daemon requesting the port address of the rpc it needs to execute the port number is returned  and the rpc calls can be sent to that port until the process terminates  or the server crashes   this method requires the extra overhead of the initial request but is more flexible than the first approach figure 3.21 shows a sample interaction  client kejyt ! .c3  sends rness tqe  t     matchrnakecto fit  1d pgrtpuml   er messages from  client to  server f \ port  matchmakerf + 1  re  address for rpc x from  server to  client port  kernel re  rpcx port  p server figure 3.21 execution of a remote procedure call  rpc   134 chapter 3 the rpc scheme is useful in implementing a distriblited file system  chapter 17   such a system can be implemented as a set of rpc daemons and clients the messages are addressed to the distributed file system port on a server on which a file operation is to take place the message contains the disk operation to be performed the disk operation might be read  write  rename  delete  or status  corresponding to the usual file-related system calls the return message contains any data resulting from that call  which is executed by the dfs daemon on behalf of the client for instance  a message might contain a request to transfer a whole file to a client or be limited to a simple block request in the latter case  several such requests may be needed if a whole file is to be transferred  3.6.3 pipes a acts as a conduit allowin.g two processes to communicate pipes were one of the first ipc mechanisms in early unix systems and typically provide one of the simpler ways for processes to communicate with one another  although they also have some limitations in implementing a pipe  four issues must be considered  does the pipe allow unidirectional communication or bidirectional communication if two-way communication is allowed  is it half duplex  data can travel only one way at a time  or full duplex  data can travel in both directions at the same time  must a relationship  such as parent-child  exist between the commlmicating processes can the pipes communicate over a network  or must the communicating processes reside on the same machine in the following sections  we explore two common types of pipes used on both unix and windows systems  3.6.3.1 ordinary pipes ordinary pipes allow two processes to communicate in standard producerconsumer fashion ; the producer writes to one end of the  the and the consumer reads from the other end  the a result  ordinary pipes are unidirectional  allowing only one-way communication if two-way communication is required  two pipes must be used  with each pipe sending data in a different direction we next illustrate constructing ordinary pipes on both unix and windows systems in both program examples  one process writes the message greetings to the pipe  while the other process reads this message front the pipe  on unix systems  ordinary pipes are constructed using the function pipe  int fd    this function creates a pipe that is accessed through the int fd   file descriptors  fd  0  is the read-end of the pipe  and fd  1  is the write end  3.6 135 parent child fd  o  fd  1  fd  o  fd  1  u  -pip-e -ou figure 3.22 file descriptors for an ordinary pipe  unix treats a pipe as a special type of file ; thus  pipes can be accessed using ordinary read   and write   system calls  an ordinary pipe can not be accessed from outside the process that creates it thus  typically a parent process creates a pipe and uses it to comnmnicate with a child process it creates via fork    recall from section 3.3.1 that a child process inherits open files from its parent since a pipe is a special type of file  the child inherits the pipe from its parent process figure 3.22 illustrates the relationship of the file descriptor fd to the parent and child processes  in the unix progranc shown in figure 3.23  the parent process creates a pipe and then sends a fork   call creating the child process what occurs after the fork   call depends on how the data are to flow through the pipe in this instance  the parent writes to the pipe and the child reads from it it is important to notice that both the parent process and the child process initially close their unused ends of the pipe although the program shown in figure 3.23 does not require this action  it is an important step to ensure that a process reading from the pipe can detect end-of-file  read   returns 0  when the writer has closed its end of the pipe  # include sys/types.h # include stdio.h # include string.h # include unistd.h # define buffer_size 25 # define read_end 0 # define write_end 1 int main  void   char write_msg  buffer_size  char read_msg  buffer_size  ; int fd  2  ; pid_t pid ; greetings ; program continues in figure 3.24 figure 3.23 ordinary pipes in unix  136 chapter 3  i create the pipe i if  pipe  fd  = = -1   fprintf  stderr  pipe failed  ; return 1 ;  i fork a child process i pid = fork   ; if  pid 0   i error occurred i fprintf  stderr  fork failed  ; return 1 ;  if  pid 0   i parent process i  i close the unused end of the pipe i close  fd  read_end   ; i write to the pipe i write  fd  write_end   write_msg  strlen  write_msg  + 1  ; i close the write end of the pipe i close  fd  write_end   ; else  i child process i  i close the unused end of the pipe i close  fd  write_end   ; i read from the pipe i read  fd  read_end   read_msg  buffer_size  ; printf  read % s  read_msg  ; i close the write end of the pipe i close  fd  read_end   ; return 0 ; figure 3.24 continuation of figure 3.23 program  ordinary pipes on windows systems are termed and they behave similarly to their unix counterparts  they are unidirectional and employ parent-child relationships between the communicating processes  in addition  reading and writing to the pipe can be accomplished with the ordinary readfile   and wri tefile   functions the win32 api for creating pipes is the createpi pe   function  which is passed four parameters  separate handles for  1  reading and  2  writing to the pipe  as well as  3  an instance of the startupinfo structure  which is used to specify that the child process is to 3.6 # include stdio.h # include stdlib.h # include windows.h # define buffer_size 25 int main  void   handle readhandle  writehandle ; startupinfo si ; process_information pi ; char message  buffer_size  greetings ; dword written ; program continues in figure 3.26 figure 3.25 windows anonymous pipes parent process  137 inherit the handles of the pipe furthermore   4  the size of the pipe  in bytes  may be specified  figure 3.25 illustrates a parent process creating an anonymous pipe for communicating with its child unlike unix systems  in which a child process automatically inherits a pipe created by its parent  windows requires the programmer to specify which attributes the child process will inherit this is accomplished by first initializing the security--attributes structure to allow handles to be inherited and then redirecting the child process 's handles for standard input or standard output to the read or write handle of the pipe  since the child will be reading from the pipe  the parent must redirect the child 's standard input to the read handle of the pipe furthermore  as the pipes are half duplex  it is necessary to prohibit the child from inheriting the write end of the pipe creating the child process is similar to the program in figure 3.12  except that the fifth parameter is set to true  indicating that the child process is to inherit designated handles from its parent before writing to the pipe  the parent first closes its unused read end of the pipe the child process that reads from the pipe is shown in figure 3.27 before reading from the pipe  this program obtains the read handle to the pipe by invoking getstdhandle    note that ordinary pipes require a parent-child relationship between the communicating processes on both unix and windows systems this means that these pipes can be used only for communication between processes on the same machine  3.6.3.2 named pipes ordinary pipes provide a simple communication mechanism between a pair of processes however  ordinary pipes exist only while the processes are communicating with one another on both unix and windows systems  once the processes have finished communicating and terminated  the ordinary pipe ceases to exist  138 chapter 3 i set up security attributes allowing pipes to be inherited i securi1yattributes sa =  sizeof  securityattributes   null  true  ; i allocate memory i zeromemory  &pi  sizeof  pi   ; i create the pipe i if  ! createpipe  &readhandle  &writehandle  &sa  0    fprintf  stderr  create pipe failed  ; return 1 ;  i establish the startjnfo structure for the child process i getstartupinfo  &si  ; si.hstdoutput = getstdhandle  std_outputjhandle  ; i redirect standard input to the read end of the pipe i si.hstdinput = readhandle ; si dwflags = startf _usestdhandles ; i do n't allow the child to inherit the write end of pipe i sethandleinformation  wri tehandle  handle_flagjnherit  0  ; i create the child process i createprocess  null  child.exe  null,null  true  i inherit handles i 0  null  null  &si  &pi  ; i close the unused end of the pipe i closehandle  readhandle  ; i the parent writes to the pipe i if  ! wri tefile  wri tehandle  message  buffer_size  &written  null   fprintf  stderr  error writing to pipe  ; i close the write end of the pipe i closehandle  writehandle  ; i wait for the child to exit i wai tforsingleobj ect  pi hprocess  infinite  ; closehandle  pi.hprocess  ; closehandle  pi.hthread  ; return 0 ;  figure 3.26 continuation of figure 3.25 program  named pipes provide a much more powerful communication tool ; communication can be bidirectional  and no parent-child relationship is required once a named pipe is established  several processes can use it for 3.6 # include stdio.h # include windows.h # define buffer_stze 25 int main  void   handle readhandle ; char buffer  buffer_size  ; dword read ; i get the read handle of the pipe i readhandle getstdhandle  std_inpuli-iandle  ; i the child reads from the pipe i 139 if  readfile  readhandle  buffer  buffer_size  &read  null   printf  child read % s ,buffer  ; else fprintf  stderr  error reading from pipe  ; return 0 ;  figure 3.27 windows anonymous pipes -child process  communication in fact  in a typical scenario  a named pipe has several writers additionally  named pipes continue to exist after communicating processes have finished both unix and windows systems support named pipes  although the details of implementation vary greatly next  we explore named pipes in each of these systems  named pipes are referred to as fifos in unix systems once created  they appear as typical files in the file system a fifo is created with the mkfifo   system call and manipulated with the ordinary open    read    write    and close   system calls it will contirme to exist m til it is explicitly deleted from the file system although fifos allow bidirectional communication  only half-duplex transmission is permitted if data must travel in both directions  two fifos are typically used additionally  the communicating processes must reside on the same machine ; sockets  section 3.6.1  must be used if intermachine communication is required  named pipes on windows systems provide a richer communication mechanism than their unix counterparts full-duplex communication is allowed  and the communicating processes may reside on either the same or different machines additionally  only byte-oriented data may be transmitted across a unix ftfo  whereas windows systems allow either byte or message-oriented data named pipes are created with the createnamedpipe   function  and a client can connect to a named pipe using connectnamedpipe    communication over the named pipe can be accomplished using the readfile   and wri tefile   functions  140 chapter 3 3.7 pipes in practice pipes are used quite often in the unix command-line environment for situations in which the output of one command serves as input to the second for example ; the unix ls command produces a directory listing  for especially long directory listings ; the output may scroll through several screens the command more manages output by displaying only one screen of output at a time ; the user must press the space bar to move from one screen to the next setting up a pipe between the ls and more commands  which are running as individual processes  allows the output of ls to be delivered as the input to moref enabling the user to display a large directory listing a screen at a time a pipe can be constructed on the command line using the i character the complete command is ls i more in this scenario ; the ls corrm1and serves as the producer  and its output is consumed by the more command  windows systems provide a more command for the dos shell with functionality similar to that of its unix cmmterpart the dos shell also uses the i character for establishing a pipe the only difference is that to get a directory listing  dos uses the dir command rather than ls the equivalent command in dos to what is shown above is dir i more a process is a program in execution as a process executes/ it changes state the state of a process is defined by that process 's current activity each process may be in one of the following states  new  readyf running  waiting ; or terminated  each process is represented in the operating system by its own process control block  pcb   a process ; when it is not executing ; is placed in some waiting queue there are two major classes of queues in an operating system  i/0 request queuecc and the ready queue the ready queue contains all the processes that are ready to execute and are waiting for the cpu each process is represented by a pcbf and the pcbs can be linked together to form a ready queue long-term  job  scheduling is the selection of processes that will be allowed to contend for the cpu normally  long-term scheduling is heavily influenced by resourceallocation considerations  especially memory management short-term  cpu  scheduling is the selection of one process from the ready queue  operating systems must provide a mechanism for parent processes to create new child processes the parent may wait for its children to terminate before proceeding  or the parent and children may execute concurrently there are several reasons for allowing concurrent execution  information sharing  computation speedup  modularity  and convenience  141 the processes executing in the operating system may be either independent processes or cooperating processes cooperating processes require an interprocess communication mechanisnc to commlmicate with each other principally  communication is achieved through two schemes  shared mernory and message passing the shared-memory method requires communicating processes to share some variables the processes are expected to exchange information through the use of these shared variables in a shared-memory system  the responsibility for providing communication rests with the application programmers ; the operating system needs to provide only the shared memory  the message-passing method allows the processes to exchange messages  the responsibility for providing communication may rest with the operating system itself these two schemes are not mutually exclusive and can be used simultaneously within a single operating system  communication in client-server systems may use  1  sockets   2  remote procedure calls  rpcs   or  3  pipes a socket is defined as an endpoint for communication a connection between a pair of applications consists of a pair of sockets  one at each end of the communication chamcel rpcs are another form of distributed commlmication an rpc occurs when a process  or thread  calls a procedure on a remote application ordinary pipes allow communication between parent and child processes  while named pipes permit unrelated processes to communicate with one another  3.1 what are the benefits and the disadvantages of each of the following consider both the system level and the programmer level  a synchronous and asynchronous commmucation b automatic and explicit buffering c send by copy and send by reference d fixed-sized and variable-sized messages 3.2 consider the rpc mechanism describe the undesirable consequences that could arise from not enforcing either the at most once or exactly once semantic describe possible uses for a mechanism that has neither of these guarantees  3.3 with respect to the rpc mechanism  consider the exactly once semantic  does the algorithm for implementing this semantic execute correctly even if the ack message back to the client is lost due to a network problem describe the sequence of messages and discuss whether exactly once is still preserved  3.4 palm os provides no means of concurrent processing discuss three major complications that concurrent processing adds to an operating system  142 chapter 3 3.5 describe the actions taken by a kernel to context-switch between processes  3.6 the sun ultrasparc processor has multiple register sets describe what happens when a context switch occurs if the new context is already loaded into one of the register sets what happens if the new context is in memory rather than in a register set and all the register sets are in use 3.7 construct a process tree similar to figure 3.9 to obtain process information for the unix or linux system  use the command ps -ael use the command man ps to get more information about the ps command on windows systems  you will have to use the task manager  3.8 give an example of a situation in which ordinary pipes are more suitable than named pipes and an example of a situation in which named pipes are more suitable than ordinary pipes  3.9 describe the differences among short-term  medium-term  and longterm scheduling  3.10 including the initial parent process  how many processes are created by the program shown in figure 3.28 3.11 using the program in figure 3.29  identify the values of pid at lines a  b  c  and d  assume that the actual pids of the parent and child are 2600 and 2603  respectively  # include stdio.h # include unistd.h int main     i fork a child process i fork   ; i fork another child process i fork   ; i and fork another i fork   ; return 0 ; figure 3.28 how many processes are created # include sysltypes.h # include stdio h # include unistd.h int main    pid_t pid ' pid1 ;  i fork a child process i pid = fork   ; if  pid 0   i error occurred i fprintf  stderr  fork failed  ; return 1 ;  else if  pid = = 0   i child process i pid1 = getpid   ;  printf  child  pid = % d ,pid  ; i a i printf  child  pid1 = % d ,pid1  ; i b i else  i parent process i pid1 = getpid   ;  printf  parent  pid = % d ,pid  ; i c i printf  parent  pid1 = % d ,pid1  ; i d i wait  null  ; return 0 ; figure 3.29 what are the pid values 143 3.12 using the program shown in figure 3.30  explain what the output will be at line a  3.13 the fibonacci sequence is the series of numbers 0  1  1  2  3  5  8    formally  it can be expressed as  fib 0 = 0 fibl = 1 jibn = jibn-l + jibn-2 write a c program using the fork   system call that generates the fibonacci sequence in the child process the number of the sequence will be provided in the comm_and line for example  if 5 is provided  the first five numbers in the fibonacci sequence will be output by the child 144 chapter 3 # include sysltypes.h # include stdio.h # include unistd.h int value = 5 ; int main    pid_t pid ;  pid = fork   ; if  pid = = 0   i child process i value + = 15 ; return 0 ;  else if  pid 0   i parent process i wait  null  ;  printf  parent  value = % d ,value  ; i line a i return 0 ; figure 3.30 what output will be at line a process because the parent and child processes have their own copies of the dataf it will be necessary for the child to output the sequence  have the parent invoke the wait   call to wait for the child process to complete before exiting the program perform necessary error checking to ensure that a non-negative number is passed on the command line  3.14 repeat the preceding exercisef this time using the createprocess   function in the win32 api in this instancef you will need to specify a separate program to be invoked from createprocess    it is this separate program that will run as a child process outputting the fibonacci sequence perform necessary error checking to ensure that a non-negative number is passed on the command line  3.15 modify the date server shown in figure 3.19 so that it delivers random jokes rather than the current date allow the jokes to contain multiple lines the date client shown in figure 3.20 can be used to read the multi-line jokes returned by the joke server  3.16 an echo server echoes back whatever it receives from a client for examplef if a client sends the server the string hello there ! the server will respond with the exact data it received from the client-that isf hello there ! 145 write an echo server using the java networking api described in section 3.6.1 this server will wait for a client connection using the accept   method when a client connection is received  the server will loop  perfonning the following steps  read data from the socket into a buffer  write the contents of the buffer back to the client  the server will break out of the loop only when it has determined that the client has closed the connection  the server shown in figure 3.19 uses the java io bufferedreader class bufferedreader extends the java io reader class  which is used for reading character streams however  the echo server can not guarantee that it will read characters from clients ; it may receive binary data as well the class java io input stream deals with data at the byte level rather than the character level thus  this echo server must use an object that extends java io inputstrearn the read   method in the java io inputstrearn class returns -1 when the client has closed its end of the socket connection  3.17 in exercise 3.13  the child process must output the fibonacci sequence  since the parent and child have their own copies of the data another approach to designing this program is to establish a shared-memory segment between the parent and child processes this technique allows the child to write the contents of the fibonacci sequence to the sharedmemory segment and has the parent output the sequence when the child completes because the memory is shared  any changes the child makes will be reflected in the parent process as well  this program will be structured using posix shared memory as described in section 3.5.1 the program first requires creating the data structure for the shared-memory segment this is most easily accomplished using a struct this data structure will contain two items   1  a fixed-sized array of size malsequence that will hold the fibonacci values and  2  the size of the sequence the child process is to generatesequence_ size  where sequence_size     malsequence these items can be represented in a struct as follows  # define max_sequence 10 typedef struct  long fib_sequence  max_sequence  ; int sequence_size ;  shared_data ; the parent process will progress thmugh the following steps  a accept the parameter passed on the command line and perform error checking to ensure that the parameter is     max_sequence  b create a shared-memory segment of size shared_data  c attach the shared-memory segment to its address space  146 chapter 3 d set the value of sequence_size to the parameter on the command line  e fork the child process and invoke the wait   systen1 call to wait for the child to finish  f output the value of the fibonacci sequence in the shared-memory segment  g detach and remove the shared-memory segment  because the child process is a copy of the parent  the shared-memory region will be attached to the child 's address space as well as the parent 's the child process will then write the fibonacci sequence to shared memory and finally will detach the segment  one issue of concern with cooperating processes involves synchronization issues in this exercise  the parent and child processes must be synchronized so that the parent does not output the fibonacci sequence until the child finishes generating the sequence these two processes will be synchronized using the wait   system call ; the parent process will invoke wait    which will cause it to be suspended until the child process exits  3.18 design a program using ordinary pipes in which one process sends a string message to a second process  and the second process reverses the case of each character in the message and sends it back to the first process for example  if the first process sends the message hi there  the second process will return hi there this will require using two pipes  one for sending the original message from the first to the second process  and the other for sending the modified message from the second back to the first process you may write this program using either unix or windows pipes  3.19 design a file-copying program named filecopy using ordinary pipes  this program will be passed two parameters  the first is the name of the file to be copied  and the second is the name of the copied file the program will then create an ordinary pipe and write the contents of the file to be copied to the pipe the child process will read this file from the pipe and write it to the destination file for example  if we invoke the program as follows  filecopy input.txt copy.txt the file input txt will be written to the pipe the child process will read the contents of this file and write it to the destination file copy txt  you may write this program using either unix or windows pipes  3.20 most unix and linux systems provide the ipcs command this command lists the status of various posix interprocess communication mechanisms  including shared-memory segments much of the information for the command comes from the data structure struct shmid_ds  147 which is available in the /usr/include/sys/shm.h file some of the fields in this structure include  int shm_segsz-size of the shared-memory segment short shm__nattch-number of attaches to the shared-memory segment struct ipc_perm shm_perm-permission structure of the sharedmemory segment the struct ipc_perm data structure  which is available in the file /usr/include/sys/ipc .h  contains the fields  unsigned short uid -identifier of the user of the shared -memory segment unsigned short mode-permission modes key_t key  on linux systems  __ key  -user-specified key identifier the permission modes are set according to how the shared-memory segment is established with the shmget   system call permissions are identified according to the following  write permission of owner  0040 read permission of group  0020 write permission of group  0004 read permission of world  0002 write permissionof world  permissions can be accessed by using the bitwise and operator &  for example  if the statement mode & 0400 evaluates to true  the permission mode gives read permission to the owner of the sharedmemory segment  a shared-memory segment can be identified according to a userspecified key or according to the integer value returned from the shmget   system call  which represents the integer identifier of the shared-memory segment created the shm_ds structure for a given integer segment identifier can be obtained with the following shmctl   system call  i identifier of the shared memory segment / int segment_id ; shm_ds shmbuffer ; shmctl  segment_id  ipc_stat  &shmbuffer  ; 148 chapter 3 if successful  shmctl   returns 0 ; otherwise  it returns -1 indicating an error condition  the global variable errno can be accessed to determine the error condition   write a c program that is passed an identifier for a shared-memory segment this program will invoke the shmctl   function to obtain its shm_ds structure it will then output the following values of the shared-memory segment  segmentid key mode owner did size number of attaches 3.21 posix message passing  this project consists of using posix message queues for communicating temperatures between each of four external processes and a central process the project can be completed on systems that support posix message passing  such as unix  linux  and mac os x  part 1  overview four external processes will communicate temperatures to a central process  which in turn will reply with its own temperature and will indicate whether the entire system has stabilized each process will receive its initial temperature upon creation and will recalculate a new temperature according to two formulas  new external temp =  mytemp 3 + 2 centraltemp  i 5 ; new central temp =  2 centraltemp + four temps received from external processes  i 6 ; initially  each external process will send its temperature to the mailbox for the central process if all four temperatures are exactly the same as those sent by the four processes during the last iteration  the system has stabilized in this case  the central process will notify each external process that it is now finished  along with the central process itself   and each process will output the final stabilized temperature if the system has not yet become stable  the central process will send its new temperature to the mailbox for each of the outer processes and await their replies the processes will continue to run until the temperature has stabilized  149 part 2  the message passing system processes can exchange messages by using four system calls  msgget    msgsnd    msgrcv    and msgctl    the msgget   function converts a mailbox name to a message queue id  msqid  a mailbox name is an externally known message queue name that is shared among the cooperating processes  msqid  the internal identifier returned by msgget    must be passed to all subsequent system calls using this message queue to facilitate interprocess communication a typical invocation of msgget   is seen below  msqid = msgget  1234  0600 i ipc_creat  ; the first parameter is the name of the mailbox  and the second parameter instructs the operating system to create the message queue if it does not already exist  with read and write privileges only for processes with the same user id as this process if a message queue already exists for this mailbox name  msgget   returns the msqid of the existing mailbox to avoid attaching to an existing message queue  a process can first attempt to attach to the mailbox by omitting ipc_creat and then checking the return value from msgget    if msq id is negative  an error has occurred during the system calt and the globally accessible variable errno can be consulted to determine whether the error occurred because the message queue already exists or for some other reason if the process determines that the mailbox does not currently exist it can then create it by including ipc_creat  for the current project  this strategy should not be necessary if students are using standalone pcs or are assigned unique ranges of mailbox names by the instructor  once a valid msqid has been established  a process can begin to use msgsnd   to send messages and msgrcv   to receive messages  the messages being sent and received are similar in format to those described in section 3.5.2  as they include a fixed-length portion at the beginning followed by a variable-length portion obviously  the senders and receivers must agree on the format of the messages being exchanged  since the operating system specifies one field in the fixed-length portion of every message format and at least one piece of information will be sent to the receiving process  it is logical to create a data aggregate for each type of message using a struct the first field of any such struct must be a long  and it will contain the priority of the message   this project does not use this functionality ; we recommend that you simply make the first field in every message equal to the same integral value  such as 2  other fields in the messages contain the information to be shared between the communicating processes three additional fields are recommended   1  the temperature being sent   2  the process number of the external process sending the message  0 for the central process   and  3  a flag that is set to 0 but that the central process will set to 1 when it notices stability a recommended struct appears as follows  150 chapter 3 struct  long priority ; int temp ; int pid ; int stable ;  msgp ; assuming the msqid has been established  examples of msgsnd   and msgrcv   appear as such  int stat  msqid ; stat = msgsnd  msqid  &msgp  sizeof  msgp  -sizeof  long   0  ; stat msgrcv  msqid  &msgp  sizeof  msgp  -sizeof  long   2  0  ; the first parameter in both system calls must be a valid msq id ; otherwise a negative value is returned  both functions return the number of bytes sent or received upon successful completion of the operation  the second parameter is the address of where to find or store the message to be sent or received  followed by the number of information bytes to be sent or received the final parameter of 0 indicates that the operations will be synchronous and that the sender will block if the message queue is full  ipc_nowait would be used if asynchronous  or nonblocking  operations were desired each individual message queue can hold a maximum number of messages-or bytes-so it is possible for the queue to become filled  which is one reason a sender may block when attempting to transmit a message  the 2 that appears before this final parameter in msgrcv   indicates the minimum priority level of the messages the process wishes to receive ; the receiver will wait until a message of that priority  or higher  is sent to the msqid if this is a synchronous operation  once a process is finished using a message queue  it must be removed so that the mailbox can be reused by other processes unless it is removed  the message queue-and any messages that have not yet been received-will remain in the storage space that has been provided for this mailbox by the kernel to remove the message queue  and delete any unread messages therein  it is necessary to invoke msgctl    as follows  struct msgid_ds dummyparam ; status = msgctl  msqid  ipc_rmid  &dummyparam  ; the third parameter is necessary because this function requires it but it is used only if it the programmer wishes to collect some statistics about the usage of the message queue this is accomplished by substituting ipc_stat as the second parameter  all programs should include the following three header files  which are found in /usr/include/sys  ipc.h  types.h  and msg.h one possibly confusing artifact of the message queue implementation bears 151 mentioning at this point after a mailbox is removed via msgctl   ,any subsequent attempts to create another mailbox with that same name using msgget   will typically generate a different msqid  part 3  creating the processes each external process  as well as the central server  will create its own mailbox with the name x + i  where i is a numeric identifier of the external process 1..4 or zero for the central process thus  if x were 70  then the central process would receive messages in the mailbox named 70  and it would send its replies to mailboxes 71-74 outer process 2 would receive in mailbox 72 and would send to mailbox 70  and so forth  thus  each external process will attach to two mailboxes  and the central process will attach to five if each process specifies ipc_creat when invoking msgget    the first process that invokes msgget   actually creates the mailbox ; subsequent calls to msgget   attach to the existing mailbox the protocol for removal should be that the mailbox/message queue that each process is listening to should be the only one it removes -via msgctl     each external process will be uniquely identified by a command-line parameter the first parameter to each external process will be its initial temperature  and the second parameter will be its unique number  1  2  3  or 4 the central server will be passed one parameter-its initial temperature assuming the executable name of the external process is external and the central server is central  invoking all five processes will be done as follows  ./external 100 1 & ./external 22 2 & ./external 50 3 & ./external 40 4 & ./central 60 & part 4  implementation hints it might be best to start by sending one message successfully from the central process to a single outer process  and vice versa  before trying to write all the code to solve this problem it is also wise to check all the return values from the four message queue system calls for possible failed requests and to output a message to the screen after each one successfully completes the message should indicate what was accomplished and by whom -for instance  mailbox 71 has been created by outer process 1/ ' message received by central process from external process 2/ ' and so forth these messages can be removed or commented out after the problem is solved processes should also verify that they were passed the correct number of command-line parameters  via the argc parameter in main     finally  extraneous messages residing in a queue can cause a collection of cooperating processes that function correctly to appear erroneous for that reason  it is wise to remove all mailboxes relevant to this project to ensure that mailboxes are empty before the processes begin the easiest way to do this is to use the 152 chapter 3 ipcs command to list all message queues and the ipcrm command to remove existing message queues the ipcs command lists the msqid of all message queues on the system use ipcrm to remove message queues according to their msqid for example  if msqid 163845 appears with the output of ipcs  it can be deleted with the following command  ipcrm -q 163845 interprocess communication in the rc 4000 system is discussed by brinchhansen  1970   schlichting and schneider  1982  discuss asynchronous message-passing prirnitives the ipc facility implemented at the user level is described by bershad et al  1990   details of interprocess communication in unix systems are presented by gray  1997   barrera  1991  and vahalia  1996  describe interprocess communication in the mach system russinovich and solomon  2005   solomon and russinovich  2000   and stevens  1999  outline interprocess communication in windows 2003  windows 2000 and unix respectively hart  2005  covers windows systems programming in detail  the implementation of rpcs is discussed by birrell and nelson  1984   shrivastava and panzieri  1982  describes the design of a reliable rpc mechanism  and tay and ananda  1990  presents a survey of rpcs stankovic  1982  and stmmstrup  1982  discuss procedure calls versus message-passing communication  harold  2005  provides coverage of socket programming in java  hart  2005  and robbins and robbins  2003  cover pipes in windows and unix systems  respectively  4.1 chapter the process model introduced in chapter 3 assumed that a process was an executing program with a single thread of control most modern operating systems now provide features enabling a process to contain multiple threads of control this chapter introduces many concepts associated with multithreaded computer systems  including a discussion of the apis for the pthreads  win32  and java thread libraries we look at many issues related to multithreaded programming and its effect on the design of operating systems finally  we explore how the windows xp and linux operating systems support threads at the kernel level  to introduce the notion of a thread a fundamental unit of cpu utilization that forms the basis of multithreaded computer systems  to discuss the apis for the pthreads  win32  and java thread libraries  to examine issues related to multithreaded programming  a thread is a basic unit of cpu utilization ; it comprises a thread id  a program counter  a register set  and a stack it shares with other threads belonging to the same process its code section  data section  and other operating-system resources  such as open files and signals a traditional  or heavrvveighl   process has a single thread of control if a process has multiple threads of control  it can perform more than one task at a time figure 4.1 illustrates the difference between a traditional process and a process  4.1.1 motivation many software packages that run on modern desktop pcs are multithreaded  an application typically is implemented as a separate process with several threads of control a web browser might have one thread display images or 153 154 chapter 4 thread + single-threaded process multithreaded process figure 4.1 single-threaded and multithreaded processes  text while another thread retrieves data from the network  for example a word processor may have a thread for displaying graphics  another thread for responding to keystrokes from the user  and a third thread for performing spelling and grammar checking in the background  in certain situations  a single application may be required to perform several similar tasks for example  a web server accepts client requests for web pages  images  sound  and so forth a busy web server may have several  perhaps thousands of  clients concurrently accessing it if the web server ran as a traditional single-tlu eaded process  it would be able to service only one client at a time  artd a client might have to wait a very long time for its request to be serviced  one solution is to have the server run as a single process that accepts requests when the server receives a request  it creates a separate process to service that request in fact  this process-creation method was in common use before threads became popular process creation is time consuming and resource intensive  however if the new process will perform the same tasks as the existing process  why incur all that overhead it is generally more efficient to use one process that contains multiple threads if the web-server process is multithreaded  the server will create a separate thread that listens for client requests when a request is made  rather than creating another process  the server will create a new thread to service the request and resume listening for additional requests this is illustrated in figure 4.2  threads also play a vital role in remote procedure call  rpc  systems recall from chapter 3 that rpcs allow interprocess communication by providing a communication mechanism similar to ordinary function or procedure calls  typically  rpc servers are multithreaded when a server receives a message  it services the message using a separate thread this allows the server to service several concurrent requests  finally  most operating system kernels are now multithreaded ; several threads operate in the kernel  and each thread performs a specific task  such client  1  request 4.1  2  create new thread to service the request 1 1 thread '------.--r---10 server  3  resume listening for additional client requests figure 4.2 multithreaded server architecture  155 as managing devices or interrupt handling for examplef solaris creates a set of threads in the kernel specifically for interrupt handling ; linux uses a kernel thread for managing the amount of free memory in the system  4.1.2 benefits the benefits of multithreaded programming can be broken down into four major categories  responsiveness multithreading an interactive application may allow a program to continue running even if part of it is blocked or is performing a lengthy operation  thereby increasing responsiveness to the user for instancef a multithreaded web browser could allow user interaction in one thread while an image was being loaded in another thread  resource sharing processes may only share resources through techniques such as shared memory or message passing such techniques must be explicitly arranged by the programmer however  threads share the memory and the resources of the process to which they belong by default  the benefit of sharing code and data is that it allows an application to have several different threads of activity within the same address space  3 economy allocating memory and resources for process creation is costly  because threads share the resources of the process to which they belong  it is more economical to create and context-switch threads empirically gauging the difference in overhead can be difficult  but in general it is much more time consuming to create and manage processes than threads  in solarisf for example  creating a process is about thirty times slower than is creating a thread  and context switching is about five times slower  scalability the benefits of multithreading can be greatly increased in a multiprocessor architecture  where threads may be running in parallel on different processors a single-threaded process can only run on one processor  regardless how many are available multithreading on a multicpu machine increases parallelism we explore this issue further in the following section  156 chapter 4 time figure 4.3 concurrent execution on a single-core system  4.1.3 multicore programming a recent trend in system design has been to place multiple computing cores on a single chip  where each core appears as a separate processor to the operating system  section 1.3.2   multithreaded programming provides a mechanism for more efficient use of multiple cores and improved concurrency consider an application with four threads on a system with a single computing core  concurrency merely means that the execution of the threads will be interleaved over time  figure 4.3   as the processing core is capable of executing only one thread at a time on a system with multiple cores  however  concurrency means that the threads can run in parallel  as the system can assign a separate thread to each core  figure 4.4   the trend towards multicore systems has placed pressure on system designers as well as application programmers to make better use of the multiple computing cores designers of operating systems must write scheduling algorithms that use multiple processing cores to allow the parallel execution shown in figure 4.4 for application programmers  the challenge is to modify existing programs as well as design new programs that are multithreaded to take advantage of multicore systems in general  five areas present challenges in programming for multicore systems  dividing activities this involves examining applications to find areas that can be divided into separate  concurrent tasks and thus can run in parallel on individual cores  balance while identifying tasks that can run in parallel  programmers must also ensure that the tasks perform equal work of equal value in some instances  a certain task may not contribute as much value to the overall process as other tasks ; using a separate execution core to run that task may not be worth the cost  data splitting just as applications are divided into separate tasks  the data accessed and manipulated by the tasks must be divided to run on separate cores  core 1 l t1 i t3 t1 t3 i ti core 2  i  t4 t2 t4 i tz time figure 4.4 parallel execution on a multicore system  4.2 4.2 157 data dependency the data accessed by the tasks must be examined for dependencies between two or more tasks in instances where one task depends on data from another  programmers must ensure that the execution of the tasks is synchronized to accommodate the data dependency we examine such strategies in chapter 6  testing and debugging when a program is running in parallel on multiple cores  there are many different execution paths testing and debugging such concurrent programs is inherently more difficult than testing and debugging single-threaded applications  because of these challenges  many software developers argue that the advent of multicore systems will require an entirely new approach to designing software systems in the future  our discussion so far has treated threads in a generic sense however  support for threads may be provided either at the user level  for or by the kernel  for threads user threads are supported above the kernel and are managed without kernel support  whereas kernel threads are supported and managed directly by the operating system virtually all contemporary operating systems-including wiridows xp  linux  mac os x  solaris  and tru64 unix  formerly digital unix  -support kernel threads  ultimately  a relationship must exist between user threads and kernel threads in this section  we look at three common ways of establishing such a relationship  4.2.1 many-to-one model the many-to-one model  figure 4.5  maps many user-level threads to one kernel thread thread management is done by the thread library in user figure 4.5 many-to-one model  158 chapter 4  user thread figure 4.6 one-to-one model  space  so it is efficient ; but the entire process will block if a thread makes a blocking system call also  because only one thread can access the kernel at a time  multiple threads are unable to nm in parallel on multiprocessors  -a thread library available for solaris-uses this modet as does gnu 4.2.2 one-to-one model the one-to-one model  figure 4.6  maps each user thread to a kernel thread it provides more concurrency than the many-to-one model by allowing another thread to run when a thread makes a blocking system call ; it also allows multiple threads to run in parallel on multiprocessors the only drawback to this model is that creating a user thread requires creating the corresponding kernel thread because the overhead of creating kernel threads can burden the performance of an application  most implementations of this model restrict the number of threads supported by the system linux  along with the family of windows operating systems  implement the one-to-one model  4.2.3 many-to-many model the many-to-many model  figure 4.7  multiplexes many user-level threads to a smaller or equal number of kernel threads the number of kernel threads may be specific to either a particular application or a particular machine  an application may be allocated more kernel threads on a multiprocessor than on a uniprocessor   whereas the many-to-one model allows the developer to user thread k +  kernel thread figure 4.7 many-to-many model  4.3 4.3 159 2      i ~   / ' / '  '  ......._ user thread    0 -kernel thread figure 4.8 two-level model  create as many user threads as she wishes  true concurrency is not gained because the kernel can schedule only one thread at a time the one-to-one model allows for greater concurrency  but the developer has to be careful not to create too many threads within an application  and in some instances may be limited in the number of threads she can create   the many-to-many model suffers from neither of these shortcomings  developers can create as many user threads as necessary  and the corresponding kernel threads can run in parallel on a multiprocessor also  when a thread performs a blocking system call  the kernel can schedule another thread for execution  one popular variation on the many-to-many model still multiplexes many user-level threads to a smaller or equal number of kernel threads but also allows a user-level thread to be bound to a kernel thread this variation  sometimes referred to as the two-level model  figure 4.8   is supported by operating systems such as irlx  hp-ux  and tru64 unix the solaris operating system supported the two-level model in versions older than solaris 9 however  beginning with solaris 9  this system uses the one-to-one model  a provides the programmer with an api for creating and managing threads there are two primary ways of implementii g a thread library the first approach is to provide a library entirely in user space with no kernel support all code and data structures for the library exist ii user space  this means that invoking a function in the library results in a local function call in user space and not a system call  the second approach is to implement a kernel-level library supported directly by the operating system in this case  code and data structures for the library exist in kernel space invoking a function in the api for the library typically results in a system call to the kernel  three main thread libraries are in use today   1  posix pthreads   2  win32  and  3  java pthreads  the threads extension of the posix standard  may be provided as either a user or kernel-level library the win32 thread library is a kernel-level library available on windows systems the java thread api allows threads to be created and managed directly in java programs however  because in most instances the jvm is running on top of a host operating system  160 chapter 4 the java thread api is generally implemented using a thread library available on the host system this means that on windows systems  java threads are typically implemented using the win32 api ; unix and linux systems often use pthreads  in the remainder of this section  we describe basic thread creation using these three thread libraries as an illustrative example  we design a multithreaded program that performs the summation of a non-negative integer in a separate thread using the well-known summation function  n sum = i ~ i = o for example  if n were 5  this function would represent the summation of integers from 0 to 5  which is 15 each of the three programs will be n.m with the upper bounds of the summation entered on the command line ; thus  if the user enters 8  the summation of the integer values from 0 to 8 will be output  4.3.1 pthreads refers to the posix standard  ieee 1003.lc  defining an api for thread creation and synchronization this is a specification for thread behavim ~ not an implementation operating system designers may implement the specification in any way they wish numerous systems implement the pthreads specification  including solaris  linux  mac os x  and tru64 unix shareware implementations are available in the public domain for the various windows operating systems as well  the c program shown in figure 4.9 demonstrates the basic pthreads api for constructing a multithreaded program that calculates the summation of a nonnegative integer in a separate thread in a pthreads program  separate threads begin execution in a specified function in figure 4.9  this is the runner   function when this program begins  a single thread of control begins in main    after some initialization  main   creates a second thread that begins control in the runner   function both threads share the global data sum  let 's look more closely at this program all pthreads programs must include the pthread h header file the statement pthread_t tid declares the identifier for the thread we will create each thread has a set of attributes  including stack size and scheduling information the pthread_attr_t attr declaration represents the attributes for the thread we set the attributes in the function call pthread_attr ini t  &attr   because we did not explicitly set any attributes  we use the default attributes provided  in chapter 5  we discuss some of the scheduling attributes provided by the pthreads api  a separate thread is created with the pthread_create   function call in addition to passirtg the thread identifier and the attributes for the thread  we also pass the name of the function where the new thread will begin execution-in this case  the runner   function last  we pass the integer parameter that was provided on the command line  argv  1   at this point  the program has two threads  the initial  or parent  thread in main   and the summation  or child  thread performing the summation 4.3 # include pthread.h # include stdio.h int sum ; i this data is shared by the thread  s  i void runner  void param  ; i the thread i int main  int argc  char argv      pthread_t tid ; i the thread identifier i pthread_attr_t attr ; i set of thread attributes i if  argc ! = 2    fprintf  stderr  usage  a.out integer value \ n  ; return -1 ; if  atoi  argv  1   0    fprintf  stderr  % d must be = 0 \ n ,atoi  argv  1    ; return -1 ; i get the default attributes i pthread_attr_init  &attr  ; i create the thread i pthread_create  &tid,&attr,runner,argv  1   ; i wait for the thread to exit i pthread_join  tid,null  ; printf  sum = % d \ n ,sum  ; i the thread will begin control in this function i void runner  void param    inti  upper = atoi  param  ; sum = 0 ; for  i = 1 ; i = upper ; i + +  sum + = i ; pthread_exi t  0  ; figure 4.9 multithreaded c program using the pthreads api  161 operation in the runner   function after creating the summation threadf the parent thread will wait for it to complete by calling the pthread_j oin   function the summation thread will complete when it calls the function pthread_exi t    once the summation thread has returnedf the parent thread will output the value of the shared data sum  162 chapter 4 4.3.2 win32 threads the technique for creating threads using the win32 thread library is similar to the pthreads technique in several ways we illustrate the win32 thread api in the c program shown in figure 4.10 notice that we must include the windows  h header file when using the win32 api  just as in the pthreads version shown in figure 4.9  data shared by the separate threads-in this case  sum-are declared globally  the dword data type is an unsigned 32-bit integer   we also define the summation   function that is to be performed in a separate thread this function is passed a pointer to a void  which win32 defines as lpvoid the thread performing this function sets the global data sum to the value of the summation from 0 to the parameter passed to summation    threads are created in the win32 api using the createthread   function  and-just as in pthreads-a set of attributes for the thread is passed to this function these attributes il1.clude security information  the size of the stack  and a flag that can be set to indicate if the thread is to start in a suspended state in this program  we use the default values for these attributes  which do not initially set the thread to a suspended state and instead make it eligible to be rm1 by the cpu scheduler   once the summation thread is created  the parent must wait for it to complete before outputting the value of sum  as the value is set by the summation thread recall that the pthread program  figure 4.9  had the parent thread wait for the summation thread using the pthread_j oin   statement we perform the equivalent of this in the win32 api using the wai tforsingleobj ect   function  which causes the creatil1.gthread to block until the summation thread has exited  we cover synchronization objects in more detail in chapter 6  4.3.3 java threads tlu eads are the fundamental model of program execution in a java program  and the java language and its api provide a rich set of features for the creation and management of threads all java programs comprise at least a single thread of control-even a simple java program consisting of only a main   method runs as a single thread in the jvm  there are two teclmiques for creating threads in a java program one approach is to create a new class that is derived from the thread class and to override its run   method an alternative-and more commonly usedteclmique is to define a class that implements the runnable interface the runnable interface is defined as follows  public interface runnable  public abstract void run   ; when a class implements runnable  it must define a run   method the code implementing the run   method is what runs as a separate thread  figure 4.11 shows the java version of a multithreaded program that determines the summation of a non-negative integer the summation class implements the runnable interface thread creation is performed by creating 4.3 # include windows.h # include stdio.h dword sum ; i data is shared by the thread  s  i i the thread runs in this separate function i dword winapi sumrnation  lpvoid param    dword upper =  dword  param ; for  dword i = 0 ; i = upper ; i + +  sum + = i ; return 0 ; int main  int argc  char argv      dword threadid ; handle threadhandle ; int param ; i perform some basic error checking i if  argc ! = 2    fprintf  stderr  an integer parameter is required \ n  ; return -1 ; param = atoi  argv  1   ; if  param 0    fprintf  stderr  an integer = 0 is required \ n  ; return -1 ; ii create the thread threadhandle = createthread  null  ii default security attributes 0  ii default stack size summation  ii thread function &param  ii parameter to thread function 0  ii default creation flags &threadid  ; ii returns the thread identifier if  threadhandle ! = null    ii now wait for the thread to finish waitforsingleobject  threadhandle,infinite  ; ii close the thread handle closehandle  threadhandle  ; printf  surn = % d \ n ,sum  ; figure 4.10 multithreaded c program using the win32 api  163 164 chapter 4 class sum   private int sum ; public int getsum    return sum ;  public void setsum  int sum   this.sum sum ;  class summation implements runnable   private int upper ; private sum sumvalue ; public summation  int upper  sum sumvalue   this.upper = upper ; this.sumvalue = sumvalue ;  public void run    int sum = 0 ;  for  int i = 0 ; i = upper ; i + +  sum + = i ; sumvalue.setsum  sum  ; public class driver   public static void main  string   args   if  args.length 0    if  integer.parseint  args  o   0  system.err.println  args  o  + must be = 0  ; else  ii create the object to be shared sum sumobject = new sum   ; int upper = integer.parseint  args  o   ; thread thrd = new thread  new summation  upper  sumobject   ; thrd.start   ; try  thrd join   ; system.out.println  the sum of + upper + is + sumobject.getsum    ;  catch  interruptedexception ie     else system.err.println  usage  summation integer value  ;  figure 4.11 java program for the summation of a non-negative integer  4.4 4.4 165 an object instance of the thread class and passing the constructor a runnable object  creating a thread object does not specifically create the new thread ; rather  it is the start   method that creates the new thread calling the start   method for the new object does two things  it allocates memory and initializes a new thread in the jvm  it calls the run   method  making the thread eligible to be run by the jvm  note that we never call the run   method directly rathel ~ we call the start   method  and it calls the run   method on our behalf  when the summation program runs  two threads are created by the jvm  the first is the parent thread  which starts execution in the main   method  the second thread is created when the start   method on the thread object is invoked this child thread begins execution in the run   method of the summation class after outputting the value of the summation  this thread terminates when it exits from its run   method  sharing of data between threads occurs easily in win32 and pthreads  since shared data are simply declared globally as a pure object-oriented language  java has no such notion of global data ; if two or more threads are to share data in a java program  the sharing occurs by passing references to the shared object to the appropriate threads in the java program shown in figure 4.11  the main thread and the summation thread share the object instance of the sum class this shared object is referenced through the appropriate get sum   and setsum   methods  you might wonder why we do n't use an integer object rather than designing a new sum class the reason is that the integer class is immutable-that is  once its value is set  it can not change  recall that the parent threads in the pthreads and win32 libraries use pthread_j oin   and wai tforsingledbj ect    respectively  to wait for the summation threads to finish before proceeding the join   method in java provides similar functionality  notice that join   can throw an interruptedexception  which we choose to ignore  in this section  we discuss some of the issues to consider with multithreaded programs  4.4.1 the fork   and exec   system calls in chapter 3  we described how the fork   system call is used to create a separate  duplicate process the semantics of the fork   and exec   system calls change in a multithreaded program  if one thread in a program calls fork    does the new process duplicate all threads  or is the new process single-threaded some unix systems have chosen to have two versions of fork    one that duplicates all threads and another that duplicates only the thread that invoked the fork   system call  the exec   system call typically works in the same way as described in chapter 3 that is  if a thread invokes the exec   system call  the program 166 chapter 4 the jvm and the host operating system the jvm is typically implemented on top of a host operating system  see figure 2.20   this setup allows the jvm to hide the implementation details of the underlying operating system and to provide a consistent  abstract environment that allows java programs to operate on any platform that supports a jvm the specification for the jvm does not indicate how java threads are to be mapped to the underlying operating system  instead leaving that decision to the particular implementation of the jvm for example  the windows xp operating system uses the one-to-one model ; therefore  each java thread for a jvm running on such a system maps to .a kernel thread  on operating systems that use the many-to-many model  such as tru64 unix   a java thread is mapped according to the many-to-manymodel solaris initially implemented the jvm using themany ~ to-one model  the greenthreads library  mentioned earlier   later releases of the jvm were implementedusing the many-to  inany model beginning with solaris 9  java threads were mapped using the one ~ to-one model in addition  there may be a relationship between the java thread library and the thread library on the host operating system  for .example  implementations of a jvm for the windows family of operating systems might use the win32 api when creating java threads ; linux  solaris  and mac os x systems might use the pthreads api  specified in the parameter to exec   will replace the entire process-including all threads  which of the two versions of fork   to use depends on the application  if exec   is called immediately after forking  then duplicating all threads is unnecessary  as the program specified in the parameters to exec   will replace the process in this instance  duplicating only the calling thread is appropriate  if  however  the separate process does not call exec   after forking  the separate process should duplicate all threads  4.4.2 cancellation '''-'c ' ' ' ' is the task of terminating a thread before it has completed  for example  if multiple threads are concurrently searching through a database and one thread returns the result  the remaining threads might be canceled  another situation might occur when a user presses a button on a web browser that stops a web page from loading any further often  a web page is loaded using several threads-each image is loaded in a separate thread when a user presses the stop button on the browser  all threads loading the page are canceled  a thread that is to be canceled is often referred to as the cancellation of a target thread may occur in two different scenarios  asynchronous cancellation one thread immediately terminates the target thread  4.4 167 deferred cancellation the target thread periodically checks whether it should terminate  allowing it an opportunity to terminate itself in an orderly fashion  the difficulty with cancellation occurs in situations where resources have been allocated to a canceled thread or where a thread is canceled while in the midst of updating data it is sharing with other threads this becomes especially troublesome with asynchronous cancellation often  the operating system will reclaim system resources from a canceled thread but will not reclaim all resources therefore  canceling a thread asynchronously may not free a necessary system-wide resource  with deferred cancellation  in contrast  one thread indicates that a target thread is to be canceled  but cancellation occurs only after the target thread has checked a flag to determine whether or not it should be canceled the thread can perform this check at a at which it can be canceled safely pthreads refers to such points as 4.4.3 signal handling a is used in unix systems to notify a process that a particular event has occurred a signal may be received either synchronously or asynchronously  depending on the source of and the reason for the event being signaled all signals  whether synchronous or asynchronous  follow the same pattern  a signal is generated by the occurrence of a particular event  a generated signal is delivered to a process  once delivered  the signal must be handled  examples of synchronous signals include illegal memory access and division by 0 if a running program performs either of these actions  a signal is generated synchronous signals are delivered to the same process that performed the operation that caused the signal  that is the reason they are considered synchronous   when a signal is generated by an event external to a running process  that process receives the signal asynchronously examples of such signals include terminating a process with specific keystrokes  such as control c  and having a timer expire typically  an asynchronous signal is sent to another process  a signal may be handled by one of two possible handlers  a default signal handler a user-defilced signal handler every signal has a that is run by the kernel when handling that signal this default action can be overridden by a signal handle ~ that is called to handle the signal signals are handled in different ways some signals  such as changing the size of a window  are simply ignored ; others  such as an illegal memory access  are handled by terminating the program  168 chapter 4 handling signals in single-threaded programs is straightforward  signals are always delivered to a process however  delivering signals is more complicated in multithreaded programs  where a process may have several threads where  then  should a signal be delivered in generat the following options exist  deliver the signal to the thread to which the signal applies  deliver the signal to every thread in the process  deliver the signal to certain threads in the process  assign a specific thread to receive all signals for the process  the method for delivering a signal depends on the type of signal generated  for example  synchronous signals need to be delivered to the thread causing the signal and not to other threads in the process however  the situation with asynchronous signals is not as clear some asynchronous signals-such as a signal that terminates a process  control c  for example  -should be sent to all threads  most multithreaded versions of unix allow a thread to specify which signals it will accept and which it will block therefore  in some cases  an asynchronous signal may be delivered only to those threads that are not blocking it however  because signals need to be handled only once  a signal is typically delivered only to the first thread found that is not blocking it  the standard unix function for delivering a signal is kill  pid_t pid  int signal   which specifies the process  pi d  to which a particular signal is to be delivered posix pthreads provides the pthread_kill  pthread_t tid  int signal  function  which allows a signal to be delivered to a specified thread  tid   although windows does not explicitly support for signals  they can be emulated using  apcs   the apc facility allows a user thread to specify a function that is to be called when the user thread receives notification of a particular event as indicated by its name  an apc is roughly equivalent to an asynchronous signal in unix however  whereas unix must contend with how to deal with signals in a multithreaded environment  the apc facility is more straightforward  since an apc is delivered to a particular thread rather than a process  4.4.4 thread pools in section 4.1  we mentioned multithreading in a web server in this situation  whenever the server receives a request  it creates a separate thread to service the request whereas creating a separate thread is certainly superior to creating a separate process  a multithreaded server nonetheless has potential problems  the first issue concerns the amount of time required to create the thread prior to servicing the request  together with the fact that this thread will be discarded once it has completed its work the second issue is more troublesome  if we allow all concurrent requests to be serviced in a new thread  we have not placed a bound on the number of threads concurrently active in the system unlimited threads could exhaust system resources  such as cpu tince or memory one solution to this problem is to use a 4.4 169 the general idea beh_ind a thread pool is to create a number of threads at process startup and place them into a pool  where they sit and wait for work  when a server receives a request  it awakens a thread from this pool-if one is available-and passes it the request for service once the thread completes its service  it returns to the pool and awaits more work if the pool contains no available thread  the server waits until one becomes free  thread pools offer these benefits  servicing a request with an existing thread is usually faster than waiting to create a thread  a thread pool limits the number of threads that exist at any one point  this is particularly important on systems that can not support a large number of concurrent threads  the number of threads in the pool can be set heuristically based on factors such as the number of cpus in the system  the amount of physical memory  and the expected number of concurrent client requests more sophisticated thread-pool architectures can dynamically adjust the number of threads in the pool according to usage patterns such architectures provide the further benefit of having a smaller pool-thereby consuming less memory-when the load on the system is low  the win32 api provides several functions related to thread pools using the thread pool api is similar to creating a thread with the thread create   function  as described in section 4.3.2 here  a function that is to run as a separate thread is defin_ed such a function may appear as follows  dword winapi poolfunction  avoid param  / this function runs as a separate thread  / a pointer to poolfunction   is passed to one of the functions in the thread pool api  and a thread from the pool executes this function one such member in the thread pool api is the queueuserworkitemo function  which is passed three paranceters  lpthread_starlroutine function-a pointer to the function that is to nm as a separate thread pvoid param-the parameter passed to function ulong flags-flags indicating how the thread pool is to create and manage execution of the thread an example of invoking a function is  queueuserworkitem  &poolfunction  null  0  ; this causes a thread from the thread pool to invoke poolfunction   on behalf of the programmer in this instance  we pass no parameters to 170 chapter 4  lightweight process '-----'-' d0 ~ kamalthcead figure 4.12 lightweight process  lwp   poolfunction    because we specify 0 as a flag  we provide the thread pool with no special instructions for thread creation  other members in the win32 thread pool api include utilities that invoke functions at periodic intervals or when an asynchronous i/0 request completes  the java util concurrent package in java 1.5 provides a thread pool utility as well 4.4.5 thread-specific data threads belonging to a process share the data of the process indeed  this sharing of data provides one of the benefits of multithreaded programming  however  in some circumstances  each thread need its own copy of certain data we will call such data for example  in a transaction-processing system  we might service each transaction in a separate thread furthermore  each transaction might be assigned a unique identifier to associate each thread with its unique identifier  we could use thread-specific data most thread libraries-including win32 and pthreads-provide some form of support for thread-specific data java provides support as well  4.4.6 scheduler activations a final issue to be considered with multithreaded programs concerns communication between the kernel and the thread library  which may be required by the many-to-many and two-level models discussed in section 4.2.3 such coordination allows the number of kernel threads to be dynamically adjusted to help ensure the best performance  many systems implementing either the many-to-many or the two-level model place an intermediate data structure between the user and kernel threads this data structure-typically known as a lightweight process  or lwp-is shown in figure 4.12 to the user-thread library  the lwp appears to be a virtual processor on which the application can schedule a user thread to run each lwp is attached to a kernel thread  and it is kernel threads that the operating system schedules to run on physical processors if a kernel thread blocks  such as while waiting for an i/0 operation to complete   the lwp blocks as well up the chain  the user-level thread attached to the lwp also blocks  an application may require any number of lwps to run efficiently consider a cpu-bound application running on a single processor in this scenario  only 4.5 4.5 171 one thread can run at once  so one lwp is sufficient an application that is i/ointensive may require multiple lwps to execute  however typically  an lwp is required for each concurrent blocking system call suppose  for example  that five different file-read requests occur simultaneously five lwps are needed  because all could be waiting for i/0 completion in the kernel if a process has only four lwps  then the fifth request must wait for one of the lwps to return from the kernel  one scheme for communication between the user-thread library and the kernel is known as it works as follows  the kernel provides an application with a set of virtual processors  lwps   and the application can schedule user threads onto an available virtual processor  furthermore  the kernel must inform an application about certain events this procedure is known as an upcalls are handled by the thread library with an and upcall handlers must run on a virtual processor  one event that triggers an upcall occurs when an application thread is about to block in this scenario  the kernel makes an upcall to the application informing it that a thread is about to block and identifying the specific thread the kernel then allocates a new virtual processor to the application the application runs an upcall handler on this new virtual processor  which saves the state of the blocking thread and relinquishes the virtual processor on which the blocking thread is running the upcall handler then schedules another thread that is eligible to run on the new virtual processor when the event that the blocking thread was waiting for occurs  the kernel makes another upcall to the thread library informilcg it that the previously blocked thread is now eligible to run  the up call handler for this event also requires a virtual processor  and the kernel may allocate a new virtual processor or preempt one of the user threads and run the upcall handler on its virtual processor after marking the 1-mblocked thread as eligible to run  the application schedules an eligible thread to run on an available virtual processor  in this section  we explore how threads are implemented in windows xp and linux systems  4.5.1 windows xp threads windows xp implements the win32 api  which is the primary api for the family of microsoft operating systems  windows 95  98  nt  2000  and xp   indeed  much of what is mentioned in this section applies to this entire family of operating systems  a windows xp application runs as a separate process  and each process may contain one or more threads the win32 api for creating threads is covered in section 4.3.2 windows xp uses the one-to-one mapping described in section 4.2.2  where each user-level thread maps to an associated kernel thread however  windows xp also provides support for a library  which provides the functionality of the many-to-many model  section 4.2.3   by using the thread library  any thread belonging to a process can access the address space of the process  172 chapter 4 the general components of a thread include  a thread id uniquely identifying the thread a register set representing the status of the processor a user stack  employed when the thread is running in user mode  and a kernel stack  employed when the thread is running in kernel mode a private storage area used by various run-time libraries and dynamic link libraries  dlls  the register set  stacks  and private storage area are known as the rcc   nw yt of the thread the primary data structures of a thread include  ethread-executive thread block kthread-kernel thread block tee-thread environment block the key components of the ethread include a pointer to the process to which the thread belongs and the address of the routine in which the thread starts control the ethread also contains a pointer to the corresponding kthread  ethread kernel space user space figure 4.13 data structures of a windows xp thread  4.5 173 the kthread includes scheduling and synchronization inforn1.ation for the thread in addition  the kthread includes the kernel stack  used when the thread is running in kernel mode  and a pointer to the teb  the ethread and the kthread exist entirely in kernel space ; this means that only the kernel can access thern the teb is a user-space data structure that is accessed when the thread is running in user mode among other fields  the teb contains the thread identifie1 ~ a user-mode stack  and an array for threadspecific data  which windows xp terms the structure of a windows xp thread is illustrated in figure 4.13  4.5.2 linux threads linux provides the fork   system call with the traditional functionality of duplicating a process  as described in chapter 3 linux also provides the ability to create threads using the clone   system call however  linux does not distinguish between processes and threads in fact  linux generally uses the term task-rather than process or thread-when referring to a flow of control within a program  when clone   is invoked  it is passed a set of flags  which determine how much sharing is to take place between the parent and child tasks some of these flags are listed below  flag meaning clone fs  file-system information is shared  clone vm  the same memory space is shared  clone  sighand signal handlers are shared  clone files the set of open files is shared  for example  if clone   is passed the flags clone_fs  clone_vm  clone_sighand  and clone_files  the parent and child tasks will share the same file-system information  such as the current working directory   the same memory space  the same signal handlers  and the same set of open files  using clone   in this fashion is equivalent to creating a thread as described in this chapter  since the parent task shares most of its resources with its child task however  if none of these flags is set when clone   is invoked  no sharing takes place  resulting in functionality similar to that provided by the fork   system call  the varying level of sharing is possible because of the way a task is represented in the linux kernel a unique kernel data structure  specifically  struct task_struct  exists for each task in the system this data structure  instead of storing data for the task  contains pointers to other data structures where these data are stored -for example  data structures that represent the list of open files  signal-handling information  and virtual memory when fork   is invoked  a new task is created  along with a copy of all the associated data structures of the parent process a new task is also created when the clone   system call is made howevet ~ rather than copying all data structures  the new 174 chapter 4 4.6 task points to the data structures of the parent task  depending on the set of flags passed to clone    several distributions of the linux kernel now include the nptl thread library nptl  which stands for native posix thread library  provides a posix-compliant thread model for linux systems along with several other features  such as better support for smp systems  as well as taking advantage of numa support in addition  the start-up cost for creating a thread is lower with nptl than with traditional linux threads finally  with nptl  the system has the potential to support hundreds of thousands of threads such support becomes more important with the growth of multicore and other smp systems  a thread is a flow of control within a process a multithreaded process contains several different flows of control within the same address space the benefits of multithreading include increased responsiveness to the use1 ~ resource sharing within the process  economy  and scalability issues such as more efficient use of multiple cores  user-level threads are threads that are visible to the programmer and are unknown to the kernel the operating-system kernel supports and manages kernel-level threads in general  user-level threads are faster to create and manage than are kernel threads  as no intervention from the kernel is required  three different types of models relate user and kernel threads  the many-to-one model maps many user threads to a single kernel thread the one-to-one model maps each user thread to a corresponding kernel thread the many-to-many model multiplexes many user threads to a smaller or equal number of kernel threads  most modern operating systems provide kernel support for threads ; among these are windows 98  nt  2000  and xp  as well as solaris and linux  thread libraries provide the application programmer with an api for creating and managing threads three primary thread libraries are in common use  posix pthreads  win32 threads for windows systems  and java threads  multithreaded programs introduce many challenges for the programmer  including the semantics of the fork   and exec   system calls other issues include thread cancellation  signal handling  and thread-specific data  4.1 provide two programming examples in which multithreading does not provide better performance than a single-threaded solution  4.2 write a ncultithreaded java  pthreads  or win32 program that outputs prime numbers this program should work as follows  the user will run the program and will enter a number on the command line the 175 program will then create a separate thread that outputs all the prime numbers less than or equal to the number entered by the user  4.3 which of the following components of program state are shared across threads in a multithreaded process a register values b heap memory c global variables d stack memory 4.4 the program shown in figure 4.14 uses the pthreads api what would be the output from the program at line c and line p # include pthread.h # include stdio.h int value = 0 ; void runner  void param  ; i the thread i int main  int argc  char argv     int pid ; pthread_t tid ; pthread_attr t attr ;  pid = fork   ; if  pid = = 0   i child process i pthread_attr_init  &attr  ; pthread_create  &tid,&attr,runner,null  ; pthread_join  tid,null  ; printf  child  value = % d ,value  ; i line c i  else if  pid 0   i parent process i wait  null  ; printf  parent  value = % d ,value  ; i line p i  void runner  void param   value = 5 ; pthread_exi t  0  ;  figure 4.14 c program for exercise 4.4  176 chapter 4 4.5 consider a multiprocessor system and a multithreaded program written using the many-to-many threading rnodel let the number of user-level threads in the program be more than the number of processors in the system discuss the performance implicatiorts of the following scenarios  a the number of kernel threads allocated to the program is less than the number of processors  b the number of kernel threads allocated to the program is equal to the number of processors  c the number of kernel threads allocated to the program is greater than the number of processors but less than the number of userlevel threads  4.6 what are two differences between user-level threads and kernel-level threads under what circumstances is one type better than the other 4.7 exercise 3.16 in chapter 3 involves designing an echo server using the java threading api however  this server is single-threaded  meaning that the server can not respond to concurrent echo clients until the current client exits modify the solution to exercise 3.16 so that the echo server services each client in a separate request  4.8 modify the socket-based date server  figure 3.19  in chapter 3 so that the server services each client request in a separate thread  4.9 can a multithreaded solution using multiple user-level threads achieve better performance on a multiprocessor system than on a singleprocessor system explain  4.10 what resources are used when a thread is created how do they differ from those used when a process is created 4.11 under what circumstances does a multithreaded solution using multiple kernel threads provide better performance than a single-threaded solution on a single-processor system 4.12 the fibonacci sequence is the series of numbers 0  1  1  2  3  5 8    formally  it can be expressed as  fib0 = 0 fih = 1 jibn = jibn-1 + jibn-2 write a multithreaded program that generates the fibonacci sequence using either the java  pthreads  or win32 thread library this program 177 should work as follows  the user will enter on the command line the number of fibonacci numbers that the program ~ is to generate  the program will then create a separate thread that will generate the fibonacci numbers  placing the sequence in data that can be shared by the threads  an array is probably the most convenient data structure   when the thread finishes execution  the parent thread will output the sequence generated by the child thread because the parent thread can not begin outputting the fibonacci sequence until the child thread finishes  this will require having the parent thread wait for the child thread to finish  using the techniques described in section 4.3  4.13 a pthread program that performs the smmnation function was provided in section 4.3.1 rewrite this program in java  4.14 as described in section 4.5.2  linux does not distinguish between processes and threads instead  linux treats both in the same way  allowing a task to be more akin to a process or a thread depending on the set of flags passed to the clone   system call however  many operating systems-such as windows xp and solaris-treat processes and threads differently typically  such systems use a notation wherein the data structure for a process contains pointers to the separate threads belonging to the process contrast these two approaches for modeling processes and threads within the kernel  4.15 describe the actions taken by a thread library to context-switch between user-level threads  the set of projects below deal with two distinct topics-naming service and matrix muliplication  project 1  naming service project a naming service such as dns  for domain name system  can be used to resolve ip names to ip addresses for example  when someone accesses the host www westminstercollege edu  a naming service is used to determine the ip address that is mapped to the ip name www westminstercollege edu  this assignment consists of writing a multithreaded nan ling service in java using sockets  see section 3.6.1   the java net api provides the following mechanism for resolving ip names  inetaddress hostaddress = inetaddress.getbyname  www.westminstercollege.edu  ; string ipaddress = hostaddress.gethostaddress   ; where getbyname   throws an unknownhostexception if it is unable to resolve the host name  178 chapter 4 the server the server will listen to port 6052 waiting for client connections when a client connection is made  the server will service the connection in a separate thread and will resume listening for additional client connections once a client makes a connection to the server  the client will write the ip name it wishes the server to resolve-such as www westminstercollege eduto the socket the server thread will read this ip name from the socket and either resolve its ip address or  if it can not locate the host address  catch an unknownhostexception the server will write the ip address back to the client or  in the case of an unknownhostexception  will write the message unable to resolve host host name  once the server has written to the client  it will close its socket connection  the client initially  write just the server application and connect to it via telnet  for example  assuming the server is running on the localhost a telnet session would appear as follows  client responses appear in telnec localhost 6052 connected to localhost  escape character is 'a  '  \ i ~ /vv \ h 'destrninstercollege edu 146.86.1.17 connection closed by foreign host  by initially having telnet act as a client  you can more easily debug any problems you may have with your server once you are convinced your server is working properly  you can write a client application the client will be passed the ip name that is to be resolved as a parameter the client will open a socket connection to the server and then write the if name that is to be resolved it will then read the response sent back by the server as an example  if the client is named nsclient  it is invoked as follows  java nsclient www.westminstercollege.edu and the server will respond with the corresponding if address or unknown host message once the client has output the if address  it will close its socket connection  project 2  matrix multiplication project given two matrices  a and b  where matrix a contains m rows and k columns and matrix b contains k rows and n columns  the of a and b is matrix c  where c contains m rows and n coh.11m1s the entry in matrix c for row i  column j  c.j  is the sum of the products of the elements for row i in matrix a and column j in matrix b that is  179 k c,j = l a ;  11 x bn,j 11 =  1 for example  if a is a 3-by-2 matrix and b is a 2-by-3 m.atrix  element c3,1 is the sum of a3,1 x b1.1 and a3,2 x b2,1 for this project  calculate each element c ; ,j in a separate worker thread this will involve creating m x n worker threads the main-or parent-thread will initialize the matrices a and b and allocate sufficient memory for matrix c  which will hold the product of matrices a and b these matrices will be declared as global data so that each worker thread has access to a  b  and c  matrices a and b can be initialized statically  as shown below  # define m 3 # define k 2 # define n 3 int a  m   k  int b  k   n  int c  m   n  ;   1,4    2,5    3,6   ;   8,7,6    5,4,3   ; alternatively  they can be populated by reading in values from a file  passing parameters to each thread the parent thread will create m x n worker threads  passing each worker the values of row i and column j that it is to use in calculating the matrix product  this requires passing two parameters to each thread the easiest approach with pthreads and win32 is to create a data structure using a struct the members of this structure are i and j  and the structure appears as follows  i structure for passing data to threads i struct v   ; int i ; i row i int j ; i column i both the pthreads and win32 programs will create the worker threads using a strategy similar to that shown below  i we have to create m n worker threads i for  i = 0 ; i m  i + +   for  j = 0 ; j n ; j + +    struct v data =  struct v  rnalloc  sizeof  struct v   ; data i = i ; data j = j ; i now create the thread passing it data as a parameter i 180 chapter 4 public class workerthread implements runnable   private int row ; private int col ; private int     a ; private int     b ; private int     c ; public workerthread  int row  int col  int     a   int     b  int     c   this.row = row ; this.col = col ; this.a a ; this.b this.c b ' c ; public void run    i calculate the matrix product in c  row   col  i  figure 4.15 worker thread in java  the data pointer will be passed to either the pthread_create    pthreads  function or the createthread    win32  function  which in turn will pass it as a parameter to the function that is to run as a separate thread  sharing of data between java threads is different from sharing between threads in pthreads or win32 one approach is for the main thread to create and initialize the matrices a  b  and c this main thread will then create the worker threads  passing the three matrices-along with row i and column jto the constructor for each worker thus  the outline of a worker thread appears in figure 4.15  waiting for threads to complete once all worker threads have completed  the main thread will output the product contained in matrix c this requires the main thread to wait for all worker threads to finish before it can output the value of the matrix product several different strategies can be used to enable a thread to wait for other threads to finish section 4.3 describes how to wait for a child thread to complete using the win32  pthreads  and java thread libraries  win32 provides the wai tforsingleobj ect   function  whereas pthreads and java use pthread_j oin   and join    respectively however  in these programming examples  the parent thread waits for a single child thread to finish ; completing this exercise will require waiting for multiple threads  in section 4.3.2  we describe the wai tforsingleobj ect   function  which is used to wait for a single thread to finish however  the win32 api also provides the wai tformultipledbj ects   function  which is used when waiting for multiple threads to complete waitformultipleobjectso is passed four parameters  # define num_threads 10 i an array of threads to be joined upon i pthread_t workers  num_threads  ; for  int i = 0 ; i num_threads ; i + +  pthread_join  workers  i   null  ; figure 4.16 pthread code for joining ten threads  the num.ber of objects to wait for a pointer to the array of objects a flag indicating if all objects have been signaled a timeout duration  or infinite  181 for example  if thandles is an array of thread handle objects of size n  the parent thread can wait for all its child threads to complete with the statement  waitformultipleobjects  n  thandles  true  infinite  ; a simple strategy for waiting on several threads using the pthreads pthread_join   or java 's join   is to enclose the join operation within a simple for loop for example  you could join on ten threads using the pthread code depicted in figure 4.16 the equivalent code using java threads is shown in figure 4.17  final static int num_threads = 10 ; i an array of threads to be joined upon i thread   workers = new thread  num_threads  ; for  int i = 0 ; i num_threads ; i + +   try  workers  i   join   ;  catch  interruptedexception ie     figure 4.17 java code for joining ten threads  threads have had a long evolution  starting as cheap concurrency in programming languages and moving to lightweight processes  with early examples that included the thotll system  cheriton et al  1979   and the pilot system  redell et al  1980    binding  1985  described moving threads into the unix kernel mach  accetta et al  1986   tevanian et al  1987a   and v  cheriton  1988   made extensive use of threads  and eventually almost all major operating systems implemented them in some form or another  182 chapter 4 thread performance issues were discussed by anderson et al  1989   who continued their work in anderson et al  1991  by evaluating the performance of user-level threads with kernel support bershad et al  1990  describe combining threads with rpc engelschall  2000  discusses a technique for supporting user-level threads an analysis of an optimal thread-pool size can be found in ling et al  2000   scheduler activations were first presented in anderson et al  1991   and williams  2002  discusses scheduler activations in the netbsd system_ other mechanisms by which the user-level thread library and the kernel cooperate with each other are discussed in marsh et al  1991   govindan and anderson  1991   draves et al  1991   and black  1990   zabatta and young  1998  compare windows nt and solaris threads on a symmetric multiprocessor pinilla and gill  2003  compare java thread performance on lim1x  windows  and solaris  vahalia  1996  covers threading in several versions of unix mcdougall and mauro  2007  describe recent developments in threading the solaris kernel  russinovich and solomon  2005  discuss threading in the windows operating system family bovet and cesati  2006  and love  2004  explain how linux handles threading and singh  2007  covers threads in mac os x  information on pthreads programming is given in lewis and berg  1998  and butenhof  1997   oaks and wong  1999   lewis and berg  2000   and holub  2000  discuss multithreading in java goetz et al  2006  present a detailed discussion of concurrent programming in java beveridge and wiener  1997  and cohen and woodring  1997  describe multithreading using win32  5.1 cpu scheduling is the basis of multiprogrammed operating systems by switching the cpu among processes  the operating system can make the computer more productive in this chapter  we introduce basic cpu-scheduling concepts and present several cpu-scheduling algorithms we also consider the problem of selecting an algorithm for a particular system  in chapter 4  we introduced threads to the process model on operating systems that support them  it is kernel-level threads-not processes-that are in fact being scheduled by the operating system however  the terms process scheduling and thread scheduling are often used interchangeably in this chapter  we use process scheduling when discussing general scheduling concepts and thread scheduling to refer to thread-specific ideas  to introduce cpu scheduling  which is the basis for multiprogrammed operating systems  to describe various cpu-scheduling algorithms  to discuss evaluation criteria for selecting a cpu-scheduling algorithm for a particular system  in a single-processor system  only one process can run at a time ; any others must wait until the cpu is free and can be rescheduled the objective of multiprogramming is to have some process rum1ing at all times  to maximize cpu utilization the idea is relatively simple a process is executed until it must wait  typically for the completion of some i/o request in a simple computer system  the cpu then just sits idle all this waiting time is wasted ; no useful work is accomplished with multiprogramming  we try to use this time productively several processes are kept in memory at one time when one process has to wait  the operating system takes the cpu away from that 183 184 chapter 5 load store add store read from file wait for 110 store increment index write to file wait for 1/0 load store add store read from file  wait.tor ; l/0 cpu burst 1/0 burst cpu burst 1/0 burst cpu burst 1/0 burst figure 5.i alternating sequence of cpu and 1/0 bursts  process and gives the cpu to another process this pattern continues every time one process has to wait  another process can take over use of the cpu  scheduling of this kind is a fundamental operating-system function  almost all computer resources are scheduled before use the cpu is  of course  one of the primary computer resources thus  its scheduling is central to operating-system design  5.1.1 cpu-i/o burst cycle the success of cpu scheduling depends on an observed property of processes  process execution consists of a cycle of cpu execution and i/0 wait processes alternate between these two states process execution begins with a cpu burst  that is followed by an i/o burst  which is followed by another cpu burst  then another i/0 burst  and so on eventually  the final cpu burst ends with a system request to terminate execution  figure 5.1   the durations of cpu bursts have been measured extensively although they vary greatly from process to process and from computer to compute1 ~ they tend to have a frequency curve similar to that shown in figure 5.2 the curve is generally characterized as exponential or hyperexponential  with a large number of short cpu bursts and a small number of long cpu bursts  an i/o-bound program typically has many short cpu bursts a cpu-bound 5.1 185 160 140 120  0 100 c aj    l u 80 ~ 60 40 20 0 8 16 24 32 40 burst duration  milliseconds  figure 5.2 histogram of cpu-burst durations  program might have a few long cpu bursts this distribution can be important in the selection of an appropriate cpu-scheduling algorithm  5.1.2 cpu scheduler whenever the cpu becomes idle  the operating system must select one of the processes in the ready queue to be executed the selection process is carried out by the short-term scheduler  or cpu scheduler   the scheduler selects a process from the processes in memory that are ready to execute and allocates the cpu to that process  note that the ready queue is not necessarily a first-in  first-out  fifo  queue  as we shall see when we consider the various scheduling algorithms  a ready queue can be implen ented as a fifo queue  a priority queue  a tree  or sirnply an unordered linked list conceptually  howeve1 ~ all the processes in the ready queue are lined up waiting for a chance to run on the cpu the records in the queues are generally process control blocks  pcbs  of the processes  5.1.3 preemptive scheduling cpu-scheduling decisions may take place under the following four circumstances  when a process switches from the running state to the waiting state  for example  as the result of an i/0 request or an invocation of wait for the termination of one of the child processes  when a process switches from the numing state to the ready state  for example  when an interrupt occurs  186 chapter 5 when a process switches from the waiting state to the ready state  for example  at completion of i/0  when a process terminates for situations 1 and 4  there is no choice in terms of scheduling a new process  if one exists in the ready queue  must be selected for execution there is a choice  however  for situations 2 and 3  when scheduling takes place only under circumstances 1 and 4  we say that the scheduling scheme is nonpreemptive or cooperative ; otherwise  it is preemptive under nonpreemptive scheduling  once the cpu has been allocated to a process  the process keeps the cpu until it releases the cpu either by terminating or by switching to the waiting state this scheduling method was used by microsoft windows 3.x ; windows 95 introduced preemptive scheduling  and all subsequent versions of windows operating systems have used preemptive scheduling the mac os x operating system for the macintosh also uses preemptive scheduling ; previous versions of the macintosh operating system relied on cooperative scheduling cooperative scheduling is the only method that can be used on certain hardware platforms  because it does not require the special hardware  for example  a timer  needed for preemptive scheduling  unfortunately  preemptive scheduling incurs a cost associated with access to shared data consider the case of two processes that share data while one is updating the data  it is preempted so that the second process can run the second process then tries to read the data  which are in an inconsistent state in such situations  we need new mechanisms to coordinate access to shared data ; we discuss this topic in chapter 6  preemption also affects the design of the operating-system kernel during the processing of a system call  the kernel may be busy with an activity on behalf of a process such activities may involve changing important kernel data  for instance  i/0 queues   what happens if the process is preempted in the middle of these changes and the kernel  or the device driver  needs to read or modify the same structure chaos ensues certain operating systems  including most versions of unix  deal with this problem by waiting either for a system call to com.plete or for an i/o block to take place before doing a context switch this scheme ensures that the kernel structure is simple  since the kernel will not preempt a process while the kernel data structures are in an inconsistent state unfortunately  this kernel-execution model is a poor one for supportil1g real-time computing and multiprocessing  these problems  and their solutions  are described i.j.1 sections 5.5 and 19.5  because interrupts can  by definition  occur at any time  and because they can not always be ignored by the kernel  the sections of code affected by interrupts must be guarded from simultaneous use the operating system needs to accept interrupts at almost all times ; otherwise  input might be lost or output overwritten so that these sections of code are not accessed concurrently by several processes  they disable interrupts at entry and reenable interrupts at exit it is important to note that sections of code that disable interrupts do not occur very often and typically contain few instructions  5.2 5.2 187 5.1.4 dispatcher another component involved in the cpu-scheduling function is the dispatcher  the dispatcher is the module that gives control of the cpu to the process selected by the short-term scheduler this function involves the following  switching context switching to user mode jumping to the proper location in the user program to restart that program the dispatcher should be as fast as possible  since it is invoked during every process switch the time it takes for the dispatcher to stop one process and start another running is known as the dispatch latency  different cpu-scheduling algorithms have different properties  and the choice of a particular algorithm may favor one class of processes over another in choosing which algorithm to use in a particular situation  we must consider the properties of the various algorithms  many criteria have been suggested for comparing cpu-scheduling algorithms  which characteristics are used for comparison can make a substantial difference in which algorithm is judged to be best the criteria include the following  cpu utilization we want to keep the cpu as busy as possible conceptually  cpu utilization can range from 0 to 100 percent in a real system  it should range from 40 percent  for a lightly loaded system  to 90 percent  for a heavily used system   throughput if the cpu is busy executing processes  then work is being done one measure of work is the number of processes that are completed per time unit  called throughput for long processes  this rate may be one process per hour ; for short transactions  it may be ten processes per second  turnaround time from the point of view of a particular process  the important criterion is how long it takes to execute that process the interval from the time of submission of a process to the time of completion is the turnaround time turnaround tim.e is the sum of the periods spent waiting to get into memory  waiting in the ready queue  executing on the cpu  and doing i/0  waiting time the cpu-scheduling algorithm does not affect the amount of time during which a process executes or does i/0 ; it affects only the an1.ount of time that a process spends waiting in the ready queue waiting time is the sum of the periods spent waiting in the ready queue  response time in an interactive system  turnaround time may not be the best criterion often  a process can produce some output fairly early and can continue computing new results while previous results are being 188 chapter 5 5.3 output to the user thus  another measure is the time from the submission of a request until the first response is produced this measure  called response time  is the tince it takes to start responding  not the time it takes to output the response the turnaround time is generally limited by the speed of the output device  it is desirable to maximize cpu utilization and throughput and to minirnize turnaround time  waiting time  and response time in most cases  we optimize the average measure however  under some circumstances  it is desirable to optimize the minimum or maximum values rather than the average for example  to guarantee that all users get good service  we may want to minirnize the maximum response time  investigators have suggested that  for interactive systems  such as timesharing systerns   it is more important to minimize the variance in the response time than to minimize the average response time a system with reasonable and predictable response time may be considered more desirable than a system that is faster on the average but is highly variable howeve1 ~ little work has been done on cpu-scheduling algorithms that minimize variance  as we discuss various cpu-scheduling algorithms in the following section  we illustrate their operation an accurate illustration should involve many processes  each a sequence of several hundred cpu bursts and i/o bursts  for simplicity  though  we consider only one cpu burst  in milliseconds  per process in our examples our measure of comparison is the average waiting time more elaborate evaluation mechanisms are discussed in section 5.7  cpu scheduling deals with the problem of deciding which of the processes in the ready queue is to be allocated the cpu there are many different cpu-scheduling algorithms in this section  we describe several of them  5.3.1 first-come  first-served scheduling by far the simplest cpu-scheduling algorithm is the first-come  first-served  fcfs  scheduling algorithm with this scheme  the process that requests the cpu first is allocated the cpu first the implementation of the fcfs policy is easily managed with a fifo queue when a process enters the ready queue  its pcb is linked onto the tail of the queue when the cpu is free  it is allocated to the process at the head of the queue the running process is then removed from the queue the code for fcfs scheduling is simple to write and understand  on the negative side  the average waiting time under the fcfs policy is often quite long consider the following set of processes that arrive at time 0  with the length of the cpu burst given in milliseconds  process burst time  p  24 p2 3 po   3 5.3 189 if the processes ani ve in the order p1  p2  p3  and are served in fcfs order  we get the result shown in the following gantt chart  which is a bar chart that illustrates a particular schedule  including the start and finish times of each of the participating processes  0 24 27 30 the waiting time is 0 milliseconds for process p1  24 milliseconds for process p2  and 27 milliseconds for process p3  thus  the average waiting time is  0 + 24 + 27  /3 = 17 ncilliseconds if the processes arrive in the order p2  p3  p1  however  the results will be as shown in the following gantt chart  0 3 6 30 the average waiting time is now  6 + 0 + 3  /3 = 3 milliseconds this reduction is substantial thus  the average waiting time under an fcfs policy is generally not minimal and may vary substantially if the processes cpu burst times vary greatly  in addition  consider the performance of fcfs scheduling in a dynamic situation assume we have one cpu-bound process and many i/o-bound processes as the processes flow armmd the system  the following scenario may result the cpu-bound process will get and hold the cpu during this time  all the other processes will finish their i/0 and will move into the ready queue  waiting for the cpu while the processes wait in the ready queue  the i/0 devices are idle eventually  the cpu-bound process finishes its cpu burst and moves to an i/0 device all the i/o-bound processes  which have short cpu bursts  execute quickly and move back to the i/0 queues at this point  the cpu sits idle the cpu-bound process will then move back to the ready queue and be allocated the cpu again  all the i/0 processes end up waiting in the ready queue until the cpu-bound process is done there is a convoy effect as all the other processes wait for the one big process to get off the cpu this effect results in lower cpu and device utilization than might be possible if the shorter processes were allowed to go first  note also that the fcfs scheduling algorithm is nonpreemptive once the cpu has been allocated to a process  that process keeps the cpu until it releases the cpu  either by terminating or by requesting i/0 the fcfs algorithm is thus particularly troublesome for time-sharing systems  where it is important that each user get a share of the cpu at regular intervals it would be disastrous to allow one process to keep the cpu for an extended period  5.3.2 shortest-job-first scheduling a different approach to cpu scheduling is the shortest-job-first  sjf  scheduling algorithm this algorithm associates with each process the length of the process 's next cpu burst when the cpu is available  it is assigned to the process 190 chapter 5 that has the smallest next cpu burst if the next cpu bursts of two processes are the same  fcfs scheduling is used to break the tie note that a more appropriate term for this scheduling method would be the shortest-next-cpu-burst algorithm  because scheduling depends on the length of the next cpu burst of a process  rather than its total length we use the term sjf because m.ost people and textbooks use this term to refer to this type of scheduling  as an example of sjf scheduling  consider the following set of processes  with the length of the cpu burst given in milliseconds  process burst time pl 6 p2 8 p3 7 p4 3 using sjf scheduling  we would schedule these processes according to the following gantt chart  0 3 9 16 24 the waiting time is 3 milliseconds for process p1  16 milliseconds for process p2  9 milliseconds for process p3  and 0 milliseconds for process p4  thus  the average waiting time is  3 + 16 + 9 + 0  i 4 = 7 milliseconds by comparison  if we were using the fcfs scheduling scheme  the average waiting time would be 10.25 milliseconds  the sjf scheduling algorithm is provably optimal  in that it gives the minimum average waiting time for a given set of processes moving a short process before a long one decreases the waiting time of the short process more than it increases the waiting time of the long process consequently  the average waiting time decreases  the real difficulty with the sjf algorithm is knowing the length of the next cpu request for long-term  job  scheduling in a batch system  we can use as the length the process time limit that a user specifies when he submits the job thus  users are motivated to estimate the process time limit accurately  since a lower value may mean faster response  too low a value will cause a time-limit-exceeded error and require resubmission  sjf scheduling is used frequently in long-term scheduling  although the sjf algorithm is optimal  it can not be implemented at the level of short-term cpu scheduling with short-term scheduling  there is no way to know the length of the next cpu burst one approach is to try to approximate sjf scheduling we may not know the length of the next cpu burst  but we may be able to predict its value we expect that the next cpu burst will be similar in length to the previous ones by computing an approximation of the length of the next cpu burst  we can pick the process with the shortest predicted cpu burst  5.3 191 the next cpu burst is generally predicted as an exponential average of the measured lengths of previous cpu bursts we can define the exponential average with the following formula let t11 be the length of the nth cpu burst  and let t11 + t be our predicted value for the next cpu burst then  for a  0  s a 1  define the value of tn contains our most recent information ; t11 stores the past history  the parameter a controls the relative weight of recent and past history in our prediction if a = 0  then tn + l = t11  and recent history has no effect  current conditions are assumed to be transient   if a = 1  then tn + l = t11  and only the most recent cpu burst matters  history is assumed to be old and irrelevant   more commonly  a = 1/2  so recent history and past history are equally weighted  the initial to can be defined as a constant or as an overall system average  figure 5.3 shows an exponential average with a = 1/2 and to = 10  to lmderstand the behavior of the exponential average  we can expand the formula for tn + l by substituting for t 11  to find  j jj ' 1 tn + l = atn +  1  a atn-1 + +  1 a  atn-j + +  1 a  'to  since both a and  1  a  are less than or equal to 1  each successive term has less weight than its predecessor  the sjf algorithm can be either preemptive or nonpreemptive the choice arises when a new process arrives at the ready queue while a previous process is still executing the next cpu burst of the newly arrived process may be shorter time + cpu burst  f  6 4 6 4 13 13 13 guess  t ;  10 8 6 6 5 9 1 1 12 figure 5.3 prediction of the length of the next cpu burst  192 chapter 5 than what is left of the currently executing process a preemptive sjf algorithm will preempt the currently executing process  whereas a nonpreemptive sjf algorithm will allow the currently running process to finish its cpu burst  preemptive sjf scheduling is sometimes called shortest-remaining-time-first scheduling  as an example  consider the following four processes  with the length of the cpu burst given in milliseconds  process arrival time burst time pl 0 8 p2 1 4 p3 2 9 p4 3 5 if the processes arrive at the ready queue at the times shown and need the indicated burst times  then the resulting preemptive sjf schedule is as depicted in the following gantt chart  0 5 10 17 26 process p1 is started at time 0  since it is the only process in the queue process p2 arrives at time 1 the remaining time for process p1  7 milliseconds  is larger than the time required by process p2  4 milliseconds   so process p1 is preempted  and process p2 is scheduled the average waiting time for this example is   10 1  +  1  1  +  17 2  +  5-3   / 4 = 26/4 = 6.5 milliseconds  nonpreemptive sjf scheduling would result in an average waiting time of 7.75 milliseconds  5.3.3 priority scheduling the sjf algorithm is a special case of the general priority scheduling algorithm  a priority is associated with each process  and the cpu is allocated to the process with the highest priority equal-priority processes are scheduled in fcfs order  an sjf algorithm is simply a priority algorithm where the priority  p  is the inverse of the  predicted  next cpu burst the larger the cpu burst  the lower the priority  and vice versa  note that we discuss scheduling in terms of high priority and low priority  priorities are generally indicated by some fixed range of numbers  such as 0 to 7 or 0 to 4,095 however  there is no general agreement on whether 0 is the highest or lowest priority some systems use low numbers to represent low priority ; others use low numbers for high priority this difference can lead to confusion in this text  we assume that low numbers represent high priority  as an example  consider the following set of processes  assumed to have arrived at time 0 in the order p1  p2   ps  with the length of the cpu burst given in milliseconds  5.3 193 process burst time ~  ~   rity pl 10   0 p2 1 1 p3 2 4 p4 1 5 ps 5 2 using priority scheduling  we would schedule these processes according to the following gantt chart  0 6 16 18 19 the average waiting time is 8.2 milliseconds  priorities can be defined either internally or externally internally defined priorities use some nceasurable quantity or quantities to compute the priority of a process for example  time limits  memory requirements  the number of open files  and the ratio of average i/0 burst to average cpu burst have been used in computing priorities external priorities are set by criteria outside the operating system  such as the importance of the process  the type and amount of funds being paid for computer use  the department sponsoring the work  and other  often politicat factors  priority scheduling can be either preemptive or nonpreemptive when a process arrives at the ready queue  its priority is compared with the priority of the currently running process a preemptive priority scheduling algorithm will preempt the cpu if the priority of the newly arrived process is higher than the priority of the currently running process a nonpreemptive priority scheduling algorithm will simply put the new process at the head of the ready queue  a rnajor problem with priority scheduling algorithms is indefinite blocking  or starvation a process that is ready to run but waiting for the cpu can be considered blocked a priority scheduling algorithm can leave some lowpriority processes waiting indefinitely in a heavily loaded computer system  a steady stream of higher-priority processes can prevent a low-priority process from ever getting the cpu generally  one of two things will happen either the process will eventually be run  at 2 a.m sunday  when the system is finally lightly loaded   or the cornputer systern will eventually crash and lose all unfinished low-priority processes  rumor has it that when they shut down the ibm 7094 at mit in 1973  they found a low-priority process that had been submitted in 1967 and had not yet been run  a solution to the problem of indefinite blockage of low-priority processes is aging aging is a techniqtje of gradually increasing the priority of processes that wait in the system for a long time for example  if priorities range from 127  low  to 0  high   we could increase the priority of a waiting process by 1 every 15 minutes eventually  even a process with an initial priority of 127 would have the highest priority in the system and would be executed in fact  it would take no more than 32 hours for a priority-127 process to age to a priority-0 process  194 chapter 5 5.3.4 round-robin scheduling the round-robin  rr  scheduling algorithm is designed especially for timesharing systems it is similar to fcfs scheduling  but preemption is added to enable the system to switch between processes a small unit of time  called a time quantum or time slice  is defined a time quantum is generally fronc 10 to 100 milliseconds in length the ready queue is treated as a circular queue  the cpu scheduler goes around the ready queue  allocating the cpu to each process for a time interval of up to 1 time quantum  to implement rr scheduling  we keep the ready queue as a fifo queue o processes new processes are added to the tail of the ready queue the cpu scheduler picks the first process from the ready queue  sets a timer to interrupt after 1 time quantum  and dispatches the process  one of two things will then happen the process may have a cpu burst of less than 1 time quantum in this case  the process itself will release the cpu voluntarily the scheduler will then proceed to the next process in the ready queue otherwise  if the cpu burst of the currently running process is longer than 1 time quantum  the timer will go off and will cause an interrupt to the operating system a context switch will be executed  and the process will be put at the tail o the ready queue the cpu scheduler will then select the next process in the ready queue  the average waiting time under the rr policy is often long consider the following set of processes that arrive at time 0  with the length of the cpu burst given in milliseconds  process burst time if we use a time quantum of 4 milliseconds  then process p1 gets the first 4 milliseconds since it requires another 20 milliseconds  it is preempted after the first time quantum  and the cpu is given to the next process in the queue  process p2  process p2 does not need 4 milliseconds  so it quits before its time quantum expires the cpu is then given to the next process  process p3 once each process has received 1 time quantum  the cpu is returned to process p1 for an additional time quantum the resulting rr schedule is as follows  0 4 7 10 14 18 22 26 30 let 's calculate the average waiting time for the above schedule p1 waits for 6 millisconds  10 4   p2 waits for 4 millisconds  and p3 waits for 7 millisconds  thus  the average waiting time is 17/3 = 5.66 milliseconds  in the rr scheduling algorithm  no process is allocated the cpu for more than 1 time quantum in a row  unless it is the only runnable process   if a 5.3 195 process 's cpu burst exceeds 1 time quantum  that process is preempted and is p11t back in the ready queue the rr scheduling algorithm is thus preemptive  if there are n processes in the ready queue and the time quantum is q  then each process gets 1 in of the cpu time in chunks of at most q time units  each process must wait no longer than  11  1  x q time units until its next time quantum for example  with five processes and a time quantum of 20 milliseconds  each process will get up to 20 milliseconds every 100 milliseconds  the performance of the rr algorithm depends heavily on the size of the time quantum at one extreme  if the time quantum is extremely large  the rr policy is the same as the fcfs policy in contrast  if the time quantum is extremely small  say  1 millisecond   the rr approach is called processor sharing and  in theory  creates the appearance that each of 11 processes has its own processor running at 1 i 11 the speed of the real processor this approach was used in control data corporation  cdc  hardware to implement ten peripheral processors with only one set of hardware and ten sets of registers  the hardware executes one instruction for one set of registers  then goes on to the next this cycle continues  resulting in ten slow processors rather than one fast one  actually  since the processor was much faster than memory and each instruction referenced memory  the processors were not much slower than ten real processors would have been  in software  we need also to consider the effect of context switching on the performance of rr scheduling assume  for example  that we have only one process of 10 time units if the quantum is 12 time units  the process finishes in less than 1 time quantum  with no overhead if the quantum is 6 time units  however  the process requires 2 quanta  resulting in a context switch if the time quantum is 1 time unit  then nine context switches will occur  slowing the execution of the process accordingly  figure 5.4   thus  we want the time quantum to be large with respect to the contextswitch time if the context-switch time is approximately 10 percent of the time quantum  then about 10 percent of the cpu time will be spent in context switching in practice  most modern systems have time quanta ranging from 10 to 100 milliseconds the time required for a context switch is typically less than 10 microseconds ; thus  the context-switch time is a small fraction of the time quantum  process time = 10 quantum context switches 12 0 0 10 6 0 6 10 r.r r  r  r    .r r -lr  -r r 9 0 2 3 4 5 6 7 8 9 10 figure 5.4 how a smaller time quantum increases context switches  196 chapter 5 process time 12.5 .p1 6 12.0 pz 3 p3 1 q  e 11.5 p4 7  ;   ; 0 c 11.0   j 0  a e 10.5 .2 q  10.0 en ~ q  9.5 c1l 9.0 2 3 4 5 6 7 time quantum figure 5.5 how turnaround time varies with the time quantum  turnaround time also depends on the size of the time quantum as we can see from figure 5.5  the average turnaround time of a set of processes does not necessarily improve as the time-quantum size increases in general  the average turnaround time can be improved if most processes finish their next cpu burst in a single time quantum for example  given three processes of 10 time units each and a quantum of 1 time unit  the average turnaround time is 29 if the time quantum is 10  however  the average turnaround time drops to 20 if context-switch time is added in  the average turnaround time increases even more for a smaller time quantum  since more context switches are required  although the time quantum should be large compared with the contextswitch time  it should not be too large if the time quantum is too large  rr scheduling degenerates to an fcfs policy a rule of thumb is that 80 percent of the cpu bursts should be shorter than the time quantum  5.3.5 multilevel queue scheduling another class of scheduling algorithms has been created for situations in which processes are easily classified into different groups for example  a common division is made between foreground  interactive  processes and background  batch  processes these two types of processes have different response-time requirements and so may have different scheduling needs in addition  foreground processes may have priority  externally defined  over background processes  a multilevel queue scheduling algorithm partitions the ready queue into several separate queues  figure 5.6   the processes are permanently assigned to one queue  generally based on some property of the process  such as memory 5.3 197 highest priority = = = = ~ '-------'i-'-n_te_r ~ ac_t_iv_e_e  .d_it ~ in_g'-'-p ~ r-.o'-c_ e'---ss ~ e-s  --'-'---l = = = = i = = = = = = ~ '---------'b_a_tc_h_p_r_o_ce_s_s_e_s ______ _j = = = = = = ~ = = = = = = ~ '-------s_tu_d_e_n_t_p_ro_c_e_s_s_es_ _____ _jl = = = = = = i lowest priority figure 5.6 multilevel queue scheduling  size  process priority  or process type each queue has its own scheduling algorithm for example  separate queues might be used for foreground and background processes the foreground queue might be scheduled by an rr algorithm  while the background queue is scheduled by an fcfs algorithm  in addition  there must be scheduling among the queues  which is commonly implemented as fixed-priority preemptive scheduling for example  the foreground queue may have absolute priority over the background queue  let 's look at an example of a multilevel queue scheduling algorithm with five queues  listed below in order of priority  system processes interactive processes interactive editing processes batch processes student processes each queue has absolute priority over lower-priority queues no process in the batch queue  for example  could run unless the queues for system processes  interactive processes  and interactive editing processes were all empty if an interactive editing process entered the ready queue while a batch process was running  the batch process would be preempted  another possibility is to time-slice among the queues here  each queue gets a certain portion of the cpu time  which it can then schedule among its various processes for instance  in the foreground-background queue example  the foreground queue can be given 80 percent of the cpu time for rr scheduling among its processes  whereas the background queue receives 20 percent of the cpu to give to its processes on an fcfs basis  198 chapter 5 5.3.6 multilevel feedback queue scheduling normally  when the multilevel queue scheduling algorithm is used  processes are permanently assigned to a queue when they enter the system if there are separate queues for foreground and background processes  for example  processes do not move from one queue to the other  since processes do not change their foreground or background nature this setup has the advantage of low scheduling overhead  but it is inflexible  the multilevel feedback queue scheduling algorithm  in contrast  allows a process to move between queues the idea is to separate processes according to the characteristics of their cpu bursts if a process uses too much cpu time  it will be moved to a lower-priority queue this scheme leaves i/o-bound and interactive processes in the higher-priority queues in addition  a process that waits too long in a lower-priority queue may be moved to a higher-priority queue this form of aging prevents starvation  for example  consider a multilevel feedback queue scheduler with three queues  numbered from 0 to 2  figure 5.7   the scheduler first executes all processes in queue 0 only when queue 0 is empty will it execute processes in queue 1 similarly  processes in queue 2 will only be executed if queues 0 and 1 are empty a process that arrives for queue 1 will preempt a process in queue 2 a process in queue 1 will in turn be preempted by a process arriving for queue 0  a process entering the ready queue is put in queue 0 a process in queue 0 is given a time quantum of 8 milliseconds if it does not filcish within this time  it is moved to the tail of queue 1 if queue 0 is empty  the process at the head of queue 1 is given a quantum of 16 milliseconds if it does not complete  it is preempted and is put into queue 2 processes in queue 2 are run on an fcfs basis but are run only when queues 0 and 1 are empty  this scheduling algorithm gives highest priority to any process with a cpu burst of 8 milliseconds or less such a process will quickly get the cpu  finish its cpu burst  and go off to its next i/0 burst processes that need more than 8 but less than 24 milliseconds are also served quickly  although with lower priority than shorter processes long processes automatically sink to queue 2 and are served in fcfs order with any cpu cycles left over from queues 0 and 1  figure 5.7 multilevel feedback queues  5.4 5.4 199 in general  a multilevel feedback queue scheduler is defined by the following parameters  the number of queues the scheduling algorithm for each queue the method used to determine when to upgrade a process to a higherpriority queue the method used to determine when to demote a process to a lowerpriority queue the method used to determine which queue a process will enter when that process needs service the definition of a multilevel feedback queue scheduler makes it the most general cpu-scheduling algorithm it can be configured to match a specific system under design unfortunately  it is also the most complex algorithm  since defining the best scheduler requires some means by which to select values for all the parameters  in chapter 4  we introduced threads to the process model  distinguishing between user-level and kernel-level threads on operating systems that support them  it is kernel-level threads-not processes-that are being scheduled by the operating system user-level threads are managed by a thread library  and the kernel is unaware of them to run on a cpu  user-level threads must ultimately be mapped to an associated kernel-level thread  although this mapping may be indirect and may use a lightweight process  lwp   in this section  we explore scheduling issues involving user-level and kernel-level threads and offer specific examples of scheduling for pthreads  5.4.1 contention scope one distinction between user-level and kernel-level threads lies in how they are scheduled on systems implementing the many-to-one  section 4.2.1  and many-to-many  section 4.2.3  models  the thread library schedules user-level threads to run on an available lwp  a scheme known as process-contention scope  pcs   since competition for the cpu takes place among threads belonging to the same process when we say the thread library schedules user threads onto available lwps  we do not mean that the thread is actually running on a cpu ; this would require the operating system to schedule the kernel thread onto a physical cpu to decide which kernel thread to schedule onto a cpu  the kernel uses system-contention scope  scs   competition for the cpu with scs scheduling takes place among all threads in the system systems usilcg the one-to-one model  section 4.2.2   such as windows xp  solaris  and linux  schedule threads using only scs  typically  pcs is done according to priority-the scheduler selects the runnable thread with the highest priority to run user-level thread priorities 200 chapter 5 5.5 are set by the programmer and are not adjusted by the thread library  although some thread libraries may allow the programmer to change the priority of a thread it is important to note that pcs will typically preempt the thread currently running in favor of a higher-priority thread ; however  there is no guarantee of time slicing  section 5.3.4  among threads of equal priority  5.4.2 pthread scheduling we provided a sample postx pthread program in section 4.3.1  along with an introduction to thread creation with pthreads now  we highlight the posix pthread api that allows specifying either pcs or scs during thread creation  pthreads identifies the following contention scope values  pthread_scope_process schedules threads using pcs scheduling  pthread_scope_system schedules threads using scs scheduling  on systems implementing the many-to-many model  the pthread_scope_process policy schedules user-level threads onto available lwps the number of lwps is maintained by the thread library  perhaps using scheduler activations  section 4.4.6   the pthread_scope_system scheduling policy will create and bind an lwp for each user-level thread on many-to-many systems  effectively mapping threads using the one-to-one policy  the pthread ipc provides two functions for getting-and setting-the contention scope policy  pthread_attr_setscope  pthread_attr_t attr  int scope  pthread_attr_getscope  pthread_attr_t attr  int scope  the first parameter for both functions contains a pointer to the attribute set for the thread the second parameter for the pthread_attr_setscope   function is passed either the pthread_scope_system or the pthread_scope_process value  indicating how the contention scope is to be set in the case of pthread_attr_getscope    this second parameter contaiils a pointer to an int value that is set to the current value of the contention scope if an error occurs  each of these functions returns a non-zero value  in figure 5.8  we illustrate a pthread scheduling api the program first determines the existing contention scope and sets it to pthread_scoplprocess it then creates five separate threads that will run using the scs scheduling policy note that on some systems  only certain contention scope values are allowed for example  linux and mac os x systems allow only pthread_scope_system  our discussion thus far has focused on the problems of scheduling the cpu in a system with a single processor if multiple cpus are available  load sharing becomes possible ; however  the scheduling problem becomes correspondingly 505 # include pthreadoh # include stdiooh # define num_threads 5 int main  int argc  char argv      int i  scope ; pthread_t tid  num_threads  ; pthread_attr_t attr ; i get the default attributes i pthread_attr_init  &attr  ; i first inquire on the current scope i if  pthread_attr_getscope  &attr  &scope  ! = 0  fprintf  stderr  unable to get scheduling scope \ n  ; else   if  scope = = pthread_scope_process  printf  pthread_scoplprocess  ; else if  scope = = pthread_scope_system  printf  pthread_scope_system  ; else fprintf  stderr  illegal scope valueo \ n  ; i set the scheduling algorithm to pcs or scs i pthread_attr_setscope  &attr  pthread_scope_system  ; i create the threads i for  i = 0 ; i num_threads ; i + +  pthread_create  &tid  i  ,&attr,runner,null  ; i now join on each thread i for  i = 0 ; i num_threads ; i + +  pthread_join  tid  i   null  ; i each thread will begin control in this function i void runner  void param   i do some work 0 0 0 i pthread_exi t  0  ;  figure 508 pthread scheduling api  201 more complex many possibilities have been tried ; and as we saw with singleprocessor cpu scheduling  there is no one best solution here  we discuss several concerns in multiprocessor scheduling we concentrate on systems 202 chapter 5 in which the processors are identical-homogeneous-in terms of their functionality ; we can then use any available processor to run any process in the queue  note  however  that even with homogeneous multiprocessors  there are sometimes limitations on scheduling consider a system with an l/0 device attached to a private bus of one processor processes that wish to use that device must be scheduled to run on that processor  5.5.1 approaches to multiple-processor scheduling one approach to cpu scheduling in a n1.ultiprocessor system has all scheduling decisions  i/o processing  and other system activities handled by a single processor-the master server the other processors execute only user code  this asymmetric multiprocessing is simple because only one processor accesses the system data structures  reducing the need for data sharing  a second approach uses symmetric multiprocessing  smp   where each processor is self-scheduling all processes may be in a common ready queue  or each processor may have its own private queue of ready processes regardless  scheduling proceeds by having the scheduler for each processor examine the ready queue and select a process to execute as we shall see in chapter 61 if we have multiple processors trying to access and update a common data structure  the scheduler must be programmed carefully we must ensure that two processors do not choose the same process and that processes are not lost from the queue virtually all modern operating systems support smp  including windows xp  windows 2000  solaris  linux  and mac os x in the remainder of this section  we discuss issues concerning smp systems  5.5.2 processor affinity consider what happens to cache memory when a process has been running on a specific processor the data most recently accessed by the process populate the cache for the processor ; and as a result  successive memory accesses by the process are often satisfied in cache memory now consider what happens if the process migrates to another processor the contents of cache memory must be invalidated for the first processor  and the cache for the second processor must be repopulated because of the high cost of invalidating and repopulating caches  most smp systems try to avoid migration of processes from one processor to another and instead attempt to keep a process rumung on the same processor this is known as processor affinity-that is  a process has an affinity for the processor on which it is currently rumting  processor affinity takes several forms when an operating system has a policy of attempting to keep a process running on the same processor-but not guaranteeing that it will do so-we have a situation known as soft affinity  here  it is possible for a process to migrate between processors some systems -such as lim.ix -also provide system calls that support hard affinity  thereby allowing a process to specify that it is not to migrate to other processors solaris allows processes to be assigned to limiting which processes can run on which cpus it also implements soft affinity  the main-memory architecture of a system can affect processor affinity issues figure 5.9 illustrates an architecture featuring non-uniform memory access  numa   in which a cpu has faster access to some parts of main memory than to other parts typically  this occurs in systems containing combined cpu 5.5 203 computer figure 5.9 numa and cpu scheduling  and memory boards the cpus on a board can access the memory on that board with less delay than they can access memory on other boards in the system  if the operating system 's cpu scheduler and memory-placement algorithms work together  then a process that is assigned affinity to a particular cpu can be allocated memory on the board where that cpu resides this example also shows that operating systems are frequently not as cleanly defined and implemented as described in operating-system textbooks rather  the solid lines between sections of an operating system are frequently only dotted lines  with algorithms creating connections in ways aimed at optimizing performance and reliability  5.5.3 load balancing on smp systems  it is important to keep the workload balanced among all processors to fully utilize the benefits of having more than one processor  otherwise  one or more processors may sit idle while other processors have high workloads  along with lists of processes awaiting the cpu load balancing attempts to keep the workload evenly distributed across all processors in an smp system it is important to note that load balancing is typically only necessary on systems where each processor has its own private queue of eligible processes to execute on systems with a common run queue  load balancing is often unnecessary  because once a processor becomes idle  it immediately extracts a rmmable process from the common run queue it is also important to note  howeve1 ~ that in most contemporary operating systems supporting smp  each processor does have a private queue of eligible processes  there are two general approaches to load balancing  push migration and pull migration with push migration  a specific task periodically checks the load on each processor and -if it finds an imbalance-evenly distributes the load by moving  or pushing  processes from overloaded to idle or less-busy processors pull migration occurs when an idle processor pulls a waiting task from a busy processor push and pull migration need not be mutually exclusive and are in fact often implemented in parallel on load-balancing systems for example  the linux scheduler  described in section 5.6.3  and the ule scheduler 204 chapter 5 available for freebsd systems implement both techniql1es linux runs its loadbalancing algorithm every 200 milliseconds  push migration  or whenever the run queue for a processor is empty  pull migration   interestingly  load balancing often counteracts the benefits of processor affinity  discussed in section 5.5.2 that is  the benefit of keeping a process running on the same processor is that the process can take advantage of its data being in that processor 's cache memory either pulling or pushing a process from one processor to another invalidates this benefit as is often the case in systems engineering  there is no absolute rule concerning what policy is best thus  in some systems  an idle processor always pulls a process from a non-idle processor ; and in other systems  processes are moved only if the imbalance exceeds a certain threshold  5.5.4 multicore processors traditionally  smp systems have allowed several threads to run concurrently by providing multiple physical processors however  a recent trend in computer hardware has been to place multiple processor cores on the same physical chip  resulting in a  each core has a register set to maintain its architectural state and appears to the operating system to be a separate physical processor smp systems that use multicore processors are faster and consume less power than systems in which each processor has its own physical chip  multicore processors may complicate scheduling issues let 's consider how this can happen researchers have discovered that when a processor accesses memory  it spends a significant amount of time waiting for the data to become available this situation  known as a may occur for various reasons  such as a cache miss  accessing data that is not in cache memory   figure 5.10 illustrates a memory stall in this scenario  the processor can spend up to 50 percent of its time waiting for data to become available from memory  to remedy this situation  many recent hardware designs have implemented multithreaded processor cores in which two  or more  hardware threads are assigned to each core that way  if one thread stalls while waiting for memory  the core can switch to another thread figure 5.11 illustrates a dual-threaded processor core on which the execution of thread 0 and the execution of thread 1 are interleaved from an operating-system perspective  each hardware thread appears as a logical processor that is available to run a software thread thus  on a dual-threaded  dual-core system  four logical processors are presented to the operating system the ultrasparc tl cpu has eight cores per chip and four 0 compute cycle ~ memory stall cycle thread c m c m c m c m time figure 5.10 memory stall  5.5 205 thread1 c m c m c m c thread0 c m c m c m c time figure 5.11 multithreaded multicore system  hardware threads per core ; from the perspective of the operating system  there appear to be 32 logical processors  in general  there are two ways to multithread a processor  ~ __u.,u c   ; u  chccu multithreading with coarse-grained multithreading  a thread executes on a processor until a long-latency event such as a memory stall occurs  because of the delay caused by the long-latency event  the processor must switch to another thread to begin execution however  the cost of switching between threads is high  as the instruction pipeline must be flushed before the other thread can begin execution on the processor core once this new thread begins execution  it begins filling the pipeline with its instructions  fine-grained  or interleaved  multithreading switches between threads at a much finer level of granularity-typically at the boundary of an instruction cycle however  the architectural design of fine-grained systems includes logic for thread switching as a result  the cost of switching between threads is small  notice that a multithreaded multicore processor actually requires two different levels of scheduling on one level are the scheduling decisions that must be made by the operating system as it chooses which software thread to run on each hardware thread  logical processor   for this level of scheduling  the operating system may choose any scheduling algorithm  such as those described in section 5.3 a second level of scheduling specifies how each core decides which hardware thread to run there are several strategies to adopt in this situation the ultrasparc tl  mentioned earlier  uses a simple roundrobin algorithm to schedule the four hardware threads to each core another example  the intel itanium  is a dual-core processor with hvo hardwaremanaged threads per core assigned to each hardware thread is a dynamic urgency value ranging from 0 to 7  with 0 representing the lowest urgency  and 7 the highest the itanium identifies five different events that may trigger a thread switch when one of these events occurs  the thread-switching logic compares the urgency of the two threads and selects the thread with the highest urgency value to execute on the processor core  5.5.5 virtualization and scheduling a system with virtualization  even a single-cpu system  frequently acts like a multiprocessor system the virtualization software presents one or more virtual cpus to each of the virtual machines rum1.ing on the system and then schedules the use of the physical cpus among the virtual machines  the significant variations between virtualization technologies make it difficult to summarize the effect of virtualization on scheduling  see section 2.8   in general  though  most virtualized environments have one host operating 206 chapter 5 5.6 system and many guest operating systems the host operating system creates and manages the virtual machines  and each virtual n achine has a guest operating system installed and applications running within that guest eacb guest operating system may be fine-tuned for specific use cases  applications  and users  including time sharing or even real-time operation  any guest operating-system scheduling algorithm that assumes a certain amount of progress in a given amount of time will be negatively impacted by virtualization consider a time-sharing operating system that tries to allot 100 milliseconds to each time slice to give users a reasonable response time within a virtual machine  this operating system is at the mercy of the virtualization system as to what cpu resources it actually receives a given 100-millisecond time slice may take much more than 100 milliseconds of virtual cpu time  depending on how busy the system is  the time slice may take a second or more  resulting in very poor response times for users logged into that virtual machine  the effect on a real-time operating system would be even more catastrophic  the net effect of such scheduling layering is that individual virtualized operating systems receive only a portion of the available cpu cycles  even though they believe they are receiving all of the cycles and indeed that they are scheduling all of those cycles commonly  the time-of-day clocks in virtual machines are incorrect because timers take longer to trigger than they would on dedicated cpus virtualization can thus lmdo the good scheduling-algorithm efforts of the operating systems within virtual machines  we turn next to a description of the scheduling policies of the solaris  windows xp  and linux operating systems it is important to remember that we are describing the scheduling of kernel tlueads with solaris and windows xp  recall that linux does not distinguish between processes and threads ; thus  we use the term task when discussing the linux scheduler  5.6.1 example  solaris scheduling solaris uses priority-based thread scheduling where each thread belongs to one of six classes  time sharing  ts  interactive  ia  real time  rt  system  sys  fair share  fss  fixed priority  fp  within each class there are different priorities and different scheduling algorithms  the default scheduling class for a process is time sharing the scheduling policy for the time-sharing class dynamically alters priorities and assigns time 5.6 207 10 160 0 51 15 160 5 51 20 120 10 52 25 120 15 52 30 80 20 53 35 80 25 54 40 40 30 55 45 40 35 56 50 40 40 58 55 40 45 58 59 20 49 59 figure 5.12 solaris dispatch table for time-sharing and interactive threads  slices of different lengths using a multilevel feedback queue by default  there is an inverse relationship between priorities and time slices the higher the priority  the smaller the time slice ; and the lower the priority  the larger the time slice interactive processes typically have a higher priority ; cpu-bound processes  a lower priority this scheduling policy gives good response time for interactive processes and good throughput for cpu-bound processes the interactive class uses the same scheduling policy as the time-sharing class  but it gives windowing applications-such as those created by the kde or gnome window managers-a higher priority for better performance  figure 5.12 shows the dispatch table for scheduling time-sharing and interactive threads these two scheduling classes include 60 priority levels  but for brevity  we display only a handful the dispatch table shown in figure 5.12 contains the following fields  priority the class-dependent priority for the time-sharing and interactive classes a higher number indicates a higher priority  time quantum the time quantum for the associated priority this illustrates the inverse relationship between priorities and time quanta  the lowest priority  priority 0  has the highest tince quantum  200 milliseconds   and the highest priority  priority 59  has the lowest time quantum  20 milliseconds   time quantum expired the new priority of a thread that has used its entire time quantum without blocking such threads are considered 208 chapter 5 cpu-intensive as shown in the table  these threads have their priorities lowered  return from sleep the priority of a thread that is returning from sleeping  such as waiting for i/0   as the table illustrates  when i/0 is available for a waiting thread  its priority is boosted to between 50 and 59  thus supporting the scheduling policy of providing good response time for interactive processes  threads in the real-time class are given the highest priority this assignment allows a real-time process to have a guaranteed response from the system within a bounded period of time a real-time process will run before a process in any other class in general  however  few processes belong to the real-time class  solaris uses the system class to run kernel threads  such as the scheduler and paging daemon once established  the priority of a system thread does not change the system class is reserved for kernel use  user processes rum1ing in kernel mode are not in the system class   the fixed-priority and fair-share classes were introduced with solaris 9  threads in the fixed-priority class have the same priority range as those in the time-sharing class ; however  their priorities are not dynamically adjusted  the fair-share scheduling class uses cpu instead of priorities to make scheduling decisions cpu shares indicate entitlement to available cpu resources and are allocated to a set of processes  known as a project   each scheduling class includes a set of priorities however  the scheduler converts the class-specific priorities into global priorities and selects the thread with the highest global priority to n.m the selected thread n.ms on the cpu until it  1  blocks   2  uses its time slice  or  3  is preempted by a higher-priority thread if there are multiple threads with the same priority  the scheduler uses a round-robin queue figure 5.13 illustrates how the six scheduling classes relate to one another and how they map to global priorities notice that the kernel maintains 10 threads for servicing interrupts these threads do not belong to any scheduling class and execute at the highest priority  160-169   as mentioned  solaris has traditionally used the many-to-many model  section 4.2.3  but switched to the one-to-one model  section 4.2.2  beginning with solaris 9  5.6.2 example  windows xp scheduling windows xp schedules threads using a priority-based  preemptive scheduling algorithm the windows xp scheduler ensures that the highest-priority thread will always run the portion of the windows xp kernel that handles scheduling is called the dispatcher a thread selected to run by the dispatcher will run until it is preempted by a higher-priority thread  until it terminates  until its time quantum ends  or until it calls a blocking system call  such as for i/0 if a higher-priority real-time thread becomes ready while a lower-priority thread is running  the lower-priority thread will be preempted this preemption gives a real-time thread preferential access to the cpu when the thread needs such access  the dispatcher uses a 32-level priority scheme to determine the order of thread execution priorities are divided into two classes the global priority highest lowest 169 160 159 100 99 60 59 0 5.6 figure 5.13 solaris scheduling  scheduling order first last 209 contains threads having priorities from 1 to 15  and the contains threads with priorities ranging from 16 to 31  there is also a thread running at priority 0 that is used for memory management  the dispatcher uses a queue for each scheduling priority and traverses the set of queues from highest to lowest until it finds a thread that is ready to run if no thread is found  the dispatcher will execute a special thread called the there is a relationship between the numeric priorities of the windows xp kernel and the win32 api the win32 api identifies several priority classes to which a process can belong these include  realtime_priority _class higf-lpriority _class abovknormalpriority class normalpriority class 210 chapter 5 .15 12 10 14 11 9 13 10 8 12 9 7 22 1.1 8 6 16 figure 5.14 windows xp priorities  below normal...priority _class idle...priority _class 8 6 7 5 6 4 5 3 4 2 priorities in all classes except the realtime...priority _class are variable  meaning that the priority of a thread belonging to one of these classes can change  a thread within a given priority classes also has a relative priority the values for relative priorities include  time_critical highest above_normal normal below normal lowest idle the priority of each thread is based on both the priority class it belongs to and its relative priority within that class this relationship is shown in figure 5.14 the values of the priority classes appear in the top row the left column contains the values for the relative priorities for example  if the relative priority of a thread in the above_normal...priority_class is normal  the nunceric priority of that thread is 10  furthermore  each thread has a base priority representing a value in the priority range for the class the thread belongs to by default  the base priority is the value of the normal relative priority for that class the base priorities for each priority class are  realtime...priority_class-24 higrlpriority class-13 5.6 above_normalpriority_class-10 normalpriority _class-8 below _normalpriority _class-6 idle_priority _class-4 211 processes are typically members of the normalpriority_class a process belongs to this class unless the parent of the process was of the idle_priority _class or unless another class was specified when the process was created the initial priority of a thread is typically the base priority of the process the thread belongs to  when a thread 's time quantun1 runs out  that thread is interrupted ; if the thread is in the variable-priority class  its priority is lowered the priority is never lowered below the base priority  however lowering the priority tends to limit the cpu consumption of compute-bound threads when a variablepriority thread is released from a wait operation  the dispatcher boosts the priority the amount of the boost depends on what the thread was waiting for ; for example  a thread that was waiting for keyboard i/0 would get a large increase  whereas a thread waiting for a disk operation would get a moderate one this strategy tends to give good response times to interactive threads that are using the mouse and windows it also enables i/o-bound threads to keep the i/0 devices busy while permitting compute-bound threads to use spare cpu cycles in the background this strategy is used by several time-sharing operating systems  including unix in addition  the window with which the user is currently interacting receives a priority boost to enhance its response time  when a user is running an interactive program  the system needs to provide especially good performance for this reason  windows xp has a special scheduling rule for processes in the normalpriority_class windows xp distinguishes between the foreground process that is currently selected on the screen and the background processes that are not currently selected when a process moves into the foreground  windows xp increases the scheduling quantum by some factor-typically by 3 this increase gives the foreground process three times longer to run before a time-sharing preemption occurs  5.6.3 example  linux scheduling prior to version 2.5  the linux kernel ran a variation of the traditional unix scheduling algorithm two problems with the traditional unix scheduler are that it does not provide adequate support for smp systems and that it does not scale well as the number of tasks on the system grows with version 2.5  the scheduler was overhauled  and the kernel now provides a scheduling algorithm that runs in constant time-known as 0  1  -regardless of the number of tasks on the system the new scheduler also provides increased support for smp  including processor affinity and load balancing  as well as providing fairness and support for interactive tasks  the linux scheduler is a preemptive  priority-based algorithm with two separate priority ranges  a real-time range from 0 to 99 and a nice value ranging from 100 to 140 these two ranges map into a global priority scheme wherein numerically lower values indicate higher priorities  212 chapter 5 numeric priority 0 99 100 140 relative priority highest lowest time quantum 200 ms 10 ms figure 5.15 the relationship between priorities and time-slice length  unlike schedulers for many other systems  including solaris  section 5.6.1  and windows xp  section 5.6.2   lim1x assigns higher-priority tasks longer time quanta and lower-priority tasks shorter time quanta the relationship between priorities and tim.e-slice length is shown in figure 5.15  a runnable task is considered eligible for execution on the cpu as long as it has time remaining in its time slice when a task has exhausted its time slice  it is considered expired and is not eligible for execution again until all other tasks have also exhausted their time quanta the kernel maintains a list of all runnable tasks in a data structure because of its support for smp  each processor maintains its own nmqueue and schedules itself independently  each runqueue contains two priority arrays  and the active array contains all tasks with time remaining in their time slices  and the expired array contains all expired tasks each of these priority arrays contains a list of tasks indexed according to priority  figure 5.16   the scheduler chooses the task with the highest priority from the active array for execution on the cpu on multiprocessor machines  this means that each processor is scheduling the highest-priority task from its own runqueue structure when all tasks have exhausted their time slices  that is  the active array is empty   the two priority arrays are exchanged ; the expired array becomes the active array  and vice versa  linux implements real-time scheduling as defined by posix.1b  which is described in section 5.4.2 real-time tasks are assigned static priorities all active array priority  0   1   140  task lists o-o 0--0--0 0 expired array priority  0   1   140  task lists o--o-o 0 figure 5.16 list of tasks indexed according to priority  5.7 5.7 213 other tasks have dynamic priorities that are based on their nice values plus or minus the value 5 the interactivity of a task determines whether the value 5 will be added to or subtracted from the nice value a task 's interactivity is deterncined by how long it has been sleeping while waiting for i/0 tasks that are more interactive typically have longer sleep times and therefore are more likely to have adjustments closer to -5  as the scheduler favors interactive tasks the result of such adjustments will be higher priorities for these tasks  conversely  tasks with shorter sleep times are often more cpu-bound and thus will have their priorities lowered  a task 's dynamic priority is recalculated when the task has exhausted its time quantum and is to be moved to the expired array thus  when the two arrays are exchanged  all tasks in the new active array have been assigned new priorities and corresponding time slices  how do we select a cpu-scheduling algorithm for a particular system as we saw in section 5.3  there are many scheduling algorithms  each with its own parameters as a result  selecting an algorithm can be difficult  the first problem is defining the criteria to be used in selecting an algorithm  as we saw in section 5.2  criteria are often defined in terms of cpu utilization  response time  or thxoughput to select an algorithm  we must first define the relative importance of these elements our criteria may include several measures  such as  maximizing cpu utilization under the constraint that the maximum response time is 1 second maximizing throughput such that turnaround time is  on average  linearly proportional to total execution time once the selection criteria have been defined  we want to evaluate the algorithms under consideration we next describe the various evaluation methods we can use  5.7.1 deterministic modeling one major class of evaluation methods is analytic evaluation analytic evaluation uses the given algorithm and the system workload to produce a formula or number that evaluates the performance of the algorithm for that workload  deterministic modeling is one type of analytic evaluation this method takes a particular predetermined workload and defines the performance of each algorithm for that workload for example  assume that we have the workload shown below all five processes arrive at time 0  in the order given  with the length of the cpu burst given in milliseconds  214 chapter 5 process burst time  ~ pj 10 p2 29 po c  j 0 p4 7 ps 12 consider the fcfs  sjf  and rr  quantum = 10 milliseconds  scheduling algorithms for this set of processes which algorithm would give the minimum average waiting time for the fcfs algorithm  we would execute the processes as 0 10 39 42 49 61 the waiting time is 0 milliseconds for process p1  10 milliseconds for process p2  39 milliseconds for process p3  42 milliseconds for process p4  and 49 milliseconds for process p5  thus  the average waiting time is  0 + 10 + 39 + 42 + 49  /5 = 28 milliseconds  with nonpreemptive sjf scheduling  we execute the processes as 0 3 10 20 p 5 32 61 the waiting time is 10 milliseconds for process p11 32 milliseconds for process p2  0 milliseconds for process p3  3 milliseconds for process p4  and 20 milliseconds for process p5  thus  the average waiting time is  10 + 32 + 0 + 3 + 20  i 5 = 13 milliseconds  0 with the rr algorithm  we execute the processes as p 1 10 20 23 30 40 50 52 61 the waiting time is 0 milliseconds for process p1  32 milliseconds for process p2  20 milliseconds for process p3  23 milliseconds for process p4  and 40 milliseconds for process p5  thus  the average waiting time is  0 + 32 + 20 + 23 + 40  /5 = 23 milliseconds  we see that  in this case  the average waiting time obtained with the sjf policy is less than half that obtained with fcfs scheduling ; the rr algorithm gives us an intermediate value  deterministic modeling is simple and fast it gives us exact numbers  allowing us to compare the algorithms however  it requires exact numbers for input  and its answers apply only to those cases the main uses of deterministic modeling are in describing scheduling algorithms and providing examples in 5.7 215 cases where we are running the same program over and over again and can measure the program 's processing requirements exactly  we may be able to use deterministic modeling to select a scheduling algorithm furthermore  over a set of examples  deterministic modeling may indicate trends that can then be analyzed and proved separately for example  it can be shown that  for the environment described  all processes and their times available at tirne 0   the sjf policy will always result in the rninimum waiting time  5.7.2 queueing models on many systems  the processes that are run vary from day to day  so there is no static set of processes  or times  to use for deterministic modeling what can be determined  however  is the distribution of cpu and i/0 bursts these distributions can be measured and then approximated or simply estimated the result is a mathematical formula describing the probability of a particular cpu burst commonly  this distribution is exponential and is described by its mean  similarly  we can describe the distribution of times when processes arrive in the system  the arrival-time distribution   fron1 these two distributions  it is possible to compute the average throughput  utilization  waiting time  and so on for most algorithms  the computer system is described as a network of servers each server has a queue of waiting processes the cpu is a server with its ready queue  as is the i/0 system with its device queues knowing arrival rates and service rates  we can compute utilization  average queue length  average wait time  and so on this area of study is called queueing-network analysis  as an example  let n be the average queue length  excluding the process being serviced   let w be the average waiting time in the queue  and let a be the average arrival rate for new processes in the queue  such as three processes per second   we expect that during the time w that a process waits  a x w new processes will arrive in the queue if the system is in a steady state  then the number of processes leaving the queue must be equal to the number of processes that arrive thus  n = ax w  this equation  known as little 's formula  is particularly useful because it is valid for any scheduling algorithm and arrival distribution  we can use little 's formula to compute one of the three variables if we know the other two for example  if we know that 7 processes arrive every second  on average   and that there are normally 14 processes in the queue  then we can compute the average waiting time per process as 2 seconds  queueing analysis can be useful in comparing scheduling algorithms  but it also has limitations at the moment  the classes of algorithms and distributions that can be handled are fairly limited the mathematics of complicated algorithms and distributions can be difficult to work with thus  arrival and service distributions are often defined in mathematically tractable -but unrealistic-ways it is also generally necessary to make a number of independent assumptions  which may not be accurate as a result of these difficulties  queueing models are often only approximations of real systems  and the accuracy of the computed results may be questionable  216 chapter 5 performance statistics for fcfs performance statistics for sjf performance statistics for rr  q = 14  figure 5.17 evaluation of cpu schedulers by simulation  5.7.3 simulations to get a more accurate evaluation of scheduling algorithms  we can use simulations rumung simulations involves programming a model of the computer system software data structures represent the major components of the system the simulator has a variable representing a clock ; as this variable 's value is increased  the simulator modifies the system state to reflect the activities of the devices  the processes  and the scheduler as the simulation executes  statistics that indicate algorithm performance are gathered and printed  the data to drive the simulation can be generated in several ways the most common method uses a random-number generator that is programmed to generate processes  cpu burst times  arrivals  departures  and so on  according to probability distributions the distributions can be defined mathematically  uniform  exponential  poisson  or empirically if a distribution is to be defined empirically  measurements of the actual system under study are taken the results define the distribution of events in the real system ; this distribution can then be used to drive the simulation  a distribution-driven simulation may be inaccurate  however  because of relationships between successive events in the real system the frequency distribution indicates only how many instances of each event occur ; it does not indicate anything about the order of their occurrence to correct this problem  we can use trace tapes we create a trace tape by monitoring the real system and recording the sequence of actual events  figure 5.17   we then use this sequence to drive the simulation trace tapes provide an excellent way to compare two algorithms on exactly the same set of real inputs this method can produce accurate results for its inputs  simulations can be expensive  often requiring hours of computer time a more detailed simulation provides more accurate results  but it also takes more computer time in addition  trace tapes can require large amounts of storage 5.8 5.8 217 space finally  the design  coding  and debugging of the simulator can be a major task  5.7.4 implementation even a simulation is of limited accuracy the only con'lpletely accurate way to evaluate a scheduling algorithm is to code it up  put it in the operating system  and see how it works this approach puts the actual algorithm in the real system for evaluation under real operating conditions  the major difficulty with this approach is the high cost the expense is incurred not only in coding the algorithm and modifying the operating system to support it  along with its required data structures  but also in the reaction of the users to a constantly changing operating system most users are not interested in building a better operating system ; they merely want to get their processes executed and use their results a constantly changing operating system does not help the users to get their work done  another difficulty is that the environment in which the algorithm is used will change the environment will change not only in the usual way  as new programs are written and the types of problems change  but also as a result of the performance of the scheduler if short processes are given priority  then users may break larger processes into sets of smaller processes if interactive processes are given priority over noninteractive processes  then users may switch to interactive use  for example  researchers designed one system that classified interactive and noninteractive processes automatically by looking at the amount of terminal i/0 if a process did not input or output to the terminal in a 1-second interval  the process was classified as noninteractive and was moved to a lower-priority queue in response to this policy  one programmer modified his programs to write an arbitrary character to the terminal at regular intervals of less than 1 second the system gave his programs a high priority  even though the terminal output was completely meaningless  the most flexible scheduling algorithms are those that can be altered by the system managers or by the users so that they can be tuned for a specific application or set of applications a workstation that performs high-end graphical applications  for instance  may have scheduling needs different from those of a web server or file server some operating systemsparticularly several versions of unix-allow the system manager to fine-tune the scheduling parameters for a particular system configuration for example  solaris provides the dispadmin command to allow the system administrator to modify the parameters of the scheduling classes described  in section 5.6.1  another approach is to use apis that modify the priority of a process or thread the java  /posix  and /winapi/ provide such functions the downfall of this approach is that performance-tuning a system or application most often does not result in improved performance in more general situations  cpu scheduling is the task of selecting a waiting process from the ready queue and allocating the cpu to it the cpu is allocated to the selected process by the dispatcher  218 chapter 5 first-come  first-served  fcfs  scheduling is the simplest scheduling algorithm  but it can cause short processes to wait for very long processes shortestjob first  sjf  scheduling is provably optimal  providing the shortest average waiting time implementing sjf scheduling is difficult  howeve1 ~ because predicting the length of the next cpu burst is difficult the sjf algorithm is a special case of the general priority scheduling algorithm  which simply allocates the cpu to the highest-priority process both priority and sjf scheduling may suffer from starvation aging is a technique to prevent starvation  round-robin  rr  scheduling is more appropriate for a time-shared  interactive  system rr scheduling allocates the cpu to the first process in the ready queue for q time units  where q is the time quantum after q time units  if the process has not relinquished the cpu  it is preem.pted  and the process is put at the tail of the ready queue the major problem is the selection of the time quantum if the quantum is too large  rr scheduling degenerates to fcfs scheduling ; if the quantum is too small  scheduling overhead in the form of context-switch time becomes excessive  the fcfs algorithm is nonpreemptive ; the rr algorithm is preemptive the sjf and priority algorithms may be either preemptive or nonpreemptive  multilevel queue algorithms allow different algorithms to be used for different classes of processes the most common model includes a foreground interactive queue that uses rr scheduling and a background batch queue that uses fcfs scheduling multilevel feedback queues allow processes to move from one queue to another  many contemporary computer systems support multiple processors and allow each processor to schedule itself independently typically  each processor maintains its own private queue of processes  or threads   all of which are available to run additional issues related to multiprocessor scheduling include processor affinity  load balancing  and multicore processing as well as scheduling on virtualization systems  operating systems supporting threads at the kernel level must schedule threads-not processes-for execution this is the case with solaris and windows xp both of these systems schedule threads using preemptive  priority-based scheduling algorithms  including support for real-time threads  the linux process scheduler uses a priority-based algorithm with real-time support as well the scheduling algorithms for these three operating systems typically favor interactive over batch and cpu-bound processes  the wide variety of scheduling algorithms demands that we have methods to select among algorithms analytic methods use mathematical analysis to determine the performance of an algorithm simulation methods determine performance by imitating the scheduling algorithm on a representative sample of processes and computing the resulting performance however  simulation can at best provide an approximation of actual system performance ; the only reliable technique for evaluating a scheduling algorithm is to implencent the algorithm on an actual system and monitor its performance in a real-world environment  5.1 why is it important for the scheduler to distinguish t /0-bound programs from cpu-bound programs 219 5.2 a cpu-scheduling algorithm determines an order for the execution of its scheduled processes given n processes to be scheduled on one processor  how many different schedules are possible give a formula in tentls of n  5.3 consider a systenc running ten i/o-bound tasks and one cpu-bound task assume that the i/o-bound tasks issue an i/o operation once for every millisecond of cpu computing and that each i/0 operation takes 10 milliseconds to complete also assume that the context-switching overhead is 0.1 millisecond and that all processes are long-running tasks  describe the cpu utilization for a round-robin scheduler when  a the time quantum is 1 millisecond b the time quantum is 10 milliseconds 5.4 what advantage is there in having different time-quantum sizes at different levels of a multilevel queueing system 5.5 consider a system implementing multilevel queue scheduling what strategy can a computer user employ to maximize the amount of cpu time allocated to the user 's process 5.6 consider the scheduling algorithm in the solaris operating system for time-sharing threads  a what is the time quantum  in milliseconds  for a thread with priority 10 with priority 55 b assume that a thread with priority 35 has used its entire time quantum without blocking what new priority will the scheduler assign this thread c assume that a thread with priority 35 blocks for i/0 before its time quantum has expired what new priority will the scheduler assign this thread 5.7 explain the differences in how much the following scheduling algorithms discriminate in favor of short processes  a fcfs b rr c multilevel feedback queues 5.8 consider the exponential average formula used to predict the length of the next cpu burst what are the implications of assigning the following values to the parameters used by the algorithm a ex = 0 and to = 100 milliseconds b ex = 0.99 and to = 10 milliseconds 220 chapter 5 5.9 which of the following scheduling algorithms could result in starvation a first-come  first-served b shortest job first c round robin d priority 5.10 suppose that a scheduling algorithm  at the level of short-term cpu scheduling  favors those processes that have used the least processor time in the recent past why will this algorithm favor i/o-bound programs and yet not permanently starve cpu-bound programs 5.11 using the windows xp scheduling algorithm  determine the numeric priority of each of the following threads  a a thread in the realtimeyriority _class with a relative priority of highest b a thread in the normalyriority_class with a relative priority of normal c a thread in the highyriority _class with a relative priority of above..normal 5.12 consider a variant of the rr scheduling algorithm in which the entries in the ready queue are pointers to the pcbs  a what would be the effect of putting two pointers to the same process in the ready queue b what would be two major advantages and two disadvantages of this scheme c how would you modify the basic rr algorithm to achieve the same effect without the duplicate pointers 5.13 consider the following set of processes  with the length of the cpu burst given in milliseconds  process burst time priority  pt 10 3 p2 1 1 p3 2 3 p4 1 4 ps 5 2 221 the processes are assumed to have arrived in the order p1  p2  p3  p4  ps  all at time 0  a draw four gantt charts that illustrate the execution of these processes using the following scheduling algorithms  fcfs  sjf  nonpreemptive priority  a smaller priority number implies a higher priority   and rr  quantum = 1   b what is the turnaround time of each process for each of the scheduling algorithms in part a c what is the waiting ti1r1e of each process for each of these scheduling algorithms d which of the algorithms results in the minimum average waiting time  over all processes  5.14 the traditional unix scheduler enforces an inverse relationship between priority numbers and priorities  the higher the numbe1 ~ the lower the priority the scheduler recalculates process priorities once per second using the following function  priority =  recent cpu usage i 2  + base where base = 60 and recent cpu usage refers to a value indicating how often a process has used the cpu since priorities were last recalculated  assume that recent cpu usage for process p1 is 40  for process p2 is 18  and for process p3 is 10 what will be the new priorities for these three processes when priorities are recalculated based on this information  does the traditional unix scheduler raise or lower the relative priority of a cpu-bound process 5.15 discuss how the following pairs of scheduling criteria conflict in certain settings  a cpu utilization and response time b average turnaround time and maximum waiting time c i/0 device utilization and cpu utilization 5.16 consider a preemptive priority scheduling algorithm based on dynamically changing priorities larger priority numbers imply higher priority  when a process is waiting for the cpu  in the ready queue  but not running   its priority changes at a rate a ; when it is running  its priority changes at a rate ~  all processes are given a priority of 0 when they enter the ready queue the parameters a and ~ can be set to give many different scheduling algorithms  a what is the algorithm that results from ~ a 0 b what is the algorithm that results from a ~ 0 5.17 suppose that the following processes arrive for execution at the times indicated each process will run for the amount of time listed in answering the questions  use nonpreemptive scheduling  and base all 222 chapter 5 decisions on the information you have at the time the decision must be made  process arrival time burst time  pl 0.0 8 p2 0.4 4 p3 1.0 1 a what is the average turnaround time for these processes with the fcfs scheduling algorithm b what is the average turnaround time for these processes with the sjf scheduling algorithm c the sjf algorithm is supposed to improve performance  but notice that we chose to run process p1 at time 0 because we did not k11ow that two shorter processes would arrive soon compute what the average turnaround time will be if the cpu is left idle for the first 1 unit and then sjf scheduling is used remember that processes p1 and p2 are waiting durirtg this idle time  so their waiting time may increase this algorithm could be known as future-knowledge scheduling  feedback queues were originally implemented on the ctss system described in corbato et al  1962   this feedback queue scheduling system was analyzed by schrage  1967   the preemptive priority scheduling algorithm of exercise 5.16 was suggested by kleinrock  1975   anderson et al  1989   lewis and berg  1998   and philbin et al  1996  discuss thread scheduling multicore scheduling is examined in mcnairy and bhatia  2005  and kongetira et al  2005   scheduling techniques that take into account information regarding process execution times from previous runs are described in fisher  1981   hall et al  1996   and lowney et al  1993   fair-share schedulers are covered by henry  1984   woodside  1986   and kay and la uder  1988   scheduling policies used in the unix v operating system are described by bach  1987  ; those for unix freebsd 5.2 are presented by mckusick and neville-neil  2005  ; and those for the mach operating system are discussed by black  1990   love  2005  covers scheduling in lim.ix details of the ule scheduler can be found in roberson  2003   solaris scheduling is described by mauro and mcdougall  2007   solomon  1998   solomon and russinovich  2000   and russinovich and solomon  2005  discuss scheduling in windows internals butenhof  1997  and lewis and berg  1998  describe scheduling in pthreads systems siddha et al  2007  discuss scheduling challenges on multicore systems  part three 6.1 c er a cooperating process is one that can affect or be affected by other processes executing in the system cooperating processes can either directly share a logical address space  that is  both code and data  or be allowed to share data only through files or messages the former case is achieved through the use of threads  discussed in chapter 4 concurrent access to shared data may result in data inconsistency  however in this chapter  we discuss various mechanisms to ensure the orderly execution of cooperating processes that share a logical address space  so that data consistency is maintained  to introduce the critical-section problem  whose solutions can be used to ensure the consistency of shared data  to present both software and hardware solutions of the critical-section problem  to introduce the concept of an atomic transaction and describe mechanisms to ensure atomicity  in chapter 3  we developed a model of a system consisting of cooperating sequential processes or threads  all running asynchronously and possibly sharing data we illustrated this model with the producer-consumer problem  which is representative of operating systems specifically  in section 3.4.1  we described how a bounded buffer could be used to enable processes to share memory  let 's return to our consideration of the bounded buffer as we pointed out  our original solution allowed at most buffer_size  1 items in the buffer at the same time suppose we want to modify the algorithm to remedy this deficiency one possibility is to add an integer variable counter  initialized to 0 counter is incremented every time we add a new item to the buffer and is 225 226 chapter 6 decremented every time we remove one item from the buffer the code for the producer process can be modified as follows  while  true    i produce an item in nextproduced i while  counter = = buffer_size  ; i do nothing i buffer  in  = nextproduced ; in =  in + 1  % buffer_size ; counter + + ; the code for the consumer process can be modified as follows  while  true    while  counter = = 0  ; i do nothing i nextconsumed = buffer  out  ; out =  out + 1  % buffer_size ; counter ; i consume the item in nextconsumed i although both the producer and consumer routines shown above are correct separately  they may not function correctly when executed concurrently  as an illustration  suppose that the value of the variable counter is currently 5 and that the producer and consumer processes execute the statements counter + + and counter concurrently following the execution of these two statements  the value of the variable counter may be 4  5  or 6 ! the only correct result  though  is counter = = 5  which is generated correctly if the producer and consumer execute separately  we can show that the value of counter may be incorrect as follows note that the statement counter + + may be implemented in machine language  on a typical machine  as register1 = counter register1 = register1 + 1 counter = register1 where register1 is one of the local cpu registers similarly  the statement register2 counter is implemented as follows  register2 = counter register2 = register2 ~ 1 counter = register2 where again register2 is on eo the local cpu registers even though register1 and register2 may be the same physical register  an accumulator  say   remember that the contents of this register will be saved and restored by the interrupt handler  section 1.2.3   6.2 6.2 227 the concurrent execution of counter + + and counter is equivalent to a sequential execution in which the lower-level statements presented previously are interleaved in some arbitrary order  but the order within each high-level statement is preserved   one such interleaving is to  producer execute register1 = counter  register1 = 5  t1  producer execute register1 = register1 + 1  register1 = 6  t2  consumer execute register2 = counter  register2 = 5  t3  consumer execute register2 = register2 1  register2 = 4  t4  producer execute counter = register1  counter = 6  ts  consumer execute counter = register2  counter = 4  notice that we have arrived at the incorrect state counter = = 4  indicating that four buffers are full  when  in fact  five buffers are full if we reversed the order of the statements at t4 and t5  we would arrive at the incorrect state counter = = 6  we would arrive at this incorrect state because we allowed both processes to manipulate the variable counter concurrently a situation like this  where several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place  is called a to guard against the race condition above  we need to ensure that only one process at a time can be manipulating the variable counter to make such a guarantee  we require that the processes be synchronized in some way  situations such as the one just described occur frequently in operating systems as different parts of the system manipulate resources furthermore  with the growth of multicore systems  there is an increased emphasis on developing multithreaded applications wherein several threads-which are quite possibly sharing data-are rmming in parallel on different processing cores clearly  we want any changes that result from such activities not to interfere with one another because of the importance of this issue  a major portion of this chapter is concerned with and amongst cooperating processes  consider a system consisting of n processes  po  p1    p11 _ i   each process has a segment of code  called a cdticall in which the process may be changing common variables  updating a table  writing a file  and so on  the important feature of the system is that  when one process is executing in its critical section  no other process is to be allowed to execute in its critical section that is  no two processes are executing in their critical sections at the same time the critical-section problem is to design a protocol that the processes can use to cooperate each process must request permission to enter its critical section the section of code implementing this request is the the critical section may be followed by an exit the remaining code is the the general structure of a typical process pi is shown in 228 chapter 6 do  i entry section i critical section i exit section i remainder section  while  true  ; figure 6.1 general structure of a typical process a  figure 6.1 the entry section and exit section are enclosed in boxes to highlight these important segments of code  a solution to the critical-section problem must satisfy the following three requirements  1 mutual exclusion if process pi is executing in its critical section  then no other processes can be executing in their critical sections  2 progress if no process is executing in its critical section and some processes wish to enter their critical sections  then only those processes that are not executing in their remainder sections can participate in deciding which will enter its critical section next  and this selection carmot be postponed indefinitely  bounded waiting there exists a bound  or limit  on the number of times that other processes are allowed to enter their critical sections after a process has made a request to enter its critical section and before that request is granted  we assume that each process is executing at a nonzero speed however  we can make no assumption concerning the relative of the n processes  at a given point in time  many kernel-mode processes may be active in the operating system as a result  the code implementing an operating system  kernel code  is subject to several possible race conditions consider as an example a kernel data structure that maintains a list of all open files in the system this list must be modified when a new file is opened or closed  adding the file to the list or removing it from the list   if two processes were to open files simultaneously  the separate updates to this list could result in a race condition  other kernel data structures that are prone to possible race conditions include structures for maintaining memory allocation  for maintaining process lists  and for interrupt handling it is up to kernel developers to ensure that the operating system is free from such race conditions  two general approaches are used to handle critical sections in operating systems   1  preemptive kernels and  2  nonpreemptive kernels a preemptive kernel allows a process to be preempted while it is running in kernel mode  a nonpreemptive kernel does not allow a process running in kernel mode 6.3 6.3 229 to be preempted ; a kernel-mode process will run until it exits kernel mode  blocks  or voluntarily yields control of the cpu obviously  a nonpreemptive kernel is essentially free from race conditions on kernel data structures  as only one process is active in the kernel at a time we can not say the same about preemptive kernels  so they must be carefully designed to ensure that shared kernel data are free from race conditions preemptive kernels are especially difficult to design for smp architectures  since in these environments it is possible for two kernel-mode processes to run simultaneously on different processors  why  then  would anyone favor a preemptive kernel over a nonpreemptive one a preemptive kernel is more suitable for real-time programming  as it will allow a real-time process to preempt a process currently running in the kernel  furthermore  a preemptive kernel may be more responsive  since there is less risk that a kernel-mode process will run for an arbitrarily long period before relinquishing the processor to waiting processes of course  this effect can be minimized by designing kernel code that does not behave in this way later in this chapter  we explore how various operating systems manage preemption within the kernel  next  we illustrate a classic software-based solution to the critical-section problem known as peterson 's solution because of the way modern computer architectures perform basic machine-language instructions  such as load and store  there are no guarantees that peterson 's solution will work correctly on such architectures howeve1 ~ we present the solution because it provides a good algorithmic description of solving the critical-section problem and illustrates some of the complexities involved in designing software that addresses the requirements of mutual exclusion  progress  and bomcded waiting  peterson 's solution is restricted to two processes that alternate execution between their critical sections and remainder sections the processes are numbered po and p1 for convenience  when presenting pi  we use pj to denote the other process ; that is  j equals 1  i  peterson 's solution requires the two processes to share two data items  int turn ; boolean flag  2  ; the variable turn indicates whose turn it is to enter its critical section that is  if turn = = i  then process pi is allowed to execute in its critical section the flag array is used to indicate if a process is ready to enter its critical section  for example  if flag  i  is true  this value indicates that pi is ready to enter its critical section with an explanation of these data structures complete  we are now ready to describe the algorithm shown in figure 6.2  to enter the critical section  process pi first sets flag  i  to be true and then sets turn to the value j  thereby asserting that if the other process wishes to enter the critical section  it can do so if both processes try to enter at the same time  turn will be set to both i and j at roughly the sance time only one of these assignments will last ; the other will occur but will be overwritten immediately  230 chapter 6 do  flag  i  = true ; turn = j ; while  flag  j  && turn j  ; critical section i flag  i  = false ; i remainder section  while  true  ; figure 6.2 the structure of process a in peterson 's solution  the eventual value of turn determines which of the two processes is allowed to enter its critical section first  we now prove that this solution is correct we need to show that  mutual exclusion is preserved  the progress requirement is satisfied  the bounded-waiting requirement is met  to prove property 1  we note that each p ; enters its critical section only if either flag  j  = = false or turn = = i also note that  if both processes can be executing in their critical sections at the same time  then flag  0  = = flag  1  = = true these two observations imply that po and p1 could not have successfully executed their while statements at about the same time  since the value of turn can be either 0 or 1 but camwt be both hence  one of the processes -say  pi -must have successfully executed the while statencent  whereas p ; had to execute at least one additional statement  turn = = j   however  at that time  flag  j  = = true and turn = = j  and this condition will persist as long as pi is in its critical section ; as a result  mutual exclusion is preserved  to prove properties 2 and 3  we note that a process p ; can be prevented from entering the critical section only if it is stuck in the while loop with the condition flag  j  = = true and turn = = = j ; this loop is the only one possible if pi is not ready to enter the critical section  then flag  j  = = false  and p ; can enter its critical section if pj has set flag  j  to true and is also executing in its while statement  then either turn = = = i or turn = = = j if turn = = i  then p ; will enter the critical section if turn = = j  then pi will enter the critical section however  once pi exits its critical section  it will reset flag  j  to false  allowing p ; to enter its critical section if pi resets flag  j  to true  it must also set turn to i  thus  since p ; does not change the value of the variable turn while executing the while statement  p ; will enter the critical section  progress  after at most one entry by p1  bounded waiting   6.4 6.4 231 do  acquire lock critical section i release lock i remainder section  while  true  ; figure 6.3 solution to the critical-section problem using locks  we have just described one software-based solution to the critical-section problem however  as mentioned  software-based solutions such as peterson 's are not guaranteed to work on modern computer architectures instead  we can generally state that any solution to the critical-section problem requires a simple tool-a lock race conditions are prevented by requiring that critical regions be protected by locks that is  a process must acquire a lock before entering a critical section ; it releases the lock when it exits the critical section  this is illustrated in figure 6.3  in the following discussions  we explore several more solutions to the critical-section problem using techniques ranging from hardware to softwarebased apis available to application programmers all these solutions are based on the premise of locking ; however  as we shall see  the designs of such locks can be quite sophisticated  we start by presenting some simple hardware instructions that are available on many systems and showing how they can be used effectively in solving the critical-section problem hardware features can make any programming task easier and improve system efficiency  the critical-section problem could be solved simply in a uniprocessor environment if we could prevent interrupts from occurring while a shared variable was being modified in this manner  we could be sure that the current sequence of instructions would be allowed to execute in order without preemption no other instructions would be run  so no unexpected modifications could be made to the shared variable this is often the approach taken by nonpreemptive kernels  unfortunately  this solution is not as feasible in a multiprocessor environment  disabling interrupts on a multiprocessor can be time consuming  as the boolean testandset  boolean target   boolean rv = target ; target = true ; return rv ;  figure 6.4 the definition of the testandset   instruction  232 chapter 6 do  while  testandset  &lock   ; ii do nothing ii critical section lock = false ; ii remainder section  while  true  ; figure 6.5 mutual-exclusion implementation with testandset    message is passed to all the processors this message passing delays entry into each critical section  and system efficiency decreases also consider the effect on a system 's clock if the clock is kept updated by interrupts  many modern computer systems therefore provide special hardware instructions that allow us either to test and modify the content of a word or to swap the contents of two words is  as one unin.terruptible unit we can use these special instructions to solve the critical-section problem in a relatively simple manner rather than discussing one specific instruction for one specific machine  we abstract the main concepts behind these types of instructions by describing the testandset   and swap   instructions  the testandset   instruction can be defined as shown in figure 6.4 the important characteristic of this instruction is that it is executed atomically  thus  if two testandset   instructions are executed simultaneously  each on a different cpu   they will be executed sequentially in some arbitrary order if the machine supports the testandset   instruction  then we can implement mutual exclusion by declaring a boolean variable lock  initialized to false  the structure of process p ; is shown in figure 6.5  the swap   instruction  in contrast to the testandset   instruction  operates on the contents of two words ; it is defined as shown in figure 6.6  like the testandset   instruction  it is executed atomically if the machine supports the swap   instruction  then mutual exclusion can be provided as follows a global boolean variable lock is declared and is initialized to false  in addition  each process has a local boolean variable key the structure of process p ; is shown in figure 6.7  although these algorithms satisfy the mutual-exclusion requirement  they do not satisfy the bounded-waiting requirement in figure 6.8  we present another algorithm using the testandset   instruction that satisfies all the critical-section requirements the common data structures are void swap  boolean a  boolean b   boolean temp = a ; a b ; b = temp ;  figure 6.6 the definition of the swap   instruction  6.4 do  key = true ; while  key = = true  swap  &lock  &key  ; ii critical section lock = false ; ii remainder section  while  true  ; figure 6.7 mutual-exclusion implementation with the swap   instruction  boolean waiting  n  ; boolean lock ; 233 these data structures are initialized to false to prove that the mutualexclusion requirement is met  we note that process p ; can enter its critical section only if either waiting  i  = = false or key = = false the value of key can become false only if the testandset   is executed the first process to execute the testandset   will find key = = false ; all others must wait the variable waiting  i  can become false only if another process leaves its critical section ; only one waiting  i  is set to false  maintaining the mutual-exclusion requirement  do  waiting  i  = true ; key = true ; while  waiting  i  && key  key = testandset  &lock  ; waiting  i  = false ; ii critical section j =  i + 1  % n ; while   j ! = i  && ! waiting  j   j =  j + 1  % n ; if  j = = i  lock = false ; else waiting  j  = false ; ii remainder section  while  true  ; figure 6.8 bounded-waiting mutual exclusion with testandset    234 chapter 6 6.5 to prove that the progress requirement is met  we note that the arguments presented for mutual exclusion also apply here  since a process exiting the critical section either sets lock to false or sets waiting  j  to false both allow a process that is waiting to enter its critical section to proceed  to prove that the bounded-waiting requirement is met  we note that  when a process leaves its critical section  it scans the array waiting in the cyclic ordering  i + 1  i + 2    n 1  0    i 1   it designates the first process in this ordering that is in the entry section  waiting  j  = = true  as the next one to enter the critical section any process waiting to enter its critical section will thus do so within n  1 turns  unfortunately for hardware designers  implementing atomic testandset   instructions on multiprocessors is not a trivial task such implementations are discussed in books on computer architecture  the hardware-based solutions to the critical-section problem presented in section 6.4 are complicated for application programmers to use to overcmrte this difficulty  we can use a synchronization tool called a a semaphore s is an integer variable that  apart from initialization  is accessed only through two standard atomic operations  wait   and signal    the wait   operation was originally termed p  from the dutch proberen  to test  ; signal   was originally called v  from verhogen  to increment   the definition of wait   is as follows  wait  s    while s = 0 ii no-op s ' the definition of signal   is as follows  signal  s   s + + ;  all modifications to the integer value of the semaphore in the wait   and signal   operations must be executed indivisibly that is  when one process modifies the semaphore value  no other process can simultaneously modify that same semaphore value in addition  in the case of wait  s   the testing of the integer value of s  s  s 0   as well as its possible modification  s   must be executed without interruption we shall see how these operations can be implemented in section 6.5.2 ; first  let us see how semaphores can be used  6.5.1 usage operating systems often distinguish between counting and binary semaphores  the value of a counting semaphore can range over an unrestricted domain  the value of a binary semaphore can range only between 0 and 1 on some 6.5 235 systems  binary semaphores are lmown as mutex locks  as they are locks that provide mutual exclusion  we can use binary semaphores to deal with the critical-section problem or mljltiple processes then processes share a semaphore  mutex  initialized to 1  each process pi is organized as shown in figure 6.9  counting semaphores can be used to control access to a given resource consisting of a finite number o instances the semaphore is initialized to the number of resources available each process that wishes to use a resource performs a wait   operation on the semaphore  thereby decrementing the count   when a process releases a resource  it performs a signal   operation  incrementing the count   when the count for the semaphore goes to 0  all resources are being used after that  processes that wish to use a resource will block until the count becomes greater than 0  we can also use semaphores to solve various synchronization problems  for example  consider two concurrently numing processes  p1 with a statement 51 and p2 with a statement 52  suppose we require that 52 be executed only after 51 has completed we can implement this scheme readily by letting p1 and p2 share a common semaphore synch  initialized to 0  and by inserting the statements 51 ; signal  synch  ; in process p1 and the statements wait  synch  ; 52 ; in process p2 because synch is initialized to 0  p2 will execute 52 only after p1 has invoked signal  synch   which is after statement 51 has been executed  6.5.2 implementation the main disadvantage of the semaphore definition given here is thatit requires while a process is in its critical section  any other process that tries to enter its critical section must loop continuously in the entry code this continual looping is clearly a problem in a real multiprogramming system  do  wait  mutex  ; ii critical section signal  mutex  ; ii remainder section  while  true  ; figure 6.9 mutual-exclusion implementation with semaphores  236 chapter 6 where a single cpu is shared among ncany processes busy waiting wastes cpu cycles that some other process might be able to use productively this type of semaphore is also called a because the process spins while waiting for the lock  spinlocks do have an advantage in that no context switch is required when a process must wait on a lock  and a context switch may take considerable time thus  when locks are expected to be held for short times  spinlocks are useful ; they are often employed on multiprocessor systems where one thread can spin on one processor while another thread performs its critical section on another processor  to overcome the need for busy waiting  we can modify the definition of the wait   and signal   semaphore operations when a process executes the wait   operation and finds that the semaphore value is not positive  it must wait however  rather than engaging in busy waiting  the process can block itself the block operation places a process into a waiting queue associated with the semaphore  and the state of the process is switched to the waiting state then control is transferred to the cpu scheduler  which selects another process to execute  a process that is blocked  waiting on a semaphore s  should be restarted when some other process executes a signal   operation the process is restarted by a wakeup   operation  which changes the process from the waiting state to the ready state the process is then placed in the ready queue  the cpu may or may not be switched from the running process to the newly ready process  depending on the cpu-scheduling algorithm  to implement semaphores under this definition  we define a semaphore as a c ' struct  typedef struct  int value ; struct process list ;  semaphore ; each semaphore has an integer value and a list of processes list when a process must wait on a semaphore  it is added to the list of processes a signal   operation removes one process from the list of waiting processes and awakens that process  the wait   semaphore operation can now be defined as wait  semaphore s   s value ;  if  s value 0    add this process to s list ; block   ; the signal   semaphore operation can now be defined as signal  semaphore s   s value + + ; if  s value = 0   6.5 remove a process p fron s list ; wakeup  p  ;   237 the block   operation suspends the process that invokes it the wakeup  p  operation resumes the execution of a blocked process p these two operations are provided by the operating system as basic system calls  note that in this implementation  semaphore values may be negative  although semaphore values are never negative under the classical definition of semaphores with busy waiting if a semaphore value is negative  its magnitude is the number of processes waiting on that semaphore this fact results from switching the order of the decrement and the test in the implementation of the wait   operation  the list of waiting processes can be easily implemented by a link field in each process control block  pcb   each semaphore contains an integer value and a pointer to a list of pcbs one way to add and rernove processes from the list so as to ensure bounded waiting is to use a fifo queue  where the semaphore contains both head and tail pointers to the queue in general  howeve1 ~ the list can use any queueing strategy correct usage of semaphores does not depend on a particular queueing strategy for the semaphore lists  it is critical that semaphores be executed atomically we must guarantee that no two processes can execute wait   and signal   operations on the same semaphore at the same time this is a critical-section problem ; and in a single-processor environment  that is  where only one cpu exists   we can solve it by simply inhibiting interrupts during the time the wait   and signal   operations are executing this scheme works in a single-processor environment because  once interrupts are inhibited  instructions from different processes can not be interleaved only the currently running process executes until interrupts are reenabled and the scheduler can regain control  in a multiprocessor environment  interrupts must be disabled on every processor ; otherwise  instructions from different processes  running on different processors  may be interleaved in some arbitrary way disabling interrupts on every processor can be a difficult task and furthermore can seriously diminish performance therefore  smp systems must provide alternative locking techniques-such as spinlocks-to ensure that wait   and signal   are performed atomically  it is important to admit that we have not completely eliminated busy waiting with this definition of the wait   and signal   operations rather  we have moved busy waiting from the entry section to the critical sections of application programs furthermore  we have limited busy waiting to the critical sections of the wait   and signal   opera times  and these sections are short  if properly coded  they sbould be no more than about ten instructions   thus  the critical section is almost never occupied  and busy waiting occurs rarely  and then for only a short time an entirely different situation exists with application programs whose critical sections may be long  minutes or 238 chapter 6 even hours  or may almost always be occupied in such casesf busy waiting is extremely inefficient  6.5.3 deadlocks and starvation the implementation of a semaphore with a waiting queue may result in a situation where two or more processes are waiting indefinitely for an event that can be caused only by one of the waiting processes the event in question is the execution of a signal   when such a state is reached  these processes are said to be to illustrate this  we consider a system consisting of two processes  po and p1  each accessing two semaphores  s and q  set to the value 1  po wait  s  ; wait  q  ; signal  s  ; signal  q  ; pl wait  q  ; wait  s  ; signal  q  ; signal  s  ; suppose that po executes wait  s  and then p1 executes wait  q   when po executes wait  q   it must wait until p1 executes signal  q   similarly  when p1 executes wait  s   it must wait until po executes signal  s   since these signal   operations cam1ot be executed  po and p1 are deadlocked  we say that a set of processes is in a deadlock state when every process in the set is waiting for an event that can be caused only by another process in the set the events with which we are mainly concerned here are resource acquisition and release however  other types of events may result in deadlocks  as we show in chapter 7 in that chapter  we describe various mechanisms for dealing with the deadlock problem  another problem related to deadlocks is or a situation in which processes wait indefinitely within the semaphore  indefinite blocking may occur if we remove processes from the list associated with a semaphore in lifo  last-in  first-out  order  6.5.4 priority inversion a scheduling challenge arises when a higher-priority process needs to read or modify kernel data that are currently being accessed by a lower-priority process-or a chain of lower-priority processes since kernel data are typically protected with a lock  the higher-priority process will have to wait for a lower-priority one to finish with the resource the situation becomes more complicated if the lower-priority process is preempted in favor of another process with a higher priority as an example  assume we have three processes  lf m  and h  whose priorities follow the order l m h assume that process h requires resource r  which is currently being accessed by process l  ordinarily  process h would wait for l to finish using resource r however  now suppose that process m becomes runnable  thereby preempting process 6.6 6.6 239 priority inversion and the mars pathfinder priority inversion can be more than a scheduling inconvenience on systems with tight time constraints  such as real-time systems-see chapter 19   priority inversion can cause a process to take longer than it should to accomplish a task when that happens  other failures can cascade  resulting in system failure  consider the mars pathfinde1 ~ a nasa space probe that landed a robot  the sojourner rove1 ~ on mars in 1997 to conduct experiments shortly after the sojourner began operating  it started to experience frequent computer resets  each reset reinitialized all hardware and software  including communications  if the problem had not been solved  the sojourner would have failed in its mission  the problem was caused by the fact that one high-priority task  bcdist  was taking longer than expected to complete its work this task was being forced to wait for a shared resource that was held by the lower-priority asi/met task  which in turn was preempted by multiple medium-priority tasks the bcdist task would stall waiting for the shared resource  and ultimately the bc_sched task would discover the problem and perform the reset the sojourner was suffering from a typical case of priority inversion  the operating system on the sojourner was vxworks  see section 19.6   which had a global variable to enable priority inheritance on all semaphores  after testing  the variable was set on the sojourner  on mars !   and the problem was solved  a full description of the problem  its detection  and its solution was written by the software team lead and is available at research.microsoft.com/ mbj /marsyathfinder i authoritative_account.html  l indirectly  a process with a lower priority-process m-has affected how long process h must wait for l to relinquish resource r  this problem is known as it occurs only in systems with more than two priorities  so one solution is to have only two priorities that is insufficient for most general-purpose operating systems  however typically these systems solve the problem by implementing a 2tic x,u   according to this protocol  all processes that are accessing resources needed by a higher-priority process inherit the higher priority until they are finished with the resources in question when they are finished  their priorities revert to their original values in the exan1.ple above  a priority-inheritance protocol would allow process l to temporarily inherit the priority of process h  thereby preventing process m from preempting its execution when process l had finished using resource r  it would relinquish its inherited priority from hand assume its original priority because resource r would now be available  process h-not m-would run next  in this section  we present a number of synchronization problems as examples of a large class of concurrency-control problems these problems are used for 240 chapter 6 do  ii produce an item in nextp wait  empty  ; wait  mutex  ; ii add nextp to buffer signal  mutex  ; signal  full  ;  while  true  ; figure 6.10 the structure of the producer process  testing nearly every newly proposed synchronization scheme in our solutions to the problems  we use semaphores for synchronization  6.6.1 the bounded-buffer problem the bounded-buffer problem was introduced in section 6.1 ; it is commonly used to illustrate the power of synchronization primitives here  we present a general structure of this scheme without committing ourselves to any particular implementation ; we provide a related programming project in the exercises at the end of the chapter  we assume that the pool consists of n buffers  each capable of holding one item the mutex semaphore provides mutual exclusion for accesses to the buffer pool and is initialized to the value 1 the empty and full semaphores comct the number of empty and full buffers the semaphore empty is initialized to the value n ; the semaphore full is initialized to the value 0  the code for the producer process is shown in figure 6.10 ; the code for the consumer process is shown in figure 6.11 note the symmetry between the producer and the consumer we can interpret this code as the producer producing full buffers for the consumer or as the consumer producing empty buffers for the producer  do  wait  full  ; wait  mutex  ; ii remove an item from buffer to nextc signal  mutex  ; signal  empty  ; ii consume the item in nextc  while  true  ; figure 6.11 the structure of the consumer process  6.6 241 6.6.2 the readers-writers problem suppose that a database is to be shared among several concurrent processes  some of these processes may want only to read the database  whereas others may want to update  that is  to read and write  the database we distinguish between these two types of processes by referring to the former as readers and to the latter as writers obviously  if two readers access the shared data simultaneously  no adverse effects will result however  if a writer and some other process  either a reader or a writer  access the database simultaneously  chaos may ensue  to ensure that these difficulties do not arise  we require that the writers have exclusive access to the shared database while writing to the database this synchronization problem is referred to as the readers-writers problem since it was originally stated  it has been used to test nearly every new synchronization primitive the readers-writers problem has several variations  all involving priorities the simplest one  referred to as the first readers-writers problem  requires that no reader be kept waiting unless a writer has already obtained permission to use the shared object in other words  no reader should wait for other readers to finish simply because a writer is waiting the second readerswriters problem requires that  once a writer is ready  that writer performs its write as soon as possible in other words  if a writer is waiting to access the object  no new readers may start reading  a solution to either problem may result in starvation in the first case  writers may starve ; in the second case  readers may starve for this reason  other variants of the problem have been proposed next  we present a solution to the first readers-writers problem refer to the bibliographical notes at the end of the chapter for references describing starvation-free solutions to the second readers-writers problem  in the solution to the first readers-writers problem  the reader processes share the following data structures  semaphore mutex  wrt ; int readcount ; the semaphores mutex and wrt are initialized to 1 ; readcount is initialized to 0 the semaphore wrt is common to both reader and writer processes  the mutex semaphore is used to ensure mutual exclusion when the variable readcount is updated the readcount variable keeps track of how many processes are currently reading the object the semaphore wrt functions as a mutual-exclusion semaphore for the writers it is also used by the first or last reader that enters or exits the critical section it is not used by readers who enter or exit while other readers are in their critical sections  the code for a writer process is shown in figure 6.12 ; the code for a reader process is shown in figure 6.13 note that  if a writer is in the critical section and n readers are waiting  then one reader is queued on wrt  and n 1 readers are queued on mutex also observe that  when a writer executes signal  wrt   we may resume the execution of either the waiting readers or a single waiting writer the selection is made by the scheduler  the readers-writers problem and its solutions have been generalized to provide locks on some systems acquiring a reader-writer lock 242 chapter 6 do  wait  wrt  ; ii writing is performed signal  wrt  ;  while  true  ; figure 6 i 2 the structure of a writer process  requires specifying the mode of the lock either read or write access when a process wishes only to read shared data  it requests the reader-writer lock in read mode ; a process wishing to modify the shared data must request the lock in write mode multiple processes are permitted to concurrently acquire a reader-writer lock in read mode  but only one process may acquire the lock for writing  as exclusive access is required for writers  reader-writer locks are most useful in the following situations  in applications where it is easy to identify which processes only read shared data and which processes only write shared data  in applications that have more readers than writers this is because readerwriter locks generally require more overhead to establish than semaphores or mutual-exclusion locks the increased concurrency of allowing multiple readers compensates for the overhead involved in setting up the readerwriter lock  6.6.3 the dining-philosophers problem consider five philosophers who spend their lives thinking and eating the philosophers share a circular table surrounded by five chairs  each belonging do  wait  mutex  ; readcount + + ; if  readcount 1  wait  wrt  ; signal  mutex  ; ii reading is performed wait  mutex  ; readcount ; if  readcount 0  signal  wrt  ; signal  mutex  ;  while  true  ; figure 6.13 the structure of a reader process  6.6 243 figure 6.14 the situation of the dining philosophers  to one philosopher in the center of the table is a bowl of rice  and the table is laid with five single chopsticks  figure 6.14   when a philosopher thinks  she does not interact with her colleagues from time to time  a philosopher gets hungry and tries to pick up the two chopsticks that are closest to her  the chopsticks that are between her and her left and right neighbors   a philosopher may pick up only one chopstick at a time obviously  she cam1ot pick up a chopstick that is already in the hand of a neighbor when a htmgry philosopher has both her chopsticks at the same time  she eats without releasing her chopsticks when she is finished eating  she puts down both of her chopsticks and starts thinking again  the dining-philosophers problem is considered a classic synchronization problem neither because of its practical importance nor because computer scientists dislike philosophers but because it is an example of a large class of concurrency-control problems it is a simple representation of the need to allocate several resources among several processes in a deadlock-free and starvation-free mam1er  one simple solution is to represent each chopstick with a semaphore a philosopher tries to grab a chopstick by executing await   operation on that semaphore ; she releases her chopsticks by executing the signal   operation on the appropriate semaphores thus  the shared data are semaphore chopstick  5  ; where all the elements of chopstick are initialized to 1 the structure of philosopher i is shown in figure 6.15  although this solution guarantees that no two neighbors are eating simultaneously  it nevertheless must be rejected because it could create a deadlock suppose that all five philosophers become hungry simultaneously and each grabs her left chopstick all the elements of chopstick will now be equal to 0 when each philosopher tries to grab her right chopstick  she will be delayed forever  several possible remedies to the deadlock problem are listed next  allow at most four philosophers to be sitting simultaneously at the table  244 chapter 6 6.7 do  wait  chopstick  i   ; wait  chopstick   i + l  % 5   ; i i eat signal  chopstick  i   ; signal  chopstick   i + l  % 5   ; ii think  while  true  ; figure 6.15 the structure of philosopher i  allow a philosopher to pick up her chopsticks only if both chopsticks are available  to do this  she must pick them up in a critical section   use an asymmetric solution ; that is  an odd philosopher picks up first her left chopstick and then her right chopstick  whereas an even philosopher picks up her right chopstick and then her left chopstick in section 6.7  we present a solution to the dining-philosophers problem that ensures freedom from deadlocks note  however  that any satisfactory solution to the dining-philosophers problem must guard against the possibility that one of the philosophers will starve to death a deadlock-free solution does not necessarily eliminate the possibility of starvation  although semaphores provide a convenient and effective mechanism for process synchronization  using them incorrectly can result in timing errors that are difficult to detect  since these errors happen only if some particular execution sequences take place and these sequences do not always occur  we have seen an example of such errors in the use of counters in our solution to the producer-consumer problem  section 6.1   in that example  the timing problem happened only rarely  and even then the counter value appeared to be reasonable-off by only 1 nevertheless  the solution is obviously not an acceptable one it is for this reason that semaphores were introduced in the first place  unfortunately  such timing errors can still occur when semaphores are used to illustrate how  we review the semaphore solution to the critical-section problem all processes share a semaphore variable mutex  which is initialized to 1 each process must execute wait  mutex  before entering the critical section and signal  mutex  afterward if this sequence is not observed  two processes may be in their critical sections simultaneously next  we examine the various difficulties that may result note that these difficulties will arise even if a single process is not well behaved this situation may be caused by an honest programming error or an uncooperative programmer  6.7 245 suppose that a process interchanges the order in which the wait   and signal   operations on the semaphore mutex are executed  resulting in the following execution  signal  mutex  ; critical section wait  mutex  ; in this situation  several processes may be executing in their critical sections simultaneously  violating the mutual-exclusion requirement this error may be discovered only if several processes are simultaneously active in their critical sections note that this situation may not always be reproducible  suppose that a process replaces signal  mutex  with wait  mutex   that is  it executes wait  mutex  ; critical section wait  mutex  ; in this case  a deadlock will occur  suppose that a process omits the wait  mutex   or the signal  mutex   or both in this case  either mutual exclusion is violated or a deadlock will occur  these examples illustrate that various types of errors can be generated easily when programmers use sencaphores incorrectly to solve the critical-section problem similar problems may arise in the other synchronization models discussed in section 6.6  to deal with such errors  researchers have developed high-level language constructs in this section  we describe one fundamental high-level synchronization construct-the monitor type  6.7.1 usage a abstract data type or adt encapsulates private data with public methods to operate on that data a monitor type is an adt which presents a set of programmer-defined operations that are provided mutual exclusion within the monitor the monitor type also contains the declaration of variables whose values define the state of an instance of that type  along with the bodies of procedures or functions that operate on those variables the syntax of a monitor type is shown in figure 6.16 the representation of a monitor type can not be used directly by the various processes thus  a procedure defined within a monitor can access only those variables declared locally within the monitor and its formal parameters similarly  the local variables of a monitor can be accessed by only the local procedures  246 chapter 6 monitor rrwnitor name  ii shared variable declarations procedure p1        procedure p2        procedure pn        initialization code         figure 6.16 syntax of a monitor  the monitor construct ensures that only one process at a time is active within the monitor consequently  the programmer does not need to code this synchronization constraint explicitly  figure 6.17   howeve1 ~ the monitor construct  as defined so fa1 ~ is not sufficiently powerful for modeling some synchronization schemes for this purpose  we need to define additional synchronization mechanisms these mechanisms are provided by the condition construct a programmer who needs to write a tailor-made synchronization scheme can define one or more variables of type condition  condition x  y ; the only operations that can be invoked on a condition variable are wait   and signal    the operation x wait   ; means that the process invoking this operation is suspended until another process invokes x signal   ; the x signal   operation resumes exactly one suspended process if no process is suspended  then the signal   operation has no effect ; that is  the state of x is the same as if the operation had never been executed  figure shared data operations initialization code 6.7 figure 6.17 schematic view of a monitor  247 6.18   contrast this operation with the signal   operation associated with semaphores  which always affects the state of the semaphore  now suppose that  when the x signal   operation is invoked by a process p  there exists a suspended process q associated with condition x clearly  if the suspended process q is allowed to resume its execution  the signaling process p must wait otherwise  both p and q would be active simultaneously within the monitor note  however  that both processes can conceptually continue with their execution two possibilities exist  signal and wait p either waits until q leaves the monitor or waits for another condition  signal and continue q either waits until p leaves the monitor or waits for another condition  there are reasonable arguments in favor of adopting either option on the one hand  since p was already executing in the monitor  the signal-and-continue method seems more reasonable on the other hand  if we allow thread p to continue  then by the time q is resumed  the logical condition for which q was waiting may no longer hold a compromise between these two choices was adopted in the language concurrent pascal when thread p executes the signal operation  it imncediately leaves the monitor hence  q is immediately resumed  many programming languages have incorporated the idea of the monitor as described in this section  including concurrent pascal  mesa  c #  pronounced c-sharp   and java other languages-such as erlang-provide some type of concurrency support using a similar mechanism  248 chapter 6 queues associated with  x  y conditions ; -_  __ ~  ~  \   operations initialization code figure 6.18 monitor with condition variables  6.7.2 dining-philosophers solution using monitors next  we illustrate monitor concepts by presenting a deadlock-free solution to the dining-philosophers problem this solution imposes the restriction that a philosopher may pick up her chopsticks only if both of them are available to code this solution  we need to distinguish among three states in which we may find a philosopher for this purpose  we introduce the following data structure  enum  thinking  hungry  eating  state  5  ; philosopher i can set the variable state  i  = eating only if her two neighbors are not eating   state   i + 4  % 5  ! = eating  and  state   i + 1  % 5  ' = eating   we also need to declare condition sel  5  ; in which philosopher i can delay herself when she is hungry but is unable to obtain the chopsticks she needs  we are now in a position to describe our solution to the dining-philosophers problem the distribution of the chopsticks is controlled by the monitor diningphilosophers  whose definition is shown in figure 6.19 each philosopher  before starting to eat  must invoke the operation pickup    this act n'lay result in the suspension of the philosopher process after the successful completion of the operation  the philosopher may eat following this  the philosopher invokes 6.7 monitor dp   enum  thinking  hungry  eating  state  5  ; condition self  5  ; void pickup  int i   state  i  = hungry ; test  i  ;  if  state  i  ! = eating  self  i   wait   ; void putdown  int i   state  i  = thinking ; test   i + 4  % 5  ; test   i + 1  % 5  ;  void test  int i    if   state   i + 4  % 5  ! = eating  &&  state  i  = = hungry  &&   state   i + 1  % 5  ! = eating    state  i  = eating ; self  i  .signal   ; initialization_code     for  int i = 0 ; i 5 ; i + +  state  i  = thinking ; figure 6.19 a monitor solution to the dining-philosopher problem  249 the put down   operation thus  philosopher i must invoke the operations pickup   and put down   in the following sequence  diningphilosophers.pickup  i  ; eat diningphilosophers.putdown  i  ; it is easy to show that this solution ensures that no two neighbors are eating simultaneously and that no deadlocks will occur we note  however  that it is possible for a philosopher to starve to death we do not present a solution to this problem but rather leave it as an exercise for you  250 chapter 6 6.7.3 implementing a monitor using semaphores we now consider a possible implementation of the nwnitor mechanism using semaphores for each ltlonitor  a semaphore mutex  initialized to 1  is provided  a process must execute wait  mutex  before entering the n1onitor and must execute signal  mutex  after leaving the monitor  since a signaling process must wait until the resumed process either leaves or waits  an additional sernaphore  next  is introduced  initialized to 0 the signaling processes can use next to suspend themselves an integer variable next_count is also provided to count the number of processes suspended on next thus  each external procedure f is replaced by wait  mutex  ; body off if  next_count 0  signal  next  ; else signal  mutex  ; mutual exclusion within a monitor is ensured  we can now describe how condition variables are implemented as well  for each condition x  we introduce a semaphore x_sem and an integer variable x_count  both initialized to 0 the operation x wait   can now be implemented as x_count + + ; if  next_count 0  signal  next  ; else signal  mutex  ; wait  x_sem  ; x_count ; the operation x signal   can be implemented as if  x_count 0   next_count + + ; signal  x_sem  ; wait  next  ; next_count ;  this implementation is applicable to the definitions of monitors given by both hoare and brinch-hansen in some cases  however  the generality of the implementation is unnecessary  and a significant improvement in efficiency is possible we leave this problem to you in exercise 6.35  6.7.4 resuming processes within a monitor we turn now to the subject of process-resumption order within a monitor if several processes are suspended on condition x  and an x signal   operation monitor resourceallocator   boolean busy ; condition x ; void acquire  int time   if  busy  x.wait  time  ; busy = true ;  void release    busy = false ; x signal   ;  initialization_code    busy = false ;  6.7 figure 6.20 a monitor to allocate a single resource  251 is executed by some process  then how do we determine which of the suspended processes should be resumed next one simple solution is to use an fcfs ordering  so that the process that has been waiting the longest is resumed first in many circumstances  however  such a simple scheduling scheme is not adequate for this purpose  the construct can be used ; it has the form x.wait  c  ; where c is an integer expression that is evaluated when the wait   operation is executed the value of c  which is called a pdos ! ty is then stored with the name of the process that is suspended when x signal   is executed  the process with the smallest priority number is resumed next  to illustrate this new mechanism  consider the resourceallocator monitor shown in figure 6.20  which controls the allocation of a single resource among competing processes each process  when requesting an allocation of this resource  specifies the maximum time it plans to use the resource the monitor allocates the resource to the process that has the shortest time-allocation request a process that needs to access the resource in question must observe the following sequence  r.acquire  t  ; access the resource ; r release   ; where r is an instance of type resourceallocator  252 chapter 6 6.8 unfortunately  the monitor concept can not guarantee that the preceding access sequence will be observed in particular  the following problems can occur  a process might access a resource without first gaining access permission to the resource  a process ntight never release a resource once it has been granted access to the resource  a process might attempt to release a resource that it never requested  a process might request the same resource twice  without first releasing the resource   the same difficulties are encountered with the use of semaphores  and these difficulties are similar in nature to those that encouraged us to develop the monitor constructs in the first place previously  we had to worry about the correct use of semaphores now  we have to worry about the correct use of higher-level programmer-defined operations  with which the compiler can no longer assist us  one possible solution to the current problem is to include the resourceaccess operations within the resourceallocator monitor however  using this solution will mean that scheduling is done according to the built-in monitor-scheduling algorithm rather than the one we have coded  to ensure that the processes observe the appropriate sequences  we must inspect all the programs that make use of the resourceallocator monitor and its managed resource we must check two conditions to establish the correctness of this system first  user processes must always make their calls on the monitor in a correct sequence second  we must be sure that an uncooperative process does not simply ignore the mutual-exclusion gateway provided by the monitor and try to access the shared resource directly  without using the access protocols only if these two conditions can be ensured can we guarantee that no time-dependent errors will occur and that the scheduling algorithm will not be defeated  although this inspection may be possible for a small  static system  it is not reasonable for a large system or a dynamic system this access-control problem can be solved only through the use of additional mechanisms that are described in chapter 14  many programming languages have incorporated the idea of the monitor as described in this section  including concurrent pascal  mesa  c #  pronounced c-sharp   and java other languages-such as erlang-provide some type of concurrency support using a similar mechanism  we next describe the synchronization mechanisms provided by the solaris  windows xp  and linux operating systems  as well as the pthreads api we have chosen these three operating systems because they provide good examples of different approaches for synchronizing the kernel  and we have included the 6.8 253 java monitors java provides a monitor-like concurrency mechanisn1 for thread synchronization  every object in java has associated with it a single lock when a method is declared to be synchronized  calling the method requires owning the lock for the object we declare a synchronized method by placing the synchronized keyword in the method definition the following defines the safemethod   as synchronized  for example  public class simpleclass   public synchronized void safemethod    i implementation of safemethod   i  next  assume we create an object instance of simpleclass  such as  simpleclass sc = new simpleclass   ; invoking the sc safemethod   method requires owning the lock on the object instance sc if the lock is already owned by another thread  the thread calling the synchronized method blocks and is placed in the entry set for the object 's lock the entry set represents the set of threads waiting for the lock to become available if the lock is available when a synchronized method is called  the calling thread becomes the owner of the object 's lock and can enter the method the lock is released when the thread exits the method ; a thread from the entry set is then selected as the new owner of the lock  java also provides wait   and notify   methods  which are similar in function to the wait   and signal 0 statements for a monitor release 1.5 of the java language provides api support for semaphores  condition variables  and mutex locks  among other concurrency mechanisms  in the java util concurrent package  pthreads api because it is widely used for thread creation and synchronization by developers on unix and linux systems as you will see in this section  the synchronization methods available in these differing systems vary in subtle and significant ways  6.8.1 synchronization in solaris to control access to critical sections  solaris provides adaptive mutexes  condition variables  sernaphores  reader-writer locks  and turnstiles solaris implements semaphores and condition variables essentially as they are presented in sections 6.5 and 6.7 in this section  we describe adaptive mlltexes  readerwriter locks  and turnstiles  254 chapter 6 an protects access to every critical data item on a multiprocessor system  an adaptive mutex starts as a standard semaphore implemented as a spinlock if the data are locked and therefore already in use  the adaptive mutex does one of two things if the lock is held by a thread that is currently running on another cpu  the thread spins while waiting for the lock to become available  because the thread holding the lock is likely to finish soon if the thread holding the lock is not currently in run state  the thread blocks  going to sleep until it is awakened by the release of the lock it is put to sleep so that it will not spin while waiting  since the lock will not be freed very soon a lock held by a sleeping thread is likely to be in this category on a single-processor system  the thread holding the lock is never rwming if the lock is being tested by another thread  because only one thread can run at a time therefore  on this type of system  threads always sleep rather than spin if they encounter a lock  solaris uses the adaptive-mutex method to protect only data that are accessed by short code segments that is  a mutex is used if a lock will be held for less than a few hundred instructions if the code segment is longer than that  the spin-waiting method is exceedingly inefficient for these longer code segments  condition variables and semaphores are used if the desired lock is already held  the thread issues a wait and sleeps when a thread frees the lock  it issues a signal to the next sleeping thread in the queue the extra cost of putting a thread to sleep and waking it  and of the associated context switches  is less than the cost of wasting several hundred instructions waiting in a spinlock  reader-writer locks are used to protect data that are accessed frequently but are usually accessed in a read-only manner in these circumstances  reader-writer locks are more efficient than semaphores  because multiple threads can read data concurrently  whereas semaphores always serialize access to the data reader-writer locks are relatively expensive to implement  so again they are used only on long sections of code  solaris uses turnstiles to order the list of threads waiting to acquire either an adaptive n1.utex or a reader-writer lock a is a queue structure containing threads blocked on a lock for example  if one thread currently owns the lock for a synchronized object  all other threads trying to acquire the lock will block and enter the turnstile for that lock when the lock is released  the kernel selects a thread from the turnstile as the next owner of the lock  each synchronized object with at least one thread blocked on the object 's lock requires a separate turnstile however  rather than associating a turnstile with each synchronized object  solaris gives each kernel thread its own turnstile  because a thread can be blocked only on one object at a time  this is more efficient than having a turnstile for each object  the turnstile for the first thread to block on a synchronized object becomes the turnstile for the object itself threads subsequently blocking on the lock will be added to this turnstile when the initial thread ultimately releases the lock  it gains a new turnstile from a list of free turnstiles maintained by the kernel to prevent a priority inversion  turnstiles are organized according to a priorityinheritance protocol this means that if a lower-priority thread currently holds a lock on which a higher-priority thread is blocked  the thread with the lower priority will temporarily inherit the priority of the higher-priority thread upon releasing the lock  the thread will revert to its original priority  6.8 255 note that the locking mechanisms used by the kernel are implemented for user-level threads as well  so the same types of locks are available inside and outside the kernel a crucial implementation difference is the priorityinheritance protocol kernel-locking routines adhere to the kernel priorityinheritance methods used by the scheduler  as described in section 19.4 ; user-level thread-locking mechanisms do not provide this functionality  to optimize solaris performance  developers have refined and fine-tuned the locking methods because locks are used frequently and typically are used for crucial kernel functions  tuning their implem.entation and use can produce great performance gains  6.8.2 synchronization in windows xp the windows xp operating system is a multithreaded kernel that provides support for real-time applications and multiple processors when the windows xp kernel accesses a global resource on a uniprocessor system  it temporarily masks interrupts for all interrupt handlers that may also access the global resource on a multiprocessor system  windows xp protects access to global resources using spinlocks just as in solaris  the kernel uses spinlocks only to protect short code segments furthermore  for reasons of efficiency  the kernel ensures that a thread will never be preempted while holding a spinlock  for thread synchronization outside the kernel  windows xp provides ~   using a dispatcher object  threads synchronize according to several different mechanisms  including mutexes  semaphores  events  and timers the system protects shared data by requiring a tluead to gain ownership of a mutex to access the data and to release ownership when it is finished  semaphores behave as described in section 6.5 are similar to condition variables ; that is  they may notify a waiting thread when a desired condition occurs finally  timers are used to notify one  or more than one  thread that a specified amount of time has expired  dispatcher objects may be in either a signaled state or a nonsignaled state  a si ,7'2led indicates that an object is available and a thread will not block when acquiring the object a indicates that an object is not available and a thread will block when attempting to acquire the object we illustrate the state transitions of a mutex lock dispatcher object in figure 6.21  a relationship exists between the state of a dispatcher object and the state of a thread when a thread blocks on a nonsignaled dispatcher object  its state changes frmn ready to waiting  and the thread is placed in a waiting queue for that object when the state for the dispatcher object moves to signaled  the kernel checks whether any threads are waiting on the object if so  the owner thread releases mutex lock thread acquires mutex lock figure 6.21 mutex dispatcher object  256 chapter 6 kernel moves one thread -or possibly nlore threads-from the waiting state to the ready state  where they can resume executing the number of threads the kernel selects from the waiting queue depends on the type of dispatcher object for which it is waiting the kernel will select only one thread from the waiting queue for a mutex  since a mutex object may be owned by only a single thread for an event object  the kernel will select all threads that are waiting for the event  we can use a mutex lock as an illustration of dispatcher objects and thread states if a thread tries to acquire a mutex dispatcher object that is in a nonsignaled state  that thread will be suspended and placed in a waiting queue for the mutex object when the mutex moves to the signaled state  because another thread has released the lock on the mutex   the thread waiting at the front of the queue will be moved from the waiting state to the ready state and will acquire the mutex lock  we provide a programming project at the end of this chapter that uses mutex locks and semaphores in the win32 api  6.8.3 synchronization in linux prior to version 2.6  linux was a nonpreemptive kernel  meaning that a process running in kernel mode could not be preempted -even if a higher-priority process became available to run now  however  the linux kernel is fully preemptive  so a task can be preempted when it is running in the kernel the linux kernel provides spinlocks and semaphores  as well as readerwriter versions of these two locks  for locking in the kernel on smp machines  the fundamental locking mechanism is a spinlock  and the kernel is designed so that the spinlock is held only for short durations on single-processor machines  spinlocks are inappropriate for use and are replaced by enabling and disabling kernel preemption that is  on single-processor machines  rather than holding a spinlock  the kernel disables kernel preemption ; and rather than releasing the spinlock  it enables kernel preemption this is summarized below  disable kernel preemption  acquirespin lock  enable kernel preemption release spin lock  linux uses an interesting approach to disable and enable kernel preemption  it provides two simple system calls-preempldisable   and preempt_ enable   -for disabling and enabling kernel preemption in addition  however  the kernel is not preemptible if a kernel-mode task is holding a lock  to enforce this rule  each task irl the system has a thread-info structure containing a counter  preemplcount  to indicate the number of locks being held by the task when a lock is acquired  preemplcount is incremented it is decremented when a lock is released if the value of preempt_count for the task currently running is greater than zero  it is not safe to preempt the kernel  as this task currently holds a lock if the count is zero  the kernel can safely be interrupted  assuncing there are no outstanding calls to preempldisable     6.9 6.9 257 spinlocks-along with enabling and disabling kernel preemption-are used in the kernel only when a lock  or disabling kernel preemption  is held for a short duration when a lock must be held for a longer period  semaphores are appropriate for use  6.8.4 synchronization in pthreads the pthreads api provides mutex locks  condition variables  and read-write locks for thread synchronization this api is available for programmers and is not part of any particular kernel mutex locks represent the fundamental synchronization technique used with pthreads a mutex lock is used to protect critical sections of code-that is  a thread acquires the lock before entering a critical section and releases it upon exiting the critical section condition variables in pthreads behave much as described in section 6.7 read-write locks behave similarly to the locking mechanism described in section 6.6.2  many systems that implement pthreads also provide semaphores  although they are not part of the pthreads standard and instead belong to the posix sem extension other extensions to the pthreads api include spinlocks  but not all extensions are considered portable from one implementation to another we provide a programming project at the end of this chapter that uses pthreads mutex locks and semaphores  the mutual exclusion of critical sections ensures that the critical sections are executed atomically -that is  as one uninterruptible unit if two critical sections are instead executed concurrently  the result is equivalent to their sequential execution in some unknown order although this property is useful in many application domains  in many cases we would like to make sure that a critical section forms a single logical unit of work that either is performed in its entirety or is not performed at all an example is funds transfer  in which one account is debited and another is credited clearly  it is essential for data consistency either that both the credit and debit occur or that neither occurs  consistency of data  along with storage and retrieval of data  is a concern often associated with recently  there has been an upsurge of interest in using database-systems techniques in operating systems operating systems can be viewed as manipulators of data ; as such  they can benefit from the advanced techniques and models available from database research for instance  many of the ad hoc techniques used in operating systems to manage files could be more flexible and powerful if more formal database methods were used in their place in sections 6.9.2 to 6.9.4  we describe some of these database techniques and explain how they can be used by operating systems  first  however  we deal with the general issue of transaction atomicity it is this property that the database techniques are meant to address  6.9.1 system model a collection of instructions  or operations  that performs a single logical function is called a a major issue in processing transactions is the 258 chapter 6 preservation of atomicity despite the possibility of failures within the computer system  we can think of a transaction as a program unit that accesses and perhaps updates various data items that reside on a disk within some files from our point of view  such a transaction is simply a sequence of read and write operations terminated by either a commit operation or an abort operation  a commit operation signifies that the transaction has terminated its execution successfully  whereas an abort operation signifies that the transaction has transactional memory with the emergence of multicore systems has come increased pressure to develop multithreaded applications that take advantage of multiple processing cores however  multithreaded applications present an increased risk of race conditions and deadlocks traditionally  techniques such as locks  semaphores  and monitors have been used to address these issues however  provides an alternative strategy fordeveloping thread-safe concurrent applications  a is a sequence of memory read-write operations that are atomic if all operations in a transaction are completed  the memory transaction is committed ; otherwise  the operations must be aborted and rolled back the benefits of transactional memory can be obtained through features added to a programming language  consider an example suppose we have a function update   that modifies shared data traditionally  this function would be written using locks such as the following  update    acquire   ;  i modify shared data i release   ; however  using synchronization mechanisms such as locks and semaphores involves many potential problems  including deadlocks additionally  as the number of threads increases  traditional locking does not scale well  as an alternative to traditional methods  new features that take advantage of transactional memory can be added to a programming language in our example  suppose we add the construct atomic  s   which ensures that the operations in s execute as a transaction this allows us to rewrite the update   method as follows  update    atomic  i modify shared data i   continued on following page  6.9 259 transactional memory  continued  the advantage of using such a mechanism rather than locks is that the transactional memoi y system ~ not the developer-isrespon.sible for guaranteeing atomicity additionally  the system can identify which statements in atomic blocks can be executed concurrently  such as concurrent read access to a shared variable it is  of course  possible for a programmer to identify these situations and use reader-writer locks  but the task becomes increasingly difficult as the number ofthreads within anapplicationgrows  transactional memory can be implemented in either software or hardware  software transactional memory  stm   as the nam ~ suggests  imp lee ments transactional memory exclusivelyin software ~ nospecial hardware is needed stm works by inserting instrumentation code inside transaction blocks the code is inserted by a compiler and manages each transaction by examining where statements may run concurrently and where specific lowlevellockingis required hardware transactional memory  small htm  uses hardware cache hierarchies and cache coherency protocols to manage and resolve conflicts involving shared data residing in separate processors caches  htm requires no special code instmmentation and thus has less overhead than stm however  htm does require that existing cache hierarchies and cachecoherencyprotocolsbe modified to support transactional memory  transactional memory has existed for several years without widespread implementation however  the growth of multi core systems and the associated emphasis on concurrent programming have prompted a significant amolmt ofresearch in this area on the part of both academics and hard ware vendors  including intel and sun microsystems  ended its normal execution due to some logical error or a system failure  if a terminated transaction has completed its execution successfully  it is otherwise  it is since an aborted transaction may already have modified the data that it has accessed  the state of these data may not be the same as it would have been if the transaction had executed atomically so that atomicity is ensured  an aborted transaction must have no effect on the state of the data that it has already modified thus  the state of the data accessed by an aborted transaction must be restored to what it was just before the transaction started executing we say that such a transaction has been it is part of the responsibility of the system to ensure this property  to determ.ine how the system should ensure atomicity  we need first to identify the properties of devices used for storing the various data accessed by the transactions various types of storage media are distinguished by their relative speed  capacity  and resilience to failure  volatile storage information residing in volatile storage does not usually survive system crashes examples of such storage are main and cache merrwry access to volatile storage is extremely fast  both because of the speed of the memory access itself and because it is possible to access directly any data item in volatile storage  260 chapter 6 nonvolatile storage information residing in nonvolatile storage usually survives system crashes examples of m.edia for such storage are disks and magnetic tapes disks are more reliable than main memory but less reliable than magnetic tapes both disks and tapes  however  are subject to failure  which may result in loss of inform.ation currently  nonvolatile storage is slower than volatile storage by several orders of magnitude  becm1se disk and tape devices are electromechanical and require physical motion to access data  stable storage information residing in stable storage is never lost  never should be taken with a grain of salt  since theoretically such absolutes can not be guaranteed   to implement an approximation of such storage  we need to replicate information in several nonvolatile storage caches  usually disk  with independent failure modes and to update the inform.ation in a controlled manner  section 12.8   here  we are concerned only with ensuring transaction atomicity in an environment where failures result in the loss of inform.ation on volatile storage  6.9.2 log-based recovery one way to ensure atomicity is to record  on stable storage  information describing all the modifications made by the transaction to the various data it accesses the most widely used method for achieving this form of recording is here  the system maintains  on stable storage  a data structure called the each log record describes a single operation of a transaction write and has the following fields  transaction name the unique name of the transaction that performed the write operation data item name the unique name of the data item written old value the value of the data item prior to the write operation new value the value that the data item will have after the write other special log records exist to record significant events during transaction processing  such as the start of a transaction and the commit or abort of a transaction  before a transaction t ; starts its execution  the record t ; starts is written to the log during its execution  any write operation by t ; is preceded by the writing of the appropriate new record to the log when t ; commits  the record t ; commits is written to the log  because the information in the log is used in reconstructing the state of the data items accessed by the various transactions  we can not allow the actual update to a data item to take place before the corresponding log record is written out to stable storage we therefore require that  prior to execution of a wri te  x  operation  the log records corresponding to x be written onto stable storage  note the performance penalty inherent in this system two physical writes are required for every logical write requested also  more storage is needed  both for the data themselves and for the log recording the changes in cases 6.9 261 where the data are extremely important and fast failure recovery is necessary  however  the functionality is worth tl1e price  using the log  the system can handle any failure that does not result in the loss of information on nonvolatile storage the recovery algorithm uses two procedures  undo  t ;   which restores the value of all data updated by transaction t ; to the old values redo  t ;   which sets the value of all data updated by transaction t ; to the new values the set of data updated by t ; and the appropriate old and new values can be found in the log note that the undo and redo operations must be idempotent  that is  multiple executions must have the same result as does one execution  to guarantee correct behavior even if a failure occurs during the recovery process  if a transaction t ; aborts  then we can restore the state of the data that it has updated by simply executing undo  t ;   if a system failure occurs  we restore the state of all updated data by consulting the log to determine which transactions need to be redone and which need to be lmdone this classification of transactions is accomplished as follows  transaction t ; needs to be undone if the log contains the i ; starts record but does not contain the t ; corrnni ts record  transaction t ; needs to be redone if the log contains both the t ; starts and the t ; corrnni ts records  6.9.3 checkpoints when a system failure occurs  we must consult the log to determine which transactions need to be redone and which need to be undone in principle  we need to search the entire log to make these determinations there are two major drawbacks to this approach  the searching process is time consuming  most of the transactions that  according to our algorithm  need to be redone have already actually updated the data that the log says they need to modify although redoing the data modifications will cause no harm  due to idempotency   it will nevertheless cause recovery to take longer  to reduce these types of overhead  we introduce the concept of during execution  the system maintains the write-ahead log in addition  the system periodically performs checkpoints that require the following sequence of actions to take place  output all log records currently residing in volatile storage  usually main memory  onto stable storage  output all modified data residing in volatile storage to the stable storage  output a log record checkpoint onto stable storage  262 chapter 6 the presence of a checkpoint record in the log allows the systen'l to streamline its recovery procedure consider a transaction i ; that committed prior to the checkpoint the t ; commits record appears in the log before the checkpoint record any modifications made by t ; must have been written to stable storage either prior to the checkpoint or as part of the checkpoint itself thus  at recovery time  there is no need to perform a redo operation on t ;  this observation allows us to refine our previous recovery algorithm after a failure has occurred  the recovery routine examines the log to determine the most recent transaction t ; that started executing before the most recent checkpoint took place it finds such a transaction by searching the log backward to find the first checkpoint record and then finding the subsequent t ; start record  once transaction t ; has been identified  the redo and undo operations need be applied only to transaction t ; and all transactions t1 that started executing after transaction i ;  we 'll call these transactions set t the remainder of the log can be ignored the recovery operations that are required are as follows  for all transactions 'nc in t for which the record tic commits appears in the log  execute redo  t/c  for all transactions 'nc in t that have no tic commits record in the log  execute undo  t ; c   6.9.4 concurrent atomic transactions we have been considering an environment in which only one transaction can be executing at a time we now turn to the case where multiple transactions are active simultaneously because each transaction is atomic  the concurrent execution of transactions must be equivalent to the case where these transactions are executed serially in some arbih ary order this property  called can be maintained by simply executing each transaction within a critical section that is  all transactions share a common semaphore mutex  which is initialized to 1 when a transaction starts executing  its first action is to execute wai t  mutex   after the transaction either commits or aborts  it executes signal  mutex   although this scheme ensures the atomicity of all concurrently executing transactions  it is nevertheless too restrictive as we shall see  in many cases we can allow transactions to overlap their execution while maintaining serializability a number of different ensure serializability  and we describe these algorithms next  6.9.4.1 serializability consider a system with two data items  a and b  that are both read and written by two transactions  to and t1 suppose that these transactions are executed atomically in the order t0 followed by t1 this execution sequence  which is called a schedule  is represented in figure 6.22 in schedule 1 of figure 6.22  the sequence of instruction steps is in chronological order from top to bottom  with instructions of to appearing in the left column and instructions of t1 appearing in the right colunm  6.9 263 to t1 read  a  write  a  read  b  write  b  read  a  write  a  read  b  write  b  figure 6.22 schedule i  a serial schedule in which to is followed by t1 a schedule in which each transaction is executed atomically is called a a serial schedule consists of a sequence of instructions from various transactions wherein the instructions belonging to a particular transaction appear together thus  for a set of n transactions  there exist n ! different valid serial schedules each serial schedule is correct  because it is equivalent to the atomic execution of the various participating transactions in some arbitrary order  if we allow the two transactions to overlap their execution  then the resulting schedule is no longer serial a  cj,sef'i  al does not necessarily imply an incorrect execution  that is  an execution that is not equivalent to one represented by a serial schedule   to see that this is the case  we need to define the notion of nflic ; cing consider a schedule s in which there are two consecutive operations 0 ; and oi of transactions ~ and ti  respectively we say that 0 ; and oj conflict if they access the same data item and at least one of them is a write operation  to illustrate the concept of conflicting operations  we consider the nonserial schedule 2 of figure 6.23 the wri te  a  operation of to conflicts with the read  a  operation of t1 however  the wri te  a  operation of t1 does not conflict with the read  b  operation of to  because the two operations access different data items  to t1 read  a  write  a  read  a  write  a  read  b  write  b  read  b  write  b  figure 6.23 schedule 2  a concurrent serializable schedule  264 chapter 6 let 0 ; and 0 ; be consecutive operations of a schedule 5 if 0 ; and oi are operations of different transactions and 0 ; and oi do not conflict then we can swap the order of 0 ; and 0 ; to produce a new schedule 5' we expect 5 to be equivalent to 5 '  as all operations appear in the same order in both schedules  except for 0 ; and 0 1  whose order does not matter  we can illustrate the swapping idea by considering again schedule 2 of figure 6.23 as the wri te  a  operation of t1 does not conflict with the read  b  operation of t0  we can swap these operations to generate an equivalent schedule regardless of the initial system state  both schedules produce the same final system state continuing with this procedure of swapping nonconflicting operations  we get  swap the read  b  operation of to with the read  a  operation of t1  swap the write  b  operation of to with the write  a  operation of t1  swap the wri te  b  operation of to with the read  a  operation of t1  the final result of these swaps is schedule 1 in figure 6.22  which is a serial schedule thus  we have shown that schedule 2 is equivalent to a serial schedule this result implies that regardless of the initial system state  schedule 2 will produce the same final state as will some serial schedule  if a schedule 5 can be transformed into a serial schedule 5 ' swaps of nonconflicting operations  we say that a schedule 5 is izable thus  schedule 2 is conflict serializable  because it can be transformed into the serial schedule 1  6.9.4.2 locking protocol one way to ensure serializability is to associate a lock with each data item and to require that each transaction follow a that governs how locks are acquired and released there are various modes in which a data item can be locked in this section  we restrict our attention to two modes  shared if a transaction 7i has obtained a shared-mode lock  denoted by s  on data item q  then 1i can read this item but can not write q  exclusive if a transaction t ; has obtained an exclusive-mode lock  denoted by x  on data item q  then 7i can both read and write q  we require that every transaction request a lock in an appropriate m.ode on data item q  depending on the type of operations it will perform on q  to access data item q  transaction 1i must first lock q in the appropriate mode if q is not currently locked  then the lock is granted  and t ; can now access it however  if the data item q is currently locked by some other transaction  then t ; may have to wait more specifically  suppose that 1i requests an exclusive lock on q in this case  1i must wait until the lock on q is released  if t ; requests a shared lock on q  then t ; must wait if q is locked in exclusive mode otherwise  it can obtain the lock and access q notice that this scheme is quite similar to the readers-writers algorithm discussed in section 6.6.2  a transaction may unlock a data item that it locked at an earlier point  it must  however  hold a lock on a data item as long as it accesses that item  6.9 265 moreove1 ~ it is not always desirable for a transaction to unlock a data item immediately after its last access of that data item  because serializability may not be ensured  one protocol that ensures serializability is the this protocol requires that each transaction issue lock and unlock requests in two phases  growing phase a transaction may obtain locks but may not release any locks  shrinking phase a transaction may release locks but may not obtain any new locks  initially a transaction is in the growing phase the transaction acquires locks as needed once the transaction releases a lock  it enters the shrinking phase  and no more lock requests can be issued  the two-phase locking protocol ensures conflict serializability  exercise 6.14   it does not  however  ensure freedom from deadlock in addition  it is possible that  for a given set of transactions  there are conflict-serializable schedules that can not be obtained by use of the two-phase locking protocol  to improve performance over two-phase locking  we need either to have additional information about the transactions or to impose some structure or ordering on the set of data  6.9.4.3 timestamp-based protocols in the locking protocols described above  the order followed by pairs of conflicting transactions is determined at execution time another method for determining the serializability order is to select an order in advance the most common method for doing so is to use a ordering scheme  with each transaction ~ in the system  we associate a unique fixed timestamp  denoted by ts  t ;   this timestamp is assigned by the system before the transaction t ; starts execution if a transaction ~ has been assigned timestamp ts  ~   and later a new transaction ti enters the system  then ts  t ;  ts  tj   there are two simple methods for implementing this scheme  use the value of the system clock as the timestamp ; that is  a transaction 's timestamp is equal to the value of the clock when the transaction enters the system this method will not work for transactions that occur on separate systems or for processors that do not share a clock  use a logical counter as the timestamp ; that is  a transaction 's timestamp is equal to the value of the counter when the transaction enters the system  the counter is incremented after a new timestamp is assigned  the timestamps of the transactions determine the serializability order  thus  if ts  ti  ts  tj   then the system must ensure that the schedule produced is equivalent to a serial schedule in which transaction ~ appears before transaction tj  to implement this scheme  we associate with each data item q two timestamp values  266 chapter 6 w-timestamp  q  denotes the largest timestamp of any transaction that successfully executed wri te  q   r-timestamp  q  denotes the largest timestamp of any transaction that successfully executed read  q   these timestamps are updated whenever a new read  q  or wri te  q  instruction is executed  the timestamp ordering protocol ensures that any conflicting read and write operations are executed in timestamp order this protocol operates as follows  suppose that transaction t ; issues read  q   o if ts  ti  w-timestamp    then t ; needs to read a value of q that was already overwritten hence  the read operation is rejected  and t ; is rolled back  o if ts  t ;  2   w-timestamp  q   then the read operation is executed  and r-timestamp  q  is set to the maximum of r-timestamp  q  and ts  t ;   suppose that transaction t ; issues wri te  q   o if ts  t ;  r-timestamp  q   then the value of q that t ; is producing was needed previously and t ; assumed that this value would never be produced hence  the write operation is rejected  and t ; is rolled back  o if ts  t ;  w-timestamp  q   then t ; is attempting to write an obsolete value of q hence  this write operation is rejected  and t ; is rolled back  o otherwise  the write operation is executed  a transaction t ; that is rolled back as a result of either a read or write operation is assigned a new timestamp and is restarted  to illustrate this protocol  consider schedule 3 in figure 6.24  which includes transactions t2 and t3 we assume that a transaction is assigned a timestamp immediately before its first instruction thus  in schedule 3  ts  t2  ts  t3   and the schedule is possible under the timestamp protocol  this execution can also be produced by the two-phase locking protocol  howeve1 ~ some schedules are possible under the two-phase locking protocol but not under the timestamp protocol  and vice versa  t2 t3 read  b  read  b  write  b  read  a  read  a  write  a  figure 6.24 schedule 3  a schedule possible under the timestamp protocol  6.10 267 the timestamp protocol ensures conflict serializability this capability follows from the fact that conflicting operations are processed in timestamp order the protocol also ensures freedom fron1 deadlocl   because no transaction ever waits  given a collection of cooperating sequential processes that share data  mutual exclusion must be provided to ensure that a critical section of code is used by only one process or thread at a tince typically  computer hardware provides several operations that ensure mutual exclusion however  such hardware-based solutions are too complicated for most developers to use  semaphores overcome this obstacle semaphores can be used to solve various synchronization problems and can be implemented efficiently  especially if hardware support for atomic operations is available  various synchronization problems  such as the bounded-buffer problem  the readers-writers problem  and the dining-philosophers problem  are important mainly because they are examples of a large class of concurrency-control problems these problems are used to test nearly every newly proposed synchronization scheme  the operating system must provide the means to guard against timing errors several language constructs have been proposed to deal with these problems  monitors provide the synchronization mechanism for sharing abstract data types a condition variable provides a method by which a monitor procedure can block its execution until it is signaled to continue  operating systems also provide support for synchronization for example  solaris  windows xp  and linux provide mechanisms such as semaphores  mutexes  spinlocks  and condition variables to control access to shared data  the pthreads api provides support for mutexes and condition variables  a transaction is a program unit that must be executed atomically ; that is  either all the operations associated with it are executed to completion  or none are performed to ensure atomicity despite system failure  we can use a write-ahead log all updates are recorded on the log  which is kept in stable storage if a system crash occurs  the information in the log is used in restoring the state of the updated data items  which is accomplished by use of the undo and redo operations to reduce the overhead in searching the log after a system failure has occurred  we can use a checkpoint scheme  to ensure serializability when the execution of several transactions overlaps  we must use a concurrency-control scheme various concurrency-control schemes ensure serializability by delaying an operation or aborting the transaction that issued the operation the most common ones are locking protocols and timestamp ordering schemes  6.1 the first known correct software sohjtion to the critical-section problem for two processes was developed by dekker the two processes  p0 and p1  share the following variables  boolean flag  2  ; i initially false i int turn ; 268 chapter 6 do  flag  i  = true ; while  flag  j     if  turn = = j   flag  i  = false ; while  turn = = j  ; ii do nothing flag  i  = true ;  ii critical section turn = j ; flag  i  = false ; ii remainder section  while  true  ; figure 6.25 the structure of process a in dekker 's algorithm  the structure of process pi  i = = 0 or 1  is shown in figure 6.25 ; the other process is p1  j = = 1 or 0   prove that the algorithm satisfies all three requirements for the critical-section problem  6.2 explain why interrupts are not appropriate for implementing synchronization primitives in multiprocessor systems  6.3 the first known correct software solution to the critical-section problem for n processes with a lower bound on waiting of n  1 turns was presented by eisenberg and mcguire the processes share the following variables  enum pstate  idle  want_in  in_cs  ; pstate flag  n  ; int turn ; all the elements of flag are initially idle ; the initial value of turn is immaterial  between 0 and n-1   the structure of process pi is shown in figure 6.26 prove that the algorithm satisfies all three requiren'lents for the critical-section problem  6.4 write a monitor that implements an alarm clock that enables a calling program to delay itself for a specified number of tirne units  ticks   you may assume the existence of a real hardware clock that invokes a procedure hclc in your monitor at regular intervals  6.5 a file is to be shared among different processes  each of which has a unique number the file can be accessed simultaneously by several processes  subject to the following constraint  the sum of all unique do  while  true   flag  i  = want_in ; j = turn ;  while  j ! = i    if  flag  j  i = idle   j = turn ; else j =  j + 1  % n ; flag  i  j = 0 ; in_cs ; while   j n  &&  j j + + ; if   j = n  &&  turn break ; ii critical section j =  turn + 1  % n ; while  flag  j  = = idle  j =  j + 1  % n ; turn = j ; flag  i  = idle ; ii remainder section  while  true  ; i ii flag  j  ! = in_cs   i i i flag  turn  idle   figure 6.26 the structure of process a in eisenberg and mcguire 's algorithm  269 numbers associated with all the processes currently accessing the file must be less than n write a monitor to coordinate access to the file  6.6 the decrease_count   function in the previous exercise currently returns 0 if sufficient resources are available and -1 otherwise this leads to awkward programming for a process that wishes to obtain a number of resources  while  decrease_count  count  = = -1  rewrite the resource-manager code segment using a monitor and condition variables so that the decrease_count   function suspends 270 chapter 6 the process until sufficient resources are available this will allow a process to invoke decrease_count   by simply calling decrease_count  count  ; the process will return from this function call only when sufficient resources are available  6.7 exercise 4.12 requires the parent thread to wait for the child thread to finish its execution before printing out the computed values if we let the parent thread access the fibonacci numbers as soon as they have been computed by the child thread  rather than waiting for the child thread to terminate explain what changes would be necessary to the solution for this exercise implement your modified solution  6.8 in section 6.4  we mentioned that disabling interrupts frequently can affect the system 's clock explain why this can occur and how such effects can be mil1.imized  6.9 servers can be designed to limit the number of open coru1.ections for example  a server may wish to have only n socket com1.ections at any point in time as soon as n connections are made  the server will not accept another incoming connection until an existing connection is released explain how semaphores can be used by a server to limit the number of concurrent connections  6.10 why do solaris  lil1.ux  and windows xp use spinlocks as a synchronization mechanism only on multiprocessor systems and not on single-processor systems 6.11 show that  if the wait   and signal   semaphore operations are not executed atomically  then mutual exclusion may be violated  6.12 show how to implement the wait   and signal   semaphore operations in multiprocessor environments using the testandset   instruction  the solution should exhibit minimal busy waiting  6.13 suppose we replace the wait   and signal   operations of monitors with a single construct await  b   where b is a general boolean expression that causes the process executing it to wait until b becomes true  a write a monitor using this scheme to implement the readerswriters problem  b explain why  in general  this construct can not be implemented efficiently  c what restrictions need to be put on the await statement so that it can be implemented efficiently  hint  restrict the generality of b ; see kessels  1977    271 6.14 show that the two-phase locking protocol ensures conflict serializability  6.15 how does the signal   operation associated with monitors differ from the corresponding operation defined for semaphores 6.16 describe how volatile  nonvolatile  and stable storage differ in cost  6.17 explain why implementing synchronization primitives by disabling interrupts is not appropriate in a single-processor system if the synchronization primitives are to be used in user-level programs  6.18 consider a system consisting of processes p1  p2    p11  each of which has a unique priority number write a monitor that allocates three identical line printers to these processes  using the priority numbers for deciding the order of allocation  6.19 describe two kernel data structures in which race conditions are possible  be sure to include a description of how a race condition can occur  6.20 assume that a finite number of resources of a single resource type must be managed processes may ask for a number of these resources and -once finished-will return them as an example  many commercial software packages provide a given number of licenses  indicating the number of applications that may run concurrently when the application is started  the license count is decremented when the application is terminated  the license count is incremented if all licenses are in use  requests to start the application are denied such requests will only be granted when an existing license holder terminates the application and a license is returned  the following program segment is used to manage a finite number of instances of an available resource the maximum number of resources and the number of available resources are declared as follows  # define max_resources 5 int available_resources = max_resources ; when a process wishes to obtain a number of resources  it invokes the decrease_count   function  i decrease available_resources by count resources i i return 0 if sufficient resources available  i i otherwise return -1 i int decrease_count  int count    if  available_resources count  return -1 ; else  available_resources count ; return 0 ;  272 chapter 6 when a process wants to return a number of resourcesf it calls the increase_count   function  i increase available_resources by count i int increase_count  int count   available_resources + = count ; return 0 ;  the preceding program segment produces a race condition do the following  a identify the data involved in the race condition  b identify the location  or locations  in the code where the race condition occurs  c using a semaphoref fix the race condition it is ok to modify the decrease_count   fun.ction so that the calling process is blocked until sufficient resources are available  6.21 explain why spinlocks are not appropriate for single-processor systems yet are often used in multiprocessor systems  6.22 the cigarette-smokers problem consider a system with three smoker processes and one agent process each smoker continuously rolls a cigarette and then smokes it but to roll and smoke a cigarettef the smoker needs three ingredients  tobaccof paperf and matches one of the smoker processes has paperf another has tobaccof and the third has matches the agent has an infinite supply of all three materials the agent places two of the ingredients on the table the smoker who has the remaining ij.l.gredient then makes and smokes a cigarette  signaling the agent on completion the agent then puts out another two of the three ingredients  and the cycle repeats write a program to synchronize the agent and the smokers using java synchronization  6.23 describe how the swap   instruction can be used to provide mutual exclusion that satisfies the bounded-waiting requirement  6.24 a new lightweight synchronization tool called locks whereas most implementations of readerwriter locks favor either readers or writers  or perhaps order waiting threads using a fifo policy  slim reader-writer locks favor neither readers nor writers  nor are waiting threads ordered in a fifo queue  explain the benefits of providing such a synchronization tool  6.25 what are the implications of assigning a new timestamp to a transaction that is rolled back how does the system process transactions that were issued after the rolled -back transaction b-ut that have timestamps smaller than the new timestamp of the rolled-back transaction 273 6.26 discuss the tradeoff between fairness and throughput of operations in the readers-writers problem propose a method for solving the readers-writers problem without causing starvation  6.2'7 when a signal is performed on a condition inside a monitor  the signaling process can either continue its execution or transfer control to the process that is signaled how would the solution to the preceding exercise differ with these two different ways in which signaling can be performed 6.28 what is the meaning of the term busy waiting what other kinds of waiting are there in an operating system can busy waiting be avoided altogether explain your answer  6.29 demonstrate that monitors and semaphores are equivalent insofar as they can be used to implement the same types of synchronization problems  6.30 in log-based systems that provide support for transactions  updates to data items can not be performed before the corresponding entries are logged why is this restriction necessary 6.31 explain the purpose of the checkpoint mechanism how often should checkpoints be performed describe how the frequency of checkpoints affects  system performance when no failure occurs the time it takes to recover from a system crash the time it takes to recover from a disk crash 6.32 write a bounded-buffer monitor in which the buffers  portions  are embedded within the monitor itself  6.33 the strict mutual exclusion within a monitor makes the bounded-buffer monitor of exercise 6.32 mainly suitable for small portions  a explain why this is true  b design a new scheme that is suitable for larger portions  6.34 race conditions are possible in many computer systems consider a banking system with two functions  deposit  amount  and withdraw  amount   these two functions are passed the amount that is to be deposited or withdrawn from a bank account assume a shared bank account exists between a husband and wife and concurrently the husband calls the withdraw   function and the wife calls deposit    describe how a race condition is possible and what might be done to prevent the race condition from occurring  274 chapter 6 6.35 suppose the signal   statement can appear only as the last statement in a monitor procedure suggest how the implementation described in section 6.7 can be simplified in this situation  6.36 the sleeping-barber problem a barbershop consists of a waiting room with n chairs and a barber roorn with one barber chair if there are no customers to be served  the barber goes to sleep if a customer enters the barbershop and all chairs are occupied  then the customer leaves the shop if the barber is busy but chairs are available  then the customer sits in one of the free chairs if the barber is asleep  the customer wakes up the barber write a program to coordinate the barber and the customers  6.37 producer-consumer problem in section 6.6.1  we had presented a semaphore-based solution to the producer-consumer problem using a bounded buffer in this project  we will design a programming solution to the bounded-buffer problem using the producer and consumer processes shown in figures 6.10 and 6.11 the solution presented in section 6.6.1 uses three semaphores  empty and full  which count the number of empty and full slots in the buffer  and mutex  which is a binary  or mutual-exclusion  semaphore that protects the actual insertion or removal of items in the buffer for this project  standard counting semaphores will be used for empty and full  and a mutex lock  rather than a binary semaphore  will be used to represent mutex the producer and consumer-running as separate threads-will move items to and from a buffer that is synchronized with these empty  full  and mutex structures you can solve this problem using either pthreads or the win32 api  the buffer internally  the buffer will consist of a fixed-size array of type buffer_i tern  which will be defined using a typedef   the array of buffer_i tern objects will be manipulated as a circular queue the definition of buffer _i tern  along with the size of the buffer  can be stored in a header file such as the following  i buffer.h i typedef int buffer_item ; # define buffer_size 5 the buffer will be manipulated with two functions  insert_i tern   and remove_i tern   ,which are called by the producer and consumer threads  respectively a skeleton outlining these functions appears in figure 6.27  # include buffer.h i the buffer i buffer_item buffer  buffer_size  ; int insert_item  buffer_item item   i insert item into buffer return 0 if successful  otherwise return -1 indicating an error condition i  int remove_item  buffer_item item    i remove an object from buffer placing it in item return 0 if successful  otherwise return -1 indicating an error condition i figure 6.27 a skeleton program  275 the insert_item   and remove_item   functions will synchronize the producer and consumer using the algorithms outlined in figures 6.10 and 6.11 the buffer will also require an initialization function that initializes the mutual-exclusion object mutex along with the empty and full semaphores  the main   f-lmction will initialize the buffer and create the separate producer and consumer threads once it has created the producer and consumer threads  the main   function will sleep for a period of time and  upon awakening  will terminate the application the main   function will be passed three parameters on the command line  a how long to sleep before terminating b the number of producer threads c the nuncber of consumer threads a skeleton for this function appears in figure 6.28  # include buffer.h int main  int argc  char argv      i 1 get command line arguments argv  1  ,argv  2  ,argv  3  i i 2 initialize buffer i i 3 create producer thread  s  i i 4 create consumer thread  s  i i 5 sleep i i 6 exit i figure 6.28 a skeleton program  276 chapter 6 producer and consumer threads the producer thread will alternate between sleeping for a random period of time and inserting a random integer into the buffer random numbers will be produced using the rand   function  which produces random integers between 0 and rand..max the consumer will also sleep for a random period of time and  upon awakening  will attempt to remove an item from the buffer an outline of the producer and consumer threads appears in figure 6.29  in the following sections  we first cover details specific to pthreads and then describe details of the win32 api  pthreads thread creation creating threads using the pthreads api is discussed in chapter 4 please refer to that chapter for specific instructions regarding creation of the producer and consumer using pthreads  # include stdlib.h i required for rand   i # include buffer.h void producer  void pararn   buffer_item item ;  while  true   i sleep for a random period of time i sleep    ; i generate a random number i item = rand   ; if  insert_item  item   fprintf  report error condition  ; else printf  producer produced % d \ n ,item  ; void consumer  void pararn   buffer_item item ;  while  true   i sleep for a random period of time i sleep    ; if  remove_item  &item   fprintf  report error condition  ; else printf  consumer consumed % d \ n ,item  ; figure 6.29 an outline of the producer and consumer threads  # include pthread.h pthread_mutex_t mutex ; i create the mutex lock i pthread_mutex_init  &mutex,null  ; i acquire the mutex lock i pthread_mutex_lock  &mutex  ; i critical section i i release the mutex lock i pthread_mutex_unlock  &mutex  ; figure 6.30 code sample  pthreads mutex locks 277 the code sample depicted in figure 6.30 illustrates how mutex locks available in the pthread api can be used to protect a critical section  pthreads uses the pthread_mutex_t data type for mutex locks  a mutex is created with the pthread_mutex_ini t  &mutex  null  function  with the first parameter being a pointer to the mutex  by passing null as a second parameter  we initialize the mutex to its default attributes the mutex is acquired and released with the pthread_mutex_lock   and pthread_mutex_unlock   functions  if the mutex lock is unavailable when pthread_mutex_lock   is invoked  the callil1.g thread is blocked until the owner invokes pthread_mutex_unlock 0 all mutex ftmctions return a value of 0 with correct operation ; if an error occurs  these functions return a nonzero error code  pthreads semaphores pthreads provides two types of semaphores-named and unnamed for this project  we use unnamed semaphores the code below illush ates how a semaphore is created  # include semaphore.h sem_t sem ; i create the semaphore and initialize it to 5 i sem_init  &sem  0  5  ; the sem_ini t   creates and initializes a semaphore this function is passed three parameters  a a pointer to the semaphore b a flag indicating the level of sharing c the semaphore 's initial value 278 chapter 6 # include semaphore.h sem_t mutex ; i create the semaphore i sem_init  &mutex  0  1  ; i acquire the semaphore i sem_wait  &mutex  ; i critical section i i release the semaphore i sem_post  &mutex  ; figure 6.31 aaa5  in this example  by passing the flag 0  we are indicating that this semaphore can only be shared by threads belonging to the same process that created the semaphore a nonzero value would allow other processes to access the semaphore as well in this example  we initialize the semaphore to the value 5  in section 6.5  we described the classical wait   and signal   semaphore operations pthreads names the wait   and signal   operations sem_wai t   and sem_post   ,respectively the code example shown in figure 6.31 creates a binary semaphore mutex with an initial value of 1 and illustrates its use in protecting a critical section  win32 details concerning thread creation using the win32 api are available in chapter 4 please refer to that chapter for specific instructions  win32 mutex locks mutex locks are a type of dispatcher object  as described in section 6.8.2 the following illustrates how to create a mutex lock using the createmutex   function  # include windows.h handle mutex ; mutex = createmutex  null  false  null  ; the first parameter refers to a security attribute for the mutex lock by setting this attribute to null  we are disallowing any children of the process creating this mutex lock to inherit the handle of the mutex  the second parameter indicates whether the creator of the mutex is the initial owner of the mutex lock passing a value off alse indicates that the thread creating the mutex is not the initial owner ; we shall soon see how mutex locks are acquired the third parameter allows naming of 279 the mutex however  because we provide a value of null  we do not name the mutex if successful  createmutex   returns a handle to the mutex lock ; otherwise  it returns null  in section 6.8.2  we identified dispatcher objects as being either signaled or nonsignaled a signaled object is available for ownership ; once a dispatcher object  such as a mutex lock  is acquired  it moves to the nonsignaled state when the object is released  it returns to signaled  mutex locks are acquired by invoking the wai tforsingleobj ect   function  passing the function the handle to the lock and a flag indicating how long to wait the following code demonstrates how the mutex lock created above can be acquired  waitforsingleobject  mutex  infinite  ; the parameter value infinite indicates that we will wait an infinite amount of time for the lock to become available other values could be used that would allow the calling thread to time out if the lock did not become available within a specified time if the lock is in a signaled state  wai tforsingleobj ect   returns immediately  and the lock becomes nonsignaled a lock is released  moves to the signaled state  by invoking re leas emu t ex    such as  releasemutex  mutex  ; win32 semaphores semaphores in the win32 api are also dispatcher objects and thus use the same signaling mechanism as mutex locks semaphores are created as follows  # include windows.h handle sem ; sem = createsemaphore  null  1  5  null  ; the first and last parameters identify a security attribute and a name for the semaphore  similar to what was described for mutex locks the second and third parameters indicate the initial value and maximum value of the semaphore in this instance  the initial value of the semaphore is 1  and its maximum value is 5 if successful  createsemaphore   returns a handle to the mutex lock ; otherwise  it returns null  semaphores are acquired with the same wai tforsingleobj ect   function as mutex locks we acquire the semaphore sem created in this example by using the statement  waitforsingleobject  semaphore  infinite  ; if the value of the semaphore is 0  the semaphore is in the signaled state and thus is acquired by the calling thread otherwise  the calling thread blocks indefinitely-as we are specifying infinite-until the semaphore becomes signaled  280 chapter 6 the equivalent of the signal   operation on win32 semaphores is the releasesemaphore   function this function is passed three parameters  a the handle of the semaphore b the amount by which to increase the value of the semaphore c a pointer to the previous value of the semaphore we can increase sem by 1 using the following statement  releasesemaphore  sem  1  ~ ll  ; both releasesemaphore   and releasemutex   return nonzero if successful and zero otherwise  the mutual-exclusion problem was first discussed in a classic paper by dijkstra  1965a   dekker 's algorithm  exercise 6.1  -the first correct software solution to the two-process mutual-exclusion problem-was developed by the dutch mathematician t dekker this algorithm also was discussed by dijkstra  1965a   a simpler solution to the two-process mutual-exclusion problem has since been presented by peterson  1981   figure 6.2   dijkstra  1965b  presented the first solution to the mutual-exclusion problem for n processes this solution  however  does not have an upper bound on the amount of time a process must wait before it is allowed to enter the critical section knuth  1966  presented the first algorithm with a bound ; his bound was 211 turns a refinement of knuth 's algorithm by debruijn  1967  reduced the waiting time to n2 turns  after which eisenberg and mcguire  1972  succeeded in reducing the time to the lower bound of n-1 turns another algorithm that also requires n-1 turns but is easier to program and to understand is the bakery algorithm  which was developed by lamport  1974   burns  1978  developed the hardware-solution algorithm that satisfies the bounded-waiting requirement  general discussions concerning the mutual-exclusion problem were offered by lamport  1986  and lamport  1991   a collection of algorithms for mutual exclusion was given by raynal  1986   the semaphore concept was suggested by dijkstra  1965a   patil  1971  examined the question of whether semaphores can solve all possible synchronization problems parnas  1975  discussed some of the flaws in patil 's arguments kosaraju  1973  followed up on patil 's work to produce a problem that can not be solved by wait   and signal   operations lipton  1974  discussed the limitations of various synchronization primitives  the classic process-coordination problems that we have described are paradigms for a large class of concurrency-control problems the boundedbuffer problem  the dining-philosophers problem  and the sleeping-barber problem  exercise 6.36  were suggested by dijkstra  1965a  and dijkstra  1971   the cigarette-smokers problem  exercise 6.22 was developed by patil  1971   the readers-writers problem was suggested by courtois et al  1971   the 281 issue of concurrent reading and writing was discussed by lamport  1977   the problem of synchronization of independent processes was discussed by lamport  1976   the critical-region concept was suggested by hoare  1972  and by brinchhansen  1972   the monitor concept was developed by brinch-hansen  1973   a complete description of the monitor was given by hoare  1974   kessels  1977  proposed an extension to the monitor to allow automatic signalil1.g  experience obtained from the use of monitors in concurrent programs was discussed by lampson and redell  1979   they also examined the priority inversion problem general discussions concerning concurrent programming were offered by ben-ari  1990  and birrell  1989   optimizing the performance of lockil1.g primitives has been discussed in many works  such as lamport  1987   mellor-crummey and scott  1991   and anderson  1990   the use of shared objects that do not require the use of critical sections was discussed in herlihy  1993   bershad  1993   and kopetz and reisinger  1993   novel hardware instructions and their utility in implementing synchronization primitives have been described in works such as culler et al   1998   goodman et al  1989   barnes  1993   and herlihy and moss  1993   some details of the locking mechanisms used in solaris were presented in mauro and mcdougall  2007   note that the locking mechanisms used by the kernel are implemented for user-level threads as well  so the same types of locks are available inside and outside the kernel details of windows 2000 synchronization can be found in solomon and russinovich  2000   goetz et al   2006  presents a detailed discussion of concurrent programming in java as well as the java util concurrent package  the write-ahead log scheme was first mtroduced in system r by gray et al   1981   the concept of serializability was formulated by eswaran et al  1976  in connection with their work on concurrency control for system r the two-phase locking protocol was introduced by eswaran et al  1976   the timestampbased concurrency-control scheme was provided by reed  1983   various timestamp-based concurrency-control algorithms were explail1.ed by bernstem and goodman  1980   adl-tabatabai et al  2007  discusses transactional memory  7.1 ch er in a multiprogramming environment  several processes may compete for a finite number of resources a process requests resources ; if the resources are not available at that time  the process enters a waiting state sometimes  a waiting process is never again able to change state  because the resources it has requested are held by other waiting processes this situation is called a deadlock we discussed this issue briefly in chapter 6 in cmmection with semaphores  perhaps the best illustration of a deadlock can be drawn from a law passed by the kansas legislature early in the 20th century it said  in part  when two trains approach each other at a crossing  both shall come to a full stop and neither shall start up again until the other has gone  in this chapter  we describe methods that an operating system can use to prevent or deal with deadlocks although some applications can identify programs that may deadlock  operating systems typically do not provide deadlock-prevention facilities  and it remains the responsibility of programmers to ensure that they design deadlock-free programs deadlock problems can only become more common  given current trends  including larger numbers of processes  multithreaded programs  many more resources withirt a system  and an emphasis on long-lived file and database servers rather than batch systems  to develop a description of deadlocks  which prevent sets of concurrent processes from completing their tasks  to present a number of different methods for preventing or avoiding deadlocks in a computer system  a system consists of a finite number of resources to be distributed among a number of competing processes the resources are partitioned into several 283 284 chapter 7 types  each consisting of some number of identical instances memory space  cpu cycles  files  and i/0 devices  such as printers and dvd drives  are examples of resource types if a system has two cpus  then the resource type cpu has two instances similarly  the resource type printer may have five instances  if a process requests an instance of a resource type  the allocation of any instance of the type will satisfy the request if it will not  then the instances are not identical  and the resource type classes have not been defined properly for example  a system may have two printers these two printers may be defined to be in the same resource class if no one cares which printer prints which output  however  if one printer is on the ninth floor and the other is in the basement  then people on the ninth floor may not see both printers as equivalent  and separate resource classes may need to be defined for each printer  a process must request a resource before using it and must release the resource after using it a process may request as many resources as it requires to carry out its designated task obviously  the number of resources requested may not exceed the total number of resources available in the system in other words  a process can not request three printers if the system has only two  under the normal mode of operation  a process may utilize a resource in only the following sequence  request the process requests the resource if the request can not be granted immediately  for example  if the resource is being used by another process   then the requesting process must wait until it can acquire the resource  use the process can operate on the resource  for example  if the resource is a printer  the process can print on the printer   release the process releases the resource  the request and release of resources are system calls  as explained in chapter 2 examples are the request   and release   device  open   and close   file  and allocate   and free   memory system calls request and release of resources that are not managed by the operating system can be accomplished through the wait   and signal   operations on semaphores or through acquisition and release of a mutex lock for each use of a kernelmanaged resource by a process or thread  the operating system checks to make sure that the process has requested and has been allocated the resource  a system table records whether each resource is free or allocated ; for each resource that is allocated  the table also records the process to which it is allocated if a process requests a resource that is currently allocated to another process  it can be added to a queue of processes waiting for this resource  a set of processes is in a deadlocked state when every process in the set is waiting for an event that can be caused only by another process in the set the events with which we are mainly concerned here are resource acquisition and release the resources may be either physical resources  for example  printers  tape drives  memory space  and cpu cycles  or logical resources  for example  files  semaphores  and monitors   however  other types of events may result in deadlocks  for example  the ipc facilities discussed in chapter 3   to illustrate a deadlocked state  consider a system with three cd rw drives  suppose each of three processes holds one of these cd rw drives if each process 7.2 7.2 285 now requests another drive  the three processes will be in a deadlocked state  each is waiting for the event cd rw is released  which can be caused only by one of the other waiting processes this example illustrates a deadlock involving the same resource type  deadlocks may also involve different resource types for example  consider a system with one printer and one dvd drive suppose that process p ; is holding the dvd and process pi is holding the printer if p ; requests the printer and p1 requests the dvd drive  a deadlock occurs  a programmer who is developing multithreaded applications must pay particular attention to this problem multithreaded programs are good candidates for deadlock because multiple threads can compete for shared resources  in a deadlock  processes never finish executing  and system resources are tied up  preventing other jobs from starting before we discuss the various methods for dealing with the deadlock problem  we look more closely at features that characterize deadlocks  7.2.1 necessary conditions a deadlock situation can arise if the following four conditions hold simultaneously in a system  mutual exclusion at least one resource must be held in a nonsharable mode ; that is  only one process at a time can use the resource if another deadlock with mutex locks let 's see how deadlock can occur in a multithreaded pthread program using mutex locks the pthread....mutex_ini t   function initializes an unlocked mutex mutex locks are acquired and released using pthread....mutex_lock   and pthread....mutex_unlock    respectively  if a thread attempts to acquire a locked mutex  the call to pthread....mutex_lock 0 blocks the thread until the owner of the mutex lock invokes pthread....mutex_unlock    two mutex locks are created in the following code example  i create and initialize the mutex locks i pthread....mutex_t first....mutex ; pthread....mutex_t second_nmtex ; pthread....mutex_ini t  &first....mutex  null  ; pthread....mutex_ini t  &second....mutex  null  ; next  two threads-thread_one and thread_two-'-are created  and both these threads have access to both mutex locks thread_one and thread_ two run in the functions do_work_one   and do_work_two    respectively  as shown in figure 7.1  286 chapter 7 deadlock with mutex locks  continued  i thread_one runs in this function i void do_work_one  void param    pthread_mutex_lock  &first_mutex  ; pthread_mutex_lock  &second_mutex  ; i do some work i pthread_mutex  _unlock  &second_mutex  ; pthread_mutex_unlock  &first_mutex  ; pthread_exit  0  ; i thread_two runs in this function i void do_work_two  void param    pthread_mutex_lock  &second_mutex  ; pthread_mutex_lock  &first_mutex  ; i do some work i pthread_mutex_unlock  &first_mutex  ; pthread_mutex_unlock  &second_mutex  ; pthread_exi t  0  ; figure 7.1 deadlock example  in this example  thread_one attempts to acquire the mutex locks in the order  1  first_mutex   2  second_mutex  while thread_two attempts to acquire the mutex locks in the order  1  second__mutex   2  first_mutex  deadlock is possible if thread_one acquires first __mutex while thread_ two aacquites second__mutex  note that  even though deadlock is possible  it will not occur if thread_one is able to acquire and release the mutex locks for first_mutex and second_ mutex before thread_two attemptsto acquire the locks this example illustrates a problem with handling deadlocks  it is difficult to identify and test for deadlocks that may occur only under certain circumstances  process requests that resource  the requesting process must be delayed until the resource has been released  hold and wait a process must be holding at least one resource and waiting to acquire additional resources that are cmrently being held by other processes  7.2 287 no preemption resources can not be preempted ; that is  a resource can be released only voluntarily by the process holding it  after that process has completed its task  circular wait a set  p0  pl    p11  of waiting processes must exist such that po is waiting for a resource held by p1  p1 is waiting for a resource held by p2    pn-1 is waiting for a resource held by p,v and p11 is waiting for a resource held by po  we emphasize that all four conditions must hold for a deadlock to occur the circular-wait condition implies the hold-and-wait condition  so the four conditions are not completely independent we shall see in section 7.4  however  that it is useful to consider each condition separately  7.2.2 resource-allocation graph deadlocks can be described more precisely in terms of a directed graph called a graph this graph consists of a set of vertices v and a set of edges e the set of vertices vis partitioned into two different types of nodes  p = =  p1  p2    pn   the set consisting of all the active processes in the system  and r = =  r1  r2    rml the set consisting of all resource types in the system  a directed edge from process g to resource type rj is denoted by p ;  + rj ; it signifies that process p ; has requested an instance of resource type rj and is currently waiting for that resource a directed edge from resource type rj to process p ; is denoted by r1  + p ; ; it signifies that an instance of resource type r1 has been allocated to process p ;  a directed edge p ;  + rj is called a edge ; a directed edge r1  + p ; is called an pictorially we represent each process p ; as a circle and each resource type rj as a rectangle since resource type ri may have more than one instance  we represent each such instance as a dot within the rectangle note that a request edge points to only the rectangle r1  whereas an assignment edge must also designate one of the dots in the rectangle  when process p ; requests an instance of resource type ri  a request edge is inserted in the resource-allocation graph when this request can be fulfilled  the request edge is instantaneously transformed to an assignment edge when the process no longer needs access to the resource  it releases the resource ; as a result  the assignment edge is deleted  the resource-allocation graph shown in figure 7.2 depicts the following situation  the sets p  k and e  o p = =  p1  p2  p3  or = =  r1  r2  r3  ~  0 e = =  pl + rlf p2 + r3f rl + p2f r2 + p2f r2 + pl  r3 + p3  resource instances  o one instance of resource type r1 o two instances of resource type r2 288 chapter 7 figure 7.2 resource-allocation graph  o one instance of resource type r3 o three instances of resource type ~ process states  o process p1 is holding an instance of resource type r2 and is waiting for an instance of resource type r1  o process p2 is holding an instance of r1 and an instance of r2 and is waiting for an instance of r3  o process p3 is holding an instance of r3  given the definition of a resource-allocation graph  it can be shown that  if the graph contains no cycles  then no process in the system is deadlocked if the graph does contain a cycle  then a deadlock may exist  if each resource type has exactly one instance  then a cycle implies that a deadlock has occurred if the cycle involves only a set of resource types  each of which has only a single instance  then a deadlock has occurred each process involved in the cycle is deadlocked in this case  a cycle in the graph is both a necessary and a sufficient condition for the existence of deadlock  if each resource type has several instances  then a cycle does not necessarily imply that a deadlock has occurred in this case  a cycle in the graph is a necessary but not a sufficient condition for the existence of deadlock  to illustrate this concept  we return to the resource-allocation graph depicted in figure 7.2 suppose that process p3 requests an instance of resource type r2 since no resource instance is currently available  a request edge p3  + r2 is added to the graph  figure 7.3   at this point  two minimal cycles exist in the system  p1  + r 1  + p2  + r3  + p3  + r2  + p1 p2  + r3  + p3  + r2  + p2 7.2 deadlock characterization 289 figure 7.3 resource-allocation graph with a deadlock  processes p1  pz  and p3 are deadlocked process pz is waiting for the resource r3  which is held by process p3 process p3 is waiting for either process p1 or process pz to release resource r2 in addition  process p1 is waiting for process pz to release resource r1  now consider the resource-allocation graph in figure 7.4 in this example  we also have a cycle  however  there is no deadlock observe that process p4 may release its instance of resource type r2 that resource can then be allocated to p3  breaking the cycle  in summary  if a resource-allocation graph does not have a cycle  then the system is not in a deadlocked state if there is a cycle  then the system may or may not be in a deadlocked state this observation is important when we deal with the deadlock problem  figure 7.4 resource-allocation graph with a cycle but no deadlock  290 chapter 7 7.3 generally speaking  we can deal with the deadlock problem in one of three ways  we can use a protocol to prevent or avoid deadlocks  ensuring that the system will never enter a deadlocked state  we can allow the system to enter a deadlocked state  detect it  and recover  we can ignore the problem altogether and pretend that deadlocks never occur in the system  the third solution is the one used by most operating systems  including unix and windows ; it is then up to the application developer to write programs that handle deadlocks  next  we elaborate briefly on each of the three methods for handling deadlocks then  in sections 7.4 through 7.7  we present detailed algorithms  before proceeding  we should mention that some researchers have argued that none of the basic approaches alone is appropriate for the entire spectrum of resource-allocation problems in operating systems the basic approaches can be combined  however  allowing us to select an optimal approach for each class of resources in a system  to ensure that deadlocks never occur  the prevention or a deadlock-avoidance scheme provides a set of methods for ensuring that at least one of the necessary conditions  section 7.2.1  can not hold these methods prevent deadlocks by constraining how requests for resources can be made we discuss these methods in section 7.4  requires that the operating system be given in advance additional information concerning which resources a process will request and use during its lifetime with this additional knowledge  it can decide for each request whether or not the process should wait to decide whether the current request can be satisfied or must be delayed  the system must consider the resources currently available  the resources currently allocated to each process  and the future requests and releases of each process we discuss these schemes in section 7.5  if a system does not employ either a deadlock-prevention or a deadlockavoidance algorithm  then a deadlock situation may arise in this environment  the system can provide an algorithm that examines the state of the system to determine whether a deadlock has occurred and an algorithm to recover from the deadlock  if a deadlock has indeed occurred   we discuss these issues in section 7.6 and section 7.7  in the absence of algorithms to detect and recover from deadlocks  we may arrive at a situation in which the system is in a deadlock state yet has no way of recognizing what has happened in this case  the undetected deadlock will result in deterioration of the system 's performance  because resources are being held by processes that can not run and because more and more processes  as they make requests for resources  will enter a deadlocked state eventually  the system will stop functioning and will need to be restarted manually  7.4 7.4 291 although this method may not seem to be a viable approach to the deadlock problem  it is nevertheless used in most operating systems  as mentioned earlier in many systems  deadlocks occur infrequently  say  once per year  ; thus  this method is cheaper than the prevention  avoidance  or detection and recovery methods  which must be used constantly also  in some circumstances  a system is in a frozen state but not in a deadlocked state we see this situation  for example  with a real-time process running at the highest priority  or any process running on a nonpreemptive scheduler  and never returning control to the operating system the system must have manual recovery methods for such conditions and may simply use those techniques for deadlock recovery  as we noted in section 7.2.1  for a deadlock to occur  each of the four necessary conditions must hold by ensuring that at least one of these conditions can not hold  we can prevent the occurrence of a deadlock we elaborate on this approach by examining each of the four necessary conditions separately  7.4.1 mutual exclusion the mutual-exclusion condition must hold for nonsharable resources for example  a printer can not be simultaneously shared by several processes  sharable resources  in contrast  do not require mutually exclusive access and thus can not be involved in a deadlock read-only files are a good example of a sharable resource if several processes attempt to open a read-only file at the same time  they can be granted simultaneous access to the file a process never needs to wait for a sharable resource in general  however  we can not prevent deadlocks by denying the mutual-exclusion condition  because some resources are intrinsically nonsharable  7.4.2 hold and wait to ensure that the hold-and-wait condition never occurs in the system  we must guarantee that  whenever a process requests a resource  it does not hold any other resources one protocol that can be used requires each process to request and be allocated all its resources before it begins execution we can implement this provision by requiring that system calls requesting resources for a process precede all other system calls  an alternative protocol allows a process to request resources only when it has none a process may request some resources and use them before it can request any additional resources  however  it must release all the resources that it is currently allocated  to illustrate the difference between these two protocols  we consider a process that copies data from a dvd drive to a file on disk  sorts the file  and then prints the results to a printer if all resources must be requested at the beginning of the process  then the process must initially request the dvd drive  disk file  and printer it will hold the printer for its entire execution  even though it needs the printer only at the end  the second method allows the process to request initially only the dvd drive and disk file it copies from the dvd drive to the disk and then releases 292 chapter 7 both the dvd drive and the disk file the process must then again request the disk file and the printer after copying the disk file to the printer  it releases these two resources and terminates  both these protocols have two main disadvantages first  resource utilization may be low  since resources may be allocated but unused for a long period  in the example given  for instance  we can release the dvd drive and disk file  and then again request the disk file and printe1 ~ only if we can be sure that our data will remain on the disk file otherwise  we must request all resources at the beginning for both protocols  second  starvation is possible a process that needs several popular resources may have to wait indefinitely  because at least one of the resources that it needs is always allocated to some other process  7.4.3 no preemption the third necessary condition for deadlocks is that there be no preemption of resources that have already been allocated to ensure that this condition does not hold  we can use the following protocol if a process is holding some resources and requests another resource that can not be immediately allocated to it  that is  the process must wait   then all resources the process is currently holding are preempted in other words  these resources are implicitly released the preempted resources are added to the list of resources for which the process is waiting the process will be restarted only when it can regain its old resources  as well as the new ones that it is requesting  alternatively  if a process requests some resources  we first check whether they are available if they are  we allocate them if they are not  we check whether they are allocated to some other process that is waiting for additional resources if so  we preempt the desired resources from the waiting process and allocate them to the requesting process if the resources are neither available nor held by a waiting process  the requesting process must wait while it is waiting  some of its resources may be preempted  but only if another process requests them a process can be restarted only when it is allocated the new resources it is requesting and recovers any resources that were preempted while it was waiting  this protocol is often applied to resources whose state can be easily saved and restored later  such as cpu registers and memory space it can not generally be applied to such resources as printers and tape drives  7 .4.4 circular wait the fourth and final condition for deadlocks is the circular-wait condition one way to ensure that this condition never holds is to impose a total ordering of all resource types and to require that each process requests resources in an increasing order of enumeration  to illustrate  we let r =  r1  r2    rm  be the set of resource types we assign to each resource type a unique integer number  which allows us to compare two resources and to determine whether one precedes another in our ordering formally  we define a one-to-one hmction f  r ___ n  where n is the set of natural numbers for example  if the set of resource types r includes tape drives  disk drives  and printers  then the function f might be defined as follows  7.4 f  tape drive  = 1 f  disk drive  = 5 f  printer  = 12 293 we can now consider the following protocol to prevent deadlocks  each process can request resources only in an increasing order of enumeration that is  a process can initially request any number of instances of a resource type -say  r ;  after that  the process can request instances of resource type rj if and only if f  rj  f  r ;   for example  using the function defined previously  a process that wants to use the tape drive and printer at the same time must first request the tape drive and then request the printer alternatively  we can require that a process requesting an instance of resource type rj must have released any resources r ; such that f  ri    =   f  rj   it must also be noted that if several iilstances of the same resource type are needed  a single request for all of them must be issued  if these two protocols are used  then the circular-wait condition can not hold we can demonstrate this fact by assuming that a circular wait exists  proof by contradiction   let the set of processes involved in the circular wait be  p0  p1    p11   where pi is waiting for a resource r ;  which is held by process pi + l  modulo arithmetic is used on the indexes  so that p11 is waiting for a resource r11 held by p0   then  since process pi + l is holding resource ri while requesting resource ri + l ' we must have f  ri  f  r ; h  for all i but this condition means that f  ro  f  r1   f  r11  f  ro   by transitivity  f  ro  f  ro   which is impossible therefore  there can be no circular wait  we can accomplish this scheme in an application program by developing an ordering among all synchronization objects in the system all requests for synchronization objects must be made in increasing order for example  if the lock ordering in the pthread program shown in figure 7.1 was f  first_mutex  = 1 f  second_mutex  = 5 then thread_ two could not request the locks out of order  keep in mind that developing an ordering  or hierarchy  does not in itself prevent deadlock it is up to application developers to write programs that follow the ordering also note that the function f should be defined according to the normal order of usage of the resources in a system for example  because the tape drive is usually needed before the printer  it would be reasonable to define f  tape drive  f  printer   although ensuring that resources are acquired in the proper order is the responsibility of application developers  certain software can be used to verify that locks are acquired in the proper order and to give appropriate warnings when locks are acquired out of order and deadlock is possible one lock-order verifier  which works on bsd versions of unix such as freebsd  is known as witness witness uses mutual-exclusion locks to protect critical sections  as described in chapter 6 ; it works by dynamically maintaining the relationship of lock orders in a system let 's use the program shown in figure 7.1 as an example assume that thread_one is the first to acquire the locks and does so in the order  1  first_mutex   2  second_mutex wih1ess records the relationship that first_mutex must be acquired before second_mutex if thread_two later 294 chapter 7 7.5 acquires the locks out of order  witness generates a warning message on the system console  it is also important to note that imposing a lock ordering does not guarantee deadlock prevention if locks can be acquired dynamically for example  assume we have a function that transfers funds between two accounts to prevent a race condition  each account has an associated semaphore that is obtained from a get lock   function such as the following  void transaction  account from  account to  double amount    semaphore lock1  lock2 ; lock1 getlock  from  ; lock2 = getlock  to  ; wait  lock1  ; wait  lock2  ; withdraw  from  amount  ; deposit  to  amount  ; signal  lock2  ; signal  lock1  ; deadlock is possible if two threads simultaneously invoke the trans action   function  transposing different accounts that is  one thread might invoke transaction  checkingaccount  savingsaccount  25  ; and another might invoke transaction  savingsaccount  checkingaccount  50  ; we leave it as an exercise for students to fix this situation  deadlock-prevention algorithms  as discussed in section 7.4  prevent deadlocks by restraining how requests can be made the restraints ensure that at least one of the necessary conditions for deadlock can not occur and  hence  that deadlocks can not hold possible side effects of preventing deadlocks by this method  however  are low device utilization and reduced system throughput  an alternative method for avoiding deadlocks is to require additional information about how resources are to be requested for example  in a system with one tape drive and one printer  the system might need to know that process p will request first the tape drive and then the printer before releasing both resources  whereas process q will request first the printer and then the tape drive with this knowledge of the complete sequence of requests and releases for each process  the system can decide for each request whether or not the process should wait in order to avoid a possible future deadlock each request requires that in making this decision the system consider the resources 7.5 deadlock avoidance 295 currently available  the resources currently allocated to each process  and the future requests and releases of each process  the various algorithms that use this approach differ in the amount and type of information required the simplest and most useful model requires that each process declare the maximum number of resources of each type that it may need  given this a priori information  it is possible to construct an algorithm that ensures that the system will never enter a deadlocked state such an algorithm defines the deadlock-avoidance approach a deadlock-avoidance algorithm dynamically examines the resource-allocation state to ensure that a circularwait condition can never exist the resource-allocation state is defined by the number of available and allocated resources and the maximum demands of the processes in the following sections  we explore two deadlock-avoidance algorithms  7.5.1 safe state a state is safe if the system can allocate resources to each process  up to its maximum  in some order and still avoid a deadlock more formally  a system is in a safe state only if there exists a safe sequence a sequence of processes p1  p2    pn is a safe sequence for the current allocation state if  for each pi  the resource requests that pi can still make can be satisfied by the currently available resources plus the resources held by all pj  with j i in this situation  if the resources that pi needs are not immediately available  then pi can wait until all pj have finished when they have finished  pi can obtain all of its needed resources  complete its designated task  return its allocated resources  and terminate when pi terminates  pi + l can obtain its needed resources  and so on if no such sequence exists  then the system state is said to be unsafe  a safe state is not a deadlocked state conversely  a deadlocked state is an unsafe state not all unsafe states are deadlocks  however  figure 7.5   an unsafe state may lead to a deadlock as long as the state is safe  the operating system can avoid unsafe  and deadlocked  states in an unsafe state  the operating system can not prevent processes from requesting resources in such a way that a deadlock occurs the behavior of the processes controls unsafe states  figure 7.5 safe  unsafe  and deadlocked state spaces  296 chapter 7 deadlocks to illustrate  we consider a system with twelve magnetic tape drives and three processes  po  p1  and p2 process po requires ten tape drives  process p1 may need as many as four tape drives  and process p2 may need up to nine tape drives suppose that  at time to  process po is holding five tape drives  process p1 is holding two tape drives  and process p2 is holding two tape drives  thus  there are three free tape drives  maximum needs current needs 10 4 9 5 2 2 at time t0  the system is in a safe state the sequence p1  p0  p2 satisfies the safety condition process p1 can immediately be allocated all its tape drives and then return them  the system will then have five available tape drives  ; then process po can get all its tape drives and return them  the system will then have ten available tape drives  ; and finally process p2 can get all its tape drives and return them  the system will then have all twelve tape drives available   a system can go from a safe state to an unsafe state suppose that  at time t1  process p2 requests and is allocated one more tape drive the system is no longer in a safe state at this point  only process p1 can be allocated all its tape drives when it returns them  the system will have only four available tape drives since process po is allocated five tape drives but has a maximum of ten  it may request five more tape drives if it does so  it will have to wait  because they are unavailable similarly  process p2 may request six additional tape drives and have to wait  resulting in a deadlock our mistake was in granting the request from process p2 for one more tape drive if we had made p2 wait until either of the other processes had finished and released its resources  then we could have avoided the deadlock  given the concept of a safe state  we can define avoidance algorithms that ensure that the system will never deadlock the idea is simply to ensure that the system will always remain in a safe state initially  the system is in a safe state  whenever a process requests a resource that is currently available  the system must decide whether the resource can be allocated immediately or whether the process must wait the request is granted only if the allocation leaves the system in a safe state  in this scheme  if a process requests a resource that is currently available  it may still have to wait thus  resource utilization may be lower than it would otherwise be  7.5.2 resource-allocation-graph algorithm if we have a resource-allocation system with only one instance of each resource type  we can use a variant of the resource-allocation graph defined in section 7.2.2 for deadlock avoidance in addition to the request and assignment edges already described  we introduce a new type of edge  called a claim edge  a claim edge pi ~ rj indicates that process pi may request resource rj at some time in the future this edge resembles a request edge in direction but is represented in the graph by a dashed line when process pi requests resource 7.5 297 figure 7.6 resource-allocation graph for deadlock avoidance  r1  the claim edge p ;  + r1 is converted to a request edge similarly  when a resource r1 is released by p ;  the assignment edge rj  + p ; is reconverted to a claim edge p ;  + rj  we note that the resources must be claimed a priori in the system that is  before process p ; starts executing  all its claim edges must already appear in the resource-allocation graph we can relax this condition by allowing a claim edge p ;  + r1 to be added to the graph only if all the edges associated with process p ; are claim edges  now suppose that process p ; requests resource rj the request can be granted only if converting the request edge p ;  + rj to an assignment edge r1  + p ; does not result in the formation of a cycle in the resource-allocation graph we check for safety by using a cycle-detection algorithm an algorithm for detecting a cycle in this graph requires an order of n2 operations  where n is the number of processes in the system  if no cycle exists  then the allocation of the resource will leave the system in a safe state if a cycle is found  then the allocation will put the system in an unsafe state in that case  process p ; will have to wait for its requests to be satisfied  to illustrate this algorithm  we consider the resource-allocation graph of figure 7.6 suppose that p2 requests r2  although r2 is currently free  we can not allocate it to p2  since this action will create a cycle in the graph  figure 7.7   a cycle  as mentioned  indicates that the system is in an unsafe state if p1 requests r2  and p2 requests r1  then a deadlock will occur  figure 7.7 an unsafe state in a resource-allocation graph  298 chapter 7 7.5.3 banker 's algorithm the resource-allocation-graph algorithm is not applicable to a resourceallocation system with multiple instances of each resource type the deadlockavoidance algorithm that we describe next is applicable to such a system but is less efficient than the resource-allocation graph scheme this algorithm is commonly known as the banker 's algorithm the name was chosen because the algorithm could be used in a banking system to ensure that the bank never allocated its available cash in such a way that it could no longer satisfy the needs of all its customers  when a new process enters the system  it must declare the maximum number of instances of each resource type that it may need this nun1.ber may not exceed the total number of resources in the system when a user requests a set of resources  the system must determine whether the allocation of these resources will leave the system in a safe state if it will  the resources are allocated ; otherwise  the process must wait until some other process releases enough resources  several data structures must be maintained to implement the banker 's algorithm these data structures encode the state of the resource-allocation system we need the following data structures  where n is the number of processes in the system and m is the number of resource types  available a vector of length m indicates the number of available resources of each type if available  j  equals k  then k instances of resource type ri are available  max an n x m matrix defines the maximum demand of each process  if max  i   j  equals k  then process p ; may request at most k instances of resource type ri  allocation an 11 x m matrix defines the number of resources of each type currently allocated to each process if allocation  i   j  equals lc  then process p ; is currently allocated lc instances of resource type rj  need an n x m matrix indicates the remaining resource need of each process if need  i   j  equals k  then process p ; may need k more instances of resource type ri to complete its task note that need  i   j  equals max  i   j   allocation  i   j   these data structures vary over time in both size and value  to simplify the presentation of the banker 's algorithm  we next establish some notation let x andy be vectors of length 11 we say that x   = y if and only if x  i    = y  i  for all i = 1  2    n for example  if x =  1,7,3,2  and y =  0,3,2,1   then y   = x in addition  y x if y   = x andy # x  we can treat each row in the matrices allocation and need as vectors and refer to them as allocation ; and need ;  the vector allocation ; specifies the resources currently allocated to process p ; ; the vector need ; specifies the additional resources that process p ; may still request to complete its task  7.5.3.1 safety algorithm we can now present the algorithm for finding out whether or not a systern is in a safe state this algorithm can be described as follows  7.5 299 let work and finish be vectors of length m and n  respectively initialize work = available and finish  i  = false for i = 0  1    n  1  find an index i such that both a finish  i  = = false b need ;   ; work if no such i exists  go to step 4  work = work + allocation ; finish  i  = true go to step 2  if finish  i  = = true for all i  then the system is in a safe state  this algorithm may require an order of m x n2 operations to determine whether a state is safe  7.5.3.2 resource-request algorithm next  we describe the algorithm for determining whether requests can be safely granted  let request ; be the request vector for process p ;  if request ;  j  = = k  then process p ; wants k instances of resource type rj when a request for resources is made by process p ;  the following actions are taken  if request ;     ; need ;  go to step 2 otherwise  raise an error condition  since the process has exceeded its maximum claim  if request ;   ; available  go to step 3 otherwise  p ; must wait  since the resources are not available  have the system pretend to have allocated the requested resources to process p ; by modifyil1.g the state as follows  available = available request ; ; allocation ; = allocation ; + request ; ; need ; = need ;  request ; ; if the resulting resource-allocation state is safe  the transaction is completed  and process p ; is allocated its resources however  if the new state is unsafe  then p ; must wait for request ;  and the old resource-allocation state is restored  7.5.3.3 an illustrative example to illustrate the use of the banker 's algorithm  consider a system with five processes po through p4 and three resource types a  b  and c resource type a has ten instances  resource type b has five instances  and resource type c has seven instances suppose that  at time t0  the following snapshot of the system has been taken  300 chapter 7 allocation max available abc abc abc po 010 753 332 pl 200 322 p2 302 902 p3 2 11 222 p4 002 433 the content of the matrix need is defined to be max  allocation and is as follows  need abc po 743 pl 122 p2 600 p3 011 p4 431 we claim that the system is currently in a safe state indeed  the sequence plt p3  p4  p2  po satisfies the safety criteria suppose now that process p1 requests one additional instance of resource type a and two instances of resource type c  so request1 =  1,0,2   to decide whether this request can be immediately granted  we first check that request1 s available-that is  that  1,0,2  s  3,3,2   which is true we then pretend that this request has been fulfilled  and we arrive at the following new state  allocation need available abc abc abc po 010 743 230 pl 302 020 p2 302 600 p3 211 0 11 p4 002 431 we must determine whether this new system state is safe to do so  we execute our safety algorithm and find that the sequence p1  p3  p4  po  p2 satisfies the safety requirement hence  we can immediately grant the request of process p1  you should be able to see  however  that when the system is in this state  a request for  3,3,0  by p4 can not be granted  since the resources are not available  furthermore  a request for  0,2,0  by po can not be granted  even though the resources are available  since the resulting state is unsafe  we leave it as a programming exercise for students to implement the banker 's algorithm  7.6 7.6 301 if a system does not employ either a deadlock-prevention or a deadlockavoidance algorithm  then a deadlock situation may occur in this environment  the system may provide  an algorithm that examines the state of the system to determine whether a deadlock has occurred an algorithm to recover from the deadlock in the following discussion  we elaborate on these two requirements as they pertain to systems with only a single instance of each resource type  as well as to systems with several instances of each resource type at this point  however  we note that a detection-and-recovery scheme requires overhead that includes not only the run-time costs of maintaining the necessary information and executing the detection algorithm but also the potential losses inherent in recovering from a deadlock  7.6.1 single instance of each resource type if all resources have only a single instance  then we can define a deadlockdetection algorithm that uses a variant of the resource-allocation graph  called a wait-for graph we obtain this graph from the resource-allocation graph by removing the resource nodes and collapsing the appropriate edges  more precisely  an edge from pi to pi in a wait-for graph implies that process pz is waiting for process p1 to release a resource that p ; needs an edge pz  + pi exists iil a wait-for graph if and only if the corresponding resourceallocation graph contains two edges pz  + rq and rq  + pi for some resource rq for example  in figure 7.8  we present a resource-allocation graph and the corresponding wait-for graph  as before  a deadlock exists in the system if and only if the wait-for graph contains a cycle to detect deadlocks  the system needs to maintain the wait-for graph and periodically invoke an algorithm that searches for a cycle in the graph  an algorithm to detect a cycle in a graph requires an order of n2 operations  where n is the number of vertices in the graph  7.6.2 several instances of a resource type the wait-for graph scheme is not applicable to a resource-allocation system with multiple instances of each resource type we turn now to a deadlockdetection algorithm that is applicable to such a system the algorithm employs several time-varying data structures that are similar to those used in the banker 's algorithm  section 7.5.3   available a vector of length nz indicates the number of available resources of each type  allocation ann x nz matrix defines the number of resources of each type currently allocated to each process  302 chapter 7  a   b  figure 7.8  a  resource-allocation graph  b  corresponding wait-for graph  request an n x m matrix indicates the current request of each process  if request  i   j  equals k  then process p ; is requesting k more instances of resource type rj  the     relation between two vectors is defined as in section 7.5.3 to simplify notation  we again treat the rows in the matrices allocation and request as vectors ; we refer to them as allocation ; and request ;  the detection algorithm described here simply investigates every possible allocation sequence for the processes that remain to be completed compare this algorithm with the banker 's algorithm of section 7.5.3  let work and finish be vectors of length m and n  respectively initialize work = available fori = 0  1    n-1  if allocation ; # 0  then finish  i  = false ; otherwise  finish  i  = tme  2 find an index i such that both a finish  i  = = false b request ;     work if no such i exists  go to step 4  work = work + allocation ; finish  i  = true go to step 2  4 if finish  i  = = false for some i  0     i n  then the system is in a deadlocked state moreover  if finish  i  = = false  then process p ; is deadlocked  this algorithm requires an order o m x n2 operations to detect whether the system is in a deadlocked state  7.6 303 you may wonder why we reclaim the resources of process p ;  in step 3  as soon as we determine that request ;  s work  in step 2b   we know that p ; is currently not involved in a deadlock  since request ;  s work   thus  we take an optimistic attitude and assume that p ; will require no more resources to complete its task ; it will thus soon return all currently allocated resources to the system if our assumption is incorrect  a deadlock may occur later that deadlock will be detected the next tince the deadlock-detection algorithm is invoked  to illustrate this algorithm  we consider a system with five processes po through p4 and three resource types a  b  and c resource type a has seven instances  resource type b has two instances  and resource type c has six instances suppose that  at time t0  we have the following resource-allocation state  allocation request available abc abc abc po 0 1 0 000 000 pl 200 202 p2 303 000 p3 2 11 100 p4 002 002 we claim that the system is not in a deadlocked state indeed  if we execute our algorithm  we will find that the sequence po  p2  p3  plt p4 results in finish  i  = = true for all i  suppose now that process p2 makes one additional request for an instance of type c the request matrix is modified as follows  request abc po 000 pl 202 p2 001 p3 100 p4 002 we claim that the system is now deadlocked although we can reclaim the resources held by process po  the number of available resources is not sufficient to fulfill the requests of the other processes thus  a deadlock exists  consisting of processes p1  p2  p3  and p4  7.6.3 detection-algorithm usage when should we invoke the detection algorithm the answer depends on two factors  1 how often is a deadlock likely to occur how many processes will be affected by deadlock when it happens 304 chapter 7 7.7 if deadlocks occur frequently  then the detection algorithm should be invoked frequently resources allocated to deadlocked processes will be idle until the deadlock can be broken in addition  the number of processes involved in the deadlock cycle may grow  deadlocks occur only when some process makes a request that can not be granted immediately this request may be the final request that completes a chain of waiting processes in the extreme  then  we can invoke the deadlockdetection algorithm every time a request for allocation can not be granted immediately in this case  we can identify not only the deadlocked set of processes but also the specific process that caused the deadlock  in reality  each of the deadlocked processes is a link in the cycle in the resource graph  so all of them  jointly  caused the deadlock  if there are many different resource types  one request may create many cycles in the resource graph  each cycle completed by the most recent request and caused by the one identifiable process  of course  invoking the deadlock-detection algorithm for every resource request will incur considerable overhead in computation time a less expensive alternative is simply to invoke the algorithm at defined intervals-for example  once per hour or whenever cpu utilization drops below 40 percent  a deadlock eventually cripples system throughput and causes cpu utilization to drop  if the detection algorithm is invoked at arbitrary points in time  the resource graph may contain many cycles in this case  we generally can not tell which of the many deadlocked processes caused the deadlock when a detection algorithm determines that a deadlock exists  several alternatives are available one possibility is to inform the operator that a deadlock has occurred and to let the operator deal with the deadlock manually another possibility is to let the system recover from the deadlock automatically there are two options for breaking a deadlock one is simply to abort one or more processes to break the circular wait the other is to preempt some resources from one or more of the deadlocked processes  7.7.1 process termination to eliminate deadlocks by aborting a process  we use one of two methods in both methods  the system reclaims all resources allocated to the terminated processes  abort all deadlocked processes this method clearly will break the deadlock cycle  but at great expense ; the deadlocked processes may have computed for a long time  and the results of these partial computations must be discarded and probably will have to be recomputed later  abort one process at a time until the deadlock cycle is eliminated this method incurs considerable overhead  since after each process is aborted  a deadlock-detection algorithnc rnust be invoked to determine whether any processes are still deadlocked  7.7 305 aborting a process may not be easy if the process was in the midst of updating a file  terminating it will leave that file in an incorrect state similarly  if the process was in the midst of printing data on a printer  the system must reset the printer to a correct state before printing the next job  if the partial termination method is used  then we must determine which deadlocked process  or processes  should be terminated this determination is a policy decision  similar to cpu-scheduling decisions the question is basically an economic one ; we should abort those processes whose termination will incur the minimum cost unfortunately  the term minimum cost is not a precise one  many factors may affect which process is chosen  including  1 what the priority of the process is 2 how long the process has computed and how much longer the process will compute before completing its designated task how many and what types of resources the process has used  for example  whether the resources are simple to preempt  how many more resources the process needs in order to complete 5 how many processes will need to be terminated whether the process is interactive or batch 7.7.2 resource preemption to eliminate deadlocks using resource preemption  we successively preempt some resources from processes and give these resources to other processes 1-m til the deadlock cycle is broken  if preemption is required to deal with deadlocks  then three issues need to be addressed  selecting a victim which resources and which processes are to be preempted as in process termil ation  we must determine the order of preemption to minimize cost cost factors may include such parameters as the number of resources a deadlocked process is holding and the amount of time the process has thus far consumed during its execution  rollback if we preempt a resource from a process  what should be done with that process clearly  it can not contil ue with its normal execution ; it is missing some needed resource we must roll back the process to some safe state and restart it from that state  since  in general  it is difficult to determine what a safe state is  the simplest solution is a total rollback  abort the process and then restart it although it is more effective to roll back the process only as far as necessary to break the deadlock  this method requires the system to keep more information about the state of all running processes  starvation how do we ensure that starvation will not occur that is  how can we guarantee that resources will not always be preempted from the same process 306 chapter 7 7.8 in a system where victim selection is based primarily on cost factors  it may happen that the same process is always picked as a victim as a result  this process never completes its designated task  a starvation situation that must be dealt with in any practical system clearly  we must ensure that a process can be picked as a victim only a  small  finite number of times the most common solution is to include the number of rollbacks in the cost factor  a deadlocked state occurs when two or more processes are waiting indefinitely for an event that can be caused only by one of the waiting processes there are three principal methods for dealing with deadlocks  use some protocol to prevent or avoid deadlocks  ensuring that the system will never enter a deadlocked state  allow the system to enter a deadlocked state  detect it  and then recover  ignore the problem altogether and pretend that deadlocks never occur in the system  the third solution is the one used by most operating systems  including unix and windows  a deadlock can occur only if four necessary conditions hold simultaneously in the system  mutual exclusion  hold and wait  no preemption  and circular wait to prevent deadlocks  we can ensure that at least one of the necessary conditions never holds  a method for avoiding deadlocks  rather than preventing them  requires that the operating system have a priori information about how each process will utilize system resources the banker 's algorithm  for example  requires a priori information about the maximunl number of each resource class that each process may request using this information  we can define a deadlockavoidance algorithm  if a system does not employ a protocol to ensure that deadlocks will never occur  then a detection-and-recovery scheme may be employed a deadlockdetection algorithm must be invoked to detennine whether a deadlock has occurred if a deadlock is detected  the system must recover either by terminating some of the deadlocked processes or by preempting resources from some of the deadlocked processes  where preemption is used to deal with deadlocks  three issues must be addressed  selecting a victim  rollback  and starvation in a system that selects victims for rollback primarily on the basis of cost factors  starvation may occur  and the selected process can never complete its designated task  researchers have argued that none of the basic approaches alone is appropriate for the entire spectrum of resource-allocation problems in operating systems the basic approaches can be combined  however  allowing us to select an optimal approach for each class of resources in a system  307 7.1 a single-lane bridge connects the two vermont villages of north tunbridge and south tunbridge farmers in the two villages use this bridge to deliver their produce to the neighboring town the bridge can become deadlocked if a northbound and a southbound farmer get on the bridge at the same time  vermont farmers are stubborn and are unable to back up  using semaphores  design an algorithm that prevents deadlock initially  do not be concerned about starvation  the situation in which northbound farmers prevent southbound farmers from using the bridge  or vice versa   7.2 modify your solution to exercise 7.1 so that it is starvation-free  7.3 consider a system consisting of four resources of the same type that are shared by three processes  each of which needs at most two resources  show that the system is deadlock free  7.4 consider the traffic deadlock depicted in figure 7.9  a show that the four necessary conditions for deadlock hold in this example  b state a simple rule for avoiding deadlocks in this system  7.5 in a real computer system  neither the resources available nor the demands of processes for resources are consistent over long periods  months   resources break or are replaced  new processes come and go  and new resources are bought and added to the system if deadlock is controlled by the banker 's algorithm  which of the following changes figure 7.9 traffic deadlock for exercise 7.4 308 chapter 7 can be made safely  without introducing the possibility of deadlock   and under what circumstances a increase available  new resources added   b decrease available  resource permanently removed from system   c increase max for one process  the process needs or wants rnore resources than allowed   d decrease max for one process  the process decides it does not need that many resources   e increase the number of processes  f decrease the number of processes  7.6 we can obtain the banker 's algorithm for a single resource type from the general banker 's algorithm simply by reducing the dimensionality of the various arrays by 1 show through an example that we can not implement the multiple-resource-type banker 's scheme by applying the sil1.gle-resource-type scheme to each resource type individually  7.7 consider the following resource-allocation policy requests for and releases of resources are allowed at any time if a request for resources can not be satisfied because the resources are not available  then we check any processes that are blocked waiting for resources if a blocked process has the desired resources  then these resources are taken away from it and are given to the requestmg process the vector of resources for which the blocked process is waiting is increased to include the resources that were taken away  for example  consider a system with three resource types and the vector available initialized to  4,2,2   if process po asks for  2,2,1   it gets them if p1 asks for  1,0,1   it gets them then  if po asks for  0,0,1   it is blocked  resource not available   if p2 now asks for  2,0,0   it gets the available one  1,0,0  and one that was allocated to po  since po is blocked   po 's allocation vector goes down to  1,2,1   and its need vector goes up to  1,0,1   a can deadlock occur if you answer yes  give an example if you answer no  specify which necessary condition can not occur  b can indefinite blocking occur explain your answer  7.8 a possible method for preventing deadlocks is to have a single  higherorder resource that must be requested before any other resource for example  if multiple threads attempt to access the synchronization objects a e  deadlock is possible  such synchronization objects may include mutexes  semaphores  condition variables  and the like  we can prevent the deadlock by adding a sixth object f whenever a thread wants to acquire the synchronization lock for any object a e  it must first acquire the lock for object f this solution is known as containment  the locks for objects a e are contained within the lock for object f  compare this scheme with the circular-wait scheme of section 7.4.4  309 7.9 compare the circular-wait scheme with the various deadlock-avoidance schemes  like the banker 's algorithnc  with respect to the following issues  a runtime overheads b system throughput 7.10 consider the following snapshot of a system  allocation max available  abcd abcd abcd po 0012 0012 1520 pl 1000 1750 p2 1354 2356 p3 0632 0652 p4 0014 0656 answer the following questions using the banker 's algorithm  a what is the content of the matrix need b is the system in a safe state c if a request from process p1 arrives for  0,4,2,0   can the request be granted immediately 7.11 consider a system consisting of m resources of the same type being shared by n processes a process can request or release only one resource at a time show that the system is deadlock free if the following two conditions hold  a the maximum need of each process is between one resource and m resources  b the sum of all maximum needs is less than m + n  7.12 consider a computer system that runs 5,000 jobs per month and has no deadlock-prevention or deadlock-avoidance scheme deadlocks occur about twice per month  and the operator must terminate and rerun about 10 jobs per deadlock each job is worth about $ 2  in cpu time   and the jobs terminated tend to be about half-done when they are aborted  a systems programmer has estimated that a deadlock-avoidance algorithm  like the banker 's algorithm  could be installed in the system with an increase in the average execution time per job of about 10 percent  since the machine currently has 30 percent idle time  all 5,000 jobs per month could still be run  although turnaround time would increase by about 20 percent on average  a what are the arguments for installing the deadlock-avoidance algorithm b what are the arguments against installing the deadlock-avoidance algorithm 310 chapter 7 7.13 consider the deadlock situation that can occur in the diningphilosophers problem when the philosophers obtain the chopsticks one at a time discuss how the four necessary conditions for deadlock hold in this setting discuss how deadlocks could be avoided by eliminating any one of the four necessary conditions  7.14 what is the optimistic assumption made in the deadlock-detection algorithm how can this assumption be violated 7.15 consider the version of the dining-philosophers problem in which the chopsticks are placed at the center of the table and any two of them can be used by a philosopher assume that requests for chopsticks are made one at a time describe a simple rule for determining whether a particular request can be satisfied without causing deadlock given the current allocation of chopsticks to philosophers  7.16 is it possible to have a deadlock involving only a single process explain your answer  7.17 consider again the setting in the preceding question assume now that each philosopher requires three chopsticks to eat resource requests are still issued one at a time describe some simple rules for determining whether a particular request can be satisfied without causing deadlock given the current allocation of chopsticks to philosophers  7.18 in section 7.4.4  we describe a situation in which we prevent deadlock by ensuring that all locks are acquired in a certain order however  we also point out that deadlock is possible in this situation if two threads simultaneously invoke the transaction   function fix the transaction   function to prevent deadlocks  7.19 write a multithreaded program that implements the banker 's algorithm discussed in section 7.5.3 create n threads that request and release resources from the bank the banker will grant the request only if it leaves the system in a safe state you may write this program using either pthreads or win32 threads it is important that shared data be safe from concurrent access to ensure safe access to shared data  you can use mutex locks  which are available in both the pthreads and win32 apis the use of mutex locks in both of these libraries is described in the project entitled producer-consumer problem at the end of chapter 6  dijkstra  1965a  was one of the first and most influential contributors in the deadlock area holt  1972  was the first person to formalize the notion of deadlocks in terms of an allocation-graph model similar to the one presented in this chapter starvation was also covered by holt  1972   hyman  1985  provided the deadlock example from the kansas legislature a recent study of deadlock handling is provided in levine  2003   311 the various prevention algorithms were suggested by havender  1968   who devised the resource-ordering scheme for the ibm os/360 systen'l  the banker 's algorithm for avoiding deadlocks was developed for a single resource type by dijkstra  1965a  and was extended to multiple resource types by habermam'l  1969   exercises 7.3 and 7.11 are from holt  1971   the deadlock-detection algorithm for multiple instances of a resource type  which is described in section 7.6.2  was presented by coffman et al  1971   bach  1987  describes how many of the algorithms in the traditional unix kernel handle deadlock solutions to deadlock problems in networks are discussed in works such as culler et al  1998  and rodeheffer and schroeder  1991   the witness lock-order verifier is presented in baldwin  2002   part four the main purpose of a computer system is to execute programs these programs  together with the data they access  must be at least partially in main memory during execution  to improve both the utilization of the cpu and the speed of its response to users  a general-purpose computer must keep several processes in memory many memory-management schemes exist  reflecting various approaches  and the effectiveness of each algorithm depends on the situation selection of a memory-management scheme for a system depends on many factors  especially on the hardware design of the system most algorithms require hardware support  8.1 c in chapter 5  we showed how the cpu can be shared by a set of processes as a result of cpu scheduling  we can improve both the utilization of the cpu and the speed of the computer 's response to its users to realize this increase in performance  however  we must keep several processes in memory ; that is  we must share memory  in this chapter  we discuss various ways to manage memory the memorymanagement algorithms vary from a primitive bare-machine approach to paging and segmentation strategies each approach has its own advantages and disadvantages selection of a memory-management method for a specific system depends on many factors  especially on the hardware design of the system as we shall see  many algorithms require hardware support  although recent designs have closely integrated the hardware and operating system  to provide a detailed description of various ways of organizing memory hardware  to discuss various memory-management techniques  including paging and segmentation  to provide a detailed description of the intel pentium  which supports both pure segmentation and segmentation with paging  as we saw in chapter 1  memory is central to the operation of a modern computer system memory consists of a large array of words or bytes  each with its own address the cpu fetches instructions from memory according to the value of the program counter these instructions may cause additional loading from and storing to specific memory addresses  a typical instruction-execution cycle  for example  first fetches an instruction from memory the instruction is then decoded and may cause operands to be fetched from memory after the instruction has been executed on the 315 316 chapter 8 operands  results may be stored back in memory the mernory unit sees only a stream of memory addresses ; it does not know how they are generated  by the instruction counter  indexing  indirection  literal addresses  and so on  or what they are for  instructions or data   accordingly  we can ignore hozu a program generates a memory address we are interested only in the sequence of memory addresses generated by the running program  we begin our discussion by covering several issues that are pertinent to the various techniques for managing memory this coverage includes an overview of basic hardware issues  the binding of symbolic memory addresses to actual physical addresses  and the distinction between logical and physical addresses  we conclude the section with a discussion of dynamically loading and linking code and shared libraries  8.1.1 basic hardware main memory and the registers built into the processor itself are the only storage that the cpu can access directly there are machine instructions that take memory addresses as arguments  but none that take disk addresses therefore  any instructions in execution  and any data being used by the instructions  must be in one of these direct-access storage devices if the data are not in memory  they must be moved there before the cpu can operate on them  registers that are built into the cpu are generally accessible within one cycle of the cpu clock most cpus can decode instructions and perform simple operations on register contents at the rate of one or more operations per clock tick the same can not be said of main memory  which is accessed via a transaction on the memory bus completing a memory access may take many cycles of the cpu clock in such cases  the processor normally needs to stall  since it does not have the data required to complete the instruction that it is executing this situation is intolerable because of the frequency of memory accesses the remedy is to add fast memory between the cpu and 0 operating system 256000 process 300040 i soa  lj.o i process base 420940 i 120 ! 1go i i  limit process 880000 1024000 figure 8.1 a base and a limit register define a logical address space  8.1 317 main memory a memory buffer used to accommodate a speed differential  called a is described in section 1.8.3  not only are we concerned with the relative speed of accessing physical memory  but we also must ensure correct operation to protect the operating system from access by user processes and  in addition  to protect user processes from one another this protection must be provided by the hardware it can be implemented in several ways  as we shall see throughout the chapter in this section  we outline one possible implementation  we first need to make sure that each process has a separate memory space  to do this  we need the ability to determine the range of legal addresses that the process may access and to ensure that the process can access only these legal addresses we can provide this protection by using two registers  usually a base and a limit  as illustrated in figure 8.1 the base holds the smallest legal physical memory address ; the specifies the size of the range for example  if the base register holds 300040 and the limit register is 120900  then the program can legally access all addresses from 300040 through 420939  inclusive   protection of memory space is accomplished by having the cpu hardware compare every address generated in user mode with the registers any attempt by a program executing in user mode to access operating-system memory or other users ' memory results in a trap to the operating system  which treats the attempt as a fatal error  figure 8.2   this scheme prevents a user program from  accidentally or deliberately  modifying the code or data structures of either the operating system or other users  the base and limit registers can be loaded only by the operating system  which uses a special privileged instruction since privileged instructions can be executed only in kernel mode  and since only the operating system executes in kernel mode  only the operating system can load the base and limit registers  this scheme allows the operating system to change the value of the registers but prevents user programs from changing the registers ' contents  the operating system  executing in kernel mode  is given unrestricted access to both operating system memory and users ' memory this provision allows the operating system to load users ' programs into users ' memory  to yes no trap to operating system monitor-addressing error memory figure 8.2 hardware address protection with base and limit registers  318 chapter 8 dump out those programs in case of errors  to access and modify parameters of system calls  and so on  8.1.2 address binding usually  a program resides on a disk as a binary executable file to be executed  the program must be brought into memory and placed within a process  depending on the memory management in use  the process may be moved between disk and memory during its execution the processes on the disk that are waiting to be brought into memory for execution form the the normal procedure is to select one of the processes in the input queue and to load that process into memory as the process is executed  it accesses instructions and data from memory eventually  the process terminates  and its memory space is declared available  most systems allow a user process to reside in any part of the physical memory thus  although the address space of the computer starts at 00000  the first address of the user process need not be 00000 this approach affects the addresses that the user program can use in most cases  a user program will go through several steps-some of which may be optional-before bein.g executed  figure 8.3   addresses may be represented in different ways during these steps addresses in the source program are generally symbolic  such as count   a compiler will typically bind these symbolic addresses to relocatable addresses  such as 14 bytes from the beginning of this module   the lin.kage editor or loader will in turn bind the relocatable addresses to absolute addresses  such as 74014   each binding is a mapping from one address space to another  classically  the binding of instructions and data to memory addresses can be done at any step along the way  compile time if you know at compile time where the process will reside in memory  then can be generated for example  if you krww that a user process will reside starting at location r  then the generated compiler code will start at that location and extend up from there if  at some later time  the starting location changes  then it will be necessary to recompile this code the ms-dos .com-format programs are bound at compile time  load time if it is not known at compile time where the process will reside in memory  then the compiler must generate in this case  final binding is delayed until load time if the starting address changes  we need only reload the user code to incorporate this changed value  execution time if the process can be moved during its execution from one memory segment to another  then binding must be delayed until run time special hardware must be available for this scheme to work  as will be discussed in section 8.1.3 most general-purpose operating systems 11se this method  a major portion of this chapter is devoted to showing how these various bindings can be implemented effectively in a computer system and to discussing appropriate hardware support  8.1 compile time load time  execution time  run time  figure 8.3 multistep processing of a user program  8.1.3 logical versus physical address space an address generated by the cpu is commonly referred to as a 319 whereas an address seen by the memory unit-that is  the one loaded into the of the memory-is commonly referred to as a the compile-time and load-time address-binding methods generate identical logical and physical addresses however  the execution-time addressbinding scheme results in differing logical and addresses in this case  we usually refer to the logical address as a we use logical address and virtual address interchangeably in this text the set of all logical addresses generated by a program is a logical the set of all physical addresses corresponding to these logical addresses is a physical thus  in_ the execution-time address-binding scheme  the logical and physical address spaces differ  the run-time mapping from virtual to physical addresses is done by a hardware device called the we can choose from many different methods to accomplish such mapping  as we discuss in 320 chapter 8 figure 8.4 dynamic relocation using a relocation register  sections 8.3 through 8.7 for the time being  we illustrate this mapping with a simple mmu scheme that is a generalization of the base-register scheme described in section 8.1.1 the base register is now called a the value in the relocation register is added to every address generated by a user process at the time the address is sent to memory  see figure 8.4   for example  if the base is at 14000  then an attempt by the user to address location 0 is dynamically relocated to location 14000 ; an access to location 346 is mapped to location 14346 the ms-dos operating system running on the intel 80x86 family of processors used four relocation registers when loading and running processes  the user program never sees the real physical addresses the program can create a pointer to location 346  store it in memory  manipulate it  and compare it with other addresses-all as the number 346 only when it is used as a memory address  in an indirect load or store  perhaps  is it relocated relative to the base register the user program deals with logical addresses the memory-mapping hardware converts logical addresses into physical addresses this form of execution-time binding was discussed in section 8.1.2 the final location of a referenced memory address is not determined until the reference is made  we now have two different types of addresses  logical addresses  in the range 0 to max  and physical addresses  in the ranger + 0 tor + max for a base valuer   the user generates only logical addresses and thinks that the process runs in locations 0 to max the user program generates only logical addresses and thinks that the process runs in locations 0 to max however  these logical addresses must be mapped to physical addresses before they are used  the concept of a logical address space that is bound to a separate physical address space is central to proper memory management  8.1.4 dynamic loading in our discussion so far  it has been necessary for the entire program and all data of a process to be in physical memory for the process to execute the size of a process has thus been limited to the size of physical memory to obtain better memory-space utilization  we can use dynamic with dynancic 8.1 321 loading  a routine is not loaded until it is called all routines are kept on disk in a relocatable load format the main program is loaded into memory and is executed when a routine needs to call another routine  the calling routine first checks to see whether the other routine has been loaded if it has not  the relocatable linking loader is called to load the desired routine into menwry and to update the program 's address tables to reflect this change then control is passed to the newly loaded routine  the advantage of dynamic loading is that an unused routine is never loaded this method is particularly useful when large amounts of code are needed to handle infrequently occurring cases  such as error routines in this case  although the total program size may be large  the portion that is used  and hence loaded  may be much smaller  dynamic loading does not require special support from the operating system it is the responsibility of the users to design their programs to take advantage of such a method operating systems may help the programmer  however  by providing library routines to implement dynamic loading  8.1.5 dynamic linking and shared libraries figure 8.3 also shows some operating systems support only linking  in system language libraries are treated like any other object module and are combined by the loader into the binary program image dynamic linking  in contrast  is similar to dynamic loading  here  though  linking  rather than loading  is postponed until execution time  this feature is usually used with system libraries  such as language subroutine libraries without this facility  each program on a system must include a copy of its language library  or at least the routines referenced by the program  in the executable image this requirement wastes both disk space and main memory  with dynamic linking  a stub is included in the image for each libraryroutine reference the stub is a small piece of code that indicates how to locate the appropriate memory-resident library routine or how to load the library if the routine is not already present when the stub is executed  it checks to see whether the needed routine is already in memory if it is not  the program loads the routine into memory either way  the stub replaces itself with the address of the routine and executes the routine thus  the next time that particular code segment is reached  the library routine is executed directly  incurring no cost for dynamic linking under this scheme  all processes that use a language library execute only one copy of the library code  this feature can be extended to library updates  such as bug fixes   a library may be replaced by a new version  and all programs that reference the library will automatically use the new version without dynamic linking  all such programs would need to be relinked to gain access to the new library so that programs will not accidentally execute new  incompatible versions of libraries  version information is included in both the program and the library more than one version of a library may be loaded into memory  and each program uses its version information to decide which copy of the library to use versions with minor changes retain the same version number  whereas versions with major changes increment the number thus  only programs that are compiled with the new library version are affected by any incompatible changes incorporated 322 chapter 8 8.2 in it other programs linked before the new library was installed will continue using the older library this system is also known as 'h  = ' unlike dynamic loading  dynamic linking generally requires help from the operating system if the processes in memory are protected from one another  then the operating system is the only entity that can check to see whether the needed routine is in another process 's memory space or that can allow multiple processes to access the same memory addresses we elaborate on this concept when we discuss paging in section 8.4.4  a process must be in memory to be executed a process  however  can be temporarily out of memory to a and then brought into memory for continued execution for example  assume a multiprogramming environment with a round-robin cpu-scheduling algorithm when a quantum expires  the memory manager will start to swap out the process that just finished and to swap another process into the memory space that has been freed  figure 8.5   in the meantime  the cpu scheduler will allocate a time slice to some other process in memory when each process finishes its quantum  it will be swapped with another process ideally  the memory manager can swap processes fast enough that some processes will be in memory  ready to execute  when the cpu scheduler wants to reschedule the cpu in addition  the quantum must be large enough to allow reasonable amounts of computing to be done between swaps  a variant of this swapping policy is used for priority-based scheduling algorithms if a higher-priority process arrives and wants service  the memory manager can swap out the lower-priority process and then load and execute the higher-priority process when the higher-priority process finishes  the @ swap out @ swap in backing store main memory figure 8.5 swapping of two processes using a disk as a backing store  8.2 323 lower-priority process can be swapped back in and continued this variant of swapping is sometimes called roll normally  a process that is swapped out will be swapped back into the same memory space it occupied previously this restriction is dictated by the method of address binding if binding is done at assembly or load time  then the process can not be easily moved to a different location if execution-time binding is being used  however  then a process can be swapped into a different memory space  because the physical addresses are computed during execution time  swapping requires a backing store the backing store is commonly a fast disk it must be large enough to accommodate copies of all memory images for all users  and it must provide direct access to these memory images the system maintains a consisting of all processes whose memory images are on the backing store or in memory and are ready to run whenever the cpu scheduler decides to execute a process  it calls the dispatcher the dispatcher checks to see whether the next process in the queue is in memory  if it is not  and if there is no free memory region  the dispatcher swaps out a process currently in memory and swaps in the desired process it then reloads registers and transfers control to the selected process  the context-switch time in such a swapping system is fairly high to get an idea of the context-switch time  let us assume that the user process is 100 mb in size and the backing store is a standard hard disk with a transfer rate of 50mb per second the actual transfer of the 100-mb process to or from main memory takes 100mb/50mb per second = 2 seconds  assuming an average latency of 8 milliseconds  the swap time is 2008 milliseconds since we must both swap out and swap in  the total swap time is about 4016 milliseconds  notice that the major part of the swap time is transfer time the total transfer time is directly proportional to the amount of memory swapped if we have a computer system with 4 gb of main memory and a resident operating system taking 1 gb  the maximum size of the user process is 3gb however  many user processes may be much smaller than this-say  100 mb a 100-mb process could be swapped out in 2 seconds  compared with the 60 seconds required for swapping 3 gb clearly  it would be useful to know exactly how much memory a user process is using  not simply how much it might be using  then we would need to swap only what is actually used  reducing swap time  for this method to be effective  the user must keep the system informed of any changes in memory requirements thus  a process with dynamic memory requirements will need to issue system calls  request memory and release memory  to inform the operating system of its changing memory needs  swapping is constrained by other factors as well if we want to swap a process  we must be sure that it is completely idle of particular concern is any pending i/0 a process may be waiting for an i/0 operation when we want to swap that process to free up memory however  if the i/0 is asynchronously accessing the user memory for i/0 buffers  then the process can not be swapped assume that the i/0 operation is queued because the device is busy if we were to swap out process p1 and swap in process p2  the 324 chapter 8 8.3 i/0 operation might then attempt to use memory that now belongs to process p2  there are two main solutions to this problem  never swap a process with pending i/0  or execute i/0 operations only into operating-system buffers  transfers between operating-system buffers and process memory then occur only when the process is swapped in  the assumption  mentioned earlier  that swapping requires few  if any  head seeks needs further explanation we postpone discussing this issue until chapter 12  where secondary-storage structure is covered generally  swap space is allocated as a chunk of disk  separate from the file system  so that its use is as fast as possible  currently  standard swapping is used in few systems it requires too much swapping time and provides too little execution time to be a reasonable memory-management solution modified versions of swapping  however  are found on many systems  a modification of swapping is used in many versions of unix swapping is normally disabled but will start if many processes are running and are using a threshold amount of memory swapping is again halted when the load on the system is reduced memory management in unix is described fully in sections 21.7 and a.6  early pcs-which lacked the sophistication to implement more advanced memory-management methods-ran multiple large processes by using a modified version of swapping a prime example is the microsoft windows 3.1 operating system  which supports concurrent execution of processes in memory if a new process is loaded and there is insufficient main memory  an old process is swapped to disk this operating system does not provide full swapping  however  because the user  rather than the scheduler  decides when it is time to preempt one process for another any swapped-out process remains swapped out  and not executing  until the user selects that process to run subsequent versions of microsoft operating systems take advantage of the advanced mmu features now found in pcs we explore such features in section 8.4 and in chapter 9  where we cover virtual memory  the main memory must accommodate both the operating system and the various user processes we therefore need to allocate main menlory in the most efficient way possible this section explains one common method  contiguous memory allocation  the memory is usually divided into two partitions  one for the resident operating system and one for the user processes we can place the operating system in either low memory or high memory the major factor affecting this decision is the location of the interrupt vector since the interrupt vector is often in low memory  programmers usually place the operating system in low memory as well thus  in this text  we discuss only the situation in which the operating system resides in low memory the development of the other situation is similar  we usually want several user processes to reside in memory at the same time we therefore need to consider how to allocate available memory to the processes that are in the input queue waiting to be brought into memory  8.3 325 in contiguous memory allocation  each process is contained in a single contiguous section of memory  8.3.1 memory mapping and protection before discussing memory allocation further  we must discuss the issue of memory mapping and protection we can provide these features by using a relocation register  as discussed in section 8.1.3  together with a limit register  as discussed in section 8.1.1 the relocation register contaitls the value of the smallest physical address ; the limit register contains the range of logical addresses  for example  relocation = 100040 and limit = 74600   with relocation and limit registers  each logical address must be less than the limit register ; the mmu maps the logical address dynamically by adding the value in the relocation register this mapped address is sent to memory  figure 8.6   when the cpu scheduler selects a process for execution  the dispatcher loads the relocation and limit registers with the correct values as part of the context switch because every address generated by a cpu is checked against these registers  we can protect both the operating system and the other users ' programs and data from being modified by this running process  the relocation-register scheme provides an effective way to allow the operating system 's size to change dynamically this flexibility is desirable in many situations for example  the operating system contains code and buffer space for device drivers if a device driver  or other operating-system service  is not commonly used  we do not want to keep the code and data in memory  as we might be able to use that space for other purposes such code is sometimes called transient operating-system code ; it comes and goes as needed thus  using this code changes the size of the operating system during program execution  8.3.2 memory allocation now we are ready to turn to memory allocation one of the simplest methods for allocating memory is to divide memory into several fixed-sized each partition may contain exactly one process thus  the degree no trap  addressing error figure 8.6 hardware supportfor relocation and limit registers  326 chapter 8 of multiprogramming is bound by the number of partitions in this when a partition is free  a process is selected from the input queue and is loaded into the free partition when the process terminates  the partition becomes available for another process this method was originally used by the ibm os/360 operating system  called mft  ; it is no longer in use  the method described next is a generalization of the fixed-partition scheme  called mvt  ; it is used primarily in batch environments many of the ideas presented here are also applicable to a time-sharing environment in which pure segmentation is used for memory management  section 8.6   in the scheme  the operating system keeps a table indicating which parts of memory are available and which are occupied  initially  all memory is available for user processes and is considered one large block of available memory a eventually as you will see  memory contains a set of holes of various sizes  as processes enter the system  they are put into an input queue the operating system takes into account the memory requirements of each process and the amount of available memory space in determining which processes are allocated memory when a process is allocated space  it is loaded into memory  and it can then compete for cpu time when a process terminates  it releases its memory which the operating system may then fill with another process from the input queue  at any given time  then  we have a list of available block sizes and an input queue the operating system can order the input queue according to a scheduling algorithm memory is allocated to processes untit finally  the memory requirements of the next process can not be satisfied -that is  no available block of memory  or hole  is large enough to hold that process the operating system can then wait until a large enough block is available  or it can skip down the input queue to see whether the smaller memory requirements of some other process can be met  in generat as mentioned  the memory blocks available comprise a set of holes of various sizes scattered throughout memory when a process arrives and needs memory  the system searches the set for a hole that is large enough for this process if the hole is too large  it is split into two parts one part is allocated to the arriving process ; the other is returned to the set of holes when a process terminates  it releases its block of memory  which is then placed back in the set of holes if the new hole is adjacent to other holes  these adjacent holes are merged to form one larger hole at this point  the system may need to check whether there are processes waiting for memory and whether this newly freed and recombined memory could satisfy the demands of any of these waiting processes  this procedure is a particular instance of the general which concerns how to satisfy a request of size n from a there are many solutions to this problem the and strategies are the ones most commonly used to select a free hole from the set of available holes  first fit allocate the first hole that is big enough searching can start either at the beginning of the set of holes or at the location where the previous first-fit search ended we can stop searching as soon as we find a free hole that is large enough  8.3 327 best fit allocate the smallest hole that is big enough we must search the entire list  unless the list is ordered by size this strategy produces the smallest leftover hole  worst fit allocate the largest hole again  we must search the entire list  unless it is sorted by size this strategy produces the largest leftover hole  which may be more useful than the smaller leftover hole from a best-fit approach  simulations have shown that both first fit and best fit are better than worst fit in terms of decreasing time and storage utilization neither first fit nor best fit is clearly better than the other in terms of storage utilization  but first fit is generally faster  8.3.3 fragmentation both the first-fit and best-fit strategies for memory allocation suffer from external as processes are loaded and removed from memory  the free memory space is broken into little pieces external fragmentation exists when there is enough total memory space to satisfy a request but the available spaces are not contiguous ; storage is fragmented into a large number of small holes this fragmentation problem can be severe in the worst case  we could have a block of free  or wasted  memory between every two processes if all these small pieces of memory were in one big free block instead  we might be able to run several more processes  whether we are using the first-fit or best-fit strategy can affect the amount of fragmentation  first fit is better for some systems  whereas best fit is better for others  another factor is which end of a free block is allocated  which is the leftover piece-the one on the top or the one on the bottom  no matter which algorithm is used  however  external fragmentation will be a problem  depending on the total amount of memory storage and the average process size  external fragmentation may be a minor or a major problem statistical analysis of first fit  for instance  reveals that  even with some optimization  given n allocated blocks  another 0.5 n blocks will be lost to fragmentation  that is  one-third of memory may be unusable ! this property is known as the memory fragmentation can be internal as well as external consider a multiple-partition allocation scheme with a hole of 18,464 bytes suppose that the next process requests 18,462 bytes if we allocate exactly the requested block  we are left with a hole of 2 bytes the overhead to keep track of this hole will be substantially larger than the hole itself the general approach to avoiding this problem is to break the physical memory into fixed-sized blocks and allocate memory in units based on block size with this approach  the memory allocated to a process may be slightly larger than the requested memory the difference between these two numbers is internal memory that is internal to a partition  one solution to the problem of external fragmentation is the goal is to shuffle the memory contents so as to place all free n'lemory together in one large block compaction is not always possible  however if relocation is static and is done at assembly or load time  compaction can not be done ; compaction is possible only if relocation is dynamic and is done at execution 328 chapter 8 8.4 time if addresses are relocated dynamically  relocation requires only moving the program and data and then changing the base register to reflect the new base address when compaction is possible  we must determine its cost the simplest compaction algorithm is to move all processes toward one end of memory ; all holes move in the other direction  producing one large hole of available memory this scheme can be expensive  another possible solution to the external-fragmentation problem is to permit the logical address space of the processes to be noncontiguous  thus allowing a process to be allocated physical memory wherever such memory is available two complementary techniques achieve this solution  paging  section 8.4  and segmentation  section 8.6   these techniques can also be combined  section 8.7   is a memory-management scheme that permits the physical address space a process to be noncontiguous paging avoids external fragmentation and the need for compaction it also solves the considerable problem of fitting memory chunks of varying sizes onto the backin.g store ; most memorymanagement schemes used before the introduction of paging suffered from this problem the problem arises because  when some code fragments or data residing in main memory need to be swapped out  space must be fmmd on the backing store the backing store has the same fragmentation problems discussed in connection with main memory  but access is much slower  so compaction is impossible because of its advantages over earlier methods  paging in its various forms is used in most operating systems  physical address foooo  0000 f1111  1111 page table figure 8.7 paging hardware  1---------1 physical memory 8.4 329 traditionally  support for paging has been handled by hardware however  recent designs have implemented paging by closely integrating the hardware and operating system  especially on 64-bit microprocessors  8.4.1 basic method the basic method for implementing paging involves breaking physical memory into fixed-sized blocks called harnes and breaking logical memory into blocks of the same size called when a process is to be executed  its pages are loaded into any available memory frames from their source  a file system or the backing store   the backing store is divided into fixed-sized blocks that are of the san1.e size as the memory frames  the hardware support for paging is illustrated in figure 8.7 every address generated the cpu is divided into two parts  a  p  and a  the page number is used as an index into a the page table contains the base address of each page in physical memory this base address is combined with the page offset to define the physical memory address that is sent to the memory unit the paging model of memory is shown in figure 8.8  the page size  like the frame size  is defined by the hardware the size of a page is typically a power of 2  varying between 512 bytes and 16 mb per page  depending on the computer architecture the selection of a power of 2 as a page size makes the translation of a logical address into a page number and page offset particularly easy if the size of the logical address space is 2m  and a page size is 271 addressing units  bytes or wordst then the high-order m n bits of a logical address designate the page number  and the n low-order bits designate the page offset thus  the logical address is as follows  logical memory ~ w page table frame number physical memory figure 8.8 paging model of logical and physical memory  330 chapter 8 page number page offset d m -n n where p is an index into the page table and d is the displacement within the page  as a concrete  although minuscule  example  consider the memory in figure 8.9 here  in the logical address  n = 2 and m = 4 using a page size of 4 bytes and a physical memory of 32 bytes  8 pages   we show how the user 's view of memory can be mapped into physical memory logical address 0 is page 0  offset 0 indexing into the page table  we find that page 0 is in frame 5 thus  logical address 0 maps to physical address 20  =  5 x 4  + 0   logical address 3  page 0  offset 3  maps to physical address 23  =  5 x 4  + 3   logical address 4 is page 1  offset 0 ; according to the page table  page 1 is mapped to frame 6 thus  logical address 4 maps to physical address 24  =  6 x 4  + o   logical address 13 maps to physical address 9  you may have noticed that paging itself is a form of dynamic relocation  every logical address is bound by the paging hardware to some physical address using paging is similar to using a table of base  or relocation  registers  one for each frame of memory  ~ m6 2 1 3 2 page table logical memory physical memory figure 8.9 paging example for a 32-byte memory with 4-byte pages  8.4 331 when we use a paging scheme  we have no external fragmentation  any free frame can be allocated to a process that needs it however  we may have some internal fragmentation notice that frames are allocated as units if the memory requirements of a process do not happen to coincide with page boundaries  the last frame allocated may not be completely full for example  if page size is 2,048 bytes  a process of 72,766 bytes will need 35 pages plus 1,086 bytes it will be allocated 36 frames  resulting in internal fragmentation of 2,048  1,086 = 962 bytes in the worst case  a process would need 11 pages plus 1 byte it would be allocated 11 + 1 frames  resulting in internal fragmentation of almost an entire frame  if process size is independent of page size  we expect internal fragmentation to average one-half page per process this consideration suggests that small page sizes are desirable however  overhead is involved in each page-table entry  and this overhead is reduced as the size of the pages increases also  disk i/0 is more efficient when the amount data being transferred is larger  chapter 12   generally  page sizes have grown over time as processes  data sets  and main memory have become larger today  pages typically are between 4 kb and 8 kb in size  and some systems support even larger page sizes some cpus and kernels even support multiple page sizes for instance  solaris uses page sizes of 8 kb and 4 mb  depending on the data stored by the pages  researchers are now developing support for variable on-the-fly page size  usually  each page-table entry is 4 bytes long  but that size can vary as well  a 32-bit entry can point to one of 232 physical page frames if frame size is 4 kb  then a system with 4-byte entries can address 244 bytes  or 16 tb  of physical memory  when a process arrives in the system to be executed  its size  expressed in pages  is examined each page of the process needs one frame thus  if the process requires 11 pages  at least 11 frames must be available in memory if n frames are available  they are allocated to this arriving process the first page of the process is loaded injo one of the allocated frames  and the frame number is put in the page table for this process the next page is loaded into another frame  its frame number is put into the page table  and so on  figure 8.10   an important aspect of paging is the clear separation between the user 's view of memory and the actual physical memory the user program views memory as one single space  containing only this one program in fact  the user program is scattered throughout physical memory  which also holds other programs the difference between the user 's view of memory and the actual physical memory is reconciled by the address-translation hardware the logical addresses are translated into physical addresses this mapping is hidden from the user and is controlled by the operating system notice that the user process by definition is unable to access memory it does not own it has no way of addressing memory outside of its page table  and the table includes only those pages that the process owns  since the operating system is managing physical memory  it must be aware of the allocation details of physical memory-which frames are allocated  which frames are available  how many total frames there are  and so on this information is generally kept in a data structure called a frame the frame table has one entry for each physical page frame  indicating whether the latter is free or allocated and  if it is allocated  to which page of which process or processes  332 chapter 8 free-frame list free-frame list 14 13 15 13 13 18 20 14 14 15 15 15 16 16 17 17 18 18 19 01 19 1 13 20 2 18 20 3.20 21 new-process page table 21  a   b  figure 8.10 free frames  a  before allocation and  b  after allocation  in addition  the operating system must be aware that user processes operate in user space  and all logical addresses must be mapped to produce physical addresses if a user makes a system call  to do i/0  for example  and provides an address as a parameter  a buffe1 ~ for instance   that address must be mapped to produce the correct physical address the operating system maintains a copy of the page table for each process  just as it maintains a copy of the instruction counter and register contents this copy is used to translate logical addresses to physical addresses whenever the operating system must map a logical address to a physical address manually it is also used by the cpu dispatcher to define the hardware page table when a process is to be allocated the cpu paging therefore increases the context-switch time  8.4.2 hardware support each operating system has its own methods for storing page tables most allocate a page table for each process a pointer to the page table is stored with the other register values  like the instruction counter  in the process control block when the dispatcher is told to start a process  it must reload the user registers and define the correct hardware page-table values from the stored user page table  the hardware implementation of the page table can be done in several in the simplest case  the page table is implemented as a set of dedicated these registers should be built with very high-speed logic to make the paging-address translation efficient every access to memory nlust go through the paging map  so efficiency is a major consideration the cpu dispatcher reloads these registers  just as it reloads the other registers instructions to load or modify the page-table registers are  of course  privileged  so that only the operating system can change the memory map the dec pdp-11 is an example of such an architecture the address consists of 16 bits  and the page size is 8 kb the page table thus consists of eight entries that are kept in fast registers  8.4 333 the use of registers for the page table is satisfactory if the page table is reasonably sncall  for example  256 entries   most contemporary computers  however  allow the page table to be very large  for example  1 million entries   for these machines  the use of fast registers to implement the page table is not feasible rather  the page table is kept in main memory  and a points to the page table changing page tables requires changing only this one register  substantially reducing context-switch time  the problem with this approach is the time required to access a user memory location if we want to access location i  we must first index into the page table  using the value in the ptbr offset by the page number fori this task requires a memory access it provides us with the frame number  which is combined with the page offset to produce the actual address we can then access the desired place in memory with this scheme  two memory accesses are needed to access a byte  one for the page-table entry  one for the byte   thus  memory access is slowed by a factor of 2 this delay would be intolerable under most circumstances we might as well resort to swapping ! the standard solution to this problem is to use a special  small  fastlookup hardware cache  called a bc.1her the tlb is associative  high-speed memory each entry in the tlb consists of two parts  a key  or tag  and a value when the associative memory is presented with an item  the item is compared with all keys simultaneously if the item is found  the corresponding value field is returned the search is fast ; the hardware  however  is expensive typically  the number of entries in a tlb is small  often numbering between 64 and 1,024  the tlb is used with page tables in the following way the tlb contains only a few of the page-table entries when a logical address is generated by the cpu  its page number is presented to the tlb if the page number is found  its frame number is immediately available and is used to access memory the whole task may take less than 10 percent longer than it would if an unmapped memory reference were used  if the page number is not in the tlb  known as a a memory reference to the page table must be made when the frame number is obtained  we can use it to access memory  figure 8.11   in addition  we add the page number and frame number to the tlb  so that they will be found quickly on the next reference if the tlb is already full of entries  the operating system must select one for replacement replacement policies range from least recently used  lru  to random furthermore  some tlbs allow certain entries to be meaning that they can not be removed from the tlb typically  tlb entries for kernel code are wired down  some tlbs store in each tlb entry an asid uniquely identifies each process and is used to provide address-space protection for that process when the tlb attempts to resolve virtual page numbers  it ensures that the asid for the currently running process matches the asid associated with the virtual page if the asids do not match  the attempt is treated as a tlb miss in addition to providing address-space protection  an asid allows the tlb to contain entries for several different processes simultaneously  if the tlb does not support separate asids  then every time a new table is selected  for instance  with each context switch   the tlb must  or erased  to ensure that the next executing process does not use the wrong translation information otherwise  the tlb could include old entries that 334 chapter 8 tlb hit tlb p tlb miss page table figure 8.11 paging hardware with tlb  physical memory contain valid virtual addresses but have incorrect or invalid physical addresses left over from the previous process  the percentage of times that a particular page number is found in the tlb is called the an 80-percent hit ratio  for example  means that we find the desired page number in the tlb 80 percent of the time if it takes 20 nanoseconds to search the tlb and 100 nanoseconds to access memory  then a mapped-memory access takes 120 nanoseconds when the page number is in the tlb if we fail to find the page number in the tlb  20 nanoseconds   then we must first access memory for the page table and frame number  100 nanoseconds  and then access the desired byte in memory  100 nanoseconds   for a total of 220 nanoseconds to find the effective we weight the case by its probability  effective access time = 0.80 x 120 + 0.20 x 220 = 140 nanoseconds  in this example  we suffer a 40-percent slowdown in memory-access time  from 100 to 140 nanoseconds   for a 98-percent hit ratio  we have effective access time = 0.98 x 120 + 0.02 x 220 = 122 nanoseconds  this increased hit rate produces only a 22 percent slowdown in access time  we will further explore the impact of the hit ratio on the tlb in chapter 9  8.4 335 8.4.3 protection memory protection in a paged environment is accomplished by protection bits associated with each frame normally  these bits are kept in the page table  one bit can define a page to be read-write or read-only every reference to memory goes through the page table to find the correct frame nuncber at the same time that the physical address is being computed  the protection bits can be checked to verify that no writes are being made to a read-only page an attempt to write to a read-only page causes a hardware trap to the operating system  or memory-protection violation   we can easily expand this approach to provide a finer level of protection  we can create hardware to provide read-only  read-write  or execute-only protection ; or  by providing separate protection bits for each kind of access  we can allow any combination of these accesses illegal attempts will be trapped to the operating system  one additional bit is generally attached to each entry in the page table  a bit when this bit is set to valid  the associated page is in the process 's logical address space and is thus a legal  or valid  page when the bit is set to invalid  the page is not in the process 's logical address space illegal addresses are trapped by use of the valid -invalid bit the operating system sets this bit for each page to allow or disallow access to the page  suppose  for example  that in a system with a 14-bit address space  0 to 16383   we have a program that should use only addresses 0 to 10468 given a page size of 2 kb  we have the situation shown in figure 8.12 addresses in 0 frame number j valid-invalid bit 0 10,468 1 2,287 '-----'--'--' ' page n figure 8 i 2 valid  v  or invalid  i  bit in a page table  336 chapter 8 pages 0  1  2  3  4  and 5 are mapped normally through the page table any attempt to generate an address in pages 6 or 7  however  will find that the valid -invalid bit is set to invalid  and the computer will trap to flee operating system  invalid page reference   notice that this scheme has created a problem because the program extends only to address 10468  any reference beyond that address is illegal  howeve1 ~ references to page 5 are classified as valid  so accesses to addresses up to 12287 are valid only the addresses from 12288 to 16383 are invalid this problem is a result of the 2-kb page size and reflects the internal fragmentation of paging  rarely does a process use all its address range in fact many processes use only a small fraction of the address space available to them it would be wasteful in these cases to create a page table with entries for every page in the address range most of this table would be unused but would take up valuable memory space some systems provide hardware  in the form of a length to indicate the size of the page table value is checked against every logical address to verify that the address is in the valid range for the process failure of this test causes an error trap to the operating system  8.4.4 shared pages an advantage of paging is the possibility of sharing common code this consideration is particularly important in a time-sharing environment consider a system that supports 40 users  each of whom executes a text editor if the text editor consists of 150 kb of code and 50 kb of data space  we need 8,000 kb to support the 40 users if the code is  or pure however  it can be shared  as shown in figure 8.13 here we see a three-page editor-each page 50 kb in size  the large page size is used to simplify the figure  -being shared among three processes each process has its own data page  reentrant code is non-self-modifying code  it never changes during execution  thus  two or more processes can execute the same code at the same time  each process has its own copy of registers and data storage to hold the data for the process 's execution the data for two different processes wilt of course  be different  only one copy of the editor need be kept in physical memory each user 's page table maps onto the same physical copy of the editor  but data pages are mapped onto different frames thus  to support 40 users  we need only one copy of the editor  150 kb   plus 40 copies of the 50 kb of data space per user  the total space required is now 2  50 kb instead of 8,000 kb-a significant savings  other heavily used programs can also be shared -compilers  window systems  run-time libraries  database systems  and so on to be sharable  the code must be reentrant the read-only nature of shared code should not be left to the correctness of the code ; the operating system should enforce this property  the sharing of memory among processes on a system is similar to the sharing of the address space of a task by threads  described in chapter 4  furthermore  recall that in chapter 3 we described shared memory as a method 8.5 ed 1   ed 2 ed 3 data .1 process p1 process p3 page table for p1 page table for p3 8.5 ed 1 ed 2 ed 3 data 2 process p2 0 data 1 2 data 3 3 ed 1 ed 2 ed 3  4 5 6 data 2 page table for p2 7 8 9 10 11 figure 8.13 sharing of code in a paging environment  337 of interprocess corrununication some operating systems implement shared memory using shared pages  organizing memory according to pages provides numerous benefits in addition to allowing several processes to share the same physical pages we cover several other benefits in chapter 9  in this section  we explore some of the most common techniques for structuring the page table  8.5.1 hierarchical paging most modern computer systems support a large logical address space  232 to 264   in such an environment  the page table itself becomes excessively large for example  consider a system with a 32-bit logical address space if the page size in such a system is 4 kb  212   then a page table may consist of up to 1 million entries  232 /212   assuming that each entry consists of 4 bytes  each process may need up to 4mb of physical address space for the page table alone clearly  we would not want to allocate the page table contiguously in main memory one simple solution to this problem is to divide the page table into smaller pieces we can accomplish this division in several ways  one way is to use a two-level paging algorithm  in which the page table itself is also paged  figure 8.14   for example  consider again the system with 338 chapter 8 0 page table memory figure 8.14 a two-level page-table scheme  a 32-bit logical address space and a page size of 4 kb a logical address is divided into a page number consisting of 20 bits and a page offset consisting of 12 bits because we page the page table  the page number is further divided into a 10-bit page number and a 10-bit page offset thus  a logical address is as follows  page number page offset d 10 10 12 where p1 is an index into the outer page table and p2 is the displacement within the page of the outer page table the address-translation method for this architecture is shown in figure 8.15 because address translation works from the outer page table inward  this scheme is also known as a the vax architecture supports a variation of two-level paging the vax is a 32-bit machine with a page size of 512 bytes the logical address space of a process is divided into four equal sections  each of which consists of 230 bytes  each section represents a different part of the logical address space of a process  the first 2 high-order bits of the logical address designate the appropriate section the next 21 bits represent the logical page number of that section  and the final 9 bits represent an offset in the desired page by partitioning the page outer page table 8.5 figure 8 15 address translation for a two-level 32-bit paging architecture  339 table in this manner  the operating system can leave partitions unused until a process needs them an address on the vax architecture is as follows  section page offset s p d 2 21 9 where s designates the section number  p is an index into the page table  and d is the displacement within the page even when this scheme is used  the size of a one-level page table for a vax process using one section is 221 bits 4 bytes per entry = 8mb to further reduce main-memory use  the vax pages the user-process page tables  for a system with a 64-bit logical address space  a two-level paging scheme is no longer appropriate to illustrate this point  let us suppose that the page size in such a system is 4 kb  212   in this case  the page table consists of up to 252 entries if we use a two-level paging scheme  then the iml.er page tables can conveniently be one page long  or contain 210 4-byte entries the addresses look like this  outer page inner page offset i  pl  i p2  i d 42 10 12 the outer page table consists of 242 entries  or 244 bytes the obvious way to avoid such a large table is to divide the outer page table into smaller pieces   this approach is also used on some 32-bit processors for added flexibility and efficiency  we can divide the outer page table in various ways we can page the outer page table  giving us a three-level paging scheme suppose that the outer page table is made up of standard-size pages  210 entries  or 212 bytes   in this case  a 64-bit address space is still daunting  2nd outer page outer page inner page offset i pr   p2 i p3 i d 32 10 10 12 the outer page table is sti11234 bytes in size  340 chapter 8 the next step would be a four-level paging scheme  where the second-level outer page table itself is also paged  and so forth the 64-bit ultrasparc would require seven levels of paging-a prohibitive number of memory accessesto translate each logical address you can see from this example why  for 64-bit architectures  hierarchical page tables are generally considered inappropriate  8.5.2 hashed page tables a common approach for handling address spaces larger than 32 bits is to use a with the hash value being the virtual page number each entry in the hash table contains a linked list of elements that hash to the same location  to handle collisions   each element consists of three fields   1  the virtual page number   2  the value of the mapped page frame  and  3  a pointer to the next element in the linked list  the algorithm works as follows  the virtual page number in the virtual address is hashed into the hash table the virtual page number is compared with field 1 in the first element in the linked list if there is a match  the corresponding page frame  field 2  is used to form the desired physical address  if there is no match  subsequent entries in the linked list are searched for a matching virtual page number this scheme is shown in figure 8.16  a variation of this scheme that is favorable for 64-bit address spaces has been proposed this variation uses which are similar to hashed page tables except that each entry in the hash table refers to several pages  such as 16  rather than a single page therefore  a single page-table entry can store the mappings for multiple physical-page frames clustered page tables are particularly useful for address spaces  where memory references are noncontiguous and scattered throughout the address space  8.5.3 inverted page tables usually  each process has an associated page table the page table has one entry for each page that the process is using  or one slot for each virtual hash table figure 8.16 hashed page table  physical address physical memory 8.5 341 address  regardless of the latter 's validity   this table representation is a natural one  since processes reference pages through the pages ' virtual addresses the operating system must then translate this reference into a physical memory address since the table is sorted by virtual address  the operating system is able to calculate where in the table the associated physical address entry is located and to use that value directly one of the drawbacks of this method is that each page table may consist of millions of entries these tables may consume large amounts of physical memory just to keep track of how other physical memory is being used  to solve this problem  we can use an page an inverted page table has one entry for each real page  or frame  of memory each entry consists of the virtual address of the page stored in that real memory location  with information about the process that owns the page thus  only one page table is in the system  and it has only one entry for each page of physical memory figure 8.17 shows the operation of an inverted page table compare it with figure 8.7  which depicts a standard page table in operation inverted page tables often require that an address-space identifier  section 8.4.2  be stored in each entry of the page table  since the table usually contains several different address spaces mapping physical memory storing the address-space identifier ensures that a logical page for a particular process is mapped to the corresponding physical page frame examples of systems using inverted page tables include the 64-bit ultrasparc and powerpc  to illustrate this method  we describe a simplified version of the i11verted page table used in the ibm rt each virtual address in the system consists of a triple  process-id  page-number  offset  each inverted page-table entry is a pair process-id  page-number where the process-id assumes the role of the address-space identifier when a memory page table physical address figure 8.17 inverted page table  physical memory 342 chapter 8 8.6 reference occurs  part of the virtual address  consisting of process-id  pagenumber  is presented to the memory subsystem the inverted page table is then searched for a match if a match is found-say  at entry i-then the physical address i  offset is generated if no match is found  then an illegal address access has been attempted  although this scheme decreases the amount of memory needed to store each page table  it increases the amount of time needed to search the table when a page reference occurs because the inverted page table is sorted by physical address  but lookups occur on virtual addresses  the whole table might need to be searched for a match this search would take far too long to alleviate this problem  we use a hash table  as described in section 8.5.2  to limit the search to one-or at most a few-page-table entries of course  each access to the hash table adds a memory reference to the procedure  so one virtual memory reference requires at least two real memory reads-one for the hash-table entry and one for the page table  recall that the tlb is searched first  before the hash table is consulted  offering some performance improvement  systems that use inverted page tables have difficulty implementing shared memory shared memory is usually implemented as multiple virtual addresses  one for each process sharing the memory  that are mapped to one physical address this standard method can not be used with inverted page tables ; because there is only one virtual page entry for every physical page  one physical page can not have two  or more  shared virtual addresses a simple technique for addressing this issue is to allow the page table to contain only one mapping of a virtual address to the shared physical address this means that references to virtual addresses that are not mapped result in page faults  an important aspect of memory management that became unavoidable with paging is the separation of the user 's view of memory from the actual physical memory as we have already seen  the user 's view of memory is not the same as the actual physical memory the user 's view is mapped onto physical memory this mapping allows differentiation between logical memory and physical memory  8.6.1 basic method do users think of memory as a linear array of bytes  some containing instructions and others containing data most people would say no rather  users prefer to view memory as a collection of variable-sized segments  with no necessary ordering among segments  figure 8.18   consider how you think of a program when you are writing it you think of it as a main program with a set of methods  procedures  or functions it may also include various data structures  objects  arrays  stacks  variables  and so on each of these modules or data elements is referred to by name you talk about the stack  the math library  the n1.ain program  without caring what addresses in memory these elements occupy you are not concerned with whether the stack is stored before or after the sqrt   function each of these segments is of variable length ; the length is intrinsically defined by subroutine symbol table  main program logical address 8.6 figure 8.18 user 's view of a program  343 the purpose of the segment in the program elements within a segment are identified by their offset from the begim1.ing of the segment  the first statement of the program  the seventh stack frame entry in the stack  the fifth instruction of the sqrt    and so on  is a memory-management scheme that supports this user view of memory a logical address space is a collection of segments each segment has a name and a length the addresses specify both the segment name and the offset within the segment the user therefore specifies each address by two quantities  a segment name and an offset  contrast this scheme with the paging scheme  in which the user specifies only a single address  which is partitioned by the hardware into a page number and an offset  all invisible to the programmer  for simplicity of implementation  segments are numbered and are referred to by a segn lent number  rather than by a segment name thus  a logical address consists of a two tuple  segment-number  offset  normally  the user program is compiled  and the compiler automatically constructs segments reflecting the input program  a c compiler might create separate segments for the following  the code global variables the heap  from which memory is allocated the stacks used by each thread the standard c library 344 chapter 8 no segment table yes trap  addressing error + figure 8.19 segmentation hardware  physical memory libraries that are linked in during compile time might be assign.ed separate segments the loader would take all these segments and assign them segment numbers  8.6.2 hardware although the user can now refer to objects in the program by a two-dimensional address  the actual physical memory is still  of course  a one-dimensional sequence of bytes thus  we must define an implementation to map twodimensional user-defined addresses into one-dimensional physical addresses  this mapping is effected by a each entry in the segment table has a segment base and a segment limit the segment base contains the startilcg physical address where the segment resides in memory  and the segment limit specifies the length of the segment  the use of a segment table is illustrated in figure 8.19 a logical address consists of two parts  a segment number  s  and an offset into that segment  d  the segment number is used as an index to the segment table the offset d of the logical address must be between 0 and the segment limit if it is not  we trap to the operating system  logical addressing attempt beyond end of segment   when an offset is legal  it is added to the segment base to produce the address in physical memory of the desired byte the segment table is thus essentially an array of base-limit register pairs  as an example  consider the situation shown in figure 8.20 we have five segments numbered from 0 through 4 the segments are stored in physical memory as shown the segment table has a separate entry for each segment  giving the beginning address of the segment in physical memory  or base  and the length of that segment  or limit   for example  segment 2 is 400 bytes long and begins at location 4300 thus  a reference to byte 53 of segment 2 is mapped 8.7 subroutine segment o segment1 symbol table  segment 4 main program segment 2 logical address space 8.7 0 2 3 4 limit base 1000 1400 400 6300 400 4300 1100 3200 1000 4700 segment table figure 8.20 example of segmentation  14001---1 segment o 2400 3200 1-----1 segment 3 4300 1 ~ --1 4700 segment 2 segment 4 5700 f--------1 6300   s ~ gt \ 1e ! it 1 6700 physical memory 345 onto location 4300 + 53 = 4353 a reference to segment 3  byte 852  is mapped to 3200  the base of segment 3  + 852 = 4052 a reference to byte 1222 of segment 0 would result in a trap to the operating system  as this segment is only tooo bytes long  both paging and segmentation have advantages and disadvantages in fact some architectures provide both in this section  we discuss the intel pentium architecture  which supports both pure segmentation and segmentation with paging we do not give a complete description of the memory-management structure of the pentium in this text rather  we present the major ideas on which it is based we conclude our discussion with an overview of linux address translation on pentium systems  in pentium systems  the cpu generates logical addresses  which are given to the segmentation unit the segmentation unit produces a linear address for each logical address the linear address is then given to the paging unit  which in turn generates the physical address in main memory thus  the segmentation and paging units form the equivalent of the memory-management unit  mmu   this scheme is shown in figure 8.21  8.7.1 pentium segmentation the pentium architecture allows a segment to be as large as 4 gb  and the maximum number of segments per process is 16 k the logical-address space 346 chapter 8 i cpu i figure 8.21 logical to physical address translation in the pentium  of a process is divided into two partitions the first partition consists of up to 8 k segments that are private to that process the second partition consists of up to 8 k segments that are shared all the processes information about the first partition is kept in the information about the second partition is kept in the each entry in the ldt and gdt consists of an 8-byte segment descriptor with detailed information about a particular segment  including the base location and limit of that segment  the logical address is a pair  selector  offset   where the selector is a 16-bit number  g p 13 2 in which s designates the segment number  g indicates whether the segment is in the gdt or ldt  and p deals with protection the offset is a 32-bit number specifying the location of the byte  or word  within the segment in question  the machine has six segment registers  allowing six segments to be addressed at any one time by a process it also has six 8-byte microprogram registers to hold the corresponding descriptors from either the ldt or gdt  this cache lets the pentium avoid having to read the descriptor from memory for every memory reference  the linear address on the pentium is 32 bits long and is formed as follows  the segment register points to the appropriate entry in the ldt or gdt the base and limit information about the segment in question is used to generate a first  the limit is used to check for address validity if the address is not valid  a memory fault is generated  resulting in a trap to the operating system if it is valid  then the value of the offset is added to the value of the base  resulting in a 32-bit linear address this is shown in figure 8.22 in the following section  we discuss how the paging unit turns this linear address into a physical address  8.7.2 pentium paging the pentium architecture allows a page size of either 4 kb or 4 mb for 4-kb pages  the pentium uses a two-level paging schence in which the division of the 32-bit linear address is as follows  page number page offset d 10 10 12 the address-translation scheme for this architecture is similar to the scheme shown in figure 8.15 the intel pentium address translation is shown in more 8.7 347 logical address offset + 32-bit linear address figure 8.22 intel pentium segmentation  detail in figure 8.23 the 10 high-order bits reference an entry in the outern'lost page table  which the pentium terms the page directory  the cr3 register points to the page directory for the current process  the page directory entry points to an inner page table that is indexed by the contents of the innermost 10 bits in the linear address finally  the low-order bits 0-11 refer to the offset in the 4-kb page pointed to in the page table  one entry in the page directory is the page size flag  which-if setindicates that the size of the page frame is 4 mb and not the standard 4 kb  if this flag is set  the page directory points directly to the 4-mb page frame  bypassing the inner page table ; and the 22 low-order bits in the linear address refer to the offset in the 4-mb page frame  31 cr3 registe r page directory page directory page directory  logical address  page table 22 21 l 1211 page table  i offset 31 22 21 offset j 4-kb page 4-mb page figure 8.23 paging in the pentium architecture  0 0 3l ! 8 chapter 8 to improve the efficiency of physical memory use  intel pentium page tables can be swapped to disk in this case  an invalid bit is used in the page directory entry to indicate whether the table to which the entry is pointing is in memory or on disk if the table is on disk  the operating system can use the other 31 bits to specify the disk location of the table ; the table then can be brought into memory on demand  8.7.3 linux on pentium systems as an illustration  consider the linux operating system running on the intel pentium architecture because linux is designed to run on a variety of processors many of which may provide only limited support for segmentationlinux does not rely on segmentation and uses it minimally on the pentium  linux uses only six segments  a segment for kernel code a segment for kernel data a segment for user code a segment for user data a task-state segment  tss  1i a default ldt segment the segments for user code and user data are shared by all processes running in user mode this is possible because all processes use the same logical address space and all segment descriptors are stored in the global descriptor table  gdt   furthermore  each process has its own task-state segment  tss   and the descriptor for this segment is stored in the gdt the tss is used to store the hardware context of each process during context switches the default ldt segment is normally shared by all processes and is usually not used however  if a process requires its own ldt  it can create one and use that instead of the default ldt  as noted  each segment selector includes a 2-bit field for protection thus  the pentium allows four levels of protection of these four levels  limlx only recognizes two  user mode and kernel mode  although the pentium uses a two-level paging model  linux is designed to run on a variety of hardware platforms  many of which are 64-bit platforms where two-level paging is not plausible therefore  linux has adopted a threelevel paging strategy that works well for both 32-bit and 64-bit architectures  the linear address in linux is broken into the following four parts  global directory middle directory page table figure 8.24 highlights the three-level paging model in linux  the number of bits in each part of the linear address varies according to architecture however  as described earlier in this section  the pentium architecture only uses a two-level paging model how  then  does linux apply 8.8 lglobal directory global directory cr3 __,.c__ ___ __l register 8.8  linear address  middle directory figure 8.24 three-level paging in linux  offset page frame 349 its three-level model on the pentium in this situation  the size of the middle directory is zero bits  effectively bypassing the middle directory  each task in linux has its own set of page tables and -just as in figure 8.23 -the cr3 register points to the global directory for the task currently executing  during a context switch  the value of the cr3 register is saved and restored in the tss segments of the tasks involved in the context switch  memory-management algorithms for multiprogrammed operating systems range from the simple single-user system approach to paged segmentation  the most important determinant of the method used in a particular system is the hardware provided every memory address generated by the cpu must be checked for legality and possibly mapped to a physical address the checking can not be implemented  efficiently  in software hence  we are constrained by the hardware available  the various memory-management algorithms  contiguous allocation  paging  segmentation  and combinations of paging and segmentation  differ in many aspects in comparing different memory-management strategies  we use the following considerations  hardware support a simple base register or a base-limit register pair is sufficient for the single and multiple-partition schemes  whereas paging and segmentation need mapping tables to define the address map  performance as the memory-management algorithm becomes more complex  the time required to map a logical address to a physical address increases for the simple systems  we need only compare or add to the logical address-operations that are fast paging and segmentation can be as fast if the mapping table is implemented in fast registers if the table is 350 chapter 8 in memory  however  user memory accesses can be degraded substantially  a tlb can reduce the performance degradation to an acceptable level  fragmentation a multiprogrammed system will generally perform more efficiently if it has a higher level of multiprogramming for a given set of processes  we can increase the multiprogramming level only by packing more processes into memory to accomplish this task  we must reduce memory waste  or fragmentation systems with fixed-sized allocation units  such as the single-partition scheme and paging  suffer from internal fragmentation systems with variable-sized allocation units  such as the multiple-partition scheme and segmentation  suffer from external fragmentation  relocation one solution to the external-fragmentation problem is compaction  compaction involves shifting a program in memory in such a way that the program does not notice the change this consideration requires that logical addresses be relocated dynamically  at execution time  if addresses are relocated only at load time  we can not compact storage  swapping swapping can be added to any algorithm at intervals determined by the operating system  usually dictated by cpu-scheduling policies  processes are copied from main memory to a backing store and later are copied back to main memory this scheme allows more processes to be run than can be fit into memory at one time  sharing another means of increasing the multiprogramming level is to share code and data among different users sharing generally requires that either paging or segmentation be used to provide small packets of information  pages or segments  that can be shared sharing is a means of running many processes with a limited amount of memory  but shared programs and data must be designed carefully  protection if paging or segmentation is provided  different sections of a user program can be declared execute-only  read -only  or read-write this restriction is necessary with shared code or data and is generally useful in any case to provide simple run-time checks for common programming errors  8.1 explain the difference between internal and external fragmentation  8.2 compare the memory organization schemes of contiguous memory allocation  pure segmentation  and pure paging with respect to the following issues  a external fragmentation b internal fragmentation c ability to share code across processes 351 8.3 why are segmentation and paging sometimes combined into one scheme 8.4 most systems allow a program to allocate more memory to its address space during execution allocation of data in the heap segments of programs is an example of such allocated memory what is required to support dynamic memory allocation in the following schemes a contiguous memory allocation b pure segmentation c pure paging 8.5 consider the intel address-translation scheme shown in figure 8.22  a describe all the steps taken by the intel pentium in translatil g a logical address into a physical address  b what are the advantages to the operating system of hardware that provides such complicated memory translation c are there any disadvantages to this address-translation system if so  what are they if not  why is this scheme not used by every manufacturer 8.6 what is the purpose of paging the page tables 8.7 explain why sharil g a reentrant module is easier when segmentation is used than when pure paging is used  8.8 on a system with paging  a process can not access memory that it does not own why how could the operating system allow access to other memory why should it or should it not 8.9 compare the segmented pagil g scheme with the hashed page table scheme for handling large address spaces under what circumstances is one scheme preferable to the other 8.10 consider a paging system with the page table stored in memory  a if a memory reference takes 200 nanoseconds  how long does a paged memory reference take b if we add tlbs  and 75 percent of all page-table references are found in the tlbs  what is the effective memory reference time  assume that finding a page-table entry in the tlbs takes zero time  if the entry is there  352 chapter 8 8.11 compare paging with segmentation with respect to the amount of memory required by the address translation structures in order to convert virtual addresses to physical addresses  8.12 consider a system in which a program can be separated into two parts  code and data the cpu knows whether it wants an instruction  instruction fetch  or data  data fetch or store   therefore  two baselimit register pairs are provided  one for instructions and one for data  the instruction base-limit register pair is automatically read-only  so programs can be shared among different users discuss the advantages and disadvantages of this scheme  8.13 consider the following process for generating binaries a compiler is used to generate the object code for individual modules  and a linkage editor is used to combine multiple object modules into a single program bilcary how does the linkage editor change the bindmg of instructions and data to memory addresses what information needs to be passed from the compiler to the linkage editor to facilitate the memory-binding tasks of the linkage editor 8.14 consider a logical address space of 64 pages of 1,024 words each  mapped onto a physical memory of 32 frames  a how many bits are there in the logical address b how many bits are there in the physical address 8.15 consider the hierarchical paging scheme used by the vax architecture  how many memory operations are performed when a user program executes a memory-load operation 8.16 given five memory partitions of 100 kb  500 kb  200 kb  300 kb  and 600 kb  ill order   how would the first-fit  best-fit  and worst-fit algorithms place processes of 212 kb  417 kb  112 kb  and 426 kb  in order  which algorithm makes the most efficient use of memory 8.17 describe a mechanism by which one segment could belong to the address space of two different processes  8.18 consider a computer system with a 32-bit logical address and 4-kb page size the system supports up to 512mb of physical memory how many entries are there in each of the following a a conventional single-level page table b an inverted page table 353 8.19 assuming a 1-kb page size  what are the page numbers and offsets for the following address references  provided as decimal numbers   a 2375 b 19366 c 30000 d 256 e 16385 8.20 program binaries in many systems are typically structured as follows  code is stored starting with a small  fixed virtual address  such as 0 the code segment is followed by the data segment that is used for storing the program variables when the program starts executing  the stack is allocated at the other end of the virtual address space and is allowed to grow toward lower virtual addresses what is the significance of this structure for the following schemes a contiguous memory allocation b pure segmentation c pure paging 8.21 consider the following segment table  segment base length 0 219 600 1 2300 14 2 90 100 3 1327 580 4 1952 96 what are the physical addresses for the following logical addresses a 0,430 b 1,10 c 2,500 d 3,400 e 4,112 8.22 consider a logical address space of 32 pages with 1,024 words per page  mapped onto a physical memory of 16 frames  a how many bits are required in the logical address b how many bits are required in the physical address 354 chapter 8 8.23 sharing segments among processes without requiring that they have the same segment number is possible in a dynamically linked segmentation system  a define a system that allows static linking and sharing of segments without requiring that the segment numbers be the same  b describe a paging scheme that allows pages to be shared without requiring that the page numbers be the same  8.24 assume that a system has a 32-bit virtual address with a 4-kb page size  write a c program that is passed a virtual address  in decincal  on the command line and have it output the page number and offset for the given address as an example  your program would run as follows  ./a.out 19986 your program would output  the address 19986 contains  page number = 4 offset = 3602 writing this program will require using the appropriate data type to store 32 bits we encourage you to use unsigned data types as well  dynamic storage allocation was discussed by knuth  1973   section 2.5   who found through simulation results that first fit is generally superior to best fit  knuth  1973  also discussed the 50-percent rule  the concept of paging can be credited to the designers of the atlas system  which has been described by kilburn et al  1961  and by howarth et al   1961   the concept of segmentation was first discussed by dennis  1965   paged segmentation was first supported in the ge 645  on which multics was originally implemented  organick  1972  and daley and dennis  1967    inverted page tables are discussed in an article about the ibm rt storage manager by chang and mergen  1988   address translation in software is covered in jacob and mudge  1997   hennessy and patterson  2002  explains the hardware aspects of tlbs  caches  and mmus talluri et al  1995  discusses page tables for 64-bit address spaces alternative approaches to enforcing memory protection are proposed and studied in wahbe et al  1993a   chase et al  1994   bershad et al  1995   and thorn  1997   dougan et al  1999  and jacob and mudge  2001  discuss 355 tedmiques for managing the tlb fang et al  2001  evaluate support for large pages  tanenbaum  2001  discusses intel80386 paging memory management for several architectures-such as the pentiunl ii  powerpc  and ultrasparcare described by jacob and mudge  1998a   segmentation on lim1x systems is presented in bovet and cesati  2002   9.1 c er in chapter 8  we discussed various memory-management strategies used in computer systems all these strategies have the same goal  to keep many processes in memory simultaneously to allow multiprogramming however  they tend to require that an entire process be in memory before it can execute  virtual memory is a tecrucique that allows the execution of processes that are not completely in memory one major advantage of this scheme is that programs can be larger than physical memory further  virtual memory abstracts main memory into an extremely large  uniform array of storage  separating logical memory as viewed by the user from physical memory  this technique frees programmers from the concerns of memory-storage limitations virtual memory also allows processes to share files easily and to implement shared memory in addition  it provides an efficient mechanism for process creation virtual memory is not easy to implement  however  and may substantially decrease performance if it is used carelessly in this chapter  we discuss virtual memory in the form of demand paging and examine its complexity and cost  to describe the benefits of a virtual memory system  to explain the concepts of demand paging  page-replacement algorithms  and allocation of page frames  to discuss the principles of the working-set model  the memory-management algorithms outlined in chapter 8 are necessary because of one basic requirement  the instructions being executed must be in physical memory the first approach to meeting this requirement is to place the entire logical address space in physical memory dynamic loading can help to ease this restriction  but it generally requires special precautions and extra work by the programmer  357 358 chapter 9 the requirement that instructions m.ust be in physical memory to be executed seems both necessary and reasonable ; but it is also unfortunate  since it limits the size of a program to the size of physical memory in fact  an examination of real programs shows us that  in many cases  the entire program is not needed for instance  consider the following  programs often have code to handle unusual error conditions since these errors seldom  if ever  occur in practice  this code is almost never executed  arrays,lists  and tables are often allocated more memory than they actually need an array may be declared 100 by 100 elements  even though it is seldom larger than 10 by 10 elements an assembler symbol table may have room for 3,000 symbols  although the average program has less than 200 symbols  certain options and features of a program may be used rarely for instance  the routines on u.s government computers that balance the budget have not been used in many years  even in those cases where the entire program is needed  it may not all be needed at the same time  the ability to execute a program that is only partially in memory would confer many benefits  a program would no longer be constrained by the amount of physical memory that is available users would be able to write programs for an extremely large virtual address space  simplifying the programming task  page 0 page 1 page 2 page v virtual memory memory map physical memory figure 9.1 diagram showing virtual memory that is larger than physical memory  9.1 359 because each user program could take less physical memory  more programs could be run at the sance time  with a corresponding increase in cpu utilization and throughput but with no increase in response time or turnaround time  less i/o would be needed to load or swap user programs into memory  so each user program would run faster  thus  running a program that is not entirely in memory would benefit both the system and the user  involves the separation of logical memory as perceived by users from physical memory this separation allows an extremely large virtual memory to be provided for programmers when only a smaller physical memory is available  figure 9.1   virtual memory makes the task of programming much easier  because the programmer no longer needs to worry about the amount of physical memory available ; she can concentrate instead on the problem to be programmed  the address space of a process refers to the logical  or virtual  view of how a process is stored in memory typically  this view is that a process begins at a certain logical address-say  address 0-and exists in contiguous memory  as shown in figure 9.2 recall from chapter 8  though  that in fact physical memory may be organized in page frames and that the physical page frames assigned to a process may not be contiguous it is up to the memorymanagement unit  mmu  to map logical pages to physical page frames in memory  note in figure 9.2 that we allow for the heap to grow upward in memory as it is used for dynamic memory allocation similarly  we allow for the stack to grow downward in memory through successive function calls the large blank space  or hole  between the heap and the stack is part of the virtual address figure 9.2 virtual address space  360 chapter 9 space but will require actual physical pages only if the heap or stack grows  virtual address spaces that include holes are known as sparse address spaces  using a sparse address space is beneficial because the holes can be filled as the stack or heap segments grow or if we wish to dynam.ically link libraries  or possibly other shared objects  during program execution  in addition to separating logical memory from physical memory  virtual memory allows files and memory to be shared by two or more processes through page sharing  section 8.4.4   this leads to the following benefits  system libraries can be shared by several processes through mapping of the shared object into a virtual address space although each process considers the shared libraries to be part of its virtual address space  the actual pages where the libraries reside in physical memory are shared by all the processes  figure 9.3   typically  a library is mapped read-only into the space of each process that is linked with it  similarly  virtual memory enables processes to share memory recall from chapter 3 that two or more processes can communicate through the use of shared memory virtual memory allows one process to create a region of memory that it can share with another process processes sharing this region consider it part of their virtual address space  yet the actual physical pages of memory are shared  much as is illustrated in figure 9.3  virtual memory can allow pages to be shared during process creation with the fork   system calt thus speeding up process creation  we further explore these-and other-benefits of virtual memory later in this chapter first though  we discuss implementing virtual memory through demand paging  shared library shared pages shared library figure 9.3 shared library using virtual memory  9.2 9.2 361 consider how an executable program might be loaded from disk into n'lemory  one option is to load the entire program in physical memory at program execution time however  a problent with this approach is that we may not initially need the entire program in memory suppose a program starts with a list of available options from which the user is to select loading the entire program into memory results in loading the executable code for all options  regardless of whether an option is ultimately selected by the user or not an alternative strategy is to load pages only as they are needed this technique is known as paging and is commonly used in virtual memory systems  with demand-paged virtual memory  pages are only loaded when they are demanded during program execution ; pages that are never accessed are thus never loaded into physical memory  a demand-paging system is similar to a paging system with swapping  figure 9.4  where processes reside in secondary memory  usually a disk   when we want to execute a process  we swap it into memory rather than swapping the entire process into memory  however  we use a a lazy swapper never swaps a page into memory unless that page will be needed  since we are now viewing a process as a sequence of pages  rather than as one large contiguous address space  use of the term swapper is technically incorrect  a swapper manipulates entire processes  whereas a is concerned with the individual pages of a process we thus use pager  rather than swapper  in connection with demand paging  program a program b main memory swap out so 90100110 120130140150 swap in 16017 figure 9.4 transfer of a paged memory to contiguous disk space  362 chapter 9 9.2.1 basic concepts when a process is to be swapped in  the pager guesses which pages will be used before the process is swapped out again instead of swapping in a whole process  the pager brings only those pages into memory thus  it avoids reading into memory pages that will not be used anyway  decreasing the swap time and the amount of physical memory needed  with this scheme  we need some form of hardware support to distinguish between the pages that are in memory and the pages that are on the disk  the valid -invalid bit scheme described in section 8.4.3 can be used for this purpose this time  however  when this bit is set to valid/ ' the associated page is both legal and in n1.emory if the bit is set to invalid/ ' the page either is not valid  that is  not in the logical address space of the process  or is valid but is currently on the disk the page-table entry for a page that is brought into memory is set as usuat but the page-table entry for a page that is not currently in memory is either simply marked invalid or contains the address of the page on disk this situation is depicted in figure 9.5  notice that marking a page invalid will have no effect if the process never attempts to access that page hence  if we guess right and page in all and only those pages that are actually needed  the process will run exactly as though we had brought in all pages while the process executes and accesses pages that are execution proceeds normally  0 2 3 4 5 6 7 valid-invalid frame bit ' \  i 0 4 v logical memory physical memory dod d  1j   @ jtb  odd figure 9.5 page table when some pages are not in main memory  operating system reference   ; \  page is on \   v backing store trap restart instruction page table reset page table physical memory 9.2 0 bring in missing page figure 9.6 steps in handling a page fault  363 but what happens if the process tries to access a page that was not brought into memory access to a page marked invalid causes a the paging hardware  in translating the address through the page table  will notice that the invalid bit is set  causing a trap to the operating system this trap is the result of the operating system 's failure to bring the desired page into memory  the procedure for handling this page fault is straightforward  figure 9.6   we check an internal table  usually kept with the process control block  for this process to determine whether the reference was a valid or an invalid memory access  if the reference was invalid  we terminate the process if it was valid  but we have not yet brought in that page  we now page it in  we find a free frame  by taking one from the free-frame list  for example   we schedule a disk operation to read the desired page into the newly allocated frame  when the disk read is complete  we modify the internal table kept with the process and the page table to indicate that the page is now in memory  we restart the instruction that was interrupted by the trap the process can now access the page as though it had always been in memory  in the extreme case  we can start executing a process with no pages in memory when the operating system sets the instruction pointer to the first 364 chapter 9 instruction of the process  which is on a non-memory-resident page  the process immediately faults for the page after this page is brought into memory  the process continues to execute  faulting as necessary until every page that it needs is in memory at that it can execute with no more faults this scheme is never bring a page into memory until it is required  theoretically  some programs could access several new pages of memory with each instruction execution  one page for the instruction and many for data   possibly causing multiple page faults per instruction this situation would result in unacceptable system performance fortunately  analysis of running processes shows that this behavior is exceedingly unlikely programs tend to have described in section 9.6.1  which results in reasonable performance from demand paging  the hardware to support demand paging is the same as the hardware for paging and swapping  page table this table has the ability to mark an entry invalid through a valid -invalid bit or a special value of protection bits  secondary memory this memory holds those pages that are not present in main memory the secondary memory is usually a high-speed disk it is known as the swap device  and the section of disk used for this purpose is known as swap-space allocation is discussed in chapter 12  a crucial requirement for demand paging is the ability to restart any instruction after a page fault because we save the state  registers  condition code  instruction counter  of the interrupted process when the page fault occurs  we must be able to restart the process in exactly the same place and state  except that the desired page is now in memory and is accessible in most cases  this requirement is easy to meet a page fault may occur at any memory reference if the page fault occurs on the instruction fetch  we can restart by fetching the instruction again if a page fault occurs while we are fetching an operand  we must fetch and decode the instruction again and then fetch the operand  as a worst-case example  consider a three-address instruction such as add the content of a to b  placing the result in c these are the steps to execute this instruction  fetch and decode the instruction  add   fetch a fetch b  add a and b  store the sum in c  if we fault when we try to store inc  because c is in a page not currently in memory   we will have to get the desired page  bring it in  correct the page table  and restart the instruction the restart will require fetching the instruction again  decoding it again  fetching the two operands again  and 9.2 365 then adding again however  there is not much repeated work  less than one complete instruction   and the repetition is necessary only when a page fault occurs  the major difficulty arises when one instruction may modify several different locations for example  consider the ibm system 360/370 mvc  move character  instruction  which can ncove up to 256 bytes from one location to another  possibly overlapping  location if either block  source or destination  straddles a page boundary  a page fault might occur after the move is partially done in addition  if the source and destination blocks overlap  the source block may have been modified  in which case we can not simply restart the instruction  this problem can be solved in two different ways in one solution  the microcode computes and attempts to access both ends of both blocks if a page fault is going to occm ~ it will happen at this step  before anything is modified  the move can then take place ; we know that no page fault can occur  since all the relevant pages are in memory the other solution uses temporary registers to hold the values of overwritten locations if there is a page fault  all the old values are written back into memory before the trap occurs this action restores memory to its state before the instruction was started  so that the instruction can be repeated  this is by no means the only architectural problem resulting from adding paging to an existing architecture to allow demand paging  but it illustrates some of the difficulties involved paging is added between the cpu and the memory in a computer system it should be entirely transparent to the user process thus  people often assume that paging can be added to any system  although this assumption is true for a non-demand-paging environment  where a page fault represents a fatal errm ~ it is not true where a page fault means only that an additional page must be brought into memory and the process restarted  9.2.2 performance of demand paging demand paging can significantly affect the performance of a computer system  to see why  let 's compute the effective access time for a demand-paged memory for most computer systems  the memory-access time  denoted ma  ranges from 10 to 200 nanoseconds as long as we have no page faults  the effective access time is equal to the memory access time if  howeve1 ~ a page fault occurs  we must first read the relevant page from disk and then access the desired word  let p be the probability of a page fault  0    ; p    ; 1   we would expect p to be close to zero-that is  we would expect to have only a few page faults the t'tp r ' ! nrr access is then effective access time =  1  p  x ma + p x page fault time  to compute the effective access time  we must know how much time is needed to service a page fault a page fault causes the following sequence to occur  trap to the operating system  save the user registers and process state  366 chapter 9 deterncine that the interrupt was a page fault  check that the page reference was legal and determine the location of the page on the disk issue a read from the disk to a free frame  a wait in a queue for this device until the read request is serviced  b wait for the device seek and/ or latency time  c begin the transfer of the page to a free frame  while waiting  allocate the cpu to some other user  cpu scheduling  optional   receive an interrupt from the disk i/0 subsystem  i/0 completed   save the registers and process state for the other user  if step 6 is executed   determine that the interrupt was from the disk correct the page table and other tables to show that the desired page is now in memory  wait for the cpu to be allocated to this process again  restore the user registers  process state  and new page table  and then resume the interrupted instruction  not all of these steps are necessary in every case for example  we are assuming that  in step 6  the cpu is allocated to another process while the i/o occurs  this arrangement allows multiprogramming to maintain cpu utilization but requires additional time to resume the page-fault service routine when the i/0 transfer is complete  in any case  we are faced with tlu ee major components of the page-fault service time  service the page-fault interrupt  read in the page  restart the process  the first and third tasks can be reduced  with careful coding  to several hundred instructions these tasks may take from 1 to 100 microseconds each  the page-switch time  however  will probably be close to 8 milliseconds   a typical hard disk has an average latency of 3 milliseconds  a seek of 5 milliseconds  and a transfer time of 0.05 milliseconds thus  the total paging time is about 8 milliseconds  including hardware and software time  remember also that we are looking at only the device-service time if a queue of processes is waiting for the device  we have to add device-queueing time as we wait for the paging device to be free to service our request  increasing even more the time to swap  with an average page-fault service time of 8 milliseconds and a memoryaccess time of 200 nanoseconds  the effective access time in nanoseconds is 3 9.3 effective access time =  1  p  x  200  + p  8 milliseconds  =  1 p  x 200 + p x 8,000,000 = 200 + 7,999,800 x p  367 we see  then  that the effective access time is directly proportional to the if one access out of 1,000 causes a page fault  the effective access time is 8.2 microseconds the computer will be slowed down by a factor of 40 because of demand paging ! if we want performance degradation to be less than 10 percent  we need 220 200 + 7,999,800 x p  20 7,999,800 x p  p 0.0000025  that is  to keep the slowdown due to paging at a reasonable level  we can allow fewer than one memory access out of 399,990 to page-fault in sum  it is important to keep the page-fault rate low in a demand-paging system  otherwise  the effective access time increases  slowing process execution dramatically  an additional aspect of demand paging is the handling and overall use of swap space disk i/0 to swap space is generally faster than that to the file system it is faster because swap space is allocated in much larger blocks  and file lookups and indirect allocation methods are not used  chapter 12   the system can therefore gain better paging throughput by copying an entire file image into the swap space at process startup and then performing demand paging from the swap space another option is to demand pages from the file system initially but to write the pages to swap space as they are replaced this approach will ensure that only needed pages are read from the file system but that all subsequent paging is done from swap space  some systems attempt to limit the amount of swap space used through demand paging of binary files demand pages for such files are brought directly from the file system however  when page replacement is called for  these frames can simply be overwritten  because they are never modified   and the pages can be read in from the file system again if needed using this approach  the file system itself serves as the backing store howeve1 ~ swap space must still be used for pages not associated with a file ; these pages include the stack and heap for a process this method appears to be a good compromise and is used in several systems  including solaris and bsd unix  in section 9 .2  we illustrated how a process can start quickly by merely demandpaging in the page containing the first instruction however  process creation using the fork   system call may initially bypass the need for demand paging by using a technique similar to page sharing  covered in section 8.4.4   this technique provides for rapid process creation and minimizes the number of new pages that must be allocated to the newly created process  368 chapter 9 physical figure 9.7 before process i modifies page c  recall thatthe fork   system call creates a child process that is a duplicate of its parent traditionally  fork   worked by creating a copy of the parent 's address space for the child  duplicating the pages belonging to the parent  however  considering that many child processes invoke the exec   system call immediately after creation  the copying of the parent 's address space may be unnecessary instead  we can use a technique known as which works by allowing the parent and child processes initially to share the same pages these shared pages are marked as copy-on-write pages  meaning that if either process writes to a shared page  a copy of the shared page is created copy-on-write is illustrated in figures 9.7 and figure 9.8  which show the contents of the physical memory before and after process 1 modifies page c  for example  assume that the child process attempts to modify a page containing portions of the stack  with the pages set to be copy-on-write the operating system will create a copy of this page  nl.apping it to the address space of the child process the child process will then modify its copied page and not the page belonging to the parent process obviously  when the copy-on-write technique is used  only the pages that are modified by either process are copied ; all unmodified pages can be shared by the parent and child processes note  too  process1 physical memory figure 9.8 after process 1 modifies page c  process2 9.4 9.4 369 that only pages that can be nwdified need be m ~ arked as copy-on-write pages that can not be modified  pages containing executable code  can be shared by the parent and child copy-on-write is a common technique used by several operating systems  including windows xp  linux  and solaris  when it is determined that a page is going to be duplicated using copyon write  it is important to note the location from which the free page will be allocated many operating systems provide a of free pages for such requests these free pages are typically allocated when the stack or heap for a process must expand or when there are copy-on-write pages to be managed  operating systems typically allocate these pages using a technique known as zem-fhl-on-den  1and zero-fill-on-demand pages have been zeroed-out before being allocated  thus erasing the previous contents  several versions of unix  including solaris and linux  provide a variation ofthe fork   system call-vfork    for fori    that operates differently from fork   with copy-on-write with vfork    the parent process is suspended  and the child process uses the address space of the parent  because vfork   does not use copy-on-write  if the child process changes any pages of the parent 's address space  the altered pages will be visible to the parent once it resumes therefore  vf ork   must be used with caution to ensure that the child process does not modify the address space of the parent vf or k   is intended to be used when the child process calls exec   immediately after creation because no copying of pages takes place  vf ork   is an extremely efficient method of process creation and is sometimes used to implement unix command-line shell interfaces  in our earlier discussion of the page-fault rate  we assumed that each page faults at most once  when it is first referenced this representation is not strictly accurate  however if a process of ten pages actually uses only half of them  then demand paging saves the i/0 necessary to load the five pages that are never used we could also increase our degree of multiprogramming by running twice as many processes thus  if we had forty frames  we could run eight processes  rather than the four that could run if each required ten frames  five of which were never used   if we increase our degree of multiprogramming  we are memory if we run six processes  each of which is ten pages in size but uses only five pages  we have higher cpu utilization and throughput  ten frames to spare it is possible  however  that each of these processes  for a particular data set  may suddenly try to use all ten of its pages  resulting in a need for sixty frames when only forty are available  further  consider that system memory is not used only for holding program pages buffers for i/ 0 also consume a considerable amount of memory this use can increase the strain on memory-placement algorithms deciding how much memory to allocate to i/0 and how much to program pages is a significant challenge some systems allocate a fixed percentage of memory for i/0 buffers  whereas others allow both user processes and the i/0 subsystem to compete for all system memory  370 chapter 9 valid-invalid pc   -_ = ' ~ ~ = =  ! came f il logical memory for user 1 page table for user 1 valid-invalid 0 frame ~ bi ~ r ~ v v ~ -------' ' 2 3 logical memory for user 2 page table for user 2 0 monitor 2 3 4 5 j 6 a 7 e physical memory figure 9.9 need for page replacement over-allocation of memory manifests itself as follows while a user process is executing  a page fault occurs the operating system determines where the desired page is residing on the disk but then finds that there are no free frames on the free-frame list ; all memory is in use  figure 9.9   the operating system has several options at this point it could terminate the user process however  demand paging is the operating system 's attempt to improve the computer system 's utilization and throughput users should not be aware that their processes are running on a paged system-paging should be logically transparent to the user so this option is not the best choice  the operating system could instead swap out a process  freeing all its frames and reducing the level of multiprogramming this option is a good one in certain circumstances  and we consider it further in section 9.6 here  we discuss the most common solution  9.4.1 basic page replacement page replacement takes the following approach if no frame is free  we find one that is not currently being used and free it we can free a frame by writing its contents to swap space and changing the page table  and all other tables  to indicate that the page is no longer in memory  figure 9.10   we can now use the freed frame to hold the page for which the process faulted we modify the page-fault service routine to include page replacement  find the location of the desired page on the disk  find a free frame  a if there is a free frame  use it  9.4 371 b if there is no free frame  use a page-replacement algorithnc to select a c write the victim frame to the disk ; change the page and frame tables accordingly  read the desired page into the newly freed frame ; change the page and frame tables  restart the user process  notice that  if no frames are free  two page transfers  one out and one in  are required this situation effectively doubles the page-fault service time and increases the effective access time accordingly  we can reduce this overhead by using a  or when this scheme is used  each page or frame has a modify bit associated with it in the hardware the modify bit for a page is set by the hardware whenever any word or byte in the page is written into  indicating that the page has been modified  when we select a page for replacement  we examine its modify bit if the bit is set  we know that the page has been modified since it was read in from the disk in this case  we must write the page to the disk if the modify bit is not set  however  the page has not been modified since it was read into memory in this case  we need not write the memory page to the disk  it is already there this technique also applies to read-only pages  for example  pages of binary code   such pages can not be modified ; thus  they may be discarded when desired  this scheme can significantly reduce the time required to service a page fault  since it reduces i/o time by one-half if the page has not been modified  frame valid-invalid bit ' \  / physical memory figure 9.10 page replacement 372 chapter 9 page replacement is basic to demand paging it completes the separation between logical memory and physical memory with this mechanism  an enormous virtual memory can be provided for programn'lers on a smaller physical memory with no demand paging  user addresses are mapped into physical addresses  so the two sets of addresses can be different all the pages of a process still must be in physical memory  however with demand paging  the size of the logical address space is no longer constrained by physical memory  if we have a user process of twenty pages  we can execute it in ten frames simply by using demand paging and using a replacement algorithm to find a free frame whenever necessary if a page that has been modified is to be replaced  its contents are copied to the disk a later reference to that page will cause a page fault at that time  the page will be brought back into memory  perhaps replacing some other page in the process  we must solve two major problems to implement demand develop a algorithm and a ' ' '  ' ' l tceme ~ lu ~ ~ f ' ' ~ ~ ''h ' that is  if we have multiple processes in memory  we must decide how many frames to allocate to each process ; and when page replacement is required  we must select the frames that are to be replaced designing appropriate algorithms to solve these problems is an important task  because disk i/0 is so expensive even slight improvements in demand-paging methods yield large gains in system performance  there are many different page-replacement algorithms every operating system probably has its own replacement scheme how do we select a particular replacement algorithm in general  we want the one with the lowest page-fault rate  we evaluate an algorithm by running it on a particular string of memory references and computing the number of page faults the string of memory references is called a reference we can generate reference strings artificially  by using a random-number generator  for example   or we can trace a given system and record the address of each memory reference the latter choice produces a large number of data  on the order of 1 million addresses per second   to reduce the number of data  we use two facts  first  for a given page size  and the page size is generally fixed by the hardware or system   we need to consider only the page number  rather than the entire address second  if we have a reference to a page p  then any references to page p that immediately follow will never cause a page fault page p will be in memory after the first reference  so the immediately following references will not fault  for example  if we trace a particular process  we might record the following address sequence  0100,0432,0101,0612,0102,0103,0104,0101,0611,0102,0103  0104,0101,0610,0102,0103,0104,0101,0609,0102,0105 at 100 bytes per page  this sequence is reduced to the following reference string  1  4  1  6  1  6  1  6  1  6  1 9.4 373 16 g  14    j  ; 2 12 q  ol co 10 0  0 8 ' q  ..0 6 e    j c 4 2 2 3 4 5 6 number of frames figure 9.1 i graph of page faults versus number of frames  to determine the number of page faults for a particular reference string and page-replacement algorithm  we also need to know the number of page frames available obviously  as the number of frames available increases  the number of page faults decreases for the reference stril'lg considered previously  for example  if we had three or more frames  we would have only three faultsone fault for the first reference to each page in contrast  with only one frame available  we would have a replacement with every reference  resulting in eleven faults in general  we expect a curve such as that in figure 9.11 as the number of frames increases  the number of page faults drops to some minimal level of course  adding physical memory increases the number of frames  we next illustrate several page-replacement algorithms in doing so  we use the reference string for a memory with three frames  9.4.2 fifo page replacement the simplest page-replacement algorithm is a first-in  first-out  fifo  algorithm  a fifo replacement algorithm associates with each page the time when that page was brought into memory when a page must be replaced  the oldest page is chosen notice that it is not strictly necessary to record the time when a page is brought in we can create a fifo queue to hold all pages in memory  we replace the page at the head of the queue when a page is brought into memory  we insert it at the tail of the queue  for our example reference string  our three frames are initially empty the first three references  7  0  1  cause page faults and are brought into these empty frames the next reference  2  replaces page 7  because page 7 was brought in first since 0 is the next reference and 0 is already in memory  we have no fault for this reference the first reference to 3 results in replacement of page 0  since 374 chapter 9 reference string 7 0 2 0 3 0 4 2 3 0 3 2 2 0 7 0 page frames figure 9.12 fifo page-replacement algorithm  it is now first in line because of this replacement  the next reference  to 0  will fault page 1 is then replaced by page 0 this process continues as shown in figure 9.12 every time a fault occurs  we show which pages are in our three frames there are fifteen faults altogether  the fifo page-replacement algorithm is easy to lmderstand and program  however  its performance is not always good on the one hand  the page replaced may be an initialization module that was used a long time ago and is no longer needed on the other hand  it could contain a heavily used variable that was initialized early and is in constant use  notice that  even if we select for replacement a page that is in active use  everything still works correctly after we replace an active page with a new one  a fault occurs almost immediately to retrieve the active page some other page must be replaced to bring the active page back into memory thus  a bad replacement choice increases the page-fault rate and slows process execution  it does not  however  cause incorrect execution  to illustrate the problems that are possible with a fifo page-replacement algorithm  we consider the following reference string  1  2  3  4  1  2  5  1  2  3  4  5 figure 9.13 shows the curve of page faults for this reference string versus the number of available frames notice that the number of faults for four frames  ten  is greater than the number of faults for three frames  nine  ! this most unexpected result is known as  for some page-replacement algorithms  the page-fault rate may increase as the number of allocated frames increases we would expect that giving more memory to a process would improve its performance in some early research  investigators noticed that this assumption was not always true belady 's anomaly was discovered as a result  9.4.3 optimal page replacement of belady 's anomaly was the search for an which has the lowest page-fault rate of all algorithms and will never suffer from belady 's anomaly such an algorithm does exist and has been called opt or min it is simply this  replace the page that will not be used for the longest period of time  9.4 375 16 ~    5 2 12 cj  moj 10 0  0 8 cj _o e 6    5 c 4 2 number of frames figure 9.13 page-fault curve for fifo replacement on a reference string  use of this page-replacement algorithm guarantees the lowest possible pagefault rate for a fixed number of frames  for example  on our sample reference string  the optimal page-replacement algorithm would yield nine page faults  as shown in figure 9.14 the first three references cause faults that fill the three empty frames the reference to page 2 replaces page 7  because page 7 will not be used until reference 18  whereas page 0 will be used at 5  and page 1 at 14 the reference to page 3 replaces page 1  as page 1 will be the last of the three pages in memory to be referenced again with only nine page faults  optimal replacement is much better than a fifo algorithm  which results in fifteen faults  if we ignore the first three  which all algorithms must suffer  then optimal replacement is twice as good as fifo replacement  irt fact  no replacement algorithm can process this reference string in three frames with fewer than nine faults  unfortunately  the optimal page-replacement algorithm is difficult to implement  because it requires future knowledge of the reference string  we encountered a similar situation with the sjf cpu-schedulin.g algorithm in section 5.3.2  as a result  the optimal algorithm is used mainly for comparison studies for instance  it may be useful to know that  although a new algorithm reference string 7 0 2 0 3 0 4 2 3 0 3 2 2 0 7 0 page frames figure 9.14 optimal page-replacement algorithm  376 chapter 9 is not optimat it is within 12.3 percent of optimal at worst and within 4.7 percent on average  9.4.4 lru page replacement lf the optimal algorithm is not feasible  perhaps an approximation of the optimal algorithm is possible the key distinction between the fifo and opt algorithms  other than looking backward versus forward in time  is that the fifo algorithm uses the time when a page was brought into memory  whereas the opt algorithm uses the time when a page is to be used if we use the recent past as an approximation of the near future  then we can replace the that has not been used for the longest period of time this approach is the lru replacement associates with each page the time of that page 's last use  when a page must be replaced  lru chooses the page that has not been used for the longest period of time we can think of this strategy as the optimal page-replacement algorithm looking backward in time  rather than forward   strangely  if we let sr be the reverse of a reference strings  then the page-fault rate for the opt algorithm on sis the same as the page-fault rate for the opt algorithm on sr similarly  the page-fault rate for the lru algorithm on sis the same as the page-fault rate for the lru algorithm on sr  the result of applying lru replacement to our example reference string is shown in figure 9.15 the lru algorithm produces twelve faults notice that the first five faults are the same as those for optimal replacement when the reference to page 4 occurs  however  lru replacement sees that  of the three frames in memory  page 2 was used least recently thus  the lru algorithm replaces page 2  not knowing that page 2 is about to be used when it then faults for page 2  the lru algorithm replaces page 3  since it is now the least recently used of the three pages in memory despite these problems  lru replacement with twelve faults is much better than fifo replacement with fifteen  the lru policy is often used as a page-replacement algorithm and is considered to be good the major problem is how to implement lru replacement an lru page-replacement algorithm may require substantial hardware assistance the problem is to determine an order for the frames defined by the time of last use two implementations are feasible  counters in the simplest case  we associate with each page-table entry a time-of-use field and add to the cpu a logical clock or counter the clock is reference string 7 0 2 0 3 0 4 2 3 0 3 2 2 0 7 0 page frames figure 9.15 lru page-replacement algorithm  9.4 377 incremented for every memory reference whenever a reference to a page is made  the contents of the clock register are copied to the ti1ne-of-use field in the page-table entry for that page in this way  we always have the time of the last reference to each page we replace the page with the smallest time value this scheme requires a search of the page table to find the lru page and a write to memory  to the time-of-use field in the page table  for each memory access the times must also be m ~ aintained when page tables are changed  due to cpu scheduling   overflow of the clock must be considered  stack another approach to implementing lru replacement is to keep a stack of page numbers whenever a page is referenced  it is removed from the stack and put on the top in this way  the most recently used page is always at the top of the stack and the least recently used page is always at the bottom  figure 9.16   because entries must be removed from the middle of the stack  it is best to implement this approach by using a doubly linked list with a head pointer and a tail pointer removing a page and putting it on the top of the stack then requires changing six pointers at worst each update is a little more expensive  but there is no search for a replacement ; the tail pointer points to the bottom of the stack  which is the lru page this approach is particularly appropriate for software or microcode implementations of lru replacement  like optimal replacement  lru replacement does not suffer from belady 's both belong to a class of page-replacement algorithms  called si  ack that can never exhibit belady 's anomaly a stack algorithm is an algorithm for which it can be shown that the set of pages in memory for n frames is always a subset of the set of pages that would be in memory with n + 1 frames for lru replacement  the set of pages in memory would be the n most recently referenced pages if the number of frames is increased  these n pages will still be the most recently referenced and so will still be in memory  note that neither implementation of lru would be conceivable without hardware assistance beyond the standard tlb registers the updating of the reference string 4 7 0 7 stack before a 0 2 stack after b 2 7 2 i l a b figure 9.16 use of a stack to record the most recent page references  378 chapter 9 clock fields or stack must be done for every memory reference if we were to use an interrupt for every reference to allow software to update such data structures  it would slow every memory reference by a factor of at least ten  hence slowing every user process by a factor of ten few systems could tolerate that level of overhead for memory management  9.4.5 lru-approximation page replacement few computer systems provide sufficient hardware support for true lru page replacement some systems provide no hardware support  and other pagereplacement algorithms  such as a fifo algorithm  must be used many systems provide some help  however  in the form of a the reference bit for a page is set by the hardware whenever that page is referenced  either a read or a write to any byte in the page   reference bits are associated with each entry in the page table  initially  all bits are cleared  to 0  by the operating system as a user process executes  the bit associated with each page referenced is set  to 1  by the hardware after some time  we can determine which pages have been used and which have not been used by examining the reference bits  although we do not know the order of use this information is the basis for many page-replacement algorithms that approximate lru replacement  9.4.5.1 additional-reference-bits algorithm we can gain additional ordering information by recording the reference bits at regular intervals we can keep an 8-bit byte for each page in a table in memory  at regular intervals  say  every 100 milliseconds   a timer interrupt transfers control to the operating system the operating system shifts the reference bit for each page into the high-order bit of its 8-bit byte  shifting the other bits right by 1 bit and discarding the low-order bit these 8-bit shift registers contain the history of page use for the last eight time periods if the shift register contains 00000000  for example  then the page has not been used for eight time periods ; a page that is used at least once in each period has a shift register value of 11111111 a page with a history register value of 11000100 has been used more recently than one with a value of 01110111 if we interpret these 8-bit bytes as unsigned integers  the page with the lowest number is the lru page  and it can be replaced notice that the numbers are not guaranteed to be unique  however we can either replace  swap out  all pages with the smallest value or use the fifo method to choose among them  the number of bits of history included in the shift register can be varied  of course  and is selected  depending on the hardware available  to make the updating as fast as possible in the extreme case  the number can be reduced to zero  leaving only the reference bit itself this algorithm is called the 9.4.5.2 second-chance algorithm the basic algorithm of second-chance replacement is a fifo replacement algorithm when a page has been selected  however  we inspect its reference bit if the value is 0  we proceed to replace this page ; but if the reference bit is set to 1  we give the page a second chance and move on to select the next next victim 9.4 reference pages reference pages bits bits circular queue of pages circular queue of pages  a   b  figure 9.17 second-chance  clock  page-replacement algorithm  379 fifo page when a page gets a second chance  its reference bit is cleared  and its arrival time is reset to the current time thus  a page that is given a second chance will not be replaced until all other pages have been replaced  or given second chances   in addition  if a page is used often enough to keep its reference bit set  it will never be replaced  one way to implement the second-chance algorithm  sometimes referred to as the clock algorithm  is as a circular queue a poi11ter  that is  a hand on the clock  indicates which page is to be replaced next when a frame is needed  the pointer advances until it finds a page with a 0 reference bit as it advances  it clears the reference bits  figure 9.17   once a victim page is found  the page is replaced  and the new page is inserted in the circular queue in that position  notice that  in the worst case  when all bits are set  the pointer cycles through the whole queue  giving each page a second chance it clears all the reference bits before selecting the next page for replacement second-chance replacement degenerates to fifo replacement if all bits are set  9.4.5.3 enhanced second-chance algorithm we can enhance the second-chance algorithm by considering the reference bit and the modify bit  described in section 9.4.1  as an ordered pair with these two bits  we have the following four possible classes   0  0  neither recently used nor modified -best page to replace 380 chapter 9  0  1  not recently used hut modified-not quite as good  because the page will need to be written out before replacement  1  0  recently used but clean-probably will be used again soon  1  1  recently used and modified -probably will be used again soon  and the page will be need to be written out to disk before it can be replaced each page is in one of these four classes when page replacement is called for  we use the same scheme as in the clock algorithm ; but instead of examining whether the page to which we are pointing has the reference bit set to 1  we examine the class to which that page belongs we replace the first page encountered in the lowest nonempty class notice that we may have to scan the circular queue several times before we find a page to be replaced  the major difference between this algorithm and the simpler clock algorithm is that here we give preference to those pages that have been modified to reduce the number of i/os required  9.4.6 counting-based page replacement there are many other algorithms that can be used for page replacement for example  we can keep a counter of the number of references that have been made to each page and develop the following two schemes  the least frequently used  lfu  page-replacement algorithm requires that the page with the smallest count be replaced the reason for this selection is that an actively used page should have a large reference count  a problem arises  however  when a page is used heavily during the initial phase of a process but then is never used again since it was used heavily  it has a large count and remains in memory even though it is no longer needed one solution is to shift the counts right by 1 bit at regular intervals  forming an exponentially decaying average usage count  the most frequently used  mfu  page-replacement algorithm is based on the argument that the page with the smallest count was probably just brought in and has yet to be used  as you might expect  neither mfu nor lfu replacement is common the implementation of these algorithms is expensive  and they do not approxin'late opt replacement well  9.4.7 page-buffering algorithms other procedures are often used in addition to a specific page-replacement algorithm for example  systems commonly keep a pool of free frames when a page fault occurs  a victim frame is chosen as before however  the desired page is read into a free frame from the pool before the victim is written out this procedure allows the process to restart as soon as possible  without waiting for the victim page to be written out when the victim is later written out  its frame is added to the free-frame pool  9.4 381 an expansion of this idea is to maintain a list of modified pages whenever the paging device is idle  a modified page is selected and is written to the disk  its modify bit is then reset this scheme increases the probability that a page will be clean when it is selected for replacement and will not need to be written out  another modification is to keep a pool of free frames but to remember which page was in each frame since the frame contents are not modified when a frame is written to the disk  the old page can be reused directly fronc the free-frame pool if it is needed before that frame is reused no i/o is needed in this case when a page fault occurs  we first check whether the desired page is in the free-frame pool if it is not  we must select a free frame and read into it  this technique is used in the vax/vms system along with a fifo replacement algorithm when the fifo replacement algorithm mistakenly replaces a page that is still in active use  that page is quickly retrieved from the free-frame pool  and no i/o is necessary the free-frame buffer provides protection against the relatively poor  but sirnple  fifo replacement algorithm this method is necessary because the early versions of vax did not implement the reference bit correctly  some versions of the unix system use this method in conjunction with the second-chance algorithm it can be a useful augmentation to any pagereplacement algorithm  to reduce the penalty incurred if the wrong victim page is selected  9.4.8 applications and page replacement in certain cases  applications accessing data through the operating system 's virtual memory perform worse than if the operating system provided no buffering at all a typical example is a database  which provides its own memory management and i/0 buffering applications like this understand their memory use and disk use better than does an operating system that is implementing algorithms for general-purpose use if the operating system is buffering i/0  and the application is doing so as well  then twice the memory is being used for a set of i/0  in another example  data warehouses frequently perform massive sequential disk reads  followed by computations and writes the lru algorithm would be removing old pages and preserving new ones  while the application would more likely be reading older pages than newer ones  as it starts its sequential reads again   here  mfu would actually be more efficient than lru  because of such problems  some operating systems give special programs the ability to use a disk partition as a large sequential array of logical blocks  without any file-system data structures this array is sometimes called the raw disk  and i/o to this array is termed raw i/0 raw i/0 bypasses all the filesystem services  such as file i/0 demand paging  file locking  prefetching  space allocation  file names  and directories note that although certain applications are more efficient when implementing their own special-purpose storage services on a raw partition  most applications perform better when they use the regular file-system services  382 chapter 9 9.5 we turn next to the issue of allocation how do we allocate the fixed amount of free memory among the various processes if we have 93 free frames and two processes  how many frames does each process get the simplest case is the single-user system consider a single-user system with 128 kb of memory composed of pages 1 kb in size this system has 128 frames the operating system may take 35 kb  leaving 93 frames for the user process under pure demand paging  all 93 frames would initially be put on the free-frame list when a user process started execution  it would generate a sequence of page faults the first 93 page faults would all get free frames from the free-frame list when the free-frame list was exhausted  a page-replacement algorithm would be used to select one of the 93 in-memory pages to be replaced with the 94th  and so on when the process terminated  the 93 frames would once again be placed on the free-frame list  there are many variations on this simple strategy we can require that the operating system allocate all its buffer and table space from the free-frame list  when this space is not in use by the operating system  it can be used to support user paging we can try to keep three free frames reserved on the free-frame list at all times thus  when a page fault occurs  there is a free frame available to page into while the page swap is taking place  a replacement can be selected  which is then written to the disk as the user process continues to execute  other variants are also possible  but the basic strategy is clear  the user process is allocated any free frame  9.5.1 minimum number of frames our strategies for the allocation of frames are constrained in various ways we can not  for example  allocate more than the total number of available frames  unless there is page sharing   we must also allocate at least a minimum number of frames here  we look more closely at the latter requirement  one reason for allocating at least a minimum number of frames involves performance obviously  as the number of frames allocated to each process decreases  the page-fault rate increases  slowing process execution in addition  remember that when a page fault occurs before an executing ilcstruction is complete  the instruction must be restarted consequently we must have enough frames to hold all the different pages that any single ilcstruction can reference  for example  consider a machine in which all memory-reference instructions may reference only one memory address in this case  we need at least one frame for the instruction and one frame for the mernory reference in addition  if one-level indirect addressing is allowed  for example  a load instruction on page 16 can refer to an address on page 0  which is an indirect reference to page 23   then paging requires at least three frames per process think about what might happen if a process had only two frames  the minimum number of frames is defined by the computer architecture  for example  the move instruction for the pdp-11 includes more than one word for some addressing modes  and thus the ilcstruction itself may straddle two pages in addition  each of its two operands may be indirect references  for a total of six frames another example is the ibm 370 mvc instruction since the 9.5 383 instruction is from storage location to storage location  it takes 6 bytes and can straddle two pages the block of characters to move and the area to which it is to be m.oved can each also straddle two pages this situation would require six frames the worst case occurs when the mvc instruction is the operand of an execute instruction that straddles a page boundary ; in this case  we need eight frames  the worst-case scenario occurs in computer architectures that allow multiple levels of indirection  for example  each 16-bit word could contain a 15-bit address plus a 1-bit indirect indicator   theoretically  a simple load instruction could reference an indirect address that could reference an indirect address  on another page  that could also reference an indirect address  on yet another page   and so on  until every page in virtual memory had been touched  thus  in the worst case  the entire virtual memory must be in physical memory  to overcome this difficulty  we must place a limit on the levels of indirection  for example  limit an instruction to at most 16levels of indirection   when the first indirection occurs  a counter is set to 16 ; the counter is then decremented for each successive irtdirection for this instruction if the counter is decremented to 0  a trap occurs  excessive indirection   this limitation reduces the maximum number of memory references per instruction to 17  requiring the same number of frames  whereas the minimum number of frames per process is defined by the architecture  the maximum number is defined by the amount of available physical memory in between  we are still left with significant choice in frame allocation  9.5.2 allocation algorithms the easiest way to split m frames among n processes is to give everyone an equal share  m/n frames for instance  if there are 93 frames and five processes  each process will get 18 frames the three leftover frames can be used as a free-frame buffer pool this scheme is called an alternative is to recognize that various processes will need differing amounts of memory consider a system with a 1-kb frame size if a small student process of 10 kb and an interactive database of 127 kb are the only two processes running in a system with 62 free frames  it does not make much sense to give each process 31 frames the student process does not need more than 10 frames  so the other 21 are  strictly speaking  wasted  to solve this problem  we can use in which we allocate available memory to each process according to its size let the size of the virtual memory for process p ; be s ;  and define s = ls ;  then  if the total number of available frames is m  we allocate a ; frames to process p ;  where a ; is approximately a ; = s ; /s x m  384 chapter 9 of course  we must adjust each ai to be an integer that is greater than the ncinimum number of frames required by tl1e instruction set  with a sum not exceeding m  with proportional allocation  we would split 62 frames between two processes  one of 10 pages and one of 127 pages  by allocating 4 frames and 57 frames  respectively  since 10/137 x 62 ~ 4  and 127/137 x 62 ~ 57  in this way  both processes share the available frames according to their needs  rather than equally  in both equal and proportional allocation  of course  the allocation may vary according to the multiprogramming level if the multiprogramming level is increased  each process will lose some frames to provide the memory needed for the new process conversely  if the multiprogramming level decreases  the frames that were allocated to the departed process can be spread over the remaining processes  notice that  with either equal or proportional allocation  a high-priority process is treated the same as a low-priority process by its definition  however  we may want to give the high-priority process more memory to speed its execution  to the detriment of low-priority processes one solution is to use a proportional allocation scheme wherein the ratio of frames depends not on the relative sizes of processes but rather on the priorities of processes or on a combination of size and priority  9.5.3 global versus local allocation another important factor in the way frames are allocated to the various processes is page replacement with multiple processes competing for frames  we can classify page-replacement algorithms into two broad categories  ; .no ' '-c'u ~ ' ' and local global replacement allows a process to a replacement frame from the set of all frames  even if that frame is currently allocated to some other process ; that is  one process can take a frame from another local replacement requires that each process select from only its own set of allocated frames  for example  consider an allocation scheme wherein we allow high-priority processes to select frames from low-priority processes for replacement a process can select a replacement from among its own frames or the frames of any lower-priority process this approach allows a high-priority process to increase its frame allocation at the expense of a low-priority process with a local replacement strategy  the number of frames allocated to a process does not change with global replacement  a process may happen to select only frames allocated to other processes  thus increasing the number of frames allocated to it  assuming that other processes do not choose its frames for replacement   one problem with a global replacement algorithm is that a process can not control its own page-fault rate the set of pages in memory for a process depends not only on the paging behavior of that process but also on the paging behavior of other processes therefore  the same process may perform quite 9.5 385 differently  for example  taking 0.5 seconds for one execution and 10.3 seconds for the next execution  because of totally external circuntstances such is not the case with a local replacement algorithm under local replacement  the set of pages in memory for a process is affected by the paging behavior of only that process local replacement might hinder a process  however  by not making available to it other  less used pages of memory thus  global replacement generally results in greater system throughput and is therefore the more common method  9.5.4 non-uniform memory access thus far in our coverage of virtual memory  we have assumed that all main memory is created equal-or at least that it is accessed equally on many computer systems  that is not the case often  in systems with multiple cpus  section 1.3.2   a given cpu can access some sections of main memory faster than it can access others these performance differences are caused by how cpus and memory are interconnected in the system frequently  such a system is made up of several system boards  each containing multiple cpus and some memory the system boards are interconnected in various ways  ranging from system busses to high-speed network connections like infiniband as you might expect  the cpus on a particular board can access the memory on that board with less delay than they can access memory on other boards in the system systems in which memory access times vary significantly are known collectively as systems  and without exception  they are slower than systems in which memory and cpus are located on the same motherboard  managing which page frames are stored at which locations can significantly affect performance in numa systems if we treat memory as uniform in such a system  cpus may wait significantly longer for memory access than if we modify memory allocation algorithms to take numa into account similar changes must be rnade to the scheduling system the goal of these changes is to have memory frames allocated as close as possible to the cpu on which the process is running the definition of close is with minimum latency  which typically means on the same system board as the cpu  the algorithmic changes consist of having the scheduler track the last cpu on which each process ran if the scheduler tries to schedule each process onto its previous cpu  and the memory-management system tries to allocate frames for the process close to the cpu on which it is being scheduled  then improved cache hits and decreased memory access times will result  the picture is more complicated once threads are added for example  a process with many running threads may end up with those threads scheduled on many different system boards how is the memory to be allocated in this case solaris solves the problem by creating an entity in the kernel each lgroup gathers together close cpus and memory in fact  there is a hierarchy of lgroups based on the amount of latency between the groups solaris tries to schedule all threads of a process and allocate all memory of a process within an lgroup if that is not possible  it picks nearby lgroups for the rest of the resources needed in this manner  overall memory latency is minimized  and cpu cache hit rates are maximized  386 chapter 9 9.6 if the number of frames allocated to a low-priority process falls below the minimum number required by the computer architecture  we must suspend that process 's execution we should then page out its remaining pages  freeing all its allocated frames this provision introduces a swap-in  swap-out level of intermediate cpu scheduling  in fact  look at any process that does not have enough frames if the process does not have the num.ber of frames it needs to support pages in active use  it will quickly page-fault at this point  it must replace some page  however  since all its pages are in active use  it must replace a page that will be needed again right away consequently  it quickly faults again  and again  and again  replacing pages that it must back in immediately  this high paging activity is called a process is thrashing if it is spending more time paging than executing  9.6.1 cause of thrashing thrashing results in severe performance problems consider the following scenario  which is based on the actual behavior of early paging systems  the operating system monitors cpu utilization if cpu utilization is too low  we increase the degree of multiprogramming by introducing a new process to the system a global page-replacement algorithm is used ; it replaces pages without regard to the process to which they belong now suppose that a process enters a new phase in its execution and needs more frames it starts faulting and taking frames away from other processes these processes need those pages  however  and so they also fault  taking frames from other processes these faulting processes must use the pagin.g device to swap pages in and out as they queue up for the paging device  the ready queue empties as processes wait for the paging device  cpu utilization decreases  the cpu scheduler sees the decreasing cpu utilization and increases the degree of multiprogramming as a result the new process tries to get started by taking frames from running processes  causing more page faults and a longer queue for the paging device as a result  cpu utilization drops even further  and the cpu scheduler tries to increase the degree of multiprogramming even more thrashing has occurred  and system throughput plunges the pagefault rate increases tremendously as a result  the effective m.emory-access time increases no work is getting done  because the processes are spending all their time paging  this phenomenon is illustrated in figure 9.18  in which cpu utilization is plotted against the degree of multiprogramming as the degree of multiprogramming increases  cpu utilization also ilccreases  although more slowly  until a maximum is reached if the degree of multiprogramming is increased even further  thrashing sets in  and cpu utilization drops sharply at this point  to increase cpu utilization and stop thrashing  we must decrease the degree of multiprogramming  we can limit the effects of thrashing by using a  or with local replacement  if one process starts thrashing  it can not frames from another process and cause the latter to thrash as well however  the problem is not entirely solved if processes are 9.6 387 degree of multiprogramming figure 9.18 thrashing  thrashing  they will be in the queue for the paging device most of the time the average service time for a page fault will increase because of the longer average queue for the paging device thus  the effective access time will increase even for a process that is not thrashing  to prevent thtashing  we must provide a process with as many frames as it needs but how do we know how many frames it needs there are several teclmiques the working-set strategy  section 9.6.2  starts by looking at how frames a process is actually using this approach defines the locality of process execution  the locality model states that  as a process executes  it moves from locality to locality a locality is a set of pages that are actively used together  figure 9.19   a program is generally composed of several different localities  which may overlap  for example  when a function is called  it defines a new locality in this locality  memory references are made to the instructions of the function call  its local variables  and a subset of the global variables when we exit the function  the process leaves this locality  since the local variables and instructions of the function are no longer in active use we may return to this locality later  thus  we see that localities are defined by the program structure and its data structures the locality model states that all programs will exhibit this basic memory reference structure note that the locality model is the unstated principle behind the caching discussions so far in this book if accesses to any types of data were random rather than patterned  caching would be useless  suppose we allocate enough frames to a process to accommodate its current locality it will fault for the pages in its locality until all these pages are in memory ; then  it will not fault again until it changes localities if we do not allocate enough frames to accommodate the size of the current locality  the process will thrash  since it can not keep in memory all the pages that it is actively using  9.6.2 working-set model as mentioned  the is based on the assumption of locality  this model uses a paramete1 ~ / '    to define the vrindovv the idea 388 chapter 9 32 ~ ~  ~ ~ = = ~ ~ ~ ~ ~ wl ~ ~ #  ~ ~  ~ ~ ~  \ jjl  jlli111 28  j   j   !  0 0  lj 26 i ' c 0 i e i  lj e execution time   figure 9.19 locality in a memory-reference pattern  is to examine the most recent 6 references the set of pages in the most recent 6 page references is the  figure 9.20   if a page is in active use  it will be in the working set if it is no longer being used  it will drop from the working set 6 time units after its last reference thus  the working set is an approximation of the program 's locality  for example  given the sequence of memory references shown in figure 9.20  if 6 = 10 memory references  then the working set at time t1 is  1  2  5  6  7   by time t2  the working set has changed to  3  4   the accuracy of the working set depends on the selection of 6 if 6 is too small  it will not encompass the entire locality ; if 6 is too large  it may overlap 9.6 page reference table    2 6 1 5 7 7 7 7 5 1 6 2 3 4 1 2 3 4 4 4 3 4 3 4 4 4 1 3 2 3 4 4 4 3 4 4 4  ~ ~ r ~ r t1 ws  t1  =  1 ,2,5,6,7  figure 9.20 working-set model  389 several localities in the extrem.e  if l is infinite  the working set is the set of pages touched during the process execution  the most important property of the working set  then  is its size if we compute the working-set size  wss ;  for each process in the system  we can then consider that where dis the total demand for frames each process is actively using the pages in its working set thus  process i needs wss ; frames if the total demand is greater than the total number of available frames  d m   thrashing will occur  because some processes will not have enough frames  once l has been selected  use of the working-set model is simple the operating system monitors the working set of each process and allocates to that working set enough frames to provide it with its working-set size if there are enough extra frames  another process can be initiated if the sum of the working-set sizes increases  exceeding the total number of available frames  the operating system selects a process to suspend the process 's pages are written out  swapped   and its frames are reallocated to other processes the suspended process can be restarted later  this working-set strategy prevents thrashing while keeping the degree of multiprogramming as high as possible thus  it optimizes cpu utilization  the difficulty with the working-set model is keeping track of the working set the working-set window is a moving window at each memory reference  a new reference appears at one end and the oldest reference drops off the other end a page is in the working set if it is referenced anywhere in the working-set window  we can approximate the working-set model with a fixed-interval timer interrupt and a reference bit for example  assum.e that l equals 10,000 references and that we can cause a timer interrupt every 5,000 references  when we get a timer interrupt  we copy and clear the reference-bit values for each page thus  if a page fault occurs  we can examine the current reference bit and two in-memory bits to determine whether a page was used within the last 10,000 to 15,000 references if it was used  at least one of these bits will be on if it has not been used  these bits will be off those pages with at least one bit on will be considered to be in the working set note that this arrangement is not entirely accurate  because we can not tell where  within an interval of 5,000  a reference occurred we can reduce the uncertainty by increasing the number of history bits and the frequency of interrupts  for example  10 bits and interrupts every 1,000 references   however  the cost to service these more frequent interrupts will be correspondingly higher  390 chapter 9 9.7 number of frames figure 9.21 page-fault frequency  9.6.3 page-fault frequency the working-set model is successful  and knowledge of the working set can be useful for prepaging  section 9.9.1   but it seems a clumsy way to control thrashilcg a strategy that uses the takes a more direct approach  the specific problem is how to prevent thrashilcg thrashing has a high page-fault rate thus  we want to control the page-fault rate when it is too high  we know that the process needs more frames conversely  if the page-fault rate is too low  then the process may have too many frames we can establish upper and lower bounds on the desired page-fault rate  figure 9.21   if the actual page-fault rate exceeds the upper limit  we allocate the process another frame ; if the page-fault rate falls below the lower limit  we remove a frame from the process thus  we can directly measure and control the page-fault rate to prevent thrashing  as with the working-set strategy  we may have to suspend a process if the page-fault rate ilccreases and no free frames are available  we must select some process and suspend it the freed frames are then distributed to processes with high page-fault rates  consider a sequential read of a file on disk using the standard system calls open   ,read    and write    each file access requires a system call and disk access alternatively  we can use the virtual memory techniques discussed so far to treat file i/0 as routine memory accesses this approach  known as a file  allows a part of the virtual address space to be logically associated with the file as we shall see  this can lead to significant performance increases when performing i/0  9.7 391 working sets and page faultrates there is a directrelationship between the working set of a process and its page-fault rate typically as shown in figure 9.20  the working set ofa process changes pver time as references to data and code sections move from one locality to another assuming there is sufficient memory to store the working set of .a process  that is  the processis 11.ot thrashing   tbe page-fault rate of the process will transition between peaks and valleys over time this general behavior is shown in figure 9.22  page fault rate working set time figure 9.22 page fault rate over time  a peak in the page-fault rate occurs when we begin demand-paging a new locality however  once the working set of this new locality is in memory  the page-fault rate falls when the process moves to a new working set  the page  fault rate rises toward a peak once again  returning to a lower rate once the new working set is loaded into memory the span oftime between the start of one peak and the start of thenext peak represents the transition from one working set to another  9.7.1 basic mechanism memory mapping a file is accomplished by mapping a disk block to a page  or pages  in memory initial access to the file proceeds through ordinary demand paging  resulting in a page fault however  a page-sized portion of the file is read from the file system into a physical page  some systems may opt to read in more than a page-sized chunk of memory at a time   subsequent reads and writes to the file are handled as routine memory accesses  thereby simplifying file access and usage by allowing the system to manipulate files through memory rather than incurring the overhead of using the read   and write   system calls similarly  as file l/0 is done in memory as opposed to using system calls that involve disk i/0  file access is much faster as well  note that writes to the file mapped in memory are not necessarily imm.ediate  synchronous  writes to the file on disk some systems may choose to update the physical file when the operating system periodically checks 392 chapter 9 whether the page in memory has been modified when the file is closed  all the memory-mapped data are written back to disk and ren loved from the virtual memory of the process  some operating systems provide memory mapping only through a specific system call and use the standard system calls to perform all other file i/0  however  some systems choose to memory-map a file regardless of whether the file was specified as memory-mapped let 's take solaris as an example if a file is specified as memory-mapped  using the mmap   system call   solaris maps the file into the address space of the process if a file is opened and accessed using ordinary system calls  such as open    read    and write    solaris still memory-maps the file ; however  the file is mapped to the kernel address space regardless of how the file is opened  then  solaris treats all file i/0 as memory-mapped  allowing file access to take place via the efficient memory subsystem  multiple processes may be allowed to map the same file concurrently  to allow sharing of data writes by any of the processes modify the data in virtual memory and can be seen by all others that map the same section of the file given our earlier discussions of virtual memory  it should be clear how the sharing of memory-mapped sections of memory is implemented  the virtual memory map of each sharing process points to the same page of physical memory-the page that holds a copy of the disk block this memory sharing is illustrated in figure 9.23 the memory-mapping system calls can also support copy-on-write functionality  allowing processes to share a file in read-only mode but to have their own copies of any data they modify so that r i i i 1  r   ; i i 1 1 i -1 ii i i i i i j---r ' -rl..-r i i i i -r ' i i i i 1 -1 i i 1 _ i i i i i f +  =   .....c.c ~ ..-'---r ~   i i  .l i j i i i 1 i i i i i i i i i i l_ ~ i process a 1 1 1 virtual memory  ~ 1  disk file figure 9.23 memory-mapped files  process b virtual memory 9.7 memory-mapped file figure 9.24 shared memory in windows using memory-mapped 1/0  393 access to the shared data is coordinated  the processes involved might use one of the mechanisms for achieving mutual exclusion described in chapter 6  in many ways  the sharing of memory-mapped files is similar to shared memory as described in section 3.4.1 not all systems use the same mechanism for both ; on unix and linux systems  for example  memory mapping is accomplished with the mmap   system call  whereas shared memory is achieved with the posix-compliant shmget   and shmat   systems calls  section 3.5.1   on windows nt  2000  and xp systems  howeve1 ~ shared memory is accomplished by memory mapping files on these systems  processes can communicate using shared memory by having the communicating processes memory-map the same file into their virtual address spaces the memorymapped file serves as the region of shared memory between the communicating processes  figure 9.24   in the following section  we illustrate support in the win32 api for shared memory using memory-mapped files  9.7.2 shared memory in the win32 api the general outline for creating a region of shared memory using memorymapped files in the win32 api involves first creating a file mapping for the file to be mapped and then establishing a view of the mapped file in a process 's virtual address space a second process can then open and create a view of the mapped file in its virtual address space the mapped file represents the shared-menwry object that will enable communication to take place between the processes  we next illustrate these steps in more detail in this example  a producer process first creates a shared-memory object using the memory-mapping features available in the win32 api the producer then writes a message to shared m.emory after that  a consumer process opens a mapping to the shared-memory object and reads the message written by the consum.er  to establish a memory-mapped file  a process first opens the file to be mapped with the createfile   function  which returns a handle to the opened file the process then creates a mapping of this file handle using the createfilemapping   function once the file mapping is established  the process then establishes a view of the mapped file in its virtual address space with the mapviewdffile   function the view of the mapped file represents the portion of the file being mapped in the virtual address space of the process 394 chapter 9 # include windows.h # include stdio.h int main  int argc  char argv      handle hfile  hmapfile ; lpvoid lpmapaddress ; hfile = createfile  temp.txt  //file name genericjread i generic_write  // read/write access 0  ii no sharing of the file null  //default security open_always  //open new or existing file file_attribute_normal  //routine file attributes null  ; //no file template hmapfile = createfilemapping  hfile  //file handle null  //default security pagejreadwrite  //read/write access to mapped pages 0  ii map entire file 0  text  sharedobject   ; //named shared memory object lpmapaddress = mapviewdffile  hmapfile  //mapped object handle filejmap_all_access  //read/write access 0  ii mapped view of entire file 0  0  ; ii write to shared memory sprintf  lpmapaddress  shared memory message  ; unmapviewoffile  lpmapaddress  ; closehandle  hfile  ; closehandle  hmapfile  ; figure 9.25 producer writing to shared memory using the win32 api  -the entire file or only a portion of it may be mapped we illustrate this sequence in the program shown in figure 9 .25  we eliminate much of the error checking for code brevity  the call to createfilemapping   creates a named shared-memory object called sharedobj ect the consumer process will communicate using this shared-memory segment by creating a mapping to the same named object  the producer then creates a view of the memory-mapped file in its virtual address space by passing the last three parameters the value 0  it indicates that the mapped view is the entire file it could instead have passed values specifying an offset and size  thus creating a view containing only a subsection of the file  it is important to note that the entire mapping may not be loaded # include windows.h # include stdio.h int main  int argc  char argv     handle hmapfile ; lpvoid lpmapaddress ; 9.7 395 hmapfile = openfilemapping  file_map_all_access  // r/w access false  //no inheritance  text  sharedobject   ; //name of mapped file object lpmapaddress = mapviewoffile  hmapfile  //mapped object handle filejmap_all_access  //read/write access 0  ii mapped view of entire file 0  0  ; ii read from shared memory printf  read message % s  lpmapaddress  ; unmapviewoffile  lpmapaddress  ; closehandle  hmapfile  ; figure 9.26 consumer reading from shared memory using the win32 api  into memory when the mapping is established rather  the mapped file may be demand-paged  thus bringing pages into memory only as they are accessed  the mapviewoffile   fm1ction returns a pointer to the shared-memory object ; any accesses to this memory location are thus accesses to the memory-mapped file in this ii1stance  the producer process writes the message shared memory message to shared memory  a program illustrating how the consumer process establishes a view of the named shared-memory object is shown in figure 9.26 this program is somewhat simpler than the one shown in figure 9.25  as all that is necessary is for the process to create a mapping to the existii1g named shared-memory object the consumer process must also create a view of the mapped file  just as the producer process did ii1 the program in figure 9.25 the consumer then reads from shared memory the message shared memory message thatwas written by the producer process  finally  both processes remove the view of the mapped file with a call to unmapviewoffile    we provide a programming exercise at the end of this chapter using shared memory with memory mapping in the win32 api  9.7.3 memory-mapped i/0 in the case of i/0  as mentioned in section 1.2.1  each i/0 controller includes registers to hold commands and the data being transferred usually  special i/0 instructions allow data transfers between these registers and system memory  396 chapter 9 9.8 to allow more convenient access to i/0 devices1 many computer architectures provide in this case/ ranges of memory addresses are set aside and are mapped to the device registers reads and writes to these memory addresses cause the data to be transferred to and from the device registers this method is appropriate for devices that have fast response times/ such as video controllers in the ibm pc each location on the screen is mapped to a n1.emory location displaying text on the screen is almost as easy as writing the text into the appropriate memory-mapped locations  memory-mapped i/o is also convenient for other devices/ such as the serial and parallel ports used to connect modems and printers to a computer the cpu transfers data through these kinds of devices by reading and writing a few device registers/ called an i/0 to send out a long string of bytes through a memory-mapped serial port1 the cpu writes one data byte to the data register and sets a bit in the control register to signal that the byte is available the device takes the data byte and then clears the bit in the control register to signal that it is ready for the next byte then the cpu can transfer the next byte if the cpu uses polling to watch the control bit/ constantly looping to see whether the device is ready/ this method of operation is called if the cpu does not poll the control bit/ but instead receives an interrupt when the device is ready for the next byte/ the data transfer is said to be when a process running in user rnode requests additional memory/ pages are allocated from the list of free page frames maintained by the kernel  this list is typically populated using a page-replacement algorithm such as those discussed in section 9.4 and most likely contains free pages scattered throughout physical memory/ as explained earlier remember/ too/ that if a user process requests a single byte of memory/ internal fragmentation will result/ as the process will be granted an entire page frame  kernel memory/ however1 is often allocated from a free-memory pool different from the list used to satisfy ordinary user-mode processes there are two primary reasons for this  the kernel requests memory for data structures of varying sizes  some of which are less than a page in size as a result1 the kernel must use memory conservatively and attempt to minimize waste due to fragmentation this is especially important because many operating systems do not subject kernel code or data to the paging system  2 pages allocated to user-mode processes do not necessarily have to be in contiguous physical memory however/ certain hardware devices interact directly with physical memory-without the benefit of a virtual memory interface-and consequently may require memory residing in physically contiguous pages  in the following sections/ we examine two strategies for managing free memory that is assigned to kernel processes  the buddy system and slab allocation  9.8 397 9.8.1 buddy system tbe buddy system allocates memory from a fixed-size segment consisting of physically contiguous pages memory is allocated from this segment using a power-of-2 allocator  which satisfies requests in units sized as a power of 2  4 kb  8 kb  16 kb  and so forth   a request in units not appropriately sized is rounded up to the next highest power of 2 for example  if a request for 11 kb is made  it is satisfied with a 16-kb segment  let 's consider a simple example assume the size of a memory segment is initially 256 kb and the kernel requests 21 kb of memory the segment is initially divided into two buddies-which we will call al and ar -each 128 kb in size one of these buddies is further divided into two 64-kb buddiesbland br however  the next-highest power of 2 from 21 kb is 32 kb so either bt or br is again divided into two 32-kb buddies  cl and cr one of these buddies is used to satisfy the 21-kb request this scheme is illustrated in figure 9.27  where cl is the segment allocated to the 21 kb request  an advantage of the buddy system is how quickly adjacent buddies can be combined to form larger segments using a teclmique known as coalescing in figure 9.27  for example  when the kernel releases the cl unit it was allocated  the system can coalesce c l and c r into a 64-kb segment this segment  b l  can in turn be coalesced with its buddy b r to form a 128-kb segment ultimately  we can end up with the original256-kb segment  the obvious drawback to the buddy system is that rounding up to the next highest power of 2 is very likely to cause fragmentation within allocated segments for example  a 33-kb request can only be satisfied with a 64 kb segment in fact  we can not guarantee that less than 50 percent of the allocated unit will be wasted due to internal fragmentation in the following section  we explore a memory allocation scheme where no space is lost due to fragmentation  physically contiguous pages 256 kb figure 9.27 buddy system allocation  398 chapter 9 9.8.2 slab allocation a second strategy for allocating kernel memory is known as a is made up of one or nwre physically contiguous pages a consists of one or more slabs there is a single cache for each unique kernel data structure -for example  a separate cache for the data structure representing process descriptors  a separate cache for file objects  a separate cache for semaphores  and so forth each cache is populated with that are instantiations of the kernel data structure the cache represents for example  the cache representing semaphores stores instances of semaphore objects  the cache representing process descriptors stores instances of process descriptor objects  and so forth  the relationship between slabs  caches  and objects is shown in figure 9.28  the figure shows two kernel objects 3 kb in size and three objects 7 kb in size  these objects are stored in their respective caches  the slab-allocation algorithm uses caches to store kernel objects when a cache is created  a number of objects-which are initially marked as free-are allocated to the cache the number of objects in the cache depends on the size of the associated slab for example  a 12-kb slab  made up of three continguous 4-kb pages  could store six 2-kb objects initially  all objects in the cache are marked as free when a new object for a kernel data structure is needed  the allocator can assign any free object from the cache to satisfy the request the object assigned from the cache is marked as used  let 's consider a scenario in which the kernel requests memory from the slab allocator for an object representing a process descriptor in linux systems  a process descriptor is of the type struct task_struct  which requires approximately 1.7 kb of memory when the linux kernel creates a new task  it requests the necessary memory for the struct task_struct object from its cache the cache will fulfill the request using a struct task_struct object that has already been allocated in a slab and is marked as free  in linux  a slab may be in one of three possible states  kernel objects slabs 3-kb objects 7-kb objects figure 9.28 slab allocation  physically contiguous pages 9.9 9.9 full all objects in the slab are marked as used  empty all objects in the slab are marked as free  partial the slab consists of both used and free objects  399 the slab allocator first attempts to satisfy the request with a free object in a partial slab if none exist  a free object is assigned from an empty slab if no empty slabs are available  a new slab is allocated from contiguous physical pages and assigned to a cache ; memory for the object is allocated from this slab  the slab allocator provides two main benefits  no memory is wasted due to fragmentation fragn entation is not an issue because each unique kernel data structure has an associated cache  and each cache is made up of one or more slabs that are divided into chunks the size of the objects being represented thus  when the kernel requests memory for an object  the slab allocator returns the exact amount of memory required to represent the object  memory requests can be satisfied quickly the slab allocation scheme is thus particularly effective for mm aging memory when objects are frequently allocated and deallocated  as is often the case with requests from the kernel the act of allocating-and releasing-memory can be a time-consuming process however  objects are created in advance and thus can be quickly allocated from the cache furthermore  when the kernel has finished with an object and releases it  it is marked as free and returned to its cache  thus making it immediately available for subsequent requests fi om the kernel  the slab allocator first appeared in the solaris 2.4 kernel because of its general-purpose nature  this allocator is now also used for certain user-mode memory requests in solaris linux originally used the buddy system ; however  beginning with version 2.2  the linux kernel adopted the slab allocator  the major decisions that we make for a paging system are the selections of a replacement algorithm and an allocation policy  which we discussed earlier in this chapter there are many other considerations as well  and we discuss several of them here  9.9.1 prepaging an obvious property of pure demand paging is the large number of page faults that occur when a process is started this situation results from trying to get the initial locality into memory the same situation may arise at other times for instance  when a swapped-out process is restarted  all its are on the disk  and each must be brought in by its own page fault is an attempt to prevent this high level of initial paging the strategy is to bring into memory at 400 chapter 9 one tin1.e all the pages that will be needed some operating systerns-notably solaris-prepage the page frames for small files  in a system using the working-set model  for example  we keep with each process a list of the pages in its working set if we must suspend a process  due to an i/0 wait or a lack of free frames   we remember the working set for that process when the process is to be resumed  because i/0 has finished or enough free frames have become available   we automatically bring back into memory its entire working set before restarting the process  prepaging may offer an advantage in some cases the question is simply whether the cost of using prepaging is less than the cost of servicing the corresponding page faults it may well be the case that many of the pages brought back into memory by prepaging will not be used  assume that s pages are prepaged and a fraction a of these s pages is actually used  0  '    a  '    1   the question is whether the cost of the s .a saved page faults is greater or less than the cost of prepaging s   1  a  unnecessary pages if a is close to 0  prepaging loses ; if a is close to 1  prepaging wins  9.9.2 page size the designers of an operating system for an existing machine seldom have a choice concerning the page size however  when new machines are being designed  a decision regarding the best page size must be made as you might expect  there is no single best page size rather  there is a set of factors that support various sizes page sizes are invariably powers of 2  generally ranging from 4,096  212  to 4,194,304  222  bytes  how do we select a page size one concern is the size of the page table for a given virtual memory space  decreasing the page size increases the number of pages and hence the size of the page table for a virtual memory of 4 mb  222   for example  there would be 4,096 pages of 1,024 bytes but only 512 pages of 8,192 bytes because each active process must have its own copy of the page table  a large page size is desirable  memory is better utilized with smaller pages  however if a process is allocated memory starting at location 00000 and continuing until it has as much as it needs  it probably will not end exactly on a page boundary thus  a part of the final page must be allocated  because pages are the units of allocation  but will be unused  creating internal fragmentation   assuming independence of process size and page size  we can expect that  on the average  half of the final page of each process will be wasted this loss is only 256 bytes for a page of 512 bytes but is 4,096 bytes for a page of 8,192 bytes to minimize internal fragmentation  then  we need a small page size  another problem is the time required to read or write a page i/0 time is composed of seek  latency  and transfer times transfer time is proportional to the amount transferred  that is  the page size  -a fact that would seem to argue for a small page size howeve1 ~ as we shall see in section 12.1.1  latency and seek time normally dwarf transfer time at a transfer rate of 2 mb per second  it takes only 0.2 milliseconds to transfer 512 bytes latency time  though  is perhaps 8 milliseconds and seek time 20 milliseconds of the total i/0 time  28.2 milliseconds   therefore  only 1 percent is attributable to the actual transfer doubling the page size increases i/0 time to only 28.4 milliseconds it takes 28.4 milliseconds to read a single page of 1,024 bytes but 9.9 401 56.4 milliseconds to read the sam.e amount as two pages of 512 bytes each  thus  a desire to minimize 1/0 time argues for a larger page size  with a smaller page size  though  to tall /0 should be reduced  since locality will be improved a smaller page size allows each page to match program locality more accurately for example  consider a process 200 kb in size  of which only half  100 kb  is actually used in an execution if we have only one large page  we must bring in the entire page  a total of 200 kb transferred and allocated if instead we had pages of only 1 byte  then we could bring in only the 100 kb that are actually used  resulting in only 100 kb transferred and allocated with a smaller page size  we have better allowing us to isolate only the memory that is actually needed with a larger page size  we must allocate and transfer not only what is needed but also anything else that happens to be in the page  whether it is needed or not thus  a smaller page size should result in less i/0 and less total allocated memory  but did you notice that with a page size of 1 byte  we would have a page fault for each byte a process of 200 kb that used only half of that memory would generate only one page fault with a page size of 200 kb but 102,400 page faults with a page size of 1 byte each page fault generates the large amount of overhead needed for processing the interrupt  saving registers  replacing a page  queueing for the paging device  and updating tables to minimize the number of page faults  we need to have a large page size  other factors must be considered as well  such as the relationship between page size and sector size on the paging device   the problem has no best answer as we have seen  some factors  internal fragmentation  locality  argue for a small page size  whereas others  table size  i/0 time  argue for a large page size however  the historical trend is toward larger page sizes indeed  the first edition of operating system concepts  1983  used 4,096 bytes as the upper bound on page sizes  and this value was the most common page size in 1990  modern systems may now use much larger page sizes  as we will see in the following section  9.9.3 tlb reach in chapter 8  we introduced the of the tlb recall that the hit ratio for the tlb refers to the percentage of virtual address translations that are resolved in the tlb rather than the page table clearly  the hit ratio is related to the number of entries in the tlb  and the way to increase the hit ratio is by increasing the number of entries in the tlb this  however  does not come cheaply  as the associative memory used to construct the tlb is both expensive and power hungry  related to the hit ratio is a similar metric  the the tlb reach refers to the amount of memory accessible from the tlb and is simply the number of entries multiplied by the page size ideally  the working set for a process is stored in the tlb if it is not  the process will spend a considerable amount of time resolving memory references in the page table rather than the tlb if we double the number of entries in the tlb  we double the tlb reach however  for some memory-intensive applications  this may still prove insufficient for storing the working set  another approacl1 for increasing the tlb reach is to either increase the size of the page or provide multiple page sizes if we increase the page size-say  402 chapter 9 from 8 kb to 32 kb-we quadruple the tlb reach however  this may lead to an increase in fragmentation for some applications that do not require such a large page size as 32 kb alternatively  an operating system may provide several different page sizes for example  the ultrasparc supports page sizes of 8 kb  64 kb  512 kb  and 4mb of these available pages sizes  solaris uses both 8-kb and 4-mb page sizes and with a 64-entry tlb  the tlb reach for solaris ranges from 512 kb with 8-kb pages to 256mb with 4-mb pages for the majority of applications  the 8-kb page size is sufficient  although solaris maps the first 4 mb of kernel code and data with two 4-mb pages solaris also allows applications-such as databases-to take advantage of the large 4-mb page size  providing support for multiple page sizes requires the operating system -not hardware-to manage the tlb for example  one of the fields in a tlb entry must indicate the size of the page frame corresponding to the tlb entry managing the tlb in software and not hardware comes at a cost in performance howeve1 ~ the increased hit ratio and tlb reach offset the performance costs indeed  recent trends indicate a move toward softwaremanaged tlbs and operating-system support for multiple page sizes the ultrasparc  mips  and alpha architectures employ software-managed tlbs  the powerpc and pentium manage the tlb in hardware  9.9.4 inverted page tables section 8.5.3 introduced the concept of the inverted page table the purpose of this form of page management is to reduce the amount of physical memory needed to track virtual-to-physical address translations we accomplish this savings by creating a table that has one entry per page of physical memory  indexed by the pair process-id  page-number  because they keep information about which virtual memory page is stored in each physical frame  inverted page tables reduce the amount of physical memory needed to store this information however  the inverted page table no longer contains complete information about the logical address space of a process  and that information is required if a referenced page is not currently in memory demand paging requires this information to process page faults  for the information to be available  an external page table  one per process  must be kept each such table looks like the traditional per-process page table and contains information on where each virtual page is located  but do external page tables negate the utility of inverted page tables since these tables are referenced only when a page fault occurs  they do not need to be available quickly instead  they are themselves paged in and out of memory as necessary unfortunately  a page fault may now cause the virtual memory n1.anager to generate another page fault as it pages in the external page table it needs to locate the virtual page on the backing store this special case requires careful handling in the kernel and a delay in the page-lookup processing  9.9.5 program structure demand paging is designed to be transparent to the user program in many cases  the user is completely unaware of the paged nature of memory in other cases  however  system performance can be improved if the user  or compiler  has an awareness of the underlying demand paging  9.9 403 let 's look at a contrived but informative example assume that pages are 128 words in size consider a c program whose function is to initialize to 0 each element of a 128-by-128 array the following code is typical  inti  j ; int  128j  128j data ; for  j = 0 ; j 128 ; j + +  for  i = 0 ; i 128 ; i + +  data  ij  jj = 0 ; notice that the array is stored row major ; that is  the array is stored data  oj  oj  data  oj  1j   data  oj  127j  data  1j  oj  data  1j  1j   data  127j  127j for pages of 128 words  each row takes one page thus  the preceding code zeros one word in each page  then another word in each page  and so on if the operating system allocates fewer than 128 frames to the entire program  then its execution will result in 128 x 128 = 16,384 page faults  in contrast  suppose we change the code to inti  j ; int  128j  128j data ; for  i = 0 ; i 128 ; i + +  for  j = 0 ; j 128 ; j + +  data  ij  jj = 0 ; this code zeros all the words on one page before starting the next page  reducing the number of page faults to 128  careful selection of data structures and programming structures can increase locality and hence lower the page-fault rate and the number of pages in the working set for example  a stack has good locality  since access is always made to the top a hash table  in contrast  is designed to scatter references  producing bad locality of course  locality of reference is just one measure of the efficiency of the use of a data structure other heavily weighted factors include search speed  total number of memory references  and total number of pages touched  at a later stage  the compiler and loader can have a sigicificant effect on paging separating code and data and generating reentrant code means that code pages can be read-only and hence will never be modified clean pages do not have to be paged out to be replaced the loader can avoid placing routines across page boundaries  keeping each routine completely in one page  routines that call each other many times can be packed into the same page  this packaging is a variant of the bin-packing problem of operations research  try to pack the variable-sized load segments into the fixed-sized pages so that interpage references are minimized such an approach is particularly useful for large page sizes  the choice of programming language can affect paging as well for example  c and c + + use pointers frequently  and pointers tend to randomize access to memory  thereby potentially diminishing a process 's locality some studies have shown that object-oriented programs also tend to have a poor locality of reference  404 chapter 9 9.9.6 1/0 interlock when demand paging is used  we sometimes need to allow some of the pages to be in n emory one such situation occurs when i/0 is done to or from user  virtual  memory l/0 is often implemented by a separate i/0 processor  for example  a controller for a usb storage device is generally given the number of bytes to transfer and a memory address for the buffer  figure 9.29   when the transfer is complete  the cpu is interrupted  we must be sure the following sequence of events does not occur  a process issues an i/0 request and is put in a queue for that i/o device meanwhile  the cpu is given to other processes these processes cause page faults ; and one of them  using a global replacement algorithm  replaces the page containing the memory buffer for the waiting process the pages are paged out some time later  when the i/o request advances to the head of the device queue  the i/o occurs to the specified address however  this frame is now being used for a different page belonging to another process  there are two common solutions to this problem one solution is never to execute i/0 to user memory instead  data are always copied between system memory and user memory i/0 takes place only between system memory and the i/0 device to write a block on tape  we first copy the block to system memory and then write it to tape this extra copying may result in unacceptably high overhead  another solution is to allow pages to be locked into memory here  a lock bit is associated with every frame if the frame is locked  it can not be selected for replacement under this approach  to write a block on tape  we lock into memory the pages containing the block the system can then continue as usual locked pages can not be replaced when the i/o is complete  the pages are unlocked  figure 9.29 the reason why frames used for 1/0 must be in memory  9.10 9.10 405 lock bits are used in various situations frequently  some or all of the operating-system kernel is locked into memory  as many operating systems can not tolerate a page fault caused by the kernel  another use for a lock bit involves normal page replacement consider the following sequence of events  a low-priority process faults selecting a replacement frame  the paging system reads the necessary page into memory  ready to continue  the low-priority process enters the ready queue and waits for the cpu since it is a low-priority process  it may not be selected by the cpu scheduler for a time while the low-priority process waits  a high-priority process faults looking for a replacement  the paging system sees a page that is in memory but has not been referenced or modified  it is the page that the low-priority process just brought in this page looks like a perfect replacement  it is clean and will not need to be written out  and it apparently has not been used for a long time  whether the high-priority process should be able to replace the low-priority process is a policy decision after all  we are simply delaying the low-priority process for the benefit of the high-priority process however  we are wasting the effort spent to bring in the page for the low-priority process if we decide to prevent replacement of a newly brought-in page until it can be used at least once  then we can use the lock bit to implement this mechanism when a page is selected for replacement  its lock bit is turned on ; it remains on until the faulting process is again dispatched  using a lock bit can be dangerous  the lock bit may get turned on but never turned off should this situation occur  because of a bug in the operating system  for example   the locked frame becomes unusable on a single-user system  the overuse of locking would hurt only the user doing the locking  multiuser systems must be less trusting of users for instance  solaris allows locking hints  but it is free to disregard these hints if the free-frame pool becomes too small or if an individual process requests that too many pages be locked in memory  in this section  we describe how windows xp and solaris implement virtual memory  9.10.1 windows xp windows xp implements virtual memory using demand paging with clustering handles page faults by bringing in not only the faultil1.g page also several pages following the faulting page when a process is first created  it is assigned a working-set minimum and maximum the is the minimum number of pages the process is guaranteed to in memory if sufficient memory is available  a process may be assigned as many pages as its for most applications  the value of working-set minimum and working-set maximum is 50 and 345 pages  respectively  in some circumstances  a process may be allowed to exceed its working-set maximum  the virtual memory manager maintains a list of free page frames associated with this list is a threshold value that is used to 406 chapter 9 indicate whether sufficient free memory is available if a page fault occurs for a process that is below its working-set maximum  the virtual memory manager allocates a page from this list of free pages if a process that is at its working-set rnaximum incurs a page fault  it must select a page for replacement using a local page-replacement policy  when the amount of free memory falls below the threshold  the virtual memory manager uses a tactic known as to restore the value above the threshold automatic working-set trimming works by evaluating the number of pages allocated to processes if a process has been allocated more pages than its working-set minimum  the virtual memory manager removes pages until the process reaches its working-set minimum a process that is at its working-set minimum may be allocated pages from the free-page-frame list once sufficient free memory is available  the algorithm used to determine which page to remove from a working set depends on the type of processor on single-processor 80x86 systems  windows xp uses a variation of the clock algorithm discussed in section 9.4.5.2 on alpha and multiprocessor x86 systems  clearing the reference bit may require invalidatil g the entry in the translation look-aside buffer on other processors  rather than incurring this overhead  windows xp uses a variation on the fifo algorithm discussed in section 9.4.2  9.10.2 solaris in solaris  when a thread incurs a page fault  the kernel assigns a page to the faulting thread from the list of free pages it maintains therefore  it is imperative that the kernel keep a sufficient amount of free memory available  associated with this list of free pages is a parameter-zotsfree-that represents a threshold to begin paging the lotsfree parameter is typically set to 1/64 the size of the physical memory four times per second  the kernel checks whether the amount of free memory is less than lotsfree if the number of free pages falls below lotsfree  a process known as a pageout starts up the pageout process is similar to the second-chance algorithm described in section 9.4.5.2  except that it uses two hands while scanning pages  rather than one the pageout process works as follows  the front hand of the clock scans all pages in memory  setting the reference bit to 0 later  the back hand of the clock examines the reference bit for the pages in memory  appending each page whose reference bit is still set to 0 to the free list and writing to disk its contents if modified solaris maintains a cache list of pages that have been freed but have not yet been overwritten  the free list contains frames that have invalid contents pages can be reclaimed from the cache list if they are accessed before being moved to the free list  the pageout algorithm uses several parameters to control the rate at which pages are scam ed  known as the scanrate   the scanrate is expressed in pages per second and ranges from slowscan to fastscan when free memory falls below lotsfree  scanning occurs at slowscan pages per second and progresses to fastscan  depending on the amount of free memory available the default value of slowscan is 100 pages per second ; fasts can is typically set to the value  total physical pages  /2 pages per second  with a maximum of 8,192 pages per second this is shown in figure 9.30  withfastscan set to the maximum   the distance  in pages  between the hands of the clock is determil ed by a system parameter  handspread the amount of time between the front hand 's 9.11 8192 fastscan cll 7 c  1j u en 100 slowscan minfree desfree amount of free memory figure 9.30 solaris page scanner  9.11 407 lotsfree clearing a bit and the back hand 's investigating its value depends on the scanrate and the handspread if scam-ate is 100 pages per second and handspread is 1,024 pages  10 seconds can pass between the time a bit is set by the front hand and the time it is checked by the back hand however  because of the demands placed on the memory system  a scanrate of several thousand is not uncommon  this means that the amount of time between clearing and investigating a bit is often a few seconds  as mentioned above  the pageout process checks memory four times per second however  if free memory falls below desfree  figure 9.30   pageout will nm 100 times per second with the intention of keeping at least desfree free memory available if the pageout process is unable to keep the amount of free memory at desfree for a 30-second average  the kernel begins swapping processes  thereby freeing all pages allocated to swapped processes in general  the kernel looks for processes that have been idle for long periods of time if the system is unable to maintain the amount of free memory at minfree  the pageout process is called for every request for a new page  recent releases of the solaris kernel have provided enhancements of the paging algorithm one such enhancement involves recognizing pages from shared libraries pages belonging to libraries that are being shared by several processes-even if they are eligible to be claimed by the scannerare skipped during the page-scanning process another enhancement concerns distinguishing pages that have been allocated to processes from pages allocated to regularfiles this is known as and is covered in section 11.6.2  it is desirable to be able to execute a process whose logical address space is larger than the available physical address space virtual memory is a technique 408 chapter 9 that enables us to map a large logical address space onto a smaller physical menlory virtual memory allows us to run extremely large processes and to raise the degree of multiprogramming  increasing cpu utilization further  it frees application programmers from worrying about memory availability in addition  with virtual memory  several processes can share system libraries and memory virtual memory also enables us to use an efficient type of process creation known as copy-on-write  wherein parent and child processes share actual pages of memory  virtual memory is commonly implemented by demand paging pure demand paging never brings in a page until that page is referenced the first reference causes a page fault to the operating system the operating-system kernel consults an internal table to determine where the page is located on the backing store it then finds a free frame and reads the page in from the backing store the page table is updated to reflect this change  and the instruction that caused the page fault is restarted this approach allows a process to run even though its entire memory image is not in main memory at once as long as the page-fault rate is reasonably low  performance is acceptable  we can use demand paging to reduce the number of frames allocated to a process this arrangement can increase the degree of multiprogramming  allowing more processes to be available for execution at one time  and-in theory  at least-the cpu utilization of the system it also allows processes to be run even though their memory requirements exceed the total available physical memory such processes run in virtual memory  if total memory requirements exceed the capacity of physical memory  then it may be necessary to replace pages from memory to free frames for new pages various page-replacement algorithms are used fifo page replacement is easy to program but suffers from belady 's anomaly optimal page replacement requires future knowledge lru replacement is an approximation of optimal page replacement  but even it may be difficult to implement  most page-replacement algorithms  such as the second-chance algorithm  are approximations of lru replacement  in addition to a page-replacement algorithm  a frame-allocation policy is needed allocation can be fixed  suggesting local page replacement  or dynamic  suggesting global replacement the working-set model assumes that processes execute in localities the working set is the set of pages in the current locality accordingly  each process should be allocated enough frames for its current working set if a process does not have enough memory for its working set  it will thrash providing enough frames to each process to avoid thrashing may require process swapping and schedulil g  most operating systems provide features for memory mappil1g files  thus allowing file i/0 to be treated as routine memory access the win32 api implements shared memory through memory mappil1g files  kernel processes typically req1.1ire memory to be allocated using pages that are physically contiguous the buddy system allocates memory to kernel processes in units sized according to a power of 2  which often results in fragmentation slab allocators assign kernel data structures to caches associated with slabs  which are made up of one or more physically contiguous pages  with slab allocation  no memory is wasted due to fragmentation  and memory requests can be satisfied quickly  409 in addition to reqmnng that we solve the major problems of page replacement and frame allocation  the proper design of a paging systern requires that we consider prep aging  page size  tlb reach  inverted page tables  program structure  i/0 interlock  and other issues  9.1 assume there is a 1,024-kb segment where memory is allocated using the buddy system using figure 9.27 as a guide  draw a tree illustrating how the following memory requests are allocated  request 240 bytes request 120 bytes request 60 bytes request 130 bytes next modify the tree for the followilcg releases of memory perform coalescing whenever possible  release 240 bytes release 60 bytes release 120 bytes 9.2 consider the page table for a system with 12-bit virtual and physical addresses with 256-byte pages the list of free page frames is d  e  f  that is  dis at the head of the list e is second  and f is last   410 chapter 9 convert the following virtual addresses to their equivalent physical addresses in hexadecimal all numbers are given in hexadecimal  a dash for a page frame indicates that the page is not in memory  9ef 111 700 off 9.3 a page-replacement algorithm should minimize the number of page faults we can achieve this minimization by distributing heavily used pages evenly over all of memory  rather than having them compete for a small number of page frames we can associate with each page frame a counter of the number of pages associated with that frame then  to replace a page  we can search for the page frame with the smallest counter  a define a page-replacement algorithm using this basic idea specifically address these problems  i what is the initial value of the counters ii when are counters increased iii when are counters decreased 1v how is the page to be replaced selected b how many page faults occur for your algorithm for the following reference string with four page frames 1  2  3  4  5  3  4  1  6  7  8  7  8  9  7  8  9  5  4  5  4  2  c what is the minimum number of page faults for an optimal pagereplacement strategy for the reference string in part b with four page frames 9.4 consider a demand-paging system with the following time-measured utilizations  cpu utilization paging disk other i/0 devices 20 % 97.7 % 5 % for each of the following  say whether it will  or is likely to  improve cpu utilization explain your answers  a install a faster cpu  b install a bigger paging disk  c increase the degree of multiprogramming  d decrease the degree of multiprogramming  411 e install more main n1.enl0ry  f install a faster hard disk or multiple controllers with multiple hard disks  g add prepaging to the page-fetch algorithms  h increase the page size  9.5 consider a demand-paged computer system where the degree of multiprogramming is currently fixed at four the system was recently measured to determine utilization of the cpu and the paging disk  the results are one of the following alternatives for each case  what is happening can the degree of multiprogramming be increased to increase the cpu utilization is the paging helping a cpu utilization 13 percent ; disk utilization 97 percent b cpu utilization 87 percent ; disk utilization 3 percent c cpu utilization 13 percent ; disk utilization 3 percent 9.6 consider a demand-paging system with a paging disk that has an average access and transfer time of 20 milliseconds addresses are translated through a page table in main memory  with an access time of 1 microsecond per memory access thus  each memory reference through the page table takes two accesses to improve this time  we have added an associative memory that reduces access time to one memory reference if the page-table entry is in the associative memory  assume that 80 percent of the accesses are in the associative memory and that  of those remaining  10 percent  or 2 percent of the total  cause page faults what is the effective memory access time 9.7 a simplified view of thread states is ready  running  and blocked  where a thread is either ready and waiting to be scheduled  is running on the processor  or is blocked  i.e is waiting for i/0  this is illustrated in figure 9.31 assuming a thread is in the running state  answer the following questions   be sure to explain your answer  a will the thread change state if it incurs a page fault if so  to what new state figure 9.31 thread state diagram for exercise 9.7  412 chapter 9 b will the thread change state if it generates a tlb miss that is resolved in the page table if so  to what new state c will the thread change state if an address reference is resolved in the page table if so  to what new state 9.8 discuss the hardware support required to support demand paging  9.9 consider the following page reference string  1  2  3  4  2  1  5  6  2  1  2  3  7  6  3  2  1  2  3  6  how many page faults would occur for the following replacement algorithms  assuming one  two  three  four  five  six  and seven frames remember that all frames are initially empty  so your first unique pages will cost one fault each  lru replacement fifo replacement optimal replacement 9.10 consider a system that allocates pages of different sizes to its processes  what are the advantages of such a paging scheme what modifications to the virtual memory system provide this functionality 9.11 discuss situations in which the most frequently used page-replacement algorithm generates fewer page faults than the least recently used page-replacement algorithm also discuss under what circumstances the opposite holds  9.12 under what circumstances do page faults occur describe the actions taken by the operating system when a page fault occurs  9.13 suppose that a machine provides instructions that can access memory locations using the one-level indirect addressing scheme what sequence of page faults is ilccurred when all of the pages of a program are currently nonresident and the first instruction of the program is an indirect memory-load operation what happens when the operating system is using a per-process frame allocation technique and only two pages are allocated to this process 9.14 consider a system that provides support for user-level and kernellevel threads the mapping in this system is one to one  there is a corresponding kernel thread for each user thread   does a multithreaded process consist of  a  a working set for the entire process or  b  a working set for each thread explain  413 9.15 what is the copy-on-write feature  and under what circumstances is it beneficial to use this feature what hardware support is required to implement this feature 9.16 consider the two-dimensional array a  int a     = new int  100   100  ; where a  oj  oj is at location 200 in a paged memory system with pages of size 200 a small process that manipulates the matrix resides in page 0  locations 0 to 199   thus  every instruction fetch will be from page 0  for three page frames  how many page faults are generated by the following array-initialization loops  using lru replacement and assuming that page frame 1 contains the process and the other two are initially empty a for  int j = 0 ; j 100 ; j + +  for  int i = 0 ; i 100 ; i + +  a  i   j  = 0 ; b for  int i = 0 ; i 100 ; i + +  for  int j = 0 ; j 100 ; j + +  a  i   j  = 0 ; 9.17 discuss situations in which the least frequently used page-replacement algorithm generates fewer page faults than the least recently used page-replacement algorithm also discuss under what circumstances the opposite holds  9.18 what is the cause of thrashing how does the system detect thrashing once it detects thrashing  what can the system do to eliminate this problem 9.19 assume that you are monitoring the rate at which the pointer in the clock algorithm  which indicates the candidate page for replacement  moves what can you say about the system if you notice the following behavior  a pointer is moving fast  b pointer is moving slow  9.20 the vax/vms system uses a fifo replacement algorithm for resident pages and a free-frame pool of recently used pages assume that the free-frame pool is managed using the least recently used replacement policy answer the following questions  a if a page fault occurs and if the page does not exist in the free-frame pool  how is free space generated for the newly requested page 414 chapter 9 b if a page fault occurs and if the page exists in the free-frame pool  how is the resident page set and the free-france pool managed to make space for the requested page c what does the system degenerate to if the number of resident pages is set to one d what does the system degenerate to if the number of pages in the free-frame pool is zero 9.21 the slab-allocation algorithm uses a separate cache for each different object type assuming there is one cache per object type  explain why this scheme does n't scale well with multiple cpus what could be done to address this scalability issue 9.22 assume that we have a demand-paged memory the page table is held in registers it takes 8 milliseconds to service a page fault if an empty frame is available or if the replaced page is not modified and 20 milliseconds if the replaced page is modified memory-access time is 100 nanoseconds  assume that the page to be replaced is modified 70 percent of the time what is the maximum acceptable page-fault rate for an effective access time of no more than 200 nanoseconds 9.23 segmentation is similar to paging but uses variable-sized pages define two segment-replacement algorithms based on fifo and lru pagereplacement schemes remember that since segments are not the same size  the segment that is chosen to be replaced may not be big enough to leave enough consecutive locations for the needed segment consider strategies for systems where segments cam ot be relocated and strategies for systems where they can  9.24 which of the following programming techniques and structures are good for a demand-paged environment which are not good explain your answers  a stack b hashed symbol table c sequential search d binary search e pure code f vector operations a indirection b 9.25 when a page fault occurs  the process requesting the page must block while waiting for the page to be brought from disk into physical memory  assume that there exists a process with five user-level threads and that the mapping of user threads to kernel threads is many to one if one user thread incurs a page fault while accessing its stack  would the other user user threads belonging to the same process also be affected by the page fault-that is  would they also have to wait for the faulting page to be brought into memory explain  415 9.26 consider a system that uses pure demand paging  a when a process first starts execution  how would you characterize the page fault rate b once the working set for a process is loaded into memory  how would you characterize the page fault rate c assume that a process changes its locality and the size of the new working set is too large to be stored in available free memory  identify some options system designers could choose from to handle this situation  9.27 assume that a program has just referenced an address in virtual memory  describe a scenario in which each of the following can occur  if no such scenario can occur  explain why  tlb miss with no page fault tlb miss and page fault tlb hit and no page fault tlb hit and page fault 9.28 a certain computer provides its users with a virtual memory space of 232 bytes the computer has 218 bytes of physical memory the virtual memory is implemented by paging  and the page size is 4,096 bytes  a user process generates the virtual address 11123456 explain how the system establishes the corresponding physical location distinguish between software and hardware operations  9.29 when virtual memory is implemented in a computing system  there are certain costs associated with the technique and certain benefits list the costs and the benefits is it possible for the costs to exceed the benefits if it is  what measures can be taken to ensure that this does not happen 9.30 give an example that illustrates the problem with restarting the move character instruction  mvc  on the ibm 360/370 when the source and destination regions are overlapping  9.31 consider the parameter 6 used to define the working-set window in the working-set model what is the effect of setting 6 to a small value on the page-fault frequency and the number of active  nonsuspended  processes currently executing in the system what is the effect when 6  is set to a very high value 9.32 is it possible for a process to have two working sets  one representing data and another representing code explain  9.33 suppose that your replacement policy  in a paged system  is to examine each page regularly and to discard that page if it has not been used since the last examination what would you gain and what would you lose by using this policy rather than lru or second-chance replacement 416 chapter 9 9.34 write a program that implements the fifo and lru page-replacement algorithms presented in this chapter first  generate a random pagereference string where page numbers range from 0 to 9 apply the random page-reference string to each algorithm  and record the number of page faults incurred by each algorithm implement the replacement algorithms so that the number of page frames can vary from 1 to 7  assume that demand paging is used  9.35 the catalan numbers are an integer sequence c11 that appear in treeenumeration problems the first catalan numbers for n = 1  2  3   are 1  2  5  14  42  132   a formula generating c11 is 1  2n   2n  ! ell =  n + 1   ; ; =  n + 1  ! n ! design two programs that communicate with shared memory using the win32 api as outlined in section 9.7.2 the producer process will generate the catalan sequence and write it to a shared memory object  the consumer process will then read and output the sequence from shared memory  in this instance  the producer process will be passed an integer parameter on the command line specifying how many catalan numbers to produce  for example  providing 5 on the command line means the producer process will generate the first five catalan numbers   demand paging was first used iil the atlas system  implemented on the manchester university muse computer around 1960  kilburn et al  1961    another early demand-paging system was multics  implemented on the ge 645 system  organick  1972    belady et al  1969  were the first researchers to observe that the fifo replacement strategy may produce the anomaly that bears belady 's name  mattson et al  1970  demonstrated that stack algorithms are not subject to belady 's anomaly  the optimal replacement algorithm was presented by belady  1966  and was proved to be optimal by mattson et al  1970   belady ' s optimal algorithm is for a fixed allocation ; prieve and fabry  1976  presented an optimal algorithm for situations in which the allocation can vary  the enl lanced clock algorithm was discussed by carr and hennessy  1981   the working-set model was developed by denning  1968   discussions concerning the working-set model were presented by denning  1980   the scheme for monitoring the page-fault rate was developed by wulf  1969   who successfully applied this technique to the burroughs bssoo computer system  wilson et al  1995  presented several algoritluns for dynamic memory allocation  jolmstone and wilson  1998  described various memory-fragmentation 417 issues buddy system memory allocators were described in knowlton  1965l peterson and norman  1977   and purdom  jr and stigler  1970   bonwick  1994  discussed the slab allocator  and bonwick and adams  2001  extended the discussion to multiple processors other memory-fitting algorithms can be found in stephenson  1983   bays  1977   and brent  1989   a survey of memory-allocation strategies can be found in wilson et al  1995   solomon and russinovich  2000  and russinovich and solomon  2005  described how windows implements virtual memory mcdougall and mauro  2007  discussed virtual memory in solaris virtual memory techniques in linux and bsd were described by bovet and cesati  2002  and mckusick et al  1996   respectively ganapathy and schimmel  1998  and navarro et al   2002  discussed operating system support for multiple page sizes ortiz  2001  described virtual memory used in a real-time embedded operating system  jacob and mudge  1998b  compared implementations of virtual memory in the mips  powerpc  and pentium architectures a companion article  jacob and mudge  1998a   described the hardware support necessary for implementation of virtual memory in six different architectures  including the ultrasparc  part five since main memory is usually too small to accommodate all the data and programs permanently  the computer system must provide secondary storage to back up main memory modern computer systems use disks as the primary on-line storage medium for information  both programs and data   the file system provides the mechanism for on-line storage of and access to both data and programs residing on the disks a file is a collection of related information defined by its creator the files are mapped by the operating system onto physical devices files are normally organized into directories for ease of use  the devices that attach to a computer vary in many aspects some devices transfer a character or a block of characters at a time some can be accessed only sequentially  others randomly some transfer data synchronously  others asynchronously some are dedicated  some shared they can be read-only or read-write they vary greatly in speed  in many ways  they are also the slowest major component of the computer  because of all this device variation  the operating system needs to provide a wide range of functionality to applications  to allow them to control all aspects of the devices one key goal of an operating system 's 1/0 subsystem is to provide the simplest interface possible to the rest of the system because devices are a performance bottleneck  another key is to optimize 1/0 for maximum concurrency  10.1 r for most users  the file system is the most visible aspect of an operating system  it provides the mechanism for on-line storage of and access to both data and programs of the operating system and all the users of the computer system the file system consists of two distinct parts  a collection of files  each storing related data  and a directory structure  which organizes and provides information about all the files in the system file systems live on devices  which we explore fully irl the following chapters but touch upon here in this chapter  we consider the various aspects of files and the major directory structures we also discuss the semantics of sharing files among multiple processes  users  and computers  finally  we discuss ways to handle file protection  necessary when we have multiple users and we want to control who may access files and how files may be accessed  to explain the function of file systems  to describe the interfaces to file systems  to discuss file-system design tradeoffs  including access methods  file sharing  file locking  and directory structures  to explore file-system protection  computers can store information on various storage media  such as magnetic disks  magnetic tapes  and optical disks so that the computer system will be convenient to use  the operating system provides a uniform logical view of information storage the operating system abstracts from the physical properties of its storage devices to define a logical storage unit  the file files are mapped by the operating system onto physical devices these storage devices are usually nonvolatile  so the contents are persistent through power failures and system reboots  421 422 chapter 10 a file is a named collection of related information that is recorded on secondary storage from a user 's perspective  a file is the smallest allotment of logical secondary storage ; that is  data can not be written to secondary storage unless they are within a file commonly  files represent programs  both source and object forms  and data data files may be numeric  alphabetic  alphanumeric  or binary files may be free form  such as text files  or may be formatted rigidly in general  a file is a sequence of bits  bytes  lines  or records  the meaning of which is defined by the file 's creator and user the concept of a file is thus extremely general the information in a file is defined by its creator many different types of information may be stored in a file-source programs  object programs  executable programs  numeric data  text  payroll records  graphic images  sound recordings  and so on a file has a certain defined which depends on its type a text file is a sequence of characters organized into lines  and possibly pages   a source file is a sequence of subroutines and functions  each of which is further organized as declarations followed by executable statements an object file is a sequence of bytes organized in.to blocks nnderstandable by the system 's linker an executable file is a series of code sections that the loader can bring into memory and execute  10.1.1 file attributes a file is named  for the convenience of its human users  and is referred to by its name a name is usually a string of characters  such as example.c some systems differentiate between uppercase and lowercase characters in names  whereas other systems do not when a file is named  it becomes independent of the process  the user  and even the system that created it for instance  one user might create the file example.c  and another user might edit that file by specifying its name the file 's owner might write the file to a floppy disk  send it in an e-mail  or copy it across a network  and it could still be called example.c on the destination system  a file 's attributes vary from one operating system to another but typically consist of these  name the symbolic file name is the only information kept in humanreadable form  identifier this unique tag  usually a number  identifies the file within the file system ; it is the non-human-readable name for the file  type this information is needed for systems that support different types of files  location this information is a pointer to a device and to the location of the file on that device  size the current size of the file  in bytes  words  or blocks  and possibly the maximum allowed size are included in this attribute  protection access-control information determines who can do reading  writing  executing  and so on  10.1 423 time  date  and user identification this information may be kept for creation  last modification  and last use these data can be useful for protection  security  and usage monitoring  the information about all files is kept in the directory structure  which also resides on secondary storage typically  a directory entry consists of the file 's name and its unique identifier the identifier in turn locates the other file attributes it may take more than a kilobyte to record this information for each file in a system with many files  the size of the directory itself may be megabytes because directories  like files  must be nonvolatile  they must be stored on the device and brought into memory piecemeal  as needed  10.1.2 file operations a file is an to define a file properly  we need to consider the operations that can be performed on files the operating system can provide system calls to create  write  read  reposition  delete  and truncate files let 's examine what the operating system must do to perform each of these six basic file operations it should then be easy to see how other similar operations  such as renaming a file  can be implemented  creating a file two steps are necessary to create a file first  space in the file system must be found for the file we discuss how to allocate space for the file in chapter 11 second  an entry for the new file must be made in the directory  writing a file to write a file  we make a system call specifying both the name of the file and the information to be written to the file given the name of the file  the system searches the directory to find the file 's location  the system must keep a write pointer to the location in the file where the next write is to take place the write pointer must be updated whenever a write occurs  reading a file to read from a file  we use a system call that specifies the name of the file and where  in memory  the next block of the file should be put again  the directory is searched for the associated entry  and the system needs to keep a read pointer to the location in the file where the next read is to take place once the read has taken place  the read pointer is updated because a process is usually either reading from or writing to a file  the current operation location can be kept as a per-process  both the read and write operations use this same pointer  saving space and reducing system complexity  repositioning within a file the directory is searched for the appropriate entry  and the current-file-position pointer is repositioned to a given value  repositioning within a file need not involve any actual i/0 this file operation is also kn.own as a file seek  deleting a file to delete a file  we search the directory for the named file  having found the associated directory entry  we release all file space  so that it can be reused by other files  and erase the directory entry  424 chapter 10 truncating a file the user may want to erase the contents of a file but keep its attributes rather than forcing the user to delete the file and then recreate it  this function allows all attributes to remain unchanged -except for file length-but lets the file be reset to length zero and its file space released  these six basic operations comprise the minimal set of required file operations other common operations include appending new information to the end of an existing file and renaming an existing file these primitive operations can then be combined to perform other file operations for instance  we can create a copy of a file  or copy the file to another i/o device  such as a printer or a display  by creating a new file and then reading from the old and writing to the new we also want to have operations that allow a user to get and set the various attributes of a file for example  we may want to have operations that allow a user to determine the status of a file  such as the file 's length  and to set file attributes  such as the file 's owner  most of the file operations mentioned involve searching the directory for the entry associated with the named file to avoid this constant searching  many systems require that an open   system call be made before a file is first used actively the operating system keeps a small table  called the containing information about all open files when a file operation is requested  the file is specified via an index into this table  so no searching is required  when the file is no longer being actively used  it is closed by the process  and the operating system removes its entry from the open-file table create and delete are system calls that work with closed rather than open files  some systems implicitly open a file when the first reference to it is made  the file is automatically closed when the job or program that opened the file terminates most systems  however  require that the programmer open a file explicitly with the open   system call before that file can be used the open   operation takes a file name and searches the directory  copying the directory entry into the open-file table the open   call can also accept accessmode information-create  read-only  read-write  append-only  and so on  this mode is checked against the file 's permissions if the request mode is allowed  the file is opened for the process the open   system call typically returns a pointer to the entry in the open-file table this pointer  not the actual file name  is used in all i/0 operations  avoiding any further searching and simplifying the system-call interface  the implementation of the open   and close   operations is more complicated in an environment where several processes may open the file simultaneously this may occur in a system ~ where several different applications open the same file at the same time typically  the operating system uses two levels of internal tables  a per-process table and a system-wide table the perprocess table tracks all files that a process has open stored in this table is information regarding the use of the file by the process for instance  the current file pointer for each file is found here access rights to the file and accounting information can also be included  each entry in the per-process table in turn points to a system-wide open-file table the system-wide table contains process-independent information  such as the location of the file on disk  access dates  and file size once a file has been opened by one process  the system-wide table includes an entry for the file  10.1 425 when another process executes an open   calt a new entry is simply added to the process 's open-file table pointing to the appropriate entry in the systemwide table typically  the open-file table also has an open count associated with each file to indicate how ncany processes have the file open each close   decreases this open count  and when the open count reaches zero  the file is no longer in use  and the file 's entry is removed from the open-file table  in summary  several pieces of information are associated with an open file  file pointer on systems that do not include a file offset as part of the read   and write   system calls  the systein must track the last readwrite location as a current-file-position pointer this pointer is unique to each process operating on the file and therefore must be kept separate from the on-disk file attributes  file-open count as files are closed  the operating system must reuse its open-file table entries  or it could run out of space in the table because multiple processes may have opened a file  the system must wait for the last file to close before removing the open-file table entry the file-open counter tracks the number of opens and closes and reaches zero on the last close the system can then remove the entry  disk location of the file most file operations require the system to modify data within the file the information needed to locate the file on disk is kept in memory so that the system does not have to read it from disk for each operation  access rights each process opens a file in an access mode this information is stored on the per-process table so the operating system can allow or deny subsequent i/0 requests  some operating systems provide facilities for locking an open file  or sections of a file   file locks allow one process to lock a file and prevent other processes from gaining access to it file locks are useful for files that are shared by several processes-for example  a system log file that can be accessed and modified by a number of processes in the system  file locking in java in the java api  acquiring a lock requires firstobtaini  ng the f  i..lechannel fbr thefile to be locked the loc ; k   method of the filechannel is used to acquir  o the lock the api of the lock   method is filelock lock  l.ong begin  long end  l ; ooleqn shared  where begin and end are the h  ~ gi1iningand ending positions of the region being locked settingshared to true isfb ~ shared locks ; setting shared to false acquires the lock exclusively tice lock is released by invoking the release   of the filelock returned by the lock   operati n  the program in figure 10.1 illusttates file locking in java  this program acquires two locks on thefilefile  txt the first half of.the file is acquired as an exclusive lock ~ the lock for the second half is a shared lock  426 chapter 10 file locks provide functionality similar to reader-writer locks  covered in section 6.6.2 a shared lock is akin to a reader lock in that several processes can acquire the lock concurrently an exclusive lock behaves like a writer lock ; only one process at a time can acquire such a lock it is important to note 10.1 427 that not au operating systems provide both types of locks ; some systems only provide exclusive file locking  furthermore  operating systems may provide either mandatory or advisory file-locking mechanisms if a lock is n1.andatory  then once a process acquires an exclusive lock  the operating system will prevent any other process from accessing the locked file for example  assume a process acquires an exclusive lock on the file system .log if we attempt to open system .log from another process-for example  a text editor-the operating system will prevent access until the exclusive lock is released this occurs even if the text editor is not written explicitly to acquire the lock alternatively  if the lock is advisory  then the operating system will not prevent the text editor from acquiring access to system .log rather  the text editor must be written so that it manually acquires the lock before accessing the file in other words  if the locking scheme is mandatory  the operating system ensures locking integrity  for advisory locking  it is up to software developers to ensure that locks are appropriately acquired and released as a general rule  windows operating systems adopt mandatory locking  and unix systems employ advisory locks  the use of file locks requires the same precautions as ordinary process synchronization for example  programmers developing on systems with mandatory locking must be careful to hold exclusive file locks only while they are accessing the file ; otherwise  they will prevent other processes from accessing the file as well furthermore  some measures must be taken to ensure that two or more processes do not become involved in a deadlock while trying to acquire file locks  10.1.3 file types when we design a file system-indeed  an entire operating system-we always consider whether the operating system should recognize and support file types if an operating system recognizes the type of a file  it can then operate on the file in reasonable ways for example  a common mistake occurs when a user tries to print the binary-object form of a program this attempt normally produces garbage ; however  the attempt can succeed if the operating system has been told that the file is a binary-object program  a common technique for implementing file types is to include the type as part of the file name the name is split into two parts-a name and an extension  usually separated by a period character  figure 10.2   in this way  the user and the operating system can tell from the name alone what the type of a file is  for example  most operating systems allow users to specify a file name as a sequence of characters followed by a period and terminated by an extension of additional characters file name examples include resume.doc  server.java  and readerthread c  the system uses the extension to indicate the type of the file and the type of operations that can be done on that file only a file with a .com  .exe  or .bat extension can be executed  for instance the .com and .exe files are two forms of binary executable files  whereas a .bat file is a containing  in ascii format  commands to the operating system ms-dos recognizes only a few extensions  but application programs also use extensions to indicate file types in which they are interested for example  assemblers expect source files to have an .asm extension  and the microsoft word word processor expects its files to 428 chapter 10 !   isnl ~ 1  f '  ~   j \  ir ~ i  tji ~  ~   '' ' r  ~ ~  r    ~ ; ,'u  ~ rt ~ tt ~ ~ ~   ~ \        ' ' ~   '   c executable exe  com  bin ready ~ to-run machineor none language program object obj  o compiled  machine language  not linked source code c  cc  java  pas  source code in various asm  a languages batch bat  sh commands to the command interpreter text txt  doc textual data  documents wo rdprocessor wp,tex  rtf  various wordcprocessor doc formats library lib  a  so  dll libraries o.troutines for .programmers print or view ps  pdf  jpg ascii or binary file in a format for printing or viewing archive arc  zip  .tar 1 related files grouped into .one file,sometimes compressed  for archiving or storage multimedia mpeg  mov  rm  binary file containing mp3  avi audio or a/v information figure 10.2 common file types  end with a .doc extension these extensions are not required  so a user may specify a file without the extension  to save typing   and the application will look for a file with the given name and the extension it expects because these extensions are not supported by the operating system  they can be considered as hints to the applications that operate on them  another example of the utility of file types comes from the tops-20 operating system if the user tries to execute an object program whose source file has been modified  or edited  since the object file was produced  the source file will be recompiled automatically this function ensures that the user always runs an up-to-date object file otherwise  the user could waste a significant amount of time executing the old object file for this function to be possible  the operating system must be able to discriminate the source file from the object file  to check the time that each file was created or last modified  and to determine the language of the source program  in order to use the correct compiler   consider  too  the mac os x operating system in this system  each file has a type  such as text  for text file  or appl  for application   each file also has a creator attribute containing the name of the program that created it this attribute is set by the operating system during the create   call  so its use is enforced and supported by the system for instance  a file produced by a word processor has the word processor 's name as its creator when the user opens that file  by double-clicking the mouse on the icon representing the file  10.1 429 the word processor is invoked automatically  and the file is loaded  ready to be edited  the unix system uses a crude stored at the beginning of some files to indicate roughly the type of the file-executable program  batch file  or postscript file  and so on not all files have magic numbers  so system features can not be based solely on this information unix does not record the name of the creating program  either unix does allow file-nameextension hints  but these extensions are neither enforced nor depended on by the operating system ; they are meant mostly to aid users in determining what type of contents the file contains extensions can be used or ignored by a given application  but that is up to the application 's programmer  10.1.4 file structure file types also can be used to indicate the internal structure of the file as mentioned in section 10.1.3  source and object files have structures that match the expectations of the programs that read them further  certain files must conform to a required structure that is understood by the operating system for example  the operating system requires that an executable file have a specific structure so that it can determine where in memory to load the file and what the location of the first instruction is some operating systems extend this idea into a set of system-supported file structures  with sets of special operations for manipulating files with those structures for instance  dec 's vms operating system has a file system that supports three defined file structures  this point brings us to one of the disadvantages of having the operating system support multiple file structures  the resulting size of the operating system is cumbersome if the operating system defines five different file structures  it needs to contain the code to support these file structures  in addition  it may be necessary to define every file as one of the file types supported by the operating system when new applications require information structured in ways not supported by the operating system  severe problems may result  for example  assume that a system supports two types of files  text files  composed of ascii characters separated by a carriage return and line feed  and executable binary files now  if we  as users  want to define an encrypted file to protect the contents from being read by unauthorized people  we may find neither file type to be appropriate the encrypted file is not ascii text lines but rather is  apparently  random bits although it may appear to be a binary file  it is not executable as a result  we may have to circumvent or misuse the operating system 's file-type mechanism or abandon our encryption scheme  some operating systems impose  and support  a minimal number of file structures this approach has been adopted in unix  ms-dos  and others un1x considers each file to be a sequence of 8-bit bytes ; no interpretation of these bits is made by the operating systen'l this scheme provides maximum flexibility but little support each application program must include its own code to interpret an input file as to the appropriate structure however  all operating systems must support at least one structure-that of an executable file-so that the system is able to load and run programs  the macintosh operating system also supports a minimal number of file structures it expects files to contain two parts  a and a 430 chapter 10 10.2 the resource fork contains information of interest to the user  for instance  it holds the labels of any buttons displayed by the program  a foreign user may want to re-label these buttons in his own language  and the macintosh operating system provides tools to allow modification of the data in the resource fork the data fork contains program code or data-the traditional file contents to accomplish the same task on a unix or ms-dos system  the programmer would need to change and recompile the source code  unless she created her own user-changeable data file clearly  it is useful for an operating system to support structures that will be used frequently and that will save the programmer substantial effort too few structures make programming inconvenient  whereas too many cause operating-system bloat and programmer confusion  10.1.5 internal file structure internally  locating an offset within a file can be complicated for the operating system disk systems typically have a well-defined block size determined by the size of a sector all disk i/0 is performed in units of one block  physical record   and all blocks are the same size it is unlikely that the physical record size will exactly match the length of the desired logical record logical records may even vary in length paddng a number of logical records into physical blocks is a common solution to this problem  for example  the unix operating system defines all files to be simply streams of bytes each byte is individually addressable by its offset from the begi1ming  or end  of the file in this case  the logical record size is 1 byte the file system automatically packs and unpacks bytes into physical disk blockssay  512 bytes per block-as necessary  the logical record size  physical block size  and packing technique determine how many logical records are in each physical block the packing can be done either by the user 's application program or by the operating system in either case  the file may be considered a sequence of blocks all the basic i/o functions operate in terms of blocks the conversion from logical records to physical blocks is a relatively simple software problem  because disk space is always allocated in blocks  some portion of the last block of each file is generally wasted if each block were 512 bytes  for example  then a file of 1,949 bytes would be allocated four blocks  2,048 bytes  ; the last 99 bytes would be wasted the waste incurred to keep everything in units of blocks  instead of bytes  is all file systems suffer from internal fragmentation ; the larger the block size  the greater the internal fragmentation  files store information when it is used  this information must be accessed and read into computer memory the information in the file can be accessed in several ways some systems provide only one access method for files other systems  such as those of ibm  support many access methods  and choosing the right one for a particular application is a major design problem  10.2 431 beginning current position end  ;        = = =  rewind ~ read or write ~ figure 10.3 sequential-access file  10.2.1 sequential access the simplest access method is  information in the file is processed in order  one record after the other this mode of access is by far the most common ; for example  editors and compilers usually access files in this fashion  reads and writes make up the bulk of the operations on a file a read operation-read next-reads the next portion of the file and automatically advances a file pointer  which tracks the i/o location similarly  the write operation-write next-appends to the end of the file and advances to the end of the newly written material  the new end of file   such a file can be reset to the beginning ; and on some systems  a program may be able to skip forward or backward n records for some integer n-perhaps only for n = 1 sequential access  which is depicted in figure 10.3  is based on a tape model of a file and works as well on sequential-access devices as it does on random-access ones  10.2.2 direct access  or a file is made up of fixedlength that allow programs to read and write records rapidly in no particular order the direct-access method is based on a disk model of a file  since disks allow random access to any file block for direct access  the file is viewed as a numbered sequence of blocks or records thus  we may read block 14  then read block 53  and then write block 7 there are no restrictions on the order of reading or writing for a direct-access file  direct-access files are of great use for immediate access to large amounts of information databases are often of this type when a query concerning a particular subject arrives  we compute which block contains the answer and then read that block directly to provide the desired information  as a simple example  on an airline-reservation system  we might store all the information about a particular flight  for example  flight 713  in the block identified by the flight number thus  the number of available seats for flight 713 is stored in block 713 of the reservation file to store il1formation about a larger set such as people  we might compute a hash function on the people 's names or search a small in-ncemory index to determine a block to read and search  for the direct-access method  the file operations must be modified to include the block number as a parameter thus  we have read n  where n is the block number  rather than read next  and write n rather than write next an alternative approach is to retain read next and write next  as with sequential 432 chapter 10 figure 10.4 simulation of sequential access on a direct-access file  access  and to add an operation position file to n  where n is the block number  then  to effect a read n  we would position to n and then read next  the block number by the user to the operating system is normally a a relative block number is an index relative to the begirm.ing of the file thus  the first relative block of the file is 0  the next is 1  and so on  even though the absolute disk address may be 14703 for the first block and 3192 for the second the use of relative block numbers allows the operating system to decide where the file should be placed  called the allocation problem  as discussed in chapter 11  and helps to prevent the user from accessing portions of the file system that may not be part of her file some systems start their relative block numbers at 0 ; others start at 1  how  then  does the system satisfy a request for record nina file assuming we have a logical record length l  the request for record n is turned into an i/0 request for l bytes starting at location l  n  within the file  assuming the first record is n = 0   since logical records are of a fixed size  it is also easy to read  write  or delete a record  not all operating systems support both sequential and direct access for files some systems allow only sequential file access ; others allow only direct access some systems require that a file be defined as sequential or direct when it is created ; such a file can be accessed only in a manner consistent with its declaration we can easily simulate sequential access on a direct-access file by simply keeping a variable cp that defines our current position  as shown in figure 10.4 simulating a direct-access file on a sequential-access file  however  is extremely inefficient and clumsy  10.2.3 other access methods other access methods can be built on top of a direct-access method these methods generally involve the construction of an index for the file the like an index in the back of a contains pointers to the various blocks to find a record in the file  we first search the index and then use the to access the file directly and to find the desired record  for example  a retail-price file might list the universal codes  upcs  items  with the associated prices each record consists a 10-digit upc and a 6-digit price  a 16-byte record if our disk has 1,024 bytes per we can store 64 records per block a file of 120,000 records would occupy about 2,000 blocks  2 million bytes   by keeping the file sorted by upc  we can define an index consisting of the first upc in each block this index would have entries of 10 digits each  or 20,000 bytes  and thus could be kept in memory to 10.3 10.3 433 logical record last name number adams arthur asher sm ! th,jol  ir ! social ~ security  age  / e  smith .' '  / index file relative file figure 10.5 example of irdex and relative files  find the price of a particular item  we can make a binary search of the index  from this search  we learn exactly which block contains the desired record and access that block this structure allows us to search a large file doing little i/0  with large files  the index file itself may become too large to be kept in memory one solution is to create an index for the index file the primary index file would contain pointers to secondary index files  which would point to the actual data items  for example  ibm 's indexed sequential-access method  isam  uses a small master index that points to disk blocks of a secondary index the secondary index blocks point to the actual file blocks the file is kept sorted on a defined key to find a particular item  we first make a binary search of the master index  which provides the block number of the secondary index this block is read in  and again a binary search is used to find the block containing the desired record finally  this block is searched sequentially in this way  any record can be located from its key by at most two direct-access reads figure 10.5 shows a similar situation as implemented by vms index and relative files  next  we consider how to store files certainly  no general-purpose computer stores just one file there are typically thousand  millions  and even billions of files within a computer files are stored on random-access storage devices  including hard disks  optical disks  and solid state  memory-based  disks  a storage device can be used in its entirety for a file system it can also be subdivided for finer-grained control for example  a disk can be into quarters  and each quarter can hold a file system storage devices can also be collected together into raid sets that provide protection from the failure of a single disk  as described in section 12.7   sometimes  disks are subdivided and also collected into raid sets  partitioning is useful for limiting the sizes of individual file systems  putting multiple file-system types on the same device  or leaving part of the device available for other uses  such as swap space or unformatted  rz ; c  .v  disk 434 chapter 10 directory  directory partition a files disk 2 1-7 ~ ~ ~ disk 1 directory partition c files partition b files disk 3 figure 10.6 a typical file-system organization  space partitions are also known as or  in the ibm world  a file system can be created on each of these parts of the disk any entity containing a file system is generally known as a the volume may be a subset of a device  a whole device  or multiple devices linked together into a raid set each volume can be thought of as a virtual disk volumes can also store multiple operating systems  allowing a system to boot and run more than one operating system  each volume that contains a file system must also contain information about the files in the system this information is kept in entries in a or ~ the device directory  more commonly known simply as that records information -such as name  location  size  and type-for all files on that volume figure 10.6 shows a typical file-system organization  10.3.1 storage structure as we have just seen  a general-purpose computer system has multiple storage devices  and those devices can be sliced up into volumes that hold file systems  computer systems may have zero or more file systems  and the file systems may be of varying types for example  a typical solaris system may have dozens of file systems of a dozen different types  as shown in the file system list in fig1-1re 10.7  in this book  we consider only general-purpose file systems it is worth noting  though  that there are many special-purpose file systems consider the types of file systems in the solaris example mentioned above  tmpfs-a temporary file system that is created in volatile main memory and has its contents erased if the system reboots or crashes objfs-a virtual file system  essentially an interface to the kernel that looks like a file system  that gives debuggers access to kernel symbols dfs-a virtual file system that maintains contract information to manage which processes start when the system boots and must continue to run during operation 10.3 435 i ufs /devices devfs /dev dev i system/ contract ctfs /proc proc /etc/mnttab mntfs i etc/ svc/volatile tmpfs i system/ object objfs /lib /libc.so.l lofs /dev/fd fd /var ufs /tmp tmpfs /var/run tmpfs /opt ufs /zpbge zfs i zpbge/backup zfs i export/home zfs /var/mail zfs /var/spool/inqueue zfs /zpbg zfs /zpbg/zones zfs figure 10.7 solaris file system  lofs-a loop back file system that allows one file system to be accessed in place of another one prods-a virtual file system that presents information on all processes as a file system ufs  zfs-general-purpose file systems the file systems of computers  then  can be extensive even within a file system  it is useful to segregate files into groups and manage and act on those groups this organization involves the use of directories in the remainder of this section  we explore the topic of directory structure  10.3.2 directory overview the directory can be viewed as a symbol table that translates file names into their directory entries if we take such a view  we see that the directory itself can be organized in many ways we want to be able to insert entries  to delete entries  to search for a named entry  and to list all the entries in the directory  in this section  we examine several schemes for defining the logical structure of the directory system  when considering a particular directory structure  we need to keep in mind the operations that are to be performed on a directory  search for a file we need to be able to search a directory structure to find the entry for a particular file since files have symbolic names  and similar 436 chapter 10 names may indicate a relationship between files  we may want to be able to find all files whose names match a particular pattern  create a file new files need to be created and added to the directory  delete a file when a file is no longer needed  we want to be able to remove it from the directory  list a directory we need to be able to list the files in a directory and the contents of the directory entry for each file in the list  rename a file because the name of a file represents its contents to its users  we must be able to change the name when the contents or use of the file changes renaming a file may also allow its position within the directory structure to be changed  traverse the file system we may wish to access every directory and every file within a directory structure for reliability  it is a good idea to save the contents and structure of the entire file system at regular intervals often  we do this by copyin.g all files to magn.etic tape this technique provides a backup copy in case of system failure in addition  if a file is no longer in use  the file can be copied to tape and the disk space of that file released for reuse by another file  in the following sections  we describe the most common schemes for defining the logical structure of a directory  10.3.3 single-level directory the simplest directory structure is the single-level directory all files are contained in the same directory  which is easy to support and understand  figure 10.8   a single-level directory has significant limitations  however  when the number of files increases or when the system has more than one user since all files are in the same directory  they must have unique names if two users call their data file test  then the unique-name rule is violated for example  in one programming class  23 students called the program for their second assignment prog2 ; another 11 called it assign2 although file names are generally selected to reflect the content of the file  they are often limited in length  complicating the task of making file names unique the ms-dos operating system allows only 11-character file names ; unix  in contrast  allows 255 characters  even a single user on a single-level directory may find it difficult to remember the names of all the files as the number of files increases it is not directory files figure 10.8 single-level directory  10.3 437 uncommon for a user to have hundreds of files on one computer system and an equal number of additional files on another system keeping track of so many files is a daunting task  10.3.4 two-level directory as we have seen  a single-level directory often leads to confusion of file names among different users the standard solution is to create a separate directory for each user  in the two-level directory structure  each user has his own the ufds have similar structures  but each lists only the files of a single user w11en a user job starts or a user logs in  the system 's is searched the mfd is indexed by user name or account number  and each entry points to the ufd for that user  figure 10.9   when a user refers to a particular file  only his own ufd is searched thus  different users may have files with the same name  as long as all the file names within each ufd are unique to create a file for a user  the operating system searches only that user 's ufd to ascertain whether another file of that name exists to delete a file  the operating system confines its search to the local ufd ; thus  it can not accidentally delete another user 's file that has the same name  the user directories themselves must be created and deleted as necessary  a special system program is run with the appropriate user name and account information the program creates a new ufd and adds an entry for it to the mfd  the execution of this program might be restricted to system administrators the allocation of disk space for user directories can be handled with the teduciques discussed in chapter 11 for files themselves  although the two-level directory structure solves the name-collision problem  it still has disadvantages this structure effectively isolates one user from another isolation is an advantage when the users are completely independent but is a disadvantage when the users want to cooperate on some task and to access one another 's files some systems simply do not allow local user files to be accessed by other users  if access is to be pennitted  one user must have the ability to name a file in another user 's directory to name a particular file lmiquely in a two-level directory  we must give both the user name and the file name a two-level directory can be thought of as a tree  or an inverted tree  of height 2 the root of the tree is the mfd its direct descendants are the ufds the descendants of user file directory figure i 0.9 two-level directory structure  438 chapter 10 the ufds are the files themselves the files are the leaves of the tree specifying a user name and a file name defines a path in the tree from the root  the mfd  to a leaf  the specified file   thus  a user name and a file name define a path name every file in the system has a path name to name a file uniquely  a user must know the path name of the file desired  for example  if user a wishes to access her own test file named test  she can simply refer to test to access the file named test of user b  with directory-entry name userb   however  she might have to refer to /userb/test every system has its own syntax for naming files in directories other than the user 's own  additional syntax is needed to specify the volume of a file for instance  in ms-dos a volume is specified by a letter followed by a colon thus  a file specification might be c  \ userb \ fest some systems go even further and separate the volume  directory name  and file name parts of the specification for instance  in vms  the file login.com might be specified as  u   sst.jdeck  login.com ; l  where u is the name of the volume  sst is the name of the directory  jdeck is the name of the subdirectory  and 1 is the version number other systems simply treat the volume name as part of the directory name the first name given is that of the volume  and the rest is the directory and file for instance  /u/pbg/test might specify volume u  directory pbg  and file test  a special case of this situation occurs with the system files programs provided as part of the system -loaders  assemblers  compilers  utility routines  libraries  and so on-are generally defined as files when the appropriate commands are given to the operating system  these files are read by the loader and executed many command interpreters simply treat such a command as the name of a file to load and execute as the directory system is defined presently  this file name would be searched for in the current ufd one solution would be to copy the system files into each ufd however  copying all the system files would waste an enormous amount of space  if the system files require 5 mb  then supporting 12 users would require 5 x 12 = = 60 mb just for copies of the system files  the standard solution is to complicate the search procedure slightly a special user directory is defined to contain the system files  for example  user 0   whenever a file name is given to be loaded  the operating system first searches the local ufd if the file is found  it is used if it is not found  the system automatically searches the special user directory that contains the system files  the sequence of directories searched when a file is named is called the  the search path can be extended to contain an unlimited list of directories to search when a command name is given this method is the one most used in unix and ms-dos systems can also be designed so that each user has his own search path  10.3.5 tree-structured directories once we have seen how to view a two-level directory as a two-level tree  the natural generalization is to extend the directory structure to a tree of arbitrary height  figure 10.10   this generalization allows users to create their own subdirectories and to organize their files accordingly a tree is the most common directory structure the tree has a root directory  and every file in the system has a unique path name  10.3 439 root ititi 0 0 figure i 0.10 tree-structured directory structure  a directory  or subdirectory  contains a set of files or subdirectories a directory is simply another file  but it is treated in a special way all directories have the same internal format one bit in each directory entry defines the entry as a file  0  or as a subdirectory  1   special system calls are used to create and delete directories  in normal use  each process has a current directory the should contain most of the files that are of current interest to the process  when reference is made to a file  the current directory is searched if a file is needed that is not in the current directory  then the user usually must either specify a path name or change the current directory to be the directory holding that file to change directories  a system call is provided that takes a directory name as a parameter and uses it to redefine the current directory thus  the user can change his current directory whenever he desires from one change directory system call to the next  all open system calls search the current directory for the specified file note that the search path may or may not contain a special entry that stands for the current directory  the initial current directory of the login shell of a user is designated when the user job starts or the user logs in the operating system searches the accounting file  or some other predefined location  to find an entry for this user  for accounting purposes   in the accounting file is a pointer to  or the name of  the user 's initial directory this pointer is copied to a local variable for this user that specifies the user 's initial current directory from that shell  other processes can be spawned the current directory of any subprocess is usually the current directory of the parent when it was spawned  path names can be of two types  absolute and relative an begins at the root and follows a down to the specified file  giving the directory names on the path a defi11es a path from the current directory for example  in the tree-structured file system of figure 10.10  440 chapter 10 if the current directory is root/spell/mail  then the relative path nan e prt/jirst refers to the same file as does the absolute path name root/spell/mail/prt/jirst  allowing a user to define her own subdirectories permits her to impose a structure on her files this structure might result in separate directories for files associated with different topics  for example  a subdirectory was created to hold the text of this book  or different forms of information  for example  the directory programs may contain source programs ; the directory bin may store all the binaries   an interesting policy decision in a tree-structured directory concerns how to handle the deletion of a directory if a directory is empty  its entry in the directory that contains it can simply be deleted however  suppose the directory to be deleted is not ernpty but contains several files or subdirectories one of two approaches can be taken some systems  such as ms-dos  will not delete a directory unless it is empty thus  to delete a directory  the user must first delete all the files in that directory if any subdirectories exist this procedure must be applied recursively to them  so that they can be deleted also this approach can result in a substantial amount of work an alternative approach  such as that taken by the unix rm command  is to provide an option  when a request is made to delete a directory  all that directory 's files and subdirectories are also to be deleted either approach is fairly easy to implement ; the choice is one of policy the latter policy is more convenient  but it is also more dangerous  because an entire directory structure can be removed with one command if that command is issued in error  a large number of files and directories will need to be restored  assuming a backup exists   with a tree-structured directory system  users can be allowed to access  in addition to their files  the files of other users for example  user b can access a file of user a by specifying its path names user b can specify either an absolute or a relative path name alternatively  user b can change her current directory to be user a 's directory and access the file by its file names  a path to a file in a tree-struch1red directory can be longer than a path in a two-level directory to allow users to access programs without having to remember these long paths  the macintosh operating system automates the search for executable programs one method it uses is to maintain a file  called the desktop file  containing the metadata code and the name and location of all executable programs it has seen when a new hard disk is added to the system  or the network is accessed  the operating system traverses the directory structure  searching for executable programs on the device and recording the pertinent information this mechanism supports the double-dick execution functionality described previously a double-dick on a file causes its creatorattribute data to be read and the desktop file to be searched for a match once the match is found  the appropriate executable program is started with the clicked-on file as its input  10.3.6 acyclic-graph directories consider two programmers who are working on a joint project the files associated with that project can be stored in a subdirectory  separating them from other projects and files of the two programmers but since both programmers are equally responsible for the project  both want the subdirectory to be in 10.3 directory and disk structure 441 figure 10.11 acyclic-graph directory structure  their own directories the common subdirectory should be shared a shared directory or file will exist in the file system in two  or more  places at once  a tree structure prohibits the sharing of files or directories an acyclic graph -that is  a graph with no cycles-allows directories to share subdirectories and files  figure 10.11   the same file or subdirectory may be in two different directories the acyclic graph is a natural generalization of the tree-structured directory scheme  it is important to note that a shared file  or directory  is not the same as two copies of the file with two copies  each programmer can view the copy rather than the original  but if one programmer changes the file  the changes will not appear in the other 's copy with a shared file  only one actual file exists  so any changes made by one person are immediately visible to the other sharing is particularly important for subdirectories ; a new file created by one person will automatically appear in all the shared subdirectories  when people are working as a team  all the files they want to share can be put into one directory the ufd of each team member will contain this directory of shared files as a subdirectory even in the case of a single user  the user 's file organization may require that some file be placed in different subdirectories  for example  a program written for a particular project should be both in the directory of all programs and in the directory for that project  shared files and subdirectories can be implemented in several ways a common way  exemplified by many of the unix systems  is to create a new directory entry called a link a link is effectively a pointer to another file or subdirectory for example  a link may be implemented as an absolute or a relative path name when a reference to a file is made  we search the directory if the directory entry is marked as a link  then the name of the real file is included in the link information we resolve the link by using that path name to locate the real file links are easily identified by their format in the directory entry  or by having a special type on systems that support types  and are effectively 442 chapter 10 indirect pointers the operating system ignores these links when traversing directory trees to preserve the acyclic structure of the system  another common approach to implementing shared files is simply to duplicate all information about them in both sharing directories thus  both entries are identical and equal consider the difference between this approach and the creation of a link the link is clearly different from the original directory entry ; thus  the two are not equal duplicate directory entries  however  make the original and the copy indistinguishable a major problem with duplicate directory entries is maintaining consistency when a file is modified  an acyclic-graph directory structure is more flexible than is a simple tree structure  but it is also more complex several problems must be considered carefully a file may now have multiple absolute path names consequently  distinct file names may refer to the same file this situation is similar to the aliasing problem for programming languages if we are trying to traverse the entire file system-to find a file  to accumulate statistics on all files  or to copy all files to backup storage-this problem becomes significant  since we do not want to traverse shared structures more than once  another problem involves deletion when can the space allocated to a shared file be deallocated and reused one possibility is to remove the file whenever anyone deletes it  but this action may leave dangling pointers to the now-nonexistent file worse  if the remaining file pointers contain actual disk addresses  and the space is subsequently reused for other files  these dangling pointers may point into the middle of other files  in a system where sharing is implemented by symbolic links  this situation is somewhat easier to handle the deletion of a link need not affect the original file ; only the link is removed if the file entry itself is deleted  the space for the file is deallocated  leaving the links dangling we can search for these links and remove them as well  but unless a list of the associated links is kept with each file  this search can be expensive alternatively  we can leave the links until an attempt is made to use them at that time  we can determine that the file of the name given by the link does not exist and can fail to resolve the link name ; the access is treated just as with any other illegal file name  in this case  the system designer should consider carefully what to do when a file is deleted and another file of the same name is created  before a symbolic link to the original file is used  in the case of unix  symbolic links are left when a file is deleted  and it is up to the user to realize that the orig  llcal file is gone or has been replaced microsoft windows  all flavors  uses the same approach  another approach to deletion is to preserve the file until all references to it are deleted to implement this approach  we must have some mechanism for determining that the last reference to the file has been deleted we could keep a list of all references to a file  directory entries or symbolic links   when a link or a copy of the directory entry is established  a new entry is added to the file-reference list when a link or directory entry is deleted  we remove its entry on the list the file is deleted when its file-reference list is empty  the trouble with this approach is the variable and potentially large size of the file-reference list however  we really do not need to keep the entire list -we need to keep only a count of the number of references adding a new link or directory entry increments the reference count ; deleting a link or entry decrements the count when the count is 0  the file can be deleted ; there are no remaining references to it the unix operating system uses this approach 10.3 443 for nonsymbolic links  or keeping a reference count in the file information block  or inode ; see appendix a.7.2   by effectively prohibiting multiple references to directories  we maintain an acyclic-graph structure  to avoid problems such as the ones just discussed  some systems do not allow shared directories or links for example  in ms-dos  the directory structure is a tree structure rather than an acyclic graph  10.3.7 general graph directory a serious problem with using an acyclic-graph structure is ensuring that there are no cycles if we start with a two-level directory and allow users to create subdirectories  a tree-structured directory results it should be fairly easy to see that simply adding new files and subdirectories to an existing tree-structured directory preserves the tree-structured nature howeve1 ~ when we add links  the tree structure is destroyed  resulting in a simple graph structure  figure 10.12   the primary advantage of an acyclic graph is the relative simplicity of the algorithms to traverse the graph and to determine when there are no more references to a file we want to avoid traversing shared sections of an acyclic graph twice  mainly for performance reasons if we have just searched a major shared subdirectory for a particular file without finding it  we want to avoid searching that subdirectory again ; the second search would be a waste of time  if cycles are allowed to exist in the directory  we likewise want to avoid searching any component twice  for reasons of correctness as well as performance a poorly designed algorithm might result in an infinite loop continually searching through the cycle and never terminating one solution is to limit arbitrarily the number of directories that will be accessed during a search  a similar problem exists when we are trying to determine when a file can be deleted with acyclic-graph directory structures  a value of 0 in the reference count means that there are no more references to the file or directory  figure 10.12 general graph directory  444 chapter 10 10.4 and the file can be deleted however  when cycles exist  the reference count may not be 0 even when it is no longer possible to refer to a directory or file  this anomaly results from the possibility of self-referencing  or a cycle  in the directory structure in this case  we generally need to use a garbage-collection scheme to determine when the last reference has been deleted and the disk space can be reallocated garbage collection involves traversing the entire file system  marking everything that can be accessed then  a second pass collects everything that is not marked onto a list of free space  a similar marking procedure can be used to ensure that a traversal or search will cover everything in the file system once and only once  garbage collection for a disk-based file system  however  is extremely time consuming and is thus seldom attempted  garbage collection is necessary only because of possible cycles in the graph  thus  an acyclic-graph structure is much easier to work with the difficulty is to avoid cycles as new links are added to the structure how do we know when a new lir1k will complete a cycle there are algorithms to detect cycles in graphs ; however  they are computationally expensive  especially when the graph is on disk storage a simpler algorithm in the special case of directories and links is to bypass links during directory traversal cycles are avoided  and no extra overhead is incurred  just as a file must be opened before it is used  a file system must be mounted before it can be available to processes on the system more specifically  the directory structure may be built out of multiple volumes  which must be mounted to make them available within the file-system name space  the mount procedure is straightforward the operating system is given the name of the device and the location within the file structure where the file system is to be attached some operating systems require that a file system type be provided  while others inspect the structures of the device and determine the type of file system typically  a mount point is an empty directory for instance  on a unix system  a file system containing a user 's home directories might be mounted as /home ; then  to access the directory structure within that file system  we could precede the directory names with /home  as in /home/jane motmting that file system under /users would result in the path name /users/jane  which we could use to reach the same directory  next  the operating system verifies that the device contains a valid file system it does so by asking the device driver to read the device directory and verifying that the directory has the expected format finally  the operating system notes in its directory structure that a file system is n1.ounted at the specified mount point this scheme enables the operating system to traverse its directory structure  switching among file systems  and even file systems of varying types  as appropriate  to illustrate file mounting  consider the file system depicted in figure 10.13  where the triangles represent subtrees of directories that are of interest  figure 10.13  a  shows an existing file system  while figure 10.13  b  shows an unmounted volume residing on /device/ds ! c at this point  only the files on the existing file system can be accessed figure 10.14 shows the effects of mounting 10.4 file-system mounting 445 bill  a   b  figure 10.13 file system  a  existing system  b  unmounted volume  the volume residing on /device/dsk over /users if the volume is unmounted  the file system is restored to the situation depicted in figure 10.13  systems impose semantics to clarify functionality for example  a system may disallow a mount over a directory that contains files ; or it may make the mounted file system available at that directory and obscure the directory 's existing files until the file system is unmounted  terminating the use of the file system and allowing access to the original files in that directory as another example  a system may allow the same file system to be mounted repeatedly  at different mount points ; or it may only allow one mount per file system  consider the actions of the classic macintosh operating system whenever the system encounters a disk for the first time  hard disks are found at boot time  and optical disks are seen when they are inserted into the drive   the macintosh operating system searches for a file system on the device if it finds one  it automatically mounts the file system at the root level  adding a folder icon on the screen labeled with the name of the file system  as stored in the i figure 10.14 mount point  446 chapter 10 10.5 device directory   the user is then able to click on the icon and thus display the newly mounted file system mac os x behaves much like bsd unix  on which it is based all file systems are mounted under the /volumes directory the mac os x gui hides this fact and shows the file systems as if they were all mounted at the root level  the microsoft windows family of operating systems  95  98  nt  small 2000  2003  xp  vista  maintains an extended two-level directory structure  with devices and volumes assigned drive letters volumes have a general graph directory structure associated with the drive letter the path to a specific file takes the form of drive-letter  \ path \ to \ file the more recent versions of windows allow a file system to be mounted anywhere in the directory tree  just as unix does windows operating systems automatically discover all devices and mount all located file systems at boot time in some systems  like unix  the mount commands are explicit a system configuration file contains a list of devices and mount points for automatic mounting at boot time  but other mounts may be executed manually  issues concerning file system mounting are further discussed in section 11.2.2 and in appendix a.7.5  in the previous sections  we explored the motivation for file sharing and some of the difficulties involved in allowing users to share files such file sharing is very desirable for users who want to collaborate and to reduce the effort required to achieve a computing goal therefore  user-oriented operating systems must accommodate the need to share files in spite of the inherent difficulties  in this section  we examine more aspects of file sharing we begin by discussing general issues that arise when multiple users share files once multiple users are allowed to share files  the challenge is to extend sharing to multiple file systems  including remote file systems ; we discuss that challenge as well finally  we consider what to do about conflicting actions occurring on shared files for instance  if multiple users are writing to a file  should all the writes be allowed to occurf or should the operating system protect the users ' actions from one another 10.5.1 multiple users when an operating system accommodates multiple users  the issues of file sharing  file naming  and file protection become preeminent given a directory structure that allows files to be shared by users  the system must mediate the file sharing the system can either allow a user to access the files of other users by default or require that a user specifically grant access to the files these are the issues of access control and protection  which are covered in section 10.6  to implement sharing and protection  the system must maintain more file and directory attributes than are needed on a single-user system although many approaches have been taken to meet this requirement  most systems have evolved to use the concepts of file  or directory  owner  or user  and group  the owner is the user who can change attributes and grant access and who has the most control over the file the group attribute defines a subset of users who 10.5 447 can share access to the file for example  the owner of a file on a unix system can issue all operations on a file  while members of the file 's group can execute one subset of those operations  and all other users can execute another subset of operations exactly which operations can be executed by group members and other users is definable by the file 's owner more details on permission attributes are included in the next section  the owner and group ids of a given file  or directory  are stored with the other file attributes when a user requests an operation on a file  the user id can be compared with the owner attribute to determine if the requesting user is the owner of the file likewise  the group ids can be compared the result indicates which permissions are applicable the system then applies those permissions to the requested operation and allows or denies it  many systems have multiple local file systems  including volumes of a single disk or multiple volumes on multiple attached disks in these cases  the id checking and permission matching are straightforward  once the file systems are mounted  10.5.2 remote file systems with the advent of networks  chapter 16   communication among remote computers became possible networking allows the sharing of resources spread across a campus or even around the world one obvious resource to share is data in the form of files  through the evolution of network and file technology  remote file-sharing methods have changed the first implemented method involves manually transferring files between machines via programs like ftp the second major method uses a  dfs  in which remote directories are visible from a local machine in some ways  the third method  the is a reversion to the first a browser is needed to gain access to the remote files  and separate operations  essentially a wrapper for ftp  are used to transfer files  ftp is used for both anonymous and authenticated access  allows a user to transfer files without having an account on the remote system the world wide web uses anonymous file exchange almost exclusively  dfs involves a much tighter integration between the machine that is accessing the remote files and the machine providing the files this integration adds complexity  which we describe in this section  10.5.2.1 the client-server model remote file systems allow a computer to mom1.t one or more file systems from one or more remote machines in this case  the machine containing the files is the server  and the machine seeking access to the files is the client the client-server relationship is common with networked machines generally  the server declares that a resource is available to clients and specifies exactly which resource  in this case  which files  and exactly which clients a server can serve multiple clients  and a client can use multiple servers  depending on the implementation details of a given client-server facility  the server usually specifies the available files on a volume or directory level client identification is more difficult a client can be specified network name or other identifier  such as an ip address  but these can be 448 chapter 10 or imitated as a result of spoofing  an unauthorized client could be allowed access to the server more secure solutions include secure authentication of the client via encrypted keys unfortunately  with security come many challenges  including ensuring compatibility of the client and server  they must use the same encryption algorithms  and security of key exchanges  intercepted keys could again allow unauthorized access   because of the difficulty of solving these problems  unsecure authentication methods are most commonly used  in the case of unix and its network file system  nfs   authentication takes place via the client networking information  by default in this scheme  the user 's ids on the client and server must match lf they do not  the server will be unable to determine access rights to files consider the example of a user who has an id of 1000 on the client and 2000 on the server a request from the client to the server for a specific file will not be handled appropriately  as the server will determine if user 1000 has access to the file rather than basing the determination on the real user id of 2000 access is thus granted or denied based on incorrect authentication information the server must trust the client to present the correct user id note that the nfs protocols allow many-to-many relationships that is  many servers can provide files to many clients in fact a given machine can be both a server to some nfs clients and a client of other nfs servers  once the remote file system is mounted  file operation requests are sent on behalf of the user across the network to the server via the dfs protocol  typically  a file-open request is sent along with the id of the requesting user  the server then applies the standard access checks to determine if the user has credentials to access the file in the mode requested the request is either allowed or denied if it is allowed  a file handle is returned to the client application  and the application then can perform read  write  and other operations on the file the client closes the file when access is completed the operating system may apply semantics similar to those for a local file-system mount or may use different semantics  10.5.2.2 distributed information systems to make client-server systems easier to manage  also known as provide unified access to the information needed for remote computing the provides host-name-to-network-address translations for the entire internet  including the world wide web   before dns became widespread  files containing the same information were sent via e-mail or ftp between all networked hosts this methodology was not scalable dns is further discussed in section 16.5.1  other distributed information systems provide user name/password/user id/group id space for a distributed facility unix systems have employed a wide variety of distributed-information methods sun microsystems introduced yellow pages  since renamed or and most of the industry adopted its use it centralizes storage of user names  host names  printer information  and the like unfortunately  it uses unsecure authentication methods  including sending user passwords unencrypted  in clear text  and identifying hosts by ip address sun 's nis + is a much more secure replacement for nis but is also much more complicated and has not been widely adopted  10.5 449 network information is used in conjunction with user authentication  user name and password  to create a that the server uses to decide whether to allow or deny access to a requested file system for this authentication to be valid  the user names m.u.st match from machine to machine  as with nfs   microsoft uses two distributed naming structures to provide a single name space for users the older naming technology is the newer technology  available in windows xp and windows 2000  is once established  the distributed naming facility is used by all clients servers to authenticate users  the industry is moving toward use of the as a secure distributed naming mechanism in fact  active is based on ldap sun microsystems includes ldap with the operating system and allows it to be employed for user authentication as well as system-wide retrieval of information  such as availability of printers  conceivably  one distributed ldap directory could be used by an organization to store all user and resource information for all the organization 's computers  the result would be for users  who would enter their authentication information once for access to all computers within the organization it would also ease system-administration efforts by combining  in one location  information that is currently scattered in various files on each system or in different distributed information services  10.5.2.3 failure modes local file systems can fail for a variety of reasons  including failure of the disk containing the file system  corruption of the directory structure or other disk-management information  collectively called disk-controller failure  cable failure  and host-adapter failure user or system-administrator failure can also cause files to be lost or entire directories or volumes to be deleted many of these failures will cause a host to crash and an error condition to be displayed  and human intervention will be required to repair the damage  remote file systems have even more failure modes because of the complexity of network systems and the required interactions between remote machines  many more problems can interfere with the proper operation of remote file systems in the case of networks  the network can be interrupted between two hosts such interruptions can result from hardware failure  poor hardware configuration  or networking implementation issues although some networks have built-in resiliency  including multiple paths between hosts  many do not any single failure can thus interrupt the flow of dfs commands  consider a client in the midst of using a remote file system it has files open from the remote host ; among other activities  it may be performing directory lookups to open files  reading or writing data to files  and closing files now consider a partitioning of the network  a crash of the server  or even a scheduled shutdown of the server suddenly  the remote file system is no longer reachable  this scenario is rather common  so it would not be appropriate for the client system to act as it would if a local file system were lost rather  the system can either terminate all operations to the lost server or delay operations until the server is again reachable these failure semantics are defined and in plemented as part of the remote-file-system protocol termination of all operations can 450 chapter 10 result in users ' losing data-and patience thus  most dfs protocols either enforce or allow delaying of file-system operations to rencote hosts  with the hope that the remote host will become available again  to implement this kind of recovery from failure  some kind of may be maintained on both the client and the server if both server and client maintain knowledge of their current activities and open files  then they can seamlessly recover from a failure in the situation where the server crashes but must recognize that it has remotely rnounted exported file systems and opened files  nfs takes a simple approach  implementing a dfs  in essence  it assumes that a client request for a file read or write would not have occurred unless the file system had been remotely mounted and the file had been previously open the nfs protocol carries all the information needed to locate the appropriate file and perform the requested operation similarly  it does not track which clients have the exported volumes mounted  again assuming that if a request comes in  it must be legitimate while this stateless approach makes nfs resilient and rather easy to implement  it also makes it unsecure for example  forged read or write requests could be allowed by an nfs server even though the requisite mount request and permission check had not taken place these issues are addressed in the industry standard nfs version 4  in which nfs is made stateful to improve its security  performance  and functionality  10.5.3 consistency semantics represent an important criterion for evaluating any file system that supports file sharing these semantics specify how multiple users of a system are to access a shared file simultaneously in particular  they specify when modifications of data by one user will be observable by other users these semantics are typically implemented as code with the file system  consistency semantics are directly related to the process-synchronization algorithms of chapter 6 however  the complex algorithms of that chapter tend not to be implemented in the case of file i/0 because of the great latencies and slow transfer rates of disks and networks for example  performing an atomic transaction to a remote disk could involve several network communications  several disk reads and writes  or both systems that attempt such a full set of functionalities tend to perform poorly a successful implementation of complex sharing semantics can be found in the andrew file system  for the following discussion  we assume that a series of file accesses  that is  reads and writes  attempted by a user to the same file is always enclosed between the open   and close   operations the series of accesses between the open   and close   operations makes up a to illustrate the concept  we sketch several prominent examples of consistency semantics  10.5.3.1 unix semantics the unix file system  chapter 17  uses the following consistency semantics  writes to an open file by a user are visible immediately to other users who have this file open  one mode of sharing allows users to share the pointer of current location into the file thus  the advancing of the pointer by one user affects all 10.6 10.6 451 sharing users here  a file has a single image that interleaves all accesses  regardless of their origin  in the unix semantics  a file is associated with a single physical image that is accessed as an exclusive resource contention for this single image causes delays in user processes  10.5.3.2 session semantics the andrew file system  afs   chapter 17  uses the following consistency semantics  writes to an open file by a user are not visible immediately to other users that have the same file open  once a file is closed  the changes made to it are visible only in sessions starting later already open instances of the file do not reflect these changes  according to these semantics  a file may be associated temporarily with several  possibly different  images at the same time consequently  multiple users are allowed to perform both read and write accesses concurrently on their images of the file  without delay almost no constraints are enforced on scheduling accesses  10.5.3.3 immutable-shared-files semantics a unique approach is that of once a file is declared as shared by its creator  it cam1ot be modified an immutable ile has two key properties  its name may not be reused  and its contents may not be altered  thus  the name of an immutable file signifies that the contents of the file are fixed the implementation of these semantics in a distributed system  chapter 17  is simple  because the sharing is disciplined  read-only   when information is stored in a computer system  we want to keep it safe from physical damage  the issue of reliability  and improper access  the issue of protection   reliability is generally provided by duplicate copies of files many computers have systems programs that automatically  or through computer-operator intervention  copy disk files to tape at regular intervals  once per day or week or month  to maintain a copy should a file system be accidentally destroyed  file systems can be damaged by hardware problems  such as errors in reading or writing   power surges or failures  head crashes  dirt  temperature extremes  and vandalism files may be deleted accidentally bugs in the file-system software can also cause file contents to be lost reliability is covered in more detail in chapter 12  protection can be provided in many ways for a small single-user system  we might provide protection by physically removing the floppy disks and locking them in a desk drawer or file cabinet in a multiuser system  however  other mechanisms are needed  452 chapter 10 10.6.1 types of access the need to protect files is a direct result of the ability to access files systems that do not permit access to the files of other users do not need protection thus  we could provide complete protection by prohibiting access alternatively  we could provide free access with no protection both approaches are too extreme for general use what is needed is protection mechanisms provide controlled access by limitin.g the types of file access that can be made access is permitted or denied depending on several factors  one of which is the type of access requested several different types of operations may be controlled  read read from the file  write write or rewrite the file  execute load the file into memory and execute it  append write new information at the end of the file  delete delete the file and free its space for possible reuse  list list the name and attributes of the file  other operations  such as renaming  copying  and editing the file  may also be controlled for many systems  however  these higher-level fm1ctions may be implemented by a system program that makes lower-level system calls  protection is provided at only the lower level for instance  copying a file may be implemented simply by a sequence of read requests in this case  a user with read access can also cause the file to be copied  printed  and so on  many protection mechanisms have been proposed each has advantages and disadvantages and must be appropriate for its intended application a small computer system that is used by only a few members of a research group  for example  may not need the same types of protection as a large corporate computer that is used for research  finance  and personnel operations we discuss some approaches to protection in the following sections and present a more complete treatment in chapter 14  10.6.2 access control the most common approach to the protection problem is to make access dependent on the identity of the user different users may need different types of access to a file or directory the most general scheme to implement dependent access is to associate with each file and directory an  acju specifying user names and the types of access allowed for each user  when a user requests access to a particular file  the operating system checks the access list associated with that file if that user is listed for the requested access  the access is allowed otherwise  a protection violation occurs  and the user job is denied access to the file  this approach has the advantage of enabling complex access methodologies  the main problem with access lists is their length if we want to allow everyone to read a file  we must list all users with read access this technique has two undesirable consequences  10.6 453 constructing such a list may be a tedious and unrewarding task  especially if we do not know in advance the list of users in the system  the directory entry  previously of fixed size  now must be of variable size  resulting in more complicated space management  these problems can be resolved by use of a condensed version of the access list  to condense the length of the access-control list  many systems recognize three classifications of users in connection with each file  owner the user who created the file is the owner  group a set of users who are sharing the file and need similar access is a group  or work group  universe all other users in the system constitute the universe  the most common recent approach is to combine access-control lists with the more general  and easier to implement  owner  group  and universe accesscontrol scheme just described for example  solaris 2.6 and beyond use the three categories of access by default but allow access-control lists to be added to specific files and directories when more fine-grained access control is desired  to illustrate  consider a person  sara  who is writing a new book she has hired three graduate students  jim  dawn  and jill  to help with the project  the text of the book is kept in a file named book the protection associated with this file is as follows  sara should be able to invoke all operations on the file  jim  dawn  and jill should be able only to read and write the file ; they should not be allowed to delete the file  all other users should be able to read  but not write  the file  sara is interested in letting as many people as possible read the text so that she can obtain feedback  to achieve such protection  we must create a new group-say  textwith members jim  dawn  and jill the name of the group  text  must then be associated with the file book  and the access rights must be set in accordance with the policy we have outlined  now consider a visitor to whom sara would like to grant temporary access to chapter 1 the visitor can not be added to the text group because that would give him access to all chapters because a file can only be in one group  sara can not add another group to chapter 1 \ nith the addition of access-control-list functionality  though  the visitor can be added to the access control list of chapter 1  for this scheme to work properly  permissions and access lists must be controlled tightly this control can be accomplished in several ways for example  in the unix system  groups can be created and modified only by the manager of the facility  or by any superuser   thus  control is achieved through human interaction in the vms system  the owner of the file can create 454 chapter 10 and modify the access-control list access lists are discussed further in section 14.5.2  with the more limited protection classification  only three fields are needed to define protection often  each field is a collection of bits  and each bit either allows or prevents the access associated with it for example  the unix system defines three fields of 3 bits each -rwx  where r controls read access  w controls write access  and x controls execution a separate field is kept for the file owner  for the file 's group  and for all other users in this scheme  9 bits per file are needed to record protection information thus  for our example  the protection fields for the file book are as follows  for the owner sara  all bits are set ; for the group text  the rand w bits are set ; and for the universe  only the r bit is set  one difficulty in combining approaches comes in the user interface users must be able to tell when the optional acl permissions are set on a file in the solaris example  a + appends the regular permissions  as in  f1l s'/stetvl  ji users  pbg-la.ptof \ users  permissions for gue  ; t full contml h-1odi ~ ,. f ; _e a.d g execute r.ead 'vi/rite spec  ia.l permissions a.llo w for specia.l permissions orfor advanced settings  click .a.dva.nced  .a.dva.nced figure 10.15 windows xp access-control list management  10.6 455 19 -rw-r--r + 1 jim staff 130 may 25 22  13 file1 a separate set of commands  setfacl and getfacl  is used to manage the acls  windows xp users typically manage access-control lists via the cui figure 10.15 shows a file-permission window on windows xp 's ntfs file system in this example  user guest is specifically denied access to the file lo.tex  another difficulty is assigning precedence when permission and acls conflict for example  if joe is in a file 's group  which has read permission  but the file has an acl granting joe read and write permission  should a write by joe be granted or denied solaris gives acls precedence  as they are more fine-grained and are not assigned by default   this follows the general rule that specificity should have priority  10.6.3 other protection approaches another approach to the protection problem is to associate a password with each file just as access to the computer system is often controlled by a password  access to each file can be controlled in the same way if the passwords are chosen randomly and changed often  this scheme may be effective in limiting access to a file the use of passwords has a few disadvantages  however first  the number of passwords that a user needs to remember may permissions in a unix system in the unix system  directory protection and file protection are handled similarly associated with each subdirectory are three fields-owner  group  and universe-each consisting of the three bits rwx thus  a user can list the content of a subdirectory only if the r bit is set in the appropriate field  similarly  a user can change his current directory to another current directory  say  faa  only if the x bit associated with the faa subdirectory is set in the appropriate field  a sample directory listing from a unix environment is shown in figure 10.16 the first field describes the protecti.on of the file or directory ad as the first character indicates a s11bdirectory also shown are the number of links to the file  the owner 's name  the group 's name  the size of the file in bytes  the date of last modification  and finally the file 's name  with optional extension   -rw-rw-r l pbg staff 31200 sep 30l  uo intro.ps drwx 5 pbg staff 512 jul 8 09.33 private/ drwxrwxr-x 2 pbg staff 512 jul8 09  35 doc/ drwxrwx 2 pbg student 512 aug 3 14  13 student-proj/ -rw-r--r 1 pbg staff 9423 feb 24 2003 program.c -rwxr-xr-x l pbg staff 20471 feb 24 2003 program drwx ~ -x--x 4 pbg faculty 512 jul 31 10  31 lib/ drwx 3 pbg staff 1024 aug 29 06  52 mail/ drwxrwxrwx 3 pbg staff 512 jul 8 09  35 test/ figure 10.16 a sample directory listing  456 chapter 10 10.7 become large  making the scheme impractical second  if only one password is used for all the files  then once it is discovered  all files are accessible ; protection is on an all-or-none basis some systems  for example  tops-20  allow a user to associate a password with a subdirectory  rather than with an individual file  to deal with this problem the ibmvm/cms operating system allows three passwords for a minidisk-one each for read  write  and nrultiwrite access  some single-user operating systencs-such as ms-dos and versions of the macintosh operating system prior to mac os x-provide little in terms of file protection in scenarios where these older systems are now being placed on networks file sharing and communication  protection mechanisms must be into them designing a feature for a new operating system is almost always easier than adding a feature to an existing one such updates are usually less effective and are not seamless  in a multilevel directory structure  we need to protect not only individual files but also collections of files in subdirectories ; that is  we need to provide a mechanism for directory protection the directory operations that must be protected are somewhat different from the file operations we want to control the creation and deletion of files in a directory in addition  we probably want to control whether a user can determine the existence of a file in a directory  sometimes  knowledge of the existence and name of a file is significant in itself  thus  listing the contents of a directory must be a protected operation similarly  if a path name refers to a file in a directory  the user must be allowed access to both the directory and the file in systems where files may have numerous path names  such as acyclic or general graphs   a given user may have different access rights to a particular file  depending on the path name used  a file is an abstract data type defined and implemented by the operating system it is a sequence of logical records a logical record may be a byte  a line  of fixed or variable length   or a more complex data item the operating system may specifically support various record types or may leave that support to the application program  the major task for the operating system is to map the logical file concept onto physical storage devices such as magnetic tape or disk since the physical record size of the device may not be the same as the logical record size  it may be necessary to order logical records into physical records again  this task may be supported by the operating system or left for the application program  each device in a file system keeps a volume table of contents or a device directory listing the location of the files on the device in addition  it is useful to create directories to allow files to be organized a single-level directory in a multiuser system causes naming problems  since each must have a unique name a two-level directory solves this creating a separate directory for each users files the directory lists name and includes the file 's location on the disk  length  type  owner  time creation  time of last use  and so on  the natural generalization of a two-level directory is a tree-structured directory a tree-structured directory allows a user to create subdirectories to organize files acyclic-graph directory structures enable users to share 457 subdirectories and files but complicate searching and deletion a general graph structure allows complete flexibility in the sharing of files and directories but sometimes requires garbage collection to recover unused disk space  disks are segmented into one or more volumes/ each containing a file system or left raw file systems may be mounted into the system 's naming structures to make them available the naming scheme varies by operating system once mounted  the files within the volume are available for use file systems may be unmounted to disable access or for maintenance  file sharing depends on the semantics provided by the system files may have multiple readers  multiple writers  or limits on sharing distributed file systems allow client hosts to mount volumes or directories from servers  as long as they can access each other across a network remote file systems present challenges in reliability  performance  and security distributed information systems maintain user/ host/ and access information so that clients and servers can share state information to ncanage use and access  since files are the main information-storage mechanism in most computer systems  file protection is needed access to files can be controlled separately for each type of access-read  write  execute  append  delete  list directory  and so on file protection can be provided by access lists  passwords  or other techniques  10.1 some systems provide file sharing by maintaining a single copy of a file ; other systems maintain several copies  one for each of the users sharing the file discuss the relative merits of each approach  10.2 some systems automatically open a file when it is referenced for the first time and close the file when the job terminates discuss the advantages and disadvantages of this scheme compared with the more traditional one  where the user has to open and close the file explicitly  10.3 in some systems  a subdirectory can be read and written by an authorized user  just as ordinary files can be  a describe the protection problems that could arise  b suggest a scheme for dealing with each of these protection problems  10.4 do some systems keep track of the type of a file  while others leave it to the user and others simply do not implement multiple file types which system is better 10.5 consider a system that supports 5,000 users suppose that you want to allow 4,990 of these users to be able to access one file  a howwould specify this protection scheme in unix b can you suggest another protection scheme that can be used more effectively for this purpose than the scheme provided by unix 458 chapter 10 10.6 what are the advantages and disadvantages of providing ncandatory locks instead of advisory locks whose usage is left to users ' discretion 10.7 explain the purpose of the open   and close   operations  10.8 the open-file table is used to maintain information about files that are currently open should the operating system maintain a separate table for each user or just maintain one table that contains references to files that are currently being accessed by all users if the same file is being accessed by two different programs or users  should there be separate entries in the open-file table 10.9 give an example of an application that could benefit from operatingsystem support for random access to indexed files  10.10 discuss the advantages and disadvantages of associating with remote file systems  stored on file servers  a set of failure semantics different from that associated with local file systems  10.11 could you simulate a multilevel directory structure with a single-level directory structure in which arbitrarily long names can be used if your answer is yes  explain how you can do so  and contrast this scheme with the multilevel directory scheme if your answer is no  explain what prevents your simulation 's success how would your answer change if file names were limited to seven characters 10.12 what are the implications of supporting unix consistency semantics for shared access for files stored on remote file systems 10.13 if the operating system knew that a certain application was going to access file data in a sequential manner  how could it exploit this information to improve performance 10.14 consider a file system in which a file can be deleted and its disk space reclaimed while links to that file still exist what problems may occur if a new file is created in the same storage area or with the same absolute path name how can these problems be avoided 10.15 discuss the advantages and disadvantages of supporting links to files that cross mount points  that is  the file link refers to a file that is stored in a different volume   10.16 what are the advantages and disadvantages of recording the name of the creating program with the file 's attributes  as is done in the macintosh operating system  general discussions concerning file systems are offered by grosshans  1986   golden and pechura  1986  describe the structure of microcomputer file systems database systems and their file structures are described in full in silberschatz et al  2001   a multilevel directory structure was first implemented on the multics system  organick  1972    most operating systems now implement multilevel 459 directory structures these include linux  bovet and cesati  2002    mac os x  http  / /www.apple.com/macosx/   solaris  mcdougall and mauro  2007    and all versions of windows  russinovich and solomon  2005    the network file system  nfs   designed by sun microsystems  allows directory structures to be spread across networked computer systems nfs is fully described in chapter 17 nfs version 4 is described in rfc3505  http  / /www.ietf.org/rfc/rfc3530.txt   general discussion of solaris file systems is found in the sun system administration guide  devices and file systems  http  / i docs sun com/ app i docs/ doc/817-5093   dns was first proposed by su  1982  and has gone through several revisions since  with mockapetris  1987  adding several major features eastlake  1999  has proposed security extensions to let dns hold security keys  ldap  also known as x.509  is a derivative subset of the x.soo distributed directory protocol it was defined by yeong et al  1995  and has been implemented on many operating systems  interesting research is ongoing in the area of file-system interfaces-in particular  on issues relating to file naming and attributes for example  the plan 9 operating system from bell laboratories  lucent technology  makes all objects look like file systems thus  to display a list of processes on a system  a user simply lists the contents of the /proc directory similarly  to display the time of day  a user need only type the file i dev i time  11.1 c as we saw in chapter 10  the file system provides the mechanism for on-line storage and access to file contents  including data and programs the file system resides permanently on secondary storage  which is designed to hold a large amount of data permanently this chapter is primarily concerned with issues surrounding file storage and access on the most common secondary-storage medium  the disk we explore ways to structure file use  to allocate disk space  to recover freed space  to track the locations of data  and to interface other parts of the operating system to secondary storage performance issues are considered throughout the chapter  to describe the details of implementing local file systems and directory structures  to describe the implementation of remote file systems  to discuss block allocation and free-block algorithms and trade-offs  disks provide the bulk of secondary storage on which a file system is maintained they have two characteristics that make them a convenient medium for storing multiple files  a disk can be rewritten in place ; it is possible to read a block from the disk  modify the block  and write it back into the sance place  a disk can access directly any block of information it contains thus  it is simple to access any file either sequentially or randomly  and switching from one file to another requires only moving the read-write heads and waiting for the disk to rotate  we discuss disk structure in great detail in chapter 12  461 462 chapter 11 to improve i/0 efficiency  i/0 transfers between memory and disk are performed in units of blocks each block has one or more sectors depending on the disk drive  sector size varies from 32 bytes to 4,096 bytes ; the usual size is 512 bytes  provide efficient and convenient access to the disk by allowing data to be stored  located  and retrieved easily a file system poses two quite different design problems the first problem is defining how the file system should look to the user this task involves defining a file and its attributes  the operations allowed on a file  and the directory structure for organizing files the second problem is creating algorithms and data structures to map the logical file system onto the physical secondary-storage devices  the file system itself is generally composed of many different levels the structure shown in figure 11.1 is an example of a layered design each level in the design uses the features of lower levels to create new features for use by higher levels  the lowest level  the i/o control  consists of and interrupt handlers to transfer information between the main memory and the disk system a device driver can be thought of as a translator its input consists of high-level commands such as retrieve block 123 its output consists of lowlevel  hardware-specific instructions that are used by the hardware controller  which interfaces the i/0 device to the rest of the system the device driver usually writes specific bit patterns to special locations in the i/0 controller 's memory to tell the controller which device location to act on and what actions to take the details of device drivers and the i/o infrastructure are covered in chapter 13  the needs only to issue generic commands to the appropriate device driver to read and write physical blocks on the disk each physical block is identified by its numeric disk address  for example  drive 1  cylilcder 73  track 2  sector 10   this layer also manages the memory buffers and caches that hold various file-system  directory  and data blocks a block application programs ~ logical file system ~ file-organization module ~ basic file system ~ 1/0 control devices figure 11.1 layered file system  11.1 463 in the buffer is allocated before the transfer of a disk block can occur when the buffer is full  the buffer m ~ anager must find more buffer ncemory or free up buffer space to allow a requested i/o to complete caches are used to hold frequently used file-system metadata to improve performance  so managing their contents is critical for optimum system performance  the knows about files and their logical blocks  as well as physical blocks by knowing the type of file allocation used and the location of the file  the file-organization module can translate logical block addresses to physical block addresses for the basic file system to transfer  each file 's logical blocks are numbered from 0  or 1  through n since the physical blocks containing the data usually do not match the logical numbers  a translation is needed to locate each block the file-organization module also includes the free-space manager  which tracks unallocated blocks and provides these blocks to the file-organization module when requested  finally  the f ! je manages metadata information metadata includes all of the file-system structure except the actual data  or contents of the files   the logical file system manages the directory structure to provide the fileorganization module with the information the latter needs  given a symbolic file name it maintains file structure via file-control blocks a flle-corttml  an in most unix file systems  contains information about the file  including ownership  permissions  and location of the file contents the logical file system is also responsible for protection and security  as discussed in chapters 10 and 14  when a layered structure is used for file-system implementation  duplication of code is minimized the i/o control and sometimes the basic file-system code can be used by multiple file systems each file system can then have its own logical file-system and file-organization modules unfortunately  layering can introduce more operating system overhead  which may result in decreased performance the use of layering  including the decision about how many layers to use and what each layer should do  is a major challenge in designing new systems  many file systems are in use today most operating systems support more than one for example  most cd-roms are written in the iso 9660 format  a standard format agreed on by cd-rom manufacturers in addition to removable-media file systems  each operating system has one or more diskbased file systems unix uses the fee which is based on the berkeley fast file system  ffs   windows nt  2000  and xp support disk file-system formats of fat  fat32  and ntfs  or windows nt file system   as well as cd-rom  dvd  and floppy-disk file-system formats although linux supports over forty different file systerns  the standard linux file system is known as the with the most common versions being ext2 and ext3 there are also distributed file systems in which a file system on a server is mounted by one or more client computers across a network  file-system research continues to be an active area of operating-system design and implementation coogle created its own file system to meet the company 's specific storage and retrieval needs another interesting project is the fuse file-system  which provides flexibility in file-system use by implementing and executing file systems as user-level rather than kernel-level code using fuse  a user can add a new file system to a variety of operating systems and can use that file system to manage her files  464 chapter 11 11.2 as was described in section 10.1.2  operating systems implement open   and close   systems calls for processes to request access to file contents  in this section  we delve into the structures and operations used to implement file-system operations  11.2.1 overview several on-disk and in-memory structures are used to implement a file system  these structures vary depending on the operating system and the file system  but some general principles apply  on disk  the file system may contain information about how to boot an operating system stored there  the total number of blocks  the number and location of free blocks  the directory structure  and individual files many of these structures are detailed throughout the remainder of this chapter ; here  we describe them briefly  a  per volume  can contain information needed by the system to boot an operating system from that volume if the disk does not contain an operating system  this block can be empty it is typically the first block of a volume in ufs  it is called the b,jsck ; in ntfs  it is the  per volume  contains volume  or partition  details  such as the number of blocks in the partition  the size of the blocks  a free-block count and free-block pointers  and a free-fcb count and fcb pointers in ufs  this is called a in ntfs  it is stored in the a directory structure  per file system  is used to organize the files in ufs  this includes file names and associated inode numbers in ntfs  it is stored in the master file table  a per-file fcb contains many details about the file it has a unique identifier number to allow association with a directory entry in ntfs  this information is actually stored within the master file table  which uses a relational database structure  with a row per file  the in-memory in.formation is used for both file-system management and performance improvement via caching the data are loaded at mount time  updated during file-system operations  and discarded at dismount several types of structures may be included  an in-memory volume  contains information about each mounted an in-memory directory-structure cache holds the directory information of recently accessed directories  for directories at which volumes are mounted  it can contain a pointer to the volume table  the contains a copy of the fcb of each open file  as well as other information  11.2 465 file dates  create  access  write  file owner group  acl file data blocks or pointers to file data blocks figure 11.2 a typical file-control block  the contains a pointer to the appropriate entry in the system-wide open-file table  as well as other information  buffers hold file-system blocks when they are being read from disk or written to disk  to create a new file  an application program calls the logical file system  the logical file system knows the format of the directory structures to create a new file  it allocates a new fcb  alternatively  if the file-system implementation creates all fcbs at file-system creation time  an fcb is allocated from the set of free fcbs  the system then reads the appropriate directory into memory  updates it with the new file name and fcb  and writes it back to the disk a typical fcb is shown in figure 11.2  some operating systems  including unix  treat a directory exactly the same as a file-one with a type field indicating that it is a directory other operating systems  includii g windows nt  implement separate system calls for files and directories and treat directories as entities separate from files whatever the larger structural issues  the logical file system can call the file-organization module to map the directory i/0 into disk-block numbers  which are passed on to the basic file system and i/o control system  now that a file has been created  it can be used for i/0 first  though  it must be opened the open   call passes a file name to the logical file system  the open   system call first searches the system-wide open-file table to see if the file is already in use by another process if it is  a per-process open-file table entry is created pointing to the existing system-wide open-file table this algorithm can save substantial overhead if the file is not already open  the directory structure is searched for the given file name parts of the directory structure are usually cached in memory to speed directory operations once the file is found  the fcb is copied into a system-wide open-file table in memory  this table not only stores the fcb but also tracks the number of processes that have the file open  next  an entry is made in the per-process open-file table  with a pointer to the entry in the system-wide open-file table and some other fields these other fields may include a pointer to the current location in the file  for the next read   or write   operation  and the access mode in which the file is open  the open   call returns a pointer to the appropriate entry in the per-process 466 chapter 11 user space user space kernel memory  a  kernel memory  b  ,  ---..,...  + -t-ilej d do secondary storage secondary storage figure 11.3 in-memory file-system structures  a  file open  b  file read  file-system table all file operations are then performed via this pointer the file name may not be part of the open-file table  as the system has no use for it once the appropriate fcb is located on disk it could be cached  though  to save time on subsequent opens of the same file the name given to the entry varies unix systems refer to it as a windows refers to it as a when a process closes the file  the per-process table entry is removed  and the system-wide entry 's open count is decremented when all users that have opened the file close it  any updated metadata is copied back to the disk-based directory structure  and the system-wide open-file table entry is removed  some systems complicate this scheme further by using the file system as an interface to other system aspects  such as networking for example  in ufs  the system-wide open-file table holds the inodes and other information for files and directories it also holds similar information for network connections and devices in this way  one mechanism can be used for multiple purposes  the caching aspects of file-system structures should not be overlooked  most systems keep all information about an open file  except for its actual data blocks  in memory the bsd unix system is typical in its use of caches wherever disk i/0 can be saved its average cache hit rate of 85 percent shows that these techniques are well worth implementing the bsd unix system is described fully in appendix a  the operating structures of a file-system implementation are summarized in figure 11.3  11.2 467 11.2.2 partitions and mounting the layout of a disk can have many variations  depending on the operating system a disk can be sliced into multiple partitions  or a volume can span multiple partitions on multiple disks the former layout is discussed here  while the latter  which is more appropriately considered a form of raid  is covered in section 12.7  each partition can be either raw  containing no file system  or cooked  containing a file system is used where no file system is appropriate  unix swap space can use a raw partition  for example  as it uses its own format on disk and does not use a file system likewise  some databases use raw disk and format the data to suit their needs raw disk can also hold information needed by disk raid systems  such as bit maps indicating which blocks are mirrored and which have changed and need to be mirrored similarly  raw disk can contain a miniature database holding raid configuration information  such as which disks are members of each raid set raw disk use is further discussed in section 12.5.1  boot information can be stored in a separate partition again  it has its own format  because at boot time the system does not have the file-system code loaded and therefore can not interpret the file-system format rather  boot information is usually a sequential series of blocks  loaded as an image into memory execution of the image starts at a predefined location  such as the first byte this in turn knows enough about the file-system structure to be able to find and load the kernel and start it executing it can contain more than the instructions for how to boot a specific operating system for instance  pcs and other systems can be multiple operating systems can be installed on such a system how does the system know which one to boot a boot loader that understands multiple file systems and multiple operating systems can occupy the boot space once loaded  it can boot one of the operating systems available on the disk the disk can have multiple partitions  each containing a different type of file system and a different operating system  the which contains the operating-system kernel and sometimes other system files  is mounted at boot time other volumes can be automatically mounted at boot or manually mounted later  depending on the operating system as part of a successful mount operation  the operating system verifies that the device contains a valid file system it does so by asking the device driver to read the device directory and verifying that the directory has the expected format if the format is invalid  the partition must have its consistency checked and possibly corrected  either with or without user intervention finally  the operating system notes in its in-memory mount table that a file system is mounted  along with the type of the file system the details of this function depend on the operating system microsoft windows-based systems mount each volume in a separate name space  denoted by a letter and a colon to record that a file system is mounted at f   for example  the operating system places a pointer to the file system in a field of the device structure corresponding to f   when a process specifies the driver letter  the operating system finds the appropriate file-system pointer and traverses the directory structures on that device to find the specified file or directory  later versions of windows can mount a file system at any point within the existing directory structure  468 chapter 11 on unix  file systems can be mounted at any directory mounting is implemented by setting a flag in the in-memory copy of the inode for that directory the flag indicates that the directory is a mount point a field then points to an entry in the mount table  indicating which device is mounted there  the mount table entry contains a pointer to the superblock of the file system on that device this scheme enables the operating system to traverse its directory structure  switching seamlessly among file systems of varying types  11.2.3 virtual file systems the previous section m.akes it clear that modern operating systems must concurrently support multiple types of file systems but how does an operating system allow multiple types of file systems to be integrated into a directory structure and how can users seamlessly move between file-system types as they navigate the file-system space we now discuss some of these implementation details  an obvious but suboptimal method of implementing multiple types of file systems is to write directory and file routines for each type instead  however  most operating systems  including unix  use object-oriented techniques to simplify  organize  and modularize the implementation the use of these methods allows very dissimilar file-system types to be implemented within the same structure  including network file systems  such as nfs users can access files that are contained within multiple file systems on the local disk or even on file systems available across the network  data structures and procedures are used to isolate the basic systemcall functionality from the implementation details thus  the file-system implementation consists of three major layers  as depicted schematically in figure 11.4 the first layer is the file-system interface  based on the open    read    write    and close   calls and on file descriptors  the second layer is called the layer the vfs layer serves two important functions  it separates file-system-generic operations from their implementation by defining a clean vfs interface several implementations for the vfs interface may coexist on the same machine  allowing transparent access to different types of file systems mounted locally  it provides a mechanism for uniquely representing a file throughout a network the vfs is based on a file-representation structure  called a that contains a numerical designator for a network-wide unique file  unix inodes are unique within only a single file system  this network-wide uniqueness is required for support of network file systems  the kernel maintains one vnode structure for each active node  file or directory   thus  the vfs distinguishes local files from remote ones  and local files are further distinguished according to their file-system types  the vfs activates file-system-specific operations to handle local requests according to their file-system types and calls the nfs protocol procedures for remote requests file handles are constructed from the relevant vnodes and are passed as arguments to these procedures the layer implementing the 11.2 469 network figure 11.4 schematic view of a virtual file system  file-system type or the remote-file-system protocol is the third layer of the architecture  let 's briefly examine the vfs architecture in linux the four main object types defined by the linux vfs are  the inode object  which represents an individual file the file object  which represents an open file the superblock object  which represents an entire file system the dentry object  which represents an individual directory entry for each of these four object types  the vfs defines a set of operations that must be implemented every object of one of these types contains a pointer to a f1.mction table the function table lists the addresses of the actual functions that implement the defined operations for that particular object for example  an abbreviated api for some of the operations for the file object include  int open      -open a file  ssize_t read      -read from a file  ssize_t write      -write to a file  int mmap    -memory-map a file  an implementation of the file object for a specific file type is required to implement each function specified in the definition of the file object  the complete definition ofthe file object is specified in the struct f ile_operat ions  which is located in the file /usr/include/linux/fs .h  470 chapter 11 11.3 thus  the vfs software layer can perform an operation on one of these objects by calling the appropriate function from the object 's function table  without having to know in advance exactly what kind of object it is dealing with the vfs does not know  or care  whether an inode represents a disk file  a directory file  or a remote file the appropriate function for that file 's read   operation will always be at the same place in its function table  and the vfs software layer will call that function without caring how the data are actually read  the selection of directory-allocation and directory-management algorithms significantly affects the efficiency  performance  and reliability of the file system in this section  we discuss the trade-offs involved in choosing one of these algorithms  11.3.1 linear list the simplest method of implementing a directory is to use a linear list of file names with pointers to the data blocks this method is simple to program but time-consuming to execute to create a new file  we must first search the directory to be sure that no existing file has the same name then  we add a new entry at the end of the directory to delete a file  we search the directory for the named file and then release the space allocated to it to reuse the directory entry  we can do one of several things we can mark the entry as unused  by assigning it a special name  such as an all-blank name  or with a used -unused bit in each entry   or we can attach it to a list of free directory entries a third alternative is to copy the last entry in the directory into the freed location and to decrease the length of the directory a linked list can also be used to decrease the time required to delete a file  the real disadvantage of a linear list of directory entries is that finding a file requires a linear search directory information is used frequently  and users will notice if access to it is slow in fact  many operating systems implement a software cache to store the most recently used directory information a cache hit avoids the need to constantly reread the information from disk a sorted list allows a binary search and decreases the average search time however  the requirement that the list be kept sorted may complicate creating and deleting files  since we may have to move substantial amounts of directory information to maintain a sorted directory a more sophisticated tree data structure  such as a b-h ee  might help here an advantage of the sorted list is that a sorted directory listing can be produced without a separate sort step  11.3.2 hash table another data structure used for a file directory is a with this method  a linear list stores the directory entries  but a hash data structure is also used the hash table takes a value computed from the file name and returns a pointer to the file name in the linear list therefore  it can greatly decrease the directory search time insertion and deletion are also fairly straightforward  although some provision must be made for collisions-situations in which two file names hash to the same location  11.4 11.4 471 the major difficulties with a hash table are its generally fixed size and the dependence of the hash function on that size for example  assume that we make a linear-probing hash table that holds 64 entries the hash function converts file names into integers from 0 to 63  for instance  by using the remainder of a division by 64 if we later try to create a 65th file  we must enlarge the directory hash table-say  to 128 entries as a result  we need a new hash function that must map file narnes to the range 0 to 127  and we must reorganize the existing directory entries to reflect their new hash-function values  alternatively  a chained-overflow hash table can be used each hash entry can be a linked list instead of an individual value  and we can resolve collisions by adding the new entry to the linked list lookups may be somewhat slowed  because searching for a name might require stepping through a linked list of colliding table entries still  this method is likely to be much faster than a linear search through the entire directory  the direct-access nature of disks allows us flexibility in the implementation of files in almost every case  many files are stored on the same disk the main problem is how to allocate space to these files so that disk space is utilized effectively and files can be accessed quickly three major methods of allocating disk space are in wide use  contiguous  linked  and indexed each method has advantages and disadvantages some systems  such as data general 's rdos for its nova line of computers  support all three more commonly  a system uses one method for all files within a file-system type  11.4.1 contiguous allocation requires that each file occupy a set of contiguous blocks on disk disk addresses define a linear ordering on the disk with this ordering  assuming that only one job is accessil1.g the disk  accessing block b + 1 after block b normally requires no head movement when head movement is needed  from the last sector of one cylil1.der to the first sector of the next cylinder   the head need only move from one track to the next thus  the number of disk seeks required for accessing contiguously allocated files is minimal  as is seek time when a seek is finally needed the ibm vm/cms operatil1.g system uses contiguous allocation because it provides such good performance  contiguous allocation of a file is defined by the disk address and length  in block units  of the first block if the file is n blocks long and starts at location b  then it occupies blocks b  b + 1  b + 2    b + n  1 the directory entry for each file indicates the address of the starting block and the length of the area allocated for this file  figure 11.5   accessing a file that has been allocated contiguously is easy for sequential access  the file system remembers the disk address of the last block referenced and  when necessary  reads the next block for direct access to block i of a file that starts at block b  we can immediately access block b + i thus  both sequential and direct access can be supported by contiguous allocation  472 chapter 11 directory file start length count 0 2 tr 14 3 mail 19 6 list 28 4 f 6 2 figure 1 i .5 contiguous allocation of disk space  contiguous allocation has some problems  however one difficulty is finding space for a new file the system chosen to manage free space determines how this task is accomplished ; these management systems are discussed in section 11.5 any management system can be used  but some are slower than others  the contiguous-allocation problem can be seen as a particular application of the general problem discussed in section 8.3  which involves to satisfy a request of size n from a list of free holes first fit and best fit are the most common strategies used to select a free hole from the set of available holes simulations have shown that both first fit and best fit are more efficient than worst fit in terms of both time and storage utilization  neither first fit nor best fit is clearly best in terms of storage utilization  but first fit is generally faster  all these algorithms suffer from the problem of as files are allocated and deleted  the free disk space is broken into pieces  external fragmentation exists whenever free space is broken into chunks it becomes a problem when the largest contiguous chunk is insufficient for a request ; storage is fragncented into a number of holes  none of which is large enough to store the data depending on the total amount of disk storage and the average file size  external fragmentation may be a minor or a major problem  one strategy for preventing loss of significant amounts of disk space to external fragmentation is to copy an entire file system onto another disk or tape the original disk is then freed completely  creating one large contiguous free space we then copy the files back onto the original disk by allocating contiguous space from this one large hole this scheme effectively all free space into one contiguous space  solving the fragmentation however  the cost of this compaction is time and it can be particularly severe for 