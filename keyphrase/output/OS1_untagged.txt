o my children  lemar  sivan  and aaron and my nicolette avi silberschatz to my wife  carla  and my children  gwen  owen  and maddie peter baer galvin to my wife  pat  and our sons  tom and jay greg gagne abraham silberschatz is the sidney j weinberg professor & chair of computer science at yale university prior to joining yale  he was the vice president of the information sciences research center at bell laboratories prior to that  he held a chaired professorship in the department of computer sciences at the university of texas at austin professor silberschatz is an acm fellow and an ieee fellow he received the 2002 ieee taylor l booth education award  the 1998 acm karl v karlstrom outstanding educator award  and the 1997 acm sigmod contribution award in recognition of his outstanding level of innovation and technical excellence  he was awarded the bell laboratories president 's award for three different projects-the qtm project  1998   the datablitz project  1999   and the netlnventory project  2004   professor silberschatz ' writings have appeared in numerous acm and ieee publications and other professional conferences and journals he is a coauthor of the textbook database system concepts he has also written op-ed articles for the new york times  the boston globe  and the hartford courant  among others peter baer galvin is the chief technologist for corporate technologies  www.cptech.com   a computer facility reseller and integrator before that  mr galvin was the systems manager for brown university 's computer science department he is also sun columnist for ; login  magazine mr galvin has written articles for byte and other magazines  and has written columns for sun world and sysadmin magazines as a consultant and trainer  he has given talks and taught tutorials on security and system administration worldwide greg gagne is chair of the computer science department at westminster college in salt lake city where he has been teaching since 1990 in addition to teaching operating systems  he also teaches computer networks  distributed systems  and software engineering he also provides workshops to computer science educators and industry professionals operating systems are an essential part of any computer system similarly  a course on operating systems is an essential part of any computer-science education this field is undergoing rapid change  as computers are now prevalent in virtually every application  from games for children through the most sophisticated planning tools for governments and multinational firms yet the fundamental concepts remain fairly clear  and it is on these that we base this book we wrote this book as a text for an introductory course in operating systems at the junior or senior undergraduate level or at the first-year graduate level we hope that practitioners will also find it useful it provides a clear description of the concepts that underlie operating systems as prerequisites  we assume that the reader is familiar with basic data struchues  computer organization  and a high-level language  such as c or java the hardware topics required for an understanding of operating systems are included in chapter 1 for code examples  we use predominantly c  with some java  but the reader can still understand the algorithms without a thorough knowledge of these languages concepts are presented using intuitive descriptions important theoretical results are covered  but formal proofs are omitted the bibliographical notes at the end of each chapter contain pointers to research papers in which results were first presented and proved  as well as references to material for further reading in place of proofs  figures and examples are used to suggest why we should expect the result in question to be true the fundamental concepts and algorithms covered in the book are often based on those used in existing conunercial operating systems our aim is to present these concepts and algorithms in a general setting that is not tied to one particular operating system we present a large number of examples that pertain to the most popular and the most im1.ovative operating systems  including sun microsystems ' solaris ; linux ; microsoft windows vista  windows 2000  and windows xp ; and apple mac os x when we refer to windows xp as an example operating system  we are implying windows vista  windows xp  and windows 2000 if a feature exists in a specific release  we state this explicitly vii viii the organization of this text reflects our many years of teaching courses on operating systems consideration was also given to the feedback provided by the reviewers of the text  as well as comments submitted by readers of earlier editions in addition  the content of the text corresponds to the suggestions from computing curricula 2005 for teaching operating systems  published by the joint task force of the ieee computing society and the association for computing machinery  acm   on the supporting web site for this text  we provide several sample syllabi that suggest various approaches for using the text in both introductory and advanced courses as a general rule  we encourage readers to progress sequentially through the chapters  as this strategy provides the most thorough study of operating systems however  by using the sample syllabi  a reader can select a different ordering of chapters  or subsections of chapters   on-line support for the text is provided by wileyplus on this site  students can find sample exercises and programming problems  and instructors can assign and grade problems in addition  in wileyplus  students can access new operating-system simulators  which are used to work through exercises and hands-on lab activities references to the simulators and associated activities appear at the ends of several chapters in the text the text is organized in nine major parts  overview chapters 1 and 2 explain what operating systems are  what they do  and how they are designed and constructed these chapters discuss what the common features of an operating system are  what an operating system does for the user  and what it does for the computer-system operator the presentation is motivational and explanatory in nature we have avoided a discussion of how things are done internally in these chapters therefore  they are suitable for individual readers or for students in lower-level classes who want to learn what an operating system is without getting into the details of the internal algorithms process management and process coordination chapters 3 through 7 describe the process concept and concurrency as the heart of modern operating systems a process is the unit of work in a system  such a system consists of a collection of concurrently executing processes  some of which are operating-system processes  those that execute system code  and the rest of which are user processes  those that execute user code   these chapters cover n1.ethods for process scheduling  interprocess communication  process synchronization  and deadlock handling also included is a discussion of threads  as well as an examination of issues related to multicore systems memory management chapters 8 and 9 deal with the management of main memory during the execution of a process to improve both the utilization of the cpu and the speed of its response to its users  the computer must keep several processes in memory there are many different ix management  and the effectiveness of a particular algorithm depends on the situation storage management chapters 10 through 13 describe how the file system  mass storage  and i/0 are handled in a modern computer system the file system provides the mechanism for on-line storage of and access to both data and programs we describe the classic internal algorithms and structures of storage management and provide a firm practical understanding of the algorithms used -their properties  advantages  and disadvantages our discussion of storage also includes matters related to secondary and tertiary storage since the i/0 devices that attach to a computer vary widely  the operating system needs to provide a wide range of functionality to applications to allow them to control all aspects of these devices we discuss system i/o in depth  including i/o system design  interfaces  and internal system structures and functions in many ways  i/o devices are the slowest major components of the computer because they represent a performance bottleneck  we also examine performance issues associated with i/0 devices protection and security chapters 14 and 15 discuss the mechanisms necessary for the protection and security of computer systems the processes in an operating system must be protected from one another 's activities  and to provide such protection  we must ensure that only processes that have gained proper authorization from the operating system can operate on the files  memory  cpu  and other resources of the system protection is a mechanism for controlling the access of programs  processes  or users to the resources defined by a computer system this mechanism must provide a means of specifying the controls to be imposed  as well as a means of enforcement security protects the integrity of the information stored in the system  both data and code   as well as the physical resources of the system  from 1.mauthorized access  malicious destruction or alteration  and accidental introduction of inconsistency distributed systems chapters 16 through 18 deal with a collection of processors that do not share memory or a clock-a distributed system by providing the user with access to the various resources that it maintains  a distributed system can improve computation speed and data availability and reliability such a system also provides the user with a distributed file system  which is a file-service system whose users  servers  and storage devices are dispersed among the sites of a distributed system a distributed system must provide various mechanisms for process synchronization and communication  as well as for dealing with deadlock problems and a variety of failures that are not encountered in a centralized system special-purpose systems chapters 19 and 20 deal with systems used for specific purposes  including real-time systems and multimedia systems these systems have specific requirements that differ from those of the general-purpose systems that are the focus of the remainder of the text real-time systems may require not only that computed results be correct but also that the results be produced within a specified deadline period multimedia systems require quality-of-service guarantees ensuring that the multimedia data are delivered to clients within a specific time frame x case studies chapters 21 through 23 in the book  and appendices a through c  which are available on www.wiley.comj go i global/ silberschatz and in wileyplus   integrate the concepts described in the earlier chapters by describing real operating systems these systems include linux  windows xp  freebsd  mach  and windows 2000 we chose linux and freebsd because unix-at one time-was almost small enough to understand yet was not a toy operating system most of its internal algorithms were selected for simplicity  rather than for speed or sophistication both linux and freebsd are readily available to computer-science departments  so many students have access to these systems we chose windows xp and windows 2000 because they provide an opporhmity for us to study a modern operating system with a design and implementation drastically different from those of unix chapter 23 briefly describes a few other influential operating systems this book uses examples of many real-world operating systems to illustrate fundamental operating-system concepts however  particular attention is paid to the microsoft family of operating systems  including windows vista  windows 2000  and windows xp  and various versions of unix  including solaris  bsd  and mac os x   we also provide a significant amount of coverage of the linux operating system reflecting the most recent version of the kernel -version 2.6-at the time this book was written the text also provides several example programs written in c and java these programs are intended to run in the following programming environments  windows systems the primary programming environment for windows systems is the win32 api  application programming interface   which provides a comprehensive set of functions for managing processes  threads  memory  and peripheral devices we provide several c programs illustrating the use of the win32 api example programs were tested on systems rum1.ing windows vista  windows 2000  and windows xp posix posix  which stands for portable operating system inte1jace  represents a set of standards implemented primarily for unix-based operating systems although windows vista  windows xp  and windows 2000 systems can also run certain posix programs  our coverage of posix focuses primarily on unix and linux systems posix-compliant systems must implement the posix core standard  posix.1   linux  solaris  and mac os x are examples of posix-compliant systems posix also defines several extensions to the standards  including real-time extensions  posixl.b  and an extension for a threads library  posix1.c  better known as pthreads   we provide several programn1.ing examples written inc illustrating the posix base api  as well as pthreads and the extensions for real-time programming these example programs were tested on debian linux 2.4 and 2.6 systems  mac os x 10.5  and solaris 10 using the gee 3.3 and 4.0 compilers java java is a widely used programming language with a rich api and built-in language support for thread creation and management java xi programs run on any operating system supporting a java virtual machine  or jvm   we illustrate various operating system and networking concepts with several java programs tested using the java 1.5 jvm we have chosen these three programming environments because it is our opinion that they best represent the two most popular models of operating systems  windows and unix/linux  along with the widely used java environment most programming examples are written in c  and we expect readers to be comfortable with this language ; readers familiar with both the c and java languages should easily understand most programs provided in this text in some instances-such as thread creation-we illustrate a specific concept using all three programming environments  allowing the reader to contrast the three different libraries as they address the same task in other situations  we may use just one of the apis to demonstrate a concept for example  we illustrate shared memory using just the posix api ; socket programming in tcp /ip is highlighted using the java api as we wrote the eighth edition of operating system concepts  we were guided by the many comments and suggestions we received from readers of our previous editions  as well as by our own observations about the rapidly changing fields of operating systems and networking we have rewritten material in most of the chapters by bringing older material up to date and removing material that was no longer of interest or relevance we have made substantive revisions and organizational changes in many of the chapters most importantly  we have added coverage of open-source operating systems in chapter 1 we have also added more practice exercises for students and included solutions in wileyplus  which also includes new simulators to provide demonstrations of operating-system operation below  we provide a brief outline of the major changes to the various chapters  chapter 1  introduction  has been expanded to include multicore cpus  clustered computers  and open-source operating systems chapter 2  system structures  provides significantly updated coverage of virtual machines  as well as multicore cpus  the grub boot loader  and operating-system debugging chapter 3  process concept  provides new coverage of pipes as a form of interprocess communication chapter 4  multithreaded programming  adds new coverage of programming for multicore systems chapter 5  process scheduling  adds coverage of virtual machine scheduling and multithreaded  multicore architectures chapter 6  synchronization  adds a discussion of mutual exclusion locks  priority inversion  and transactional memory chapter 8  memory-management strategies  includes discussion of numa xii chapter 9  virtual-memory management  updates the solaris example to include solaris 10 memory managernent chapter 10  file system  is updated with current technologies and capacities chapter 11  implementing file systems  includes a full description of sun 's zfs file system and expands the coverage of volumes and directories chapter 12  secondary-storage structure  adds coverage of iscsi  volumes  and zfs pools chapter 13  i/0 systems  adds coverage of pcix pci express  and hypertransport chapter 16  distributed operating systems  adds coverage of 802.11 wireless networks chapter 21  the limix system  has been updated to cover the latest version of the limix kernel chapter 23  influential operating systems  increases coverage of very early computers as well as tops-20  cp/m  ms-dos  windows  and the original mac os to emphasize the concepts presented in the text  we have added several programming problems and projects that use the posix and win32 apis  as well as java we have added more than 15 new programming problems  which emphasize processes  threads  shared memory  process synchronization  and networking in addition  we have added or modified several programming projects that are more involved than standard programming exercises these projects include adding a system call to the linux kernel  using pipes on both unix and windows systems  using unix message queues  creating multithreaded applications  and solving the producer-consumer problem using shared memory the eighth edition also incorporates a set of operating-system simulators designed by steven robbins of the university of texas at san antonio the simulators are intended to model the behavior of an operating system as it performs various tasks  such as cpu and disk-head schedulil1.g  process creation and interprocess communication  starvation  and address translation these simulators are written in java and will run on any computer systern with java 1.4 students can download the simulators from wileyplus and observe the behavior of several operating system concepts in various scenarios in addition  each simulator includes several exercises that ask students to set certain parameters of the simulator  observe how the system behaves  and then explain this behavior these exercises can be assigned through wileyplus the wileyplus course also includes algorithmic problems and tutorials developed by scott m pike of texas a&m university xiii the following teaching supplencents are available in wileyplus and on www.wiley.coml go i global/ silberschatz  a set of slides to accompany the book  model course syllabi  all c and java source code  up-to-date errata  three case study appendices and the distributed communication appendix the wileyplus course also contains the simulators and associated exercises  additional practice exercises  with solutions  not found in the text  and a testbank of additional problems students are encouraged to solve the practice exercises on their own and then use the provided solutions to check their own answers to obtain restricted supplements  such as the solution guide to the exercises in the text  contact your local j orne wiley & sons sales representative note that these supplements are available only to faculty who use this text we use the mailman system for communication among the users of operating system concepts if you wish to use this facility  please visit the following url and follow the instructions there to subscribe  http  i i mailman.cs.yale.edul mailmanllistinfo i os-book the mailman mailing-list system provides many benefits  such as an archive of postings  as well as several subscription options  including digest and web only to send messages to the list  send e-mail to  os-book @ cs.yale.edu depending on the message  we will either reply to you personally or forward the message to everyone on the mailing list the list is moderated  so you will receive no inappropriate mail students who are using this book as a text for class should not use the list to ask for answers to the exercises they will not be provided we have attempted to clean up every error in this new edition  but-as happens with operating systems-a few obscure bugs may remain we would appreciate hearing from you about any textual errors or omissions that you identify if you would like to suggest improvements or to contribute exercises  we would also be glad to hear from you please send correspondence to os-book-authors @ cs.yale.edu this book is derived from the previous editions  the first three of which were coauthored by james peterson others who helped us with previous editions include hamid arabnia  rida bazzi  randy bentson  david black  xiv joseph boykin  jeff brumfield  gael buckley  roy campbell  p c capon  john carpenter  gil carrick  thomas casavant  bart childs  ajoy kum.ar datta  joe deck  sudarshan k dhall  thomas doeppner  caleb drake  m racsit eskicioglu  hans flack  robert fowler  g scott graham  richard guy  max hailperin  rebecca i-iartncan  wayne hathaway  christopher haynes  don heller  bruce hillyer  mark holliday  dean hougen  michael huangs  ahmed kamet marty kewstet richard kieburtz  carol kroll  marty k westet thomas leblanc  john leggett  jerrold leichter  ted leung  gary lippman  carolyn miller  michael molloy  euripides montagne  yoichi muraoka  jim m ng  banu ozden  ed posnak  boris putanec  charles qualline  john quarterman  mike reiter  gustavo rodriguez-rivera  carolyn j c schauble  thomas p skimcer  yannis smaragdakis  jesse st laurent  john stankovic  adam stauffer  steven stepanek  john sterling  hal stern  louis stevens  pete thomas  david umbaugh  steve vinoski  tommy wagner  larry l wear  jolm werth  james m westall  j s weston  and yang xiang parts of chapter 12 were derived from a paper by hillyer and silberschatz  1996   parts of chapter 17 were derived from a paper by levy and silberschatz  1990   chapter 21 was derived from an unpublished manuscript by stephen tweedie chapter 22 was derived from an unpublished manuscript by dave probert  cliff martin  and avi silberschatz appendix c was derived from an unpublished manuscript by cliff martin cliff martin also helped with updating the unix appendix to cover freebsd some of the exercises and accompanying solutions were supplied by arvind krishnamurthy mike shapiro  bryan cantrill  and jim mauro answered several solarisrelated questions bryan cantrill from sun microsystems helped with the zfs coverage steve robbins of the university of texas at san antonio designed the set of simulators that we incorporate in wileyplus reece newman of westminster college initially explored this set of simulators and their appropriateness for this text josh dees and rob reynolds contributed coverage of microsoft 's .net the project for posix message queues was contributed by john trona of saint michael 's college in colchester  vermont marilyn turnamian helped generate figures and presentation slides mark wogahn has made sure that the software to produce the book  e.g  latex macros  fonts  works properly our associate publisher  dan sayre  provided expert guidance as we prepared this edition he was assisted by carolyn weisman  who managed many details of this project smoothly the senior production editor ken santor  was instrumental in handling all the production details lauren sapira and cindy jolmson have been very helpful with getting material ready and available for wileyplus beverly peavler copy-edited the manuscript the freelance proofreader was katrina avery ; the freelance indexer was word co  inc abraham silberschatz  new haven  ct  2008 peter baer galvin  burlington  ma 2008 greg gagne  salt lake city  ut  2008 part one overview chapter 1 introduction 1.1 what operating systems do 3 1.2 computer-system organization 6 1.3 computer-system architecture 12 1.4 operating-system sh ucture 18 1.5 operating-system operations 20 1.6 process management 23 1.7 memory management 24 1.8 storage management 25 chapter 2 system structures 2.1 operating-system services 49 2.2 user operating-system interface 52 2.3 system calls 55 2.4 types of system calls 58 2.5 system programs 66 2.6 operating-system design and implementation 68 2.7 operating-system structure 70 1.9 protection and security 29 1.10 distributed systems 30 1.11 special-purpose systems 32 1.12 computing environments 34 1.13 open-source operating systems 37 1.14 summary 40 exercises 42 bibliographical notes 46 2.8 virtual machines 76 2.9 operating-system debugging 84 2.10 operating-system generation 88 2.11 system boot 89 2.12 summary 90 exercises 91 bibliographical notes 97 part two process management chapter 3 process concept 3.1 process concept 101 3.2 process scheduling 105 3.3 operations on processes 110 3.4 interprocess communication 116 3.5 examples of ipc systems 123 3.6 communication in clientserver systems 128 3.7 summary 140 exercises 141 bibliographical notes 152 xv xvi chapter 4 multithreaded programming 4.1 overview 153 4.2 multithreading models 157 4.3 thread libraries 159 4.4 threading issues 165 chapter 5 process scheduling 5.1 basic concepts 183 5.2 scheduling criteria 187 5.3 scheduling algorithms 188 5.4 thread scheduling 199 5.5 multiple-processor scheduling 200 4.5 operating-system examples 171 4.6 summary 174 exercises 174 bibliographical notes 181 5.6 operating system examples 206 5.7 algorithm evaluation 213 5.8 summary 217 exercises 218 bibliographical notes 222 part three process coordination chapter 6 synchronization 6.1 backgrmmd 225 6.2 the critical-section problem 227 6.3 peterson 's solution 229 6.4 synchronization hardware 231 6.5 semaphores 234 6.6 classic problems of synchronization 239 chapter 7 deadlocks 7.1 system model 283 7.2 deadlock characterization 285 7.3 methods for handling deadlocks 290 7.4 deadlock prevention 291 7.5 deadlock avoidance 294 6.7 monitors 244 6.8 synchronization examples 252 6.9 atomic transactions 257 6.10 summary 267 exercises 267 bibliographical notes 280 7.6 deadlock detection 301 7.7 recovery from deadlock 304 7.8 summary 306 exercises 307 bibliographical notes 310 part four memory management chapter 8 memory-management strategies 8.1 background 315 8.2 swapping 322 8.3 contiguous memory allocation 324 8.4 paging 328 8.5 structure of the page table 337 8.6 segmentation 342 8.7 example  the intel pentium 345 8.8 summary 349 exercises 350 bibliographical notes 354 xvii chapter 9 virtual-memory management 9.1 background 357 9.2 demand paging 361 9.3 copy-on-write 367 9.4 page replacement 369 9.5 allocation of frames 382 9.6 thrashing 386 9.7 memory-mapped files 390 9.8 allocating kernel memory 396 9.9 other considerations 399 9.10 operating-system examples 405 9.11 summary 407 exercises 409 bibliographical notes 416 part five storage management chapter 10 file system 10.1 file concept 421 10.2 access methods 430 10.3 directory and disk structure 433 10.4 file-system mounting 444 10.5 file sharing 446 10.6 protection 451 10.7 summary 456 exercises 457 bibliographical notes 458 chapter 11 implementing file systems 11.1 file-system structure 461 11.2 file-system implementation 464 11.3 directory implementation 470 11.4 allocation methods 471 11.5 free-space management 479 11.6 efficiency and performance 482 11.7 recovery 486 11.8 nfs 490 11.9 example  the wafl file system 496 11.10 summary 498 exercises 499 bibliographical notes 502 chapter 12 secondary-storage structure 12.1 overview of mass-storage structure 505 12.2 disk structure 508 12.3 disk attachment 509 12.4 disk scheduling 510 12.5 disk man.agement 516 12.6 swap-space management 520 chapter 13 i/0 systems 13.1 overview 555 13.2 i/0 hardware 556 13.3 application i/0 interface 565 13.4 kernel i/0 subsystem 571 13.5 transforming i/0 requests to hardware operations 578 12.7 raid structure 522 12.8 stable-storage implementation 533 12.9 tertiary-storage struchue 534 12.10 summary 543 exercises 545 bibliographical notes 552 13.6 streams 580 13.7 performance 582 13.8 summary 585 exercises 586 bibliographical notes 588 xviii part six protection and security chapter 14 system protection 14.1 goals of protection 591 14.2 principles of protection 592 14.3 domain of protection 593 14.4 access matrix 598 14.5 implementation of access matrix 602 14.6 access control 605 chapter 15 system security 15.1 the security problem 621 15.2 program threats 625 15.3 system and network threats 633 15.4 cryptography as a security tool 638 15.5 user authentication 649 15.6 implementing security defenses 654 15.7 firewalling to protect systems and networks 661 14.7 revocation of access rights 606 14.8 capability-based systems 607 14.9 language-based protection 610 14.10 surnmary 615 exercises 616 bibliographical notes 618 15.8 computer-security classifications 662 15.9 an example  windows xp 664 15.10 summary 665 exercises 666 bibliographical notes 667 part seven distributed systems chapter 16 distributed operating systems 16.1 motivation 673 16.2 types of networkbased operating systems 675 16.3 network structure 679 16.4 network topology 683 16.5 communication structure 684 16.6 communication protocols 690 16.7 robustness 694 16.8 design issues 697 16.9 an example  networking 699 16.10 summary 701 exercises 701 bibliographical notes 703 chapter 17 distributed file systems 17.1 background 705 17.2 naming and transparency 707 17.3 remote file access 710 17.4 stateful versus stateless service 715 17.5 file replication 716 17.6 an example  afs 718 17.7 summary 723 exercises 724 bibliographical notes 725 chapter 18 distributed synchronization 18.1 event ordering 727 18.2 mutual exclusion 730 18.3 atomicity 733 18.4 concurrency control 736 18.5 deadlock handling 740 18.6 election algorithms 747 18.7 reaching agreement 750 18.8 summary 752 exercises 753 bibliographical notes 754 part eight special purpose systems chapter 19 real-time systems 19.1 overview 759 19.2 system characteristics 760 19.3 features of real-time kernels 762 19.4 implementing real-time operating systems 764 19.5 real-time cpu scheduling 768 19.6 an example  vxworks 5.x 774 19.7 summary 776 exercises 777 bibliographical notes 777 chapter 20 multimedia systems 20.1 what is multimedia 779 20.2 compression 782 20.3 requirements of multimedia kernels 784 20.4 cpu scheduling 786 20.5 disk scheduling 787 20.6 network management 789 20.7 an example  cineblitz 792 20.8 summary 795 exercises 795 bibliographical notes 797 part nine case studies chapter 21 the linux system 21.1 linux history 801 21.2 design principles 806 21.3 kernel modules 809 21.4 process management 812 21.5 scheduling 815 21.6 memory management 820 21.7 file systems 828 chapter 22 windows xp 22.1 history 847 22.2 design principles 849 22.3 system components 851 22.4 environmental subsystems 874 22.5 file system 878 21.8 input and output 834 21.9 interprocess communication 837 21.10 network structure 838 21.11 security 840 21.12 summary 843 exercises 844 bibliographical notes 845 22.6 networking 886 22.7 programmer interface 892 22.8 sum.mary 900 exercises 900 bibliographical notes 901 chapter 23 influential operating systems 23.1 feature migration 903 23.2 early systems 904 23.3 atlas 911 23.4 xds-940 912 23.5 the 913 23.6 rc 4000 913 23.7 ctss 914 23.8 multics 915 23.9 ibm os/360 915 23.10 tops-20 917 23.11 cp/m and ms/dos 917 23.12 macintosh operating system and windows 918 23.13 mach 919 23.14 other systems 920 exercises 921 xix xx chapter a bsd unix a1 unix history 1 a2 design principles 6 a3 programmer interface 8 a.4 user interface 15 as process management 18 a6 memory management 22 appendix b the mach system b.l history of the mach system 1 b.2 design principles 3 b.3 system components 4 b.4 process management 7 b.s interprocess conununication 13 b.6 memory management 18 appendix c windows 2000 c.1 history 1 c.2 design principles 2 c.3 system components 3 c.4 enviromnental subsystems 19 c.s file system 22 bibliography 923 credits 941 index 943 a7 file system 25 as i/0 system 32 a9 interprocess communication 35 alo summary 40 exercises 41 bibliographical notes 42 b.7 programmer interface 23 b.s summary 24 exercises 25 bibliographical notes 26 credits 27 c.6 networking 28 c.7 programmer interface 33 c.s summary 40 exercises 40 bibliographical notes 41 part one an operating system acts as an intermediary between the user of a computer and the computer hardware the purpose of an operating system is to provide an environment in which a user can execute programs in a convenient and efficient manner an operating system is software that manages the computer hardware the hardware must provide appropriate mechanisms to ensure the correct operation of the computer system and to prevent user programs from interfering with the proper operation of the system internally  operating systems vary greatly in their makeup  since they are organized along many different lines the design of a new operating system is a major task it is impmtant that the goals of the system be well defined before the design begins these goals form the basis for choices among various algorithms and strategies because an operating system is large and complex  it must be created piece by piece each of these pieces should be a well delineated portion of the system  with carefully defined inputs  outputs  and functions 1.1 ch er an is a program that manages the computer hardware it also provides a basis for application programs and acts as an intermediary between the computer user and the computer hardware an amazing aspect of operating systems is how varied they are in accomplishing these tasks mainframe operating systems are designed primarily to optimize utilization of hardware personal computer  pc  operating systems support complex games  business applications  and everything in between operating systems for handheld computers are designed to provide an environment in which a user can easily interface with the computer to execute programs thus  some operating systems are designed to be convenient  others to be efficient  and others some combination of the two before we can explore the details of computer system operation  we need to know something about system structure we begin by discussing the basic functions of system startup  i/0  and storage we also describe the basic computer architecture that makes it possible to write a functional operating system because an operating system is large and complex  it must be created piece by piece each of these pieces should be a well-delineated portion of the system  with carefully defined inputs  outputs  and functions in this chapter  we provide a general overview of the major components of an operating system to provide a grand tour of the major components of operating systems to describe the basic organization of computer systems we begin our discussion by looking at the operating system 's role in the overall computer system a computer system can be divided roughly into 3 4 chapter 1 compiler assembler text editor operating system database system figure 1.1 abstract view of the components of a computer system four components  the hardware/ the operating system  the application programs/ and the users  figure 1.1   the hardwa.te-the the and the ievices-provides the basic computing resources for the system the as word processors/ spreadsheets/ compilers  and web browsers-define the ways in which these resources are used to solve users ' computing problems the operating system controls the hardware and coordinates its use among the various application programs for the various users we can also view a computer system as consisting of hardware/ software/ and data the operating system provides the means for proper use of these resources in the operation of the computer system an operating system is similar to a government like a government  it performs no useful function by itself it simply provides an environment within which other programs can do useful work to understand more fully the operating systemfs role  we next explore operating systems from two viewpoints  that of the user and that of the system 1.1.1 user view the user 's view of the computer varies according to the interface being used most computer users sit in front of a pc  consisting of a monitor/ keyboard/ mouse  and system unit such a system is designed for one user to monopolize its resources the goal is to maximize the work  or play  that the user is performing in this case/ the operating system is designed mostly for with some attention paid to performance and none paid to various hardware and software resources are shared performance is  of course  important to the user ; but such systems 1.1 5 are optimized for the single-user experience rather than the requirements of multiple users in other cases  a user sits at a terminal connected to a or a other users are accessing the sance computer through other terminals these users share resources and may exchange information the operating system in s llclc cases is designed to maximize resource utilizationto assure that all available cpu time  memory  and i/0 are used efficiently and tbat no individual user takes more than her fair share in still otber cases  users sit at connected to networks of other workstations and these users have dedicated resources at their disposal  but they also share resources such as networking and servers-file  compute  and print servers therefore  their operating system is designed to compromise between individual usability and resource utilization recently  many varieties of handheld computers have come into fashion most of these devices are standalone units for individual users some are connected to networks  either directly by wire or  more often  through wireless modems and networking because of power  speed  and interface limitations  they perform relatively few remote operations their operating systems are designed mostly for individual usability  but performance per unit of battery life is important as well some computers have little or no user view for example  embedded computers in home devices and automobiles may have numeric keypads and may turn indicator lights on or off to show status  but they and their operating systems are designed primarily to run without user intervention 1.1.2 system view from the computer 's point of view  the operating system is the program most intimately involved with the hardware in this context  we can view an operating system as a  a computer system has many resources that may be required to solve a problem  cpu time  memory space  file-storage space  i/0 devices  and so on the operating system acts as the manager of these resources facing numerous and possibly conflicting requests for resources  the operating system must decide how to allocate them to specific programs and users so that it can operate the computer system efficiently and fairly as we have seen  resource allocation is especially important where many users access the same mainframe or minicomputer a slightly different view of an operating system emphasizes the need to control the various i/0 devices and user programs an operating system is a control program a manages the execution of user programs to prevent errors and improper use of the computer it is especially concerned with the operation and control of i/o devices 1.1.3 defining operating systems we have looked at the operating system 's role from the views of the user and of the system how  though  can we define what an operating system is in general  we have no completely adequate definition of an operating system operating systems exist because they offer a reasonable way to solve the problem of creating a usable computing system the fundamental goal of computer systems is to execute user programs and to make solving user 6 chapter 1 1.2 storage definitions and notation a is the basic unit of computer storage it can contain one of two values  zero and one all other storage in a computer is based on collections of bits given enough bits  it is amazing how many things a computer can represent  numbers  letters  images  movies  sounds  documents  and programs  to name a few a is 8 bits  and on most computers it is the smallest convenient chunk of storage for example  most computers do n't have an instruction to move a bit but do have one to move a byte a less common term is which is a given computer architecture 's native storage unit a word is generally made up of one or more bytes for example  a computer may have instructions to move 64-bit  8-byte  words a kilobyte  or kb  is 1,024 bytes ; a megabyte  or mb  is 1,0242 bytes ; and a gigabyte  or gb  ! s 1,0243 bytes computer manufacturers often round off these numbers and say that a megabyte is 1 million bytes and a gigabyte is 1 billion bytes problems easier toward this goal  computer hardware is constructed since bare hardware alone is not particularly easy to use  application programs are developed these programs require certain common operations  such as those controlling the ii 0 devices the common functions of controlling and allocating resources are then brought together into one piece of software  the operating system in addition  we have no universally accepted definition of what is part of the operating system a simple viewpoint is that it includes everything a vendor ships when you order the operating system the features included  however  vary greatly across systems some systems take up less than 1 megabyte of space and lack even a full-screen editor  whereas others require gigabytes of space and are entirely based on graphical windowing systems a more common definition  and the one that we usually follow  is that the operating system is the one program running at all times on the computer-usually called the   along with the kernel  there are two other types of programs  which are associated with the operating system but are not part of the kernel  and which include all programs not associated with the operation of the system  the matter of what constitutes an operating system has become increasingly important in 1998  the united states deparhnent of justice filed suit against microsoft  in essence claiming that microsoft included too much functionality in its operating systems and thus prevented application vendors from competing for example  a web browser was an integral part of the operating systems as a result  microsoft was found guilty of using its operating-system monopoly to limit competition before we can explore the details of how computer systems operate  we need general knowledge of the structure of a computer system in this section  we look at several parts of this structure the section is mostly concerned 1.2 the study ofoperating systems there has neverbeenarnore interestirighnwtostud yoperating systems  and it has neverb.een.e ~ sier.theopen-sourc ; e movernent has overtaken .operating systems  caj.tsing marly ofthenctobemadeavailable in both source and binary  e ~ ecuta  jle  fonnat .this iistindud ~ ~ linu    bsdunix/solat is,and part of  \ ii ~ cos.x th ~ availa ~ ilityqf source.code.q,llowsus.tostudyoperq,til  .gsy tems frorrt theinsid,eout '  questionsthat previo  1sly could onlyb ~ answerecl ~ y looking atdocumentaticmor thebehayior.ofan op ~ rating system c.annow be answered by examining the code itself in additi n the rise of virtualization as a ll  .ainsfreafll  andfrequelltly free  cmnp  1ter ftmctionmakesitpos ; ~ i1jlet   runnmnyoperqtingsystems.ontop.of onecoresystem  forexample,vmware  j  lttp  //www  vmware   .com   provides afree ''player' ' on which hundreds.of free .''virtualappliilnces' ' cann.m.using this method,students call tryolit hundreds ofoperatingsystems.withintheir existing operatingsystems .atno cost         operating .sy ~ temsthat are no lortge ~ ~ ofllmerci ~ lly viableltave been opell ~ o  lrced asvvell  enablirtg .usto study how system ~ pperated i ~ time.of  f ~ v.r ~ r cpu  ll  .emory  etnd.storcrge .resoj.trces  .an  exten ~ iye.b  .it not complete   list   f 9pen'-sourct operafirtg system pr j ~ ts is  availa ~ le rom ht ~ p  // dm   ~ ' org/ c  omp  1ters/softp  lre /operati g  -systems/p ~ ~ m._sourc ~ / s i..m   .u l a t .o r s  o .f s  p e  c i ..f i c  ....h a  ...r ....d w  .a  r e   ar..e  .a l s .o   a   v  a.i.l 1 b  .le  .i n    s om   .e   .c  a  s e s '  al i ....o w   m ...g th ~ operat ~ g systell  .to.runon.''na ~ ve''.hardware   all ~ ithrrtthec l  .fines of a modem co ! tipj-iter and moderj1 opf/'atirtg ~ ystem for  example  a decsystemc20 simulator running on mac os x can boot tops-20  loa ~  the ~ ource.tages ;  and modify al'ld comp ~ le l .j  t.evvtops-20 .k ~ rnel art interested stltdent ~ ar search theint ~ rnet to find the origillal papers that de ~ cribe the operating systemand  the.origipa ~ manuals  tl e adve ~ t fogen-source operafirtg sy ~ te1tis also l   lal es it easy t .make the move fromstu ~ enttooper  lting ~ systemdeveloper.with some knov.rledge  som ~ effo1't  a11d an internet connection,a student c ; al'leven create a new operating-systemdistribution ! justa fev.r years  ~ go itwas diffic  _llt or if1lpossible to get acce ~ s  to source co e   n v.r  that access is lijnited only bylt   wmuchtimeand disk space a student has 7 with computer-system organization  so you can skim or skip it if you already understand the concepts 1.2.1 computer-system operation a modern general-purpose computer system consists of one or more cpus and a number of device controllers connected through a common bus that provides access to shared memory  figure 1.2   each device controller is in charge of a specific type of device  for example  disk drives  audio devices  and video displays   the cpu and the device controllers can execute concurrently  competing for memory cycles to ensure orderly access to the shared memory  a memory controller is provided whose function is to synchronize access to the memory for a computer to start rum ing-for instance  when it is powered up or rebooted-it needs to have an initial program to run this initial 8 chapter 1 mouse keyboard printer monitor o ~ ~ ~  _rlo i-nneh b figure 1.2 a modern computer system program  or tends to be simple typically  it is stored in read-only memory or electrically erasable programmable read-only memory known by the general term within the computer hardware it initializes all aspects of the system  from cpu registers to device controllers to memory contents the bootstrap program must know how to load the operating system and how to start executing that system to accomplish this goal  the bootstrap program must locate and load into memory the operatingsystem kernel the operating system then starts executing the first process  such as init  and waits for some event to occur the occurrence of an event is usually signaled by an from either the hardware or the software hardware may trigger an interrupt at any time by sending a signal to the cpu  usually by way of the system bus software may trigger an interrupt executing a special operation called a  also called a when the cpu is interrupted  it stops what it is doing and immediately transfers execution to a fixed location the fixed location usually contains the starting address where the service routine for the interrupt is located the interrupt service routine executes ; on completion  the cpu resumes the interrupted computation a time line of this operation is shown in figure 1.3 interrupts are an important part of a computer architecture each computer design has its own interrupt mechanism  but several functions are common the interrupt must transfer control to the appropriate interrupt service routine the straightforward method for handling this transfer would be to invoke a generic routine to examine the interrupt information ; the routine  in turn  would call the interrupt-specific handler however  interrupts must be handled quickly since only a predefined number of interrupts is possible  a table of pointers to interrupt routines can be used instead to provide the necessary speed the interrupt routine is called indirectly through the table  with no intermediate routine needed generally  the table of pointers is stored in low memory  the first hundred or so locations   these locations hold the addresses of the interrupt service routines for the various devices this array  or of addresses is then indexed by a unique device number  given with the interrupt request  to provide the address of the interrupt service routine for cpu user 1/0 device process executing 1/0 interrupt processing idle ~ ~  ~  tmcefeniog i l 1/0 request 1.2 ll v  ~ ~ ' ''''' ' ~ ' ' '  ~ ~  ~ ~   t ~  ' 'm '  l  ~ ~ ~ ~ transfer done 1/0 transfer request done figure 1.3 interrupt time line for a single process doing output 9 the interrupting device operating systems as different as windows and unix dispatch interrupts in this manner the interrupt architecture must also save the address of the interrupted instruction many old designs simply stored the interrupt address in a fixed location or in a location indexed by the device number more recent architectures store the return address on the system stack if the interrupt routine needs to modify the processor state-for instance  by modifying register values-it must explicitly save the current state and then restore that state before returning after the interrupt is serviced  the saved return address is loaded into the program counter  and the interrupted computation resumes as though the interrupt had not occurred 1.2.2 storage structure the cpu can load instructions only from memory  so any programs to run must be stored there general-purpose computers run most of their programs from rewriteable memory  called main memory  also called or ram   main commonly is implemented in a semiconductor technology called computers use other forms of memory as well because the read-only memory  rom  camwt be changed  only static programs are stored there the immutability of rom is of use in game cartridges eeprom camwt be changed frequently and so contains mostly static programs for example  smartphones have eeprom to store their factory-il stalled programs all forms of memory provide an array of words each word has its own address interaction is achieved through a sequence of load or store instructions to specific memory addresses the load instruction moves a word from main memory to an internal register within the cpu  whereas the store instruction moves the content of a register to main memory aside from explicit loads and stores  the cpu automatically loads instructions from main memory for execution a typical instruction-execution cycle  as executed on a system with a architecture  first fetches an il1struction from memory and stores that instruction in the  the instruction is then decoded and may cause operands to be fetched from memory and stored in some 10 chapter 1 internal register after the instruction on the operands has been executed  the result may be stored back in memory notice that the memory unit sees only a stream of memory addresses ; it does not know how they are generated  by the instruction counter  indexing  indirection  literal addresses  or some other means  or what they are for  instructions or data   accordingly  we can ignore how a memory address is generated by a program we are interested only in the sequence of memory addresses generated by the running program ideally  we want the programs and data to reside in main ncemory permanently this arrangement usually is not possible for the following two reasons  main memory is usually too small to store all needed programs and data permanently main memory is a volatile storage device that loses its contents when power is turned off or otherwise lost thus  most computer systems provide as an extension of main memory the main requirement for secondary storage is that it be able to hold large quantities of data permanently the most common secondary-storage device is a which provides storage for both programs and data most programs  system and application  are stored on a disk until they are loaded into memory many programs then use the disk as both the source and the destination of their processing hence  the proper management of disk storage is of central importance to a computer system  as we discuss in chapter 12 in a larger sense  however  the storage structure that we have describedconsisting of registers  main memory  and magnetic disks-is only one of many possible storage systems others include cache memory  cd-rom  magnetic tapes  and so on each storage system provides the basic functions of storing a datum and holding that datum until it is retrieved at a later time the main differences among the various storage systems lie in speed  cost  size  and volatility the wide variety of storage systems in a computer system can be organized in a hierarchy  figure 1.4  according to speed and cost the higher levels are expensive  but they are fast as we move down the hierarchy  the cost per bit generally decreases  whereas the access time generally increases this trade-off is reasonable ; if a given storage system were both faster and less expensive than another-other properties being the same-then there would be no reason to use the slower  more expensive memory in fact  many early storage devices  including paper tape and core memories  are relegated to museums now that magnetic tape and have become faster and cheaper the top four levels of memory in figure 1.4 may be constructed using semiconductor memory in addition to differing in speed and cost  the various storage systems are either volatile or nonvolatile as mentioned earlier  loses its contents when the power to the device is removed in the absence of expensive battery and generator backup systems  data must be written to for safekeeping in the hierarchy shown in figure 1.4  the the electronic disk are volatile  whereas those below 1.3 15 figure 1.6 symmetric multiprocessing architecture solaris the benefit of this model is that many processes can run simultaneously -n processes can run if there are n cpus-without causing a significant deterioration of performance however  we must carefully control i/0 to ensure that the data reach the appropriate processor also  since the cpus are separate  one may be sitting idle while another is overloaded  resulting in inefficiencies these inefficiencies can be avoided if the processors share certain data structures a multiprocessor system of this form will allow processes and resources-such as memory-to be shared dynamically among the various processors and can lower the variance among the processors such a system must be written carefully  as we shall see in chapter 6 virtually all modern operating systems-including windows  windows xp  mac os x  and linux -now provide support for smp the difference between symmetric and asymmetric multiprocessing may result from either hardware or software special hardware can differentiate the multiple processors  or the software can be written to allow only one master and multiple slaves for instance  sun 's operating system sunos version 4 provided asymmetric multiprocessing  whereas version 5  solaris  is symmetric on the same hardware multiprocessing adds cpus to increase computing power if the cpu has an integrated memory controller  then adding cpus can also increase the amount of memory addressable in the system either way  multiprocessing can cause a system to change its memory access model from uniform memory access to non-uniform memory access uma is defined as the situation in which access to any ram from any cpu takes the same amount of time with numa  some parts of memory may take longer to access than other parts  creating a performance penalty operating systems can minimize the numa penalty through resource management_  as discussed in section 9.5.4 a recent trend in cpu design is to in.clude multiple computing on a single chip in essence  these are multiprocessor chips they can be more efficient than multiple chips with single cores because on-chip communication is faster than between-chip communication in addition  one chip with multiple cores uses significantly less power than multiple single-core chips as a result  multicore systems are especially well suited for server systems such as database and web servers 16 chapter 1 figure 1.7 a dual-core design with two cores placed on the same chip in figure 1.7  we show a dual-core design with two cores on the same chip in this design  each core has its own register set as well as its own local cache ; other designs might use a shared cache or a combination of local and shared caches aside from architectural considerations  such as cache  memory  and bus contention  these multicore cpus appear to the operating system as n standard processors this tendency puts pressure on operating system designers-and application programmers-to make use of those cpus finally  are a recent development in which multiple processor boards  i/0 boards  and networking boards are placed in the same chassis the difference between these and traditional multiprocessor systems is that each blade-processor board boots independently and runs its own operating system some blade-server boards are n1.ultiprocessor as well  which blurs the lines between types of computers in essence  these servers consist of multiple independent multiprocessor systems 1.3.3 clustered systems another type of multiple-cpu system is the like multiprocessor systems  clustered systems gather together multiple cpus to accomplish computational work clustered systems differ from multiprocessor systems  however  in that they are composed of two or more individual systems-or nodes-joined together the definition of the term clustered is not concrete ; many commercial packages wrestle with what a clustered system is and why one form is better than another the generally accepted definition is that clustered computers share storage and are closely linked via a jc'.h.a  o.x  as described in section 1.10  or a faster interconnect  such as infiniband clustering is usually used to provide service ; that is  service will continue even if one or more systems in the cluster fail high availability is generally obtained by adding a level of redundancy in the system a layer of cluster software runs on the cluster nodes each node can monitor one or more of the others  over the lan   if the monitored machine fails  the monitoring machine can take ownership of its storage and restart the applications that were running on the failed machine the users and clients of the applications see only a brief interruption of service 1.3 beowulf clusters beowulf clusters are designed for solving high-performance computing tasks these clusters are built using comm.odi ty hard ware-such as personal computers-that are connected via a simple local area network interestingly  a beowulf duster uses no one specific software package but rather consists of a set of open-source software libraries that allow the con1puting nodes in the cluster to communicate with one another  thus,.there are a variety of approaches for constructing a beowulf cluster  although beowulf computing nodes typically run the linux operating system since beowulf clusters require no special hardware and operate using open ~ source software that is freely available  they offer a low-cost strategy for building a high ~ performance computing cluster in fact  some beowulf clusters built from collections of discarded personal computers are using ht.mdreds of cornputing nodes to solve computationally expensive problems in scientific computing clusterin.g can be structured or symmetrically in 17 one machine is in while the other is rmming the applications the hot-standby host machine does nothing but monitor the active server if that server fails  the hot-standby host becomes the active server in two or more hosts are rmming applications and are monitoring each other this mode is obviously more efficient  as it uses all of the available hardware it does require that more than one application be available to run as a cluster consists of several clusters may also be used to provide environments such systems can supply significantly greater computational power than single-processor or even smp systems because they are capable of running an application concurrently on all computers in the cluster however  applications must be written to take advantage of the cluster by using a technique known as which consists of dividing a program into separate components that run in parallel on individual computers in the cluster typically  these applications are designed so that once each computing node in the cluster has solved its portion of the problem  the results from all the nodes are combined into a final solution other forms of clusters include parallel clusters and clustering over a wide-area network  wan   as described in section 1.10   parallel clusters allow multiple hosts to access the same data on the shared storage because most operating systems lack support for simultaneous data access by multiple hosts  parallel clusters are usually accomplished by use of special versions of software and special releases of applications for example  oracle real application cluster is a version of oracle 's database that has been designed to run on a parallel cluster each machine runs oracle  and a layer of software tracks access to the shared disk each machine has full access to all data in the database to provide this shared access to data  the system must also supply access control and locking to ensure that no conflicting operations occur this function  commonly known as a is included in some cluster technology 18 chapter 1 1.4 interconnect interconnect computer computer computer figure 1.8 general structure of a clustered system cluster technology is changing rapidly some cluster products support dozens of systems in a cluster  as well as clustered nodes that are separated by miles many of these improvements are made possible by  saj ~ is   as described in section 12.3.3  which allow many systems to attach to a pool of storage if the applications and their data are stored on the san  then the cluster software can assign the application to run on any host that is attached to the san if the host fails  then any other host can take over in a database cluster  dozens of hosts can share the same database  greatly increasing performance and reliability figure 1.8 depicts the general structure of a clustered system now that we have discussed basic information about computer-system organization and architecture  we are ready to talk about operating systems an operating system provides the envirorunent within which programs are executed internally  operating systems vary greatly in their makeup  since they are organized along many different lines there are  however  many commonalities  which we consider in this section one of the most important aspects of operating systems is the ability to multiprogram a single program can not  in generat k ~ ~ p ~ ith_er thg cpu ortbt j/qgey  ic  es 1jusy_c1t all times  single users frequently have multiple programs running il.ul increases cpu utilization byorganizing jobs  codeand datafso      _ hasoi1  0to execl1te   fhe idea is as follows  the op-ei  atlng system keeps several jobs in memory simultaneously  figure 1.9   since  in generat main memory is too small to accommodate all jobs  the jobs are kept initially on the disk in the this pool consists of all processes residing on disk awaiting allocation of main memory ih ~ setofjobs inmemg_ry_canbe asubt  ; et of the jobs kept in thejql  jpoo1 the operating system picks and begins to execute one of the jobs in memory eventually  the job may have to wait for some task  such as an i/o operation  1.4 19 figure 1.9 memory layout for a multiprogramming system !   _c   _tnpl ~ te  in a non-multiprogrammed system  the cpu would sit idle in a multiprogrammed system  the operatilcg system simply switches to  and executes  another job when that job needs to wait  the cpu is switched to another job  and so on eventually the first job finishes waiting and gets the cpu back as long as at least one job needs to execute  the cpu is never idle this idea is common in other life situations a lawyer does not work for only one client at a time  for example while one case is waiting to go to trial or have papers typed  the lawyer can work on another case if he has enough clients  the lawyer will never be idle for lack of work  idle lawyers tend to become politicians  so there is a certain social value in keeping lawyers busy  multiprogrammed systems provide an environment in which the various system resources  for example  cpu  memory  and peripheral devices  are utilized effectively  but they do not provide for user interaction with the computer system is_ ~ l   gi ~ alex_tension of multiprogramming ~ ' time-s ! caring syste ~ s,the cpl  execu ~ eslnl1ltiplejobs by switcll.ing ~ ainong them  but the switches occur so frequently that the ~ 1sers canh ~ teract with eachprograffi ~ v ere l.t1sil.mning. -ti1ne shar  il ~ g requi.i-es an    or  which provides direct communication between the user and the system the user gives instructions to the operating system or to a program directly  using a input device such as a keyboard or a mouse  and waits for immediate results on an output device accordingly  ! ! 'te sho ~ 1ld be sh   rt = typically less than one second a time-shared operating system allows many users to share the computer simultaneously since each action or command in a time-shared system tends to be short  only a little cpu time is needed for each user as the system switches rapidly from one user to the next  each user is given the impression that the entire computer system is dedicated to his use  even though it is being shared among many users a time-shared operating system 11ses cpu scheduling and multiprogramming to provide each user with a small portion of a time-shared computer eachuserhas atleast or  t_e s parateprogra111inmemory a program loaded into 20 1.5 chapter 1 memory and executing is called a when a process executes  it typically executes for only a short tirne it either finishes or needs to perform i/0 i/0 may be interactive ; that is  output goes to a display for the user  and input comes from a user keyboard  mouse  or other device since interactive i/0 typically runs at people speeds  it may take a long time to complete input  for example  may be bounded by the user 's typing speed ; seven characters per second is fast for people but incredibly slow for computers rather than let the cpu sit idle as this interactive input takes place  the operating system will rapidly switch the cpu to the program of some other user time sharing and multiprogramming require that several jobs be kept simultaneously in memory if several jobs are ready to be brought into memory  and if there is not enough room for all of them  then the system must choose among them making this decision is which is discussed in chapter 5 when the operating system selects a job from the job pool  it loads that job into memory for execution having several programs in memory at the same time requires some form of memory management  which is covered in chapters 8 and 9 in addition  ! f_s ~ verajjq  jsaxere  lcly to rw ~ at the same time  the system must choose among them making this decision i ~ _ _ sd1,2dviii lg  which is discussed in chapter 5 finally  running multiple jobscoi ~ cl.lrl  ei1hy requires that their ability to affect one another be limited in all phases of the operating system  including process scheduling  disk storage  and memory management these considerations are discussed throughout the text in a time-sharing system  the operating system must ensure reasonable response time  which is sometimes accomplished through where processes are swapped in and out of main memory to the disk a more common method for achieving this goal tec  hdiql1 ~ _fuc ! t _ cillqws._ the execution of aprocessthat isnot completely inl1le1yl_cld ~   chapter 9   the main advai1tage of the virtual-memory scheme is that it enables users to run programs that are larger than actual  further  it abstracts main memory into a large  uniform array of storage  separating logical as viewed by the user from physical memory this arrangement frees programmers from concern over memory-storage limitations time-sharing systems must also provide a file system  chapters 10 and 11   the file system resides on a collection of disks ; hence  disk management must be provided  chapter 12   also  time-sharing systems provide a mechanism for protecting resources from inappropriate use  chapter 14   to ensure orderly execution  the system must provide mechanisms for job synchronization and communication  chapter 6   and it may ensure that jobs do not get stuck in a deadlock  forever waiting for one another  chapter 7    \ si1  e11tio11ecl ~ arlier  rn   cletnopexatli1ksystems_m ~ e _ if there are no processes to execute  no i/0 devices to service  and no users to whom to respond  an operating system will sit quietly waiting for something to happen events are almost always signaled by the occurrence of an interrupt or a trap  or an is_ a software ~ generated interruptca ~ seci ~ it  ler byan error  for division byzero or invalid memory acc ~ ss_  or by a specific request from a user program that an operating-system service 1.5 21 be performed the interrupt-driven nature of an operating system defines that system 's general structure for each type of interrupt  separate segments of code in the operating system determine what action should be taken an interrupt service routine is provided that is responsible for dealing with the interrupt since the operating system and the users share the hardware and software resources of the computer system  we need to make sure that an error in a user program could cause problems only for the one program running with sharing  many processes could be adversely affected by a bug in one program for example  if a process gets stuck in an infinite loop  this loop could prev.ent the correct operation of many other processes more subtle errors can occur in a multiprogramming system  where one erroneous program might modify another program  the data of another program  or even the operating system itself without protection against these sorts of errors  either the computer must execute only one process at a time or all output must be suspect a properly designed operating system must ensure that an incorrect  or malicious  program can not cause other program ~ to  ~ x.t ; cute incorrectly ~ ~  ; ~ ,_c  ; ..c ~ 1.5.1 dual-mode operation in order to ensure the proper execution of the operating system  we must be able to distinguish between the execution of operating-system code and userdefined code the approach taken by most computer systems is to provide hardware support that allows us to differentiate among various modes of execution at the very least we need two and  also called or a bit  called the is added to the hardware of the computer to indicate the current mode  kernel  0  or user  1   \ ! viththeplode1  jit \ ! ve2lrea  jle to distinguishbetween a task that is executed onbehalf of the operating system aicd one that is executeci on behalfofthejjser  when tl ~ e computer systel.n1s executing on behalf of a user application  the system is in user mode however  when a user application requests a service from the operating system  via a  system call   it must transition from user to kernel mode to fulfill the request / this is shown in figure 1.10 as we shall see  this architectural enhancement is useful for many other aspects of system operation as well execute system call figure 1 i 0 transition from user to kernel mode user mode  mode bit = i  kernel mode  mode bit = 0  22 chapter 1 at system boot time  the hardware starts in kernel mode the operating system is then loaded and starts user applications in user mode whenever a trap or interrupt occurs  the hardware switches from user mode to kernel mode  that is  changes the state of the mode bit to 0   thus  whenever the operating system gains control of the computer  it is in kernel mode the system always switches to user mode  by setting the mode bit to 1  before passing control to a user program the dual mode of operation provides us with the means for protecting the operating system from errant users-and errant users from one another  ye _  ! cc011lplishthis protection by designating some ofthe machineine ; tructions ~ ha !  trlijjt cal1_sej ~ i  i ~ l11 ins trucrci \   l  il1e hardware all ~ \ \ 'spl iyileg ~ d instrl  ctionsto be o11ly inkern ~ ll11qq_ ~  if an attempt is made to execute a privileged instruction in user mode  the hardware does not execute the instruction but rather treats it as illegal and traps it to the operating system the instruction to switch to kernel mode is an example of a privileged instruction some other examples include i/0 controt timer management and interrupt management as we shall see throughout the text  there are many additional privileged instructions we can now see the life cycle of instruction execution in a computer system initial control resides in the operating system  where instructions are executed in kernel mode when control is given to a user application  the mode is set to user mode eventually  control is switched back to the operating system via an interrupt  a trap  or a system call _5ysiemcalls proyide the means for auser program to ask the operating 2  'st ~ m to perforp  t tasks re_ erved forjhe operating syst ~ m gr1 the 1.lser .12l  qgra1ll'sbeha,lf a system call is invoked in a variety of ways  depending on the functionality provided by the underlying processor in all forms  it is the method used by a process to request action by the operating system a system call usually takes the form of a trap to a specific location in the interrupt vector this trap can be executed by a generic trap instruction  although some systems  such as the mips r2000 family  have a specific syscall instruction when asystep1 calljs e   ecutect it is treated by the hardware as a software -i  rlt ~ rr  l.l   if  c   iltrol passes through the interrupt vector to a service routine in the operating system/ and the m   de bit is set to kernel mode the systemcaflserv1ce routine is a part of the operating system the-kernel examines the interrupting instruction to determine what system call has occurred ; a ~ parameter indicates what type of service the user program is requesting additional information needed for the r ~ quest_may be passed in registers  on the stack/ or in memory  with pointers to the memory locations passed in registers   the kernel vedfies that the parameters are correct and legat executes ti1erequest  and returns control to the instruction following the system call we describe system calls more fully in section 2.3 the lack of a hardware-supported dual mode can cause serious shortcomings in an operating system for instance  ms-dos was written for the intel 8088 architecture  which has no mode bit and therefore no dual mode a user program rum1ing awry can wipe out the operating system by writing over it with data ; and multiple programs are able to write to a device at the same time  with potentially disastrous results recent versions of the intel cpu do provide dual-mode operation accordingly  most contemporary operating systemssuch as microsoft vista and windows xp  as well as unix  linux  and solaris 1.6 1.6 23 -take advantage of this dual-mode feature and provide greater protection for the operating system once hardware protection is in place  it detects errors that violate modes these errors are normally handled by the operating system if a user program fails in some way-such as by making an attempt either to execute an illegal instruction or to access memory that is not in the user 's address space-then the hardware traps to the operating system the trap transfers control through the interrupt vector to the operating system  just as an interrupt does when a program error occurs  the operating system must terminate the program abnormally this situation is handled by the same code as a user-requested abnormal termination an appropriate error message is given  and the memory of the program may be dumped the memory dump is usually written to a file so that the user or programmer can examine it and perhaps correct it and restart the program 1.5.2 timer wer  r1,_ust ensure th t ! the ope  j ; atil  gsystemij  taintains t  ontrol overthe c  j_ !  l_ ~ we cam1.ot allow a userp ~ ogram to_ get stuc  kin e1ninfinite loop or to fail to call syste1n seryices and never retltrn control to the c  perating system to ~ c  9 ! ll 1i  s ~ tl1.1s = g ~ at we_can usea _a_tirn ~ r_can beset to interrupt th ~ c  c  mp_ut ~ r af_t ~ ril p ~ c  ified peri   d the period may be fixed  for example  1/60 second  or variable  for example  from 1 millisecond to 1 second   a is generally implemented by a fixed-rate clock and a counter the operating system sets the counter every time the clock ticks  the counter is decremented when the counter reaches 0  an interrupt occurs for instance  a 10-bit counter with a 1-millisecond clock allows interrupts at intervals from 1 millisecond to 1,024 milliseconds  in steps of 1 millisecond before turning over control to the user  the operating system ensures that the timer is set to interrupt ll ~ ll ~ __ tij11e_ _il1t ~ rrl1pts/control transfers automatically totll.e   pel  9  t ~ ~ y ! epl,_ \ thicfl__ ! l-1  1ytreat the interrupt as a faiaf error or n  taygi-y_etll.ep_rograrn rnc  r ~ ! i  rn ~   clearly,il ~ structions that modify the content of the timer are privileged thus  we can use the timer to prevent a user program from running too long a simple technique is to il1.itialize a counter with the amount of time that a program is allowed to run a program with a 7-minute time limit  for example  would have its counter initialized to 420 every second  the timer interrupts and the counter is decremented by 1 as long as the counter is positive  control is returned to the user program when the counter becomes negative  the operating system terminates the program for exceeding the assigned time limit a program does nothing unless its instructions are executed by a cpu a program in execution  as mentioned  is a process a time-shared user program such as a compiler is a process a word-processing program being run by an individual user on a pc is a process a system task  such as sending output to a printer  can also be a process  or at least part of one   for now  you can consider a process to be a job or a time-shared program  but later you will learn 24 chapter 1 1.7 that the concept is more general as we shall see in chapter 3  it is possible to provide system calls that allow processes to create subprocesses to execute concurrent ! y a process needs certain resources---including cpu time  me111ory  files  and-i ; o devices    _  _ to accomplish its  task these i esources are e ! tl1er given to the process when it is created or allocated to it while it is running in addition to the various physical and logical resources that a process obtains when it is created  various initialization data  input  may be passed along for example  consider a process whose function is to display the status of a file on the screen of a terminal the process will be given as an input the name of the file and will execute the appropriate instructions and system calls to obtain and display on the terminal the desired information when the process terminates  the operating system will reclaim any reusable resources l ve ~ _111pl  t21size that a program by itselfis nota process ; a program is a y_assive er ~ ! ~ ty  likt  tl1e c   i1terltsof a fil  storecl_m1 c ! iskl ~ a.thereasc \ _pr  jce ~ ~ s_1s 21 ~ 1 active entity a si-dgl ~   1hr  eaded proc ~ ss has on ~ _pr_ogra111 cou11 ! er s  eecifying the nexf1il ~ r  uc_tiogt   _ex ~ cljte  threads are covered in chapter 4  the -execi.rtioil of such a process must be sequential the cpu executes one instruction of the process after another  until the process completes further  at any time  one instruction at most is executed on behalf of the process thus  although two processes may be associated with the same program  they are nevertheless considered two separate execution sequences a multithreaded process has multiple program counters  each pointing to the next instruction to execute for a given thread a process is the unit of work in a system such a system consists of a collection of processes  some of which are operating-system processes  those that execute system code  and the rest of which are user processes  those that execute user code   al  jheseprocesses canp   t ~ ! ltially execute concurrently _lly.ij  lli  ! p_l ~   _i ! lg   i ' \ a sir1gle _c  pl  ,for_ ~   ample      the operating system is responsible for the following activities in connection with process management  scheduling processes and threads on the cpus creating and deleting both user and system processes suspending and resuming processes providing mechanisms for process synchronization providing mechanisms for process communication we discuss process-management techniques in chapters 3 through 6 as we discussed in section 1.2.2  the main memory is central to the operation of a modern computer system main memory is a large array of words or bytes  ranging in size from hundreds of thousands to billions each word or byte has its own address main memory is a repository of quickly accessible data shared by the cpu and i/0 devices the central processor reads instructions from main 1.8 1.8 25 memory during the instruction-fetch cycle and both reads and writes data from main memory during the data-fetch cycle  on a von neumann architecture   as noted earlier  the main memory is generallythe only large storage device that the cpu is able to address and access directly for example  for the cpu to process data from disk  those data mu.st first be transferred to main n lemory by cpu-generated i/0 calls in the same way  instructions must be in memory for the cpu to execute them for a program to be executed  it must be mapped to absolute addresses and loaded into memory as the program executes  it accesses program instructions and data from memory by generating these absolute addresses eventually  the program terminates  its memory space is declared available  and the next program can be loaded and executed to improve both the utilization of the cpu and the speed of the computer 's response to its users  general-purpose computers must keep several programs in memory  creating a need for memory management many different memorymanagement schemes are used these schemes reflect various approaches  and the effectiveness of any given algorithm depends on the situation in selecting a memory-management scheme for a specific system  we must take into account many factors-especially the hardware design of the system each algorithm requires its own hardware support the operating system is responsible for the following activities in connection with memory management  keeping track of which parts of memory are currently being used and by whom deciding which processes  or parts thereof  and data to move into and out of memory allocating and deallocating memory space as needed memory-management techniques are discussed il1 chapters 8 and 9 to make the computer system convenient for users  the operating system provides a uniform  logical view of information storage the operating system abstracts from the physical properties of its storage devices to define a logical storage unit  the file the operating system maps files onto physical media and accesses these files via the storage devices 1.8.1 file-system management pile management is one of the most visible components of an operating system computers can store information on several different types of physical media magnetic disk  optical disk  and magnetic tape are the most common each of these media has its own characteristics and physical organization each medium is controlled by a device  such as a disk drive or tape drive  that also has its own unique characteristics these properties include access speed  capacity  data-transfer rate  and access method  sequential or randmn   26 chapter 1 a file is a collection of related information defined by its creator commonly  files represent programs  both source and object forms  and data data files may be numeric  alphabetic  alphanumeric  or binary files may be free-form  for example  text files   or they may be formatted rigidly  for example  fixed fields   clearly  the concept of a file is an extremely general one the operating system implements the abstract concept of a file by managing mass-storage media  such as tapes and disks  and the devices that control them also  files are normally organized into directories to make them easier to use finally  when multiple users have access to files  it may be desirable to control by whom and in what ways  for example  read  write  append  files may be accessed the operating system is responsible for the following activities in connection with file management  creating and deleting files creating and deleting directories to organize files supporting primitives for manipulating files and directories mapping files onto secondary storage backing up files on stable  nonvolatile  storage media file-management teclmiques are discussed in chapters 10 and 11 1.8.2 mass-storage management as we have already seen  because main memory is too small to accommodate all data and programs  and because the data that it holds are lost when power is lost  the computer system must provide secondary storage to back up main memory most modern computer systems use disks as the principal on-line storage medium for both programs and data most programs-including compilers  assemblers  word processors  editors  and formatters-are stored on a disk until loaded into memory and then use the disk as both the source and destination of their processing hence  the proper management of disk storage is of central importance to a computer system the operating system is responsible for the following activities in connection with disk management  free-space management storage allocation disk scheduling because secondary storage is used frequently  it must be used efficiently the entire speed of operation of a computer may hinge on the speeds of the disk subsystem and the algorithms that manipulate that subsystem there are  however  many uses for storage that is slower and lower in cost  and sometimes of higher capacity  than secondary storage backups of disk data  seldom-used data  and long-term archival storage are some examples magnetic drives and their tapes and cd and dvd drives and platters are typical devices the media  tapes and optical platters  vary between  write-once  read-many-times  and  read-write  formats 1.8 27 tertiary storage is not crucial to systern performance  but it still must be managed some operating systems take on this task  while others leave tertiary-storage management to application progran1s some of the functions that operating systerns can provide include mounting and unmounting rnedia in devices  allocating and freeing the devices for exclusive use by processes  and migrating data from secondary to tertiary storage techniques for secondary and tertiary storage management are discussed in chapter 12 1.8.3 caching is an important principle of computer systems information is normally kept in some storage system  such as main memory   as it is used  it is copied into a faster storage system-the cache-on a temporary basis when we need a particular piece of information  we first check whether it is in the cache if it is  we use the information directly from the cache ; if it is not  we use the information from the source  putting a copy in the cache under the assumption that we will need it again soon in addition  internal programmable registers   such as index registers  provide a high-speed cache for main memory the programmer  or compiler  implements the register-allocation and register-replacement algorithms to decide which information to keep in registers and which to keep in main memory there are also caches that are implemented totally in hardware for instance  most systems have an instruction cache to hold the instructions expected to be executed next without this cache  the cpu would have to wait several cycles while an instruction was fetched from main memory for similar reasons  most systems have one or more high-speed data caches in the memory hierarchy we are not concerned with these hardware-only caches in this text  since they are outside the control of the operating system because caches have limited size  is an important design problem careful selection of the cache size and of a replacement policy can result in greatly increased performance figure 1.11 compares storage performance in large workstations and small servers various replacement algorithms for software-controlled caches are discussed in chapter 9 typical size 16mb 64gb 100gb implementation custom memory with on-chip or off-chip cmos dram magnetic disk technology multiple ports  cmos cmossram access time  ns  0.25-0.5 0.5-25 80-250 5,000.000 bandwidth  mb/sec  20,000 ~ 100,000 5000 10,000 1000-5000 20-150 managed by compiler hardware operating system operating system backed by cache main memory disk cd or tape figure 1.11 performance of various levels of storage 28 chapter 1 main memory can be viewed as a fast cache for secondary storage  since data in secondary storage must be copied into main memory for use  and data must be in main memory before being moved to secondary storage for safekeeping the file-system data  which resides permanently on secondary storage  may appear on several levels in the storage hierarchy at the highest level  the operating system may maintain a cache of file-system data in main memory in addition  electronic ram disks  also known as may be used for high-speed storage that is accessed through the file-system interface the bulk of secondary storage is on magnetic disks the magneticdisk storage  in turn  is often backed up onto magnetic tapes or removable disks to protect against data loss in case of a hard-disk failure some systems autoinatically archive old file data from secondary storage to tertiary storage  such as tape jukeboxes  to lower the storage cost  see chapter 12   the movement of information between levels of a storage hierarchy may be either explicit or implicit  depending on the hardware design and the controlling operating-system software f_o   '  ! lstilnce,datatransfe ~ from cache _l ~ cpu ~ '11 ~ cl_ ! ~ g ~ ~ ! ~  r-_s _is_ _ ~ 1suall y ahardvvare function  with no op-era t  ii.g = sy-s tern intervention in contrast  transfer of data-from aisk to memory is usually controlledby the-op ~ ra-t !  ri.g system   fn a 11ier2rrchical storage structure  the same data may appear in different levels of the storage system for example  suppose that an integer a that is to be incremented by 1 is located in file b  and file b resides on magnetic disk the increment operation proceeds by first issuing an i/o operation to copy the disk block on which a resides to main memory this operation is followed by copying a to the cache and to an internal register thus  the copy of a appears in several places  on the magnetic disk  in main memory  in the cache  and in an internal register  see figure 1.12   once the increment takes place in the internal register  the value of a differs in the various storage systems the value of a becomes the same only after the new value of a is written from the internal register back to the magnetic disk in a computing environment where only one process executes at a tim.e  this arrangement poses no difficulties  since an access to integer a will always be to the copy at the highest level of the hierarchy however  in a multitasking environment  where the cpu is switched back and -forth-among var1ous processes ~ extreme care must be taken to ensure that  if several processe ~ vv  is  l i  o-accessa  then each of these processes will obtain the most recently updated ___ c_ = --.c of a     the situation becomes more complicated in a multiprocessor environment where  in addition to maintaining internal registers  each of the cpus also contains a local cache  figure 1.6   ~ ' 1_ su ~ bc1  .1l_  _n ~ i o ! _l  _il'l_ ~ 1lt  ~ s 2ey   f_a ij.t ~ y exist simultaneouslyinseyeral caches since the variouscpus can all execute .s  2 ~ 1c ~ r ~ ~ l ~ tly  \ ,ve-must1nake surethat an to the value ofa in one cache figure 1.12 migration of integer a from disk to register 1.9 1.9 29 1.8.4 1/0 systems one of the purposes of a11 operating system is to hide the peculiarities ofspecific hardware d ~ ~ ic  ~ jro1n th ~ l1s ~ j   for example  in unix  the peculiarities of i/o devices are hidden from the bulk of the operating system itself by the i/0 subsystem the i/o subsystem consists of several components  a memory-management component that includes buffering  caching  and spooling a general device-driver interface drivers for specific hardware devices only the device driver knows the peculiarities of the specific device to which it is assigned we discussed in section 1.2.3 how interrupt handlers and device drivers are used in the construction of efficient i/o subsystems in chapter 13  we discuss how the i/o subsystem interfaces to the other system components  manages devices  transfers data  and detects i/0 completion if a computer system has multiple users and allows the concurrent execution of multiple processes  then access to data must be regulated for that purpose  mechanisms ensure that files  memory segments  cpu  and other resources can be operated on by only those processes that have gained proper authorization from the operating system for example  memory-addressing hardware ensures that a process can execute only within its own address space the timer ensures that no process can gain control of the cpu without eventually relinquishing control device-control registers are not accessible to users  so the integrity of the various peripheral devices is protected protection  then  is any mechanism for controlling the access of processes or users-to the resourcesdefined by a computer system this mechanism rni1st provide means to specify the confrols to be imposed and means to enforce the controls protection can improve reliability by detecting latent errors at the interfaces between component subsystems early detection of interface errors can often prevent contamination of a healthy subsystem by another subsystem that is 30 chapter 1 1.10 malfunctioning furthermore  an unprotected resource can not defend against use  or n isuse  by an unauthorized or incompetent user a protection-oriented system provides a means to distinguish between authorized and unauthorized usage  as we discuss in chapter 14 6 yt  ; terl _ca1lhave adequateprotection but still be prone to failure and ado_w inappr   priat ~ acs ~ s ~  consider a user whose authentication information  her means of identifying herself to the system  is stolen her data could be copied or deleted  even though file and memory protection are working it is the job of to defend a system from external and internal attacks such attacks spread across a huge range and include viruses and worms  denial-ofservice attacks  which use all of a system 's resources and so keep legitimate users out of the system   identity theft  and theft of service  unauthorized use of a system   prevention of some of these attacks is considered an operatingsystem function on some systems  while other systems leave the prevention to policy or additional software due to the alarming rise in security incidents  operating-system security features represent a fast-growing area of research and implementation security is discussed in chapter 15 protection and security require the system to be able to distinguish among all its users most maintain a list of user names and    in windows vista parlance  this is _ 1_ these numerical ids are unique  one per user when a user logs in system  the authentication stage determines the appropriate user id for the user that user id is associated with all of the user 's processes and threads when an id needs to be user readable  it is translated back to the user name via the user name list in some circumstances  we wish to distinguish among sets of users rather than individual users for example  the owner of a file on a unix system may be allowed to issue all operations on that file  whereas a selected set of users may only be allowed to read the file to accomplish this  we need to define a group name and the set of users belonging to that group group functionality can be implemented as a system-wide list of group names and ic'1entifiers a user can be in one or more groups  depending on operating-system design decisions the user 's group ids are also included in every associated process and thread in the course of normal use of a system  the user id and are s-l.iffici.e11t hov \ tever ; a user sometimes needs to to gain extra permissions for an activity the user may need access to a fhatis resh ; icted,for examp1e.operatmg systems provide various methods to allow privilege escalation on unix  for example  the setuid attribute on a program causes that program to run with the user id of the owner of the file  rather than the current user 's id the process runs with this until it turns off the extra privileges or terminates a distributed system is a collection of physically separate  possibly heterogeneous  computer systems that are networked to provide the users with access to the various resources that the system maintains access to a shared resource 1.10 31 increases computation speed  functionality  data availability  and reliability some operating systems generalize network access as a form of file access  with the details of networking contained in the network interface 's device driver others make users specifically invoke network functions generally  systems contain a mix of the two modes-for example ftp and nfs the protocols that create a distributed system can greatly affect that system 's utility and popularity a in the simplest terms  is a communication path between two or more systems distributed systems depend on networking for their functionality networks vary by the protocols used  the distances between nodes  and the transport media tcp /ip is the most common network protocol  although atm and other protocols are in widespread use likewise  operatingsystem support of protocols varies most operating systems support tcp /ip  including the windows and unix operating systems some systems support proprietary protocols to suit their needs to an operating system  a network protocol simply needs an interface device-a network adapter  for examplewith a device driver to manage it  as well as software to handle data these concepts are discussed throughout this book networks are characterized based on the distances between their nodes a computers within a room  a floor  or a building a n  usually links buildings  cities  or countries a global company may have a wan to com1ect its offices worldwide these networks may run one protocol or several protocols the continuing advent of new technologies brings about new forms of networks for example  a   '/ ! al i  could link buildings within '   a city bluetooth and 802.11 devices use wireless technology to commt.micate over a distance of several feet  in essence creating a such as might be found in a home the media to carry networks are equally varied they include copper wires  fiber strands  and wireless transmissions between satellites  microwave dishes  and radios when computing devices are connected to cellular phones  they create a network even very short-range infrared communication can be used for networking at a rudimentary level  whenever computers communicate  they use or create a network these networks also vary in their performance and reliability some operating systems have taken the concept of networks and distributed systems further than the notion of providing network connectivity a is an operating system that provides features such as file sharing across the network and that includes a communication scheme that allows different processes on different computers to exchange messages a computer rmming a network operating system acts autonomously from all other computers on the network  although it is aware of the network and is able to communicate with other networked computers a distributed operating system provides a less autonomous envirorunent  the different operating systems comm lmicate closely enough to provide the illusion that only a single operating system controls the network we cover computer networks and distributed systems in chapters 16 through 18 32 chapter 1 1.11 the discussion thus far has focused on the general-purpose computer systems that we are all familiar with there are  however  other classes of computer systems whose functions are more limited and whose objective is to deal with limited computation domains 1.11.1 real-time embedded systems embedded computers are the most prevalent form of computers in existence these devices are found everywhere  from car engines and manufacturing robots to dvds and microwave ovens they tend to have very specific tasks the systencs they run on are usually primitive  and so the operating systems provide limited features usually  they have little or no user interface  preferring to spend their time monitoring and managing hardware devices  such as automobile engines and robotic arms these embedded systems vary considerably some are general-purpose computers  running standard operating systems-such as unix-with special-purpose applications to implement the functionality others are hardware devices with a special-purpose embedded operating system providing just the functionality desired yet others are hardware devices with application-specific integrated circuits that perform their tasks without an operating system the use of embedded systems continues to expand the power of these devices  both as standalone units and as elements of networks and the web  is sure to increase as well even now  entire houses can be computerized  so that a central computer-either a general-purpose computer or an embedded system-can control heating and lighting  alarm systems  and even coffee makers web access can enable a home owner to tell the house to heat up before she arrives home someday  the refrigerator may call the grocery store when it notices the milk is gone embedded systems almost always run a real-time system is used when rigid time requirements been placed on the operation of a processor or the flow of data ; thus  it is often used as a control device in a dedicated application sensors bring data to the computer the computer must analyze the data and possibly adjust controls to modify the sensor inputs systems that control scientific experiments  medical imaging systems  industrial control systems  and certain display systems are realtime systems some automobile-engine fuel-injection systems  home-appliance controllers  and weapon systems are also real-time systems a real-time system has well-defined  fixed time constraints processing must be done within the defined constraints  or the system will fail for instance  it would not do for a robot arm to be instructed to halt after it had smashed into the car it was building a real-time system functions correctly only if it returns the correct result within its time constraints contrast this system with a time-sharing system  where it is desirable  but not mandatory  to respond quickly or a batch system  which may have no time constraints at all in chapter 19  we cover real-time embedded systems in great detail in chapter 5  we consider the scheduling facility needed to implement real-time functionality in an operating system in chapter 9  we describe the design 1.11 33 of memory management for real-time computing finally  in chapter 22  we describe the real-time components of the windows xp operating system 1.11.2 multimedia systems most operating systems are designed to handle conventional data such as text files  progran'ls  word-processing documents  and spreadsheets however  a recent trend in technology is the incorporation of multimedia data into computer systems multimedia data consist of audio and video files as well as conventional files these data differ from conventional data in that multimedia data-such as frames of video-must be delivered  streamed  according to certain time restrictions  for example  30 frames per second   multimedia describes a wide range of applications in popular use today these include audio files such as mp3  dvd movies  video conferencing  and short video clips of movie previews or news stories downloaded over the internet multimedia applications may also include live webcasts  broadcasting over the world wide web  of speeches or sporting events and even live webcams that allow a viewer in manhattan to observe customers at a cafe in paris multimedia applications need not be either audio or video ; rather  a multimedia application often includes a combination of both for example  a movie may consist of separate audio and video tracks nor must multimedia applications be delivered only to desktop personal computers increasingly  they are being directed toward smaller devices  including pdas and cellular telephones for example  a stock trader may have stock quotes delivered wirelessly and in real time to his pda in chapter 20  we explore the demands of multimedia applications  describe how multimedia data differ from conventional data  and explain how the nature of these data affects the design of operating systems that support the requirements of multimedia systems 1.11.3 handheld systems include personal digital assistants  pdas   such as palm and pocket-pes  and cellular telephones  many of which use special-purpose embedded operating systems developers of handheld systems and applications face many challenges  most of which are due to the limited size of such devices for example  a pda is typically about 5 inches in height and 3 inches in width  and it weighs less than one-half pound because of their size  most handheld devices have small amounts of memory  slow processors  and small display screens we take a look now at each of these limitations the amount of physical memory in a handheld depends on the device  but typically it is somewhere between 1 mb and 1 gb  contrast this with a typical pc or workstation  which may have several gigabytes of memory  as a result  the operating system and applications must manage memory efficiently this includes returning all allocated memory to the memory manager when the memory is not being used in chapter 9  we explore virtual memory  which allows developers to write programs that behave as if the system has more memory than is physically available currently  not many handheld devices use virtual memory techniques  so program developers must work within the confines of limited physical memory 34 chapter 1 1.12 a second issue of concern to developers of handheld devices is the speed of the processor used in the devices processors for most handheld devices run at a fraction of the speed of a processor in a pc faster processors require more power to include a faster processor in a handheld device would require a larger battery  which would take up more space and would have to be replaced  or recharged  more frequently most handheld devices use smaller  slower processors that consume less power therefore  the operating system and applications must be designed not to tax the processor the last issue confronting program designers for handheld devices is l/0 a lack of physical space limits input methods to small keyboards  handwriting recognition  or small screen-based keyboards the small display screens limit output options whereas a monitor for a home computer may measure up to 30 inches  the display for a handheld device is often no more than 3 inches square familiar tasks  such as reading e-mail and browsing web pages  must be condensed into smaller displays one approach for displaying the content in web pages is where only a small subset of a web page is delivered and displayed on the handheld device some handheld devices use wireless technology  such as bluetooth or 802.11  allowing remote access to e-mail and web browsing cellular telephones with connectivity to the internet fall into this category however  for pdas that do not provide wireless access  downloading data typically requires the user first to download the data to a pc or workstation and then download the data to the pda some pdas allow data to be directly copied from one device to another using an infrared link generally  the limitations in the functionality of pdas are balanced by their convenience and portability their use continues to expand as network com1ections become more available and other options  such as digital cameras and mp3 players  expand their utility so far  we have provided an overview of computer-system organization and major operating-system components we conclude with a brief overview of how these are used in a variety of computing environments 1.12.1 traditional computing as computing matures  the lines separating many of the traditional computing environments are blurring consider the typical office environment just a few years ago  this environment consisted of pcs connected to a network  with servers providing file and print services remote access was awkward  and portability was achieved by use of laptop computers terminals attached to mainframes were prevalent at many companies as well  with even fewer remote access and portability options the current trend is toward providing more ways to access these computing environments web technologies are stretching the boundaries of traditional computing companies establish which provide web accessibility to their internal servers ccxepu1as are essentially terminals that understand web-based computing handheld computers can synchronize with 1.12 35 pcs to allow very portable use of con1pany information handheld pdas can also connect to to use the company 's web portal  as well as the myriad other web resources   at home  most users had a single computer with a slow modem connection to the office  the internet  or both today  network-connection speeds once available only at great cost are relatively inexpensive  giving home users more access to more data these fast data connections are allowing home computers to serve up web pages and to run networks that include printers  client pcs  and servers some homes even have to protect their networks from security breaches those firewalls cost thousands of dollars a few years ago and did not even exist a decade ago in the latter half of the previous century  computing resources were scarce  before that  they were nonexistent !  for a period of time  systems were either batch or interactive batch systems processed jobs in bulk  with predetermined input  from files or other sources of data   interactive systems waited for input from users to optimize the use of the computing resources  multiple users shared time on these systems time-sharing systems used a timer and scheduling algorithms to rapidly cycle processes through the cpu  giving each user a share of the resources today  traditional time-sharing systems are uncommon the same scheduling technique is still in use on workstations and servers  but frequently the processes are all owned by the same user  or a single user and the operating system   user processes  and system processes that provide services to the user  are managed so that each frequently gets a slice of computer time consider the windows created while a user is working on a pc  for example  and the fact that they may be performing different tasks at the same time 1.12.2 client-server computing as pcs have become faste1 ~ more powerful  and cheaper  designers have shifted away from centralized system architecture terminals connected to centralized systems are now being supplanted by pcs correspondingly  user-interface functionality once handled directly by centralized systems is increasingly being handled by pcs as a result  many of today ' s systems act as to satisfy requests generated by this form of specialized distributed system  called a system  has the general structure depicted in figure 1.13 server systems can be broadly categorized as compute servers and file servers  figure 1.13 general structure of a client-server system 36 chapter 1 the provides an interface to which a client can send a request to perform an action  for example  read data  ; in response  the server executes the action and sends back results to the client a server running a database that responds to client requests for data is an example of such a system the provides a file-system interface where clients can create  update  read  and delete files an example of such a system is a web server that delivers files to clients running web browsers 1.12.3 peer-to-peer computing another structure for a distributed system is the peer-to-peer  p2p  system model in this model  clients and servers are not distinguished from one another ; instead  all nodes within the system are considered peers  and each ncay act as either a client or a server  depending on whether it is requesting or providing a service peer-to-peer systems offer an advantage over traditional client-server systems in a client-server system  the server is a bottleneck ; but in a peer-to-peer system  services can be provided by several nodes distributed throughout the network to participate in a peer-to-peer system  a node must first join the network of peers once a node has joined the network  it can begin providing services to-and requesting services from -other nodes in the network determining what services are available is accomplished in one of two general ways  when a node joins a network  it registers its service with a centralized lookup service on the network any node desiring a specific service first contacts this centralized lookup service to determine which node provides the service the remainder of the communication takes place between the client and the service provider a peer acting as a client must first discover what node provides a desired service by broadcasting a request for the service to all other nodes in the network the node  or nodes  providing that service responds to the peer making the request to support this approach  a discovery protocol must be provided that allows peers to discover services provided by other peers in the network peer-to-peer networks gained widespread popularity in the late 1990s with several file-sharing services  such as napster and gnutella  that enable peers to exchange files with one another the napster system uses an approach similar to the first type described above  a centralized server maintains an index of all files stored on peer nodes in the napster network  and the actual exchanging of files takes place between the peer nodes the gnutella system uses a technique similar to the second type  a client broadcasts file requests to other nodes in the system  and nodes that can service the request respond directly to the client the future of exchanging files remains uncertain because many of the files are copyrighted  music  for example   and there are laws governing the distribution of copyrighted material in any case  though  peerto peer technology undoubtedly will play a role in the future of many services  such as searching  file exchange  and e-mail 1.13 1.13 37 1.12.4 web-based computing the web has become ubiquitous/ leading to more access by a wider variety of devices than was dreamt of a few years ago pcs are still the most prevalent access devices/ with workstations/ handheld pdas1 and even cell phones also providing access web computing has increased the emphasis on networking devices that were not previously networked now include wired or wireless access devices that were networked now have faster network connectivity/ provided by either improved networking technology optimized network implementation code/ or both the implementation of web-based computing has given rise to new categories of devices/ such as which distribute network connections an1.ong a pool of similar servers operating systems like windows 951 which acted as web clients/ have evolved into linux and windows xp 1 which can act as web servers as well as clients generally/ the web has increased the complexity of devices because their users require them to be web-enabled the study of operating systems/ as noted earlier/ is made easier by the availability of a vast number of open-source releases are those made available in source-code format rather than as compiled binary code linux is the most famous open source operating system  while microsoft windows is a well-known example of the opposite dosedapproach starting with the source code allows the programmer to produce binary code that can be executed on a system doing the oppositethe source code from the binaries-is quite a lot of work1 and useful items such as comments are never recovered learning operating systems by examining the actual source code1 rather than reading summaries of that code/ can be extremely useful with the source code in hand/ a student can modify the operating system and then compile and nm the code to try out those changes1 which is another excellent learning tool this text indudes projects that involve modifying operating system source code/ while also describing algorithms at a high level to be sure all important operating system topics are covered throughout the text1 we provide pointers to examples of open-source code for deeper study there are many benefits to open-source operating systems/ including a commtmity of interested  and usually unpaid  programmers who contribute to the code by helping to debug it analyze it/ provide support/ and suggest changes arguably/ open-source code is more secure than closed-source code because many more eyes are viewing the code certainly open-source code has bugs/ but open-source advocates argue that bugs tend to be found and fixed faster owing to the number of people using and viewing the code companies that earn revenue from selling their programs tend to be hesitant to open-source their code/ but red hat/ suse1 sun/ and a myriad of other companies are doing just that and showing that commercial companies benefit/ rather than suffer/ when they open-source their code revenue can be generated through support contracts and the sale of hardware on which the software runs/ for example 38 chapter 1 1.13.1 history in the early days of modern computing  that is  the 1950s   a great deal of software was available in open-source format the original hackers  computer enthusiasts  at mit 's tech model railroad club left their programs in drawers for others to work on homebrew user groups exchanged code during their meetings later  company-specific user groups  such as digital equipment corporation 's dec  accepted contributions of source-code programs  collected them onto tapes  and distributed the tapes to interested ncembers computer and software companies eventually sought to limit the use of their software to authorized computers and paying customers releasing only the binary files compiled from the source code  rather than the source code itself  helped them to achieve this goal  as well as protecting their code and their ideas from their competitors another issue involved copyrighted material operating systems and other programs can limit the ability to play back movies and music or display electronic books to authorized computers such or digital would not be effective if the source code that implemented these limits were published laws in many countries  including the u.s digital millennium copyright act  dmca   make it illegal to reverse-engineer drm code or otherwise try to circumvent copy protection to counter the move to limit software use and redistribution  richard stallman in 1983 started the gnu project to create a free  open-source unixcompatible operating system in 1985  he published the gnu manifesto  which argues that all software should be free and open-sourced he also formed the with the goal of encouraging the free exchange of software source code and the free use of that software rather than copyright its software  the fsf copylefts the software to encourage sharing and improvement the gercera  ! codifies copylefting and is a common license under which free software is released ftmdamentally  gpl requires that the source code be distributed with any binaries and that any changes made to the source code be released under the same gpl license 1.13.2 linux as an example of an open-source operating system  consider the gnu project produced many unix-compatible tools  including compilers  editors  and utilities  but never released a kernel in 1991  a student in finland  linus torvalds  released a rudimentary unix-like kernel using the gnu compilers and tools and invited contributions worldwide the advent of the internet meant that anyone interested could download the source code  modify it  and submit changes to torvalds releasing updates once a week allowed this so-called linux operating system to grow rapidly  enhanced by several thousand programmers the gnu /linux operating system has spawned hundreds of unique or custom builds  of the system major distributions include redhat  suse  fedora  debian  slackware  and ubuntu distributions vary in function  utility  installed applications  hardware support  user interface  and purpose for example  redhat enterprise lim1x is geared to large commercial use pclinuxos is a  jvc  cd-an operating system that can be booted and run from a cd-rom without being installed on a system 's hard 1.13 39 disk one variant of pclinuxos  pclinuxos supergamer dvd  is a that includes graphics drivers and games a gamer can run it on any compatible system simply by booting from the dvd when the gamer is finished  a reboot of the system resets it to its installed operating system access to the linux source code varies by release here  we consider ubuntu linux ubuntu is a popular linux distribution that comes in a variety of types  including those tuned for desktops  servers  and students its founder pays for the printing and mailing of dvds containing the binary and source code  which helps to make it popular   the following steps outline a way to explore the ubuntu kernel source code on systems that support the free vmware player tool  download the player from uncompress and untar the downloaded file via tar xj f linux 2.6.18.1.tar.bz2 explore the source code of the ubuntu kernel  which is now in  /linux 2 6.18 .1 for more about linux  see chapter 21 for more about virtual machines  see section 2.8 1.13.3 bsd unix has a longer and more complicated history than linux it started in 1978 as a derivative of at&t 's unix releases from the university of california at berkeley  ucb  came in source and binary form  but they were not opensource because a license from at&t was required bsd unix 's development was slowed by a lawsuit by at&t  but eventually a fully functional  open-source version  4.4bsd-lite  was released in 1994 just as with lim.ix  there are many distributions of bsd unix  including freebsd  netbsd  openbsd  and dragonflybsd to explore the source code of freebsd  simply download the virtual machine image of the version of interest and boot it within vmware  as described above for ubuntu linux the source code comes with the distribution and is stored in /usr i src/ the kernel source code is in /usr/src/sys for example  to examine the virtual-memory implementation code in the freebsd kernel  see the files in /usr/src/sys/vrn darwin  the core kernel component of mac  is based on bsd unix and is open-sourced as well that source code is available from http  i /www opensource apple corn/ darwinsource/ every mac release 40 chapter 1 1.14 has its open-source components posted at that site the name of the package that contains the kernel is xnu the source code for mac kernel revision 1228  the source code to mac leopard  can be found at www.opensource.apple.coml darwinsource i tar balls i apsll xnu-1228 tar.gz appl is the commercial unix-based operating system of sun microsystems originally  sun 's operating system was based on bsd unix sun moved to at&t 's system v unix as its base in 1991 in 2005  sun open-sourced some of the solaris code  and over time  the company has added more and more to that open-source code base unfortunately  not all of solaris is open-sourced  because some of the code is still owned by at&t and other companies however  solaris can be compiled from the open source and linked with binaries of the close-sourced components  so it can still be explored  modified  compiled  and tested the source code is available from many of these projects open-source projects enable students to use source code as a learning tool they can modify programs and test them  help find and fix bugs  and otherwise explore mature  full-featured operating systems  compilers  tools  user interfaces  and other types of programs the availability of source code for historic projects  such as multics  can help students to understand those projects and to build knowledge that will help in the implementation of new projects gnu ilinux  bsd unix  and solaris are all open-source operating systems  but each has its own goals  utility  licensing  and purpose sometimes licenses are not mutually exclusive and cross-pollination occurs  allowing rapid improvements in operating-system projects for example  several major components of solaris have been ported to bsd unix the advantages of free software and open sourcing are likely to increase the number and quality of open-source projects  leading to an increase in the number of individuals and companies that use these projects an operating system is software that manages the cornputer hardware  as well as providing an environment for application programs to run perhaps the 1.14 41 most visible aspect of an operating system is the interface to the computer system it provides to the human user for a computer to do its job of executing programs  the program.s must be in main memory main memory is the only large storage area that the processor can access directly it is an array of words or bytes  ranging in size from millions to billions each word in memory has its own address the main mem.ory is usually a volatile storage device that loses its contents when power is turned off or lost most computer systems provide secondary storage as an extension of main memory secondary storage provides a form of nonvolatile storage that is capable of holding large quantities of data permanently the most common secondary-storage device is a magnetic disk  which provides storage of both programs and data the wide variety of storage systems in a computer system can be organized in a hierarchy according to speed and cost the higher levels are expensive  but they are fast as we move down the hierarchy  the cost per bit generally decreases  whereas the access time generally increases there are several different strategies for designing a computer system uniprocessor systems have only a single processor  while multiprocessor systems contain two or more processors that share physical memory and peripheral devices the most common multiprocessor design is symmetric multiprocessing  or smp   where all processors are considered peers and run independently of one another clustered systems are a specialized form of multiprocessor systems and consist of multiple computer systems connected by a local area network to best utilize the cpu  modern operating systems employ multiprogramming  which allows several jobs to be in memory at the same time  thus ensuring that the cpu always has a job to execute time-sharing systems are an extension of multiprogramming wherein cpu scheduling algorithms rapidly switch between jobs  thus providing the illusion that each job is nmning concurrently the operating system must ensure correct operation of the computer system to prevent user programs from interfering with the proper operation of the system  the hardware has two modes  user mode and kernel mode various instructions  such as i/0 instructions and halt instructions  are privileged and can be executed only in kernel mode the memory in which the operating system resides must also be protected from modification by the user a tin1.er prevents infinite loops these facilities  dual mode  privileged instructions  memory protection  and timer interrupt  are basic building blocks used by operating systems to achieve correct operation a process  or job  is the fundamental unit of work in an operating system process management includes creating and deleting processes and providing mechanisms for processes to communicate and synchronize with each other an operating system manages memory by keeping track of what parts of memory are being used and by whom the operating system is also responsible for dynamically allocating and freeing memory space storage space is also managed by the operating system ; this includes providing file systems for representing files and directories and managing space on mass-storage devices operating systems must also be concerned with protecting and securing the operating system and users protection measures are mechanisms that control the access of processes or users to the resources made available by the 42 chapter 1 computer system security measures are responsible for defending a computer system from external or internal attacks distributed systems allow users to share resources on geographically dispersed hosts connected via a computer network services may be provided through either the client-server model or the peer-to-peer n10del in a clustered system  multiple machines can perform computations on data residing on shared storage  and computing can continue even when some subset of cluster members fails lans and wans are the two basic types of networks lans enable processors distributed over a small geographical area to communicate  whereas wans allow processors distributed over a larger area to communicate lans typically are faster than wans there are several computer systems that serve specific purposes these include real-time operating systems designed for embedded environments such as consumer devices  automobiles  and robotics real-time operating systems have well-defined  fixed-time constraints processing must be done within the defined constraints  or the system will fail multimedia systems involve the delivery of multimedia data and often have special requirements of displaying or playing audio  video  or synchronized audio and video streams recently  the influence of the internet and the world wide web has encouraged the development of operating systems that include web browsers and networking and communication software as integral features the free software movement has created thousands of open-source projects  including operating systems because of these projects  students are able to use source code as a learning tool they can modify programs and test them  help find and fix bugs  and otherwise explore mature  full-featured operating systems  compilers  tools  user interfaces  and other types of programs gnu /linux  bsd unix  and solaris are all open-source operating systems the advantages of free software and open sourcing are likely to increase the number and quality of open-source projects  leadi.j.1.g to an increase in the number of individuals and companies that use these projects 1.1 how are network computers different from traditional personal computers describe some usage scenarios in which it is advantageous to use network computers 1.2 what network configuration would best suit the following environments a a dormitory floor b a university campus c a state d a nation 43 1.3 give two reasons why caches are useful what problems do they solve vvbat problems do they cause if a cache can be made as large as the device for which it is caching  for instance  a cache as large as a disk   why not make it that large and eliminate the device 1.4 under what circumstances would a user be better off using a timesharing system rather than a pc or a single-user workstation 1.5 list the four steps that are necessary to run a program on a completely dedicated machine-a computer that is running only that program 1.6 how does the distinction between kernel mode and user mode function as a rudimentary form of protection  security  system 1.7 in a multiprogramming and time-sharing environment  several users share the system simultaneously this situation can result in various security problems a what are two such problems b can we ensure the same degree of security in a time-shared machine as in a dedicated machine explain your answer 1.8 describe a mechanism for enforcing memory protection in order to prevent a program from modifying the memory associated with other programs 1.9 what are the tradeoffs inherent in handheld computers 1.10 distinguish between the client-server and peer-to-peer models of distributed systems 1.11 some computer systems do not provide a privileged mode of operation in hardware is it possible to construct a secure operating system for these computer systems give arguments both that it is and that it is not possible 1.12 what are the main differences between operating systems for mainframe computers and personal computers 1.13 which of the following instructions should be privileged a set value of timer b read the clock 44 chapter 1 c clear memory d issue a trap instruction e turn off interrupts f modify entries in device-status table g switch from user to kernel mode h access i/o device 1.14 discuss  with examples  how the problem of maintaining coherence of cached data manifests itself in the following processing environments  a single-processor systems b multiprocessor systems c distributed systems 1.15 identify several advantages and several disadvantages of open-source operating systems include the types of people who would find each aspect to be an advantage or a disadvantage 1.16 how do clustered systems differ from multiprocessor systems what is required for two machines belonging to a cluster to cooperate to provide a highly available service 1.17 what is the main difficulty that a programmer must overcome in writing an operating system for a real-time environment 1.18 direct memory access is used for high-speed i/o devices in order to avoid increasing the cpu 's execution load a how does the cpu interface with the device to coordinate the transfer b how does the cpu know when the memory operations are complete c the cpu is allowed to execute other programs while the dma controller is transferring data does this process interfere with the execution of the user programs if so  describe what forms of interference are caused 1.19 identify which of the functionalities listed below need to be supported by the operating system for  a  handheld devices and  b  real-time systems a batch programming b virtual memory c time sharing 45 1.20 some cpus provide for more than two modes of operation what are two possible uses of these multiple modes 1.21 define the essential properties of the following types of operating systems  a batch b interactive c time sharing d real time e network f parallel a distributed b h clustered 1 handheld 1.22 describe the differences between symmetric and asymmetric multiprocessing what are three advantages and one disadvantage of multiprocessor systems 1.23 the issue of resource utilization shows up in different forms in different types of operating systems list what resources must be managed carefully in the following settings  a mainframe or minicomputer systems b workstations connected to servers c handheld computers 1.24 what is the purpose of interrupts what are the differences between a trap and an interrupt can traps be generated intentionally by a user program if so  for what purpose 1.25 consider an smp system sincilar to what is shown in figure 1.6 illustrate with an example how data residing in memory could in fact have two different values in each of the local caches 1.26 consider a computing cluster consisting of two nodes running a database describe two ways in which the cluster software can manage access to the data on the disk discuss the benefits and disadvantages of each 46 chapter 1 brookshear  2003  provides an overview of computer science in general an overview of the linux operating system is presented in bovet and cesati  2006   solomon and russinovich  2000  give an overview of microsoft windows and considerable technical detail abmrt the systern internals and components russinovich and solomon  2005  update this information to windows server 2003 and windows xp mcdougall and mauro  2007  cover the internals of the solaris operating system mac os x is presented at http  i /www apple com/macosx mac os x internals are discussed in singh  2007   coverage of peer-to-peer systems includes parameswaran et al  2001   gong  2002   ripeanu et al  2002   agre  2003   balakrishnan et al  2003   and loo  2003   a discussion of peer-to-peer file-sharing systems can be found in lee  2003   good coverage of cluster computing is provided by buyya  1999   recent advances in cluster computing are described by ahmed  2000   a survey of issues relating to operating-system support for distributed systems can be found in tanenbaum and van renesse  1985   many general textbooks cover operating systems  including stallings  2000b   nutt  2004   and tanenbaum  2001   hamacher et al  2002  describe cmnputer organization  and mcdougall and laudon  2006  discuss multicore processors hennessy and patterson  2007  provide coverage of i/o systems and buses  and of system architecture in general blaauw and brooks  1997  describe details of the architecture of many computer systems  including several from ibm stokes  2007  provides an illustrated introduction to microprocessors and computer architecture cache memories  including associative memory  are described and analyzed by smith  1982   that paper also includes an extensive bibliography on the subject discussions concerning magnetic-disk technology are presented by freedman  1983  and by harker et al  1981   optical disks are covered by kenville  1982   fujitani  1984   o'leary and kitts  1985   gait  1988   and olsen and kenley  1989   discussions of floppy disks are offered by pechura and schoeffler  1983  and by sarisky  1983   general discussions concerning mass-storage technology are offered by chi  1982  and by hoagland  1985   kurose and ross  2005  and tanenbaum  2003  provide general overviews of computer networks fortier  1989  presents a detailed discussion of networking hardware and software kozierok  2005  discuss tcp in detail mullender  1993  provides an overview of distributed systems  2003  discusses recent developments in developing embedded systems issues related to handheld devices can be found in myers and beigl  2003  and dipietro and mancini  2003   a full discussion of the history of open sourcing and its benefits and challenges is found in raymond  1999   the history of hacking is discussed in levy  1994   the free software foundation has published its philosophy on its web site  detailed instructions on how to build the ubuntu linux kernel are on 47 http  i /www howtof orge com/kernelcompilation_ubuntu the open-source components of mac are available from 2.1 an operating system provides the environment within which programs are executed internally  operating systems vary greatly in their makeup  since they are organized along many different lines the design of a new operating system is a major task it is important that the goals of the system be well defined before the design begins these goals form the basis for choices among various algorithms and strategies we can view an operating system from several vantage points one view focuses on the services that the system provides ; another  on the interface that it makes available to users and programmers ; a third  on its components and their interconnections in this chapter  we explore all three aspects of operating systems  showin.g the viewpoints of users  programmers  and operating-system designers we consider what services an operating system provides  how they are provided  how they are debugged  and what the various methodologies are for designing such systems finally  we describe how operating systems are created and how a computer starts its operating system to describe the services an operating system provides to users  processes  and other systems to discuss the various ways of structuring an operating system to explain how operating systems are installed and customized and how they boot an operating system provides an environment for the execution of programs it provides certain services to programs and to the users of those programs the specific services provided  of course  differ from one operating system to another  but we can identify common classes these operating-system services are provided for the convenience of the programmer  to n1.ake the programming 49 50 chapter 2 user and other system programs hardware figure 2 i a view of operating system services task easier figure 2.1 shows one view of the various operating-system services and how they interrelate one set of operating-system services provides functions that are helpfuj to the user ~ ~ user interface almost all operating systems have a this interface can take several forms one is a dcfr '  c ; ~  which uses text commands and a method for entering them  say  a program to allow entering and editing of commands   another is a batch in which commands and directives to control those commands are entered into files  and those files are executed most commonly  a is used here  the interface is a window system with a pointing device to direct i/0  choose from menus  and make selections and a keyboard to enter text some systems provide two or all three of these variations program execution the system must be able to load a program into memory and to run that program the program must be able to end its execution  either normally or abnormally  indicating error   i/o operations a running program may require i/0  which may involve a file or an i/0 device for specific devices  special functions may be desired  such as recording to a cd or dvd drive or blanking a display screen   for efficiency and protection  users usually can not control i/0 devices directly therefore  the operating system must provide a means to do i/0 file-system manipulation the file system is of particular interest obviously  programs need to read and write files and directories they also need to create and delete them by name  search for a given file  and list file information finally  some programs include permissions management to allow or deny access to files or directories based on file ownership many operating systems provide a variety of file systems  sometimes to allow personal choice  and sometimes to provide specific features or performance characteristics 2.1 51 communications there are many circumstances in which one process needs to exchange information with another process such communication ncay occur between processes that are executing on the same computer or between processes that are executing on different computer systems tied together by a computer network communications may be implemented via shared rnenwry or through message passing  in which packets of information are moved between processes by the operating system error detection the operating system needs to be constantly aware of possible errors errors may occur in the cpu and memory hardware  such as a memory error or a power failure   in i/0 devices  such as a parity error on tape  a connection failure on a network  or lack of paper in the printer   and in the user program  such as an arithmetic overflow  an attempt to access an illegal memory location  or a too-great use of cpu time   for each type of error  the operating system should take the appropriate action to ensure correct and consistent computing of course  there is variation in how operating systems react to and correct errors debugging facilities can greatly enhance the user 's and programmer 's abilities to use the system efficiently another set of operating-system functions exists not for helping the user but rather for ensuring the efficient operation of the system itself systems with multiple users can gain efficiency by sharing the computer resources among the users resource allocation when there are i  lultiple usersormultiple jobs rmuung at the sametime  resources must be allocated to each of them many d1herent -types of resources are managed by the operating system some  such as cpu cycles  main memory  and file storage  may have special allocation code  whereas others  such as i/0 devices  may have much more general request and release code for instance  in determining how best to use the cpu  operating systems have cpu-scheduling routines that take into account the speed of the cpu  the jobs that must be executed  the number of registers available  and other factors there may also be routines to allocate printers  modems  usb storage drives  and other peripheral devices accounting vl  e want to_keeptrack of whichusers use  lovy rnl1c  hand what kindsofcomputer resources this record keeping may be used for accoun  tii1g  so thai  users can be billed  or simply for accumulating usage statistics usage statistics may be a valuable tool for researchers who wish to reconfigure the system to improve computing services protection and security the owners of information stored in a multiuser or networked computer system may want to control use of that information when several separate processes execute concurrently  it ~ hould not be possible for one process to interfere with the others or with the operating system itself protection iiwolves ensuring that all access to systerr1 resources 1s -controlled security of the system from outsiders is also important such security starts with requiring each user to authenticate himself or herself to the system  usually by means of a password  to gain access to system resources it extends to defending external i/0 devices  52 chapter 2 2.2 including modems and network adapters  from invalid access attempts and to recording all such connections for detection of break-ins if a system is to be protected and secure  precautions must be instituted throughout it a chain is only as strong as its weakest link we mentioned earlier that there are several ways for users to interface with the operating system here  we discuss two fundamental approaches one provides a command-line interface  or that allows users to directly enter commands to be performed by the operating system the other allows users to interface with the operating system via a graphical user interface  or gui 2.2.1 command interpreter some operating systems include the command interpreter in the kernel others  such as windows xp and unix  treat the command interpreter as a special program that is rmming when a job is initiated or when a user first logs on  on interactive systems   on systems with multiple command interpreters to choose from  the interpreters are known as shells for example  on unix and linux systems  a user may choose among several different shells  including the bourne shell  c shell  bourne-again shell  korn shell  and others third-party shells and free user-written shells are also available most shells provide similar functionality  and a user 's choice of which shell to use is generally based on personal preference figure 2.2 shows the bourne shell command interpreter being used on solaris 10 the main function of the command interpreter is to get and execute the next user-specified command many of the commands given at this level manipulate files  create  delete  list  print  copy  execute  and so on the ms-dos and unix shells operate in this way these commands can be implemented in two general ways in one approach  the command interpreter itself contains the code to execute the command for example  a command to delete a file may cause the command interpreter to jump to a section of its code that sets up the parameters and makes the appropriate system call in this case  the number of comn'lands that can be given determines the size of the command interpreter  since each command requires its own implementing code an alternative approach -used by unix  among other operating systems -implements most commands through system programs in this case  the command interpreter does not understand the cmnmand in any way ; it merely uses the command to identify a file to be loaded into memory and executed thus  the unix command to delete a file rm file.txt would search for a file called rm  load the file into memory  and execute it with the parameter file txt the function associated with the rm command would be defined completely by the code in the file rm in this way  programmers can add new commands to the system easily by creating new files with the proper 0.0 0.0 r/s 0.0 0.6 console 2.2 0.2 0.0 0.2 0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 extended device statistics w/s 0.0 0.0 kr./s klv/s 0.0 0.0 0.0 1 ogi nell idle 1sj un0718days wai t actv svc_t 9  tn 1i ~ b 0.0 0.0 0.0 0 0 0 0 0 load average  0.09  0.11  8.66 jcpu pcpu what 1 /usr/bin/ssh-agent  /usr/bi 18 4 w figure 2.2 the bourne shell command interpreter in solaris i 0 53 names the command-interpreter program  which can be small  does not have to be changed for new commands to be added 2.2.2 graphical user interfaces a second strategy for interfacing with the operating system is through a userfriendly graphical user interface  or cui here  rather than entering commands directly via a command-line interface  users employ a mouse-based windowand nl.enu system characterized by a metaphor the user moves the mouse to position its pointer on images  or on the screen  the desktop  that represent programs  files  directories  and system functions depending on the mouse pointer 's location  clicking a button on the mouse can invoke a program  select a file or directory-known as a folder-or pull down a menu that contains commands graphical user interfaces first appeared due in part to research taking place in the early 1970s at xerox parc research facility the first cui appeared on the xerox alto computer in 1973 however  graphical interfaces became more widespread with the advent of apple macintosh computers in the 1980s the user interface for the macintosh operating system  mac os  has undergone various changes over the years  the most significant being the adoption of the aqua interface that appeared with mac os x microsoft 's first version of windows-version 1.0-was based on the addition of a cui interface to the ms-dos operating system later versions of windows have made cosmetic changes in the appearance of the cui along with several enhancements in its functionality  including windows explorer 54 chapter 2 traditionally  unix systencs have been dominated by command-line interfaces various gul interfaces are available  however  including the common desktop environment  cde  and x-windows systems  which are common on commercial versions of unix  such as solaris and ibm 's aix system in addition  there has been significant development in gui designs from various projects  such as i desktop environment  or kde  and the gnome desktop by the gnu project both the kde and gnome desktops run on linux and various unix systems and are available under open-source licenses  which means their source code is readily available for reading and for modification under specific license terms the choice of whether to use a command-line or gui interface is mostly one of personal preference as a very general rule  many unix users prefer command-line interfaces  as they often provide powerful shell interfaces in contrast  most windows users are pleased to use the windows gui environment and almost never use the ms-dos shell interface the various changes undergone by the macintosh operating systems provide a nice study in contrast historically  mac os has not provided a command-line interface  always requiring its users to interface with the operating system using its gui however  with the release of mac os x  which is in part implemented using a unix kernel   the operating system now provides both a new aqua interface and a command-line interface figure 2.3 is a screenshot of the mac os x gui the user interface can vary from system to system and even from user to user within a system it typically is substantially removed from the actual system structure the design of a useful and friendly user interface is therefore figure 2.3 the mac os x gui 2.3 2.3 55 not a direct function of the operating systenc in this book  we concentrate on the fundamental problems of providing adequate service to user programs from the point of view of the operating system  we do not distinguish between user programs and systern programs system calls provide an interface to the services made available by an operating system these calls are generally available as routines written in c and c + +  although certain low-level tasks  for example  tasks where hardware must be accessed directly   may need to be written using assembly-language instructions before we discuss how an operating system makes system calls available  let 's first use an example to illustrate how system calls are used  writing a simple program to read data from one file and copy them to another file the first input that the program will need is the names of the two files  the input file and the output file these names can be specified in many ways  depending on the operating-system design one approach is for the program to ask the user for the names of the two files in an interactive system  this approach will require a sequence of system calls  first to write a prompting message on the screen and then to read from the keyboard the characters that define the two files on mouse-based and icon-based systems  a menu of file names is usually displayed in a window the user can then use the mouse to select the source name  and a window can be opened for the destination name to be specified this sequence requires many i/0 system calls once the two file names are obtained  the program must open the input file and create the output file each of these operations requires another system call there are also possible error conditions for each operation when the program tries to open the input file  it may find that there is no file of that name or that the file is protected against access in these cases  the program should print a message on the console  another sequence of system calls  and then terminate abnormally  another system call   if the input file exists  then we must create a new output file we may find that there is already an output file with the same name this situation may cause the program to abort  a system call   or we may delete the existing file  another system call  and create a new one  another system call   another option  in an interactive system  is to ask the user  via a sequence of system calls to output the prompting message and to read the response from the termin.al  whether to replace the existing file or to abort the program now that both files are set up  we enter a loop that reads from the input file  a system call  and writes to the output file  another system call   each read and write must return status information regarding various possible error conditions on input  the program may find that the end of the file has been reached or that there was a hardware failure in the read  such as a parity error   the write operation may encounter various errors  depending on the output device  no more disk space  printer out of paper  and so on   finally  after the entire file is copied  the program may close both files  another system call   write a message to the console or window  more system calls   and finally terminate normally  the final system call   as we 56 chapter 2 can see1 even simple programs may make heavy use of the operating system frequently/ systems execute thousands of system calls per second this systemcall sequence is shown in figure 2a most programmers never see this level of detail however typically/ applicatiol1 developers design program.s accordir1g to an   ~ jl ~ j'i   tl1e aj'ispecifies a set of functions application programmer/ including the parameters that are passed to each function and the return values the programmer can expect three of the most common apis available to application programmers are the win32 api for windows systems  the posix api for posix-based systems  which include virtually all versions of unix  linux/ and mac os x   and the java api for designing programs that run on the java virtual machine note that-unless specified -the system-call names used throughout this text are generic examples each operating system has its own name for each system call behind the scenes/ the functions that make up an api typically invoke the actual system calls on behalf of the application programmer for example  the win32 function createprocess    which unsurprisingly is used to create a new process  actually calls the ntcreateprocess   system call in the windows kernel why would an application programnl.er prefer programming according to an api rather than invoking actual system calls there are several reasons for doing so one benefit of programming according to an api concerns program portability  an application programmer designing a program using an api can expect her program to compile and run on any system that supports the same api  although in reality/ architectural differences often make this more difficult than it may appear   furthermore/ actual system calls can often be more detailed and difficult to work with than the api available to an application programmer regardless/ there often exists a strong correlation between a function in the api and its associated system call within the kernel example system call sequence acquire input file name write prompt to screen accept input acquire output file name write prompt to screen accept input open the input file if file does n't exist  abort create output file if file exists  abort loop read from input file write to output file until read fails close output file write completion message to screen terminate normally figure 2.4 example of how system calls are used 2.3 example of standard api as an example of a standard apt  consider the readfile 0 unction in the win32 api-a function for reading rom a file the api for this function appears in figure 2.5   return value ~ bool readfile c t function name  handle lpvoid dword lpdword lpoverlapped file  ~ buffer  bytes to read  parameters bytes read  ovl  ; figure 2.5 the api for the readfile   function a description of the parameters passed to readfile 0 is as follows  handle file-the file to be read lpvoid buffer-a buffer where the data will be read into and written from dword bytestoread-the number of bytes to be read into the buffer lpdword bytesread -the number of bytes read during the last read lpoverlapped ovl-indicates if overlapped i/0 is being used 57 in fact  many of the posix and win32 apis are similar to the native system calls provided by the unix  linux  and windows operating systems the run-time support system  a set of functions built into libraries included with a compiler  for most programming languages provides a system-call interface that serves as the link to system calls made available by the operating system the system-call interface intercepts function calls in the api and invokes the necessary system calls within the operating system typically  a number is associated with each system call  and the system-call interface maintains a table indexed according to these nun'lbers the system call interface then invokes the intended system call in the operating-system kernel and returns the status of the system call and any return values the caller need know nothing about how the system call is implemented or what it does during execution rathel ~ it need only obey the api and understand what the operating system will do as a result of the execution of that system call thus  most of the details of the operating-system interface are hidden from the programmer by the api and are managed by the run-time support library the relationship between an api  the system-call interface  and the operating 58 chapter 2 2.4 user mode kernel mode user application opeo    j open   implementation of open   system call return figure 2.6 the handling of a user application invoking the open   system call system is shown in figure 2.6  which illustrates how the operating system handles a user application invoking the open   system call system calls occur in different ways  depending onthe coj  rlpl1te.rjjlll e often  more information is required than simply the identity of the desired system call the exact type and ammmt of information vary according to the particular operating system and call for example  to get input  we may need to specify the file or device to use as the source  as well as the address and length of the memory buffer into which the input should be read of course  the device or file and length may be implicit in the call three general methods are used to pass parameters to the operating system the simplest approach is to pass the param.eters in registers in some cases  however  there may be more parameters than registers in these cases  the parameters are generally stored in a block  or table  in memory  and the address of the block is passed as a parameter in a register  figure 2.7   this is the approach taken by linux and solaris parameters also can be placed  or pushed  onto the stack by the program and popped oh the stacl  by the operatirl  g ~ yste111  some operating syste1ns prefer the block or stack method because those approaches do not limit the number or length of parameters being passed system calls can be grouped ~ oughly intc  six major categories  process control  file manipujation  device manipulation  information maintenance  coinmuiii ~ a1ioii ~ ;  lndpr   tediol   in seci  lo  ri.s 2.4.l ~ hi.i = o ~ l.gli 2.l  6 ~ we discllss briefly the types of system calls that may be provided by an operating system most of these system calls support  or are supported by  concepts and functions x  parameters for call load address x system call 13 +  ~  user program 2.4 register operating system figure 2.7 passing of parameters as a table 59 that are discussed in later chapters figure 2.8 summarizes the types of system calls normally provided by an operating system 2.4.1 process control a running program needs to be able to halt its execution either normally  end  or abnormally  abort   if a system call is made to terminate the currently ruru1il1g program abnormally  or if the program runs into a problem and causes an error trap  a dump of memory is sometimes taken and an error message generated the dump is written to disk and may be examined by a system program designed to aid the programmer in finding and correcting bugs-to determine the cause of the problem under either normal or abnormal circumstances  the operating system must transfer control to the invoking command mterpreter the command interpreter then reads the next cominand in an interactive system  the command interpreter simply continues with the next command ; it is assumed that the user will issue an appropriate command to respond to any error in a gui system  a pop-up wmdow might alert the user to the error and ask for guidance in a batch system  the command interpreter usually terminates the entire job and continues with the next job some systems allow control cards to indicate special recovery actions in case an error occurs a is a batch-system concept it is a command to manage the execution of a process if the program discovers an error in its input and wants to terminate abnormally  it may also want to define an error level more severe errors can be indicated by a higher-level error parameter it is then possible to combi11e normal and abnormal termination by defining a normal termination as an error at level 0 the command interpreter or a following program can use this error level to determine the next action automatically a process or jobexecuting one p !   gral11_11l  ly _ \ ; \ '  ll1tto joad andexecut ~ anoteer pro-gra1  n  . th1s feafl  11  e allows the cmnmand i11terpreter to execute a program as directed by  for example  a user command  the click of a mouse  or a batch command an interesting question is where to return control when the loaded program terminates this question is related to the problem of 60 chapter 2 process control o end  abort o load  execute o create process  terminate process o get process attributes  set process attributes o wait for time o wait event  signal event o allocate and free memory file management o create file  delete file o open  close o read  write  reposition o get file attributes  set file attributes e  device management o request device  release device o read  write  reposition o get device attributes  set device attributes o logically attach or detach devices information maintenance o get time or date  set time or date o get system data  set system data o get process  file  or device attributes o set process  file  or device attributes communications o create  delete communication connection o send  receive messages o transfer status information o attach or detach remote devices figure 2.8 types of system calls whether the existing program is lost  saved  or allowed to continue execution concurrently with the new program if control returns to the existing program when the new program terminates  we must save the memory image of the existing program ; thus  we have effectively created a mechanism for one program to call another program if both programs continue concurrently  we have created a new job or process to 2.4 61 examples of windows and unix system calls windows unix process createprocesso fork   control exi tprocess   exit   waitforsingleobject   wait   file createfile   open   manipulation readfile   read   writefile   write   closehandle   close   device setconsolemode   ioctl   manipulation readconsole   read   writeconsole   write   information getcurrentprocessid   getpid   maintenance settimero alarm   sleep   sleep   communication createpipe   pipe   createfilemapping   shmget   mapviewoffile   mmapo protection setfilesecurity   chmod   initlializesecuritydescriptor   umask   setsecuritydescriptorgroup   chown   be multi programmed often  there is a system call specifically for this purpose  create process or submit job   if we create a new job or process  or perhaps even a set of jobs or processes  we should be able to control its execution this control requires the ability to determine and reset the attributes of a job or process  including the job 's priority  its maximum allowable execution time  and so on  get process attributes and set process attributes   we may also want to terminate a job or process that we created  terminate process  if we find that it is incorrect or is no longer needed having created new jobs or processes  we may need to wait for them to finish their execution we may want to wait for a certain amount of time to pass  wait time  ; more probably  we will want to wait for a specific event to occur  wait event   the jobs or processes should then signal when that event has occurred  signal event   quite often  two or more processes may share data to ensure the integrity of the data being shared  operating systems often provide system calls allowing a process to lock shared data  thus preventing another process from accessing the data while it is locked typically such system calls include acquire lock and release lock system calls of these 62 chapter 2 example of standard c library the standard c library provides a portion o the system-call interface for many versions of unix and linux as an example  let 's assume a c program invokes the printf   statement the c library intercepts this call and invokes the necessary system call  s  in the operating system-in this instance  the write   system call the c library takes the value returned by write   and passes it back to the user program this is shown in figure 2.9 user mode kernel mode i i # include stdio.h int main     printf  greetings  ; i + return 0 ; standard c library write   system call i i  figure 2.9 standard c library handling of write    types  dealilcg with the coordination of concurrent processes  are discussed in great detail in chapter 6 there are so many facets of and variations in process and job control that we next use two examples-one involving a single-tasking system and the other a multitasking system -to clarify these concepts the ms-dos operating system is an example of a single-tasking system it has a command interpreter that is invoked when the computer is started  figure 2.10  a    because ms-dos is single-tasking  it uses a sincple method to run a program and does not create a new process it loads the program into memory  writing over most of itself to give the program as much memory as possible  figure 2.10  b    next  it sets the instruction pointer to the first instruction of the program the program then runs  and either an error causes a trap  or the program executes a system call to terminate in either case  the error code is saved in the system memory for later use following this action  the small portion of the command interpreter that was not overwritten resumes execution its first task is to reload the rest free memory command interpreter  a  2.4 free memory process command interpreter  b  figure 2.10 ms-dos execution  a  at system startup  b  running a program 63 of the command interpreter from disk then the command interpreter makes the previous error code available to the user or to the next program fre ~ _j _s_i   der_i_ \ ' ~ c ! jr   in b   j  ~ eley unix  is an example of a multitasking syst   ' ~ when a user logs on to the system ~ the shell otthe user 's-choiceis run this shell is similar to the ms-dos shell in that it accepts commands and executes programs that the user requests however  since freebsd is a multitasking system  the command interpreter may continue running while another program is executed  figure 2.11   io startanew  __process,_th_es1w1l execu ~ 2 \ _  for-k   sy ~ tem call then  the selected program is loaded into memory via an exec   system call  and the program is executed depending on the way the command was issued  the shell then either waits for the process to finish or runs the process in the background in the latter case  the shell immediately requests another command when a process is rmming in the background  it can not receive input directly fron1 the keyboard  because the process d free memory process c interpreter figure 2.11 freebsd running multiple programs 64 chapter 2 shell is using this resource i/o is therefore done through files or through a cui interface meanwhile  the user is free to ask the shell to run other programs  to monitor the progress of the running process  to change that program 's priority  and so on when the process is done  it executes an exit   system call to terminate  returning to the invoking process a status code of 0 or a nonzero error code this status or error code is then available to the shell or other programs processes are discussed in chapter 3 with a program example using thefork   and exec   systemcalls 2.4.2 file management the file system is discussed in more detail in chapters 10 and 11 we can  however  identify several common system calls dealing with files we first need to be able to create and delete files either system call requires the name of the file and perhaps some of the file 's attributes once the file is created  we need to open it and to use it we may also read  write  or reposition  rewinding or skipping to the end of the file  for example   finally  we need to close the file  indicating that we are no longer using it we may need these same sets of operations for directories if we have a directory structure for organizing files in the file system in addition  for either files or directories  we need to be able to determine the values of various attributes and perhaps to reset them if necessary file attributes include the file name  file type  protection codes  accounting information  and so on at least two system calls  get file attribute and set file attribute  are required for this function some operating systems provide many more calls  such as calls for file move and copy others might provide an api that performs those operations using code and other system calls  and others might just provide system programs to perform those tasks if the system programs are callable by other programs  then each can be considered an api by other system programs 2.4.3 device management a process may need several resources to execute-main memory  disk drives  access to files  and so on if the resources are available  they can be granted  and control can be returned to the user process otherwise  the process will have to wait until sufficient resources are available the various resources controlled by the operating system can be thought of as devices some of these devices are physical devices  for example  disk drives   while others can be thought of as abstract or virtual devices  for example  files   a system with multiple users may require us to first request the device  to ensure exclusive use of it after we are finished with the device  we release it these functions are similar to the open and close system calls for files other operating systems allow llnmanaged access to devices the hazard then is the potential for device contention and perhaps deadlock  which is described in chapter 7 once the device has been requested  and allocated to us   we can read  write  and  possibly  reposition the device  just as we can with files in fact  the similarity between i/0 devices and files is so great that many operating systems  including unix  merge the two into a combined file-device structure in this case  a set of system calls is used on both files and devices sometimes  2.4 65 l/0 devices are identified by special file names  directory placement  or file attributes the user interface can also ncake files and devices appear to be similar1 even though the underlying system calls are dissimilar this is another example of the many design decisions that go into building an operating system and user interface 2.4.4 information maintenance many system calls exist simply for the purpose of transferring information between the user program and the operating system for example  most systems have a system call to return the current time and date other system calls may return information about the system  such as the number of current users  the version number of the operating system  the amount of free memory or disk space  and so on another set of system calls is helpful in debugging a program many systems provide system calls to dump memory this provision is useful for debugging a program trace lists each system call as it is executed even microprocessors provide a cpu mode known as single step  in which a trap is executed by the cpu after every instruction the trap is usually caught by a debugger many operating systems provide a time profile of a program to indicate the amount of time that the program executes at a particular location or set of locations a time prof ~ ~ ~ ~  c_92  1i ! ~ ~ ~ i ! ! 'cee a t ~  lc ~ ki2l  ility_s e !  egl1lar tii ! '_ee interrupts at every occurrence of the timer interrupt  the value of the program c6l-i  i ~ te1 -ls recorded with sufficiently frequent timer interrupts  a statistical picture of the time spent on various parts of the program can be obtained in addition  the operating system keeps information about all its processes  and system calls are used to access this information generally  calls are also used to reset the process information  get process attributes and set process attributes   in section 3.1.3  we discuss what information is normally kept 2.4.5 communication th ~ ~ e ~ e two c   ll1l ~ cj  ji1_ _m od_e_l ~ _   fi ! ' ! e_ ! el   _c ~ ss_col'rll ! ' ~ ~ nica tion  the  l ! ' ~ ssag_e   _ passing model and the shared-memory model ! nth ~ il ! ~ s ~ ~ g_e  pa,s ~ ij1gl ! '   'lel t_l  t_ ~ _c    rrtll12injfa_fii ~ gpr c ~ ~  ~ ~ -e   c lailg ~ il'l-es ~ ~ ges with one another to transfer i  tcfo_rillaji   j   messages can be exchanged between the processes either directly or indirectly through a common mailbox before communication can take place  a connection must be opened the name of the other communicator must be known  be it another process on the same system or a process on another computer comcected by a communications network each computer in a network has a host name by which it is commonly known a host also has a network identifier  such as an ip address similarly  each process has a process narne  and this name is translated into an identifier by which the operating systemcanrefertotheprocess the get hostidand get processid system calls do this translation the identifiers are then passed to the generalpurpose open and close calls provided by the file system or to specific open connection and close connection system calls  depending on the system 's model of communication the recipient process usually must give its 66 chapter 2 2.5 permission for comnmnication to take place with an accept connection call most processes that will be receiving connections are special-purpose daemons  which are systems programs provided for that purpose they execute a wait for connection call and are awakened when a connection is rna de the source of the communication  known as the client  and the receiving daenwn  known as a server  then exchange messages by using read message and write message system calls the close connection call terminates the communication _ ! 11 the shared-me_1llorytllodel,proc ~ sses use s  tlared memorycreate and shared memory attach system calls to create 2rt1d gain access toi egions ot n1emory owned by other processes recall that  normally  the operatinisystein hiesf   prevei1foiie process-from accessing another process 's memory shared memory requires that two or more processes agree to remove this restriction they can then exchange information by reading and writing data in the shared areas the form of the data is determined by the processes and are not under the operating system 's control the processes are also responsible for ensuring that they are not writing to the same location sirnultaneously such mechanisms are discussed in chapter 6 in chapter 4  we look at a variation of the process scheme-threads-in which memory is shared by default both of the models just discussed are common in operating systems  and most systems implement both message passing is useful for exchanging smaller amounts of data  because no conflicts need be avoided it is also easier to implement than is shared memory for intercomputer communication shared memory allows maximum speed and convenience of communication  since it can be done at memory transfer speeds when it takes place within a computer problems exist  however  in the areas of protection and synchronization between the processes sharing memory 2.4.6 protection protection provides a mechanism for controlling access to the resources provided by a computer system historically  protection was a concern only on multiprogrammed computer systems with several users however  with the advent of networking and the internet  all computer systems  from servers to pdas  must be concerned with protection typically  system calls providing protection include set permission and get permission  which manipulate the permission settings of resources such as files and disks the allow user and deny user system calls specify whether particular users can-or can not-be allowed access to certain resources we cover protection in chapter 14 and the much larger issue of security in chapter 15 another aspect of a modern system is the collection of system programs recall figure 1.1  which depicted the logical computer hierarchy at the lowest level is hardware next is the operating system  then the system programs  and finally the application programs system programs  also known as system utilities  provide a convenient enviromnenf1orprograrn-aevelopmeiita1inexecuhon 2.5 67 some of them are simply user interfaces to system calls ; others are considerably more complex they can be divided into these categories  file management these programs create  delete  copy  rename  print  dump  list  and generally ncanipulate files and directories status information some programs simply ask the system for the date  time  amount of available memory or disk space  number of users  or similar status information others are more complex  providing detailed performance  logging  and debugging information typically  these programs format and print the output to the terminal or other output devices or files or display it in a window of the gui some systems also support a which is used to store and retrieve configuration information file modification several text editors may be available to create and modify the content of files stored on disk or other storage devices there may also be special commands to search contents of files or perform transformations of the text programming-language support compilers  assemblers  debuggers  and interpreters for common programming languages  such as c  c + +  java  visual basic  and perl  are often provided to the user with the operating system program loading and execution once a program is assembled or compiled  it must be loaded into memory to be executed the system may provide absolute loaders  relocatable loaders  linkage editors  and overlay loaders debugging systems for either higher-level languages or machine language are needed as well communications these programs provide the mechanism for creating virtual comcections among processes  users  and computer systems they allow users to send rnessages to one another 's screens  to browse web pages  to send electronic-mail messages  to log in remotely  or to transfer files from one machine to another in addition to systems programs  most operating systems are supplied with programs that are useful in solving common problems or performing common operations such application  jj  ogr ! lj1ls iitclllde'if \ t ~ l  l l  jrg_wsf2r ~  worg processors an i text f6-rinatteis,spreadsheets  database systems  compilers  plott1i1g ana s-tafistica  -analysis packages  ancl gan1es ~        ___ tne viewoclne opei ; ating-sysrerri-seen b  t inost users is defined by the application and system programs  rather than by the actual systern calls consider a user 's pc when a user 's computer is rumcing the mac os x operating system  the user might see the gui  featuring a mouse-and-windows interface alternatively  or even in one of the windows  the user might have a command-line unix shell both use the same set of system calls  but the system calls look different and act in different ways further confusing the user view  consider the user dual-booting from mac os x into windows vista now the same user on the same hardware has two entirely different interfaces and two sets of applications using the same physical resources on the same 68 chapter 2 2.6 hardware  then  a user can be exposed to multiple user interfaces sequentially or concurrently in this section  we discuss problems we face in designing and implementing an operating system there are  of course  no complete solutions to such problems  but there are approaches that have proved successful 2.6.1 design goals the first problem in designing a system is to define goals and specifications at the highest level  the design of the system will be affected by the choice of hardware and the type of system  batch  time shared  single user  multiuser  distributed  real time  or general purpose beyond this highest design level  the requirements may be much harder to specify the requirements can  however  be divided into two basic groups  user goals and system goals users desire certain obvious properties in a system the system should be convenient to use  easy to learn and to use  reliable  safe  and fast of course  these specifications are not particularly useful in the system design  since there is no general agreement on how to achieve them a similar set of requirements can be defined by those people who must design  create  maintain  and operate the system the system should be easy to design  implement  and maintain ; and it should be flexible  reliable  error free  and efficient again  these requirements are vague and may be interpreted in various ways there is  in short  no unique solution to the problem of defining the requirements for an operating system the wide range of systems in existence shows that different requirements can result in a large variety of solutions for different environments for example  the requirements for vxworks  a realtime operating system for embedded systems  must have been substantially different from those for mvs  a large multiuser  multiaccess operating system for ibm mainframes specifying and designing an operating system is a highly creative task although no textbook can tell you how to do it  general principles have been developed in the field of software engineering  and we turn now to a discussion of some of these principles c  2.6.2 mechanisms and policies   i one important principle is the separation of policy from mechanisiil ~ echa   1'lis ~ s   leter111il1e hcnu ! q_c  @ -son'l ~ tl-til1g ; p   lic  les  i ~ termir e  zul1dt wilcbe done for example  the timer construct  see section 1.5.2  is a mechani.sril  -forensill1ng cpu protection  but deciding how long the timer is to be set for a particular user is a policy decision _  'h ~ _s_ 122l ! cl_tig ! l    fp.qli_cy_an_ci ~ t1_echanism is imp   rtant for flexibility policies are likely to change across places o1  'over time 'rri tll'e worst case  each change in policy would require a change in the underlying mechanism a general mechanism insensitive to changes in policy would be more desirable a change 2.6 69 in policy would then require redefinition of only certain parameters of the system for instance  consider a mechanism for giving priority to certain types of programs over others if the mechanism is properly separated from policy  it can be used either to support a policy decision that i/o-intensive progran1.s should have priority over cpu-intensive ones or to support the opposite policy microkernel = based operati1lg sy_sh  ~ ms  section 2 .3  take the separation of mechai ~ 1sinai ~ cfp hcyto one extreme byimplementing a basicset   j_pri111.iti_y ~ 1jiwding bfocks these blocks are almost policy free  allowing more advanced -1necharnsms and policies to be added via user-created kernel modules or via user programs themselves as an example  consider the history of unix at first  it had a time-sharing scheduler in the latest version of solaris  scheduling is controlled by loadable tables depending on the table currently loaded  the system can be time shared  batch processing  real time  fair share  or any combination making the scheduling mechanism general purpose allows vast policy changes to be made with a single load-new-table command at th_ ~   th ~ r extreme is_il_ ~ ~ ~ t ~ l il ~  ttc  l ~ -as _ \ 1 \ t'i_ ! l_t  l   ! yj'c_ ~ \ 1 \ t ~ ~ icj  l ~ qt ~ j1 lec  ! '.c  l ~ ~ ~ 1l ~ and_p   _1i_c_y__a_  r ~ _epc    ciec  lj ~ 1._ ! he sy ~ te ~ _ t  j_e_ilforce__ ~ gl   ~ ~ l   ok an_cl_ fe_el all applications have similar interfaces  because the interface itself is built into the kernel and system libraries the mac os x operating system has similar functionality policy decisions are important for all resource allocation whenever it is necessary to decide whether or not to allocate a resource  a policy decision must be made whenever the question is how rather than what  it is a mechanism that must be determined 2.6.3 implementation once an operating system is designed  it must be implemented traditionally  operating systems have been written in assembly language now  however  they are most commonly written in higher-level languages such as cor c + +  the first system that was not written in assembly language was probably the master control program  mcp  for burroughs computers mcp was written in a variant of algol multics  developed at mit  was written mainly in pl/1 the linux and windows xp operating systems are written mostly in c  although there are some small sections of assembly code for device drivers and for saving and restoring the state of registers the advantages of using a higher-level language  or at least a systemsimplementation language  for implementing operating systems are the same as those accrued when the language is used for application programs  the code can be written faster  is more compact  and is easier to understand and debug in addition  improvements in compiler technology will improve the generated code for the entire operating system by simple recompilation finally  an operating system is far easier to port-to move to some other hardware-if it is written in a higher-level language for example  ms-dos was written in intel 8088 assembly language consequently  it runs natively only on the intel x86 family of cpus  although ms-dos runs natively only on intel x86  emulators of the x86 instruction set allow the operating system to run non-nativelyslower  with more resource use-on other cpus are programs that duplicate the functionality of one system with another system  the linux 70 chapter 2 2.7 operating system  in contrast  is written mostly inc and is available natively on a number of different cpus  including intel x86  sun sparc  and ibmpowerpc the only possible disadvantages of implementing an operating system in a higher-level language are reduced speed and increased storage requirements this  howeve1 ~ is no longer a major issue in today 's systems although an expert assembly-language programmer can produce efficient small routines  for large programs a modern compiler can perform complex analysis and apply sophisticated optimizations that produce excellent code modern processors have deep pipelining and n1.ultiple functional units that can handle the details of complex dependencies much more easily than can the human mind as is true in other systems  major performance improvements in operating systems are more likely to be the result of better data structures and algorithms than of excellent assembly-language code in addition  although operating systems are large  only a small amount of the code is critical to high performance ; the memory manager and the cpu scheduler are probably the most critical routines after the system is written and is working correctly  bottleneck routines can be identified and can be replaced with assembly-language equivalents a system as large and complex as a modern operating system must be engineered carefully if it is to function properly and be modified easily a common approach is to partition the task into small components rather than have one monolithic system each of these modules should be a well-defined portion of the system  with carefully defined inputs  outputs  and functions we have already discussed briefly in chapter 1 the common components of operating systems in this section  we discuss how these components are interconnected and melded into a kernel 2.7.1 simple structure many commercial operating systen1.s do not have well-defined structures frequently  such systems started as small  simple  and limited systems and then grew beyond their original scope ms-dos is an example of such a systen1 it was originally designed and implemented by a few people who had no idea that it would become so popular it was written to provide the most functionality in the least space  so it was not divided into modules carefully figure 2.12 shows its structure in ms-dos  the interfaces and levels of functionality are not wellseparated for rnstai1.ce  appii.cat1on programs aie able to access the basic i  b 1  outiri.es to write directly to the display and disk drives such freedom leaves ms-dos vulnerable to errant  or malicio lls  programs  causing entire system crashes when user programs fail of course  ms-dos was also limited by the hardware of its era because the intel 8088 for which it was written provides no dual mode and no hardware protection  the designers of ms-dos had no choice but to leave the base hardware accessible another example of limited structuring is the original unix operating systein like ms ~ dc5s  unix initially was limited by hard ware ft1il.cfionali.ty ft consistsoftwo separahlepai ; fs  thei  eril.el ai1d the system prograrns  thekei  nel 2.7 71 rom bios device drivers figure 2.12 ms-dos layer structure is further separated into a series of interfaces and device drivers  which have been added and expanded over the years as unix has evolved we can view the traditional unix operating system as being layered  as shown in figure 2.13 everything below the system-call interface and above the physical hardware is the kernel tb ~ l  ol ll ~ lp  rgvides__i  h ~ _fil ~ syste  rn  c   p_l ! _s ~ h ~ dulijl,g  memory management  and other operating-system fm1ctions through system calls taken i.n sum ~ thatl.sai1 enormous an1ol.lnt of functionality to be combined into one level this monolithic structure was difficult to implement and maintain 2.7.2 layered approach withproper j  tarc  l \ a ! lre support  operating systems can be brokeninto pieces that are smaller and more app1  opriate thar  t  hose allowed by the _ _2 ! i2 ; g ~ af  the users  shells and commands compilers and interpreters system libraries signals terminal handling character 1/0 system terminal drivers file system swapping block 1/0 system disk and tape drivers cpu scheduling page replacement demand paging virtual memory figure 2.13 traditional unix system structure 72 chapter 2 figure 2.14 a layered operating system m ~   .qoi'ilncil  l'j_ix systeill ~ the operating system can then retain much greater control over the computer and over the applications that make use of that computer implementers have more freedom in changing the inner workin.gs of the system and in creating modular operating systems under a topdown approach  the overall functionality and features are determined and are separated into components information hiding is also important  because it leaves programmers free to implement the low-level routines as they see fit  provided that the external interface of the routine stays unchanged and that the routine itself performs the advertised task a system can be made modular in many ways qne method is the layered approach  in which the operating system is broken ii1to a 1l.umberoflayers  lever8j.ti1eoottom.iiiyer  layer 0  .1sthetiarawai ; e ; the nig  ytesl   layern   1sfhe user interface this layering structure is depicted in figure 2.14 an operating-system layer is an implementation of an abstract object made up of data and the operations that can manipulate those data a typical operating-system layer-say  layer m -consists of data structures and a set of routines that can be invoked by higher-level layers layer m  in turn  can invoke operations on lower-level layers the main advantage of the layered approach is simplicity of construction and debugging the layers are selected so that each uses functions  operations  and services of only lower-level layers this approach simplifies debugging and .system verification the first layer can be debugged without any concern for the rest of the system  because  by definition  it uses only the basic hardware  which is assumed correct  to implement its functions once the first layer is debugged  its correct functioning can be assumed while the second layer is debugged  and so on if an error is found during the debugging of a particular layer  the error must be on that layer  because the layers below it are already debugged thus  the design and implementation of the system are simplified 2.7 73 each layer is implemented with only those operations provided by lowerlevel layers a layer does not need to know how these operations are implemented ; it needs to know only what these operations do hence  each layer hides the existence of certain data structures  operations  and hardware from higher-level layers the major difficulty with the layered approach involves appropriately defining the various layers because a layer can use only lower-level layers  careful planning is necessary for example  the device driver for the backing store  disk space used by virtual-memory algorithms  must be at a lower level than the memory-management routines  because memory management requires the ability to use the backing store other requirements may not be so obvious the backing-store driver would normally be above the cpu scheduler  because the driver may need to wait for i/0 and the cpu can be rescheduled during this time however  on a large system  the cpu scheduler m.ay have more information about all the active processes than can fit in memory therefore  this u1.formation may need to be swapped u1 and out of memory  requiring the backu1.g-store driver routine to be below the cpu scheduler a final problem with layered implementations is that they tend to be less efficient than other types for instance  when a user program executes an i/0 operation  it executes a system call that is trapped to the i/0 layer  which calls the memory-management laye1 ~ which in tum calls the cpu-scheduling layer  which is then passed to the hardware at each layer  the parameters may be modified  data may need to be passed  and so on each layer adds overhead to the system call ; the net result is a system call that takes longer than does one on a nonlayered system these limitations have caused a small backlash against layering in recent years fewer layers with more functionality are beu1.g designed  providu1.g most of the advantages of modularized code while avoidu1.g the difficult problems of layer definition and interaction 2.7.3 microkernels we have already seen that as unix expanded  the kernel became large and difficult to manage in the mid-1980s  researchers at carnegie mellon university developed an operatu1.g system called mach that modularized the kernel using the ~ i ~ roke ~ ll  ~ _ ! _ ~ ee1 ~   2lc  ~ ~ i.b ~ ._gl ~ ! b_   _  ! _0ructl ~ ~ ~ ~ --t ! ~ e operatingsystem by removing all nonessential cornponentsfrom thekemel and 1mp ~ e_l  l ~ -ll  ! ~ ~ ~ itil ~ !   t ~ ~ s ~ ~ fe_l il ~ ~ ~ ~ rl.ls_ ~ l  ~ i ~ \ r ~   jr   greili ~ ~  the.reslin is-a smarrei  kernel there is little consensus regarding which services should remain u1 the kernel and which should be implemented in user space typically  however  microkernels provide minimal process and memory management  in addition to a communication facility the main function of the micro kernel is to provide a communication facility between the client program and the various services that are also rum1.ing in user space communication is provided by message passing  which was described in section 2.4.5 for example  if the client program wishes to access a file  it must interact with the file server the client program and service never interact directly rathel ~ they communicate indirectly by exchanging messages with the microkemel 74 chapter 2 one benefit of the microkernel approach is ease of extending the operating system all new services are added to user space and consequently do not require modification of the kernel when the kernel does have to be modified  the changes tend to be fewer  because the microkernel is a smaller kernel the resulting operating system is easier to port from one hardware design to another the microkernel also provides more security and reliability  since most services are running as user-rather than kernel-processes if a service fails  the rest of the operating system remains untouched several contemporary operating systems have used the microkernel approach tru64 unix  formerly digital unix  provides a unix interface to the user  but it is implemented with a mach kernel the mach kernel maps unix system calls into messages to the appropriate user-level services the mac os x kernel  also known as darwin  is also based on the mach micro kernel another example is qnx  a real-time operating system the qnx nl.icrokernel provides services for message passing and process scheduling it also handles low-level network communication and hardware interrupts all other services in qnx are provided by standard processes that run outside the kernel in user mode unfortunately  microkernels can suffer from performance decreases due to increased system function overhead consider the history of windows nt the first release had a layered microkernel organization however  this version delivered low performance compared with that of windows 95 windows nt 4.0 partially redressed the performance problem by moving layers from user space to kernel space and integrating them more closely by the time windows xp was designed  its architecture was more monolithic than microkernel 2.7.4 modules perhaps the best current methodology for operating-system design involves using object-oriented programming techniques to create a modular kernel here  the kernel has a set of core components and links in additional services either during boot time or during run time such a strategy uses dynamically loadable modules and is common in modern implementations of unix  such as solaris  linux  and mac os x for example  the solaris operating system structure  shown in figure 2.15  is organized armmd a core kernel with seven types of loadable kernel modules  scheduling classes file systems loadable system calls executable formats streams modules miscellaneous device and bus drivers such a design allows the kernel to provide core services yet also allows certain features to be implemented dynamically for example  device and 2.7 file systems figure 2.15 solaris loadable modules loadable system calls 75 bus drivers for specific hardware can be added to the kernel  and support for different file systems can be added as loadable modules the overall result resembles a layered system in that each kernel section has defined  protected interfaces ; but it is more flexible than a layered system in that any module can call any other module furthermore  the approach is like the microkernel approach in that the primary module has only core functions and knowledge of how to load and communicate with other modules ; but it is more efficient  because modules do not need to invoke message passing in order to communicate the apple mac os x operating system uses a hybrid structure it is a layered system in which one layer consists of the mach microkernel the structure of mac os x appears in figure 2.16 the top layers include application environments and a set of services providing a graphical interface to applications below these layers is the kernel environment  which consists primarily of the mach microkernel and the bsd kernel mach provides memory management ; support for remote procedure calls  rpcs  and interprocess communication  ipc  facilities  including message passing ; and thread scheduling the bsd component provides a bsd command line interface  support for networking and file systems  and an implementation of posix apis  including pthreads kernel environment application environments and common services figure 2.16 the mac os x structure 76 chapter 2 2.8 in addition to mach and bsd  the kernel environment provides an i/0 kit for development of device drivers and dynamically loadable modules  which mac os x refers to as kernel extensions   as shown in the figure  applications and comn  10n services can make use of either the mach or bsd facilities directly the layered approach described in section 2.7.2 is taken to its logical conclusion in the concept of a the fundamental idea behind a virtual machine is to abstract the hardware of a si11.gle computer  the cpu  memory  disk drives  network interface cards  and so forth  into several different execution environments  thereby creating the illusion that each separate execution environment is run.ning its own private computer by using cpu scheduling  chapter 5  and virtual-memory techniques  chapter 9   an operating system can create the illusion that a process has its own processor with its own  virtual  memory the virtual machine provides an interface that is identical to the underlying bare hardware each process is provided with a  virtual  copy of the underlying computer  figure 2.17   usually  the guest process is in fact an operating system  and that is how a single physical machine can run multiple operating systems concurrently  each in its own virtual machine 2.8.1 history virtual machines first appeared commercially on ibm mainframes via the vm operating system in 1972 vm has evolved and is still available  and many of processes programming/ / interface 1 ~ -----1 kernel  a  processes processes processes kernel kernel kernel vm1 vm2 vm3 virtual-machine implementation  b  figure 2.17 system models  a  nonvirtual machine  b  virtual machine 2.8 77 the original concepts are found in other systems  making this facility worth exploring ibm vm370 divided a mainframe into nmltiple virtual machines  each numing its own operating system a ncajor difficulty with the vm virtualmachine approach involved disk systems suppose that the physical machine had three disk drives but wanted to support seven virtual machines clearly  it could not allocate a disk drive to each virtual machine  because the virtualmachine software itself needed substantial disk space to provide virtual memory and spooling the solution was to provide virtual disks-termed minidislcs in ibm 's vm operating system -that are identical in all respects except size the system implemented each minidisk by allocating as many tracks on the physical disks as the minidisk needed once these virtual machines were created  users could run any of the operating systems or software packages that were available on the underlying machine for the ibm vm system  a user normally ran cms-a single-user interactive operating system 2.8.2 benefits there are several reasons for creating a virtual machine most of them are fundarnentally related to being able to share the same hardware yet run several different execution environments  that is  different operating systems  concurrently one important advantage is that the host system is protected from the virtual machines  just as the virtual machines are protected from each other a virus inside a guest operating system might damage that operating system but is unlikely to affect the host or the other guests because each virtual machine is completely isolated from all other virtual machines  there are no protection problems at the same time  however  there is no direct sharing of resources two approaches to provide sharing have been implemented first  it is possible to share a file-system volume and thus to share files second  it is possible to define a network of virtual machines  each of which can send information over the virtual communications network the network is modeled after physical communication networks but is implemented in software a virtual-machine system is a perfect vehicle for operating-systems research and development normally  changing an operating system is a difficult task operating systems are large and complex programs  and it is difficult to be sure that a change in one part will not cause obscure bugs to appear in some other part the power of the operating system makes changing it particularly dangerous because the operating system executes in kernel mode  a wrong change in a pointer could cause an error that would destroy the entire file system thus  it is necessary to test all changes to the operating system carefully the operating system  however  runs on and controls the entire machine therefore  tlle current system must be stopped and taken out of use while changes are made and tested this period is comnconly called systemdevelopment time since it makes the system unavailable to users  systemdevelopment time is often scheduled late at night or on weekends  when system load is low 78 chapter 2 a virtual-machine system can eliminate much of this problem system programmers are given their own virtual machine  and system development is done on the virtual machine instead of on a physical machine normal system operation seldom needs to be disrupted for system development another advantage of virtual machines for developers is that multiple operating systems can be running on the developer 's workstation concurrently this virtualized workstation allows for rapid porting and testing of programs in varying enviromnents sin'lilarly  quality-assurance engineers can test their applications in multiple environments without buying  powering  and maintaining a computer for each environment a major advantage of virtual machines in production data-center use is system which involves taking two or more separate systems and running them in virtual machines on one system such physical-to-virtual conversions result in resource optimization  as many lightly used systems can be combined to create one more heavily used system if the use of virtual machines continues to spread  application deployment will evolve accordingly if a system can easily add  remove  and move a virtual machine  then why install applications on that system directly instead  application developers would pre-install the application on a tuned and customized operating system in a virh1al machine that virtual environment would be the release mechanism for the application this method would be an improvement for application developers ; application management would become easier  less tuning would required  and technical support of the application would be more straightforward system administrators would find the environment easier to manage as well installation would be simple  and redeploying the application to another system would be much easier than the usual steps of uninstalling and reinstalling for widespread adoption of this methodology to occur  though  the format of virtual machines must be standardized so that any virtual machine will run on any virtualization platform the open virtual machine format is an attempt to do just that  and it could succeed in unifying virtual-machine formats 2.8.3 simulation system virtualization as discussed so far is just one of many system-emulation methodologies virtualization is the most common because it makes guest operating systems and applications believe they are running on native hardware because only the system 's resources need to be virtualized  these guests run at almost full speed another methodology is in which the host system has one system architecture and the guest system was compiled for a different architecture for example  suppose a company has replaced its outdated computer system with a new system but would like to continue to run certain important programs that were compiled for the old system the programs could be run in an e1nulator that translates each of the outdated system 's instructions into the native instruction set of the new system emulation can increase the life of programs and allow us to explore old architectures without having an actual old machine  but its major challenge is performance instruction-set emulation can run an order of magnitude slower than native instructions thus  unless the new machine is ten times faster than the old  the program running on 2.8 79 the new machine will run slower than it did on its native hardware another challenge is that it is difficult to create a correct emulator because  in essence  this involves writing an entire cpu in software 2.8.4 para-virtualization is another vanat10n on this theme rather than try to trick a guest operating system into believing it has a system to itself  paravirtualization presents the guest with a system that is similar but not identical to the guest 's preferred system the guest must be modified to run on the paravirtualized hardware the gain for this extra work is more efficient use of resources and a smaller virtualization layer solaris 10 includes or that create a virtual layer between the operating system and the applications in this system  only one kernel is installed  and the hardware is not virtualized rather  the operating system and its devices are virtualized  providing processes within a container with the impression that they are the only processes on the system one or more containers can be created  and each can have its own applications  network stacks  network address and ports  user accounts  and so on cpu resources can be divided up among the containers and the systemwide processes figure 2.18 shows a solaris 10 system with two containers and the standard global user space user programs system programs cpu resources memory resources global zone user programs system programs network addresses device access cpu resources user programs system programs network addresses device access cpu resources memory resources memory resources zone 1 zone 2 virtual platform device management figure 2.18 solaris i 0 with two containers 80 chapter 2 2.8.5 implementation although the virtual-machine concept is usefut it is difficult to implement much work is required to provide an exact duplicate of the underlying machine remember that the underlying machine typically has two modes  user mode and kernel mode the virtual-machine software can run in kernel mode  since it is the operating system the virtual machine itself can execute in only user mode just as the physical machine has two modes  however  so must the virtual machine consequently  we must have a virtual user mode and a virtual kernel mode  both of which run in a physical user mode those actions that cause a transfer from user mode to kernel mode on a real machine  such as a system call or an attempt to execute a privileged instruction  must also cause a transfer from virtual user mode to virtual kernel mode on a virtual machine such a transfer can be accomplished as follows when a system calt for example  is made by a program running on a virtual machine in virtual user mode  it will cause a transfer to the virtual-machine monitor in the real machine when the virtual-machine monitor gains controt it can change the register contents and program counter for the virtual machine to simulate the effect of the system call it can then restart the virtual machine  noting that it is now in virtual kernel mode the major difference  of course  is time whereas the real i/o might have taken 100 milliseconds  the virtual i/o might take less time  because it is spooled  or more time  because it is interpreted   in addition  the cpu is being multi programmed among many virtual machines  further slowing down the virtual machines in unpredictable ways in the extreme case  it may be necessary to simulate all instructions to provide a true virtual machine vm  discussed earlier  works for ibm machines because normal instructions for the virtual machines can execute directly on the hardware only the privileged instructions  needed mainly for i/0  must be simulated and hence execute more slowly without some level of hardware support  virtualization would be impossible the more hardware support available within a system  the more feature rich  stable  and well performing the virtual machines can be all major generalpurpose cpus provide some amount of hardware support for virtualization for example  amd virtualization technology is found in several amd processors it defines two new modes of operation-host and guest virtual machine software can enable host mode  define the characteristics of each guest virtual machine  and then switch the system to guest mode  passing control of the system to the guest operating system that is running in the virtual machine in guest mode  the virtualized operating system thinks it is rum1.ing on native hardware and sees certain devices  those included in the host 's definition of the guest   if the guest tries to access a virtualized resource  then control is passed to the host to manage that interaction 2.8.6 examples despite the advantages of virtual machines  they received little attention for a number of years after they were first developed today  however  virtual machines are coming into fashion as a means of solving system compatibility problems in this section  we explore two popular contemporary virtual machines  the vmware workstation and the java virtual machine as you 2.8 81 will see  these virtual machines can typically run on top of operating systems of any of the design types discussed earlier thus  operating system design methods-simple layers  microkernels  n  wdules  and virtual machines-are not mutually exclusive 2.8.6.1 vmware most of the virtualization techniques discussed in this section require virtualization to be supported by the kernel another method involves writing the virtualization tool to run in user mode as an application on top of the operating system virtual machines running within this tool believe they are rum ing on bare hardware but in fact are running inside a user-level application is a popular commercial application that abstracts intel x86 and compatible hardware into isolated virtual machines vmware workstation runs as an application on a host operating system such as windows or linux and allows this host system to concurrently run several different guest operating systems as independent virtual machines the architecture of such a system is shown in figure 2.19 in this scenario  linux is running as the host operating system ; and freebsd  windows nt  and windows xp are rum ing as guest operating systems the virtualization layer is the heart of vmware  as it abstracts the physical hardware into isolated virtual machines running as guest operating systems each virtual machine has its own virtual cpu  memory  disk drives  network interfaces  and so forth the physical disk the guest owns and manages is really just a file within the file system of the host operating system to create an identical guest instance  we can simply copy the file copying the file to another location protects the guest instance against a disaster at the original site moving the file to another application application application application guest operating guest operating guest operating system system system  free bsd   windows nt   windows xp  virtual cpu virtual cpu virtual cpu virtual memory virtual memory virtual memory virtual devices virtual devices virtual devices virtualization layer hardware i qpu   i r ! jemgfy figure 2.19 vmware architecture 82 chapter 2 location moves the guest system these scenarios show how virtualization can improve the efficiency of system administration as well as system resource use 2.8.6.2 the java virtual machine java is a popular object-oriented programming language introduced by sun microsystems in 1995 in addition to a language specification and a large api library  java also provides a specification for a java virtual machine-or jvm java objects are specified with the class construct ; a java program consists of one or more classes for each java class  the compiler produces an architecture-neutral bytecode output  .class  file that will run on any implementation of the jvm the jvm is a specification for an abstract computer it consists of a class loader and a java interpreter that executes the architecture-neutral bytecodes  as diagrammed in figure 2.20 the class loader loads the compiled  class files from both the java program and the java api for execution by the java interpreter after a class is loaded  the verifier checks that the  class file is valid java bytecode and does not overflow or underflow the stack it also ensures that the bytecode does not perform pointer arithmetic  which could provide illegal memory access if the class passes verification  it is run by the java interpreter the jvm also automatically manages memory by performing garbage collection -the practice of reclaiming memory from objects no longer in use and returning it to the system much research focuses on garbage collection algorithms for increasing the performance of java programs in the virtual machine the jvm may be implemented in software on top of a host operating system  such as windows  linux  or mac os x  or as part of a web browser alternatively  the jvm may be implemented in hardware on a chip specifically designed to nm java programs if the jvm is implemented in software  the java interpreter interprets the bytecode operations one at a time a faster software technique is to use a just-in-time  jit  compiler here  the first time a java method is invoked  the bytecodes for the method are turned into native machine language for the host system these operations are then cached so that subsequent invocations of a method are performed using the native machine instructions and the bytecode operations need not be interpreted all over again a technique that is potentially even faster is to nm the jvm in hardware on a java program .class files  i class loader 1 +  + i java i interpreter t host system  windows  linux  etc  figure 2.20 the java virtual machine 2.8 the .net framework the .net framework is a collection of technologies  including a set of class libraries  and an execution environment that come together to provide a platform for developing software this platform allows programs to be written to target the .net framework instead of a specific architecture a program written for the .net framework need not worry aboutthe specifics of the hardware or the operating system on which it will run thus  any architecture implementing .net will be able to successfully execute the program this is because the execution environment abstracts these details and provides a virtual machine as an intermediary between the executing program and the underlying architecture at the core of the .net framework is the common language runtime  clr   the clr is the implementation of the .net virtual machine itprovides an environment for execution of programs written in any of the languages targeted at the .net framework programs written in languages such as c #  pronounced c-sharp  and vb.net are compiled into an intermediate  architecture-independent language called microsoft intermediate language  ms-il   these compiled files  called assemblies  include ms-il instructions and metadata they have file extensions of either .exe or .dll upon execution of a program  the clr loads assemblies into what .is known as the application domain as instructions are requested by the executing program  the clr converts the ms-il instructions inside the assemblies into native code that is specific to the underlying architecture using just-in-time compilation once instructions have been converted to native code  they are kept and will continue to run as native code for the cpu the architecture of the clr for the .net framework is shown in figure 2.21 compilation clr c + + source ms-il assembly vb.net source ms-il assembly host system figure 2.21 architecture ofthe.clr for the .net framework 83 84 chapter 2 2.9 special java chip that executes the java bytecode operations as native code  thus bypassing the need for either a software interpreter or a just-in-tim.e compiler broadly  is the activity of finding and fixing errors  or in a system debugging seeks to find and fix errors in both hardware and software performance problems are considered bugs  so debugging can also include which seeks to improve performance by removing  ' '  in the processing taking place within a system a discussion of hardware debugging is outside of the scope of this text in this section  we explore debugging kernel and process errors and performance problems 2.9.1 failure analysis if a process fails  most operating systems write the error information to a to alert system operators or users that the problem occurred the operating system can also take a capture of the memory  referred to as the core in the early days of computing  of the process this core image is stored in a file for later analysis running programs and core dumps can be probed by a a tool designed to allow a programmer to explore the code and memory a process debugging user-level process code is a challenge operating system kernel debugging even more complex because of the size and complexity of the kernel  its control of the hardware  and the lack of user-level debugging tools a kernel failure is called a as with a process failure  error information is saved to a log file  and the memory state is saved to a operating system debugging frequently uses different tools and techniques than process debugging due to the very different nature of these two tasks consider that a kernel failure in the file-system code would make it risky for the kernel to try to save its state to a file on the file system before rebooting a common technique is to save the kernel 's memory state to a section of disk set aside for this purpose that contains no file system  if the kernel detects an unrecoverable error  it writes the entire contents of memory  or at least the kernel-owned parts of the system memory  to the disk area when the system reboots  a process runs to gather the data from that area and write it to a crash dump file within a file system for analysis 2.9.2 performance tuning to identify bottlenecks  we must be able to monitor system performance code must be added to compute and display measures of system behavior in a number of systems  the operating system does this task by producing trace listings of system behavior all interesting events are logged with their time and important parameters and are written to a file later  an analysis program can process the log file to determine system performance and to identify bottlenecks and inefficiencies these same traces can be run as input for a simulation of a suggested improved system traces also can help people to find errors in operating-system behavior 2.9 kernighan 's law debugging is twice as hard as writing the code in the first place therefore  if you write the code as cleverly as possible  you are  by definition  not smart enough to debug it 85 another approach to performance tuning is to include interactive tools with the system that allow users and administrators to question the state of various components of the system to look for bottlenecks the unix command top displays resources used on the system  as well as a sorted list of the top resource-using processes other tools display the state of disk i/0  memory allocation  and network traffic the authors of these single-purpose tools try to guess what a user would want to see while analyzing a system and to provide that information making running operating systems easier to understand  debug  and tune is an active area of operating system research and implementation the cycle of enabling tracing as system problems occur and analyzing the traces later is being broken by a new generation of kernel-enabled performance analysis tools further  these tools are not single-purpose or merely for sections of code that were written to emit debugging data the solaris 10 dtrace dynamic tracing facility is a leading example of such a tool 2.9.3 dtrace is a facility that dynamically adds probes to a running system  both i11 user processes and in the kernel these probes can be queried via the d programming language to determine an astonishing amount about the kernel  the system state  and process activities for example  figure 2.22 follows an application as it executes a system call  ioctl  and further shows the functional calls within the kernel as they execute to perform the system call lines ending with u are executed in user mode  and lines ending in k in kernel mode debugging the interactions between user-level and kernel code is nearly impossible without a toolset that understands both sets of code and can instrument the interactions for that toolset to be truly useful  it must be able to debug any area of a system  including areas that were not written with debugging in mind  and do so without affecting system reliability this tool must also have a minimum performance impact-ideally it should have no impact when not in use and a proportional impact during use the dtrace tool meets these requirements and provides a dynamic  safe  low-impact debugging environncent until the dtrace framework and tools became available with solaris 10  kernel debugging was usually shrouded in mystery and accomplished via happenstance and archaic code and tools for example  cpus have a breakpoint feature that will halt execution and allow a debugger to examine the state of the system then execution can continue until the next breakpoint or termination this method can not be used in a multiuser operating-system kernel without negatively affecting all of the users on the system pn  reen,g  which periodically samples the instruction pointer to determine which code is being executed  can show statistical trends but not individual activities code can be included in the kernel to emit specific data under specific circumstances  but that code 86 chapter 2 # ./all.d 'pgrep xclock ' xeventsqueued dtrace  script './all.d ' matched 52377 probes cpu function 0  xeventsqueued 0  _xeventsqueued u u 0  _xlltransbytesreadable u 0  _xlltransbytesreadable u 0  _xlltranssocketbytesreadable u 0  _xlltranssocketbytesreadable u 0  ioctl u 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0      ioctl   getf  set active fd  set active fd  getf  get udatamodel  get udatamodel  releasef  clear active   clear active  cv broadcast  cv broadcast  releasef ioctl ioctl xeventsqueued xeventsqueued fd fd k k k k k k k k k k k k k k u u u figure 2.22 solaris 10 dtrace follows a system call within the kernel slows down the kernel and tends not to be included in the part of the kernel where the specific problem being debugged is occurring in contrast  dtrace runs on production systems-systems that are running important or critical applications-and causes no harm to the system it slows activities while enabled  but after execution it resets the system to its pre-debugging state it is also a broad and deep tool it can broadly debug everything happening in the system  both at the user and kernel levels and between the user and kernel layers   dtrace can also delve deeply into code  showing individual cpu instructions or kernel subroutine activities is composed of a compiler  a framework  of written within that framework  and of those probes dtrace providers create probes kernel structures exist to keep track of all probes that the providers have created the probes are stored in a hash table data structure that is hashed by name and indexed according to unique probe identifiers when a probe is enabled  a bit of code in the area to be probed is rewritten to call dtrace_probe  probe identifier  and then continue with the code 's original operation different providers create different kinds of probes for example  a kernel system-call probe works differently from a user-process probe  and that is different from an i/o probe dtrace features a compiler that generates a byte code that is run in the kernel this code is assured to be safe by the compiler for example  no 2.9 87 loops are allowed  and only specific kernel state modifications are allowed when specifically requested only users with the dtrace privileges  or root users  are allowed to use dt ! ace  as it can retrieve private kernel data  and modify data if requested   the generated code runs in the kernel and enables probes it also enables consumers in user mode and enables communications between the two a dt ! ace consumer is code that is interested in a probe and its results a consumer requests that the provider create one or more probes when a probe fires  it emits data that are managed by the kernel within the kernel  actions called or are performed when probes fire one probe can cause multiple ecbs to execute if more than one consumer is interested in that probe each ecb contains a predicate  if statement  that can filter out that ecb otherwise  the list of actions in the ecb is executed the most usual action is to capture some bit of data  such as a variable 's value at that point of the probe execution by gathering such data  a complete picture of a user or kernel action can be built further  probes firing from both user space and the kernel can show how a user-level action caused kernel-level reactions such data are invaluable for performance monitoril1.g and code optimization once the probe consumer tennil1.ates  its ecbs are removed if there are no ecbs consuming a probe  the probe is removed that involves rewriting the code to remove the dtrace_probe call and put back the original code thus  before a probe is created and after it is destroyed  the system is exactly the same  as if no probing occurred dtrace takes care to assure that probes do not use too much memory or cpu capacity  which could harm the running system the buffers used to hold the probe results are monitored for exceeding default and maximum limits cpu time for probe execution is monitored as well if limits are exceeded  the consumer is terminated  along with the offending probes buffers are allocated per cpu to avoid contention and data loss an example ofd code and its output shows some of its utility the following program shows the dtrace code to enable scheduler probes and record the amount of cpu time of each process running with user id 101 while those probes are enabled  that is  while the program nms   sched    on-cpu uid = = 101  self ts timestamp ;  sched    off -cpu self ts   time  execname  self ts = 0 ; sum  timestamp self ts  ; the output of the program  showing the processes and how much time  in nanoseconds  they spend running on the cpus  is shown in figure 2.23 88 chapter 2 2.10 # dtrace -s sched.d dtrace  script 'sched.d ' matched 6 probes ac grwme-settings-d gnome-vfs-daemon dsdm wnck-applet gnome-panel clock-applet mapping-daemon xscreensaver meta city xorg gnome-terminal mixer applet2 java 142354 158243 189804 200030 277864 374916 385475 514177 539281 2579646 5007269 7388447 10769137 figure 2.23 output of the 0 code because dtrace is part of the open-source solaris 10 operating system  it is being added to other operating systems when those systems do not have conflicting license agreements for example  dtrace has been added to mac os x 10.5 and freebsd and will likely spread further due to its unique capabilities other operating systems  especially the linux derivatives  are adding kernel-tracing functionality as well still other operating systems are beginning to include performance and tracing tools fostered by research at various institutions  including the paradyn project it is possible to design  code  and implement an operating system specifically for one machine at one site more commonly  however  operating systems are designed to nm on any of a class of machines at a variety of sites with a variety of peripheral configurations the system must then be configured or generated for each specific computer site  a process sometimes known as system generation  sysgen   the operating system is normally distributed on disk  on cd-rom or dvd-rom  or as an iso image  which is a file in the format of a cd-rom or dvd-rom to generate a system  we use a special program this sysgen program reads from a given file  or asks the operator of the system for information concerning the specific configuration of the hardware systenc  or probes the hardware directly to determine what components are there the following kinds of information must be determined what cpu is to be used what options  extended instruction sets  floatingpoint arithmetic  and so on  are installed for multiple cpu systems  each cpu may be described 2.11 2.11 89 how will the boot disk be formatted how many sections  or partitions  will it be separated into  and what will go into each partition how much memory is available some systems will determine this value themselves by referencing memory location after memory location until an illegal address fault is generated this procedure defines the final legal address and hence the amount of available memory what devices are available the system will need to know how to address each device  the device number   the device interrupt number  the device 's type and model  and any special device characteristics what operating-system options are desired  or what parameter values are to be used these options or values might include how many buffers of which sizes should be used  what type of cpu-scheduling algorithm is desired  what the maximum number of processes to be supported is  and so on once this information is determined  it can be used in several ways at one extreme  a system administrator can use it to modify a copy of the source code of the operating system the operating system then is completely compiled data declarations  initializations  and constants  along with conditional compilation  produce an output-object version of the operating system that is tailored to the system described at a slightly less tailored level  the system description can lead to the creation of tables and the selection of modules from a precompiled library these modules are linked together to form the generated operating system selection allows the library to contain the device drivers for all supported i/0 devices  but only those needed are linked into the operating system because the system is not recompiled  system generation is faster  but the resulting system may be overly general at the other extreme  it is possible to construct a system that is completely table driven all the code is always part of the system  and selection occurs at execution time  rather than at compile or lil1.k time system generation involves simply creating the appropriate tables to describe the system the major differences among these approaches are the size and generality of the generated system and the ease of modifying it as the hardware configuration changes consider the cost of modifying the system to support a newly acquired graphics termil1.al or another disk drive balanced against that cost  of course  is the frequency  or infrequency  of such changes after an operating system is generated  it must be made available for use by the hardware but how does the hardware know where the kernel is or how to load that kernel the procedure of starting a computer by loading the kernel is known as booting the system on most computer systems  a small piece of code known as the bootstrap program or bootstrap loader locates the kernel  loads it into main memory  and starts its execution some computer systems  such as pcs  use a two-step process in which a simple bootstrap loader fetches a more complex boot program from disk  which in turn loads the kernel 90 chapter 2 2.12 when a cpu receives a reset event-for instance  when it is powered up or rebooted -the instruction register is loaded with a predefined memory location  and execution starts there at that location is the initial bootstrap program this program is in the form of read-only memory  rom   because the ram is in an unknown state at system startup rom is convenient because it needs no initialization and can not easily be infected by a computer virus the bootstrap program can perform a variety of tasks usually  one task is to run diagnostics to determine the state of the machine if the diagnostics pass  the program can continue with the booting steps it can also initialize all aspects of the system  from cpu registers to device controllers and the contents of main memory sooner or later  it starts the operating system some systems-such as cellular phones  pdas  and game consoles-store the entire operating system in rom storing the operating system in rom is suitable for small operating systems  simple supporting hardware  and rugged operation a problem with this approach is that changing the bootstrap code requires changing the rom hardware chips some systems resolve this problem by using erasable programmable read-only memory  eprom   which is readonly except when explicitly given a command to become writable all forms of rom are also known as firmware  since their characteristics fall somewhere between those of hardware and those of software a problem with firmware in general is that executing code there is slower thart executing code in ram some systems store the operating system in firmware and copy it to ram for fast execution a final issue with firmware is that it is relatively expensive  so usually only small ammmts are available for large operating systems  including most general-purpose operating systems like windows  mac os x  and unix  or for systems that change frequently  the bootstrap loader is stored in firmware  and the operating system is on disk in this case  the bootstrap nms diagnostics and has a bit of code that can read a single block at a fixed location  say block zero  from disk into memory and execute the code from that b ! ock the program stored in the boot block may be sophisticated enough to load the entire operating system into memory and begin its execution more typically  it is simple code  as it fits in a single disk block  and knows only the address on disk and length of the remainder of the bootstrap program is an example of an open-source bootstrap program for linux systems all of the disk-bound bootstrap  and the operating system itself  can be easily changed by writing new versions to disk a disk that has a boot partition  more on that in section 12.5.1  is called a boot disk or system disk now that the full bootsh ap program has been loaded  it can traverse the file system to find the operating system kernel  load it into memory  and start its execution it is only at this point that the system is said to be running operating systems provide a number of services at the lowest level  system calls allow a running program to make requests from the operating system directly at a higher level  the command interpreter or shell provides a mechanism for a user to issue a request without writing a program commands may come from files during batch-mode execution or directly from a terminal 91 when in an interactive or time-shared mode system programs are provided to satisfy many common u.ser requests the types of requests vary accord  ilcg to level the system-call level must provide the basic functions  such as process control and file and device manipulation higher-level requests  satisfied by the command interpreter or system programs  are translated into a sequence of system calls system services can be classified into several categories  program controt status requests  and i/0 requests program errors can be considered implicit requests for service once the system services are defined  the structure of the operating system can be developed various tables are needed to record the information that defines the state of the computer system and the status of the system 's jobs the design of a new operating system is a major task it is important that the goals of the system be well def  ilced before the design begins the type of system desired is the foundation for choices among various algorithms and strategies that will be needed s  iilce an operating system is large  modularity is important designing a system as a sequence of layers or using a microkernel is considered a good technique the virtual-machine concept takes the layered approach and treats both the kernel of the operat  ilcg system and the hardware as though they were hardware even other operating systems may be loaded on top of this virtual machine throughout the entire operating-system design cycle  we must be careful to separate policy decisions from implementation details  mechanisms   this separation allows maximum flexibility if policy decisions are to be changed later operating systems are now almost always written in a systemsimplementation language or in a higher-level language this feature improves their implementation  maintenance  and portability to create an operating system for a particular machine configuration  we must perform system generation debugging process and kernel failures can be accomplished through the use of de buggers and other tools that analyze core dumps tools such as dtrace analyze production systems to fucd bottlenecks and understand other system behavior for a computer system to begin running  the cpu must initialize and start executing the bootstrap program in firmware the bootstrap can execute the operating system directly if the operating system is also in the firmware  or it can complete a sequence in which it loads progressively smarter programs from firmware and disk until the operating system itself is loaded into memory and executed 2.1 what are the five major activities of an operat  ilcg system with regard to file management 2.2 what are the three major activities of an operating system with regard to memory management 92 chapter 2 2.3 why is a just-in-time compiler useful for executing java programs 2.4 the services and functions provided by an operating system can be divided into two main categories briefly describe the two categories and discuss how they differ 2.5 why is the separation of mechanism and policy desirable 2.6 would it be possible for the user to develop a new command interpreter using the system-call interface provided by the operating system 2.7 what is the purpose of the command interpreter why is it usually separate from the kernel 2.8 what is the main advantage for an operating-system designer of using a virtual-machine architecture what is the main advantage for a user 2.9 it is sometimes difficult to achieve a layered approach if two components of the operating system are dependent on each other identify a scenario in which it is unclear how to layer two system components that require tight coupling of their functionalities 2.10 what is the main advantage of the layered approach to system design what are the disadvantages of using the layered approach 2.11 what is the relationship between a guest operating system and a host operating system in a system like vmware what factors need to be considered in choosing the host operating system 2.12 describe three general methods for passing parameters to the operating system 2.13 what is the main advantage of the microkemel approach to system design how do user programs and system services interact in a microkernel architecture what are the disadvantages of usil1.g the microkernel approach 2.14 what system calls have to be executed by a command interpreter or shell in order to start a new process 2.15 what are the two models of interprocess conununication what are the strengths and weaknesses of the two approaches 2.16 the experimental synthesis operating system has an assembler incorporated in the kernel to optimize system-call performance  the kernel assembles routines within kernel space to minimize the path that the system call must take through the kernel this approach is the antithesis of the layered approach  in which the path through the kernel is extended to make buildu1.g the operating system easier discuss the pros and cons of the synthesis approach to kernel design and system-performance optimization 2.17 in what ways is the modular kernel approach similar to the layered approach in what ways does it differ from the layered approach 2.18 how could a system be designed to allow a choice of operating systems from which to boot what would the bootstrap program need to do 93 2.19 what are the advantages and disadvantages of using the same systemcall interface for manipulating both files and devices 2.20 describe how you could obtain a statistical profile of the amount of time spent by a program executing different sections of its code discuss the importance of obtaining such a statistical profile 2.21 why do some systems store the operating system in firmware  while others store it on disk 2.22 in section 2.3  we described a program that copies the contents of one file to a destination file this program works by first prompting the user for the name of the source and destilcation files write this program using either the win32 or posix api be sure to include all necessary error checking  including ensuring that the source file exists once you have correctly designed and tested the program  if you used a system that supports it  run the program using a utility that traces system calls linux systems provide the ptrace utility  and solaris systems use the truss or dtrace command on mac os x  the ktrace facility provides similar functionality as windows systems do not provide such features  you will have to trace through the win32 version of this program using a debugger 2.23 adding a system call to the linux kernel in this project you will study the system-call interface provided by the linux operating system and learn how user programs communicate with the operating system kernel via this interface your task is to i11corporate a new system call into the kernet thereby expanding the functionality of the operating system part 1  getting started a user-mode procedure call is performed by passing arguments to the called procedure either on the stack or through registers  saving the current state and the value of the program counter  and jumping to the beginning of the code corresponding to the called procedure the process continues to have the same privileges as before system calls appear as procedure calls to user programs but result i11 a change in execution context and privileges in linux on the intel386 architecture  a system call is accomplished by storing the system-call number into the eax register  storing arguments to the system call in other hardware registers  and executing a trap instruction  which is the 94 chapter 2 int ox80 assembly instruction   after the trap is executed  the systemcall number is used to index into a table of code pointers to obtain the starting address for the handler code implementing the system call the process then juxnps to this address  and the privileges of the process are switched from user to kernel mode with the expanded privileges  the process can now execute kernel code  which may include privileged instructions that can not be executed in user mode the kernel code can then carry out the requested services  such as interacting with i/o devices  and can perform process management and other activities that can not be performed in user mode the system call numbers for recent versions of the linux kernel are listed in lusr i src/linux-2 xl include/ asm-i386/unistd h  for instance  __ nr_close corresponds to the system call close 0  which is invoked for closin.g a file descriptor  and is defined as value 6  the list of pointers to system-call handlers is typically stored in the file lusrlsrcllinux-2.x/arch/i386/kernel/entry.s under the heading entry  sys_calltable   notice that sys_close is stored at entry number 6 in the table to be consistent with the system-call number defined in the unistd h file  the keyword .long denotes that the entry will occupy the same number of bytes as a data value of type long  part 2  building a new kernel before adding a system call to the kernel  you must familiarize yourself with the task of building the binary for a kernel from its source code and booting the machine with the newly built kernel this activity comprises the following tasks  some of which depend on the particular installation of the linux operating system in use obtain the kernel source code for the linux distribution if the source code package has already been installed on your machine  the corresponding files might be available under lusr i srcllinux or /usr i src/linux-2 x  where the suffix corresponds to the kernel version number   if the package has not yet been installed  it can be downloaded from the provider of your linux distribution or from http  l/www.kernel.org learn how to configure  compile  and install the kernel binary this will vary among the different kernel distributions  but some typical commands for building the kernel  after entering the directory where the kernel source code is stored  include  o make xconfig o make dep o make bzimage add a new entry to the set of boatable kernels supported by the system the linux operating system typically uses utilities such as lilo and grub to maintain a list ofbootable kernels from which the 95 user can choose during machine boot-up if your system supports lilo  add an entry to lilo conf  such as  image = /boot/bzimage.mykernel label = mykernel root = /dev/hda5 read-only where lbootlbzimage my kernel is the kernel image and my kernel is the label associated with the new kernel this step will allow you to choose the new kernel during the boot-up process you will then have the option of either booting the new kernel or booting the unmodified kernel if the newly built kernel does not ftmction properly part 3  extending the kernel source you can now experiment with adding a new file to the set of source files used for compiling the kernel typically  the source code is stored in the lusr i srcllinux-2 xlkernel directory  although that location may differ in your linux distribution there are two options for adding the system call the first is to add the system call to an existing source file in this directory the second is to create a new file in the source directory and modify lusr i srcllinux-2 xlkernelimakef ile to include the newly created file in the compilation process the advantage of the first approach is that when you modify an existing file that is already part of the compilation process  the makefile need not be modified part 4  adding a system call to the kernel now that you are familiar with the various background tasks corresponding to building and booting linux kernels  you can begin the process of adding a new system call to the linux kernel in this project  the system call will have limited functionality ; it will simply transition from user mode to kernel mode  print a message that is logged with the kernel messages  and transition back to user mode we will call this the helloworld system call while it has only limited functionality  it illustrates the system-call mechanism and sheds light on the interaction between user programs and the kernel create a new file called helloworld c to define your system call include the header files linuxllinkage h and linuxlkernel h add the following code to this file  # include linuxllinkage.h # include linuxlkernel.h asmlinkage int sysjhelloworld    printk  kern_emerg hello world !  ; return 1 ;  96 chapter 2 this creates a system call with the name sys_helloworld    if you choose to add this system call to an existing file in the source directory  all that is necessary is to add the sys_hellowor ld   function to the file you choose in the code  asmlinkage is a rellli ant from the days when linux used both c + + and c code and is used to indicate that the code is written in c the printk   function is used to print messages to a kernel log file and therefore may be called only from the kernel the kernel messages specified in the parameter to printk   are logged in the file /var/log/kernel/warnings the function prototype for the printk   call is defined in /usr /include/linux/kernel h define a new system call number for __ nr_helloworld in /usr/src/linux-2.x/include/asm-i386/unistd.h a user program can use this number to identify the newly added system call also be sure to increment the value for __ nr_syscalls  which is stored in the same file this constant tracks the number of system calls currently defuced in the kernel add an entry .long sys_helloworld to the sys_calltable definedinthe/usr/src/linux-2.x/arch/i386/kernel/entry.s file as discussed earlier  the system-call number is used to index into this table to find the position of the handler code for the invoked system call add your file helloworld c to the makefile  if you created a new file for your system call  save a copy of your old kernel binary image  in case there are problems with your newly created kernel   you can now build the new kernet rename it to distinguish it from the unmodified kernet and add an entry to the loader configuration files  such as lilo conf   after completing these steps  you can boot either the old kernel or the new kernel that contains your system call part 5  using the system call from a user program when you boot with the new kernet it will support the newly defined system call ; you now simply need to invoke this system call from a user program ordinarily  the standard c library supports an interface for system calls defined for the linux operating system as your new system call is not linked into the standard c library  however  invoking your system call will require manual intervention as noted earlie1 ~ a system call is invoked by storing the appropriate value in a hardware register and performing a trap instruction unfortunately  these low-level operations can not be performed using c language statements and instead require assembly instructions fortunately  linux provides macros for instantiating wrapper functions that contain the appropriate assembly instructions for instance  the following c program uses the _syscallo   macro to invoke the newly defined system call  # include linux/errno.h # include sys/syscall.h # include linux/unistd.h _syscallo  int  helloworld  ; main    helloworld   ;  97 the _syscallo macro takes two arguments the first specifies the type of the value returned by the system call ; the second is the name of the system call the name is used to identify the systemcall number that is stored in the hardware register before the trap instruction is executed if your system call requires arguments  then a different macro  such as _syscallo  where the suffix indicates the number of arguments  could be used to instantiate the assembly code required for performing the system call compile and execute the program with the newly built kernel there should be a message hello world ! in the kernel log file /var/log/kernel/warnings to indicate that the system call has executed as a next step  consider expanding the functionality of your system call how would you pass an integer value or a character string to the system call and have it printed illto the kernel log file what are the implications of passing pointers to data stored in the user program 's address space as opposed to simply passing an integer value from the user program to the kernel using hardware registers dijkstra  1968  advocated the layered approach to operating-system desigll  brinch-hansen  1970  was an early proponent of constructing an operating system as a kernel  or nucleus  on which more complete systems can be built system instrumentation and dynamic tracing are described in tamches and miller  1999   dtrace is discussed in cantrill et al  2004   the dtrace source code is available at ms-dos  version 3.1  is described in microsoft  1986   windows nt and windows 2000 are described by solomon  1998  and solomon and russinovich  2000   windows 2003 and windows xp internals are described in russinovich and solomon  2005   hart  2005  covers windows system $ programming in detail bsd unix is described in mckusick et al  1996   bovet and cesati  2006  thoroughly discuss the linux kernel several unix systems-including mach-are treated in detail in vahalia  1996   mac os x is presented at 98 chapter 2 http  i lwww apple comlmacosx and in singh  2007   solaris is fully described in mcdougall and mauro  2007   the first operating system to provide a virtual machine was the cp i 67 on an ibm 360167 the commercially available ibm vmi370 operating system was derived from cp 167 details regarding mach  a microkernel-based operating system  can be found in young et al  1987   kaashoeket al  1997  present details regarding exokernel operating systems  wherein the architecture separates management issues from protection  thereby giving untrusted software the ability to exercise control over hardware and software resources the specifications for the java language and the java virtual machine are presented by gosling et al  1996  and by lindholm and yellin  1999   respectively the internal workings of the java virtual machine are fully described by ven11ers  1998   golm et al  2002  highlight the jx operating system ; back et al  2000  cover several issues in the design of java operating systems more information on java is available on the web at part two a process can be thought of as a program in execution a process will need certain resources-such as cpu time  memory  files  and 1/0 devices -to accomplish its task these resources are allocated to the process either when it is created or while it is executing a process is the unit of work in most systems systems consist of a collection of processes  operating-system processes execute system code  and user processes execute user code all these processes may execute concurrently although traditionally a process contained only a single thread of control as it ran  most modem operating systems now support processes that have multiple threads the operating system is responsible for the following activities in connection with process and thread management  the creation and deletion of both user and system processes ; the scheduling of processes ; and the provision of mechanisms for synchronization  communication  and deadlock handling for processes 3.1 chapter early computer systems allowed only one program to be executed at a time this program had complete control of the system and had access to all the system 's resources in contrast  current-day computer systems allow multiple programs to be loaded into memory and executed concurrently this evolution required firmer control and more compartmentalization of the various programs ; and these needs resulted in the notion of a process/ which is a program in execution a process is the unit of work in a modern time-sharing system the more complex the operating system is  the more it is expected to do on behalf of its users although its main concern is the execution of user programs  it also needs to take care of various system tasks that are better left outside the kernel itself a system therefore consists of a collection of processes  operatingsystem processes executing system code and user processes executing user code potentially/ all these processes can execute concurrently/ with the cpu  or cpus  multiplexed among them by switching the cpu between processes  the operating system can make the computer more productive in this chapter/ you will read about what processes are and how they work to introduce the notion of a process a program in execution  which forms the basis of all computation to describe the various features of processes  including scheduling  creation and termination  and communication to describe communication in client-server systems a question that arises in discussing operating systems involves what to call all the cpu activities,_f \ _qcij  c  hj3ystem ~ xeq_l_ ~ _lqq.s_ ; .i ' \ 'b 'x  c9   _2l_ ! _i_ ! 11_e     _l  it ~ ds_ys ! ~  r  tl has user programs  or tas ~ ~  even on a single-user system such as microsoft 101 102 chapter 3 windows  a user may be able to run several programs at one time  a word processor  a web browse1 ~ and an e-mail package and even if the user can execute only one program at a time  the operating system may need to support its own internal programmed activities  such as memory management in many respects  all these activities are similar  so we call all of them processes _the ten  ns ~ job i  ! dq pl ~ e ~ .s etif  '_lised almost interchangeably in this te   t although we personally prefer the term process  much of operat1ng-system theory and terminology was developed during a time when the major activity of operating systems was job processing it would be misleading to avoid the use of commonly accepted terms that include the word job  such as job scheduling  simply because process has superseded job 3.1.1 the process informally  as mentioned earlier  a process is a program in execution a process is more than the program code  which is sometimes known as the text section it also includes the current activity  as represented by the value of the program counter and the contents of the processor 's registers a process generally also includes the process stack  which contains temporary data  such as function parameters  return addresses  and local variables   and a data section  which contains global variables a process may also include a heap  which is memory thatis dynamically allocated during process run time the structure of a process in memory is shown in figure 3.1 we emphasize that a program by itself is not a process ; a program is a passive entity  such as a file containing a list of instructions stored on disk  often called an executable file   whereas a process is an active entity  with a program counter specifying the next instruction to execute and a set of associated resources a program becomes a process when an executable file is loaded into memory two common techniques for loading executable files are double-clicking an icon representing the executable file and entering the name of the executable file on the command line  as in prog exe or a out  0 figure 3.1 process in memory 3.1 103 1/0 or event completion figure 3.2 diagram of process state although two processes may be associated with the same program  they are nevertheless considered two separate execution sequences for instance  several users may be running different copies of the mail program  or the same user may invoke many copies of the web browser program each of these is a separate process ; and although the text sections are equivalent  the data  heap  and stack sections vary it is also common to have a process that spawns many processes as it runs we discuss such matters in section 3.4 3.1.2 process state as a proces   ; excr  utes  it changes state the state of a process is defil1.ed in part by the current activity of that process each process may be in one of the following states  new the process is being created running instructions are being executed waiting the process is waiting for some event to occur  such as an i/0 completion or reception of a signal   ready the process is waiting to be assigned to a processor terminated the process has finished execution these names are arbitrary  and they vary across operating systems the states that they represent are found on all systems  however certain operating systems also more finely delineate process states it is important to realize that only one process can be running on any processor at any instant many processes may be ready and waiting  however the state diagram corresponding to these states is presented in figure 3.2 3.1.3 process control block _  1cb pr   cess isrepreserlt ~ pjnthe operatir1,g system l  jy a process_ coptrol blo_ck _  pcb  -alsocalled a taskcontrozbloclc a pcb is shown in figure 3.3 it contains mi  my pieces of iil.format1o11assodated with a specific process  including these  104 chapter 3 figure 3.3 process control block  pcb   process state the state may be new  ready runnil g  waiting  halted  and so on program counter the counter indicates the address of the next instruction to be executed for this process cpu registers the registers vary in number and type  depending on the computer architecture they mclude accumulators  index registers  stack pointers  and general-purpose registers  plus any condition-code information along with the program counter  this state information must be saved when an mterrupt occurs  to allow the process to be continued correctly afterward  figure 3.4   cpu-scheduling information this information includes a process priority  pointers to scheduling queues  and any other scheduling parameters  chapter 5 describes process scheduling  memory-management information this information may include such information as the value of the base and limit registers  the page tables  or the segment tables  dependmg on the memory system used by the operating system  chapter 8   accounting information this mformation includes the amount of cpu and real time used  time limits  account numbers  job or process numbers  and so on i/o status information this information includes the list of i/o devices allocated to the process  a list of open files  and so on in briet the pcb simply serves as the repository for any information that may vary from process to process 3.1.4 threads the process model discussed so far has implied that a process is a program that performs a single thread of execution for example  when a process is running a word-processor program  a single thread of instructions is being executed this single thread of control allows the process to perform only one 3.2 process p0 idle 3.2 operating system interrupt or system call process p1 executing idle figure 3.4 diagram showing cpu switch from process to process 105 task at one time the user can not simultaneously type in characters and run the spell checker within the same process  for example many modern operatin.g systems have extended the process concept to allow a process to have multiple threads of execution and thus to perform more than one task at a time on a system that supports threads  the pcb is expanded to include information for each thread other changes throughout the system are also needed to support threads chapter 4 explores multithreaded processes in detail the objective of multiprogramming is to have some process nnming at all times  to maximize cpu utilization the objective of time sharing is to switch the cpu among processes so frequently that users can interact with each program while it is run.ning to meet these objectives  the process scheduler selects an available process  possibly from a set of several available processes  for program execution on the cpu for a single-processor system  there will never be more than one running process if there are more processes  the rest will have to wait until the cpu is free and can be rescheduled 3.2.1 scheduling queues as processes enter the system  they are put into a job queue  which consists of all processes in the system the processes that are residing in main memory and are ready and waiting to execute are kept on a list called the ready queue 106 chapter 3 process representation in linux the process control block in the linux operating system is represented by the c struch1re task_struct this structure contains all the necessary information for representing a process  including the state of the process  scheduling and memory-management information  list of open files  and pointers to the process 's parent and any of its children  a process 's parent is the process that created it ; its children are any processes that it creates  some of these fields include  pid_t pid ; i process identifier i long state ; i state of the process i unsigned int time_slice i scheduling information i struct task_struct parent ; i this process 's parent i struct list__head children ; i this process 's children i struct files_struct files ; i list of open files i struct mm_struct mm ; i address space of this process i for example  the state of a process isrepresented by the field long state in this structure within the linux kernel  all active processes are represented using a doubly linked list of task_struct  and the kernel maintains a pointer -current -to the process currently executing on the system this is shown in figure 3.5 struct task_struct process information struct task_struct process information t current  currently executing proccess  figure 3.5 active processes in linux struct task_struct process information as an illustration of how the kernel might manipulate one of the fields in the task_struct for a specified process  let 's assume the system would like to change the state of the process currently running to the value new_state if currentis a pointer to the process currently executing  its state is changed with the following  current state = new_state ; this queue is generally stored as a linked list a ready-queue header contains pointers to the first and final pcbs in the list each pcb includes a pointer field that points to the next pcb in the ready queue 3.2 107 queue header mag tape ~ = 7  c 77 ~ unit 0 k  \ \ t_82_11 ~ ~ il = = figure 3.6 the ready queue and various 1/0 device queues the system also includes other queues when a process is allocated the cpu  it executes for a while and eventually quits  is interrupted  or waits for the occurrence of a particular event  such as the completion of an i/0 request suppose the process makes an i/o request to a shared device  such as a disk since there are many processes in the system  the disk may be busy with the i/0 request of some other process the process therefore may have to wait for the disk the list of processes waiting for a particular i/0 device is called a device queue each device has its own device queue  figure 3.6   a common representation of process scheduling is a queueing diagram  such as that in figure 3.7 each rectangular box represents a queue two types of queues are present  the ready queue and a set of device queues the circles represent the resources that serve the queues  and the arrows indicate the flow of processes in the system a new process is initially put in the ready queue it waits there until it is selected for execution  or is dispatched once the process is allocated the cpu and is executing  one of several events could occur  the process could issue an i/0 request and then be placed in an i/0 queue the process could create a new subprocess and wait for the subprocess 's termination the process could be removed forcibly from the cpu  as a result of an interrupt  and be put back in the ready queue 108 chapter 3 figure 3.7 queueing-diagram representation of process scheduling in the first two cases  the process eventually switches from the waiting state to the ready state and is then put back in the ready queue a process continues this cycle until it terminates  at which time it is removed from all queues and has its pcb and resources deallocated 3.2.2 schedulers a process migrates among the various scheduling queues throughout its lifetime the operating system must select  for scheduling purposes  processes from these queues in some fashion the selection process is carried out by the appropriate scheduler often  in a batch system  more processes are submitted than can be executed immediately these processes are spooled to a mass-storage device  typically a disk   where they are kept for later execution the long-term scheduler  or job scheduler  selects processes from this pool and loads them into memory for execution the short-term scheduler  or cpu scheduler  selects from among the processes that are ready to execute and allocates the cpu to one of them the primary distinction between these two schedulers lies in frequency of execution the short-term scheduler must select a new process for the cpu frequently a process may execute for only a few milliseconds before waiting for an i/0 request often  the short-term scheduler executes at least once every 100 milliseconds because of the short time between executions  the short-term scheduler must be fast if it takes 10 milliseconds to decide to execute a process for 100 milliseconds  then 10 i  100 + 10  = 9 percent of the cpu is being used  wasted  simply for scheduling the work the long-term scheduler executes much less freqvently ; minutes may separate the creation of one new process and the next the long-term scheduler controls the degree of multiprogramming  the number of processes in memory   if the degree of multiprogramming is stable  then the average rate of process creation must be equal to the average departure rate of processes leaving the system thus  the long-term scheduler may need to be invoked 3.2 109 only when a process leaves the system because of the longer interval between executions  the long-term scheduler can afford to take more tin e to decide which process should be selected for execution it is important that the long-term scheduler make a careful selection in general  most processes can be described as either i/ 0 bound or cpu bound an i/o-bound process is one that spends more of its time doing i/o than it spends doing computations a cpu-bound process  in contrast  generates i/0 requests infrequently  using more of its time doing computations it is important that the long-term scheduler select a good process mix of i/o-bound and cpu-bound processes if all processes are i/0 bound  the ready queue will almost always be empty  and the short-term scheduler will have little to do if all processes are cpu bound  the i/0 waiting queue will almost always be empty  devices will go unused  and again the system will be unbalanced the system with the best performance will thus have a combination of cpu-bound and i/o-bound processes on some systems  the long-term scheduler may be absent or minimal for example  time-sharing systems such as unix and microsoft windows systems often have no long-term scheduler but simply put every new process in memory for the short-term scheduler the stability of these systems depends either on a physical limitation  such as the number of available terminals  or on the self-adjusting nature of human users if performance declines to m acceptable levels on a multiuser system  some users will simply quit some operating systems  such as time-sharing systems  may introduce an additional  intermediate level of scheduling this medium-term scheduler is diagrammed in figure 3.8 the key idea behind a medium-term scheduler is that sometimes it can be advantageous to remove processes from memory  and from active contention for the cpu  and thus reduce the degree of multiprogramrning later  the process can be reintroduced into memory  and its execution can be continued where it left off this scheme is called swapping the process is swapped out  and is later swapped in  by the medium-term scheduler swapping may be necessary to improve the process mix or because a change in memory requirements has overcommitted available memory  requiring memory to be freed up swapping is discussed in chapter 8 swap in      partiaii  ' exec ~ t ~ d sw11pped-out processes  swap out figure 3.8 addition of medium-term scheduling to the queueing diagram 110 chapter 3 3.3 3.2.3 context switch as mentioned in section 1.2.1  interrupts cause the operating system to change a cpu from its current task and to run a kernel routine such operations happen frequently on general-purpose systems when an interrupt occurs  the system needs to save the current of the process running on the cpu so that it can restore that context when its processing is done  essentially suspending the process and then resuming it the context is represented in the pcb of the process ; it includes the value of the cpu registers  the process state  see figure 3.2   and memory-management information generically  we perform a of the current state of the cpu  be it in kernel or user mode  and then a to resu.me operations switching the cpu to another process requires performing a state save of the current process and a state restore of a different process this task is known as a when a context switch occurs  the kernel saves the context of the old process in its pcb and loads the saved context of the new process scheduled to run context-switch time is pure overhead  because the system does no useful work while switching its speed varies from machine to machine  depending on the memory speed  the number of registers that must be copied  and the existence of special instructions  such as a single instruction to load or store all registers   typical speeds are a few milliseconds context-switch times are highly dependent on hardware support for instance  some processors  such as the sun ultrasparc  provide multiple sets of registers a context switch here simply requires changing the pointer to the current register set of course  if there are more active processes than there are register sets  the system resorts to copying register data to and from memory  as before also  the more complex the operating system  the more work must be done during a context switch as we will see in chapter 8  advanced memory-management techniques may require extra data to be switched with each context for instance  the address space of the current process must be preserved as the space of the next task is prepared for use how the address space is preserved  and what amount of work is needed to preserve it  depend on the memory-management method of the operating system the processes in most systems can execute concurrently  and they may be created and deleted dynamically thus  these systems must provide a mechanism for process creation and termination in this section  we explore the n1.echanisms involved in creating processes and illustrate process creation on unix and windows systems 3.3.1 process creation a process may create several new processes  via a create-process system call  during the course of execution the creating process is called a parent process  and the new processes are called the children of that process each of these new processes may in turn create other processes  forming a tree of processes most operating systems  including unix and the windows family of operating systems  identify processes according to a unique process identifier 3.3 111  or pid   which is typically an integer number figure 3.9 illustrates a typical process tree for the solaris operating system  showing the name of each process and its pid in solaris  the process at the top of the tree is the sched process  with pid of 0 the sched process creates several children processes-including pageout and fsflush these processes are responsible for managing memory and file systems the sched process also creates the ini t process  which serves as the root parent process for all user processes in figure 3.9  we see two children of ini t-inetd and dtlogin inetd is responsible for networking services such as telnet and ftp ; dtlogin is the process representing a user login screen when a user logs in  dtlogin creates an x-windows session  xsession   which in turns creates the sdt_shel process below sdlshel  a user 's command-line shell-the c-shell or csh-is created in this commandline interface  the user can then invoke various child processes  such as the ls and cat commands we also see a csh process with pid of 7778 representing a user who has logged onto the system using telnet this user has started the netscape browser  pid of 7785  and the emacs editor  pid of 8105   on unix  we can obtain a listing of processes by using the ps command for example  the command ps -el will list complete information for all processes currently active in the system it is easy to construct a process tree similar to what is shown in figure 3.9 by recursively tracing parent processes all the way to the ini t process in general  a process will need certain resources  cpu time  memory  files  i/0 devices  to accomplish its task when a process creates a subprocess  that inetd pid = 140 dtlogin pid = 251 figure 3.9 a tree of processes on a typical solaris system 112 chapter 3 subprocess may be able to obtain its resources directly from the operating system  or it may be constrained to a subset of the resources of the parent process the parent may have to partition its resources among its children  or it may be able to share some resources  such as ncemory or files  among several of its children restricting a child process to a subset of the parent 's resources prevents any process from overloading the system by creating too many subprocesses in addition to the various physical and logical resources that a process obtains when it is created  initialization data  input  may be passed along by the parent process to the child process for example  consider a process whose function is to display the contents of a file-say  img.jpg-on the screen of a terminal when it is created  it will get  as an input from its parent process  the name of the file img.jpg  and it will use that file name  open the file  and write the contents out it may also get the name of the output device some operating systems pass resources to child processes on such a system  the new process may get two open files  img.jpg and the terminal device  and may simply transfer the datum between the two when a process creates a new process  two possibilities exist in terms of execution  the parent continues to execute concurrently with its children the parent waits until some or all of its children have terminated there are also two possibilities in terms of the address space of the new process  the child process is a duplicate of the parent process  it has the same program and data as the parent   the child process has a new program loaded into it to illustrate these differences  let 's first consider the unix operating system in unix  as we 've seen  each process is identified by its process identifier  which is a tmique integer a new process is created by the fork   system call the new process consists of a copy of the address space of the original process this mechanism allows the parent process to communicate easily with its child process both processes  the parent and the child  continue execution at the instruction after the fork    with one difference  the return code for the fork   is zero for the new  child  process  whereas the  nonzero  process identifier of the child is returned to the parent typically  the exec   system call is used after a fork   system call by one of the two processes to replace the process 's memory space with a new program the exec   system call loads a binary file into memory  destroying the memory image of the program containing the exec   system call  and starts its execution in this manner  the two processes are able to communicate and then go their separate ways the parent can then create more children ; or  if it has nothing else to do while the child runs  it can issue await   system call to move itself off the ready queue until the termination of the child the c program shown in figure 3.10 illustrates the unix system calls previously described we now have two different processes running copies of the same program the only difference is that the value of pid  the process 3.3 # include sysltypes.h # include stdio.h # include unistd.h int main    pid_t pid ;  i fork a child process i pid = fork   ; if  pid 0   i error occurred i fprintf  stderr  fork failed  ; return 1 ;  else if  pid = = 0   i child process i execlp  lbinlls  ls ,null  ;  else  i parent process i  i parent will wait for the child to complete i wait  null  ; printf  child complete  ; return 0 ; figure 3.10 creating a separate process using the unix fork   system call 113 identifier  for the child process is zero  while that for the parent is an integer value greater than zero  in fact  it is the actual pid of the child process   the child process inherits privileges and scheduling attributes from the parent  as well certain resources  such as open files the child process then overlays its address space with the unix command lbin/ls  used to get a directory listing  using the execlp   system call  execlp   is a version of the exec   system call   the parent waits for the child process to complete with the wait   system call when the child process completes  by either implicitly or explicitly invoking exit    the parent process resumes from the call to wait   ,where it completes using the exit   system call this is also illustrated in figure 3.11 parent wait resumes child ~  +  exit   figure 3.11 process creation using fork   system call 114 chapter 3 # include stdio.h # include windows.h int main  void   startupinfo si ; process_information pi ;  ii allocate memory zeromemory  &si  sizeof  si   ; si.cb = sizeof  si  ; zeromemory  &pi  sizeof  pi   ; ii create child process if  ! createprocess  null  ii use command line c  \ \ windows \ \ system32 \ \ mspaint.exe  ii command line null  ii do n't inherit process handle   null  ii do n't inherit thread handle false  ii disable handle inheritance 0  ii no creation flags null  ii use parent 's environment block null  ii use parent 's existing directory &si  &pi   fprintf  stderr  create process failed  ; return -1 ; ii parent will wait for the child to complete waitforsingleobject  pi.hprocess  infinite  ; printf  child complete  ; ii close handles closehandle  pi.hprocess  ; closehandle  pi.hthread  ; figure 3.12 creating a separate process using the win32 api as an alternative examplef we next consider process creation in windows processes are created in the win32 api using the createprocess   functionf which is similar to fork   in that a parent creates a new child process howeverf whereas fork   has the child process inheriting the address space of its parent createprocess   requires loading a specified program into the address space of the child process at process creation furthermoref whereas fork   is passed no parametersf createprocess   expects no fewer than ten parameters the c program shown in figure 3.12 illustrates the createprocess   functionf which creates a child process that loads the application mspaint ex e we opt for many of the default values of the ten parameters passed to createprocess    readers interested in pursuing the details of process 3.3 115 creation and management in the win32 api are encouraged to consult the bibliographical notes at the end of this chapter two parameters passed to createprocess   are instances of the startupinfo and process_information structures startupinfo specifies many properties of the new process  such as window size and appearance and handles to standard input and output files the process_information structure contains a handle and the identifiers to the newly created process and its thread we invoke the zeromemory   function to allocate memory for each of these structures before proceeding with createprocess    the first two parameters passed to createprocess   are the application name and command-line parameters if the application name is null  as it is in this case   the command-line parameter specifies the application to load in this instance  we are loading the microsoft windows mspaint.exe application beyond these two initial parameters  we use the default parameters for inheriting process and thread handles as well as specifying no creation flags we also use the parent 's existing environment block and starting directory last  we provide two pointers to the startupinfo and process.lnformation structures created at the beginning of the program in figure 3.10  the parent process waits for the child to complete by invoking the wait   system call the equivalent of this in win32 is wai tforsingleobj ect    which is passed a handle of the child process-pi hprocess-and waits for this process to complete once the child process exits  control returns from the wai tforsingleobj ect   function in the parent process 3.3.2 process termination a process terminates when it finishes executing its final statement and asks the operating system to delete it by using the exit   system call at that point  the process may return a status value  typically an integer  to its parent process  via the wait   system call   all the resources of the process-including physical and virtual memory  open files  and i/0 buffers-are deallocated by the operating system termination can occur in other circumstances as well a process can cause the termination of another process via an appropriate system call  for example  terminateprocess   in win32   usually  such a system call can be invoked only by the parent of the process that is to be terminated otherwise  users could arbitrarily kill each other 's jobs note that a parent needs to know the identities of its children thus  when one process creates a new process  the identity of the newly created process is passed to the parent a parent may terminate the execution of one of its children for a variety of reasons  such as these  the child has exceeded its usage of some of the resources that it has been allocated  to determine whether this has occurred  the parent m.ust have a mechanism to inspect the state of its children  the task assigned to the child is no longer required the parent is exiting  and the operating system does not allow a child to continue if its parent terminates 116 chapter 3 3.4 some systencs  including vms  do not allow a child to exist if its parent has terminated in such systems  if a process terminates  either normally or abnormally   then all its children must also be terminated this phenomenon  referred to as cascading termination  is normally initiated by the operating system to illustrate process execution and termination  consider that  in unix  we can terminate a process by using the exit   system call ; its parent process may wait for the termination of a child process by using the wait   system call the wait   system call returns the process identifier of a terminated child so that the parent can tell which of its children has terminated if the parent terminates  however  all its children have assigned as their new parent the ini t process thus  the children still have a parent to collect their status and execution statistics processes executing concurrently in the operating system may be either independent processes or cooperating processes a process is independent if it can not affect or be affected by the other processes executing in the system any process that does not share data with any other process is independent a process is cooperating if it can affect or be affected by the other processes executing in the system clearly  any process that shares data with other processes is a cooperating process there are several reasons for providing an environment that allows process cooperation  information sharing since several users may be interested in the same piece of information  for instance  a shared file   we must provide an environment to allow concurrent access to such information computation speedup if we want a particular task to run faster  we must break it into subtasks  each of which will be executing in parallel with the others notice that such a speedup can be achieved only if the computer has multiple processing elements  such as cpus or i/o channels   modularity we may want to construct the system in a modular fashion  dividing the system functions into separate processes or threads  as we discussed in chapter 2 convenience even an individual user may work on many tasks at the same time for instance  a user may be editing  printing  and compiling in parallel cooperating processes require an interprocess communication  ipc  mechanism that will allow them to exchange data and information there are two fundamental models of interprocess communication   1  shared memory and  2  message passing in the shared-memory model  a region of memory that is shared by cooperating processes is established processes can then exchange information by reading and writing data to the shared region in the messagepassing model  communication takes place by means of messages exchanged 3.4 117 process a process a 2 2 kernel  a   b  figure 3.13 communications models  a  message passing  b  shared memory between the cooperating processes the two communications models are conh asted in figure 3.13 both of the models just discussed are common in operating systems  and many systems implement both message passing is useful for exchanging smaller ammmts of data  because no conflicts need be avoided message passing is also easier to implement than is shared memory for intercomputer communication shared memory allows maximum speed and convenience of communication shared memory is faster than message passing  as messagepassing system.s are typically implemented using system calls and thus require the more time-consuming task of kernel irttervention in contrast  in sharedmemory systems  system calls are required only to establish shared-memory regions once shared memory is established  all accesses are treated as routine memory accesses  and no assistance from the kernel is required in the ren1.ainder of this section  we explore each of these ipc models in more detail 3.4.1 shared-memory systems interprocess communication using shared memory requires communicating processes to establish a region of shared memory typically  a shared-memory region resides in the address space of the process creating the sharedmemory segment other processes that wish to communicate using this sharedmemory segment must attach it to their address space recall that  normally  the operating system tries to prevent one process from accessing another process 's memory shared memory requires that two or more processes agree to remove this restriction they can then excbange information by reading and writing data in the shared areas the form of the data and the location are determined by these processes and are not under the operating system 's control the processes are also responsible for ensuring that they are not writing to the same location simultaneously 118 chapter 3 to illustrate the concept of cooperating processes  let 's consider the producer-consumer problem  which is a common paradigm for cooperating processes a producer process produces information that is consumed by a consumer process for example  a compiler may produce assembly code  which is consumed by an assembler the assembler  in turn  ncay produce object modules  which are consumed by the loader the producer-consumer problem also provides a useful metaphor for the client-server paradigm we generally think of a server as a producer and a client as a consumer for example  a web server produces  that is  provides  html files and images  which are consumed  that is  read  by the client web browser requesting the resource one solution to the producer-consumer problem uses shared memory to allow producer and consumer processes to run concurrently  we must have available a buffer of items that can be filled by the producer and emptied by the consumer this buffer will reside in a region of memory that is shared by the producer and consumer processes a producer can produce one item while the consumer is consuming another item the producer and consumer must be synchronized  so that the consumer does not try to consume an item that has not yet been produced two types of buffers can be used the places no practical limit on the size of the buffer the consumer may have to wait for new items  but the producer can always produce new items the assumes a fixed buffer size in this case  the consumer must wait if the buffer is empty  and the producer must wait if the buffer is full let 's look more closely at how the bounded buffer can be used to enable processes to share memory the following variables reside in a region of memory shared by the producer and consumer processes  # define buffer_size 10 typedef struct  item ; item buffer  buffer_size  ; int in = 0 ; int out = 0 ; the shared buffer is implemented as a circular array with two logical pointers  in and out the variable in points to the next free position in the buffer ; out points to the first full position in the buffer the buffer is empty when in = = out ; the buffer is full when   in + 1  % buffer_size  = = out the code for the producer and consumer processes is shown in figures 3.14 and 3.15  respectively the producer process has a local variable nextproduced in which the new item to be produced is stored the consumer process has a local variable next consumed in which the item to be consumed is stored this scheme allows at most buffer_size  1 items in the buffer at the same time we leave it as an exercise for you to provide a solution where buffer_size items can be in the buffer at the same time in section 3.5.1  we illustrate the posix api for shared memory 3.4 item nextproduced ; while  true    i produce an item in nextproduced i while    in + 1  % buffer_size  = = out  ; i do nothing i buffer  in  = nextproduced ; in =  in + 1  % buffer_size ; figure 3.'14 the producer process 119 one issue this illustration does not address concerns the situation in which both the producer process and the consumer process attempt to access the shared buffer concurrently in chapter 6  we discuss how synchronization among cooperating processes can be implemented effectively in a sharedmemory environment 3.4.2 message-passing systems lrt section 3.4.1  we showed how cooperating processes can communicate in a shared-memory environment the scheme requires that these processes share a region of memory and that the code for accessing and manipulating the shared memory be written explicitly by the application programmer another way to achieve the same effect is for the operating system to provide the means for cooperating processes to comm lmicate with each other via a message-passing facility message passing provides a mechanism to allow processes to communicate and to synchronize their actions without sharing the same address space and is particularly useful in a distributed environment  where the communicating processes may reside on different computers connected by a network for example  a chat program used on the world wide web could be designed so that chat participants communicate with one another by exchanging messages a message-passing facility provides at least two operations  send  message  and recei ve  message   messages sent by a process can be of either fixed or variable size if only fixed-sized messages can be sent  the system-level implementation is straightforward this restriction  however  makes the task item nextconsumed ; while  true    while  in = = out  ; ii do nothing nextconsumed = buffer  out  ; out =  out + 1  % buffer_size ; i consume the item in nextconsumed i figure 3.15 the consumer process 120 chapter 3 of programming more difficult conversely  variable-sized messages require a 1nore complex system-level implementation  but the programming task becomes simpler this is a coitlmon kind of tradeoff seen throughout operatingsystem design if processes p and q want to communicate  they must send messages to and receive messages from each other ; a communication link must exist between them this link can be implemented in a variety of ways we are concerned here not with the link 's physical implementation  such as shared memory  hardware bus  or network  which are covered in chapter 16  but rather with its logical implementation here are several methods for logically implementing a link and the send 0 i receive   operations  direct or indirect communication synchronous or asynchronous communication automatic or explicit buffering we look at issues related to each of these features next 3.4.2.1 naming processes that want to communicate must have a way to refer to each other they can use either direct or indirect communication under direct communication  each process that wants to comm lmicate must explicitly name the recipient or sender of the communication in this scheme  the send   and receive   primitives are defined as  send  p  message  -send a message to process p receive  q  message  -receive a message from process q a communication link in this scheme has the following properties  a link is established automatically between every pair of processes that want to communicate the processes need to know only each other 's identity to communicate a link is associated with exactly two processes between each pair of processes  there exists exactly one link this scheme exhibits symmetry in addressing ; that is  both the sender process and the receiver process must name the other to communicate a variant of this scheme employs asymmetry in addressing here  only the sender names the recipient ; the recipient is not required to name the sender in this scheme  the send   and receive   primitives are defined as follows  send  p  message  -send a message to process p receive  id  message  -receive a message from any process ; the variable id is set to the name of the process with which communication has taken place the disadvantage in both of these schemes  symmetric and asymmetric  is the limited modularity of the resulting process definitions changing the identifier of a process may necessitate examining all other process definitions all references to the old identifier must be found  so that they can be modified 3.4 121 to the new identifier in general  any such hard-coding techniques  where identifiers must be explicitly stated  are less desirable than techniques involving indirection  as described next with indirect communication  the messages are sent to and received from mailboxes  or ports a mailbox can be viewed abstractly as an object into which messages can be placed by processes and from which messages can be removed each mailbox has a w1.ique identification for example  posix message queues use an integer value to identify a mailbox in this scheme  a process can communicate with some other process via a number of different mailboxes two processes can communicate only if the processes have a shared mailbox  however the send   and receive 0 primitives are defined as follows  send  a  message  -send a message to mailbox a receive  a  message  -receive a message from mailbox a in this scheme  a communication link has the following properties  a link is established between a pair of processes only if both members of the pair have a shared mailbox a link may be associated with more than two processes between each pair of communicating processes  there may be a number of different links  with each link corresponding to one mailbox now suppose that processes p1  p2  and p3 all share mailbox a process p1 sends a message to a  while both p2 and p3 execute a receive 0 from a which process will receive the message sent by p1 the answer depends on which of the following methods we choose  allow a link to be associated with two processes at most allow at most one process at a time to execute a receive 0 operation allow the system to select arbitrarily which process will receive the message  that is  either p2 or p3  but not both  will receive the message   the system also may define an algorithm for selecting which process will receive the message  that is  round robin  where processes take turns receiving messages   the system may identify the receiver to the sender a mailbox may be owned eith ~ r by a process or by the operating system if the mailbox is owned by a process  that is  the mailbox is part of the address space of the process   then we distinguish between the owner  which can only receive messages through this mailbox  and the user  which can only send messages to the mailbox   since each mailbox has a unique owner  there can be no confusion about which process should receive a message sent to this mailbox when a process that owns a mailbox terminates  the mailbox disappears any process that subsequently sends a message to this mailbox must be notified that the mailbox no longer exists in contrast  a mailbox that is owned by the operating system has an existence of its own it is independent and is not attached to any particular process the operating system then must provide a mechanism that allows a process to do the following  122 chapter 3 create a new mailbox send and receive messages through the mailbox delete a mailbox the process that creates a new mailbox is that mailbox 's owner by default initially  the owner is the only process that can receive messages through this n  tailbox however  the ownership and receiving privilege may be passed to other processes through appropriate system calls of course  this provision could result in multiple receivers for each mailbox 3.4.2.2 synchronization communication between processes takes place through calls to send   and receive   primitives there are different design options for implementing each primitive message passing may be either blocking or nonblockingalso known as synchronous and asynchronous blocking send the sending process is blocked until the message is received by the receiving process or by the mailbox nonblocking send the sending process sends the message and resumes operation blocking receive the receiver blocks until a message is available nonblocking receive the receiver retrieves either a valid message or a null different combinations of send   and receive   are possible when both send   and receive   are blocking  we have a rendezvous between the sender and the receiver the solution to the producer-consumer problem becomes trivial when we use blocking send   and receive   statements the producer merely invokes the blocking send   call and waits until the message is delivered to either the receiver or the mailbox likewise  when the consumer invokes receive    it blocks until a message is available note that the concepts of synchronous and asynchronous occur frequently in operating-system i/0 algorithms  as you will see throughout this text 3.4.2.3 buffering whether communication is direct or indirect  messages exchanged by communicating processes reside in a temporary queue basically  such queues can be implemented in three ways  zero capacity the queue has a maximum length of zero ; thus  the link can not have any messages waiting in it in this case  the sender must block until the recipient receives the message bounded capacity the que ~ ue has finite length n ; thus  at most n messages can reside in it if the queue is not full when a new message is sent  the message is placed in the queue  either the message is copied or a pointer to the message is kept   and the sender can continue execution without 3.5 3.5 123 waiting the link 's capacity is finite  however if the link is full  the sender must block until space is available in the quelie unbounded capacity the queue 's length is potentially infinite ; thus  any number of messages can wait in it the sender never blocks the zero-capacity case is sometimes referred to as a message system with no buffering ; the other cases are referred to as systems with automatic buffering in this section  we explore three different ipc systems we first cover the posix api for shared memory and then discuss message passing in the mach operating system we conclude with windows xp  which interestingly uses shared memory as a mechanism for providing certain types of message passing 3.5.1 an example  posix shared memory several ipc mechanisms are available for posix systems  including shared memory and message passing here  we explore the posix api for shared memory a process must first create a shared memory segment using the shmget   system call  shmget   is derived from shared memory get   the following example illustrates the use of shmget    segment_id = shmget  ipcprivate  size  s_lrusr i s_lwusr  ; this first parameter specifies the key  or identifier  of the shared-memory segment if this is set to ipcprivate  a new shared-memory segment is created the second parameter specifies the size  in bytes  of the shared-memory segment finally  the third parameter identifies the mode  which indicates how the shared-memory segment is to be used-that is  for reading  writing  or both by setting the mode to s_lrusr 1 s_lwusr  we are indicating that the owner may read or write to the shared-memory segment a successful call to shmget   returns an integer identifier for the shared-memory segment other processes that want to use this region of shared memory must specify this identifier processes that wish to access a shared-memory segment must attach it to their address space using the shmat    shared memory attach  system call the call to shmat   expects three parameters as well the first is the integer identifier of the shared-memory segment being attached  and the second is a pointer location in memory indicating where the shared ncemory will be attached if we pass a value of null  the operating system selects the location on the user 's behalf the third parameter identifies a flag that allows the sharedmemory region to be attached in read-only or read-write mode ; by passing a parameter of 0  we allow both reads and writes to the shared region we attach a region of shared memory using shmat   as follows  shared_memory =  char  shmat  id  null  0  ; if successful  shmat   returns a pointer to the beginning location in memory where the shared-memory region has been attached 124 chapter 3 once the region of shared ncemory is attached to a process 's address space  the process can access the shared memory as a routine memory access using the pointer returned from shmat    in this example  shmat   returns a pointer to a character string thus  we could write to the shared-memory region as follows  sprintf  shared_memory  writing to shared memory  ; other processes sharing this segment would see the updates to the sharedmemory segment typically  a process using an existing shared-memory segment first attaches the shared-memory region to its address space and then accesses  and possibly updates  the region of shared memory when a process no longer requires access to the shared-memory segment it detaches the segment from its address space to detach a region of shared memory  the process can pass the pointer of the shared-memory region to the shmdt   system call  as follows  shmdt  shared_memory  ; finally  a shared-memory segment can be removed from the system with the shmctl   system call  which is passed the identifier of the shared segrnent along with the flag ipcrmid the program shown in figure 3.16 illustrates the posix shared-memory api just discussed this program creates a 4,096-byte shared-memory segment once the region of shared memory is attached  the process writes the message hi there ! to shared memory after outputting the contents of the updated memory  it detaches and removes the shared-memory region we provide further exercises using the posix shared-memory api in the programming exercises at the end of this chapter 3.5.2 an example  mach as an example of a message-based operating system  we next consider the mach operating system  developed at carnegie mellon university we introduced mach in chapter 2 as part of the mac os x operating system the mach kernel supports the creation and destruction of multiple tasks  which are similar to processes but have multiple threads of control most communication in mach-including most of the system calls and all intertask informationis carried out by messages messages are sent to and received from mailboxes  called ports in mach even system calls are made by messages when a task is created  two special n  tailboxes-the kernel mailbox and the notify mailbox-are also created the kernel mailbox is used by the kernel to communicate with the task the kernel sends notification of event occurrences to the notify port only three system calls are needed for message transfer the msg_send   call sends a message to a mailbox a message is received via msg_recei ve    remote procedure calls  rpcs  are executed via msg_rpc   ,which sends a message and waits for exactly one return message from the sender in this way  the rpc models a typical subroutine procedure call but can work between systems-hence the term remote the porlallocate   system call creates a new mailbox and allocates space for its queue of messages the maximum size of the message queue # include stdio.h # include syslshm.h # include syslstat.h int main    3.5 i the identifier for the shared memory segment i int segmenlid ; i a pointer to the shared memory segment i char shared_memory ; i the size  in bytes  of the shared memory segment i canst int size = 4096 ; i allocate a shared memory segment i 125 segment_id = shmget  ipc_private  size  s_irusr i s_iwusr  ;  i attach the shared memory segment i shared_memory =  char  shmat  segment_id  null  0  ; i write a message to the shared memory segment i sprint  shared_memory  hi there !  ; i now print out the string from shared memory i printf  % s \ n  shared_memory  ; i now detach the shared memory segment i shmdt  shared_memory  ; i now remove the shared memory segment i shmctl  segment_id  ipc_rmid  null  ; return 0 ; figure 3.16 c program illustrating posix shared-memory api defaults to eight messages the task that creates the mailbox is that mailbox 's owner the owner is also allowed to receive from the mailbox only one task at a time can either own or receive from a mailbox  but these rights can be sent to other tasks if desired the mailbox 's message queue is initially empty as messages are sent to the mailbox  the messages are copied into the mailbox all messages have the same priority mach guarantees that multiple messages from the same sender are queued in first-in  first-out  fifo  order but does not guarantee an absolute ordering for instance  messages from two senders may be queued in any order the messages themselves consist of a fixed-length header followed by a variable-length data portion the header indicates the length of the message and  indudes two mailbox names one mailbox name is the mailbox to which the message is being sent commonly  the sending thread expects a reply ; so 126 chapter 3 the mailbox name of the sender is passed on to the receiving task  which can use it as a return address the variable part of a message is a list of typed data items each entry in the list has a type  size  and value the type of the objects specified in the message is important  since objects defined by the operating system-such as ownership or receive access rights  task states  and memory segments-n ay be sent in messages the send and receive operations themselves are flexible for instance  when a message is sent to a mailbox  the mailbox may be full if the mailbox is not full  the message is copied to the mailbox  and the sending thread continues if the mailbox is full  the sending thread has four options  wait indefinitely until there is room in the mailbox wait at most n milliseconds do not wait at all but rather return immediately temporarily cache a message one message can be given to the operating system to keep  even though the mailbox to which that message is being sent is full when the message can be put in the mailbox  a message is sent back to the sender ; only one such message to a full mailbox can be pending at any time for a given sending thread the final option is meant for server tasks  such as a line-printer driver after finishing a request  such tasks may need to send a one-time reply to the task that had requested service ; but they must also continue with other service requests  even if the reply mailbox for a client is full the receive operation must specify the mailbox or mailbox set from which a message is to be received a mailbox set is a collection of mailboxes  as declared by the task  which can be grouped together and treated as one mailbox for the purposes of the task threads in a task can receive only from a mailbox or mailbox set for which the task has receive access a porlstatus   system call returns the number of messages in a given mailbox the receive operation attempts to receive from  1  any mailbox in a mailbox set or  2  a specific  named  mailbox if no message is waiting to be received  the receiving thread can either wait at most n milliseconds or not wait at all the mach system was especially designed for distributed systems  which we discuss in chapters 16 through 18  but mach is also suitable for singleprocessor systems  as evidenced by its inclusion in the mac os x system the major problem with message systems has generally been poor performance caused by double copying of messages ; the message is copied first from the sender to the mailbox and then from the mailbox to the receiver the mach message system attempts to avoid double-copy operations by using virtual-memory-management techniques  chapter 9   essentially  mach maps the address space containing the sender 's message into the receiver 's address space the message itself is never actually copied this message-management technique provides a large performance boost but works for only intrasystem messages the mach operating system is discussed in an extra chapter posted on our website 3.5 127 3.5.3 an example  windows xp the windows xp operating system is an example of modern design that employs modularity to increase functionality and decrease the time needed to implement new features windows xp provides support for multiple operating environments/ or subsystems/ with which application programs communicate via a n1.essage-passing mechanism the application programs can be considered clients of the windows xp subsystem server the message-passing facility in windows xp is called the facility the lpc in windows xp communicates between two processes on the same machine it is similar to the standard rpc mechanism that is widely used/ but it is optimized for and specific to windows xp like mach/ windows xp uses a port object to establish and maintain a connection between two processes every client that calls a subsystem needs a communication channet which is provided by a port object and is never inherited windows xp uses two types of ports  connection ports and communication ports they are really the same but are given different names according to how they are used cmmection ports are named objects and are visible to all processes ; they give applications a way to set up communication channels  chapter 22   the communication works as follows  the client opens a handle to the subsystem/ s connection port object the client sends a cmmection request the server creates two private conimunication ports and returns the handle to one of them to the client the client and server use the corresponding port handle to send messages or callbacks and to listen for replies windows xp uses two types of rnessage-passing techniques over a port that the client specifies when it establishes the channel the simplest/ which is used for small messages/ uses the port 's message queue as intermediate storage and copies the message from one process to the other under this method  messages of up to 256 bytes can be sent if a client needs to send a larger message  it passes the message through a which sets up a region of shared memory the client has to decide when it sets up the channel whether or not it will need to send a large message if the client determines that it does want to send large messages/ it asks for a section object to be created similarly  if the server decides that replies will be large  it creates a section object so that the section object can be used  a small message is sent that contains a pointer and size information about the section object this method is more complicated than the first method  but it avoids data copying in both cases  a callback mechanism can be used when either the client or the server can not respond immediately to a request the callback mechanism allows them to perform asynchronous message handling the structure of local procedure calls in windows xp is shown in figure 3.17 it is important to note that the lpc facility in windows xp is not part of the win32 api and hence is not visible to the application programmer rather  applications using the win32 api invoke standard remote procedure calls 128 chapter 3 3.6 client server connection request j connection i handle i port i handle i client i 1 communication port i ~ i server i handle communication port i shared section object  = 256 bytes  figure 3.17 local procedure calls in windows xp when the rpc is being invoked on a process on the same system  the rpc is indirectly handled through a local procedure call lpcs are also used in a few other functions that are part of the win32 api in section 3.4  we described how processes can communicate using shared memory and message passing these techniques can be used for communication in client-server systems  section 1.12.2  as well in this section  we explore three other strategies for communication ir1 client-server systems  sockets  remote procedure calls  rpcs   and pipes 3.6.1 sockets a is defined as an endpoint for communication a pair of processes communicating over a network employ a pair of sockets-one for each process a socket is identified by an ip address concatenated with a port number in general  sockets use a client-server architecture the server waits for incoming client requests by listening to a specified port once a request is received  the server accepts a cmmection from the client socket to complete the com1ection servers implementing specific services  such as telnet  ftp  and i-htp  listen to well-known ports  a telnet server listens to port 23 ; an ftp server listens to port 21 ; and a web  or http  server listens to port 80   all ports below 1024 are considered well known ; we can use them to implement standard services when a client process initiates a request for a connection  it is assigned a port by its host computer this port is some arbitrary number greater than 1024 for example  if a client on host x with ip address 146.86.5.20 wishes to establish a connection with a web server  which is listening on port 80  at address 161.25.19.8  host x may be assigned port 1625 the connection will consist of a pair of sockets   146.86.5.20  1625  on host x and  161.25.19.8  80  on the web server this situation is illustrated in figure 3.18 the packets traveling between the hosts are delivered to the appropriate process based on the destination port number 3.6 129 host x  i 46.86.5.20  socket  i 46.86.5.20  1 625  web server  i 61 .25 i 9.8  socket  i 61 .25 i 9.8  80  figure 3.18 communication using sockets all com1.ections must be unique therefore  if another process also on host x wished to establish another connection with the same web server  it would be assigned a port number greater than 1024 and not equal to 1625 this ensures that all com1.ections consist of a unique pair of sockets although most program examples in this text use c  we will illustrate sockets using java  as it provides a much easier interface to sockets and has a rich library for networking utilities those interested in socket programming inc or c + + should consult the bibliographical notes at the end of the chapter java provides three different types of sockets are implemented with the socket class use the datagram.socket class finally  the mul ticastsocket class is a subclass of the datagram.socket class a multicast socket allows data to be sent to multiple recipients our example describes a date server that uses connection-oriented tcp sockets the operation allows clients to request the current date and time from the server the server listens to port 6013  although the port could have any arbitrary number greater than 1024 when a cmmection is received  the server returns the date and time to the client the date server is shown in figure 3.19 the server creates a serversocket that specifies it will listen to port 6013 the server then begins listening to the port with the accept   method the server blocks on the accept   method waiting for a client to request a com1.ection when a connection request is received  accept   returns a socket that the server can use to communicate with the client the details of how the server communicates with the socket are as follows the server first establishes a printwri ter objectthatitwill use to communicate with the client a printwri ter object allows the server to write to the socket using the routine print   and println   methods for output the server process sends the date to the client  calling the method println    once it has written the date to the socket  the server closes the socket to the client and resumes listening for more requests a client communicates with the server by creating a socket and connecting to the port on which the server is listening we implement such a client in the 130 chapter 3 import java.net ; import java.io ; public class dateserver   public static void main  string   args   try    serversocket sock = new serversocket  6013  ; ii now listen for connections while  true    socket client = sock.accept   ; printwriter pout = new printwriter  client.getoutputstream    true  ; ii write the date to the socket pout.println  new java.util.date   .tostring    ; ii close the socket and resume ii listening for connections client close   ; catch  ioexception ioe   system.err.println  ioe  ;  figure 3.19 date server java program shown in figure 3.20 the client creates a socket and requests a connection with the server at ip address 127.0.0.1 on port 6013 once the connection is madef the client can read from the socket using normal stream i/0 statements after it has received the date from the serverf the client closes the socket and exits the ip address 127.0.0.1 is a special ip address known as the when a computer refers to ip address 127.0.0.t it is referring to itself this mechanism allows a client and server on the same host to communicate using the tcp /ip protocol the ip address 127.0.0.1 could be replaced with the ip address of another host running the date server in addition to an ip addressf an actual host namef such as www.westminstercollege.eduf can be used as well communication using sockets-although common and efficient-is considered a low-level form of communication between distributed processes one reason is that sockets allow only an unstructured stream of bytes to be exchanged between the communicating threads it is the responsibility of the client or server application to impose a structure on the data in the next two subsectionsf we look at two higher-level methods of communication  remote procedure calls  rpcs  and pipes 3.6 import java.net ; import java.io ; public class dateclient   public static void main  string   args   try    //make connection to server socket socket sock = new socket  127.0.0.1 ,6013  ; inputstream in = sock.getinputstream   ; bufferedreader bin = new bufferedreader  new inputstreamreader  in   ; ii read the date from the socket string line ; while   line = bin.readline    ! = null  system.out.println  line  ; ii close the socket connection sock close   ; catch  idexception ioe   system.err.println  ioe  ;  figure 3.20 date client 3.6.2 remote procedure calls 131 one of the most common forms of remote service is the rpc paradigm  which we discussed briefly in section 3.5.2 the rpc was designed as a way to abstract the procedure-call mechanism for use between systems with network connections it is similar in many respects to the ipc mechanism described in section 3.4  and it is usually built on top of such a system here  howeve1 ~ because we are dealing with an environment in which the processes are executing on separate systems  we must use a message-based communication scheme to provide remote service in contrast to the ipc facility  the messages exchanged in rpc communication are well structured and are thus no longer just packets of data each message is addressed to an rpc daemon listening to a port on the remote system  and each contains an identifier of the ftmction to execute and the parameters to pass to that function the function is then executed as requested  and any output is sent back to the requester in a separate message a port is simply a number included at the start of a message packet whereas a system normally has one network address  it can have many ports within that address to differentiate the many network services it supports if a rencote process needs a service  it addresses a message to the proper port for instance  132 chapter 3 if a system wished to allow other systems to be able to list its current users  it would have a daemon supporting such an rpc attached to a port-say port 3027 any remote system could obtain the needed information  that is  the list of current users  by sending an rpc message to port 3027 oil the server ; the data would be received in a reply message the semantics of rpcs allow a client to invoke a procedure on a remote host as it would invoke a procedure locally the rpc system hides the details that allow comnmnication to take place by providing a on the client side typically  a separate stub exists for each separate remote procedure when the client invokes a remote procedure  the rpc system calls the appropriate stub  passing it the parameters provided to the remote procedure this stub locates the port on the server and marshals the parameters parameter marshalling involves packaging the parameters into a form that can be transmitted over a network the stub then transmits a message to the server using message passing a similar stub on the server side receives this message and invokes the procedure on the server if necessary  return values are passed back to the client using the same teclu1.ique one issue that must be dealt with concerns differences in data representation on the client and server machines consider the representation of 32-bit integers some systems  known as big-endian  store the most significant byte first  while other systems  known as little-endian  store the least significant byte first neither order is better per se ; rather  the choice is arbitrary within a computer architecture to resolve differences like this  many rpc systems define a machine-independent representation of data one such representation is known as data on the client side  parameter marshalling involves converting the machine-dependent data into xdr before they are sent to the server on the server side  the xdr data are m1.marshalled and converted to the machine-dependent representation for the server another important issue involves the semantics of a call whereas local procedure calls fail only under extreme circumstances  rpcs can fait or be duplicated and executed more than once  as a result of common network errors one way to address this problem is for the operating system to ensure that messages are acted on exactly once  rather than at most once most local procedure calls have the exactly once functionality  but it is more difficult to implement first  consider at most once  this semantic can be implemented by attaching a timestamp to each message the server must keep a history of all the timestamps of messages it has already processed or a history large enough to ensure that repeated messages are detected incoming messages that have a timestamp already in the history are ignored the client can then send a message one or more times and be assured that it only executes once  generation of these timestamps is discussed in section 18.1  for exactly once/ ' we need to remove the risk that the server will never receive the reqliest to accomplish this  the server must implement the at most once protocol described above but must also acknowledge to the client that the rpc call was received and executed these ack messages are common throughout networking the client must resend each rpc call periodically until it receives the ack for that call another important issue concerns the communication between a server and a client with standard procedure calls  some form of binding takes place 3.6 133 during link  load  or execution time  chapter 8  so that a procedure call 's name is replaced by the memory address of the procedure call the rpc scheme requires a similar binding of the client and the server port  but how does a client know the port numbers on the server neither system has full information about the other because they do not share memory two approaches are common first  the binding information may be predetermined  in the form of fixed port addresses at compile time  an rpc call has a fixed port number associated with it once a program is compiled  the server can not change the port number of the requested service second  binding can be done dynamically by a rendezvous mechanism an operating system provides a rendezvous  also called a daemon on a fixed rpc port a client then sends a message containing the name of the rpc to the rendezvous daemon requesting the port address of the rpc it needs to execute the port number is returned  and the rpc calls can be sent to that port until the process terminates  or the server crashes   this method requires the extra overhead of the initial request but is more flexible than the first approach figure 3.21 shows a sample interaction client kejyt ! .c3  sends rness tqe  t     matchrnakecto fit  1d pgrtpuml   er messages from  client to  server f \ port  matchmakerf + 1  re  address for rpc x from  server to  client port  kernel re  rpcx port  p server figure 3.21 execution of a remote procedure call  rpc   134 chapter 3 the rpc scheme is useful in implementing a distriblited file system  chapter 17   such a system can be implemented as a set of rpc daemons and clients the messages are addressed to the distributed file system port on a server on which a file operation is to take place the message contains the disk operation to be performed the disk operation might be read  write  rename  delete  or status  corresponding to the usual file-related system calls the return message contains any data resulting from that call  which is executed by the dfs daemon on behalf of the client for instance  a message might contain a request to transfer a whole file to a client or be limited to a simple block request in the latter case  several such requests may be needed if a whole file is to be transferred 3.6.3 pipes a acts as a conduit allowin.g two processes to communicate pipes were one of the first ipc mechanisms in early unix systems and typically provide one of the simpler ways for processes to communicate with one another  although they also have some limitations in implementing a pipe  four issues must be considered  does the pipe allow unidirectional communication or bidirectional communication if two-way communication is allowed  is it half duplex  data can travel only one way at a time  or full duplex  data can travel in both directions at the same time  must a relationship  such as parent-child  exist between the commlmicating processes can the pipes communicate over a network  or must the communicating processes reside on the same machine in the following sections  we explore two common types of pipes used on both unix and windows systems 3.6.3.1 ordinary pipes ordinary pipes allow two processes to communicate in standard producerconsumer fashion ; the producer writes to one end of the  the and the consumer reads from the other end  the a result  ordinary pipes are unidirectional  allowing only one-way communication if two-way communication is required  two pipes must be used  with each pipe sending data in a different direction we next illustrate constructing ordinary pipes on both unix and windows systems in both program examples  one process writes the message greetings to the pipe  while the other process reads this message front the pipe on unix systems  ordinary pipes are constructed using the function pipe  int fd    this function creates a pipe that is accessed through the int fd   file descriptors  fd  0  is the read-end of the pipe  and fd  1  is the write end 3.6 135 parent child fd  o  fd  1  fd  o  fd  1  u  -pip-e -ou figure 3.22 file descriptors for an ordinary pipe unix treats a pipe as a special type of file ; thus  pipes can be accessed using ordinary read   and write   system calls an ordinary pipe can not be accessed from outside the process that creates it thus  typically a parent process creates a pipe and uses it to comnmnicate with a child process it creates via fork    recall from section 3.3.1 that a child process inherits open files from its parent since a pipe is a special type of file  the child inherits the pipe from its parent process figure 3.22 illustrates the relationship of the file descriptor fd to the parent and child processes in the unix progranc shown in figure 3.23  the parent process creates a pipe and then sends a fork   call creating the child process what occurs after the fork   call depends on how the data are to flow through the pipe in this instance  the parent writes to the pipe and the child reads from it it is important to notice that both the parent process and the child process initially close their unused ends of the pipe although the program shown in figure 3.23 does not require this action  it is an important step to ensure that a process reading from the pipe can detect end-of-file  read   returns 0  when the writer has closed its end of the pipe # include sys/types.h # include stdio.h # include string.h # include unistd.h # define buffer_size 25 # define read_end 0 # define write_end 1 int main  void   char write_msg  buffer_size  char read_msg  buffer_size  ; int fd  2  ; pid_t pid ; greetings ; program continues in figure 3.24 figure 3.23 ordinary pipes in unix 136 chapter 3  i create the pipe i if  pipe  fd  = = -1   fprintf  stderr  pipe failed  ; return 1 ;  i fork a child process i pid = fork   ; if  pid 0   i error occurred i fprintf  stderr  fork failed  ; return 1 ;  if  pid 0   i parent process i  i close the unused end of the pipe i close  fd  read_end   ; i write to the pipe i write  fd  write_end   write_msg  strlen  write_msg  + 1  ; i close the write end of the pipe i close  fd  write_end   ; else  i child process i  i close the unused end of the pipe i close  fd  write_end   ; i read from the pipe i read  fd  read_end   read_msg  buffer_size  ; printf  read % s  read_msg  ; i close the write end of the pipe i close  fd  read_end   ; return 0 ; figure 3.24 continuation of figure 3.23 program ordinary pipes on windows systems are termed and they behave similarly to their unix counterparts  they are unidirectional and employ parent-child relationships between the communicating processes in addition  reading and writing to the pipe can be accomplished with the ordinary readfile   and wri tefile   functions the win32 api for creating pipes is the createpi pe   function  which is passed four parameters  separate handles for  1  reading and  2  writing to the pipe  as well as  3  an instance of the startupinfo structure  which is used to specify that the child process is to 3.6 # include stdio.h # include stdlib.h # include windows.h # define buffer_size 25 int main  void   handle readhandle  writehandle ; startupinfo si ; process_information pi ; char message  buffer_size  greetings ; dword written ; program continues in figure 3.26 figure 3.25 windows anonymous pipes parent process 137 inherit the handles of the pipe furthermore   4  the size of the pipe  in bytes  may be specified figure 3.25 illustrates a parent process creating an anonymous pipe for communicating with its child unlike unix systems  in which a child process automatically inherits a pipe created by its parent  windows requires the programmer to specify which attributes the child process will inherit this is accomplished by first initializing the security--attributes structure to allow handles to be inherited and then redirecting the child process 's handles for standard input or standard output to the read or write handle of the pipe since the child will be reading from the pipe  the parent must redirect the child 's standard input to the read handle of the pipe furthermore  as the pipes are half duplex  it is necessary to prohibit the child from inheriting the write end of the pipe creating the child process is similar to the program in figure 3.12  except that the fifth parameter is set to true  indicating that the child process is to inherit designated handles from its parent before writing to the pipe  the parent first closes its unused read end of the pipe the child process that reads from the pipe is shown in figure 3.27 before reading from the pipe  this program obtains the read handle to the pipe by invoking getstdhandle    note that ordinary pipes require a parent-child relationship between the communicating processes on both unix and windows systems this means that these pipes can be used only for communication between processes on the same machine 3.6.3.2 named pipes ordinary pipes provide a simple communication mechanism between a pair of processes however  ordinary pipes exist only while the processes are communicating with one another on both unix and windows systems  once the processes have finished communicating and terminated  the ordinary pipe ceases to exist 138 chapter 3 i set up security attributes allowing pipes to be inherited i securi1yattributes sa =  sizeof  securityattributes   null  true  ; i allocate memory i zeromemory  &pi  sizeof  pi   ; i create the pipe i if  ! createpipe  &readhandle  &writehandle  &sa  0    fprintf  stderr  create pipe failed  ; return 1 ;  i establish the startjnfo structure for the child process i getstartupinfo  &si  ; si.hstdoutput = getstdhandle  std_outputjhandle  ; i redirect standard input to the read end of the pipe i si.hstdinput = readhandle ; si dwflags = startf _usestdhandles ; i do n't allow the child to inherit the write end of pipe i sethandleinformation  wri tehandle  handle_flagjnherit  0  ; i create the child process i createprocess  null  child.exe  null,null  true  i inherit handles i 0  null  null  &si  &pi  ; i close the unused end of the pipe i closehandle  readhandle  ; i the parent writes to the pipe i if  ! wri tefile  wri tehandle  message  buffer_size  &written  null   fprintf  stderr  error writing to pipe  ; i close the write end of the pipe i closehandle  writehandle  ; i wait for the child to exit i wai tforsingleobj ect  pi hprocess  infinite  ; closehandle  pi.hprocess  ; closehandle  pi.hthread  ; return 0 ;  figure 3.26 continuation of figure 3.25 program named pipes provide a much more powerful communication tool ; communication can be bidirectional  and no parent-child relationship is required once a named pipe is established  several processes can use it for 3.6 # include stdio.h # include windows.h # define buffer_stze 25 int main  void   handle readhandle ; char buffer  buffer_size  ; dword read ; i get the read handle of the pipe i readhandle getstdhandle  std_inpuli-iandle  ; i the child reads from the pipe i 139 if  readfile  readhandle  buffer  buffer_size  &read  null   printf  child read % s ,buffer  ; else fprintf  stderr  error reading from pipe  ; return 0 ;  figure 3.27 windows anonymous pipes -child process communication in fact  in a typical scenario  a named pipe has several writers additionally  named pipes continue to exist after communicating processes have finished both unix and windows systems support named pipes  although the details of implementation vary greatly next  we explore named pipes in each of these systems named pipes are referred to as fifos in unix systems once created  they appear as typical files in the file system a fifo is created with the mkfifo   system call and manipulated with the ordinary open    read    write    and close   system calls it will contirme to exist m til it is explicitly deleted from the file system although fifos allow bidirectional communication  only half-duplex transmission is permitted if data must travel in both directions  two fifos are typically used additionally  the communicating processes must reside on the same machine ; sockets  section 3.6.1  must be used if intermachine communication is required named pipes on windows systems provide a richer communication mechanism than their unix counterparts full-duplex communication is allowed  and the communicating processes may reside on either the same or different machines additionally  only byte-oriented data may be transmitted across a unix ftfo  whereas windows systems allow either byte or message-oriented data named pipes are created with the createnamedpipe   function  and a client can connect to a named pipe using connectnamedpipe    communication over the named pipe can be accomplished using the readfile   and wri tefile   functions 140 chapter 3 3.7 pipes in practice pipes are used quite often in the unix command-line environment for situations in which the output of one command serves as input to the second for example ; the unix ls command produces a directory listing for especially long directory listings ; the output may scroll through several screens the command more manages output by displaying only one screen of output at a time ; the user must press the space bar to move from one screen to the next setting up a pipe between the ls and more commands  which are running as individual processes  allows the output of ls to be delivered as the input to moref enabling the user to display a large directory listing a screen at a time a pipe can be constructed on the command line using the i character the complete command is ls i more in this scenario ; the ls corrm1and serves as the producer  and its output is consumed by the more command windows systems provide a more command for the dos shell with functionality similar to that of its unix cmmterpart the dos shell also uses the i character for establishing a pipe the only difference is that to get a directory listing  dos uses the dir command rather than ls the equivalent command in dos to what is shown above is dir i more a process is a program in execution as a process executes/ it changes state the state of a process is defined by that process 's current activity each process may be in one of the following states  new  readyf running  waiting ; or terminated each process is represented in the operating system by its own process control block  pcb   a process ; when it is not executing ; is placed in some waiting queue there are two major classes of queues in an operating system  i/0 request queuecc and the ready queue the ready queue contains all the processes that are ready to execute and are waiting for the cpu each process is represented by a pcbf and the pcbs can be linked together to form a ready queue long-term  job  scheduling is the selection of processes that will be allowed to contend for the cpu normally  long-term scheduling is heavily influenced by resourceallocation considerations  especially memory management short-term  cpu  scheduling is the selection of one process from the ready queue operating systems must provide a mechanism for parent processes to create new child processes the parent may wait for its children to terminate before proceeding  or the parent and children may execute concurrently there are several reasons for allowing concurrent execution  information sharing  computation speedup  modularity  and convenience 141 the processes executing in the operating system may be either independent processes or cooperating processes cooperating processes require an interprocess communication mechanisnc to commlmicate with each other principally  communication is achieved through two schemes  shared mernory and message passing the shared-memory method requires communicating processes to share some variables the processes are expected to exchange information through the use of these shared variables in a shared-memory system  the responsibility for providing communication rests with the application programmers ; the operating system needs to provide only the shared memory the message-passing method allows the processes to exchange messages the responsibility for providing communication may rest with the operating system itself these two schemes are not mutually exclusive and can be used simultaneously within a single operating system communication in client-server systems may use  1  sockets   2  remote procedure calls  rpcs   or  3  pipes a socket is defined as an endpoint for communication a connection between a pair of applications consists of a pair of sockets  one at each end of the communication chamcel rpcs are another form of distributed commlmication an rpc occurs when a process  or thread  calls a procedure on a remote application ordinary pipes allow communication between parent and child processes  while named pipes permit unrelated processes to communicate with one another 3.1 what are the benefits and the disadvantages of each of the following consider both the system level and the programmer level a synchronous and asynchronous commmucation b automatic and explicit buffering c send by copy and send by reference d fixed-sized and variable-sized messages 3.2 consider the rpc mechanism describe the undesirable consequences that could arise from not enforcing either the at most once or exactly once semantic describe possible uses for a mechanism that has neither of these guarantees 3.3 with respect to the rpc mechanism  consider the exactly once semantic does the algorithm for implementing this semantic execute correctly even if the ack message back to the client is lost due to a network problem describe the sequence of messages and discuss whether exactly once is still preserved 3.4 palm os provides no means of concurrent processing discuss three major complications that concurrent processing adds to an operating system 142 chapter 3 3.5 describe the actions taken by a kernel to context-switch between processes 3.6 the sun ultrasparc processor has multiple register sets describe what happens when a context switch occurs if the new context is already loaded into one of the register sets what happens if the new context is in memory rather than in a register set and all the register sets are in use 3.7 construct a process tree similar to figure 3.9 to obtain process information for the unix or linux system  use the command ps -ael use the command man ps to get more information about the ps command on windows systems  you will have to use the task manager 3.8 give an example of a situation in which ordinary pipes are more suitable than named pipes and an example of a situation in which named pipes are more suitable than ordinary pipes 3.9 describe the differences among short-term  medium-term  and longterm scheduling 3.10 including the initial parent process  how many processes are created by the program shown in figure 3.28 3.11 using the program in figure 3.29  identify the values of pid at lines a  b  c  and d  assume that the actual pids of the parent and child are 2600 and 2603  respectively  # include stdio.h # include unistd.h int main     i fork a child process i fork   ; i fork another child process i fork   ; i and fork another i fork   ; return 0 ; figure 3.28 how many processes are created # include sysltypes.h # include stdio h # include unistd.h int main    pid_t pid ' pid1 ;  i fork a child process i pid = fork   ; if  pid 0   i error occurred i fprintf  stderr  fork failed  ; return 1 ;  else if  pid = = 0   i child process i pid1 = getpid   ;  printf  child  pid = % d ,pid  ; i a i printf  child  pid1 = % d ,pid1  ; i b i else  i parent process i pid1 = getpid   ;  printf  parent  pid = % d ,pid  ; i c i printf  parent  pid1 = % d ,pid1  ; i d i wait  null  ; return 0 ; figure 3.29 what are the pid values 143 3.12 using the program shown in figure 3.30  explain what the output will be at line a 3.13 the fibonacci sequence is the series of numbers 0  1  1  2  3  5  8   formally  it can be expressed as  fib 0 = 0 fibl = 1 jibn = jibn-l + jibn-2 write a c program using the fork   system call that generates the fibonacci sequence in the child process the number of the sequence will be provided in the comm_and line for example  if 5 is provided  the first five numbers in the fibonacci sequence will be output by the child 144 chapter 3 # include sysltypes.h # include stdio.h # include unistd.h int value = 5 ; int main    pid_t pid ;  pid = fork   ; if  pid = = 0   i child process i value + = 15 ; return 0 ;  else if  pid 0   i parent process i wait  null  ;  printf  parent  value = % d ,value  ; i line a i return 0 ; figure 3.30 what output will be at line a process because the parent and child processes have their own copies of the dataf it will be necessary for the child to output the sequence have the parent invoke the wait   call to wait for the child process to complete before exiting the program perform necessary error checking to ensure that a non-negative number is passed on the command line 3.14 repeat the preceding exercisef this time using the createprocess   function in the win32 api in this instancef you will need to specify a separate program to be invoked from createprocess    it is this separate program that will run as a child process outputting the fibonacci sequence perform necessary error checking to ensure that a non-negative number is passed on the command line 3.15 modify the date server shown in figure 3.19 so that it delivers random jokes rather than the current date allow the jokes to contain multiple lines the date client shown in figure 3.20 can be used to read the multi-line jokes returned by the joke server 3.16 an echo server echoes back whatever it receives from a client for examplef if a client sends the server the string hello there ! the server will respond with the exact data it received from the client-that isf hello there ! 145 write an echo server using the java networking api described in section 3.6.1 this server will wait for a client connection using the accept   method when a client connection is received  the server will loop  perfonning the following steps  read data from the socket into a buffer write the contents of the buffer back to the client the server will break out of the loop only when it has determined that the client has closed the connection the server shown in figure 3.19 uses the java io bufferedreader class bufferedreader extends the java io reader class  which is used for reading character streams however  the echo server can not guarantee that it will read characters from clients ; it may receive binary data as well the class java io input stream deals with data at the byte level rather than the character level thus  this echo server must use an object that extends java io inputstrearn the read   method in the java io inputstrearn class returns -1 when the client has closed its end of the socket connection 3.17 in exercise 3.13  the child process must output the fibonacci sequence  since the parent and child have their own copies of the data another approach to designing this program is to establish a shared-memory segment between the parent and child processes this technique allows the child to write the contents of the fibonacci sequence to the sharedmemory segment and has the parent output the sequence when the child completes because the memory is shared  any changes the child makes will be reflected in the parent process as well this program will be structured using posix shared memory as described in section 3.5.1 the program first requires creating the data structure for the shared-memory segment this is most easily accomplished using a struct this data structure will contain two items   1  a fixed-sized array of size malsequence that will hold the fibonacci values and  2  the size of the sequence the child process is to generatesequence_ size  where sequence_size     malsequence these items can be represented in a struct as follows  # define max_sequence 10 typedef struct  long fib_sequence  max_sequence  ; int sequence_size ;  shared_data ; the parent process will progress thmugh the following steps  a accept the parameter passed on the command line and perform error checking to ensure that the parameter is     max_sequence b create a shared-memory segment of size shared_data c attach the shared-memory segment to its address space 146 chapter 3 d set the value of sequence_size to the parameter on the command line e fork the child process and invoke the wait   systen1 call to wait for the child to finish f output the value of the fibonacci sequence in the shared-memory segment g detach and remove the shared-memory segment because the child process is a copy of the parent  the shared-memory region will be attached to the child 's address space as well as the parent 's the child process will then write the fibonacci sequence to shared memory and finally will detach the segment one issue of concern with cooperating processes involves synchronization issues in this exercise  the parent and child processes must be synchronized so that the parent does not output the fibonacci sequence until the child finishes generating the sequence these two processes will be synchronized using the wait   system call ; the parent process will invoke wait    which will cause it to be suspended until the child process exits 3.18 design a program using ordinary pipes in which one process sends a string message to a second process  and the second process reverses the case of each character in the message and sends it back to the first process for example  if the first process sends the message hi there  the second process will return hi there this will require using two pipes  one for sending the original message from the first to the second process  and the other for sending the modified message from the second back to the first process you may write this program using either unix or windows pipes 3.19 design a file-copying program named filecopy using ordinary pipes this program will be passed two parameters  the first is the name of the file to be copied  and the second is the name of the copied file the program will then create an ordinary pipe and write the contents of the file to be copied to the pipe the child process will read this file from the pipe and write it to the destination file for example  if we invoke the program as follows  filecopy input.txt copy.txt the file input txt will be written to the pipe the child process will read the contents of this file and write it to the destination file copy txt you may write this program using either unix or windows pipes 3.20 most unix and linux systems provide the ipcs command this command lists the status of various posix interprocess communication mechanisms  including shared-memory segments much of the information for the command comes from the data structure struct shmid_ds  147 which is available in the /usr/include/sys/shm.h file some of the fields in this structure include  int shm_segsz-size of the shared-memory segment short shm__nattch-number of attaches to the shared-memory segment struct ipc_perm shm_perm-permission structure of the sharedmemory segment the struct ipc_perm data structure  which is available in the file /usr/include/sys/ipc .h  contains the fields  unsigned short uid -identifier of the user of the shared -memory segment unsigned short mode-permission modes key_t key  on linux systems  __ key  -user-specified key identifier the permission modes are set according to how the shared-memory segment is established with the shmget   system call permissions are identified according to the following  write permission of owner 0040 read permission of group 0020 write permission of group 0004 read permission of world 0002 write permissionof world permissions can be accessed by using the bitwise and operator & for example  if the statement mode & 0400 evaluates to true  the permission mode gives read permission to the owner of the sharedmemory segment a shared-memory segment can be identified according to a userspecified key or according to the integer value returned from the shmget   system call  which represents the integer identifier of the shared-memory segment created the shm_ds structure for a given integer segment identifier can be obtained with the following shmctl   system call  i identifier of the shared memory segment / int segment_id ; shm_ds shmbuffer ; shmctl  segment_id  ipc_stat  &shmbuffer  ; 148 chapter 3 if successful  shmctl   returns 0 ; otherwise  it returns -1 indicating an error condition  the global variable errno can be accessed to determine the error condition   write a c program that is passed an identifier for a shared-memory segment this program will invoke the shmctl   function to obtain its shm_ds structure it will then output the following values of the shared-memory segment  segmentid key mode owner did size number of attaches 3.21 posix message passing this project consists of using posix message queues for communicating temperatures between each of four external processes and a central process the project can be completed on systems that support posix message passing  such as unix  linux  and mac os x part 1  overview four external processes will communicate temperatures to a central process  which in turn will reply with its own temperature and will indicate whether the entire system has stabilized each process will receive its initial temperature upon creation and will recalculate a new temperature according to two formulas  new external temp =  mytemp 3 + 2 centraltemp  i 5 ; new central temp =  2 centraltemp + four temps received from external processes  i 6 ; initially  each external process will send its temperature to the mailbox for the central process if all four temperatures are exactly the same as those sent by the four processes during the last iteration  the system has stabilized in this case  the central process will notify each external process that it is now finished  along with the central process itself   and each process will output the final stabilized temperature if the system has not yet become stable  the central process will send its new temperature to the mailbox for each of the outer processes and await their replies the processes will continue to run until the temperature has stabilized 149 part 2  the message passing system processes can exchange messages by using four system calls  msgget    msgsnd    msgrcv    and msgctl    the msgget   function converts a mailbox name to a message queue id  msqid  a mailbox name is an externally known message queue name that is shared among the cooperating processes  msqid  the internal identifier returned by msgget    must be passed to all subsequent system calls using this message queue to facilitate interprocess communication a typical invocation of msgget   is seen below  msqid = msgget  1234  0600 i ipc_creat  ; the first parameter is the name of the mailbox  and the second parameter instructs the operating system to create the message queue if it does not already exist  with read and write privileges only for processes with the same user id as this process if a message queue already exists for this mailbox name  msgget   returns the msqid of the existing mailbox to avoid attaching to an existing message queue  a process can first attempt to attach to the mailbox by omitting ipc_creat and then checking the return value from msgget    if msq id is negative  an error has occurred during the system calt and the globally accessible variable errno can be consulted to determine whether the error occurred because the message queue already exists or for some other reason if the process determines that the mailbox does not currently exist it can then create it by including ipc_creat  for the current project  this strategy should not be necessary if students are using standalone pcs or are assigned unique ranges of mailbox names by the instructor  once a valid msqid has been established  a process can begin to use msgsnd   to send messages and msgrcv   to receive messages the messages being sent and received are similar in format to those described in section 3.5.2  as they include a fixed-length portion at the beginning followed by a variable-length portion obviously  the senders and receivers must agree on the format of the messages being exchanged since the operating system specifies one field in the fixed-length portion of every message format and at least one piece of information will be sent to the receiving process  it is logical to create a data aggregate for each type of message using a struct the first field of any such struct must be a long  and it will contain the priority of the message  this project does not use this functionality ; we recommend that you simply make the first field in every message equal to the same integral value  such as 2  other fields in the messages contain the information to be shared between the communicating processes three additional fields are recommended   1  the temperature being sent   2  the process number of the external process sending the message  0 for the central process   and  3  a flag that is set to 0 but that the central process will set to 1 when it notices stability a recommended struct appears as follows  150 chapter 3 struct  long priority ; int temp ; int pid ; int stable ;  msgp ; assuming the msqid has been established  examples of msgsnd   and msgrcv   appear as such  int stat  msqid ; stat = msgsnd  msqid  &msgp  sizeof  msgp  -sizeof  long   0  ; stat msgrcv  msqid  &msgp  sizeof  msgp  -sizeof  long   2  0  ; the first parameter in both system calls must be a valid msq id ; otherwise a negative value is returned  both functions return the number of bytes sent or received upon successful completion of the operation  the second parameter is the address of where to find or store the message to be sent or received  followed by the number of information bytes to be sent or received the final parameter of 0 indicates that the operations will be synchronous and that the sender will block if the message queue is full  ipc_nowait would be used if asynchronous  or nonblocking  operations were desired each individual message queue can hold a maximum number of messages-or bytes-so it is possible for the queue to become filled  which is one reason a sender may block when attempting to transmit a message  the 2 that appears before this final parameter in msgrcv   indicates the minimum priority level of the messages the process wishes to receive ; the receiver will wait until a message of that priority  or higher  is sent to the msqid if this is a synchronous operation once a process is finished using a message queue  it must be removed so that the mailbox can be reused by other processes unless it is removed  the message queue-and any messages that have not yet been received-will remain in the storage space that has been provided for this mailbox by the kernel to remove the message queue  and delete any unread messages therein  it is necessary to invoke msgctl    as follows  struct msgid_ds dummyparam ; status = msgctl  msqid  ipc_rmid  &dummyparam  ; the third parameter is necessary because this function requires it but it is used only if it the programmer wishes to collect some statistics about the usage of the message queue this is accomplished by substituting ipc_stat as the second parameter all programs should include the following three header files  which are found in /usr/include/sys  ipc.h  types.h  and msg.h one possibly confusing artifact of the message queue implementation bears 151 mentioning at this point after a mailbox is removed via msgctl   ,any subsequent attempts to create another mailbox with that same name using msgget   will typically generate a different msqid part 3  creating the processes each external process  as well as the central server  will create its own mailbox with the name x + i  where i is a numeric identifier of the external process 1..4 or zero for the central process thus  if x were 70  then the central process would receive messages in the mailbox named 70  and it would send its replies to mailboxes 71-74 outer process 2 would receive in mailbox 72 and would send to mailbox 70  and so forth thus  each external process will attach to two mailboxes  and the central process will attach to five if each process specifies ipc_creat when invoking msgget    the first process that invokes msgget   actually creates the mailbox ; subsequent calls to msgget   attach to the existing mailbox the protocol for removal should be that the mailbox/message queue that each process is listening to should be the only one it removes -via msgctl     each external process will be uniquely identified by a command-line parameter the first parameter to each external process will be its initial temperature  and the second parameter will be its unique number  1  2  3  or 4 the central server will be passed one parameter-its initial temperature assuming the executable name of the external process is external and the central server is central  invoking all five processes will be done as follows  ./external 100 1 & ./external 22 2 & ./external 50 3 & ./external 40 4 & ./central 60 & part 4  implementation hints it might be best to start by sending one message successfully from the central process to a single outer process  and vice versa  before trying to write all the code to solve this problem it is also wise to check all the return values from the four message queue system calls for possible failed requests and to output a message to the screen after each one successfully completes the message should indicate what was accomplished and by whom -for instance  mailbox 71 has been created by outer process 1/ ' message received by central process from external process 2/ ' and so forth these messages can be removed or commented out after the problem is solved processes should also verify that they were passed the correct number of command-line parameters  via the argc parameter in main     finally  extraneous messages residing in a queue can cause a collection of cooperating processes that function correctly to appear erroneous for that reason  it is wise to remove all mailboxes relevant to this project to ensure that mailboxes are empty before the processes begin the easiest way to do this is to use the 152 chapter 3 ipcs command to list all message queues and the ipcrm command to remove existing message queues the ipcs command lists the msqid of all message queues on the system use ipcrm to remove message queues according to their msqid for example  if msqid 163845 appears with the output of ipcs  it can be deleted with the following command  ipcrm -q 163845 interprocess communication in the rc 4000 system is discussed by brinchhansen  1970   schlichting and schneider  1982  discuss asynchronous message-passing prirnitives the ipc facility implemented at the user level is described by bershad et al  1990   details of interprocess communication in unix systems are presented by gray  1997   barrera  1991  and vahalia  1996  describe interprocess communication in the mach system russinovich and solomon  2005   solomon and russinovich  2000   and stevens  1999  outline interprocess communication in windows 2003  windows 2000 and unix respectively hart  2005  covers windows systems programming in detail the implementation of rpcs is discussed by birrell and nelson  1984   shrivastava and panzieri  1982  describes the design of a reliable rpc mechanism  and tay and ananda  1990  presents a survey of rpcs stankovic  1982  and stmmstrup  1982  discuss procedure calls versus message-passing communication harold  2005  provides coverage of socket programming in java hart  2005  and robbins and robbins  2003  cover pipes in windows and unix systems  respectively 4.1 chapter the process model introduced in chapter 3 assumed that a process was an executing program with a single thread of control most modern operating systems now provide features enabling a process to contain multiple threads of control this chapter introduces many concepts associated with multithreaded computer systems  including a discussion of the apis for the pthreads  win32  and java thread libraries we look at many issues related to multithreaded programming and its effect on the design of operating systems finally  we explore how the windows xp and linux operating systems support threads at the kernel level to introduce the notion of a thread a fundamental unit of cpu utilization that forms the basis of multithreaded computer systems to discuss the apis for the pthreads  win32  and java thread libraries to examine issues related to multithreaded programming a thread is a basic unit of cpu utilization ; it comprises a thread id  a program counter  a register set  and a stack it shares with other threads belonging to the same process its code section  data section  and other operating-system resources  such as open files and signals a traditional  or heavrvveighl   process has a single thread of control if a process has multiple threads of control  it can perform more than one task at a time figure 4.1 illustrates the difference between a traditional process and a process 4.1.1 motivation many software packages that run on modern desktop pcs are multithreaded an application typically is implemented as a separate process with several threads of control a web browser might have one thread display images or 153 154 chapter 4 thread + single-threaded process multithreaded process figure 4.1 single-threaded and multithreaded processes text while another thread retrieves data from the network  for example a word processor may have a thread for displaying graphics  another thread for responding to keystrokes from the user  and a third thread for performing spelling and grammar checking in the background in certain situations  a single application may be required to perform several similar tasks for example  a web server accepts client requests for web pages  images  sound  and so forth a busy web server may have several  perhaps thousands of  clients concurrently accessing it if the web server ran as a traditional single-tlu eaded process  it would be able to service only one client at a time  artd a client might have to wait a very long time for its request to be serviced one solution is to have the server run as a single process that accepts requests when the server receives a request  it creates a separate process to service that request in fact  this process-creation method was in common use before threads became popular process creation is time consuming and resource intensive  however if the new process will perform the same tasks as the existing process  why incur all that overhead it is generally more efficient to use one process that contains multiple threads if the web-server process is multithreaded  the server will create a separate thread that listens for client requests when a request is made  rather than creating another process  the server will create a new thread to service the request and resume listening for additional requests this is illustrated in figure 4.2 threads also play a vital role in remote procedure call  rpc  systems recall from chapter 3 that rpcs allow interprocess communication by providing a communication mechanism similar to ordinary function or procedure calls typically  rpc servers are multithreaded when a server receives a message  it services the message using a separate thread this allows the server to service several concurrent requests finally  most operating system kernels are now multithreaded ; several threads operate in the kernel  and each thread performs a specific task  such client  1  request 4.1  2  create new thread to service the request 1 1 thread '------.--r---10 server  3  resume listening for additional client requests figure 4.2 multithreaded server architecture 155 as managing devices or interrupt handling for examplef solaris creates a set of threads in the kernel specifically for interrupt handling ; linux uses a kernel thread for managing the amount of free memory in the system 4.1.2 benefits the benefits of multithreaded programming can be broken down into four major categories  responsiveness multithreading an interactive application may allow a program to continue running even if part of it is blocked or is performing a lengthy operation  thereby increasing responsiveness to the user for instancef a multithreaded web browser could allow user interaction in one thread while an image was being loaded in another thread resource sharing processes may only share resources through techniques such as shared memory or message passing such techniques must be explicitly arranged by the programmer however  threads share the memory and the resources of the process to which they belong by default the benefit of sharing code and data is that it allows an application to have several different threads of activity within the same address space 3 economy allocating memory and resources for process creation is costly because threads share the resources of the process to which they belong  it is more economical to create and context-switch threads empirically gauging the difference in overhead can be difficult  but in general it is much more time consuming to create and manage processes than threads in solarisf for example  creating a process is about thirty times slower than is creating a thread  and context switching is about five times slower scalability the benefits of multithreading can be greatly increased in a multiprocessor architecture  where threads may be running in parallel on different processors a single-threaded process can only run on one processor  regardless how many are available multithreading on a multicpu machine increases parallelism we explore this issue further in the following section 156 chapter 4 time figure 4.3 concurrent execution on a single-core system 4.1.3 multicore programming a recent trend in system design has been to place multiple computing cores on a single chip  where each core appears as a separate processor to the operating system  section 1.3.2   multithreaded programming provides a mechanism for more efficient use of multiple cores and improved concurrency consider an application with four threads on a system with a single computing core  concurrency merely means that the execution of the threads will be interleaved over time  figure 4.3   as the processing core is capable of executing only one thread at a time on a system with multiple cores  however  concurrency means that the threads can run in parallel  as the system can assign a separate thread to each core  figure 4.4   the trend towards multicore systems has placed pressure on system designers as well as application programmers to make better use of the multiple computing cores designers of operating systems must write scheduling algorithms that use multiple processing cores to allow the parallel execution shown in figure 4.4 for application programmers  the challenge is to modify existing programs as well as design new programs that are multithreaded to take advantage of multicore systems in general  five areas present challenges in programming for multicore systems  dividing activities this involves examining applications to find areas that can be divided into separate  concurrent tasks and thus can run in parallel on individual cores balance while identifying tasks that can run in parallel  programmers must also ensure that the tasks perform equal work of equal value in some instances  a certain task may not contribute as much value to the overall process as other tasks ; using a separate execution core to run that task may not be worth the cost data splitting just as applications are divided into separate tasks  the data accessed and manipulated by the tasks must be divided to run on separate cores core 1 l t1 i t3 t1 t3 i ti core 2  i  t4 t2 t4 i tz time figure 4.4 parallel execution on a multicore system 4.2 4.2 157 data dependency the data accessed by the tasks must be examined for dependencies between two or more tasks in instances where one task depends on data from another  programmers must ensure that the execution of the tasks is synchronized to accommodate the data dependency we examine such strategies in chapter 6 testing and debugging when a program is running in parallel on multiple cores  there are many different execution paths testing and debugging such concurrent programs is inherently more difficult than testing and debugging single-threaded applications because of these challenges  many software developers argue that the advent of multicore systems will require an entirely new approach to designing software systems in the future our discussion so far has treated threads in a generic sense however  support for threads may be provided either at the user level  for or by the kernel  for threads user threads are supported above the kernel and are managed without kernel support  whereas kernel threads are supported and managed directly by the operating system virtually all contemporary operating systems-including wiridows xp  linux  mac os x  solaris  and tru64 unix  formerly digital unix  -support kernel threads ultimately  a relationship must exist between user threads and kernel threads in this section  we look at three common ways of establishing such a relationship 4.2.1 many-to-one model the many-to-one model  figure 4.5  maps many user-level threads to one kernel thread thread management is done by the thread library in user figure 4.5 many-to-one model 158 chapter 4  user thread figure 4.6 one-to-one model space  so it is efficient ; but the entire process will block if a thread makes a blocking system call also  because only one thread can access the kernel at a time  multiple threads are unable to nm in parallel on multiprocessors -a thread library available for solaris-uses this modet as does gnu 4.2.2 one-to-one model the one-to-one model  figure 4.6  maps each user thread to a kernel thread it provides more concurrency than the many-to-one model by allowing another thread to run when a thread makes a blocking system call ; it also allows multiple threads to run in parallel on multiprocessors the only drawback to this model is that creating a user thread requires creating the corresponding kernel thread because the overhead of creating kernel threads can burden the performance of an application  most implementations of this model restrict the number of threads supported by the system linux  along with the family of windows operating systems  implement the one-to-one model 4.2.3 many-to-many model the many-to-many model  figure 4.7  multiplexes many user-level threads to a smaller or equal number of kernel threads the number of kernel threads may be specific to either a particular application or a particular machine  an application may be allocated more kernel threads on a multiprocessor than on a uniprocessor   whereas the many-to-one model allows the developer to user thread k +  kernel thread figure 4.7 many-to-many model 4.3 4.3 159 2      i ~   / ' / '  '  ......._ user thread    0 -kernel thread figure 4.8 two-level model create as many user threads as she wishes  true concurrency is not gained because the kernel can schedule only one thread at a time the one-to-one model allows for greater concurrency  but the developer has to be careful not to create too many threads within an application  and in some instances may be limited in the number of threads she can create   the many-to-many model suffers from neither of these shortcomings  developers can create as many user threads as necessary  and the corresponding kernel threads can run in parallel on a multiprocessor also  when a thread performs a blocking system call  the kernel can schedule another thread for execution one popular variation on the many-to-many model still multiplexes many user-level threads to a smaller or equal number of kernel threads but also allows a user-level thread to be bound to a kernel thread this variation  sometimes referred to as the two-level model  figure 4.8   is supported by operating systems such as irlx  hp-ux  and tru64 unix the solaris operating system supported the two-level model in versions older than solaris 9 however  beginning with solaris 9  this system uses the one-to-one model a provides the programmer with an api for creating and managing threads there are two primary ways of implementii g a thread library the first approach is to provide a library entirely in user space with no kernel support all code and data structures for the library exist ii user space this means that invoking a function in the library results in a local function call in user space and not a system call the second approach is to implement a kernel-level library supported directly by the operating system in this case  code and data structures for the library exist in kernel space invoking a function in the api for the library typically results in a system call to the kernel three main thread libraries are in use today   1  posix pthreads   2  win32  and  3  java pthreads  the threads extension of the posix standard  may be provided as either a user or kernel-level library the win32 thread library is a kernel-level library available on windows systems the java thread api allows threads to be created and managed directly in java programs however  because in most instances the jvm is running on top of a host operating system  160 chapter 4 the java thread api is generally implemented using a thread library available on the host system this means that on windows systems  java threads are typically implemented using the win32 api ; unix and linux systems often use pthreads in the remainder of this section  we describe basic thread creation using these three thread libraries as an illustrative example  we design a multithreaded program that performs the summation of a non-negative integer in a separate thread using the well-known summation function  n sum = i ~ i = o for example  if n were 5  this function would represent the summation of integers from 0 to 5  which is 15 each of the three programs will be n.m with the upper bounds of the summation entered on the command line ; thus  if the user enters 8  the summation of the integer values from 0 to 8 will be output 4.3.1 pthreads refers to the posix standard  ieee 1003.lc  defining an api for thread creation and synchronization this is a specification for thread behavim ~ not an implementation operating system designers may implement the specification in any way they wish numerous systems implement the pthreads specification  including solaris  linux  mac os x  and tru64 unix shareware implementations are available in the public domain for the various windows operating systems as well the c program shown in figure 4.9 demonstrates the basic pthreads api for constructing a multithreaded program that calculates the summation of a nonnegative integer in a separate thread in a pthreads program  separate threads begin execution in a specified function in figure 4.9  this is the runner   function when this program begins  a single thread of control begins in main    after some initialization  main   creates a second thread that begins control in the runner   function both threads share the global data sum let 's look more closely at this program all pthreads programs must include the pthread h header file the statement pthread_t tid declares the identifier for the thread we will create each thread has a set of attributes  including stack size and scheduling information the pthread_attr_t attr declaration represents the attributes for the thread we set the attributes in the function call pthread_attr ini t  &attr   because we did not explicitly set any attributes  we use the default attributes provided  in chapter 5  we discuss some of the scheduling attributes provided by the pthreads api  a separate thread is created with the pthread_create   function call in addition to passirtg the thread identifier and the attributes for the thread  we also pass the name of the function where the new thread will begin execution-in this case  the runner   function last  we pass the integer parameter that was provided on the command line  argv  1   at this point  the program has two threads  the initial  or parent  thread in main   and the summation  or child  thread performing the summation 4.3 # include pthread.h # include stdio.h int sum ; i this data is shared by the thread  s  i void runner  void param  ; i the thread i int main  int argc  char argv      pthread_t tid ; i the thread identifier i pthread_attr_t attr ; i set of thread attributes i if  argc ! = 2    fprintf  stderr  usage  a.out integer value \ n  ; return -1 ; if  atoi  argv  1   0    fprintf  stderr  % d must be = 0 \ n ,atoi  argv  1    ; return -1 ; i get the default attributes i pthread_attr_init  &attr  ; i create the thread i pthread_create  &tid,&attr,runner,argv  1   ; i wait for the thread to exit i pthread_join  tid,null  ; printf  sum = % d \ n ,sum  ; i the thread will begin control in this function i void runner  void param    inti  upper = atoi  param  ; sum = 0 ; for  i = 1 ; i = upper ; i + +  sum + = i ; pthread_exi t  0  ; figure 4.9 multithreaded c program using the pthreads api 161 operation in the runner   function after creating the summation threadf the parent thread will wait for it to complete by calling the pthread_j oin   function the summation thread will complete when it calls the function pthread_exi t    once the summation thread has returnedf the parent thread will output the value of the shared data sum 162 chapter 4 4.3.2 win32 threads the technique for creating threads using the win32 thread library is similar to the pthreads technique in several ways we illustrate the win32 thread api in the c program shown in figure 4.10 notice that we must include the windows  h header file when using the win32 api just as in the pthreads version shown in figure 4.9  data shared by the separate threads-in this case  sum-are declared globally  the dword data type is an unsigned 32-bit integer   we also define the summation   function that is to be performed in a separate thread this function is passed a pointer to a void  which win32 defines as lpvoid the thread performing this function sets the global data sum to the value of the summation from 0 to the parameter passed to summation    threads are created in the win32 api using the createthread   function  and-just as in pthreads-a set of attributes for the thread is passed to this function these attributes il1.clude security information  the size of the stack  and a flag that can be set to indicate if the thread is to start in a suspended state in this program  we use the default values for these attributes  which do not initially set the thread to a suspended state and instead make it eligible to be rm1 by the cpu scheduler   once the summation thread is created  the parent must wait for it to complete before outputting the value of sum  as the value is set by the summation thread recall that the pthread program  figure 4.9  had the parent thread wait for the summation thread using the pthread_j oin   statement we perform the equivalent of this in the win32 api using the wai tforsingleobj ect   function  which causes the creatil1.gthread to block until the summation thread has exited  we cover synchronization objects in more detail in chapter 6  4.3.3 java threads tlu eads are the fundamental model of program execution in a java program  and the java language and its api provide a rich set of features for the creation and management of threads all java programs comprise at least a single thread of control-even a simple java program consisting of only a main   method runs as a single thread in the jvm there are two teclmiques for creating threads in a java program one approach is to create a new class that is derived from the thread class and to override its run   method an alternative-and more commonly usedteclmique is to define a class that implements the runnable interface the runnable interface is defined as follows  public interface runnable  public abstract void run   ; when a class implements runnable  it must define a run   method the code implementing the run   method is what runs as a separate thread figure 4.11 shows the java version of a multithreaded program that determines the summation of a non-negative integer the summation class implements the runnable interface thread creation is performed by creating 4.3 # include windows.h # include stdio.h dword sum ; i data is shared by the thread  s  i i the thread runs in this separate function i dword winapi sumrnation  lpvoid param    dword upper =  dword  param ; for  dword i = 0 ; i = upper ; i + +  sum + = i ; return 0 ; int main  int argc  char argv      dword threadid ; handle threadhandle ; int param ; i perform some basic error checking i if  argc ! = 2    fprintf  stderr  an integer parameter is required \ n  ; return -1 ; param = atoi  argv  1   ; if  param 0    fprintf  stderr  an integer = 0 is required \ n  ; return -1 ; ii create the thread threadhandle = createthread  null  ii default security attributes 0  ii default stack size summation  ii thread function &param  ii parameter to thread function 0  ii default creation flags &threadid  ; ii returns the thread identifier if  threadhandle ! = null    ii now wait for the thread to finish waitforsingleobject  threadhandle,infinite  ; ii close the thread handle closehandle  threadhandle  ; printf  surn = % d \ n ,sum  ; figure 4.10 multithreaded c program using the win32 api 163 164 chapter 4 class sum   private int sum ; public int getsum    return sum ;  public void setsum  int sum   this.sum sum ;  class summation implements runnable   private int upper ; private sum sumvalue ; public summation  int upper  sum sumvalue   this.upper = upper ; this.sumvalue = sumvalue ;  public void run    int sum = 0 ;  for  int i = 0 ; i = upper ; i + +  sum + = i ; sumvalue.setsum  sum  ; public class driver   public static void main  string   args   if  args.length 0    if  integer.parseint  args  o   0  system.err.println  args  o  + must be = 0  ; else  ii create the object to be shared sum sumobject = new sum   ; int upper = integer.parseint  args  o   ; thread thrd = new thread  new summation  upper  sumobject   ; thrd.start   ; try  thrd join   ; system.out.println  the sum of + upper + is + sumobject.getsum    ;  catch  interruptedexception ie     else system.err.println  usage  summation integer value  ;  figure 4.11 java program for the summation of a non-negative integer 4.4 4.4 165 an object instance of the thread class and passing the constructor a runnable object creating a thread object does not specifically create the new thread ; rather  it is the start   method that creates the new thread calling the start   method for the new object does two things  it allocates memory and initializes a new thread in the jvm it calls the run   method  making the thread eligible to be run by the jvm  note that we never call the run   method directly rathel ~ we call the start   method  and it calls the run   method on our behalf  when the summation program runs  two threads are created by the jvm the first is the parent thread  which starts execution in the main   method the second thread is created when the start   method on the thread object is invoked this child thread begins execution in the run   method of the summation class after outputting the value of the summation  this thread terminates when it exits from its run   method sharing of data between threads occurs easily in win32 and pthreads  since shared data are simply declared globally as a pure object-oriented language  java has no such notion of global data ; if two or more threads are to share data in a java program  the sharing occurs by passing references to the shared object to the appropriate threads in the java program shown in figure 4.11  the main thread and the summation thread share the object instance of the sum class this shared object is referenced through the appropriate get sum   and setsum   methods  you might wonder why we do n't use an integer object rather than designing a new sum class the reason is that the integer class is immutable-that is  once its value is set  it can not change  recall that the parent threads in the pthreads and win32 libraries use pthread_j oin   and wai tforsingledbj ect    respectively  to wait for the summation threads to finish before proceeding the join   method in java provides similar functionality  notice that join   can throw an interruptedexception  which we choose to ignore  in this section  we discuss some of the issues to consider with multithreaded programs 4.4.1 the fork   and exec   system calls in chapter 3  we described how the fork   system call is used to create a separate  duplicate process the semantics of the fork   and exec   system calls change in a multithreaded program if one thread in a program calls fork    does the new process duplicate all threads  or is the new process single-threaded some unix systems have chosen to have two versions of fork    one that duplicates all threads and another that duplicates only the thread that invoked the fork   system call the exec   system call typically works in the same way as described in chapter 3 that is  if a thread invokes the exec   system call  the program 166 chapter 4 the jvm and the host operating system the jvm is typically implemented on top of a host operating system  see figure 2.20   this setup allows the jvm to hide the implementation details of the underlying operating system and to provide a consistent  abstract environment that allows java programs to operate on any platform that supports a jvm the specification for the jvm does not indicate how java threads are to be mapped to the underlying operating system  instead leaving that decision to the particular implementation of the jvm for example  the windows xp operating system uses the one-to-one model ; therefore  each java thread for a jvm running on such a system maps to .a kernel thread  on operating systems that use the many-to-many model  such as tru64 unix   a java thread is mapped according to the many-to-manymodel solaris initially implemented the jvm using themany ~ to-one model  the greenthreads library  mentioned earlier   later releases of the jvm were implementedusing the many-to  inany model beginning with solaris 9  java threads were mapped using the one ~ to-one model in addition  there may be a relationship between the java thread library and the thread library on the host operating system for .example  implementations of a jvm for the windows family of operating systems might use the win32 api when creating java threads ; linux  solaris  and mac os x systems might use the pthreads api specified in the parameter to exec   will replace the entire process-including all threads which of the two versions of fork   to use depends on the application if exec   is called immediately after forking  then duplicating all threads is unnecessary  as the program specified in the parameters to exec   will replace the process in this instance  duplicating only the calling thread is appropriate if  however  the separate process does not call exec   after forking  the separate process should duplicate all threads 4.4.2 cancellation '''-'c ' ' ' ' is the task of terminating a thread before it has completed for example  if multiple threads are concurrently searching through a database and one thread returns the result  the remaining threads might be canceled another situation might occur when a user presses a button on a web browser that stops a web page from loading any further often  a web page is loaded using several threads-each image is loaded in a separate thread when a user presses the stop button on the browser  all threads loading the page are canceled a thread that is to be canceled is often referred to as the cancellation of a target thread may occur in two different scenarios  asynchronous cancellation one thread immediately terminates the target thread 4.4 167 deferred cancellation the target thread periodically checks whether it should terminate  allowing it an opportunity to terminate itself in an orderly fashion the difficulty with cancellation occurs in situations where resources have been allocated to a canceled thread or where a thread is canceled while in the midst of updating data it is sharing with other threads this becomes especially troublesome with asynchronous cancellation often  the operating system will reclaim system resources from a canceled thread but will not reclaim all resources therefore  canceling a thread asynchronously may not free a necessary system-wide resource with deferred cancellation  in contrast  one thread indicates that a target thread is to be canceled  but cancellation occurs only after the target thread has checked a flag to determine whether or not it should be canceled the thread can perform this check at a at which it can be canceled safely pthreads refers to such points as 4.4.3 signal handling a is used in unix systems to notify a process that a particular event has occurred a signal may be received either synchronously or asynchronously  depending on the source of and the reason for the event being signaled all signals  whether synchronous or asynchronous  follow the same pattern  a signal is generated by the occurrence of a particular event a generated signal is delivered to a process once delivered  the signal must be handled examples of synchronous signals include illegal memory access and division by 0 if a running program performs either of these actions  a signal is generated synchronous signals are delivered to the same process that performed the operation that caused the signal  that is the reason they are considered synchronous   when a signal is generated by an event external to a running process  that process receives the signal asynchronously examples of such signals include terminating a process with specific keystrokes  such as control c  and having a timer expire typically  an asynchronous signal is sent to another process a signal may be handled by one of two possible handlers  a default signal handler a user-defilced signal handler every signal has a that is run by the kernel when handling that signal this default action can be overridden by a signal handle ~ that is called to handle the signal signals are handled in different ways some signals  such as changing the size of a window  are simply ignored ; others  such as an illegal memory access  are handled by terminating the program 168 chapter 4 handling signals in single-threaded programs is straightforward  signals are always delivered to a process however  delivering signals is more complicated in multithreaded programs  where a process may have several threads where  then  should a signal be delivered in generat the following options exist  deliver the signal to the thread to which the signal applies deliver the signal to every thread in the process deliver the signal to certain threads in the process assign a specific thread to receive all signals for the process the method for delivering a signal depends on the type of signal generated for example  synchronous signals need to be delivered to the thread causing the signal and not to other threads in the process however  the situation with asynchronous signals is not as clear some asynchronous signals-such as a signal that terminates a process  control c  for example  -should be sent to all threads most multithreaded versions of unix allow a thread to specify which signals it will accept and which it will block therefore  in some cases  an asynchronous signal may be delivered only to those threads that are not blocking it however  because signals need to be handled only once  a signal is typically delivered only to the first thread found that is not blocking it the standard unix function for delivering a signal is kill  pid_t pid  int signal   which specifies the process  pi d  to which a particular signal is to be delivered posix pthreads provides the pthread_kill  pthread_t tid  int signal  function  which allows a signal to be delivered to a specified thread  tid   although windows does not explicitly support for signals  they can be emulated using  apcs   the apc facility allows a user thread to specify a function that is to be called when the user thread receives notification of a particular event as indicated by its name  an apc is roughly equivalent to an asynchronous signal in unix however  whereas unix must contend with how to deal with signals in a multithreaded environment  the apc facility is more straightforward  since an apc is delivered to a particular thread rather than a process 4.4.4 thread pools in section 4.1  we mentioned multithreading in a web server in this situation  whenever the server receives a request  it creates a separate thread to service the request whereas creating a separate thread is certainly superior to creating a separate process  a multithreaded server nonetheless has potential problems the first issue concerns the amount of time required to create the thread prior to servicing the request  together with the fact that this thread will be discarded once it has completed its work the second issue is more troublesome  if we allow all concurrent requests to be serviced in a new thread  we have not placed a bound on the number of threads concurrently active in the system unlimited threads could exhaust system resources  such as cpu tince or memory one solution to this problem is to use a 4.4 169 the general idea beh_ind a thread pool is to create a number of threads at process startup and place them into a pool  where they sit and wait for work when a server receives a request  it awakens a thread from this pool-if one is available-and passes it the request for service once the thread completes its service  it returns to the pool and awaits more work if the pool contains no available thread  the server waits until one becomes free thread pools offer these benefits  servicing a request with an existing thread is usually faster than waiting to create a thread a thread pool limits the number of threads that exist at any one point this is particularly important on systems that can not support a large number of concurrent threads the number of threads in the pool can be set heuristically based on factors such as the number of cpus in the system  the amount of physical memory  and the expected number of concurrent client requests more sophisticated thread-pool architectures can dynamically adjust the number of threads in the pool according to usage patterns such architectures provide the further benefit of having a smaller pool-thereby consuming less memory-when the load on the system is low the win32 api provides several functions related to thread pools using the thread pool api is similar to creating a thread with the thread create   function  as described in section 4.3.2 here  a function that is to run as a separate thread is defin_ed such a function may appear as follows  dword winapi poolfunction  avoid param  / this function runs as a separate thread / a pointer to poolfunction   is passed to one of the functions in the thread pool api  and a thread from the pool executes this function one such member in the thread pool api is the queueuserworkitemo function  which is passed three paranceters  lpthread_starlroutine function-a pointer to the function that is to nm as a separate thread pvoid param-the parameter passed to function ulong flags-flags indicating how the thread pool is to create and manage execution of the thread an example of invoking a function is  queueuserworkitem  &poolfunction  null  0  ; this causes a thread from the thread pool to invoke poolfunction   on behalf of the programmer in this instance  we pass no parameters to 170 chapter 4  lightweight process '-----'-' d0 ~ kamalthcead figure 4.12 lightweight process  lwp   poolfunction    because we specify 0 as a flag  we provide the thread pool with no special instructions for thread creation other members in the win32 thread pool api include utilities that invoke functions at periodic intervals or when an asynchronous i/0 request completes the java util concurrent package in java 1.5 provides a thread pool utility as well 4.4.5 thread-specific data threads belonging to a process share the data of the process indeed  this sharing of data provides one of the benefits of multithreaded programming however  in some circumstances  each thread need its own copy of certain data we will call such data for example  in a transaction-processing system  we might service each transaction in a separate thread furthermore  each transaction might be assigned a unique identifier to associate each thread with its unique identifier  we could use thread-specific data most thread libraries-including win32 and pthreads-provide some form of support for thread-specific data java provides support as well 4.4.6 scheduler activations a final issue to be considered with multithreaded programs concerns communication between the kernel and the thread library  which may be required by the many-to-many and two-level models discussed in section 4.2.3 such coordination allows the number of kernel threads to be dynamically adjusted to help ensure the best performance many systems implementing either the many-to-many or the two-level model place an intermediate data structure between the user and kernel threads this data structure-typically known as a lightweight process  or lwp-is shown in figure 4.12 to the user-thread library  the lwp appears to be a virtual processor on which the application can schedule a user thread to run each lwp is attached to a kernel thread  and it is kernel threads that the operating system schedules to run on physical processors if a kernel thread blocks  such as while waiting for an i/0 operation to complete   the lwp blocks as well up the chain  the user-level thread attached to the lwp also blocks an application may require any number of lwps to run efficiently consider a cpu-bound application running on a single processor in this scenario  only 4.5 4.5 171 one thread can run at once  so one lwp is sufficient an application that is i/ointensive may require multiple lwps to execute  however typically  an lwp is required for each concurrent blocking system call suppose  for example  that five different file-read requests occur simultaneously five lwps are needed  because all could be waiting for i/0 completion in the kernel if a process has only four lwps  then the fifth request must wait for one of the lwps to return from the kernel one scheme for communication between the user-thread library and the kernel is known as it works as follows  the kernel provides an application with a set of virtual processors  lwps   and the application can schedule user threads onto an available virtual processor furthermore  the kernel must inform an application about certain events this procedure is known as an upcalls are handled by the thread library with an and upcall handlers must run on a virtual processor one event that triggers an upcall occurs when an application thread is about to block in this scenario  the kernel makes an upcall to the application informing it that a thread is about to block and identifying the specific thread the kernel then allocates a new virtual processor to the application the application runs an upcall handler on this new virtual processor  which saves the state of the blocking thread and relinquishes the virtual processor on which the blocking thread is running the upcall handler then schedules another thread that is eligible to run on the new virtual processor when the event that the blocking thread was waiting for occurs  the kernel makes another upcall to the thread library informilcg it that the previously blocked thread is now eligible to run the up call handler for this event also requires a virtual processor  and the kernel may allocate a new virtual processor or preempt one of the user threads and run the upcall handler on its virtual processor after marking the 1-mblocked thread as eligible to run  the application schedules an eligible thread to run on an available virtual processor in this section  we explore how threads are implemented in windows xp and linux systems 4.5.1 windows xp threads windows xp implements the win32 api  which is the primary api for the family of microsoft operating systems  windows 95  98  nt  2000  and xp   indeed  much of what is mentioned in this section applies to this entire family of operating systems a windows xp application runs as a separate process  and each process may contain one or more threads the win32 api for creating threads is covered in section 4.3.2 windows xp uses the one-to-one mapping described in section 4.2.2  where each user-level thread maps to an associated kernel thread however  windows xp also provides support for a library  which provides the functionality of the many-to-many model  section 4.2.3   by using the thread library  any thread belonging to a process can access the address space of the process 172 chapter 4 the general components of a thread include  a thread id uniquely identifying the thread a register set representing the status of the processor a user stack  employed when the thread is running in user mode  and a kernel stack  employed when the thread is running in kernel mode a private storage area used by various run-time libraries and dynamic link libraries  dlls  the register set  stacks  and private storage area are known as the rcc   nw yt of the thread the primary data structures of a thread include  ethread-executive thread block kthread-kernel thread block tee-thread environment block the key components of the ethread include a pointer to the process to which the thread belongs and the address of the routine in which the thread starts control the ethread also contains a pointer to the corresponding kthread ethread kernel space user space figure 4.13 data structures of a windows xp thread 4.5 173 the kthread includes scheduling and synchronization inforn1.ation for the thread in addition  the kthread includes the kernel stack  used when the thread is running in kernel mode  and a pointer to the teb the ethread and the kthread exist entirely in kernel space ; this means that only the kernel can access thern the teb is a user-space data structure that is accessed when the thread is running in user mode among other fields  the teb contains the thread identifie1 ~ a user-mode stack  and an array for threadspecific data  which windows xp terms the structure of a windows xp thread is illustrated in figure 4.13 4.5.2 linux threads linux provides the fork   system call with the traditional functionality of duplicating a process  as described in chapter 3 linux also provides the ability to create threads using the clone   system call however  linux does not distinguish between processes and threads in fact  linux generally uses the term task-rather than process or thread-when referring to a flow of control within a program when clone   is invoked  it is passed a set of flags  which determine how much sharing is to take place between the parent and child tasks some of these flags are listed below  flag meaning clone fs  file-system information is shared clone vm  the same memory space is shared clone  sighand signal handlers are shared clone files the set of open files is shared for example  if clone   is passed the flags clone_fs  clone_vm  clone_sighand  and clone_files  the parent and child tasks will share the same file-system information  such as the current working directory   the same memory space  the same signal handlers  and the same set of open files using clone   in this fashion is equivalent to creating a thread as described in this chapter  since the parent task shares most of its resources with its child task however  if none of these flags is set when clone   is invoked  no sharing takes place  resulting in functionality similar to that provided by the fork   system call the varying level of sharing is possible because of the way a task is represented in the linux kernel a unique kernel data structure  specifically  struct task_struct  exists for each task in the system this data structure  instead of storing data for the task  contains pointers to other data structures where these data are stored -for example  data structures that represent the list of open files  signal-handling information  and virtual memory when fork   is invoked  a new task is created  along with a copy of all the associated data structures of the parent process a new task is also created when the clone   system call is made howevet ~ rather than copying all data structures  the new 174 chapter 4 4.6 task points to the data structures of the parent task  depending on the set of flags passed to clone    several distributions of the linux kernel now include the nptl thread library nptl  which stands for native posix thread library  provides a posix-compliant thread model for linux systems along with several other features  such as better support for smp systems  as well as taking advantage of numa support in addition  the start-up cost for creating a thread is lower with nptl than with traditional linux threads finally  with nptl  the system has the potential to support hundreds of thousands of threads such support becomes more important with the growth of multicore and other smp systems a thread is a flow of control within a process a multithreaded process contains several different flows of control within the same address space the benefits of multithreading include increased responsiveness to the use1 ~ resource sharing within the process  economy  and scalability issues such as more efficient use of multiple cores user-level threads are threads that are visible to the programmer and are unknown to the kernel the operating-system kernel supports and manages kernel-level threads in general  user-level threads are faster to create and manage than are kernel threads  as no intervention from the kernel is required three different types of models relate user and kernel threads  the many-to-one model maps many user threads to a single kernel thread the one-to-one model maps each user thread to a corresponding kernel thread the many-to-many model multiplexes many user threads to a smaller or equal number of kernel threads most modern operating systems provide kernel support for threads ; among these are windows 98  nt  2000  and xp  as well as solaris and linux thread libraries provide the application programmer with an api for creating and managing threads three primary thread libraries are in common use  posix pthreads  win32 threads for windows systems  and java threads multithreaded programs introduce many challenges for the programmer  including the semantics of the fork   and exec   system calls other issues include thread cancellation  signal handling  and thread-specific data 4.1 provide two programming examples in which multithreading does not provide better performance than a single-threaded solution 4.2 write a ncultithreaded java  pthreads  or win32 program that outputs prime numbers this program should work as follows  the user will run the program and will enter a number on the command line the 175 program will then create a separate thread that outputs all the prime numbers less than or equal to the number entered by the user 4.3 which of the following components of program state are shared across threads in a multithreaded process a register values b heap memory c global variables d stack memory 4.4 the program shown in figure 4.14 uses the pthreads api what would be the output from the program at line c and line p # include pthread.h # include stdio.h int value = 0 ; void runner  void param  ; i the thread i int main  int argc  char argv     int pid ; pthread_t tid ; pthread_attr t attr ;  pid = fork   ; if  pid = = 0   i child process i pthread_attr_init  &attr  ; pthread_create  &tid,&attr,runner,null  ; pthread_join  tid,null  ; printf  child  value = % d ,value  ; i line c i  else if  pid 0   i parent process i wait  null  ; printf  parent  value = % d ,value  ; i line p i  void runner  void param   value = 5 ; pthread_exi t  0  ;  figure 4.14 c program for exercise 4.4 176 chapter 4 4.5 consider a multiprocessor system and a multithreaded program written using the many-to-many threading rnodel let the number of user-level threads in the program be more than the number of processors in the system discuss the performance implicatiorts of the following scenarios a the number of kernel threads allocated to the program is less than the number of processors b the number of kernel threads allocated to the program is equal to the number of processors c the number of kernel threads allocated to the program is greater than the number of processors but less than the number of userlevel threads 4.6 what are two differences between user-level threads and kernel-level threads under what circumstances is one type better than the other 4.7 exercise 3.16 in chapter 3 involves designing an echo server using the java threading api however  this server is single-threaded  meaning that the server can not respond to concurrent echo clients until the current client exits modify the solution to exercise 3.16 so that the echo server services each client in a separate request 4.8 modify the socket-based date server  figure 3.19  in chapter 3 so that the server services each client request in a separate thread 4.9 can a multithreaded solution using multiple user-level threads achieve better performance on a multiprocessor system than on a singleprocessor system explain 4.10 what resources are used when a thread is created how do they differ from those used when a process is created 4.11 under what circumstances does a multithreaded solution using multiple kernel threads provide better performance than a single-threaded solution on a single-processor system 4.12 the fibonacci sequence is the series of numbers 0  1  1  2  3  5 8   formally  it can be expressed as  fib0 = 0 fih = 1 jibn = jibn-1 + jibn-2 write a multithreaded program that generates the fibonacci sequence using either the java  pthreads  or win32 thread library this program 177 should work as follows  the user will enter on the command line the number of fibonacci numbers that the program ~ is to generate the program will then create a separate thread that will generate the fibonacci numbers  placing the sequence in data that can be shared by the threads  an array is probably the most convenient data structure   when the thread finishes execution  the parent thread will output the sequence generated by the child thread because the parent thread can not begin outputting the fibonacci sequence until the child thread finishes  this will require having the parent thread wait for the child thread to finish  using the techniques described in section 4.3 4.13 a pthread program that performs the smmnation function was provided in section 4.3.1 rewrite this program in java 4.14 as described in section 4.5.2  linux does not distinguish between processes and threads instead  linux treats both in the same way  allowing a task to be more akin to a process or a thread depending on the set of flags passed to the clone   system call however  many operating systems-such as windows xp and solaris-treat processes and threads differently typically  such systems use a notation wherein the data structure for a process contains pointers to the separate threads belonging to the process contrast these two approaches for modeling processes and threads within the kernel 4.15 describe the actions taken by a thread library to context-switch between user-level threads the set of projects below deal with two distinct topics-naming service and matrix muliplication project 1  naming service project a naming service such as dns  for domain name system  can be used to resolve ip names to ip addresses for example  when someone accesses the host www westminstercollege edu  a naming service is used to determine the ip address that is mapped to the ip name www westminstercollege edu this assignment consists of writing a multithreaded nan ling service in java using sockets  see section 3.6.1   the java net api provides the following mechanism for resolving ip names  inetaddress hostaddress = inetaddress.getbyname  www.westminstercollege.edu  ; string ipaddress = hostaddress.gethostaddress   ; where getbyname   throws an unknownhostexception if it is unable to resolve the host name 178 chapter 4 the server the server will listen to port 6052 waiting for client connections when a client connection is made  the server will service the connection in a separate thread and will resume listening for additional client connections once a client makes a connection to the server  the client will write the ip name it wishes the server to resolve-such as www westminstercollege eduto the socket the server thread will read this ip name from the socket and either resolve its ip address or  if it can not locate the host address  catch an unknownhostexception the server will write the ip address back to the client or  in the case of an unknownhostexception  will write the message unable to resolve host host name  once the server has written to the client  it will close its socket connection the client initially  write just the server application and connect to it via telnet for example  assuming the server is running on the localhost a telnet session would appear as follows  client responses appear in telnec localhost 6052 connected to localhost escape character is 'a  ' \ i ~ /vv \ h 'destrninstercollege edu 146.86.1.17 connection closed by foreign host by initially having telnet act as a client  you can more easily debug any problems you may have with your server once you are convinced your server is working properly  you can write a client application the client will be passed the ip name that is to be resolved as a parameter the client will open a socket connection to the server and then write the if name that is to be resolved it will then read the response sent back by the server as an example  if the client is named nsclient  it is invoked as follows  java nsclient www.westminstercollege.edu and the server will respond with the corresponding if address or unknown host message once the client has output the if address  it will close its socket connection project 2  matrix multiplication project given two matrices  a and b  where matrix a contains m rows and k columns and matrix b contains k rows and n columns  the of a and b is matrix c  where c contains m rows and n coh.11m1s the entry in matrix c for row i  column j  c.j  is the sum of the products of the elements for row i in matrix a and column j in matrix b that is  179 k c,j = l a ;  11 x bn,j 11 =  1 for example  if a is a 3-by-2 matrix and b is a 2-by-3 m.atrix  element c3,1 is the sum of a3,1 x b1.1 and a3,2 x b2,1 for this project  calculate each element c ; ,j in a separate worker thread this will involve creating m x n worker threads the main-or parent-thread will initialize the matrices a and b and allocate sufficient memory for matrix c  which will hold the product of matrices a and b these matrices will be declared as global data so that each worker thread has access to a  b  and c matrices a and b can be initialized statically  as shown below  # define m 3 # define k 2 # define n 3 int a  m   k  int b  k   n  int c  m   n  ;   1,4    2,5    3,6   ;   8,7,6    5,4,3   ; alternatively  they can be populated by reading in values from a file passing parameters to each thread the parent thread will create m x n worker threads  passing each worker the values of row i and column j that it is to use in calculating the matrix product this requires passing two parameters to each thread the easiest approach with pthreads and win32 is to create a data structure using a struct the members of this structure are i and j  and the structure appears as follows  i structure for passing data to threads i struct v   ; int i ; i row i int j ; i column i both the pthreads and win32 programs will create the worker threads using a strategy similar to that shown below  i we have to create m n worker threads i for  i = 0 ; i m  i + +   for  j = 0 ; j n ; j + +    struct v data =  struct v  rnalloc  sizeof  struct v   ; data i = i ; data j = j ; i now create the thread passing it data as a parameter i 180 chapter 4 public class workerthread implements runnable   private int row ; private int col ; private int     a ; private int     b ; private int     c ; public workerthread  int row  int col  int     a   int     b  int     c   this.row = row ; this.col = col ; this.a a ; this.b this.c b ' c ; public void run    i calculate the matrix product in c  row   col  i  figure 4.15 worker thread in java the data pointer will be passed to either the pthread_create    pthreads  function or the createthread    win32  function  which in turn will pass it as a parameter to the function that is to run as a separate thread sharing of data between java threads is different from sharing between threads in pthreads or win32 one approach is for the main thread to create and initialize the matrices a  b  and c this main thread will then create the worker threads  passing the three matrices-along with row i and column jto the constructor for each worker thus  the outline of a worker thread appears in figure 4.15 waiting for threads to complete once all worker threads have completed  the main thread will output the product contained in matrix c this requires the main thread to wait for all worker threads to finish before it can output the value of the matrix product several different strategies can be used to enable a thread to wait for other threads to finish section 4.3 describes how to wait for a child thread to complete using the win32  pthreads  and java thread libraries win32 provides the wai tforsingleobj ect   function  whereas pthreads and java use pthread_j oin   and join    respectively however  in these programming examples  the parent thread waits for a single child thread to finish ; completing this exercise will require waiting for multiple threads in section 4.3.2  we describe the wai tforsingleobj ect   function  which is used to wait for a single thread to finish however  the win32 api also provides the wai tformultipledbj ects   function  which is used when waiting for multiple threads to complete waitformultipleobjectso is passed four parameters  # define num_threads 10 i an array of threads to be joined upon i pthread_t workers  num_threads  ; for  int i = 0 ; i num_threads ; i + +  pthread_join  workers  i   null  ; figure 4.16 pthread code for joining ten threads the num.ber of objects to wait for a pointer to the array of objects a flag indicating if all objects have been signaled a timeout duration  or infinite  181 for example  if thandles is an array of thread handle objects of size n  the parent thread can wait for all its child threads to complete with the statement  waitformultipleobjects  n  thandles  true  infinite  ; a simple strategy for waiting on several threads using the pthreads pthread_join   or java 's join   is to enclose the join operation within a simple for loop for example  you could join on ten threads using the pthread code depicted in figure 4.16 the equivalent code using java threads is shown in figure 4.17 final static int num_threads = 10 ; i an array of threads to be joined upon i thread   workers = new thread  num_threads  ; for  int i = 0 ; i num_threads ; i + +   try  workers  i   join   ;  catch  interruptedexception ie     figure 4.17 java code for joining ten threads threads have had a long evolution  starting as cheap concurrency in programming languages and moving to lightweight processes  with early examples that included the thotll system  cheriton et al  1979   and the pilot system  redell et al  1980    binding  1985  described moving threads into the unix kernel mach  accetta et al  1986   tevanian et al  1987a   and v  cheriton  1988   made extensive use of threads  and eventually almost all major operating systems implemented them in some form or another 182 chapter 4 thread performance issues were discussed by anderson et al  1989   who continued their work in anderson et al  1991  by evaluating the performance of user-level threads with kernel support bershad et al  1990  describe combining threads with rpc engelschall  2000  discusses a technique for supporting user-level threads an analysis of an optimal thread-pool size can be found in ling et al  2000   scheduler activations were first presented in anderson et al  1991   and williams  2002  discusses scheduler activations in the netbsd system_ other mechanisms by which the user-level thread library and the kernel cooperate with each other are discussed in marsh et al  1991   govindan and anderson  1991   draves et al  1991   and black  1990   zabatta and young  1998  compare windows nt and solaris threads on a symmetric multiprocessor pinilla and gill  2003  compare java thread performance on lim1x  windows  and solaris vahalia  1996  covers threading in several versions of unix mcdougall and mauro  2007  describe recent developments in threading the solaris kernel russinovich and solomon  2005  discuss threading in the windows operating system family bovet and cesati  2006  and love  2004  explain how linux handles threading and singh  2007  covers threads in mac os x information on pthreads programming is given in lewis and berg  1998  and butenhof  1997   oaks and wong  1999   lewis and berg  2000   and holub  2000  discuss multithreading in java goetz et al  2006  present a detailed discussion of concurrent programming in java beveridge and wiener  1997  and cohen and woodring  1997  describe multithreading using win32 5.1 cpu scheduling is the basis of multiprogrammed operating systems by switching the cpu among processes  the operating system can make the computer more productive in this chapter  we introduce basic cpu-scheduling concepts and present several cpu-scheduling algorithms we also consider the problem of selecting an algorithm for a particular system in chapter 4  we introduced threads to the process model on operating systems that support them  it is kernel-level threads-not processes-that are in fact being scheduled by the operating system however  the terms process scheduling and thread scheduling are often used interchangeably in this chapter  we use process scheduling when discussing general scheduling concepts and thread scheduling to refer to thread-specific ideas to introduce cpu scheduling  which is the basis for multiprogrammed operating systems to describe various cpu-scheduling algorithms to discuss evaluation criteria for selecting a cpu-scheduling algorithm for a particular system in a single-processor system  only one process can run at a time ; any others must wait until the cpu is free and can be rescheduled the objective of multiprogramming is to have some process rum1ing at all times  to maximize cpu utilization the idea is relatively simple a process is executed until it must wait  typically for the completion of some i/o request in a simple computer system  the cpu then just sits idle all this waiting time is wasted ; no useful work is accomplished with multiprogramming  we try to use this time productively several processes are kept in memory at one time when one process has to wait  the operating system takes the cpu away from that 183 184 chapter 5 load store add store read from file wait for 110 store increment index write to file wait for 1/0 load store add store read from file  wait.tor ; l/0 cpu burst 1/0 burst cpu burst 1/0 burst cpu burst 1/0 burst figure 5.i alternating sequence of cpu and 1/0 bursts process and gives the cpu to another process this pattern continues every time one process has to wait  another process can take over use of the cpu scheduling of this kind is a fundamental operating-system function almost all computer resources are scheduled before use the cpu is  of course  one of the primary computer resources thus  its scheduling is central to operating-system design 5.1.1 cpu-i/o burst cycle the success of cpu scheduling depends on an observed property of processes  process execution consists of a cycle of cpu execution and i/0 wait processes alternate between these two states process execution begins with a cpu burst that is followed by an i/o burst  which is followed by another cpu burst  then another i/0 burst  and so on eventually  the final cpu burst ends with a system request to terminate execution  figure 5.1   the durations of cpu bursts have been measured extensively although they vary greatly from process to process and from computer to compute1 ~ they tend to have a frequency curve similar to that shown in figure 5.2 the curve is generally characterized as exponential or hyperexponential  with a large number of short cpu bursts and a small number of long cpu bursts an i/o-bound program typically has many short cpu bursts a cpu-bound 5.1 185 160 140 120  0 100 c aj    l u 80 ~ 60 40 20 0 8 16 24 32 40 burst duration  milliseconds  figure 5.2 histogram of cpu-burst durations program might have a few long cpu bursts this distribution can be important in the selection of an appropriate cpu-scheduling algorithm 5.1.2 cpu scheduler whenever the cpu becomes idle  the operating system must select one of the processes in the ready queue to be executed the selection process is carried out by the short-term scheduler  or cpu scheduler   the scheduler selects a process from the processes in memory that are ready to execute and allocates the cpu to that process note that the ready queue is not necessarily a first-in  first-out  fifo  queue as we shall see when we consider the various scheduling algorithms  a ready queue can be implen ented as a fifo queue  a priority queue  a tree  or sirnply an unordered linked list conceptually  howeve1 ~ all the processes in the ready queue are lined up waiting for a chance to run on the cpu the records in the queues are generally process control blocks  pcbs  of the processes 5.1.3 preemptive scheduling cpu-scheduling decisions may take place under the following four circumstances  when a process switches from the running state to the waiting state  for example  as the result of an i/0 request or an invocation of wait for the termination of one of the child processes  when a process switches from the numing state to the ready state  for example  when an interrupt occurs  186 chapter 5 when a process switches from the waiting state to the ready state  for example  at completion of i/0  when a process terminates for situations 1 and 4  there is no choice in terms of scheduling a new process  if one exists in the ready queue  must be selected for execution there is a choice  however  for situations 2 and 3 when scheduling takes place only under circumstances 1 and 4  we say that the scheduling scheme is nonpreemptive or cooperative ; otherwise  it is preemptive under nonpreemptive scheduling  once the cpu has been allocated to a process  the process keeps the cpu until it releases the cpu either by terminating or by switching to the waiting state this scheduling method was used by microsoft windows 3.x ; windows 95 introduced preemptive scheduling  and all subsequent versions of windows operating systems have used preemptive scheduling the mac os x operating system for the macintosh also uses preemptive scheduling ; previous versions of the macintosh operating system relied on cooperative scheduling cooperative scheduling is the only method that can be used on certain hardware platforms  because it does not require the special hardware  for example  a timer  needed for preemptive scheduling unfortunately  preemptive scheduling incurs a cost associated with access to shared data consider the case of two processes that share data while one is updating the data  it is preempted so that the second process can run the second process then tries to read the data  which are in an inconsistent state in such situations  we need new mechanisms to coordinate access to shared data ; we discuss this topic in chapter 6 preemption also affects the design of the operating-system kernel during the processing of a system call  the kernel may be busy with an activity on behalf of a process such activities may involve changing important kernel data  for instance  i/0 queues   what happens if the process is preempted in the middle of these changes and the kernel  or the device driver  needs to read or modify the same structure chaos ensues certain operating systems  including most versions of unix  deal with this problem by waiting either for a system call to com.plete or for an i/o block to take place before doing a context switch this scheme ensures that the kernel structure is simple  since the kernel will not preempt a process while the kernel data structures are in an inconsistent state unfortunately  this kernel-execution model is a poor one for supportil1g real-time computing and multiprocessing these problems  and their solutions  are described i.j.1 sections 5.5 and 19.5 because interrupts can  by definition  occur at any time  and because they can not always be ignored by the kernel  the sections of code affected by interrupts must be guarded from simultaneous use the operating system needs to accept interrupts at almost all times ; otherwise  input might be lost or output overwritten so that these sections of code are not accessed concurrently by several processes  they disable interrupts at entry and reenable interrupts at exit it is important to note that sections of code that disable interrupts do not occur very often and typically contain few instructions 5.2 5.2 187 5.1.4 dispatcher another component involved in the cpu-scheduling function is the dispatcher the dispatcher is the module that gives control of the cpu to the process selected by the short-term scheduler this function involves the following  switching context switching to user mode jumping to the proper location in the user program to restart that program the dispatcher should be as fast as possible  since it is invoked during every process switch the time it takes for the dispatcher to stop one process and start another running is known as the dispatch latency different cpu-scheduling algorithms have different properties  and the choice of a particular algorithm may favor one class of processes over another in choosing which algorithm to use in a particular situation  we must consider the properties of the various algorithms many criteria have been suggested for comparing cpu-scheduling algorithms which characteristics are used for comparison can make a substantial difference in which algorithm is judged to be best the criteria include the following  cpu utilization we want to keep the cpu as busy as possible conceptually  cpu utilization can range from 0 to 100 percent in a real system  it should range from 40 percent  for a lightly loaded system  to 90 percent  for a heavily used system   throughput if the cpu is busy executing processes  then work is being done one measure of work is the number of processes that are completed per time unit  called throughput for long processes  this rate may be one process per hour ; for short transactions  it may be ten processes per second turnaround time from the point of view of a particular process  the important criterion is how long it takes to execute that process the interval from the time of submission of a process to the time of completion is the turnaround time turnaround tim.e is the sum of the periods spent waiting to get into memory  waiting in the ready queue  executing on the cpu  and doing i/0 waiting time the cpu-scheduling algorithm does not affect the amount of time during which a process executes or does i/0 ; it affects only the an1.ount of time that a process spends waiting in the ready queue waiting time is the sum of the periods spent waiting in the ready queue response time in an interactive system  turnaround time may not be the best criterion often  a process can produce some output fairly early and can continue computing new results while previous results are being 188 chapter 5 5.3 output to the user thus  another measure is the time from the submission of a request until the first response is produced this measure  called response time  is the tince it takes to start responding  not the time it takes to output the response the turnaround time is generally limited by the speed of the output device it is desirable to maximize cpu utilization and throughput and to minirnize turnaround time  waiting time  and response time in most cases  we optimize the average measure however  under some circumstances  it is desirable to optimize the minimum or maximum values rather than the average for example  to guarantee that all users get good service  we may want to minirnize the maximum response time investigators have suggested that  for interactive systems  such as timesharing systerns   it is more important to minimize the variance in the response time than to minimize the average response time a system with reasonable and predictable response time may be considered more desirable than a system that is faster on the average but is highly variable howeve1 ~ little work has been done on cpu-scheduling algorithms that minimize variance as we discuss various cpu-scheduling algorithms in the following section  we illustrate their operation an accurate illustration should involve many processes  each a sequence of several hundred cpu bursts and i/o bursts for simplicity  though  we consider only one cpu burst  in milliseconds  per process in our examples our measure of comparison is the average waiting time more elaborate evaluation mechanisms are discussed in section 5.7 cpu scheduling deals with the problem of deciding which of the processes in the ready queue is to be allocated the cpu there are many different cpu-scheduling algorithms in this section  we describe several of them 5.3.1 first-come  first-served scheduling by far the simplest cpu-scheduling algorithm is the first-come  first-served  fcfs  scheduling algorithm with this scheme  the process that requests the cpu first is allocated the cpu first the implementation of the fcfs policy is easily managed with a fifo queue when a process enters the ready queue  its pcb is linked onto the tail of the queue when the cpu is free  it is allocated to the process at the head of the queue the running process is then removed from the queue the code for fcfs scheduling is simple to write and understand on the negative side  the average waiting time under the fcfs policy is often quite long consider the following set of processes that arrive at time 0  with the length of the cpu burst given in milliseconds  process burst time  p  24 p2 3 po   3 5.3 189 if the processes ani ve in the order p1  p2  p3  and are served in fcfs order  we get the result shown in the following gantt chart  which is a bar chart that illustrates a particular schedule  including the start and finish times of each of the participating processes  0 24 27 30 the waiting time is 0 milliseconds for process p1  24 milliseconds for process p2  and 27 milliseconds for process p3  thus  the average waiting time is  0 + 24 + 27  /3 = 17 ncilliseconds if the processes arrive in the order p2  p3  p1  however  the results will be as shown in the following gantt chart  0 3 6 30 the average waiting time is now  6 + 0 + 3  /3 = 3 milliseconds this reduction is substantial thus  the average waiting time under an fcfs policy is generally not minimal and may vary substantially if the processes cpu burst times vary greatly in addition  consider the performance of fcfs scheduling in a dynamic situation assume we have one cpu-bound process and many i/o-bound processes as the processes flow armmd the system  the following scenario may result the cpu-bound process will get and hold the cpu during this time  all the other processes will finish their i/0 and will move into the ready queue  waiting for the cpu while the processes wait in the ready queue  the i/0 devices are idle eventually  the cpu-bound process finishes its cpu burst and moves to an i/0 device all the i/o-bound processes  which have short cpu bursts  execute quickly and move back to the i/0 queues at this point  the cpu sits idle the cpu-bound process will then move back to the ready queue and be allocated the cpu again  all the i/0 processes end up waiting in the ready queue until the cpu-bound process is done there is a convoy effect as all the other processes wait for the one big process to get off the cpu this effect results in lower cpu and device utilization than might be possible if the shorter processes were allowed to go first note also that the fcfs scheduling algorithm is nonpreemptive once the cpu has been allocated to a process  that process keeps the cpu until it releases the cpu  either by terminating or by requesting i/0 the fcfs algorithm is thus particularly troublesome for time-sharing systems  where it is important that each user get a share of the cpu at regular intervals it would be disastrous to allow one process to keep the cpu for an extended period 5.3.2 shortest-job-first scheduling a different approach to cpu scheduling is the shortest-job-first  sjf  scheduling algorithm this algorithm associates with each process the length of the process 's next cpu burst when the cpu is available  it is assigned to the process 190 chapter 5 that has the smallest next cpu burst if the next cpu bursts of two processes are the same  fcfs scheduling is used to break the tie note that a more appropriate term for this scheduling method would be the shortest-next-cpu-burst algorithm  because scheduling depends on the length of the next cpu burst of a process  rather than its total length we use the term sjf because m.ost people and textbooks use this term to refer to this type of scheduling as an example of sjf scheduling  consider the following set of processes  with the length of the cpu burst given in milliseconds  process burst time pl 6 p2 8 p3 7 p4 3 using sjf scheduling  we would schedule these processes according to the following gantt chart  0 3 9 16 24 the waiting time is 3 milliseconds for process p1  16 milliseconds for process p2  9 milliseconds for process p3  and 0 milliseconds for process p4  thus  the average waiting time is  3 + 16 + 9 + 0  i 4 = 7 milliseconds by comparison  if we were using the fcfs scheduling scheme  the average waiting time would be 10.25 milliseconds the sjf scheduling algorithm is provably optimal  in that it gives the minimum average waiting time for a given set of processes moving a short process before a long one decreases the waiting time of the short process more than it increases the waiting time of the long process consequently  the average waiting time decreases the real difficulty with the sjf algorithm is knowing the length of the next cpu request for long-term  job  scheduling in a batch system  we can use as the length the process time limit that a user specifies when he submits the job thus  users are motivated to estimate the process time limit accurately  since a lower value may mean faster response  too low a value will cause a time-limit-exceeded error and require resubmission  sjf scheduling is used frequently in long-term scheduling although the sjf algorithm is optimal  it can not be implemented at the level of short-term cpu scheduling with short-term scheduling  there is no way to know the length of the next cpu burst one approach is to try to approximate sjf scheduling we may not know the length of the next cpu burst  but we may be able to predict its value we expect that the next cpu burst will be similar in length to the previous ones by computing an approximation of the length of the next cpu burst  we can pick the process with the shortest predicted cpu burst 5.3 191 the next cpu burst is generally predicted as an exponential average of the measured lengths of previous cpu bursts we can define the exponential average with the following formula let t11 be the length of the nth cpu burst  and let t11 + t be our predicted value for the next cpu burst then  for a  0  s a 1  define the value of tn contains our most recent information ; t11 stores the past history the parameter a controls the relative weight of recent and past history in our prediction if a = 0  then tn + l = t11  and recent history has no effect  current conditions are assumed to be transient   if a = 1  then tn + l = t11  and only the most recent cpu burst matters  history is assumed to be old and irrelevant   more commonly  a = 1/2  so recent history and past history are equally weighted the initial to can be defined as a constant or as an overall system average figure 5.3 shows an exponential average with a = 1/2 and to = 10 to lmderstand the behavior of the exponential average  we can expand the formula for tn + l by substituting for t 11  to find  j jj ' 1 tn + l = atn +  1  a atn-1 + +  1 a  atn-j + +  1 a  'to since both a and  1  a  are less than or equal to 1  each successive term has less weight than its predecessor the sjf algorithm can be either preemptive or nonpreemptive the choice arises when a new process arrives at the ready queue while a previous process is still executing the next cpu burst of the newly arrived process may be shorter time + cpu burst  f  6 4 6 4 13 13 13 guess  t ;  10 8 6 6 5 9 1 1 12 figure 5.3 prediction of the length of the next cpu burst 192 chapter 5 than what is left of the currently executing process a preemptive sjf algorithm will preempt the currently executing process  whereas a nonpreemptive sjf algorithm will allow the currently running process to finish its cpu burst preemptive sjf scheduling is sometimes called shortest-remaining-time-first scheduling as an example  consider the following four processes  with the length of the cpu burst given in milliseconds  process arrival time burst time pl 0 8 p2 1 4 p3 2 9 p4 3 5 if the processes arrive at the ready queue at the times shown and need the indicated burst times  then the resulting preemptive sjf schedule is as depicted in the following gantt chart  0 5 10 17 26 process p1 is started at time 0  since it is the only process in the queue process p2 arrives at time 1 the remaining time for process p1  7 milliseconds  is larger than the time required by process p2  4 milliseconds   so process p1 is preempted  and process p2 is scheduled the average waiting time for this example is   10 1  +  1  1  +  17 2  +  5-3   / 4 = 26/4 = 6.5 milliseconds nonpreemptive sjf scheduling would result in an average waiting time of 7.75 milliseconds 5.3.3 priority scheduling the sjf algorithm is a special case of the general priority scheduling algorithm a priority is associated with each process  and the cpu is allocated to the process with the highest priority equal-priority processes are scheduled in fcfs order an sjf algorithm is simply a priority algorithm where the priority  p  is the inverse of the  predicted  next cpu burst the larger the cpu burst  the lower the priority  and vice versa note that we discuss scheduling in terms of high priority and low priority priorities are generally indicated by some fixed range of numbers  such as 0 to 7 or 0 to 4,095 however  there is no general agreement on whether 0 is the highest or lowest priority some systems use low numbers to represent low priority ; others use low numbers for high priority this difference can lead to confusion in this text  we assume that low numbers represent high priority as an example  consider the following set of processes  assumed to have arrived at time 0 in the order p1  p2   ps  with the length of the cpu burst given in milliseconds  5.3 193 process burst time ~  ~   rity pl 10   0 p2 1 1 p3 2 4 p4 1 5 ps 5 2 using priority scheduling  we would schedule these processes according to the following gantt chart  0 6 16 18 19 the average waiting time is 8.2 milliseconds priorities can be defined either internally or externally internally defined priorities use some nceasurable quantity or quantities to compute the priority of a process for example  time limits  memory requirements  the number of open files  and the ratio of average i/0 burst to average cpu burst have been used in computing priorities external priorities are set by criteria outside the operating system  such as the importance of the process  the type and amount of funds being paid for computer use  the department sponsoring the work  and other  often politicat factors priority scheduling can be either preemptive or nonpreemptive when a process arrives at the ready queue  its priority is compared with the priority of the currently running process a preemptive priority scheduling algorithm will preempt the cpu if the priority of the newly arrived process is higher than the priority of the currently running process a nonpreemptive priority scheduling algorithm will simply put the new process at the head of the ready queue a rnajor problem with priority scheduling algorithms is indefinite blocking  or starvation a process that is ready to run but waiting for the cpu can be considered blocked a priority scheduling algorithm can leave some lowpriority processes waiting indefinitely in a heavily loaded computer system  a steady stream of higher-priority processes can prevent a low-priority process from ever getting the cpu generally  one of two things will happen either the process will eventually be run  at 2 a.m sunday  when the system is finally lightly loaded   or the cornputer systern will eventually crash and lose all unfinished low-priority processes  rumor has it that when they shut down the ibm 7094 at mit in 1973  they found a low-priority process that had been submitted in 1967 and had not yet been run  a solution to the problem of indefinite blockage of low-priority processes is aging aging is a techniqtje of gradually increasing the priority of processes that wait in the system for a long time for example  if priorities range from 127  low  to 0  high   we could increase the priority of a waiting process by 1 every 15 minutes eventually  even a process with an initial priority of 127 would have the highest priority in the system and would be executed in fact  it would take no more than 32 hours for a priority-127 process to age to a priority-0 process 194 chapter 5 5.3.4 round-robin scheduling the round-robin  rr  scheduling algorithm is designed especially for timesharing systems it is similar to fcfs scheduling  but preemption is added to enable the system to switch between processes a small unit of time  called a time quantum or time slice  is defined a time quantum is generally fronc 10 to 100 milliseconds in length the ready queue is treated as a circular queue the cpu scheduler goes around the ready queue  allocating the cpu to each process for a time interval of up to 1 time quantum to implement rr scheduling  we keep the ready queue as a fifo queue o processes new processes are added to the tail of the ready queue the cpu scheduler picks the first process from the ready queue  sets a timer to interrupt after 1 time quantum  and dispatches the process one of two things will then happen the process may have a cpu burst of less than 1 time quantum in this case  the process itself will release the cpu voluntarily the scheduler will then proceed to the next process in the ready queue otherwise  if the cpu burst of the currently running process is longer than 1 time quantum  the timer will go off and will cause an interrupt to the operating system a context switch will be executed  and the process will be put at the tail o the ready queue the cpu scheduler will then select the next process in the ready queue the average waiting time under the rr policy is often long consider the following set of processes that arrive at time 0  with the length of the cpu burst given in milliseconds  process burst time if we use a time quantum of 4 milliseconds  then process p1 gets the first 4 milliseconds since it requires another 20 milliseconds  it is preempted after the first time quantum  and the cpu is given to the next process in the queue  process p2  process p2 does not need 4 milliseconds  so it quits before its time quantum expires the cpu is then given to the next process  process p3 once each process has received 1 time quantum  the cpu is returned to process p1 for an additional time quantum the resulting rr schedule is as follows  0 4 7 10 14 18 22 26 30 let 's calculate the average waiting time for the above schedule p1 waits for 6 millisconds  10 4   p2 waits for 4 millisconds  and p3 waits for 7 millisconds thus  the average waiting time is 17/3 = 5.66 milliseconds in the rr scheduling algorithm  no process is allocated the cpu for more than 1 time quantum in a row  unless it is the only runnable process   if a 5.3 195 process 's cpu burst exceeds 1 time quantum  that process is preempted and is p11t back in the ready queue the rr scheduling algorithm is thus preemptive if there are n processes in the ready queue and the time quantum is q  then each process gets 1 in of the cpu time in chunks of at most q time units each process must wait no longer than  11  1  x q time units until its next time quantum for example  with five processes and a time quantum of 20 milliseconds  each process will get up to 20 milliseconds every 100 milliseconds the performance of the rr algorithm depends heavily on the size of the time quantum at one extreme  if the time quantum is extremely large  the rr policy is the same as the fcfs policy in contrast  if the time quantum is extremely small  say  1 millisecond   the rr approach is called processor sharing and  in theory  creates the appearance that each of 11 processes has its own processor running at 1 i 11 the speed of the real processor this approach was used in control data corporation  cdc  hardware to implement ten peripheral processors with only one set of hardware and ten sets of registers the hardware executes one instruction for one set of registers  then goes on to the next this cycle continues  resulting in ten slow processors rather than one fast one  actually  since the processor was much faster than memory and each instruction referenced memory  the processors were not much slower than ten real processors would have been  in software  we need also to consider the effect of context switching on the performance of rr scheduling assume  for example  that we have only one process of 10 time units if the quantum is 12 time units  the process finishes in less than 1 time quantum  with no overhead if the quantum is 6 time units  however  the process requires 2 quanta  resulting in a context switch if the time quantum is 1 time unit  then nine context switches will occur  slowing the execution of the process accordingly  figure 5.4   thus  we want the time quantum to be large with respect to the contextswitch time if the context-switch time is approximately 10 percent of the time quantum  then about 10 percent of the cpu time will be spent in context switching in practice  most modern systems have time quanta ranging from 10 to 100 milliseconds the time required for a context switch is typically less than 10 microseconds ; thus  the context-switch time is a small fraction of the time quantum process time = 10 quantum context switches 12 0 0 10 6 0 6 10 r.r r  r  r    .r r -lr  -r r 9 0 2 3 4 5 6 7 8 9 10 figure 5.4 how a smaller time quantum increases context switches 196 chapter 5 process time 12.5 .p1 6 12.0 pz 3 p3 1 q  e 11.5 p4 7  ;   ; 0 c 11.0   j 0  a e 10.5 .2 q  10.0 en ~ q  9.5 c1l 9.0 2 3 4 5 6 7 time quantum figure 5.5 how turnaround time varies with the time quantum turnaround time also depends on the size of the time quantum as we can see from figure 5.5  the average turnaround time of a set of processes does not necessarily improve as the time-quantum size increases in general  the average turnaround time can be improved if most processes finish their next cpu burst in a single time quantum for example  given three processes of 10 time units each and a quantum of 1 time unit  the average turnaround time is 29 if the time quantum is 10  however  the average turnaround time drops to 20 if context-switch time is added in  the average turnaround time increases even more for a smaller time quantum  since more context switches are required although the time quantum should be large compared with the contextswitch time  it should not be too large if the time quantum is too large  rr scheduling degenerates to an fcfs policy a rule of thumb is that 80 percent of the cpu bursts should be shorter than the time quantum 5.3.5 multilevel queue scheduling another class of scheduling algorithms has been created for situations in which processes are easily classified into different groups for example  a common division is made between foreground  interactive  processes and background  batch  processes these two types of processes have different response-time requirements and so may have different scheduling needs in addition  foreground processes may have priority  externally defined  over background processes a multilevel queue scheduling algorithm partitions the ready queue into several separate queues  figure 5.6   the processes are permanently assigned to one queue  generally based on some property of the process  such as memory 5.3 197 highest priority = = = = ~ '-------'i-'-n_te_r ~ ac_t_iv_e_e  .d_it ~ in_g'-'-p ~ r-.o'-c_ e'---ss ~ e-s  --'-'---l = = = = i = = = = = = ~ '---------'b_a_tc_h_p_r_o_ce_s_s_e_s ______ _j = = = = = = ~ = = = = = = ~ '-------s_tu_d_e_n_t_p_ro_c_e_s_s_es_ _____ _jl = = = = = = i lowest priority figure 5.6 multilevel queue scheduling size  process priority  or process type each queue has its own scheduling algorithm for example  separate queues might be used for foreground and background processes the foreground queue might be scheduled by an rr algorithm  while the background queue is scheduled by an fcfs algorithm in addition  there must be scheduling among the queues  which is commonly implemented as fixed-priority preemptive scheduling for example  the foreground queue may have absolute priority over the background queue let 's look at an example of a multilevel queue scheduling algorithm with five queues  listed below in order of priority  system processes interactive processes interactive editing processes batch processes student processes each queue has absolute priority over lower-priority queues no process in the batch queue  for example  could run unless the queues for system processes  interactive processes  and interactive editing processes were all empty if an interactive editing process entered the ready queue while a batch process was running  the batch process would be preempted another possibility is to time-slice among the queues here  each queue gets a certain portion of the cpu time  which it can then schedule among its various processes for instance  in the foreground-background queue example  the foreground queue can be given 80 percent of the cpu time for rr scheduling among its processes  whereas the background queue receives 20 percent of the cpu to give to its processes on an fcfs basis 198 chapter 5 5.3.6 multilevel feedback queue scheduling normally  when the multilevel queue scheduling algorithm is used  processes are permanently assigned to a queue when they enter the system if there are separate queues for foreground and background processes  for example  processes do not move from one queue to the other  since processes do not change their foreground or background nature this setup has the advantage of low scheduling overhead  but it is inflexible the multilevel feedback queue scheduling algorithm  in contrast  allows a process to move between queues the idea is to separate processes according to the characteristics of their cpu bursts if a process uses too much cpu time  it will be moved to a lower-priority queue this scheme leaves i/o-bound and interactive processes in the higher-priority queues in addition  a process that waits too long in a lower-priority queue may be moved to a higher-priority queue this form of aging prevents starvation for example  consider a multilevel feedback queue scheduler with three queues  numbered from 0 to 2  figure 5.7   the scheduler first executes all processes in queue 0 only when queue 0 is empty will it execute processes in queue 1 similarly  processes in queue 2 will only be executed if queues 0 and 1 are empty a process that arrives for queue 1 will preempt a process in queue 2 a process in queue 1 will in turn be preempted by a process arriving for queue 0 a process entering the ready queue is put in queue 0 a process in queue 0 is given a time quantum of 8 milliseconds if it does not filcish within this time  it is moved to the tail of queue 1 if queue 0 is empty  the process at the head of queue 1 is given a quantum of 16 milliseconds if it does not complete  it is preempted and is put into queue 2 processes in queue 2 are run on an fcfs basis but are run only when queues 0 and 1 are empty this scheduling algorithm gives highest priority to any process with a cpu burst of 8 milliseconds or less such a process will quickly get the cpu  finish its cpu burst  and go off to its next i/0 burst processes that need more than 8 but less than 24 milliseconds are also served quickly  although with lower priority than shorter processes long processes automatically sink to queue 2 and are served in fcfs order with any cpu cycles left over from queues 0 and 1 figure 5.7 multilevel feedback queues 5.4 5.4 199 in general  a multilevel feedback queue scheduler is defined by the following parameters  the number of queues the scheduling algorithm for each queue the method used to determine when to upgrade a process to a higherpriority queue the method used to determine when to demote a process to a lowerpriority queue the method used to determine which queue a process will enter when that process needs service the definition of a multilevel feedback queue scheduler makes it the most general cpu-scheduling algorithm it can be configured to match a specific system under design unfortunately  it is also the most complex algorithm  since defining the best scheduler requires some means by which to select values for all the parameters in chapter 4  we introduced threads to the process model  distinguishing between user-level and kernel-level threads on operating systems that support them  it is kernel-level threads-not processes-that are being scheduled by the operating system user-level threads are managed by a thread library  and the kernel is unaware of them to run on a cpu  user-level threads must ultimately be mapped to an associated kernel-level thread  although this mapping may be indirect and may use a lightweight process  lwp   in this section  we explore scheduling issues involving user-level and kernel-level threads and offer specific examples of scheduling for pthreads 5.4.1 contention scope one distinction between user-level and kernel-level threads lies in how they are scheduled on systems implementing the many-to-one  section 4.2.1  and many-to-many  section 4.2.3  models  the thread library schedules user-level threads to run on an available lwp  a scheme known as process-contention scope  pcs   since competition for the cpu takes place among threads belonging to the same process when we say the thread library schedules user threads onto available lwps  we do not mean that the thread is actually running on a cpu ; this would require the operating system to schedule the kernel thread onto a physical cpu to decide which kernel thread to schedule onto a cpu  the kernel uses system-contention scope  scs   competition for the cpu with scs scheduling takes place among all threads in the system systems usilcg the one-to-one model  section 4.2.2   such as windows xp  solaris  and linux  schedule threads using only scs typically  pcs is done according to priority-the scheduler selects the runnable thread with the highest priority to run user-level thread priorities 200 chapter 5 5.5 are set by the programmer and are not adjusted by the thread library  although some thread libraries may allow the programmer to change the priority of a thread it is important to note that pcs will typically preempt the thread currently running in favor of a higher-priority thread ; however  there is no guarantee of time slicing  section 5.3.4  among threads of equal priority 5.4.2 pthread scheduling we provided a sample postx pthread program in section 4.3.1  along with an introduction to thread creation with pthreads now  we highlight the posix pthread api that allows specifying either pcs or scs during thread creation pthreads identifies the following contention scope values  pthread_scope_process schedules threads using pcs scheduling pthread_scope_system schedules threads using scs scheduling on systems implementing the many-to-many model  the pthread_scope_process policy schedules user-level threads onto available lwps the number of lwps is maintained by the thread library  perhaps using scheduler activations  section 4.4.6   the pthread_scope_system scheduling policy will create and bind an lwp for each user-level thread on many-to-many systems  effectively mapping threads using the one-to-one policy the pthread ipc provides two functions for getting-and setting-the contention scope policy  pthread_attr_setscope  pthread_attr_t attr  int scope  pthread_attr_getscope  pthread_attr_t attr  int scope  the first parameter for both functions contains a pointer to the attribute set for the thread the second parameter for the pthread_attr_setscope   function is passed either the pthread_scope_system or the pthread_scope_process value  indicating how the contention scope is to be set in the case of pthread_attr_getscope    this second parameter contaiils a pointer to an int value that is set to the current value of the contention scope if an error occurs  each of these functions returns a non-zero value in figure 5.8  we illustrate a pthread scheduling api the program first determines the existing contention scope and sets it to pthread_scoplprocess it then creates five separate threads that will run using the scs scheduling policy note that on some systems  only certain contention scope values are allowed for example  linux and mac os x systems allow only pthread_scope_system our discussion thus far has focused on the problems of scheduling the cpu in a system with a single processor if multiple cpus are available  load sharing becomes possible ; however  the scheduling problem becomes correspondingly 505 # include pthreadoh # include stdiooh # define num_threads 5 int main  int argc  char argv      int i  scope ; pthread_t tid  num_threads  ; pthread_attr_t attr ; i get the default attributes i pthread_attr_init  &attr  ; i first inquire on the current scope i if  pthread_attr_getscope  &attr  &scope  ! = 0  fprintf  stderr  unable to get scheduling scope \ n  ; else   if  scope = = pthread_scope_process  printf  pthread_scoplprocess  ; else if  scope = = pthread_scope_system  printf  pthread_scope_system  ; else fprintf  stderr  illegal scope valueo \ n  ; i set the scheduling algorithm to pcs or scs i pthread_attr_setscope  &attr  pthread_scope_system  ; i create the threads i for  i = 0 ; i num_threads ; i + +  pthread_create  &tid  i  ,&attr,runner,null  ; i now join on each thread i for  i = 0 ; i num_threads ; i + +  pthread_join  tid  i   null  ; i each thread will begin control in this function i void runner  void param   i do some work 0 0 0 i pthread_exi t  0  ;  figure 508 pthread scheduling api 201 more complex many possibilities have been tried ; and as we saw with singleprocessor cpu scheduling  there is no one best solution here  we discuss several concerns in multiprocessor scheduling we concentrate on systems 202 chapter 5 in which the processors are identical-homogeneous-in terms of their functionality ; we can then use any available processor to run any process in the queue  note  however  that even with homogeneous multiprocessors  there are sometimes limitations on scheduling consider a system with an l/0 device attached to a private bus of one processor processes that wish to use that device must be scheduled to run on that processor  5.5.1 approaches to multiple-processor scheduling one approach to cpu scheduling in a n1.ultiprocessor system has all scheduling decisions  i/o processing  and other system activities handled by a single processor-the master server the other processors execute only user code this asymmetric multiprocessing is simple because only one processor accesses the system data structures  reducing the need for data sharing a second approach uses symmetric multiprocessing  smp   where each processor is self-scheduling all processes may be in a common ready queue  or each processor may have its own private queue of ready processes regardless  scheduling proceeds by having the scheduler for each processor examine the ready queue and select a process to execute as we shall see in chapter 61 if we have multiple processors trying to access and update a common data structure  the scheduler must be programmed carefully we must ensure that two processors do not choose the same process and that processes are not lost from the queue virtually all modern operating systems support smp  including windows xp  windows 2000  solaris  linux  and mac os x in the remainder of this section  we discuss issues concerning smp systems 5.5.2 processor affinity consider what happens to cache memory when a process has been running on a specific processor the data most recently accessed by the process populate the cache for the processor ; and as a result  successive memory accesses by the process are often satisfied in cache memory now consider what happens if the process migrates to another processor the contents of cache memory must be invalidated for the first processor  and the cache for the second processor must be repopulated because of the high cost of invalidating and repopulating caches  most smp systems try to avoid migration of processes from one processor to another and instead attempt to keep a process rumung on the same processor this is known as processor affinity-that is  a process has an affinity for the processor on which it is currently rumting processor affinity takes several forms when an operating system has a policy of attempting to keep a process running on the same processor-but not guaranteeing that it will do so-we have a situation known as soft affinity here  it is possible for a process to migrate between processors some systems -such as lim.ix -also provide system calls that support hard affinity  thereby allowing a process to specify that it is not to migrate to other processors solaris allows processes to be assigned to limiting which processes can run on which cpus it also implements soft affinity the main-memory architecture of a system can affect processor affinity issues figure 5.9 illustrates an architecture featuring non-uniform memory access  numa   in which a cpu has faster access to some parts of main memory than to other parts typically  this occurs in systems containing combined cpu 5.5 203 computer figure 5.9 numa and cpu scheduling and memory boards the cpus on a board can access the memory on that board with less delay than they can access memory on other boards in the system if the operating system 's cpu scheduler and memory-placement algorithms work together  then a process that is assigned affinity to a particular cpu can be allocated memory on the board where that cpu resides this example also shows that operating systems are frequently not as cleanly defined and implemented as described in operating-system textbooks rather  the solid lines between sections of an operating system are frequently only dotted lines  with algorithms creating connections in ways aimed at optimizing performance and reliability 5.5.3 load balancing on smp systems  it is important to keep the workload balanced among all processors to fully utilize the benefits of having more than one processor otherwise  one or more processors may sit idle while other processors have high workloads  along with lists of processes awaiting the cpu load balancing attempts to keep the workload evenly distributed across all processors in an smp system it is important to note that load balancing is typically only necessary on systems where each processor has its own private queue of eligible processes to execute on systems with a common run queue  load balancing is often unnecessary  because once a processor becomes idle  it immediately extracts a rmmable process from the common run queue it is also important to note  howeve1 ~ that in most contemporary operating systems supporting smp  each processor does have a private queue of eligible processes there are two general approaches to load balancing  push migration and pull migration with push migration  a specific task periodically checks the load on each processor and -if it finds an imbalance-evenly distributes the load by moving  or pushing  processes from overloaded to idle or less-busy processors pull migration occurs when an idle processor pulls a waiting task from a busy processor push and pull migration need not be mutually exclusive and are in fact often implemented in parallel on load-balancing systems for example  the linux scheduler  described in section 5.6.3  and the ule scheduler 204 chapter 5 available for freebsd systems implement both techniql1es linux runs its loadbalancing algorithm every 200 milliseconds  push migration  or whenever the run queue for a processor is empty  pull migration   interestingly  load balancing often counteracts the benefits of processor affinity  discussed in section 5.5.2 that is  the benefit of keeping a process running on the same processor is that the process can take advantage of its data being in that processor 's cache memory either pulling or pushing a process from one processor to another invalidates this benefit as is often the case in systems engineering  there is no absolute rule concerning what policy is best thus  in some systems  an idle processor always pulls a process from a non-idle processor ; and in other systems  processes are moved only if the imbalance exceeds a certain threshold 5.5.4 multicore processors traditionally  smp systems have allowed several threads to run concurrently by providing multiple physical processors however  a recent trend in computer hardware has been to place multiple processor cores on the same physical chip  resulting in a  each core has a register set to maintain its architectural state and appears to the operating system to be a separate physical processor smp systems that use multicore processors are faster and consume less power than systems in which each processor has its own physical chip multicore processors may complicate scheduling issues let 's consider how this can happen researchers have discovered that when a processor accesses memory  it spends a significant amount of time waiting for the data to become available this situation  known as a may occur for various reasons  such as a cache miss  accessing data that is not in cache memory   figure 5.10 illustrates a memory stall in this scenario  the processor can spend up to 50 percent of its time waiting for data to become available from memory to remedy this situation  many recent hardware designs have implemented multithreaded processor cores in which two  or more  hardware threads are assigned to each core that way  if one thread stalls while waiting for memory  the core can switch to another thread figure 5.11 illustrates a dual-threaded processor core on which the execution of thread 0 and the execution of thread 1 are interleaved from an operating-system perspective  each hardware thread appears as a logical processor that is available to run a software thread thus  on a dual-threaded  dual-core system  four logical processors are presented to the operating system the ultrasparc tl cpu has eight cores per chip and four 0 compute cycle ~ memory stall cycle thread c m c m c m c m time figure 5.10 memory stall 5.5 205 thread1 c m c m c m c thread0 c m c m c m c time figure 5.11 multithreaded multicore system hardware threads per core ; from the perspective of the operating system  there appear to be 32 logical processors in general  there are two ways to multithread a processor  ~ __u.,u c   ; u  chccu multithreading with coarse-grained multithreading  a thread executes on a processor until a long-latency event such as a memory stall occurs because of the delay caused by the long-latency event  the processor must switch to another thread to begin execution however  the cost of switching between threads is high  as the instruction pipeline must be flushed before the other thread can begin execution on the processor core once this new thread begins execution  it begins filling the pipeline with its instructions fine-grained  or interleaved  multithreading switches between threads at a much finer level of granularity-typically at the boundary of an instruction cycle however  the architectural design of fine-grained systems includes logic for thread switching as a result  the cost of switching between threads is small notice that a multithreaded multicore processor actually requires two different levels of scheduling on one level are the scheduling decisions that must be made by the operating system as it chooses which software thread to run on each hardware thread  logical processor   for this level of scheduling  the operating system may choose any scheduling algorithm  such as those described in section 5.3 a second level of scheduling specifies how each core decides which hardware thread to run there are several strategies to adopt in this situation the ultrasparc tl  mentioned earlier  uses a simple roundrobin algorithm to schedule the four hardware threads to each core another example  the intel itanium  is a dual-core processor with hvo hardwaremanaged threads per core assigned to each hardware thread is a dynamic urgency value ranging from 0 to 7  with 0 representing the lowest urgency  and 7 the highest the itanium identifies five different events that may trigger a thread switch when one of these events occurs  the thread-switching logic compares the urgency of the two threads and selects the thread with the highest urgency value to execute on the processor core 5.5.5 virtualization and scheduling a system with virtualization  even a single-cpu system  frequently acts like a multiprocessor system the virtualization software presents one or more virtual cpus to each of the virtual machines rum1.ing on the system and then schedules the use of the physical cpus among the virtual machines the significant variations between virtualization technologies make it difficult to summarize the effect of virtualization on scheduling  see section 2.8   in general  though  most virtualized environments have one host operating 206 chapter 5 5.6 system and many guest operating systems the host operating system creates and manages the virtual machines  and each virtual n achine has a guest operating system installed and applications running within that guest eacb guest operating system may be fine-tuned for specific use cases  applications  and users  including time sharing or even real-time operation any guest operating-system scheduling algorithm that assumes a certain amount of progress in a given amount of time will be negatively impacted by virtualization consider a time-sharing operating system that tries to allot 100 milliseconds to each time slice to give users a reasonable response time within a virtual machine  this operating system is at the mercy of the virtualization system as to what cpu resources it actually receives a given 100-millisecond time slice may take much more than 100 milliseconds of virtual cpu time depending on how busy the system is  the time slice may take a second or more  resulting in very poor response times for users logged into that virtual machine the effect on a real-time operating system would be even more catastrophic the net effect of such scheduling layering is that individual virtualized operating systems receive only a portion of the available cpu cycles  even though they believe they are receiving all of the cycles and indeed that they are scheduling all of those cycles commonly  the time-of-day clocks in virtual machines are incorrect because timers take longer to trigger than they would on dedicated cpus virtualization can thus lmdo the good scheduling-algorithm efforts of the operating systems within virtual machines we turn next to a description of the scheduling policies of the solaris  windows xp  and linux operating systems it is important to remember that we are describing the scheduling of kernel tlueads with solaris and windows xp recall that linux does not distinguish between processes and threads ; thus  we use the term task when discussing the linux scheduler 5.6.1 example  solaris scheduling solaris uses priority-based thread scheduling where each thread belongs to one of six classes  time sharing  ts  interactive  ia  real time  rt  system  sys  fair share  fss  fixed priority  fp  within each class there are different priorities and different scheduling algorithms the default scheduling class for a process is time sharing the scheduling policy for the time-sharing class dynamically alters priorities and assigns time 5.6 207 10 160 0 51 15 160 5 51 20 120 10 52 25 120 15 52 30 80 20 53 35 80 25 54 40 40 30 55 45 40 35 56 50 40 40 58 55 40 45 58 59 20 49 59 figure 5.12 solaris dispatch table for time-sharing and interactive threads slices of different lengths using a multilevel feedback queue by default  there is an inverse relationship between priorities and time slices the higher the priority  the smaller the time slice ; and the lower the priority  the larger the time slice interactive processes typically have a higher priority ; cpu-bound processes  a lower priority this scheduling policy gives good response time for interactive processes and good throughput for cpu-bound processes the interactive class uses the same scheduling policy as the time-sharing class  but it gives windowing applications-such as those created by the kde or gnome window managers-a higher priority for better performance figure 5.12 shows the dispatch table for scheduling time-sharing and interactive threads these two scheduling classes include 60 priority levels  but for brevity  we display only a handful the dispatch table shown in figure 5.12 contains the following fields  priority the class-dependent priority for the time-sharing and interactive classes a higher number indicates a higher priority time quantum the time quantum for the associated priority this illustrates the inverse relationship between priorities and time quanta  the lowest priority  priority 0  has the highest tince quantum  200 milliseconds   and the highest priority  priority 59  has the lowest time quantum  20 milliseconds   time quantum expired the new priority of a thread that has used its entire time quantum without blocking such threads are considered 208 chapter 5 cpu-intensive as shown in the table  these threads have their priorities lowered return from sleep the priority of a thread that is returning from sleeping  such as waiting for i/0   as the table illustrates  when i/0 is available for a waiting thread  its priority is boosted to between 50 and 59  thus supporting the scheduling policy of providing good response time for interactive processes threads in the real-time class are given the highest priority this assignment allows a real-time process to have a guaranteed response from the system within a bounded period of time a real-time process will run before a process in any other class in general  however  few processes belong to the real-time class solaris uses the system class to run kernel threads  such as the scheduler and paging daemon once established  the priority of a system thread does not change the system class is reserved for kernel use  user processes rum1ing in kernel mode are not in the system class   the fixed-priority and fair-share classes were introduced with solaris 9 threads in the fixed-priority class have the same priority range as those in the time-sharing class ; however  their priorities are not dynamically adjusted the fair-share scheduling class uses cpu instead of priorities to make scheduling decisions cpu shares indicate entitlement to available cpu resources and are allocated to a set of processes  known as a project   each scheduling class includes a set of priorities however  the scheduler converts the class-specific priorities into global priorities and selects the thread with the highest global priority to n.m the selected thread n.ms on the cpu until it  1  blocks   2  uses its time slice  or  3  is preempted by a higher-priority thread if there are multiple threads with the same priority  the scheduler uses a round-robin queue figure 5.13 illustrates how the six scheduling classes relate to one another and how they map to global priorities notice that the kernel maintains 10 threads for servicing interrupts these threads do not belong to any scheduling class and execute at the highest priority  160-169   as mentioned  solaris has traditionally used the many-to-many model  section 4.2.3  but switched to the one-to-one model  section 4.2.2  beginning with solaris 9 5.6.2 example  windows xp scheduling windows xp schedules threads using a priority-based  preemptive scheduling algorithm the windows xp scheduler ensures that the highest-priority thread will always run the portion of the windows xp kernel that handles scheduling is called the dispatcher a thread selected to run by the dispatcher will run until it is preempted by a higher-priority thread  until it terminates  until its time quantum ends  or until it calls a blocking system call  such as for i/0 if a higher-priority real-time thread becomes ready while a lower-priority thread is running  the lower-priority thread will be preempted this preemption gives a real-time thread preferential access to the cpu when the thread needs such access the dispatcher uses a 32-level priority scheme to determine the order of thread execution priorities are divided into two classes the global priority highest lowest 169 160 159 100 99 60 59 0 5.6 figure 5.13 solaris scheduling scheduling order first last 209 contains threads having priorities from 1 to 15  and the contains threads with priorities ranging from 16 to 31  there is also a thread running at priority 0 that is used for memory management  the dispatcher uses a queue for each scheduling priority and traverses the set of queues from highest to lowest until it finds a thread that is ready to run if no thread is found  the dispatcher will execute a special thread called the there is a relationship between the numeric priorities of the windows xp kernel and the win32 api the win32 api identifies several priority classes to which a process can belong these include  realtime_priority _class higf-lpriority _class abovknormalpriority class normalpriority class 210 chapter 5 .15 12 10 14 11 9 13 10 8 12 9 7 22 1.1 8 6 16 figure 5.14 windows xp priorities below normal...priority _class idle...priority _class 8 6 7 5 6 4 5 3 4 2 priorities in all classes except the realtime...priority _class are variable  meaning that the priority of a thread belonging to one of these classes can change a thread within a given priority classes also has a relative priority the values for relative priorities include  time_critical highest above_normal normal below normal lowest idle the priority of each thread is based on both the priority class it belongs to and its relative priority within that class this relationship is shown in figure 5.14 the values of the priority classes appear in the top row the left column contains the values for the relative priorities for example  if the relative priority of a thread in the above_normal...priority_class is normal  the nunceric priority of that thread is 10 furthermore  each thread has a base priority representing a value in the priority range for the class the thread belongs to by default  the base priority is the value of the normal relative priority for that class the base priorities for each priority class are  realtime...priority_class-24 higrlpriority class-13 5.6 above_normalpriority_class-10 normalpriority _class-8 below _normalpriority _class-6 idle_priority _class-4 211 processes are typically members of the normalpriority_class a process belongs to this class unless the parent of the process was of the idle_priority _class or unless another class was specified when the process was created the initial priority of a thread is typically the base priority of the process the thread belongs to when a thread 's time quantun1 runs out  that thread is interrupted ; if the thread is in the variable-priority class  its priority is lowered the priority is never lowered below the base priority  however lowering the priority tends to limit the cpu consumption of compute-bound threads when a variablepriority thread is released from a wait operation  the dispatcher boosts the priority the amount of the boost depends on what the thread was waiting for ; for example  a thread that was waiting for keyboard i/0 would get a large increase  whereas a thread waiting for a disk operation would get a moderate one this strategy tends to give good response times to interactive threads that are using the mouse and windows it also enables i/o-bound threads to keep the i/0 devices busy while permitting compute-bound threads to use spare cpu cycles in the background this strategy is used by several time-sharing operating systems  including unix in addition  the window with which the user is currently interacting receives a priority boost to enhance its response time when a user is running an interactive program  the system needs to provide especially good performance for this reason  windows xp has a special scheduling rule for processes in the normalpriority_class windows xp distinguishes between the foreground process that is currently selected on the screen and the background processes that are not currently selected when a process moves into the foreground  windows xp increases the scheduling quantum by some factor-typically by 3 this increase gives the foreground process three times longer to run before a time-sharing preemption occurs 5.6.3 example  linux scheduling prior to version 2.5  the linux kernel ran a variation of the traditional unix scheduling algorithm two problems with the traditional unix scheduler are that it does not provide adequate support for smp systems and that it does not scale well as the number of tasks on the system grows with version 2.5  the scheduler was overhauled  and the kernel now provides a scheduling algorithm that runs in constant time-known as 0  1  -regardless of the number of tasks on the system the new scheduler also provides increased support for smp  including processor affinity and load balancing  as well as providing fairness and support for interactive tasks the linux scheduler is a preemptive  priority-based algorithm with two separate priority ranges  a real-time range from 0 to 99 and a nice value ranging from 100 to 140 these two ranges map into a global priority scheme wherein numerically lower values indicate higher priorities 212 chapter 5 numeric priority 0 99 100 140 relative priority highest lowest time quantum 200 ms 10 ms figure 5.15 the relationship between priorities and time-slice length unlike schedulers for many other systems  including solaris  section 5.6.1  and windows xp  section 5.6.2   lim1x assigns higher-priority tasks longer time quanta and lower-priority tasks shorter time quanta the relationship between priorities and tim.e-slice length is shown in figure 5.15 a runnable task is considered eligible for execution on the cpu as long as it has time remaining in its time slice when a task has exhausted its time slice  it is considered expired and is not eligible for execution again until all other tasks have also exhausted their time quanta the kernel maintains a list of all runnable tasks in a data structure because of its support for smp  each processor maintains its own nmqueue and schedules itself independently each runqueue contains two priority arrays  and the active array contains all tasks with time remaining in their time slices  and the expired array contains all expired tasks each of these priority arrays contains a list of tasks indexed according to priority  figure 5.16   the scheduler chooses the task with the highest priority from the active array for execution on the cpu on multiprocessor machines  this means that each processor is scheduling the highest-priority task from its own runqueue structure when all tasks have exhausted their time slices  that is  the active array is empty   the two priority arrays are exchanged ; the expired array becomes the active array  and vice versa linux implements real-time scheduling as defined by posix.1b  which is described in section 5.4.2 real-time tasks are assigned static priorities all active array priority  0   1   140  task lists o-o 0--0--0 0 expired array priority  0   1   140  task lists o--o-o 0 figure 5.16 list of tasks indexed according to priority 5.7 5.7 213 other tasks have dynamic priorities that are based on their nice values plus or minus the value 5 the interactivity of a task determines whether the value 5 will be added to or subtracted from the nice value a task 's interactivity is deterncined by how long it has been sleeping while waiting for i/0 tasks that are more interactive typically have longer sleep times and therefore are more likely to have adjustments closer to -5  as the scheduler favors interactive tasks the result of such adjustments will be higher priorities for these tasks conversely  tasks with shorter sleep times are often more cpu-bound and thus will have their priorities lowered a task 's dynamic priority is recalculated when the task has exhausted its time quantum and is to be moved to the expired array thus  when the two arrays are exchanged  all tasks in the new active array have been assigned new priorities and corresponding time slices how do we select a cpu-scheduling algorithm for a particular system as we saw in section 5.3  there are many scheduling algorithms  each with its own parameters as a result  selecting an algorithm can be difficult the first problem is defining the criteria to be used in selecting an algorithm as we saw in section 5.2  criteria are often defined in terms of cpu utilization  response time  or thxoughput to select an algorithm  we must first define the relative importance of these elements our criteria may include several measures  such as  maximizing cpu utilization under the constraint that the maximum response time is 1 second maximizing throughput such that turnaround time is  on average  linearly proportional to total execution time once the selection criteria have been defined  we want to evaluate the algorithms under consideration we next describe the various evaluation methods we can use 5.7.1 deterministic modeling one major class of evaluation methods is analytic evaluation analytic evaluation uses the given algorithm and the system workload to produce a formula or number that evaluates the performance of the algorithm for that workload deterministic modeling is one type of analytic evaluation this method takes a particular predetermined workload and defines the performance of each algorithm for that workload for example  assume that we have the workload shown below all five processes arrive at time 0  in the order given  with the length of the cpu burst given in milliseconds  214 chapter 5 process burst time  ~ pj 10 p2 29 po c  j 0 p4 7 ps 12 consider the fcfs  sjf  and rr  quantum = 10 milliseconds  scheduling algorithms for this set of processes which algorithm would give the minimum average waiting time for the fcfs algorithm  we would execute the processes as 0 10 39 42 49 61 the waiting time is 0 milliseconds for process p1  10 milliseconds for process p2  39 milliseconds for process p3  42 milliseconds for process p4  and 49 milliseconds for process p5  thus  the average waiting time is  0 + 10 + 39 + 42 + 49  /5 = 28 milliseconds with nonpreemptive sjf scheduling  we execute the processes as 0 3 10 20 p 5 32 61 the waiting time is 10 milliseconds for process p11 32 milliseconds for process p2  0 milliseconds for process p3  3 milliseconds for process p4  and 20 milliseconds for process p5  thus  the average waiting time is  10 + 32 + 0 + 3 + 20  i 5 = 13 milliseconds 0 with the rr algorithm  we execute the processes as p 1 10 20 23 30 40 50 52 61 the waiting time is 0 milliseconds for process p1  32 milliseconds for process p2  20 milliseconds for process p3  23 milliseconds for process p4  and 40 milliseconds for process p5  thus  the average waiting time is  0 + 32 + 20 + 23 + 40  /5 = 23 milliseconds we see that  in this case  the average waiting time obtained with the sjf policy is less than half that obtained with fcfs scheduling ; the rr algorithm gives us an intermediate value deterministic modeling is simple and fast it gives us exact numbers  allowing us to compare the algorithms however  it requires exact numbers for input  and its answers apply only to those cases the main uses of deterministic modeling are in describing scheduling algorithms and providing examples in 5.7 215 cases where we are running the same program over and over again and can measure the program 's processing requirements exactly  we may be able to use deterministic modeling to select a scheduling algorithm furthermore  over a set of examples  deterministic modeling may indicate trends that can then be analyzed and proved separately for example  it can be shown that  for the environment described  all processes and their times available at tirne 0   the sjf policy will always result in the rninimum waiting time 5.7.2 queueing models on many systems  the processes that are run vary from day to day  so there is no static set of processes  or times  to use for deterministic modeling what can be determined  however  is the distribution of cpu and i/0 bursts these distributions can be measured and then approximated or simply estimated the result is a mathematical formula describing the probability of a particular cpu burst commonly  this distribution is exponential and is described by its mean similarly  we can describe the distribution of times when processes arrive in the system  the arrival-time distribution   fron1 these two distributions  it is possible to compute the average throughput  utilization  waiting time  and so on for most algorithms the computer system is described as a network of servers each server has a queue of waiting processes the cpu is a server with its ready queue  as is the i/0 system with its device queues knowing arrival rates and service rates  we can compute utilization  average queue length  average wait time  and so on this area of study is called queueing-network analysis as an example  let n be the average queue length  excluding the process being serviced   let w be the average waiting time in the queue  and let a be the average arrival rate for new processes in the queue  such as three processes per second   we expect that during the time w that a process waits  a x w new processes will arrive in the queue if the system is in a steady state  then the number of processes leaving the queue must be equal to the number of processes that arrive thus  n = ax w this equation  known as little 's formula  is particularly useful because it is valid for any scheduling algorithm and arrival distribution we can use little 's formula to compute one of the three variables if we know the other two for example  if we know that 7 processes arrive every second  on average   and that there are normally 14 processes in the queue  then we can compute the average waiting time per process as 2 seconds queueing analysis can be useful in comparing scheduling algorithms  but it also has limitations at the moment  the classes of algorithms and distributions that can be handled are fairly limited the mathematics of complicated algorithms and distributions can be difficult to work with thus  arrival and service distributions are often defined in mathematically tractable -but unrealistic-ways it is also generally necessary to make a number of independent assumptions  which may not be accurate as a result of these difficulties  queueing models are often only approximations of real systems  and the accuracy of the computed results may be questionable 216 chapter 5 performance statistics for fcfs performance statistics for sjf performance statistics for rr  q = 14  figure 5.17 evaluation of cpu schedulers by simulation 5.7.3 simulations to get a more accurate evaluation of scheduling algorithms  we can use simulations rumung simulations involves programming a model of the computer system software data structures represent the major components of the system the simulator has a variable representing a clock ; as this variable 's value is increased  the simulator modifies the system state to reflect the activities of the devices  the processes  and the scheduler as the simulation executes  statistics that indicate algorithm performance are gathered and printed the data to drive the simulation can be generated in several ways the most common method uses a random-number generator that is programmed to generate processes  cpu burst times  arrivals  departures  and so on  according to probability distributions the distributions can be defined mathematically  uniform  exponential  poisson  or empirically if a distribution is to be defined empirically  measurements of the actual system under study are taken the results define the distribution of events in the real system ; this distribution can then be used to drive the simulation a distribution-driven simulation may be inaccurate  however  because of relationships between successive events in the real system the frequency distribution indicates only how many instances of each event occur ; it does not indicate anything about the order of their occurrence to correct this problem  we can use trace tapes we create a trace tape by monitoring the real system and recording the sequence of actual events  figure 5.17   we then use this sequence to drive the simulation trace tapes provide an excellent way to compare two algorithms on exactly the same set of real inputs this method can produce accurate results for its inputs simulations can be expensive  often requiring hours of computer time a more detailed simulation provides more accurate results  but it also takes more computer time in addition  trace tapes can require large amounts of storage 5.8 5.8 217 space finally  the design  coding  and debugging of the simulator can be a major task 5.7.4 implementation even a simulation is of limited accuracy the only con'lpletely accurate way to evaluate a scheduling algorithm is to code it up  put it in the operating system  and see how it works this approach puts the actual algorithm in the real system for evaluation under real operating conditions the major difficulty with this approach is the high cost the expense is incurred not only in coding the algorithm and modifying the operating system to support it  along with its required data structures  but also in the reaction of the users to a constantly changing operating system most users are not interested in building a better operating system ; they merely want to get their processes executed and use their results a constantly changing operating system does not help the users to get their work done another difficulty is that the environment in which the algorithm is used will change the environment will change not only in the usual way  as new programs are written and the types of problems change  but also as a result of the performance of the scheduler if short processes are given priority  then users may break larger processes into sets of smaller processes if interactive processes are given priority over noninteractive processes  then users may switch to interactive use for example  researchers designed one system that classified interactive and noninteractive processes automatically by looking at the amount of terminal i/0 if a process did not input or output to the terminal in a 1-second interval  the process was classified as noninteractive and was moved to a lower-priority queue in response to this policy  one programmer modified his programs to write an arbitrary character to the terminal at regular intervals of less than 1 second the system gave his programs a high priority  even though the terminal output was completely meaningless the most flexible scheduling algorithms are those that can be altered by the system managers or by the users so that they can be tuned for a specific application or set of applications a workstation that performs high-end graphical applications  for instance  may have scheduling needs different from those of a web server or file server some operating systemsparticularly several versions of unix-allow the system manager to fine-tune the scheduling parameters for a particular system configuration for example  solaris provides the dispadmin command to allow the system administrator to modify the parameters of the scheduling classes described  in section 5.6.1 another approach is to use apis that modify the priority of a process or thread the java  /posix  and /winapi/ provide such functions the downfall of this approach is that performance-tuning a system or application most often does not result in improved performance in more general situations cpu scheduling is the task of selecting a waiting process from the ready queue and allocating the cpu to it the cpu is allocated to the selected process by the dispatcher 218 chapter 5 first-come  first-served  fcfs  scheduling is the simplest scheduling algorithm  but it can cause short processes to wait for very long processes shortestjob first  sjf  scheduling is provably optimal  providing the shortest average waiting time implementing sjf scheduling is difficult  howeve1 ~ because predicting the length of the next cpu burst is difficult the sjf algorithm is a special case of the general priority scheduling algorithm  which simply allocates the cpu to the highest-priority process both priority and sjf scheduling may suffer from starvation aging is a technique to prevent starvation round-robin  rr  scheduling is more appropriate for a time-shared  interactive  system rr scheduling allocates the cpu to the first process in the ready queue for q time units  where q is the time quantum after q time units  if the process has not relinquished the cpu  it is preem.pted  and the process is put at the tail of the ready queue the major problem is the selection of the time quantum if the quantum is too large  rr scheduling degenerates to fcfs scheduling ; if the quantum is too small  scheduling overhead in the form of context-switch time becomes excessive the fcfs algorithm is nonpreemptive ; the rr algorithm is preemptive the sjf and priority algorithms may be either preemptive or nonpreemptive multilevel queue algorithms allow different algorithms to be used for different classes of processes the most common model includes a foreground interactive queue that uses rr scheduling and a background batch queue that uses fcfs scheduling multilevel feedback queues allow processes to move from one queue to another many contemporary computer systems support multiple processors and allow each processor to schedule itself independently typically  each processor maintains its own private queue of processes  or threads   all of which are available to run additional issues related to multiprocessor scheduling include processor affinity  load balancing  and multicore processing as well as scheduling on virtualization systems operating systems supporting threads at the kernel level must schedule threads-not processes-for execution this is the case with solaris and windows xp both of these systems schedule threads using preemptive  priority-based scheduling algorithms  including support for real-time threads the linux process scheduler uses a priority-based algorithm with real-time support as well the scheduling algorithms for these three operating systems typically favor interactive over batch and cpu-bound processes the wide variety of scheduling algorithms demands that we have methods to select among algorithms analytic methods use mathematical analysis to determine the performance of an algorithm simulation methods determine performance by imitating the scheduling algorithm on a representative sample of processes and computing the resulting performance however  simulation can at best provide an approximation of actual system performance ; the only reliable technique for evaluating a scheduling algorithm is to implencent the algorithm on an actual system and monitor its performance in a real-world environment 5.1 why is it important for the scheduler to distinguish t /0-bound programs from cpu-bound programs 219 5.2 a cpu-scheduling algorithm determines an order for the execution of its scheduled processes given n processes to be scheduled on one processor  how many different schedules are possible give a formula in tentls of n 5.3 consider a systenc running ten i/o-bound tasks and one cpu-bound task assume that the i/o-bound tasks issue an i/o operation once for every millisecond of cpu computing and that each i/0 operation takes 10 milliseconds to complete also assume that the context-switching overhead is 0.1 millisecond and that all processes are long-running tasks describe the cpu utilization for a round-robin scheduler when  a the time quantum is 1 millisecond b the time quantum is 10 milliseconds 5.4 what advantage is there in having different time-quantum sizes at different levels of a multilevel queueing system 5.5 consider a system implementing multilevel queue scheduling what strategy can a computer user employ to maximize the amount of cpu time allocated to the user 's process 5.6 consider the scheduling algorithm in the solaris operating system for time-sharing threads a what is the time quantum  in milliseconds  for a thread with priority 10 with priority 55 b assume that a thread with priority 35 has used its entire time quantum without blocking what new priority will the scheduler assign this thread c assume that a thread with priority 35 blocks for i/0 before its time quantum has expired what new priority will the scheduler assign this thread 5.7 explain the differences in how much the following scheduling algorithms discriminate in favor of short processes  a fcfs b rr c multilevel feedback queues 5.8 consider the exponential average formula used to predict the length of the next cpu burst what are the implications of assigning the following values to the parameters used by the algorithm a ex = 0 and to = 100 milliseconds b ex = 0.99 and to = 10 milliseconds 220 chapter 5 5.9 which of the following scheduling algorithms could result in starvation a first-come  first-served b shortest job first c round robin d priority 5.10 suppose that a scheduling algorithm  at the level of short-term cpu scheduling  favors those processes that have used the least processor time in the recent past why will this algorithm favor i/o-bound programs and yet not permanently starve cpu-bound programs 5.11 using the windows xp scheduling algorithm  determine the numeric priority of each of the following threads a a thread in the realtimeyriority _class with a relative priority of highest b a thread in the normalyriority_class with a relative priority of normal c a thread in the highyriority _class with a relative priority of above..normal 5.12 consider a variant of the rr scheduling algorithm in which the entries in the ready queue are pointers to the pcbs a what would be the effect of putting two pointers to the same process in the ready queue b what would be two major advantages and two disadvantages of this scheme c how would you modify the basic rr algorithm to achieve the same effect without the duplicate pointers 5.13 consider the following set of processes  with the length of the cpu burst given in milliseconds  process burst time priority  pt 10 3 p2 1 1 p3 2 3 p4 1 4 ps 5 2 221 the processes are assumed to have arrived in the order p1  p2  p3  p4  ps  all at time 0 a draw four gantt charts that illustrate the execution of these processes using the following scheduling algorithms  fcfs  sjf  nonpreemptive priority  a smaller priority number implies a higher priority   and rr  quantum = 1   b what is the turnaround time of each process for each of the scheduling algorithms in part a c what is the waiting ti1r1e of each process for each of these scheduling algorithms d which of the algorithms results in the minimum average waiting time  over all processes  5.14 the traditional unix scheduler enforces an inverse relationship between priority numbers and priorities  the higher the numbe1 ~ the lower the priority the scheduler recalculates process priorities once per second using the following function  priority =  recent cpu usage i 2  + base where base = 60 and recent cpu usage refers to a value indicating how often a process has used the cpu since priorities were last recalculated assume that recent cpu usage for process p1 is 40  for process p2 is 18  and for process p3 is 10 what will be the new priorities for these three processes when priorities are recalculated based on this information  does the traditional unix scheduler raise or lower the relative priority of a cpu-bound process 5.15 discuss how the following pairs of scheduling criteria conflict in certain settings a cpu utilization and response time b average turnaround time and maximum waiting time c i/0 device utilization and cpu utilization 5.16 consider a preemptive priority scheduling algorithm based on dynamically changing priorities larger priority numbers imply higher priority when a process is waiting for the cpu  in the ready queue  but not running   its priority changes at a rate a ; when it is running  its priority changes at a rate ~  all processes are given a priority of 0 when they enter the ready queue the parameters a and ~ can be set to give many different scheduling algorithms a what is the algorithm that results from ~ a 0 b what is the algorithm that results from a ~ 0 5.17 suppose that the following processes arrive for execution at the times indicated each process will run for the amount of time listed in answering the questions  use nonpreemptive scheduling  and base all 222 chapter 5 decisions on the information you have at the time the decision must be made process arrival time burst time  pl 0.0 8 p2 0.4 4 p3 1.0 1 a what is the average turnaround time for these processes with the fcfs scheduling algorithm b what is the average turnaround time for these processes with the sjf scheduling algorithm c the sjf algorithm is supposed to improve performance  but notice that we chose to run process p1 at time 0 because we did not k11ow that two shorter processes would arrive soon compute what the average turnaround time will be if the cpu is left idle for the first 1 unit and then sjf scheduling is used remember that processes p1 and p2 are waiting durirtg this idle time  so their waiting time may increase this algorithm could be known as future-knowledge scheduling feedback queues were originally implemented on the ctss system described in corbato et al  1962   this feedback queue scheduling system was analyzed by schrage  1967   the preemptive priority scheduling algorithm of exercise 5.16 was suggested by kleinrock  1975   anderson et al  1989   lewis and berg  1998   and philbin et al  1996  discuss thread scheduling multicore scheduling is examined in mcnairy and bhatia  2005  and kongetira et al  2005   scheduling techniques that take into account information regarding process execution times from previous runs are described in fisher  1981   hall et al  1996   and lowney et al  1993   fair-share schedulers are covered by henry  1984   woodside  1986   and kay and la uder  1988   scheduling policies used in the unix v operating system are described by bach  1987  ; those for unix freebsd 5.2 are presented by mckusick and neville-neil  2005  ; and those for the mach operating system are discussed by black  1990   love  2005  covers scheduling in lim.ix details of the ule scheduler can be found in roberson  2003   solaris scheduling is described by mauro and mcdougall  2007   solomon  1998   solomon and russinovich  2000   and russinovich and solomon  2005  discuss scheduling in windows internals butenhof  1997  and lewis and berg  1998  describe scheduling in pthreads systems siddha et al  2007  discuss scheduling challenges on multicore systems part three 6.1 c er a cooperating process is one that can affect or be affected by other processes executing in the system cooperating processes can either directly share a logical address space  that is  both code and data  or be allowed to share data only through files or messages the former case is achieved through the use of threads  discussed in chapter 4 concurrent access to shared data may result in data inconsistency  however in this chapter  we discuss various mechanisms to ensure the orderly execution of cooperating processes that share a logical address space  so that data consistency is maintained to introduce the critical-section problem  whose solutions can be used to ensure the consistency of shared data to present both software and hardware solutions of the critical-section problem to introduce the concept of an atomic transaction and describe mechanisms to ensure atomicity in chapter 3  we developed a model of a system consisting of cooperating sequential processes or threads  all running asynchronously and possibly sharing data we illustrated this model with the producer-consumer problem  which is representative of operating systems specifically  in section 3.4.1  we described how a bounded buffer could be used to enable processes to share memory let 's return to our consideration of the bounded buffer as we pointed out  our original solution allowed at most buffer_size  1 items in the buffer at the same time suppose we want to modify the algorithm to remedy this deficiency one possibility is to add an integer variable counter  initialized to 0 counter is incremented every time we add a new item to the buffer and is 225 226 chapter 6 decremented every time we remove one item from the buffer the code for the producer process can be modified as follows  while  true    i produce an item in nextproduced i while  counter = = buffer_size  ; i do nothing i buffer  in  = nextproduced ; in =  in + 1  % buffer_size ; counter + + ; the code for the consumer process can be modified as follows  while  true    while  counter = = 0  ; i do nothing i nextconsumed = buffer  out  ; out =  out + 1  % buffer_size ; counter ; i consume the item in nextconsumed i although both the producer and consumer routines shown above are correct separately  they may not function correctly when executed concurrently as an illustration  suppose that the value of the variable counter is currently 5 and that the producer and consumer processes execute the statements counter + + and counter concurrently following the execution of these two statements  the value of the variable counter may be 4  5  or 6 ! the only correct result  though  is counter = = 5  which is generated correctly if the producer and consumer execute separately we can show that the value of counter may be incorrect as follows note that the statement counter + + may be implemented in machine language  on a typical machine  as register1 = counter register1 = register1 + 1 counter = register1 where register1 is one of the local cpu registers similarly  the statement register2 counter is implemented as follows  register2 = counter register2 = register2 ~ 1 counter = register2 where again register2 is on eo the local cpu registers even though register1 and register2 may be the same physical register  an accumulator  say   remember that the contents of this register will be saved and restored by the interrupt handler  section 1.2.3   6.2 6.2 227 the concurrent execution of counter + + and counter is equivalent to a sequential execution in which the lower-level statements presented previously are interleaved in some arbitrary order  but the order within each high-level statement is preserved   one such interleaving is to  producer execute register1 = counter  register1 = 5  t1  producer execute register1 = register1 + 1  register1 = 6  t2  consumer execute register2 = counter  register2 = 5  t3  consumer execute register2 = register2 1  register2 = 4  t4  producer execute counter = register1  counter = 6  ts  consumer execute counter = register2  counter = 4  notice that we have arrived at the incorrect state counter = = 4  indicating that four buffers are full  when  in fact  five buffers are full if we reversed the order of the statements at t4 and t5  we would arrive at the incorrect state counter = = 6  we would arrive at this incorrect state because we allowed both processes to manipulate the variable counter concurrently a situation like this  where several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place  is called a to guard against the race condition above  we need to ensure that only one process at a time can be manipulating the variable counter to make such a guarantee  we require that the processes be synchronized in some way situations such as the one just described occur frequently in operating systems as different parts of the system manipulate resources furthermore  with the growth of multicore systems  there is an increased emphasis on developing multithreaded applications wherein several threads-which are quite possibly sharing data-are rmming in parallel on different processing cores clearly  we want any changes that result from such activities not to interfere with one another because of the importance of this issue  a major portion of this chapter is concerned with and amongst cooperating processes consider a system consisting of n processes  po  p1    p11 _ i   each process has a segment of code  called a cdticall in which the process may be changing common variables  updating a table  writing a file  and so on the important feature of the system is that  when one process is executing in its critical section  no other process is to be allowed to execute in its critical section that is  no two processes are executing in their critical sections at the same time the critical-section problem is to design a protocol that the processes can use to cooperate each process must request permission to enter its critical section the section of code implementing this request is the the critical section may be followed by an exit the remaining code is the the general structure of a typical process pi is shown in 228 chapter 6 do  i entry section i critical section i exit section i remainder section  while  true  ; figure 6.1 general structure of a typical process a figure 6.1 the entry section and exit section are enclosed in boxes to highlight these important segments of code a solution to the critical-section problem must satisfy the following three requirements  1 mutual exclusion if process pi is executing in its critical section  then no other processes can be executing in their critical sections 2 progress if no process is executing in its critical section and some processes wish to enter their critical sections  then only those processes that are not executing in their remainder sections can participate in deciding which will enter its critical section next  and this selection carmot be postponed indefinitely bounded waiting there exists a bound  or limit  on the number of times that other processes are allowed to enter their critical sections after a process has made a request to enter its critical section and before that request is granted we assume that each process is executing at a nonzero speed however  we can make no assumption concerning the relative of the n processes at a given point in time  many kernel-mode processes may be active in the operating system as a result  the code implementing an operating system  kernel code  is subject to several possible race conditions consider as an example a kernel data structure that maintains a list of all open files in the system this list must be modified when a new file is opened or closed  adding the file to the list or removing it from the list   if two processes were to open files simultaneously  the separate updates to this list could result in a race condition other kernel data structures that are prone to possible race conditions include structures for maintaining memory allocation  for maintaining process lists  and for interrupt handling it is up to kernel developers to ensure that the operating system is free from such race conditions two general approaches are used to handle critical sections in operating systems   1  preemptive kernels and  2  nonpreemptive kernels a preemptive kernel allows a process to be preempted while it is running in kernel mode a nonpreemptive kernel does not allow a process running in kernel mode 6.3 6.3 229 to be preempted ; a kernel-mode process will run until it exits kernel mode  blocks  or voluntarily yields control of the cpu obviously  a nonpreemptive kernel is essentially free from race conditions on kernel data structures  as only one process is active in the kernel at a time we can not say the same about preemptive kernels  so they must be carefully designed to ensure that shared kernel data are free from race conditions preemptive kernels are especially difficult to design for smp architectures  since in these environments it is possible for two kernel-mode processes to run simultaneously on different processors why  then  would anyone favor a preemptive kernel over a nonpreemptive one a preemptive kernel is more suitable for real-time programming  as it will allow a real-time process to preempt a process currently running in the kernel furthermore  a preemptive kernel may be more responsive  since there is less risk that a kernel-mode process will run for an arbitrarily long period before relinquishing the processor to waiting processes of course  this effect can be minimized by designing kernel code that does not behave in this way later in this chapter  we explore how various operating systems manage preemption within the kernel next  we illustrate a classic software-based solution to the critical-section problem known as peterson 's solution because of the way modern computer architectures perform basic machine-language instructions  such as load and store  there are no guarantees that peterson 's solution will work correctly on such architectures howeve1 ~ we present the solution because it provides a good algorithmic description of solving the critical-section problem and illustrates some of the complexities involved in designing software that addresses the requirements of mutual exclusion  progress  and bomcded waiting peterson 's solution is restricted to two processes that alternate execution between their critical sections and remainder sections the processes are numbered po and p1 for convenience  when presenting pi  we use pj to denote the other process ; that is  j equals 1  i peterson 's solution requires the two processes to share two data items  int turn ; boolean flag  2  ; the variable turn indicates whose turn it is to enter its critical section that is  if turn = = i  then process pi is allowed to execute in its critical section the flag array is used to indicate if a process is ready to enter its critical section for example  if flag  i  is true  this value indicates that pi is ready to enter its critical section with an explanation of these data structures complete  we are now ready to describe the algorithm shown in figure 6.2 to enter the critical section  process pi first sets flag  i  to be true and then sets turn to the value j  thereby asserting that if the other process wishes to enter the critical section  it can do so if both processes try to enter at the same time  turn will be set to both i and j at roughly the sance time only one of these assignments will last ; the other will occur but will be overwritten immediately 230 chapter 6 do  flag  i  = true ; turn = j ; while  flag  j  && turn j  ; critical section i flag  i  = false ; i remainder section  while  true  ; figure 6.2 the structure of process a in peterson 's solution the eventual value of turn determines which of the two processes is allowed to enter its critical section first we now prove that this solution is correct we need to show that  mutual exclusion is preserved the progress requirement is satisfied the bounded-waiting requirement is met to prove property 1  we note that each p ; enters its critical section only if either flag  j  = = false or turn = = i also note that  if both processes can be executing in their critical sections at the same time  then flag  0  = = flag  1  = = true these two observations imply that po and p1 could not have successfully executed their while statements at about the same time  since the value of turn can be either 0 or 1 but camwt be both hence  one of the processes -say  pi -must have successfully executed the while statencent  whereas p ; had to execute at least one additional statement  turn = = j   however  at that time  flag  j  = = true and turn = = j  and this condition will persist as long as pi is in its critical section ; as a result  mutual exclusion is preserved to prove properties 2 and 3  we note that a process p ; can be prevented from entering the critical section only if it is stuck in the while loop with the condition flag  j  = = true and turn = = = j ; this loop is the only one possible if pi is not ready to enter the critical section  then flag  j  = = false  and p ; can enter its critical section if pj has set flag  j  to true and is also executing in its while statement  then either turn = = = i or turn = = = j if turn = = i  then p ; will enter the critical section if turn = = j  then pi will enter the critical section however  once pi exits its critical section  it will reset flag  j  to false  allowing p ; to enter its critical section if pi resets flag  j  to true  it must also set turn to i thus  since p ; does not change the value of the variable turn while executing the while statement  p ; will enter the critical section  progress  after at most one entry by p1  bounded waiting   6.4 6.4 231 do  acquire lock critical section i release lock i remainder section  while  true  ; figure 6.3 solution to the critical-section problem using locks we have just described one software-based solution to the critical-section problem however  as mentioned  software-based solutions such as peterson 's are not guaranteed to work on modern computer architectures instead  we can generally state that any solution to the critical-section problem requires a simple tool-a lock race conditions are prevented by requiring that critical regions be protected by locks that is  a process must acquire a lock before entering a critical section ; it releases the lock when it exits the critical section this is illustrated in figure 6.3 in the following discussions  we explore several more solutions to the critical-section problem using techniques ranging from hardware to softwarebased apis available to application programmers all these solutions are based on the premise of locking ; however  as we shall see  the designs of such locks can be quite sophisticated we start by presenting some simple hardware instructions that are available on many systems and showing how they can be used effectively in solving the critical-section problem hardware features can make any programming task easier and improve system efficiency the critical-section problem could be solved simply in a uniprocessor environment if we could prevent interrupts from occurring while a shared variable was being modified in this manner  we could be sure that the current sequence of instructions would be allowed to execute in order without preemption no other instructions would be run  so no unexpected modifications could be made to the shared variable this is often the approach taken by nonpreemptive kernels unfortunately  this solution is not as feasible in a multiprocessor environment disabling interrupts on a multiprocessor can be time consuming  as the boolean testandset  boolean target   boolean rv = target ; target = true ; return rv ;  figure 6.4 the definition of the testandset   instruction 232 chapter 6 do  while  testandset  &lock   ; ii do nothing ii critical section lock = false ; ii remainder section  while  true  ; figure 6.5 mutual-exclusion implementation with testandset    message is passed to all the processors this message passing delays entry into each critical section  and system efficiency decreases also consider the effect on a system 's clock if the clock is kept updated by interrupts many modern computer systems therefore provide special hardware instructions that allow us either to test and modify the content of a word or to swap the contents of two words is  as one unin.terruptible unit we can use these special instructions to solve the critical-section problem in a relatively simple manner rather than discussing one specific instruction for one specific machine  we abstract the main concepts behind these types of instructions by describing the testandset   and swap   instructions the testandset   instruction can be defined as shown in figure 6.4 the important characteristic of this instruction is that it is executed atomically thus  if two testandset   instructions are executed simultaneously  each on a different cpu   they will be executed sequentially in some arbitrary order if the machine supports the testandset   instruction  then we can implement mutual exclusion by declaring a boolean variable lock  initialized to false the structure of process p ; is shown in figure 6.5 the swap   instruction  in contrast to the testandset   instruction  operates on the contents of two words ; it is defined as shown in figure 6.6 like the testandset   instruction  it is executed atomically if the machine supports the swap   instruction  then mutual exclusion can be provided as follows a global boolean variable lock is declared and is initialized to false in addition  each process has a local boolean variable key the structure of process p ; is shown in figure 6.7 although these algorithms satisfy the mutual-exclusion requirement  they do not satisfy the bounded-waiting requirement in figure 6.8  we present another algorithm using the testandset   instruction that satisfies all the critical-section requirements the common data structures are void swap  boolean a  boolean b   boolean temp = a ; a b ; b = temp ;  figure 6.6 the definition of the swap   instruction 6.4 do  key = true ; while  key = = true  swap  &lock  &key  ; ii critical section lock = false ; ii remainder section  while  true  ; figure 6.7 mutual-exclusion implementation with the swap   instruction boolean waiting  n  ; boolean lock ; 233 these data structures are initialized to false to prove that the mutualexclusion requirement is met  we note that process p ; can enter its critical section only if either waiting  i  = = false or key = = false the value of key can become false only if the testandset   is executed the first process to execute the testandset   will find key = = false ; all others must wait the variable waiting  i  can become false only if another process leaves its critical section ; only one waiting  i  is set to false  maintaining the mutual-exclusion requirement do  waiting  i  = true ; key = true ; while  waiting  i  && key  key = testandset  &lock  ; waiting  i  = false ; ii critical section j =  i + 1  % n ; while   j ! = i  && ! waiting  j   j =  j + 1  % n ; if  j = = i  lock = false ; else waiting  j  = false ; ii remainder section  while  true  ; figure 6.8 bounded-waiting mutual exclusion with testandset    234 chapter 6 6.5 to prove that the progress requirement is met  we note that the arguments presented for mutual exclusion also apply here  since a process exiting the critical section either sets lock to false or sets waiting  j  to false both allow a process that is waiting to enter its critical section to proceed to prove that the bounded-waiting requirement is met  we note that  when a process leaves its critical section  it scans the array waiting in the cyclic ordering  i + 1  i + 2    n 1  0    i 1   it designates the first process in this ordering that is in the entry section  waiting  j  = = true  as the next one to enter the critical section any process waiting to enter its critical section will thus do so within n  1 turns unfortunately for hardware designers  implementing atomic testandset   instructions on multiprocessors is not a trivial task such implementations are discussed in books on computer architecture the hardware-based solutions to the critical-section problem presented in section 6.4 are complicated for application programmers to use to overcmrte this difficulty  we can use a synchronization tool called a a semaphore s is an integer variable that  apart from initialization  is accessed only through two standard atomic operations  wait   and signal    the wait   operation was originally termed p  from the dutch proberen  to test  ; signal   was originally called v  from verhogen  to increment   the definition of wait   is as follows  wait  s    while s = 0 ii no-op s ' the definition of signal   is as follows  signal  s   s + + ;  all modifications to the integer value of the semaphore in the wait   and signal   operations must be executed indivisibly that is  when one process modifies the semaphore value  no other process can simultaneously modify that same semaphore value in addition  in the case of wait  s   the testing of the integer value of s  s  s 0   as well as its possible modification  s   must be executed without interruption we shall see how these operations can be implemented in section 6.5.2 ; first  let us see how semaphores can be used 6.5.1 usage operating systems often distinguish between counting and binary semaphores the value of a counting semaphore can range over an unrestricted domain the value of a binary semaphore can range only between 0 and 1 on some 6.5 235 systems  binary semaphores are lmown as mutex locks  as they are locks that provide mutual exclusion we can use binary semaphores to deal with the critical-section problem or mljltiple processes then processes share a semaphore  mutex  initialized to 1 each process pi is organized as shown in figure 6.9 counting semaphores can be used to control access to a given resource consisting of a finite number o instances the semaphore is initialized to the number of resources available each process that wishes to use a resource performs a wait   operation on the semaphore  thereby decrementing the count   when a process releases a resource  it performs a signal   operation  incrementing the count   when the count for the semaphore goes to 0  all resources are being used after that  processes that wish to use a resource will block until the count becomes greater than 0 we can also use semaphores to solve various synchronization problems for example  consider two concurrently numing processes  p1 with a statement 51 and p2 with a statement 52  suppose we require that 52 be executed only after 51 has completed we can implement this scheme readily by letting p1 and p2 share a common semaphore synch  initialized to 0  and by inserting the statements 51 ; signal  synch  ; in process p1 and the statements wait  synch  ; 52 ; in process p2 because synch is initialized to 0  p2 will execute 52 only after p1 has invoked signal  synch   which is after statement 51 has been executed 6.5.2 implementation the main disadvantage of the semaphore definition given here is thatit requires while a process is in its critical section  any other process that tries to enter its critical section must loop continuously in the entry code this continual looping is clearly a problem in a real multiprogramming system  do  wait  mutex  ; ii critical section signal  mutex  ; ii remainder section  while  true  ; figure 6.9 mutual-exclusion implementation with semaphores 236 chapter 6 where a single cpu is shared among ncany processes busy waiting wastes cpu cycles that some other process might be able to use productively this type of semaphore is also called a because the process spins while waiting for the lock  spinlocks do have an advantage in that no context switch is required when a process must wait on a lock  and a context switch may take considerable time thus  when locks are expected to be held for short times  spinlocks are useful ; they are often employed on multiprocessor systems where one thread can spin on one processor while another thread performs its critical section on another processor  to overcome the need for busy waiting  we can modify the definition of the wait   and signal   semaphore operations when a process executes the wait   operation and finds that the semaphore value is not positive  it must wait however  rather than engaging in busy waiting  the process can block itself the block operation places a process into a waiting queue associated with the semaphore  and the state of the process is switched to the waiting state then control is transferred to the cpu scheduler  which selects another process to execute a process that is blocked  waiting on a semaphore s  should be restarted when some other process executes a signal   operation the process is restarted by a wakeup   operation  which changes the process from the waiting state to the ready state the process is then placed in the ready queue  the cpu may or may not be switched from the running process to the newly ready process  depending on the cpu-scheduling algorithm  to implement semaphores under this definition  we define a semaphore as a c ' struct  typedef struct  int value ; struct process list ;  semaphore ; each semaphore has an integer value and a list of processes list when a process must wait on a semaphore  it is added to the list of processes a signal   operation removes one process from the list of waiting processes and awakens that process the wait   semaphore operation can now be defined as wait  semaphore s   s value ;  if  s value 0    add this process to s list ; block   ; the signal   semaphore operation can now be defined as signal  semaphore s   s value + + ; if  s value = 0   6.5 remove a process p fron s list ; wakeup  p  ;   237 the block   operation suspends the process that invokes it the wakeup  p  operation resumes the execution of a blocked process p these two operations are provided by the operating system as basic system calls note that in this implementation  semaphore values may be negative  although semaphore values are never negative under the classical definition of semaphores with busy waiting if a semaphore value is negative  its magnitude is the number of processes waiting on that semaphore this fact results from switching the order of the decrement and the test in the implementation of the wait   operation the list of waiting processes can be easily implemented by a link field in each process control block  pcb   each semaphore contains an integer value and a pointer to a list of pcbs one way to add and rernove processes from the list so as to ensure bounded waiting is to use a fifo queue  where the semaphore contains both head and tail pointers to the queue in general  howeve1 ~ the list can use any queueing strategy correct usage of semaphores does not depend on a particular queueing strategy for the semaphore lists it is critical that semaphores be executed atomically we must guarantee that no two processes can execute wait   and signal   operations on the same semaphore at the same time this is a critical-section problem ; and in a single-processor environment  that is  where only one cpu exists   we can solve it by simply inhibiting interrupts during the time the wait   and signal   operations are executing this scheme works in a single-processor environment because  once interrupts are inhibited  instructions from different processes can not be interleaved only the currently running process executes until interrupts are reenabled and the scheduler can regain control in a multiprocessor environment  interrupts must be disabled on every processor ; otherwise  instructions from different processes  running on different processors  may be interleaved in some arbitrary way disabling interrupts on every processor can be a difficult task and furthermore can seriously diminish performance therefore  smp systems must provide alternative locking techniques-such as spinlocks-to ensure that wait   and signal   are performed atomically it is important to admit that we have not completely eliminated busy waiting with this definition of the wait   and signal   operations rather  we have moved busy waiting from the entry section to the critical sections of application programs furthermore  we have limited busy waiting to the critical sections of the wait   and signal   opera times  and these sections are short  if properly coded  they sbould be no more than about ten instructions   thus  the critical section is almost never occupied  and busy waiting occurs rarely  and then for only a short time an entirely different situation exists with application programs whose critical sections may be long  minutes or 238 chapter 6 even hours  or may almost always be occupied in such casesf busy waiting is extremely inefficient 6.5.3 deadlocks and starvation the implementation of a semaphore with a waiting queue may result in a situation where two or more processes are waiting indefinitely for an event that can be caused only by one of the waiting processes the event in question is the execution of a signal   when such a state is reached  these processes are said to be to illustrate this  we consider a system consisting of two processes  po and p1  each accessing two semaphores  s and q  set to the value 1  po wait  s  ; wait  q  ; signal  s  ; signal  q  ; pl wait  q  ; wait  s  ; signal  q  ; signal  s  ; suppose that po executes wait  s  and then p1 executes wait  q   when po executes wait  q   it must wait until p1 executes signal  q   similarly  when p1 executes wait  s   it must wait until po executes signal  s   since these signal   operations cam1ot be executed  po and p1 are deadlocked we say that a set of processes is in a deadlock state when every process in the set is waiting for an event that can be caused only by another process in the set the events with which we are mainly concerned here are resource acquisition and release however  other types of events may result in deadlocks  as we show in chapter 7 in that chapter  we describe various mechanisms for dealing with the deadlock problem another problem related to deadlocks is or a situation in which processes wait indefinitely within the semaphore indefinite blocking may occur if we remove processes from the list associated with a semaphore in lifo  last-in  first-out  order 6.5.4 priority inversion a scheduling challenge arises when a higher-priority process needs to read or modify kernel data that are currently being accessed by a lower-priority process-or a chain of lower-priority processes since kernel data are typically protected with a lock  the higher-priority process will have to wait for a lower-priority one to finish with the resource the situation becomes more complicated if the lower-priority process is preempted in favor of another process with a higher priority as an example  assume we have three processes  lf m  and h  whose priorities follow the order l m h assume that process h requires resource r  which is currently being accessed by process l ordinarily  process h would wait for l to finish using resource r however  now suppose that process m becomes runnable  thereby preempting process 6.6 6.6 239 priority inversion and the mars pathfinder priority inversion can be more than a scheduling inconvenience on systems with tight time constraints  such as real-time systems-see chapter 19   priority inversion can cause a process to take longer than it should to accomplish a task when that happens  other failures can cascade  resulting in system failure consider the mars pathfinde1 ~ a nasa space probe that landed a robot  the sojourner rove1 ~ on mars in 1997 to conduct experiments shortly after the sojourner began operating  it started to experience frequent computer resets each reset reinitialized all hardware and software  including communications if the problem had not been solved  the sojourner would have failed in its mission the problem was caused by the fact that one high-priority task  bcdist  was taking longer than expected to complete its work this task was being forced to wait for a shared resource that was held by the lower-priority asi/met task  which in turn was preempted by multiple medium-priority tasks the bcdist task would stall waiting for the shared resource  and ultimately the bc_sched task would discover the problem and perform the reset the sojourner was suffering from a typical case of priority inversion the operating system on the sojourner was vxworks  see section 19.6   which had a global variable to enable priority inheritance on all semaphores after testing  the variable was set on the sojourner  on mars !   and the problem was solved a full description of the problem  its detection  and its solution was written by the software team lead and is available at research.microsoft.com/ mbj /marsyathfinder i authoritative_account.html l indirectly  a process with a lower priority-process m-has affected how long process h must wait for l to relinquish resource r this problem is known as it occurs only in systems with more than two priorities  so one solution is to have only two priorities that is insufficient for most general-purpose operating systems  however typically these systems solve the problem by implementing a 2tic x,u   according to this protocol  all processes that are accessing resources needed by a higher-priority process inherit the higher priority until they are finished with the resources in question when they are finished  their priorities revert to their original values in the exan1.ple above  a priority-inheritance protocol would allow process l to temporarily inherit the priority of process h  thereby preventing process m from preempting its execution when process l had finished using resource r  it would relinquish its inherited priority from hand assume its original priority because resource r would now be available  process h-not m-would run next in this section  we present a number of synchronization problems as examples of a large class of concurrency-control problems these problems are used for 240 chapter 6 do  ii produce an item in nextp wait  empty  ; wait  mutex  ; ii add nextp to buffer signal  mutex  ; signal  full  ;  while  true  ; figure 6.10 the structure of the producer process testing nearly every newly proposed synchronization scheme in our solutions to the problems  we use semaphores for synchronization 6.6.1 the bounded-buffer problem the bounded-buffer problem was introduced in section 6.1 ; it is commonly used to illustrate the power of synchronization primitives here  we present a general structure of this scheme without committing ourselves to any particular implementation ; we provide a related programming project in the exercises at the end of the chapter we assume that the pool consists of n buffers  each capable of holding one item the mutex semaphore provides mutual exclusion for accesses to the buffer pool and is initialized to the value 1 the empty and full semaphores comct the number of empty and full buffers the semaphore empty is initialized to the value n ; the semaphore full is initialized to the value 0 the code for the producer process is shown in figure 6.10 ; the code for the consumer process is shown in figure 6.11 note the symmetry between the producer and the consumer we can interpret this code as the producer producing full buffers for the consumer or as the consumer producing empty buffers for the producer do  wait  full  ; wait  mutex  ; ii remove an item from buffer to nextc signal  mutex  ; signal  empty  ; ii consume the item in nextc  while  true  ; figure 6.11 the structure of the consumer process 6.6 241 6.6.2 the readers-writers problem suppose that a database is to be shared among several concurrent processes some of these processes may want only to read the database  whereas others may want to update  that is  to read and write  the database we distinguish between these two types of processes by referring to the former as readers and to the latter as writers obviously  if two readers access the shared data simultaneously  no adverse effects will result however  if a writer and some other process  either a reader or a writer  access the database simultaneously  chaos may ensue to ensure that these difficulties do not arise  we require that the writers have exclusive access to the shared database while writing to the database this synchronization problem is referred to as the readers-writers problem since it was originally stated  it has been used to test nearly every new synchronization primitive the readers-writers problem has several variations  all involving priorities the simplest one  referred to as the first readers-writers problem  requires that no reader be kept waiting unless a writer has already obtained permission to use the shared object in other words  no reader should wait for other readers to finish simply because a writer is waiting the second readerswriters problem requires that  once a writer is ready  that writer performs its write as soon as possible in other words  if a writer is waiting to access the object  no new readers may start reading a solution to either problem may result in starvation in the first case  writers may starve ; in the second case  readers may starve for this reason  other variants of the problem have been proposed next  we present a solution to the first readers-writers problem refer to the bibliographical notes at the end of the chapter for references describing starvation-free solutions to the second readers-writers problem in the solution to the first readers-writers problem  the reader processes share the following data structures  semaphore mutex  wrt ; int readcount ; the semaphores mutex and wrt are initialized to 1 ; readcount is initialized to 0 the semaphore wrt is common to both reader and writer processes the mutex semaphore is used to ensure mutual exclusion when the variable readcount is updated the readcount variable keeps track of how many processes are currently reading the object the semaphore wrt functions as a mutual-exclusion semaphore for the writers it is also used by the first or last reader that enters or exits the critical section it is not used by readers who enter or exit while other readers are in their critical sections the code for a writer process is shown in figure 6.12 ; the code for a reader process is shown in figure 6.13 note that  if a writer is in the critical section and n readers are waiting  then one reader is queued on wrt  and n 1 readers are queued on mutex also observe that  when a writer executes signal  wrt   we may resume the execution of either the waiting readers or a single waiting writer the selection is made by the scheduler the readers-writers problem and its solutions have been generalized to provide locks on some systems acquiring a reader-writer lock 242 chapter 6 do  wait  wrt  ; ii writing is performed signal  wrt  ;  while  true  ; figure 6 i 2 the structure of a writer process requires specifying the mode of the lock either read or write access when a process wishes only to read shared data  it requests the reader-writer lock in read mode ; a process wishing to modify the shared data must request the lock in write mode multiple processes are permitted to concurrently acquire a reader-writer lock in read mode  but only one process may acquire the lock for writing  as exclusive access is required for writers reader-writer locks are most useful in the following situations  in applications where it is easy to identify which processes only read shared data and which processes only write shared data in applications that have more readers than writers this is because readerwriter locks generally require more overhead to establish than semaphores or mutual-exclusion locks the increased concurrency of allowing multiple readers compensates for the overhead involved in setting up the readerwriter lock 6.6.3 the dining-philosophers problem consider five philosophers who spend their lives thinking and eating the philosophers share a circular table surrounded by five chairs  each belonging do  wait  mutex  ; readcount + + ; if  readcount 1  wait  wrt  ; signal  mutex  ; ii reading is performed wait  mutex  ; readcount ; if  readcount 0  signal  wrt  ; signal  mutex  ;  while  true  ; figure 6.13 the structure of a reader process 6.6 243 figure 6.14 the situation of the dining philosophers to one philosopher in the center of the table is a bowl of rice  and the table is laid with five single chopsticks  figure 6.14   when a philosopher thinks  she does not interact with her colleagues from time to time  a philosopher gets hungry and tries to pick up the two chopsticks that are closest to her  the chopsticks that are between her and her left and right neighbors   a philosopher may pick up only one chopstick at a time obviously  she cam1ot pick up a chopstick that is already in the hand of a neighbor when a htmgry philosopher has both her chopsticks at the same time  she eats without releasing her chopsticks when she is finished eating  she puts down both of her chopsticks and starts thinking again the dining-philosophers problem is considered a classic synchronization problem neither because of its practical importance nor because computer scientists dislike philosophers but because it is an example of a large class of concurrency-control problems it is a simple representation of the need to allocate several resources among several processes in a deadlock-free and starvation-free mam1er one simple solution is to represent each chopstick with a semaphore a philosopher tries to grab a chopstick by executing await   operation on that semaphore ; she releases her chopsticks by executing the signal   operation on the appropriate semaphores thus  the shared data are semaphore chopstick  5  ; where all the elements of chopstick are initialized to 1 the structure of philosopher i is shown in figure 6.15 although this solution guarantees that no two neighbors are eating simultaneously  it nevertheless must be rejected because it could create a deadlock suppose that all five philosophers become hungry simultaneously and each grabs her left chopstick all the elements of chopstick will now be equal to 0 when each philosopher tries to grab her right chopstick  she will be delayed forever several possible remedies to the deadlock problem are listed next allow at most four philosophers to be sitting simultaneously at the table 244 chapter 6 6.7 do  wait  chopstick  i   ; wait  chopstick   i + l  % 5   ; i i eat signal  chopstick  i   ; signal  chopstick   i + l  % 5   ; ii think  while  true  ; figure 6.15 the structure of philosopher i allow a philosopher to pick up her chopsticks only if both chopsticks are available  to do this  she must pick them up in a critical section   use an asymmetric solution ; that is  an odd philosopher picks up first her left chopstick and then her right chopstick  whereas an even philosopher picks up her right chopstick and then her left chopstick in section 6.7  we present a solution to the dining-philosophers problem that ensures freedom from deadlocks note  however  that any satisfactory solution to the dining-philosophers problem must guard against the possibility that one of the philosophers will starve to death a deadlock-free solution does not necessarily eliminate the possibility of starvation although semaphores provide a convenient and effective mechanism for process synchronization  using them incorrectly can result in timing errors that are difficult to detect  since these errors happen only if some particular execution sequences take place and these sequences do not always occur we have seen an example of such errors in the use of counters in our solution to the producer-consumer problem  section 6.1   in that example  the timing problem happened only rarely  and even then the counter value appeared to be reasonable-off by only 1 nevertheless  the solution is obviously not an acceptable one it is for this reason that semaphores were introduced in the first place unfortunately  such timing errors can still occur when semaphores are used to illustrate how  we review the semaphore solution to the critical-section problem all processes share a semaphore variable mutex  which is initialized to 1 each process must execute wait  mutex  before entering the critical section and signal  mutex  afterward if this sequence is not observed  two processes may be in their critical sections simultaneously next  we examine the various difficulties that may result note that these difficulties will arise even if a single process is not well behaved this situation may be caused by an honest programming error or an uncooperative programmer 6.7 245 suppose that a process interchanges the order in which the wait   and signal   operations on the semaphore mutex are executed  resulting in the following execution  signal  mutex  ; critical section wait  mutex  ; in this situation  several processes may be executing in their critical sections simultaneously  violating the mutual-exclusion requirement this error may be discovered only if several processes are simultaneously active in their critical sections note that this situation may not always be reproducible suppose that a process replaces signal  mutex  with wait  mutex   that is  it executes wait  mutex  ; critical section wait  mutex  ; in this case  a deadlock will occur suppose that a process omits the wait  mutex   or the signal  mutex   or both in this case  either mutual exclusion is violated or a deadlock will occur these examples illustrate that various types of errors can be generated easily when programmers use sencaphores incorrectly to solve the critical-section problem similar problems may arise in the other synchronization models discussed in section 6.6 to deal with such errors  researchers have developed high-level language constructs in this section  we describe one fundamental high-level synchronization construct-the monitor type 6.7.1 usage a abstract data type or adt encapsulates private data with public methods to operate on that data a monitor type is an adt which presents a set of programmer-defined operations that are provided mutual exclusion within the monitor the monitor type also contains the declaration of variables whose values define the state of an instance of that type  along with the bodies of procedures or functions that operate on those variables the syntax of a monitor type is shown in figure 6.16 the representation of a monitor type can not be used directly by the various processes thus  a procedure defined within a monitor can access only those variables declared locally within the monitor and its formal parameters similarly  the local variables of a monitor can be accessed by only the local procedures 246 chapter 6 monitor rrwnitor name  ii shared variable declarations procedure p1        procedure p2        procedure pn        initialization code         figure 6.16 syntax of a monitor the monitor construct ensures that only one process at a time is active within the monitor consequently  the programmer does not need to code this synchronization constraint explicitly  figure 6.17   howeve1 ~ the monitor construct  as defined so fa1 ~ is not sufficiently powerful for modeling some synchronization schemes for this purpose  we need to define additional synchronization mechanisms these mechanisms are provided by the condition construct a programmer who needs to write a tailor-made synchronization scheme can define one or more variables of type condition  condition x  y ; the only operations that can be invoked on a condition variable are wait   and signal    the operation x wait   ; means that the process invoking this operation is suspended until another process invokes x signal   ; the x signal   operation resumes exactly one suspended process if no process is suspended  then the signal   operation has no effect ; that is  the state of x is the same as if the operation had never been executed  figure shared data operations initialization code 6.7 figure 6.17 schematic view of a monitor 247 6.18   contrast this operation with the signal   operation associated with semaphores  which always affects the state of the semaphore now suppose that  when the x signal   operation is invoked by a process p  there exists a suspended process q associated with condition x clearly  if the suspended process q is allowed to resume its execution  the signaling process p must wait otherwise  both p and q would be active simultaneously within the monitor note  however  that both processes can conceptually continue with their execution two possibilities exist  signal and wait p either waits until q leaves the monitor or waits for another condition signal and continue q either waits until p leaves the monitor or waits for another condition there are reasonable arguments in favor of adopting either option on the one hand  since p was already executing in the monitor  the signal-and-continue method seems more reasonable on the other hand  if we allow thread p to continue  then by the time q is resumed  the logical condition for which q was waiting may no longer hold a compromise between these two choices was adopted in the language concurrent pascal when thread p executes the signal operation  it imncediately leaves the monitor hence  q is immediately resumed many programming languages have incorporated the idea of the monitor as described in this section  including concurrent pascal  mesa  c #  pronounced c-sharp   and java other languages-such as erlang-provide some type of concurrency support using a similar mechanism 248 chapter 6 queues associated with  x  y conditions ; -_  __ ~  ~  \  operations initialization code figure 6.18 monitor with condition variables 6.7.2 dining-philosophers solution using monitors next  we illustrate monitor concepts by presenting a deadlock-free solution to the dining-philosophers problem this solution imposes the restriction that a philosopher may pick up her chopsticks only if both of them are available to code this solution  we need to distinguish among three states in which we may find a philosopher for this purpose  we introduce the following data structure  enum  thinking  hungry  eating  state  5  ; philosopher i can set the variable state  i  = eating only if her two neighbors are not eating   state   i + 4  % 5  ! = eating  and  state   i + 1  % 5  ' = eating   we also need to declare condition sel  5  ; in which philosopher i can delay herself when she is hungry but is unable to obtain the chopsticks she needs we are now in a position to describe our solution to the dining-philosophers problem the distribution of the chopsticks is controlled by the monitor diningphilosophers  whose definition is shown in figure 6.19 each philosopher  before starting to eat  must invoke the operation pickup    this act n'lay result in the suspension of the philosopher process after the successful completion of the operation  the philosopher may eat following this  the philosopher invokes 6.7 monitor dp   enum  thinking  hungry  eating  state  5  ; condition self  5  ; void pickup  int i   state  i  = hungry ; test  i  ;  if  state  i  ! = eating  self  i   wait   ; void putdown  int i   state  i  = thinking ; test   i + 4  % 5  ; test   i + 1  % 5  ;  void test  int i    if   state   i + 4  % 5  ! = eating  &&  state  i  = = hungry  &&   state   i + 1  % 5  ! = eating    state  i  = eating ; self  i  .signal   ; initialization_code     for  int i = 0 ; i 5 ; i + +  state  i  = thinking ; figure 6.19 a monitor solution to the dining-philosopher problem 249 the put down   operation thus  philosopher i must invoke the operations pickup   and put down   in the following sequence  diningphilosophers.pickup  i  ; eat diningphilosophers.putdown  i  ; it is easy to show that this solution ensures that no two neighbors are eating simultaneously and that no deadlocks will occur we note  however  that it is possible for a philosopher to starve to death we do not present a solution to this problem but rather leave it as an exercise for you 250 chapter 6 6.7.3 implementing a monitor using semaphores we now consider a possible implementation of the nwnitor mechanism using semaphores for each ltlonitor  a semaphore mutex  initialized to 1  is provided a process must execute wait  mutex  before entering the n1onitor and must execute signal  mutex  after leaving the monitor since a signaling process must wait until the resumed process either leaves or waits  an additional sernaphore  next  is introduced  initialized to 0 the signaling processes can use next to suspend themselves an integer variable next_count is also provided to count the number of processes suspended on next thus  each external procedure f is replaced by wait  mutex  ; body off if  next_count 0  signal  next  ; else signal  mutex  ; mutual exclusion within a monitor is ensured we can now describe how condition variables are implemented as well for each condition x  we introduce a semaphore x_sem and an integer variable x_count  both initialized to 0 the operation x wait   can now be implemented as x_count + + ; if  next_count 0  signal  next  ; else signal  mutex  ; wait  x_sem  ; x_count ; the operation x signal   can be implemented as if  x_count 0   next_count + + ; signal  x_sem  ; wait  next  ; next_count ;  this implementation is applicable to the definitions of monitors given by both hoare and brinch-hansen in some cases  however  the generality of the implementation is unnecessary  and a significant improvement in efficiency is possible we leave this problem to you in exercise 6.35 6.7.4 resuming processes within a monitor we turn now to the subject of process-resumption order within a monitor if several processes are suspended on condition x  and an x signal   operation monitor resourceallocator   boolean busy ; condition x ; void acquire  int time   if  busy  x.wait  time  ; busy = true ;  void release    busy = false ; x signal   ;  initialization_code    busy = false ;  6.7 figure 6.20 a monitor to allocate a single resource 251 is executed by some process  then how do we determine which of the suspended processes should be resumed next one simple solution is to use an fcfs ordering  so that the process that has been waiting the longest is resumed first in many circumstances  however  such a simple scheduling scheme is not adequate for this purpose  the construct can be used ; it has the form x.wait  c  ; where c is an integer expression that is evaluated when the wait   operation is executed the value of c  which is called a pdos ! ty is then stored with the name of the process that is suspended when x signal   is executed  the process with the smallest priority number is resumed next to illustrate this new mechanism  consider the resourceallocator monitor shown in figure 6.20  which controls the allocation of a single resource among competing processes each process  when requesting an allocation of this resource  specifies the maximum time it plans to use the resource the monitor allocates the resource to the process that has the shortest time-allocation request a process that needs to access the resource in question must observe the following sequence  r.acquire  t  ; access the resource ; r release   ; where r is an instance of type resourceallocator 252 chapter 6 6.8 unfortunately  the monitor concept can not guarantee that the preceding access sequence will be observed in particular  the following problems can occur  a process might access a resource without first gaining access permission to the resource a process ntight never release a resource once it has been granted access to the resource a process might attempt to release a resource that it never requested a process might request the same resource twice  without first releasing the resource   the same difficulties are encountered with the use of semaphores  and these difficulties are similar in nature to those that encouraged us to develop the monitor constructs in the first place previously  we had to worry about the correct use of semaphores now  we have to worry about the correct use of higher-level programmer-defined operations  with which the compiler can no longer assist us one possible solution to the current problem is to include the resourceaccess operations within the resourceallocator monitor however  using this solution will mean that scheduling is done according to the built-in monitor-scheduling algorithm rather than the one we have coded to ensure that the processes observe the appropriate sequences  we must inspect all the programs that make use of the resourceallocator monitor and its managed resource we must check two conditions to establish the correctness of this system first  user processes must always make their calls on the monitor in a correct sequence second  we must be sure that an uncooperative process does not simply ignore the mutual-exclusion gateway provided by the monitor and try to access the shared resource directly  without using the access protocols only if these two conditions can be ensured can we guarantee that no time-dependent errors will occur and that the scheduling algorithm will not be defeated although this inspection may be possible for a small  static system  it is not reasonable for a large system or a dynamic system this access-control problem can be solved only through the use of additional mechanisms that are described in chapter 14 many programming languages have incorporated the idea of the monitor as described in this section  including concurrent pascal  mesa  c #  pronounced c-sharp   and java other languages-such as erlang-provide some type of concurrency support using a similar mechanism we next describe the synchronization mechanisms provided by the solaris  windows xp  and linux operating systems  as well as the pthreads api we have chosen these three operating systems because they provide good examples of different approaches for synchronizing the kernel  and we have included the 6.8 253 java monitors java provides a monitor-like concurrency mechanisn1 for thread synchronization every object in java has associated with it a single lock when a method is declared to be synchronized  calling the method requires owning the lock for the object we declare a synchronized method by placing the synchronized keyword in the method definition the following defines the safemethod   as synchronized  for example  public class simpleclass   public synchronized void safemethod    i implementation of safemethod   i  next  assume we create an object instance of simpleclass  such as  simpleclass sc = new simpleclass   ; invoking the sc safemethod   method requires owning the lock on the object instance sc if the lock is already owned by another thread  the thread calling the synchronized method blocks and is placed in the entry set for the object 's lock the entry set represents the set of threads waiting for the lock to become available if the lock is available when a synchronized method is called  the calling thread becomes the owner of the object 's lock and can enter the method the lock is released when the thread exits the method ; a thread from the entry set is then selected as the new owner of the lock java also provides wait   and notify   methods  which are similar in function to the wait   and signal 0 statements for a monitor release 1.5 of the java language provides api support for semaphores  condition variables  and mutex locks  among other concurrency mechanisms  in the java util concurrent package pthreads api because it is widely used for thread creation and synchronization by developers on unix and linux systems as you will see in this section  the synchronization methods available in these differing systems vary in subtle and significant ways 6.8.1 synchronization in solaris to control access to critical sections  solaris provides adaptive mutexes  condition variables  sernaphores  reader-writer locks  and turnstiles solaris implements semaphores and condition variables essentially as they are presented in sections 6.5 and 6.7 in this section  we describe adaptive mlltexes  readerwriter locks  and turnstiles 254 chapter 6 an protects access to every critical data item on a multiprocessor system  an adaptive mutex starts as a standard semaphore implemented as a spinlock if the data are locked and therefore already in use  the adaptive mutex does one of two things if the lock is held by a thread that is currently running on another cpu  the thread spins while waiting for the lock to become available  because the thread holding the lock is likely to finish soon if the thread holding the lock is not currently in run state  the thread blocks  going to sleep until it is awakened by the release of the lock it is put to sleep so that it will not spin while waiting  since the lock will not be freed very soon a lock held by a sleeping thread is likely to be in this category on a single-processor system  the thread holding the lock is never rwming if the lock is being tested by another thread  because only one thread can run at a time therefore  on this type of system  threads always sleep rather than spin if they encounter a lock solaris uses the adaptive-mutex method to protect only data that are accessed by short code segments that is  a mutex is used if a lock will be held for less than a few hundred instructions if the code segment is longer than that  the spin-waiting method is exceedingly inefficient for these longer code segments  condition variables and semaphores are used if the desired lock is already held  the thread issues a wait and sleeps when a thread frees the lock  it issues a signal to the next sleeping thread in the queue the extra cost of putting a thread to sleep and waking it  and of the associated context switches  is less than the cost of wasting several hundred instructions waiting in a spinlock reader-writer locks are used to protect data that are accessed frequently but are usually accessed in a read-only manner in these circumstances  reader-writer locks are more efficient than semaphores  because multiple threads can read data concurrently  whereas semaphores always serialize access to the data reader-writer locks are relatively expensive to implement  so again they are used only on long sections of code solaris uses turnstiles to order the list of threads waiting to acquire either an adaptive n1.utex or a reader-writer lock a is a queue structure containing threads blocked on a lock for example  if one thread currently owns the lock for a synchronized object  all other threads trying to acquire the lock will block and enter the turnstile for that lock when the lock is released  the kernel selects a thread from the turnstile as the next owner of the lock each synchronized object with at least one thread blocked on the object 's lock requires a separate turnstile however  rather than associating a turnstile with each synchronized object  solaris gives each kernel thread its own turnstile because a thread can be blocked only on one object at a time  this is more efficient than having a turnstile for each object the turnstile for the first thread to block on a synchronized object becomes the turnstile for the object itself threads subsequently blocking on the lock will be added to this turnstile when the initial thread ultimately releases the lock  it gains a new turnstile from a list of free turnstiles maintained by the kernel to prevent a priority inversion  turnstiles are organized according to a priorityinheritance protocol this means that if a lower-priority thread currently holds a lock on which a higher-priority thread is blocked  the thread with the lower priority will temporarily inherit the priority of the higher-priority thread upon releasing the lock  the thread will revert to its original priority 6.8 255 note that the locking mechanisms used by the kernel are implemented for user-level threads as well  so the same types of locks are available inside and outside the kernel a crucial implementation difference is the priorityinheritance protocol kernel-locking routines adhere to the kernel priorityinheritance methods used by the scheduler  as described in section 19.4 ; user-level thread-locking mechanisms do not provide this functionality to optimize solaris performance  developers have refined and fine-tuned the locking methods because locks are used frequently and typically are used for crucial kernel functions  tuning their implem.entation and use can produce great performance gains 6.8.2 synchronization in windows xp the windows xp operating system is a multithreaded kernel that provides support for real-time applications and multiple processors when the windows xp kernel accesses a global resource on a uniprocessor system  it temporarily masks interrupts for all interrupt handlers that may also access the global resource on a multiprocessor system  windows xp protects access to global resources using spinlocks just as in solaris  the kernel uses spinlocks only to protect short code segments furthermore  for reasons of efficiency  the kernel ensures that a thread will never be preempted while holding a spinlock for thread synchronization outside the kernel  windows xp provides ~   using a dispatcher object  threads synchronize according to several different mechanisms  including mutexes  semaphores  events  and timers the system protects shared data by requiring a tluead to gain ownership of a mutex to access the data and to release ownership when it is finished semaphores behave as described in section 6.5 are similar to condition variables ; that is  they may notify a waiting thread when a desired condition occurs finally  timers are used to notify one  or more than one  thread that a specified amount of time has expired dispatcher objects may be in either a signaled state or a nonsignaled state a si ,7'2led indicates that an object is available and a thread will not block when acquiring the object a indicates that an object is not available and a thread will block when attempting to acquire the object we illustrate the state transitions of a mutex lock dispatcher object in figure 6.21 a relationship exists between the state of a dispatcher object and the state of a thread when a thread blocks on a nonsignaled dispatcher object  its state changes frmn ready to waiting  and the thread is placed in a waiting queue for that object when the state for the dispatcher object moves to signaled  the kernel checks whether any threads are waiting on the object if so  the owner thread releases mutex lock thread acquires mutex lock figure 6.21 mutex dispatcher object 256 chapter 6 kernel moves one thread -or possibly nlore threads-from the waiting state to the ready state  where they can resume executing the number of threads the kernel selects from the waiting queue depends on the type of dispatcher object for which it is waiting the kernel will select only one thread from the waiting queue for a mutex  since a mutex object may be owned by only a single thread for an event object  the kernel will select all threads that are waiting for the event we can use a mutex lock as an illustration of dispatcher objects and thread states if a thread tries to acquire a mutex dispatcher object that is in a nonsignaled state  that thread will be suspended and placed in a waiting queue for the mutex object when the mutex moves to the signaled state  because another thread has released the lock on the mutex   the thread waiting at the front of the queue will be moved from the waiting state to the ready state and will acquire the mutex lock we provide a programming project at the end of this chapter that uses mutex locks and semaphores in the win32 api 6.8.3 synchronization in linux prior to version 2.6  linux was a nonpreemptive kernel  meaning that a process running in kernel mode could not be preempted -even if a higher-priority process became available to run now  however  the linux kernel is fully preemptive  so a task can be preempted when it is running in the kernel the linux kernel provides spinlocks and semaphores  as well as readerwriter versions of these two locks  for locking in the kernel on smp machines  the fundamental locking mechanism is a spinlock  and the kernel is designed so that the spinlock is held only for short durations on single-processor machines  spinlocks are inappropriate for use and are replaced by enabling and disabling kernel preemption that is  on single-processor machines  rather than holding a spinlock  the kernel disables kernel preemption ; and rather than releasing the spinlock  it enables kernel preemption this is summarized below  disable kernel preemption  acquirespin lock enable kernel preemption release spin lock linux uses an interesting approach to disable and enable kernel preemption it provides two simple system calls-preempldisable   and preempt_ enable   -for disabling and enabling kernel preemption in addition  however  the kernel is not preemptible if a kernel-mode task is holding a lock to enforce this rule  each task irl the system has a thread-info structure containing a counter  preemplcount  to indicate the number of locks being held by the task when a lock is acquired  preemplcount is incremented it is decremented when a lock is released if the value of preempt_count for the task currently running is greater than zero  it is not safe to preempt the kernel  as this task currently holds a lock if the count is zero  the kernel can safely be interrupted  assuncing there are no outstanding calls to preempldisable     6.9 6.9 257 spinlocks-along with enabling and disabling kernel preemption-are used in the kernel only when a lock  or disabling kernel preemption  is held for a short duration when a lock must be held for a longer period  semaphores are appropriate for use 6.8.4 synchronization in pthreads the pthreads api provides mutex locks  condition variables  and read-write locks for thread synchronization this api is available for programmers and is not part of any particular kernel mutex locks represent the fundamental synchronization technique used with pthreads a mutex lock is used to protect critical sections of code-that is  a thread acquires the lock before entering a critical section and releases it upon exiting the critical section condition variables in pthreads behave much as described in section 6.7 read-write locks behave similarly to the locking mechanism described in section 6.6.2 many systems that implement pthreads also provide semaphores  although they are not part of the pthreads standard and instead belong to the posix sem extension other extensions to the pthreads api include spinlocks  but not all extensions are considered portable from one implementation to another we provide a programming project at the end of this chapter that uses pthreads mutex locks and semaphores the mutual exclusion of critical sections ensures that the critical sections are executed atomically -that is  as one uninterruptible unit if two critical sections are instead executed concurrently  the result is equivalent to their sequential execution in some unknown order although this property is useful in many application domains  in many cases we would like to make sure that a critical section forms a single logical unit of work that either is performed in its entirety or is not performed at all an example is funds transfer  in which one account is debited and another is credited clearly  it is essential for data consistency either that both the credit and debit occur or that neither occurs consistency of data  along with storage and retrieval of data  is a concern often associated with recently  there has been an upsurge of interest in using database-systems techniques in operating systems operating systems can be viewed as manipulators of data ; as such  they can benefit from the advanced techniques and models available from database research for instance  many of the ad hoc techniques used in operating systems to manage files could be more flexible and powerful if more formal database methods were used in their place in sections 6.9.2 to 6.9.4  we describe some of these database techniques and explain how they can be used by operating systems first  however  we deal with the general issue of transaction atomicity it is this property that the database techniques are meant to address 6.9.1 system model a collection of instructions  or operations  that performs a single logical function is called a a major issue in processing transactions is the 258 chapter 6 preservation of atomicity despite the possibility of failures within the computer system we can think of a transaction as a program unit that accesses and perhaps updates various data items that reside on a disk within some files from our point of view  such a transaction is simply a sequence of read and write operations terminated by either a commit operation or an abort operation a commit operation signifies that the transaction has terminated its execution successfully  whereas an abort operation signifies that the transaction has transactional memory with the emergence of multicore systems has come increased pressure to develop multithreaded applications that take advantage of multiple processing cores however  multithreaded applications present an increased risk of race conditions and deadlocks traditionally  techniques such as locks  semaphores  and monitors have been used to address these issues however  provides an alternative strategy fordeveloping thread-safe concurrent applications a is a sequence of memory read-write operations that are atomic if all operations in a transaction are completed  the memory transaction is committed ; otherwise  the operations must be aborted and rolled back the benefits of transactional memory can be obtained through features added to a programming language consider an example suppose we have a function update   that modifies shared data traditionally  this function would be written using locks such as the following  update    acquire   ;  i modify shared data i release   ; however  using synchronization mechanisms such as locks and semaphores involves many potential problems  including deadlocks additionally  as the number of threads increases  traditional locking does not scale well as an alternative to traditional methods  new features that take advantage of transactional memory can be added to a programming language in our example  suppose we add the construct atomic  s   which ensures that the operations in s execute as a transaction this allows us to rewrite the update   method as follows  update    atomic  i modify shared data i   continued on following page 6.9 259 transactional memory  continued  the advantage of using such a mechanism rather than locks is that the transactional memoi y system ~ not the developer-isrespon.sible for guaranteeing atomicity additionally  the system can identify which statements in atomic blocks can be executed concurrently  such as concurrent read access to a shared variable it is  of course  possible for a programmer to identify these situations and use reader-writer locks  but the task becomes increasingly difficult as the number ofthreads within anapplicationgrows transactional memory can be implemented in either software or hardware software transactional memory  stm   as the nam ~ suggests  imp lee ments transactional memory exclusivelyin software ~ nospecial hardware is needed stm works by inserting instrumentation code inside transaction blocks the code is inserted by a compiler and manages each transaction by examining where statements may run concurrently and where specific lowlevellockingis required hardware transactional memory  small htm  uses hardware cache hierarchies and cache coherency protocols to manage and resolve conflicts involving shared data residing in separate processors caches htm requires no special code instmmentation and thus has less overhead than stm however  htm does require that existing cache hierarchies and cachecoherencyprotocolsbe modified to support transactional memory transactional memory has existed for several years without widespread implementation however  the growth of multi core systems and the associated emphasis on concurrent programming have prompted a significant amolmt ofresearch in this area on the part of both academics and hard ware vendors  including intel and sun microsystems ended its normal execution due to some logical error or a system failure if a terminated transaction has completed its execution successfully  it is otherwise  it is since an aborted transaction may already have modified the data that it has accessed  the state of these data may not be the same as it would have been if the transaction had executed atomically so that atomicity is ensured  an aborted transaction must have no effect on the state of the data that it has already modified thus  the state of the data accessed by an aborted transaction must be restored to what it was just before the transaction started executing we say that such a transaction has been it is part of the responsibility of the system to ensure this property to determ.ine how the system should ensure atomicity  we need first to identify the properties of devices used for storing the various data accessed by the transactions various types of storage media are distinguished by their relative speed  capacity  and resilience to failure volatile storage information residing in volatile storage does not usually survive system crashes examples of such storage are main and cache merrwry access to volatile storage is extremely fast  both because of the speed of the memory access itself and because it is possible to access directly any data item in volatile storage 260 chapter 6 nonvolatile storage information residing in nonvolatile storage usually survives system crashes examples of m.edia for such storage are disks and magnetic tapes disks are more reliable than main memory but less reliable than magnetic tapes both disks and tapes  however  are subject to failure  which may result in loss of inform.ation currently  nonvolatile storage is slower than volatile storage by several orders of magnitude  becm1se disk and tape devices are electromechanical and require physical motion to access data stable storage information residing in stable storage is never lost  never should be taken with a grain of salt  since theoretically such absolutes can not be guaranteed   to implement an approximation of such storage  we need to replicate information in several nonvolatile storage caches  usually disk  with independent failure modes and to update the inform.ation in a controlled manner  section 12.8   here  we are concerned only with ensuring transaction atomicity in an environment where failures result in the loss of inform.ation on volatile storage 6.9.2 log-based recovery one way to ensure atomicity is to record  on stable storage  information describing all the modifications made by the transaction to the various data it accesses the most widely used method for achieving this form of recording is here  the system maintains  on stable storage  a data structure called the each log record describes a single operation of a transaction write and has the following fields  transaction name the unique name of the transaction that performed the write operation data item name the unique name of the data item written old value the value of the data item prior to the write operation new value the value that the data item will have after the write other special log records exist to record significant events during transaction processing  such as the start of a transaction and the commit or abort of a transaction before a transaction t ; starts its execution  the record t ; starts is written to the log during its execution  any write operation by t ; is preceded by the writing of the appropriate new record to the log when t ; commits  the record t ; commits is written to the log because the information in the log is used in reconstructing the state of the data items accessed by the various transactions  we can not allow the actual update to a data item to take place before the corresponding log record is written out to stable storage we therefore require that  prior to execution of a wri te  x  operation  the log records corresponding to x be written onto stable storage note the performance penalty inherent in this system two physical writes are required for every logical write requested also  more storage is needed  both for the data themselves and for the log recording the changes in cases 6.9 261 where the data are extremely important and fast failure recovery is necessary  however  the functionality is worth tl1e price using the log  the system can handle any failure that does not result in the loss of information on nonvolatile storage the recovery algorithm uses two procedures  undo  t ;   which restores the value of all data updated by transaction t ; to the old values redo  t ;   which sets the value of all data updated by transaction t ; to the new values the set of data updated by t ; and the appropriate old and new values can be found in the log note that the undo and redo operations must be idempotent  that is  multiple executions must have the same result as does one execution  to guarantee correct behavior even if a failure occurs during the recovery process if a transaction t ; aborts  then we can restore the state of the data that it has updated by simply executing undo  t ;   if a system failure occurs  we restore the state of all updated data by consulting the log to determine which transactions need to be redone and which need to be lmdone this classification of transactions is accomplished as follows  transaction t ; needs to be undone if the log contains the i ; starts record but does not contain the t ; corrnni ts record transaction t ; needs to be redone if the log contains both the t ; starts and the t ; corrnni ts records 6.9.3 checkpoints when a system failure occurs  we must consult the log to determine which transactions need to be redone and which need to be undone in principle  we need to search the entire log to make these determinations there are two major drawbacks to this approach  the searching process is time consuming most of the transactions that  according to our algorithm  need to be redone have already actually updated the data that the log says they need to modify although redoing the data modifications will cause no harm  due to idempotency   it will nevertheless cause recovery to take longer to reduce these types of overhead  we introduce the concept of during execution  the system maintains the write-ahead log in addition  the system periodically performs checkpoints that require the following sequence of actions to take place  output all log records currently residing in volatile storage  usually main memory  onto stable storage output all modified data residing in volatile storage to the stable storage output a log record checkpoint onto stable storage 262 chapter 6 the presence of a checkpoint record in the log allows the systen'l to streamline its recovery procedure consider a transaction i ; that committed prior to the checkpoint the t ; commits record appears in the log before the checkpoint record any modifications made by t ; must have been written to stable storage either prior to the checkpoint or as part of the checkpoint itself thus  at recovery time  there is no need to perform a redo operation on t ;  this observation allows us to refine our previous recovery algorithm after a failure has occurred  the recovery routine examines the log to determine the most recent transaction t ; that started executing before the most recent checkpoint took place it finds such a transaction by searching the log backward to find the first checkpoint record and then finding the subsequent t ; start record once transaction t ; has been identified  the redo and undo operations need be applied only to transaction t ; and all transactions t1 that started executing after transaction i ;  we 'll call these transactions set t the remainder of the log can be ignored the recovery operations that are required are as follows  for all transactions 'nc in t for which the record tic commits appears in the log  execute redo  t/c  for all transactions 'nc in t that have no tic commits record in the log  execute undo  t ; c   6.9.4 concurrent atomic transactions we have been considering an environment in which only one transaction can be executing at a time we now turn to the case where multiple transactions are active simultaneously because each transaction is atomic  the concurrent execution of transactions must be equivalent to the case where these transactions are executed serially in some arbih ary order this property  called can be maintained by simply executing each transaction within a critical section that is  all transactions share a common semaphore mutex  which is initialized to 1 when a transaction starts executing  its first action is to execute wai t  mutex   after the transaction either commits or aborts  it executes signal  mutex   although this scheme ensures the atomicity of all concurrently executing transactions  it is nevertheless too restrictive as we shall see  in many cases we can allow transactions to overlap their execution while maintaining serializability a number of different ensure serializability  and we describe these algorithms next 6.9.4.1 serializability consider a system with two data items  a and b  that are both read and written by two transactions  to and t1 suppose that these transactions are executed atomically in the order t0 followed by t1 this execution sequence  which is called a schedule  is represented in figure 6.22 in schedule 1 of figure 6.22  the sequence of instruction steps is in chronological order from top to bottom  with instructions of to appearing in the left column and instructions of t1 appearing in the right colunm 6.9 263 to t1 read  a  write  a  read  b  write  b  read  a  write  a  read  b  write  b  figure 6.22 schedule i  a serial schedule in which to is followed by t1 a schedule in which each transaction is executed atomically is called a a serial schedule consists of a sequence of instructions from various transactions wherein the instructions belonging to a particular transaction appear together thus  for a set of n transactions  there exist n ! different valid serial schedules each serial schedule is correct  because it is equivalent to the atomic execution of the various participating transactions in some arbitrary order if we allow the two transactions to overlap their execution  then the resulting schedule is no longer serial a  cj,sef'i  al does not necessarily imply an incorrect execution  that is  an execution that is not equivalent to one represented by a serial schedule   to see that this is the case  we need to define the notion of nflic ; cing consider a schedule s in which there are two consecutive operations 0 ; and oi of transactions ~ and ti  respectively we say that 0 ; and oj conflict if they access the same data item and at least one of them is a write operation to illustrate the concept of conflicting operations  we consider the nonserial schedule 2 of figure 6.23 the wri te  a  operation of to conflicts with the read  a  operation of t1 however  the wri te  a  operation of t1 does not conflict with the read  b  operation of to  because the two operations access different data items to t1 read  a  write  a  read  a  write  a  read  b  write  b  read  b  write  b  figure 6.23 schedule 2  a concurrent serializable schedule 264 chapter 6 let 0 ; and 0 ; be consecutive operations of a schedule 5 if 0 ; and oi are operations of different transactions and 0 ; and oi do not conflict then we can swap the order of 0 ; and 0 ; to produce a new schedule 5' we expect 5 to be equivalent to 5 '  as all operations appear in the same order in both schedules  except for 0 ; and 0 1  whose order does not matter we can illustrate the swapping idea by considering again schedule 2 of figure 6.23 as the wri te  a  operation of t1 does not conflict with the read  b  operation of t0  we can swap these operations to generate an equivalent schedule regardless of the initial system state  both schedules produce the same final system state continuing with this procedure of swapping nonconflicting operations  we get  swap the read  b  operation of to with the read  a  operation of t1 swap the write  b  operation of to with the write  a  operation of t1 swap the wri te  b  operation of to with the read  a  operation of t1 the final result of these swaps is schedule 1 in figure 6.22  which is a serial schedule thus  we have shown that schedule 2 is equivalent to a serial schedule this result implies that regardless of the initial system state  schedule 2 will produce the same final state as will some serial schedule if a schedule 5 can be transformed into a serial schedule 5 ' swaps of nonconflicting operations  we say that a schedule 5 is izable thus  schedule 2 is conflict serializable  because it can be transformed into the serial schedule 1 6.9.4.2 locking protocol one way to ensure serializability is to associate a lock with each data item and to require that each transaction follow a that governs how locks are acquired and released there are various modes in which a data item can be locked in this section  we restrict our attention to two modes  shared if a transaction 7i has obtained a shared-mode lock  denoted by s  on data item q  then 1i can read this item but can not write q exclusive if a transaction t ; has obtained an exclusive-mode lock  denoted by x  on data item q  then 7i can both read and write q we require that every transaction request a lock in an appropriate m.ode on data item q  depending on the type of operations it will perform on q to access data item q  transaction 1i must first lock q in the appropriate mode if q is not currently locked  then the lock is granted  and t ; can now access it however  if the data item q is currently locked by some other transaction  then t ; may have to wait more specifically  suppose that 1i requests an exclusive lock on q in this case  1i must wait until the lock on q is released if t ; requests a shared lock on q  then t ; must wait if q is locked in exclusive mode otherwise  it can obtain the lock and access q notice that this scheme is quite similar to the readers-writers algorithm discussed in section 6.6.2 a transaction may unlock a data item that it locked at an earlier point it must  however  hold a lock on a data item as long as it accesses that item 6.9 265 moreove1 ~ it is not always desirable for a transaction to unlock a data item immediately after its last access of that data item  because serializability may not be ensured one protocol that ensures serializability is the this protocol requires that each transaction issue lock and unlock requests in two phases  growing phase a transaction may obtain locks but may not release any locks shrinking phase a transaction may release locks but may not obtain any new locks initially a transaction is in the growing phase the transaction acquires locks as needed once the transaction releases a lock  it enters the shrinking phase  and no more lock requests can be issued the two-phase locking protocol ensures conflict serializability  exercise 6.14   it does not  however  ensure freedom from deadlock in addition  it is possible that  for a given set of transactions  there are conflict-serializable schedules that can not be obtained by use of the two-phase locking protocol to improve performance over two-phase locking  we need either to have additional information about the transactions or to impose some structure or ordering on the set of data 6.9.4.3 timestamp-based protocols in the locking protocols described above  the order followed by pairs of conflicting transactions is determined at execution time another method for determining the serializability order is to select an order in advance the most common method for doing so is to use a ordering scheme with each transaction ~ in the system  we associate a unique fixed timestamp  denoted by ts  t ;   this timestamp is assigned by the system before the transaction t ; starts execution if a transaction ~ has been assigned timestamp ts  ~   and later a new transaction ti enters the system  then ts  t ;  ts  tj   there are two simple methods for implementing this scheme  use the value of the system clock as the timestamp ; that is  a transaction 's timestamp is equal to the value of the clock when the transaction enters the system this method will not work for transactions that occur on separate systems or for processors that do not share a clock use a logical counter as the timestamp ; that is  a transaction 's timestamp is equal to the value of the counter when the transaction enters the system the counter is incremented after a new timestamp is assigned the timestamps of the transactions determine the serializability order thus  if ts  ti  ts  tj   then the system must ensure that the schedule produced is equivalent to a serial schedule in which transaction ~ appears before transaction tj to implement this scheme  we associate with each data item q two timestamp values  266 chapter 6 w-timestamp  q  denotes the largest timestamp of any transaction that successfully executed wri te  q   r-timestamp  q  denotes the largest timestamp of any transaction that successfully executed read  q   these timestamps are updated whenever a new read  q  or wri te  q  instruction is executed the timestamp ordering protocol ensures that any conflicting read and write operations are executed in timestamp order this protocol operates as follows  suppose that transaction t ; issues read  q   o if ts  ti  w-timestamp    then t ; needs to read a value of q that was already overwritten hence  the read operation is rejected  and t ; is rolled back o if ts  t ;  2   w-timestamp  q   then the read operation is executed  and r-timestamp  q  is set to the maximum of r-timestamp  q  and ts  t ;   suppose that transaction t ; issues wri te  q   o if ts  t ;  r-timestamp  q   then the value of q that t ; is producing was needed previously and t ; assumed that this value would never be produced hence  the write operation is rejected  and t ; is rolled back o if ts  t ;  w-timestamp  q   then t ; is attempting to write an obsolete value of q hence  this write operation is rejected  and t ; is rolled back o otherwise  the write operation is executed a transaction t ; that is rolled back as a result of either a read or write operation is assigned a new timestamp and is restarted to illustrate this protocol  consider schedule 3 in figure 6.24  which includes transactions t2 and t3 we assume that a transaction is assigned a timestamp immediately before its first instruction thus  in schedule 3  ts  t2  ts  t3   and the schedule is possible under the timestamp protocol this execution can also be produced by the two-phase locking protocol howeve1 ~ some schedules are possible under the two-phase locking protocol but not under the timestamp protocol  and vice versa t2 t3 read  b  read  b  write  b  read  a  read  a  write  a  figure 6.24 schedule 3  a schedule possible under the timestamp protocol 6.10 267 the timestamp protocol ensures conflict serializability this capability follows from the fact that conflicting operations are processed in timestamp order the protocol also ensures freedom fron1 deadlocl   because no transaction ever waits given a collection of cooperating sequential processes that share data  mutual exclusion must be provided to ensure that a critical section of code is used by only one process or thread at a tince typically  computer hardware provides several operations that ensure mutual exclusion however  such hardware-based solutions are too complicated for most developers to use semaphores overcome this obstacle semaphores can be used to solve various synchronization problems and can be implemented efficiently  especially if hardware support for atomic operations is available various synchronization problems  such as the bounded-buffer problem  the readers-writers problem  and the dining-philosophers problem  are important mainly because they are examples of a large class of concurrency-control problems these problems are used to test nearly every newly proposed synchronization scheme the operating system must provide the means to guard against timing errors several language constructs have been proposed to deal with these problems monitors provide the synchronization mechanism for sharing abstract data types a condition variable provides a method by which a monitor procedure can block its execution until it is signaled to continue operating systems also provide support for synchronization for example  solaris  windows xp  and linux provide mechanisms such as semaphores  mutexes  spinlocks  and condition variables to control access to shared data the pthreads api provides support for mutexes and condition variables a transaction is a program unit that must be executed atomically ; that is  either all the operations associated with it are executed to completion  or none are performed to ensure atomicity despite system failure  we can use a write-ahead log all updates are recorded on the log  which is kept in stable storage if a system crash occurs  the information in the log is used in restoring the state of the updated data items  which is accomplished by use of the undo and redo operations to reduce the overhead in searching the log after a system failure has occurred  we can use a checkpoint scheme to ensure serializability when the execution of several transactions overlaps  we must use a concurrency-control scheme various concurrency-control schemes ensure serializability by delaying an operation or aborting the transaction that issued the operation the most common ones are locking protocols and timestamp ordering schemes 6.1 the first known correct software sohjtion to the critical-section problem for two processes was developed by dekker the two processes  p0 and p1  share the following variables  boolean flag  2  ; i initially false i int turn ; 268 chapter 6 do  flag  i  = true ; while  flag  j     if  turn = = j   flag  i  = false ; while  turn = = j  ; ii do nothing flag  i  = true ;  ii critical section turn = j ; flag  i  = false ; ii remainder section  while  true  ; figure 6.25 the structure of process a in dekker 's algorithm the structure of process pi  i = = 0 or 1  is shown in figure 6.25 ; the other process is p1  j = = 1 or 0   prove that the algorithm satisfies all three requirements for the critical-section problem 6.2 explain why interrupts are not appropriate for implementing synchronization primitives in multiprocessor systems 6.3 the first known correct software solution to the critical-section problem for n processes with a lower bound on waiting of n  1 turns was presented by eisenberg and mcguire the processes share the following variables  enum pstate  idle  want_in  in_cs  ; pstate flag  n  ; int turn ; all the elements of flag are initially idle ; the initial value of turn is immaterial  between 0 and n-1   the structure of process pi is shown in figure 6.26 prove that the algorithm satisfies all three requiren'lents for the critical-section problem 6.4 write a monitor that implements an alarm clock that enables a calling program to delay itself for a specified number of tirne units  ticks   you may assume the existence of a real hardware clock that invokes a procedure hclc in your monitor at regular intervals 6.5 a file is to be shared among different processes  each of which has a unique number the file can be accessed simultaneously by several processes  subject to the following constraint  the sum of all unique do  while  true   flag  i  = want_in ; j = turn ;  while  j ! = i    if  flag  j  i = idle   j = turn ; else j =  j + 1  % n ; flag  i  j = 0 ; in_cs ; while   j n  &&  j j + + ; if   j = n  &&  turn break ; ii critical section j =  turn + 1  % n ; while  flag  j  = = idle  j =  j + 1  % n ; turn = j ; flag  i  = idle ; ii remainder section  while  true  ; i ii flag  j  ! = in_cs   i i i flag  turn  idle   figure 6.26 the structure of process a in eisenberg and mcguire 's algorithm 269 numbers associated with all the processes currently accessing the file must be less than n write a monitor to coordinate access to the file 6.6 the decrease_count   function in the previous exercise currently returns 0 if sufficient resources are available and -1 otherwise this leads to awkward programming for a process that wishes to obtain a number of resources  while  decrease_count  count  = = -1  rewrite the resource-manager code segment using a monitor and condition variables so that the decrease_count   function suspends 270 chapter 6 the process until sufficient resources are available this will allow a process to invoke decrease_count   by simply calling decrease_count  count  ; the process will return from this function call only when sufficient resources are available 6.7 exercise 4.12 requires the parent thread to wait for the child thread to finish its execution before printing out the computed values if we let the parent thread access the fibonacci numbers as soon as they have been computed by the child thread  rather than waiting for the child thread to terminate explain what changes would be necessary to the solution for this exercise implement your modified solution 6.8 in section 6.4  we mentioned that disabling interrupts frequently can affect the system 's clock explain why this can occur and how such effects can be mil1.imized 6.9 servers can be designed to limit the number of open coru1.ections for example  a server may wish to have only n socket com1.ections at any point in time as soon as n connections are made  the server will not accept another incoming connection until an existing connection is released explain how semaphores can be used by a server to limit the number of concurrent connections 6.10 why do solaris  lil1.ux  and windows xp use spinlocks as a synchronization mechanism only on multiprocessor systems and not on single-processor systems 6.11 show that  if the wait   and signal   semaphore operations are not executed atomically  then mutual exclusion may be violated 6.12 show how to implement the wait   and signal   semaphore operations in multiprocessor environments using the testandset   instruction the solution should exhibit minimal busy waiting 6.13 suppose we replace the wait   and signal   operations of monitors with a single construct await  b   where b is a general boolean expression that causes the process executing it to wait until b becomes true a write a monitor using this scheme to implement the readerswriters problem b explain why  in general  this construct can not be implemented efficiently c what restrictions need to be put on the await statement so that it can be implemented efficiently  hint  restrict the generality of b ; see kessels  1977    271 6.14 show that the two-phase locking protocol ensures conflict serializability 6.15 how does the signal   operation associated with monitors differ from the corresponding operation defined for semaphores 6.16 describe how volatile  nonvolatile  and stable storage differ in cost 6.17 explain why implementing synchronization primitives by disabling interrupts is not appropriate in a single-processor system if the synchronization primitives are to be used in user-level programs 6.18 consider a system consisting of processes p1  p2    p11  each of which has a unique priority number write a monitor that allocates three identical line printers to these processes  using the priority numbers for deciding the order of allocation 6.19 describe two kernel data structures in which race conditions are possible be sure to include a description of how a race condition can occur 6.20 assume that a finite number of resources of a single resource type must be managed processes may ask for a number of these resources and -once finished-will return them as an example  many commercial software packages provide a given number of licenses  indicating the number of applications that may run concurrently when the application is started  the license count is decremented when the application is terminated  the license count is incremented if all licenses are in use  requests to start the application are denied such requests will only be granted when an existing license holder terminates the application and a license is returned the following program segment is used to manage a finite number of instances of an available resource the maximum number of resources and the number of available resources are declared as follows  # define max_resources 5 int available_resources = max_resources ; when a process wishes to obtain a number of resources  it invokes the decrease_count   function  i decrease available_resources by count resources i i return 0 if sufficient resources available  i i otherwise return -1 i int decrease_count  int count    if  available_resources count  return -1 ; else  available_resources count ; return 0 ;  272 chapter 6 when a process wants to return a number of resourcesf it calls the increase_count   function  i increase available_resources by count i int increase_count  int count   available_resources + = count ; return 0 ;  the preceding program segment produces a race condition do the following  a identify the data involved in the race condition b identify the location  or locations  in the code where the race condition occurs c using a semaphoref fix the race condition it is ok to modify the decrease_count   fun.ction so that the calling process is blocked until sufficient resources are available 6.21 explain why spinlocks are not appropriate for single-processor systems yet are often used in multiprocessor systems 6.22 the cigarette-smokers problem consider a system with three smoker processes and one agent process each smoker continuously rolls a cigarette and then smokes it but to roll and smoke a cigarettef the smoker needs three ingredients  tobaccof paperf and matches one of the smoker processes has paperf another has tobaccof and the third has matches the agent has an infinite supply of all three materials the agent places two of the ingredients on the table the smoker who has the remaining ij.l.gredient then makes and smokes a cigarette  signaling the agent on completion the agent then puts out another two of the three ingredients  and the cycle repeats write a program to synchronize the agent and the smokers using java synchronization 6.23 describe how the swap   instruction can be used to provide mutual exclusion that satisfies the bounded-waiting requirement 6.24 a new lightweight synchronization tool called locks whereas most implementations of readerwriter locks favor either readers or writers  or perhaps order waiting threads using a fifo policy  slim reader-writer locks favor neither readers nor writers  nor are waiting threads ordered in a fifo queue explain the benefits of providing such a synchronization tool 6.25 what are the implications of assigning a new timestamp to a transaction that is rolled back how does the system process transactions that were issued after the rolled -back transaction b-ut that have timestamps smaller than the new timestamp of the rolled-back transaction 273 6.26 discuss the tradeoff between fairness and throughput of operations in the readers-writers problem propose a method for solving the readers-writers problem without causing starvation 6.2'7 when a signal is performed on a condition inside a monitor  the signaling process can either continue its execution or transfer control to the process that is signaled how would the solution to the preceding exercise differ with these two different ways in which signaling can be performed 6.28 what is the meaning of the term busy waiting what other kinds of waiting are there in an operating system can busy waiting be avoided altogether explain your answer 6.29 demonstrate that monitors and semaphores are equivalent insofar as they can be used to implement the same types of synchronization problems 6.30 in log-based systems that provide support for transactions  updates to data items can not be performed before the corresponding entries are logged why is this restriction necessary 6.31 explain the purpose of the checkpoint mechanism how often should checkpoints be performed describe how the frequency of checkpoints affects  system performance when no failure occurs the time it takes to recover from a system crash the time it takes to recover from a disk crash 6.32 write a bounded-buffer monitor in which the buffers  portions  are embedded within the monitor itself 6.33 the strict mutual exclusion within a monitor makes the bounded-buffer monitor of exercise 6.32 mainly suitable for small portions a explain why this is true b design a new scheme that is suitable for larger portions 6.34 race conditions are possible in many computer systems consider a banking system with two functions  deposit  amount  and withdraw  amount   these two functions are passed the amount that is to be deposited or withdrawn from a bank account assume a shared bank account exists between a husband and wife and concurrently the husband calls the withdraw   function and the wife calls deposit    describe how a race condition is possible and what might be done to prevent the race condition from occurring 274 chapter 6 6.35 suppose the signal   statement can appear only as the last statement in a monitor procedure suggest how the implementation described in section 6.7 can be simplified in this situation 6.36 the sleeping-barber problem a barbershop consists of a waiting room with n chairs and a barber roorn with one barber chair if there are no customers to be served  the barber goes to sleep if a customer enters the barbershop and all chairs are occupied  then the customer leaves the shop if the barber is busy but chairs are available  then the customer sits in one of the free chairs if the barber is asleep  the customer wakes up the barber write a program to coordinate the barber and the customers 6.37 producer-consumer problem in section 6.6.1  we had presented a semaphore-based solution to the producer-consumer problem using a bounded buffer in this project  we will design a programming solution to the bounded-buffer problem using the producer and consumer processes shown in figures 6.10 and 6.11 the solution presented in section 6.6.1 uses three semaphores  empty and full  which count the number of empty and full slots in the buffer  and mutex  which is a binary  or mutual-exclusion  semaphore that protects the actual insertion or removal of items in the buffer for this project  standard counting semaphores will be used for empty and full  and a mutex lock  rather than a binary semaphore  will be used to represent mutex the producer and consumer-running as separate threads-will move items to and from a buffer that is synchronized with these empty  full  and mutex structures you can solve this problem using either pthreads or the win32 api the buffer internally  the buffer will consist of a fixed-size array of type buffer_i tern  which will be defined using a typedef   the array of buffer_i tern objects will be manipulated as a circular queue the definition of buffer _i tern  along with the size of the buffer  can be stored in a header file such as the following  i buffer.h i typedef int buffer_item ; # define buffer_size 5 the buffer will be manipulated with two functions  insert_i tern   and remove_i tern   ,which are called by the producer and consumer threads  respectively a skeleton outlining these functions appears in figure 6.27 # include buffer.h i the buffer i buffer_item buffer  buffer_size  ; int insert_item  buffer_item item   i insert item into buffer return 0 if successful  otherwise return -1 indicating an error condition i  int remove_item  buffer_item item    i remove an object from buffer placing it in item return 0 if successful  otherwise return -1 indicating an error condition i figure 6.27 a skeleton program 275 the insert_item   and remove_item   functions will synchronize the producer and consumer using the algorithms outlined in figures 6.10 and 6.11 the buffer will also require an initialization function that initializes the mutual-exclusion object mutex along with the empty and full semaphores the main   f-lmction will initialize the buffer and create the separate producer and consumer threads once it has created the producer and consumer threads  the main   function will sleep for a period of time and  upon awakening  will terminate the application the main   function will be passed three parameters on the command line  a how long to sleep before terminating b the number of producer threads c the nuncber of consumer threads a skeleton for this function appears in figure 6.28 # include buffer.h int main  int argc  char argv      i 1 get command line arguments argv  1  ,argv  2  ,argv  3  i i 2 initialize buffer i i 3 create producer thread  s  i i 4 create consumer thread  s  i i 5 sleep i i 6 exit i figure 6.28 a skeleton program 276 chapter 6 producer and consumer threads the producer thread will alternate between sleeping for a random period of time and inserting a random integer into the buffer random numbers will be produced using the rand   function  which produces random integers between 0 and rand..max the consumer will also sleep for a random period of time and  upon awakening  will attempt to remove an item from the buffer an outline of the producer and consumer threads appears in figure 6.29 in the following sections  we first cover details specific to pthreads and then describe details of the win32 api pthreads thread creation creating threads using the pthreads api is discussed in chapter 4 please refer to that chapter for specific instructions regarding creation of the producer and consumer using pthreads # include stdlib.h i required for rand   i # include buffer.h void producer  void pararn   buffer_item item ;  while  true   i sleep for a random period of time i sleep    ; i generate a random number i item = rand   ; if  insert_item  item   fprintf  report error condition  ; else printf  producer produced % d \ n ,item  ; void consumer  void pararn   buffer_item item ;  while  true   i sleep for a random period of time i sleep    ; if  remove_item  &item   fprintf  report error condition  ; else printf  consumer consumed % d \ n ,item  ; figure 6.29 an outline of the producer and consumer threads # include pthread.h pthread_mutex_t mutex ; i create the mutex lock i pthread_mutex_init  &mutex,null  ; i acquire the mutex lock i pthread_mutex_lock  &mutex  ; i critical section i i release the mutex lock i pthread_mutex_unlock  &mutex  ; figure 6.30 code sample pthreads mutex locks 277 the code sample depicted in figure 6.30 illustrates how mutex locks available in the pthread api can be used to protect a critical section pthreads uses the pthread_mutex_t data type for mutex locks a mutex is created with the pthread_mutex_ini t  &mutex  null  function  with the first parameter being a pointer to the mutex by passing null as a second parameter  we initialize the mutex to its default attributes the mutex is acquired and released with the pthread_mutex_lock   and pthread_mutex_unlock   functions if the mutex lock is unavailable when pthread_mutex_lock   is invoked  the callil1.g thread is blocked until the owner invokes pthread_mutex_unlock 0 all mutex ftmctions return a value of 0 with correct operation ; if an error occurs  these functions return a nonzero error code pthreads semaphores pthreads provides two types of semaphores-named and unnamed for this project  we use unnamed semaphores the code below illush ates how a semaphore is created  # include semaphore.h sem_t sem ; i create the semaphore and initialize it to 5 i sem_init  &sem  0  5  ; the sem_ini t   creates and initializes a semaphore this function is passed three parameters  a a pointer to the semaphore b a flag indicating the level of sharing c the semaphore 's initial value 278 chapter 6 # include semaphore.h sem_t mutex ; i create the semaphore i sem_init  &mutex  0  1  ; i acquire the semaphore i sem_wait  &mutex  ; i critical section i i release the semaphore i sem_post  &mutex  ; figure 6.31 aaa5 in this example  by passing the flag 0  we are indicating that this semaphore can only be shared by threads belonging to the same process that created the semaphore a nonzero value would allow other processes to access the semaphore as well in this example  we initialize the semaphore to the value 5 in section 6.5  we described the classical wait   and signal   semaphore operations pthreads names the wait   and signal   operations sem_wai t   and sem_post   ,respectively the code example shown in figure 6.31 creates a binary semaphore mutex with an initial value of 1 and illustrates its use in protecting a critical section win32 details concerning thread creation using the win32 api are available in chapter 4 please refer to that chapter for specific instructions win32 mutex locks mutex locks are a type of dispatcher object  as described in section 6.8.2 the following illustrates how to create a mutex lock using the createmutex   function  # include windows.h handle mutex ; mutex = createmutex  null  false  null  ; the first parameter refers to a security attribute for the mutex lock by setting this attribute to null  we are disallowing any children of the process creating this mutex lock to inherit the handle of the mutex the second parameter indicates whether the creator of the mutex is the initial owner of the mutex lock passing a value off alse indicates that the thread creating the mutex is not the initial owner ; we shall soon see how mutex locks are acquired the third parameter allows naming of 279 the mutex however  because we provide a value of null  we do not name the mutex if successful  createmutex   returns a handle to the mutex lock ; otherwise  it returns null in section 6.8.2  we identified dispatcher objects as being either signaled or nonsignaled a signaled object is available for ownership ; once a dispatcher object  such as a mutex lock  is acquired  it moves to the nonsignaled state when the object is released  it returns to signaled mutex locks are acquired by invoking the wai tforsingleobj ect   function  passing the function the handle to the lock and a flag indicating how long to wait the following code demonstrates how the mutex lock created above can be acquired  waitforsingleobject  mutex  infinite  ; the parameter value infinite indicates that we will wait an infinite amount of time for the lock to become available other values could be used that would allow the calling thread to time out if the lock did not become available within a specified time if the lock is in a signaled state  wai tforsingleobj ect   returns immediately  and the lock becomes nonsignaled a lock is released  moves to the signaled state  by invoking re leas emu t ex    such as  releasemutex  mutex  ; win32 semaphores semaphores in the win32 api are also dispatcher objects and thus use the same signaling mechanism as mutex locks semaphores are created as follows  # include windows.h handle sem ; sem = createsemaphore  null  1  5  null  ; the first and last parameters identify a security attribute and a name for the semaphore  similar to what was described for mutex locks the second and third parameters indicate the initial value and maximum value of the semaphore in this instance  the initial value of the semaphore is 1  and its maximum value is 5 if successful  createsemaphore   returns a handle to the mutex lock ; otherwise  it returns null semaphores are acquired with the same wai tforsingleobj ect   function as mutex locks we acquire the semaphore sem created in this example by using the statement  waitforsingleobject  semaphore  infinite  ; if the value of the semaphore is 0  the semaphore is in the signaled state and thus is acquired by the calling thread otherwise  the calling thread blocks indefinitely-as we are specifying infinite-until the semaphore becomes signaled 280 chapter 6 the equivalent of the signal   operation on win32 semaphores is the releasesemaphore   function this function is passed three parameters  a the handle of the semaphore b the amount by which to increase the value of the semaphore c a pointer to the previous value of the semaphore we can increase sem by 1 using the following statement  releasesemaphore  sem  1  ~ ll  ; both releasesemaphore   and releasemutex   return nonzero if successful and zero otherwise the mutual-exclusion problem was first discussed in a classic paper by dijkstra  1965a   dekker 's algorithm  exercise 6.1  -the first correct software solution to the two-process mutual-exclusion problem-was developed by the dutch mathematician t dekker this algorithm also was discussed by dijkstra  1965a   a simpler solution to the two-process mutual-exclusion problem has since been presented by peterson  1981   figure 6.2   dijkstra  1965b  presented the first solution to the mutual-exclusion problem for n processes this solution  however  does not have an upper bound on the amount of time a process must wait before it is allowed to enter the critical section knuth  1966  presented the first algorithm with a bound ; his bound was 211 turns a refinement of knuth 's algorithm by debruijn  1967  reduced the waiting time to n2 turns  after which eisenberg and mcguire  1972  succeeded in reducing the time to the lower bound of n-1 turns another algorithm that also requires n-1 turns but is easier to program and to understand is the bakery algorithm  which was developed by lamport  1974   burns  1978  developed the hardware-solution algorithm that satisfies the bounded-waiting requirement general discussions concerning the mutual-exclusion problem were offered by lamport  1986  and lamport  1991   a collection of algorithms for mutual exclusion was given by raynal  1986   the semaphore concept was suggested by dijkstra  1965a   patil  1971  examined the question of whether semaphores can solve all possible synchronization problems parnas  1975  discussed some of the flaws in patil 's arguments kosaraju  1973  followed up on patil 's work to produce a problem that can not be solved by wait   and signal   operations lipton  1974  discussed the limitations of various synchronization primitives the classic process-coordination problems that we have described are paradigms for a large class of concurrency-control problems the boundedbuffer problem  the dining-philosophers problem  and the sleeping-barber problem  exercise 6.36  were suggested by dijkstra  1965a  and dijkstra  1971   the cigarette-smokers problem  exercise 6.22 was developed by patil  1971   the readers-writers problem was suggested by courtois et al  1971   the 281 issue of concurrent reading and writing was discussed by lamport  1977   the problem of synchronization of independent processes was discussed by lamport  1976   the critical-region concept was suggested by hoare  1972  and by brinchhansen  1972   the monitor concept was developed by brinch-hansen  1973   a complete description of the monitor was given by hoare  1974   kessels  1977  proposed an extension to the monitor to allow automatic signalil1.g experience obtained from the use of monitors in concurrent programs was discussed by lampson and redell  1979   they also examined the priority inversion problem general discussions concerning concurrent programming were offered by ben-ari  1990  and birrell  1989   optimizing the performance of lockil1.g primitives has been discussed in many works  such as lamport  1987   mellor-crummey and scott  1991   and anderson  1990   the use of shared objects that do not require the use of critical sections was discussed in herlihy  1993   bershad  1993   and kopetz and reisinger  1993   novel hardware instructions and their utility in implementing synchronization primitives have been described in works such as culler et al  1998   goodman et al  1989   barnes  1993   and herlihy and moss  1993   some details of the locking mechanisms used in solaris were presented in mauro and mcdougall  2007   note that the locking mechanisms used by the kernel are implemented for user-level threads as well  so the same types of locks are available inside and outside the kernel details of windows 2000 synchronization can be found in solomon and russinovich  2000   goetz et al  2006  presents a detailed discussion of concurrent programming in java as well as the java util concurrent package the write-ahead log scheme was first mtroduced in system r by gray et al  1981   the concept of serializability was formulated by eswaran et al  1976  in connection with their work on concurrency control for system r the two-phase locking protocol was introduced by eswaran et al  1976   the timestampbased concurrency-control scheme was provided by reed  1983   various timestamp-based concurrency-control algorithms were explail1.ed by bernstem and goodman  1980   adl-tabatabai et al  2007  discusses transactional memory 7.1 ch er in a multiprogramming environment  several processes may compete for a finite number of resources a process requests resources ; if the resources are not available at that time  the process enters a waiting state sometimes  a waiting process is never again able to change state  because the resources it has requested are held by other waiting processes this situation is called a deadlock we discussed this issue briefly in chapter 6 in cmmection with semaphores perhaps the best illustration of a deadlock can be drawn from a law passed by the kansas legislature early in the 20th century it said  in part  when two trains approach each other at a crossing  both shall come to a full stop and neither shall start up again until the other has gone in this chapter  we describe methods that an operating system can use to prevent or deal with deadlocks although some applications can identify programs that may deadlock  operating systems typically do not provide deadlock-prevention facilities  and it remains the responsibility of programmers to ensure that they design deadlock-free programs deadlock problems can only become more common  given current trends  including larger numbers of processes  multithreaded programs  many more resources withirt a system  and an emphasis on long-lived file and database servers rather than batch systems to develop a description of deadlocks  which prevent sets of concurrent processes from completing their tasks to present a number of different methods for preventing or avoiding deadlocks in a computer system a system consists of a finite number of resources to be distributed among a number of competing processes the resources are partitioned into several 283 284 chapter 7 types  each consisting of some number of identical instances memory space  cpu cycles  files  and i/0 devices  such as printers and dvd drives  are examples of resource types if a system has two cpus  then the resource type cpu has two instances similarly  the resource type printer may have five instances if a process requests an instance of a resource type  the allocation of any instance of the type will satisfy the request if it will not  then the instances are not identical  and the resource type classes have not been defined properly for example  a system may have two printers these two printers may be defined to be in the same resource class if no one cares which printer prints which output however  if one printer is on the ninth floor and the other is in the basement  then people on the ninth floor may not see both printers as equivalent  and separate resource classes may need to be defined for each printer a process must request a resource before using it and must release the resource after using it a process may request as many resources as it requires to carry out its designated task obviously  the number of resources requested may not exceed the total number of resources available in the system in other words  a process can not request three printers if the system has only two under the normal mode of operation  a process may utilize a resource in only the following sequence  request the process requests the resource if the request can not be granted immediately  for example  if the resource is being used by another process   then the requesting process must wait until it can acquire the resource use the process can operate on the resource  for example  if the resource is a printer  the process can print on the printer   release the process releases the resource the request and release of resources are system calls  as explained in chapter 2 examples are the request   and release   device  open   and close   file  and allocate   and free   memory system calls request and release of resources that are not managed by the operating system can be accomplished through the wait   and signal   operations on semaphores or through acquisition and release of a mutex lock for each use of a kernelmanaged resource by a process or thread  the operating system checks to make sure that the process has requested and has been allocated the resource a system table records whether each resource is free or allocated ; for each resource that is allocated  the table also records the process to which it is allocated if a process requests a resource that is currently allocated to another process  it can be added to a queue of processes waiting for this resource a set of processes is in a deadlocked state when every process in the set is waiting for an event that can be caused only by another process in the set the events with which we are mainly concerned here are resource acquisition and release the resources may be either physical resources  for example  printers  tape drives  memory space  and cpu cycles  or logical resources  for example  files  semaphores  and monitors   however  other types of events may result in deadlocks  for example  the ipc facilities discussed in chapter 3   to illustrate a deadlocked state  consider a system with three cd rw drives suppose each of three processes holds one of these cd rw drives if each process 7.2 7.2 285 now requests another drive  the three processes will be in a deadlocked state each is waiting for the event cd rw is released  which can be caused only by one of the other waiting processes this example illustrates a deadlock involving the same resource type deadlocks may also involve different resource types for example  consider a system with one printer and one dvd drive suppose that process p ; is holding the dvd and process pi is holding the printer if p ; requests the printer and p1 requests the dvd drive  a deadlock occurs a programmer who is developing multithreaded applications must pay particular attention to this problem multithreaded programs are good candidates for deadlock because multiple threads can compete for shared resources in a deadlock  processes never finish executing  and system resources are tied up  preventing other jobs from starting before we discuss the various methods for dealing with the deadlock problem  we look more closely at features that characterize deadlocks 7.2.1 necessary conditions a deadlock situation can arise if the following four conditions hold simultaneously in a system  mutual exclusion at least one resource must be held in a nonsharable mode ; that is  only one process at a time can use the resource if another deadlock with mutex locks let 's see how deadlock can occur in a multithreaded pthread program using mutex locks the pthread....mutex_ini t   function initializes an unlocked mutex mutex locks are acquired and released using pthread....mutex_lock   and pthread....mutex_unlock    respectively if a thread attempts to acquire a locked mutex  the call to pthread....mutex_lock 0 blocks the thread until the owner of the mutex lock invokes pthread....mutex_unlock    two mutex locks are created in the following code example  i create and initialize the mutex locks i pthread....mutex_t first....mutex ; pthread....mutex_t second_nmtex ; pthread....mutex_ini t  &first....mutex  null  ; pthread....mutex_ini t  &second....mutex  null  ; next  two threads-thread_one and thread_two-'-are created  and both these threads have access to both mutex locks thread_one and thread_ two run in the functions do_work_one   and do_work_two    respectively  as shown in figure 7.1 286 chapter 7 deadlock with mutex locks  continued  i thread_one runs in this function i void do_work_one  void param    pthread_mutex_lock  &first_mutex  ; pthread_mutex_lock  &second_mutex  ; i do some work i pthread_mutex  _unlock  &second_mutex  ; pthread_mutex_unlock  &first_mutex  ; pthread_exit  0  ; i thread_two runs in this function i void do_work_two  void param    pthread_mutex_lock  &second_mutex  ; pthread_mutex_lock  &first_mutex  ; i do some work i pthread_mutex_unlock  &first_mutex  ; pthread_mutex_unlock  &second_mutex  ; pthread_exi t  0  ; figure 7.1 deadlock example in this example  thread_one attempts to acquire the mutex locks in the order  1  first_mutex   2  second_mutex  while thread_two attempts to acquire the mutex locks in the order  1  second__mutex   2  first_mutex deadlock is possible if thread_one acquires first __mutex while thread_ two aacquites second__mutex note that  even though deadlock is possible  it will not occur if thread_one is able to acquire and release the mutex locks for first_mutex and second_ mutex before thread_two attemptsto acquire the locks this example illustrates a problem with handling deadlocks  it is difficult to identify and test for deadlocks that may occur only under certain circumstances process requests that resource  the requesting process must be delayed until the resource has been released hold and wait a process must be holding at least one resource and waiting to acquire additional resources that are cmrently being held by other processes 7.2 287 no preemption resources can not be preempted ; that is  a resource can be released only voluntarily by the process holding it  after that process has completed its task circular wait a set  p0  pl    p11  of waiting processes must exist such that po is waiting for a resource held by p1  p1 is waiting for a resource held by p2    pn-1 is waiting for a resource held by p,v and p11 is waiting for a resource held by po we emphasize that all four conditions must hold for a deadlock to occur the circular-wait condition implies the hold-and-wait condition  so the four conditions are not completely independent we shall see in section 7.4  however  that it is useful to consider each condition separately 7.2.2 resource-allocation graph deadlocks can be described more precisely in terms of a directed graph called a graph this graph consists of a set of vertices v and a set of edges e the set of vertices vis partitioned into two different types of nodes  p = =  p1  p2    pn   the set consisting of all the active processes in the system  and r = =  r1  r2    rml the set consisting of all resource types in the system a directed edge from process g to resource type rj is denoted by p ;  + rj ; it signifies that process p ; has requested an instance of resource type rj and is currently waiting for that resource a directed edge from resource type rj to process p ; is denoted by r1  + p ; ; it signifies that an instance of resource type r1 has been allocated to process p ;  a directed edge p ;  + rj is called a edge ; a directed edge r1  + p ; is called an pictorially we represent each process p ; as a circle and each resource type rj as a rectangle since resource type ri may have more than one instance  we represent each such instance as a dot within the rectangle note that a request edge points to only the rectangle r1  whereas an assignment edge must also designate one of the dots in the rectangle when process p ; requests an instance of resource type ri  a request edge is inserted in the resource-allocation graph when this request can be fulfilled  the request edge is instantaneously transformed to an assignment edge when the process no longer needs access to the resource  it releases the resource ; as a result  the assignment edge is deleted the resource-allocation graph shown in figure 7.2 depicts the following situation the sets p  k and e  o p = =  p1  p2  p3  or = =  r1  r2  r3  ~  0 e = =  pl + rlf p2 + r3f rl + p2f r2 + p2f r2 + pl  r3 + p3  resource instances  o one instance of resource type r1 o two instances of resource type r2 288 chapter 7 figure 7.2 resource-allocation graph o one instance of resource type r3 o three instances of resource type ~ process states  o process p1 is holding an instance of resource type r2 and is waiting for an instance of resource type r1  o process p2 is holding an instance of r1 and an instance of r2 and is waiting for an instance of r3 o process p3 is holding an instance of r3  given the definition of a resource-allocation graph  it can be shown that  if the graph contains no cycles  then no process in the system is deadlocked if the graph does contain a cycle  then a deadlock may exist if each resource type has exactly one instance  then a cycle implies that a deadlock has occurred if the cycle involves only a set of resource types  each of which has only a single instance  then a deadlock has occurred each process involved in the cycle is deadlocked in this case  a cycle in the graph is both a necessary and a sufficient condition for the existence of deadlock if each resource type has several instances  then a cycle does not necessarily imply that a deadlock has occurred in this case  a cycle in the graph is a necessary but not a sufficient condition for the existence of deadlock to illustrate this concept  we return to the resource-allocation graph depicted in figure 7.2 suppose that process p3 requests an instance of resource type r2 since no resource instance is currently available  a request edge p3  + r2 is added to the graph  figure 7.3   at this point  two minimal cycles exist in the system  p1  + r 1  + p2  + r3  + p3  + r2  + p1 p2  + r3  + p3  + r2  + p2 7.2 deadlock characterization 289 figure 7.3 resource-allocation graph with a deadlock processes p1  pz  and p3 are deadlocked process pz is waiting for the resource r3  which is held by process p3 process p3 is waiting for either process p1 or process pz to release resource r2 in addition  process p1 is waiting for process pz to release resource r1 now consider the resource-allocation graph in figure 7.4 in this example  we also have a cycle  however  there is no deadlock observe that process p4 may release its instance of resource type r2 that resource can then be allocated to p3  breaking the cycle in summary  if a resource-allocation graph does not have a cycle  then the system is not in a deadlocked state if there is a cycle  then the system may or may not be in a deadlocked state this observation is important when we deal with the deadlock problem figure 7.4 resource-allocation graph with a cycle but no deadlock 290 chapter 7 7.3 generally speaking  we can deal with the deadlock problem in one of three ways  we can use a protocol to prevent or avoid deadlocks  ensuring that the system will never enter a deadlocked state we can allow the system to enter a deadlocked state  detect it  and recover we can ignore the problem altogether and pretend that deadlocks never occur in the system the third solution is the one used by most operating systems  including unix and windows ; it is then up to the application developer to write programs that handle deadlocks next  we elaborate briefly on each of the three methods for handling deadlocks then  in sections 7.4 through 7.7  we present detailed algorithms before proceeding  we should mention that some researchers have argued that none of the basic approaches alone is appropriate for the entire spectrum of resource-allocation problems in operating systems the basic approaches can be combined  however  allowing us to select an optimal approach for each class of resources in a system to ensure that deadlocks never occur  the prevention or a deadlock-avoidance scheme provides a set of methods for ensuring that at least one of the necessary conditions  section 7.2.1  can not hold these methods prevent deadlocks by constraining how requests for resources can be made we discuss these methods in section 7.4 requires that the operating system be given in advance additional information concerning which resources a process will request and use during its lifetime with this additional knowledge  it can decide for each request whether or not the process should wait to decide whether the current request can be satisfied or must be delayed  the system must consider the resources currently available  the resources currently allocated to each process  and the future requests and releases of each process we discuss these schemes in section 7.5 if a system does not employ either a deadlock-prevention or a deadlockavoidance algorithm  then a deadlock situation may arise in this environment  the system can provide an algorithm that examines the state of the system to determine whether a deadlock has occurred and an algorithm to recover from the deadlock  if a deadlock has indeed occurred   we discuss these issues in section 7.6 and section 7.7 in the absence of algorithms to detect and recover from deadlocks  we may arrive at a situation in which the system is in a deadlock state yet has no way of recognizing what has happened in this case  the undetected deadlock will result in deterioration of the system 's performance  because resources are being held by processes that can not run and because more and more processes  as they make requests for resources  will enter a deadlocked state eventually  the system will stop functioning and will need to be restarted manually 7.4 7.4 291 although this method may not seem to be a viable approach to the deadlock problem  it is nevertheless used in most operating systems  as mentioned earlier in many systems  deadlocks occur infrequently  say  once per year  ; thus  this method is cheaper than the prevention  avoidance  or detection and recovery methods  which must be used constantly also  in some circumstances  a system is in a frozen state but not in a deadlocked state we see this situation  for example  with a real-time process running at the highest priority  or any process running on a nonpreemptive scheduler  and never returning control to the operating system the system must have manual recovery methods for such conditions and may simply use those techniques for deadlock recovery as we noted in section 7.2.1  for a deadlock to occur  each of the four necessary conditions must hold by ensuring that at least one of these conditions can not hold  we can prevent the occurrence of a deadlock we elaborate on this approach by examining each of the four necessary conditions separately 7.4.1 mutual exclusion the mutual-exclusion condition must hold for nonsharable resources for example  a printer can not be simultaneously shared by several processes sharable resources  in contrast  do not require mutually exclusive access and thus can not be involved in a deadlock read-only files are a good example of a sharable resource if several processes attempt to open a read-only file at the same time  they can be granted simultaneous access to the file a process never needs to wait for a sharable resource in general  however  we can not prevent deadlocks by denying the mutual-exclusion condition  because some resources are intrinsically nonsharable 7.4.2 hold and wait to ensure that the hold-and-wait condition never occurs in the system  we must guarantee that  whenever a process requests a resource  it does not hold any other resources one protocol that can be used requires each process to request and be allocated all its resources before it begins execution we can implement this provision by requiring that system calls requesting resources for a process precede all other system calls an alternative protocol allows a process to request resources only when it has none a process may request some resources and use them before it can request any additional resources  however  it must release all the resources that it is currently allocated to illustrate the difference between these two protocols  we consider a process that copies data from a dvd drive to a file on disk  sorts the file  and then prints the results to a printer if all resources must be requested at the beginning of the process  then the process must initially request the dvd drive  disk file  and printer it will hold the printer for its entire execution  even though it needs the printer only at the end the second method allows the process to request initially only the dvd drive and disk file it copies from the dvd drive to the disk and then releases 292 chapter 7 both the dvd drive and the disk file the process must then again request the disk file and the printer after copying the disk file to the printer  it releases these two resources and terminates both these protocols have two main disadvantages first  resource utilization may be low  since resources may be allocated but unused for a long period in the example given  for instance  we can release the dvd drive and disk file  and then again request the disk file and printe1 ~ only if we can be sure that our data will remain on the disk file otherwise  we must request all resources at the beginning for both protocols second  starvation is possible a process that needs several popular resources may have to wait indefinitely  because at least one of the resources that it needs is always allocated to some other process 7.4.3 no preemption the third necessary condition for deadlocks is that there be no preemption of resources that have already been allocated to ensure that this condition does not hold  we can use the following protocol if a process is holding some resources and requests another resource that can not be immediately allocated to it  that is  the process must wait   then all resources the process is currently holding are preempted in other words  these resources are implicitly released the preempted resources are added to the list of resources for which the process is waiting the process will be restarted only when it can regain its old resources  as well as the new ones that it is requesting alternatively  if a process requests some resources  we first check whether they are available if they are  we allocate them if they are not  we check whether they are allocated to some other process that is waiting for additional resources if so  we preempt the desired resources from the waiting process and allocate them to the requesting process if the resources are neither available nor held by a waiting process  the requesting process must wait while it is waiting  some of its resources may be preempted  but only if another process requests them a process can be restarted only when it is allocated the new resources it is requesting and recovers any resources that were preempted while it was waiting this protocol is often applied to resources whose state can be easily saved and restored later  such as cpu registers and memory space it can not generally be applied to such resources as printers and tape drives 7 .4.4 circular wait the fourth and final condition for deadlocks is the circular-wait condition one way to ensure that this condition never holds is to impose a total ordering of all resource types and to require that each process requests resources in an increasing order of enumeration to illustrate  we let r =  r1  r2    rm  be the set of resource types we assign to each resource type a unique integer number  which allows us to compare two resources and to determine whether one precedes another in our ordering formally  we define a one-to-one hmction f  r ___ n  where n is the set of natural numbers for example  if the set of resource types r includes tape drives  disk drives  and printers  then the function f might be defined as follows  7.4 f  tape drive  = 1 f  disk drive  = 5 f  printer  = 12 293 we can now consider the following protocol to prevent deadlocks  each process can request resources only in an increasing order of enumeration that is  a process can initially request any number of instances of a resource type -say  r ;  after that  the process can request instances of resource type rj if and only if f  rj  f  r ;   for example  using the function defined previously  a process that wants to use the tape drive and printer at the same time must first request the tape drive and then request the printer alternatively  we can require that a process requesting an instance of resource type rj must have released any resources r ; such that f  ri    =   f  rj   it must also be noted that if several iilstances of the same resource type are needed  a single request for all of them must be issued if these two protocols are used  then the circular-wait condition can not hold we can demonstrate this fact by assuming that a circular wait exists  proof by contradiction   let the set of processes involved in the circular wait be  p0  p1    p11   where pi is waiting for a resource r ;  which is held by process pi + l  modulo arithmetic is used on the indexes  so that p11 is waiting for a resource r11 held by p0   then  since process pi + l is holding resource ri while requesting resource ri + l ' we must have f  ri  f  r ; h  for all i but this condition means that f  ro  f  r1   f  r11  f  ro   by transitivity  f  ro  f  ro   which is impossible therefore  there can be no circular wait we can accomplish this scheme in an application program by developing an ordering among all synchronization objects in the system all requests for synchronization objects must be made in increasing order for example  if the lock ordering in the pthread program shown in figure 7.1 was f  first_mutex  = 1 f  second_mutex  = 5 then thread_ two could not request the locks out of order keep in mind that developing an ordering  or hierarchy  does not in itself prevent deadlock it is up to application developers to write programs that follow the ordering also note that the function f should be defined according to the normal order of usage of the resources in a system for example  because the tape drive is usually needed before the printer  it would be reasonable to define f  tape drive  f  printer   although ensuring that resources are acquired in the proper order is the responsibility of application developers  certain software can be used to verify that locks are acquired in the proper order and to give appropriate warnings when locks are acquired out of order and deadlock is possible one lock-order verifier  which works on bsd versions of unix such as freebsd  is known as witness witness uses mutual-exclusion locks to protect critical sections  as described in chapter 6 ; it works by dynamically maintaining the relationship of lock orders in a system let 's use the program shown in figure 7.1 as an example assume that thread_one is the first to acquire the locks and does so in the order  1  first_mutex   2  second_mutex wih1ess records the relationship that first_mutex must be acquired before second_mutex if thread_two later 294 chapter 7 7.5 acquires the locks out of order  witness generates a warning message on the system console it is also important to note that imposing a lock ordering does not guarantee deadlock prevention if locks can be acquired dynamically for example  assume we have a function that transfers funds between two accounts to prevent a race condition  each account has an associated semaphore that is obtained from a get lock   function such as the following  void transaction  account from  account to  double amount    semaphore lock1  lock2 ; lock1 getlock  from  ; lock2 = getlock  to  ; wait  lock1  ; wait  lock2  ; withdraw  from  amount  ; deposit  to  amount  ; signal  lock2  ; signal  lock1  ; deadlock is possible if two threads simultaneously invoke the trans action   function  transposing different accounts that is  one thread might invoke transaction  checkingaccount  savingsaccount  25  ; and another might invoke transaction  savingsaccount  checkingaccount  50  ; we leave it as an exercise for students to fix this situation deadlock-prevention algorithms  as discussed in section 7.4  prevent deadlocks by restraining how requests can be made the restraints ensure that at least one of the necessary conditions for deadlock can not occur and  hence  that deadlocks can not hold possible side effects of preventing deadlocks by this method  however  are low device utilization and reduced system throughput an alternative method for avoiding deadlocks is to require additional information about how resources are to be requested for example  in a system with one tape drive and one printer  the system might need to know that process p will request first the tape drive and then the printer before releasing both resources  whereas process q will request first the printer and then the tape drive with this knowledge of the complete sequence of requests and releases for each process  the system can decide for each request whether or not the process should wait in order to avoid a possible future deadlock each request requires that in making this decision the system consider the resources 7.5 deadlock avoidance 295 currently available  the resources currently allocated to each process  and the future requests and releases of each process the various algorithms that use this approach differ in the amount and type of information required the simplest and most useful model requires that each process declare the maximum number of resources of each type that it may need given this a priori information  it is possible to construct an algorithm that ensures that the system will never enter a deadlocked state such an algorithm defines the deadlock-avoidance approach a deadlock-avoidance algorithm dynamically examines the resource-allocation state to ensure that a circularwait condition can never exist the resource-allocation state is defined by the number of available and allocated resources and the maximum demands of the processes in the following sections  we explore two deadlock-avoidance algorithms 7.5.1 safe state a state is safe if the system can allocate resources to each process  up to its maximum  in some order and still avoid a deadlock more formally  a system is in a safe state only if there exists a safe sequence a sequence of processes p1  p2    pn is a safe sequence for the current allocation state if  for each pi  the resource requests that pi can still make can be satisfied by the currently available resources plus the resources held by all pj  with j i in this situation  if the resources that pi needs are not immediately available  then pi can wait until all pj have finished when they have finished  pi can obtain all of its needed resources  complete its designated task  return its allocated resources  and terminate when pi terminates  pi + l can obtain its needed resources  and so on if no such sequence exists  then the system state is said to be unsafe a safe state is not a deadlocked state conversely  a deadlocked state is an unsafe state not all unsafe states are deadlocks  however  figure 7.5   an unsafe state may lead to a deadlock as long as the state is safe  the operating system can avoid unsafe  and deadlocked  states in an unsafe state  the operating system can not prevent processes from requesting resources in such a way that a deadlock occurs the behavior of the processes controls unsafe states figure 7.5 safe  unsafe  and deadlocked state spaces 296 chapter 7 deadlocks to illustrate  we consider a system with twelve magnetic tape drives and three processes  po  p1  and p2 process po requires ten tape drives  process p1 may need as many as four tape drives  and process p2 may need up to nine tape drives suppose that  at time to  process po is holding five tape drives  process p1 is holding two tape drives  and process p2 is holding two tape drives  thus  there are three free tape drives  maximum needs current needs 10 4 9 5 2 2 at time t0  the system is in a safe state the sequence p1  p0  p2 satisfies the safety condition process p1 can immediately be allocated all its tape drives and then return them  the system will then have five available tape drives  ; then process po can get all its tape drives and return them  the system will then have ten available tape drives  ; and finally process p2 can get all its tape drives and return them  the system will then have all twelve tape drives available   a system can go from a safe state to an unsafe state suppose that  at time t1  process p2 requests and is allocated one more tape drive the system is no longer in a safe state at this point  only process p1 can be allocated all its tape drives when it returns them  the system will have only four available tape drives since process po is allocated five tape drives but has a maximum of ten  it may request five more tape drives if it does so  it will have to wait  because they are unavailable similarly  process p2 may request six additional tape drives and have to wait  resulting in a deadlock our mistake was in granting the request from process p2 for one more tape drive if we had made p2 wait until either of the other processes had finished and released its resources  then we could have avoided the deadlock given the concept of a safe state  we can define avoidance algorithms that ensure that the system will never deadlock the idea is simply to ensure that the system will always remain in a safe state initially  the system is in a safe state whenever a process requests a resource that is currently available  the system must decide whether the resource can be allocated immediately or whether the process must wait the request is granted only if the allocation leaves the system in a safe state in this scheme  if a process requests a resource that is currently available  it may still have to wait thus  resource utilization may be lower than it would otherwise be 7.5.2 resource-allocation-graph algorithm if we have a resource-allocation system with only one instance of each resource type  we can use a variant of the resource-allocation graph defined in section 7.2.2 for deadlock avoidance in addition to the request and assignment edges already described  we introduce a new type of edge  called a claim edge a claim edge pi ~ rj indicates that process pi may request resource rj at some time in the future this edge resembles a request edge in direction but is represented in the graph by a dashed line when process pi requests resource 7.5 297 figure 7.6 resource-allocation graph for deadlock avoidance r1  the claim edge p ;  + r1 is converted to a request edge similarly  when a resource r1 is released by p ;  the assignment edge rj  + p ; is reconverted to a claim edge p ;  + rj we note that the resources must be claimed a priori in the system that is  before process p ; starts executing  all its claim edges must already appear in the resource-allocation graph we can relax this condition by allowing a claim edge p ;  + r1 to be added to the graph only if all the edges associated with process p ; are claim edges now suppose that process p ; requests resource rj the request can be granted only if converting the request edge p ;  + rj to an assignment edge r1  + p ; does not result in the formation of a cycle in the resource-allocation graph we check for safety by using a cycle-detection algorithm an algorithm for detecting a cycle in this graph requires an order of n2 operations  where n is the number of processes in the system if no cycle exists  then the allocation of the resource will leave the system in a safe state if a cycle is found  then the allocation will put the system in an unsafe state in that case  process p ; will have to wait for its requests to be satisfied to illustrate this algorithm  we consider the resource-allocation graph of figure 7.6 suppose that p2 requests r2  although r2 is currently free  we can not allocate it to p2  since this action will create a cycle in the graph  figure 7.7   a cycle  as mentioned  indicates that the system is in an unsafe state if p1 requests r2  and p2 requests r1  then a deadlock will occur figure 7.7 an unsafe state in a resource-allocation graph 298 chapter 7 7.5.3 banker 's algorithm the resource-allocation-graph algorithm is not applicable to a resourceallocation system with multiple instances of each resource type the deadlockavoidance algorithm that we describe next is applicable to such a system but is less efficient than the resource-allocation graph scheme this algorithm is commonly known as the banker 's algorithm the name was chosen because the algorithm could be used in a banking system to ensure that the bank never allocated its available cash in such a way that it could no longer satisfy the needs of all its customers when a new process enters the system  it must declare the maximum number of instances of each resource type that it may need this nun1.ber may not exceed the total number of resources in the system when a user requests a set of resources  the system must determine whether the allocation of these resources will leave the system in a safe state if it will  the resources are allocated ; otherwise  the process must wait until some other process releases enough resources several data structures must be maintained to implement the banker 's algorithm these data structures encode the state of the resource-allocation system we need the following data structures  where n is the number of processes in the system and m is the number of resource types  available a vector of length m indicates the number of available resources of each type if available  j  equals k  then k instances of resource type ri are available max an n x m matrix defines the maximum demand of each process if max  i   j  equals k  then process p ; may request at most k instances of resource type ri allocation an 11 x m matrix defines the number of resources of each type currently allocated to each process if allocation  i   j  equals lc  then process p ; is currently allocated lc instances of resource type rj need an n x m matrix indicates the remaining resource need of each process if need  i   j  equals k  then process p ; may need k more instances of resource type ri to complete its task note that need  i   j  equals max  i   j   allocation  i   j   these data structures vary over time in both size and value to simplify the presentation of the banker 's algorithm  we next establish some notation let x andy be vectors of length 11 we say that x   = y if and only if x  i    = y  i  for all i = 1  2    n for example  if x =  1,7,3,2  and y =  0,3,2,1   then y   = x in addition  y x if y   = x andy # x we can treat each row in the matrices allocation and need as vectors and refer to them as allocation ; and need ;  the vector allocation ; specifies the resources currently allocated to process p ; ; the vector need ; specifies the additional resources that process p ; may still request to complete its task 7.5.3.1 safety algorithm we can now present the algorithm for finding out whether or not a systern is in a safe state this algorithm can be described as follows  7.5 299 let work and finish be vectors of length m and n  respectively initialize work = available and finish  i  = false for i = 0  1    n  1 find an index i such that both a finish  i  = = false b need ;   ; work if no such i exists  go to step 4 work = work + allocation ; finish  i  = true go to step 2 if finish  i  = = true for all i  then the system is in a safe state this algorithm may require an order of m x n2 operations to determine whether a state is safe 7.5.3.2 resource-request algorithm next  we describe the algorithm for determining whether requests can be safely granted let request ; be the request vector for process p ;  if request ;  j  = = k  then process p ; wants k instances of resource type rj when a request for resources is made by process p ;  the following actions are taken  if request ;     ; need ;  go to step 2 otherwise  raise an error condition  since the process has exceeded its maximum claim if request ;   ; available  go to step 3 otherwise  p ; must wait  since the resources are not available have the system pretend to have allocated the requested resources to process p ; by modifyil1.g the state as follows  available = available request ; ; allocation ; = allocation ; + request ; ; need ; = need ;  request ; ; if the resulting resource-allocation state is safe  the transaction is completed  and process p ; is allocated its resources however  if the new state is unsafe  then p ; must wait for request ;  and the old resource-allocation state is restored 7.5.3.3 an illustrative example to illustrate the use of the banker 's algorithm  consider a system with five processes po through p4 and three resource types a  b  and c resource type a has ten instances  resource type b has five instances  and resource type c has seven instances suppose that  at time t0  the following snapshot of the system has been taken  300 chapter 7 allocation max available abc abc abc po 010 753 332 pl 200 322 p2 302 902 p3 2 11 222 p4 002 433 the content of the matrix need is defined to be max  allocation and is as follows  need abc po 743 pl 122 p2 600 p3 011 p4 431 we claim that the system is currently in a safe state indeed  the sequence plt p3  p4  p2  po satisfies the safety criteria suppose now that process p1 requests one additional instance of resource type a and two instances of resource type c  so request1 =  1,0,2   to decide whether this request can be immediately granted  we first check that request1 s available-that is  that  1,0,2  s  3,3,2   which is true we then pretend that this request has been fulfilled  and we arrive at the following new state  allocation need available abc abc abc po 010 743 230 pl 302 020 p2 302 600 p3 211 0 11 p4 002 431 we must determine whether this new system state is safe to do so  we execute our safety algorithm and find that the sequence p1  p3  p4  po  p2 satisfies the safety requirement hence  we can immediately grant the request of process p1 you should be able to see  however  that when the system is in this state  a request for  3,3,0  by p4 can not be granted  since the resources are not available furthermore  a request for  0,2,0  by po can not be granted  even though the resources are available  since the resulting state is unsafe we leave it as a programming exercise for students to implement the banker 's algorithm 7.6 7.6 301 if a system does not employ either a deadlock-prevention or a deadlockavoidance algorithm  then a deadlock situation may occur in this environment  the system may provide  an algorithm that examines the state of the system to determine whether a deadlock has occurred an algorithm to recover from the deadlock in the following discussion  we elaborate on these two requirements as they pertain to systems with only a single instance of each resource type  as well as to systems with several instances of each resource type at this point  however  we note that a detection-and-recovery scheme requires overhead that includes not only the run-time costs of maintaining the necessary information and executing the detection algorithm but also the potential losses inherent in recovering from a deadlock 7.6.1 single instance of each resource type if all resources have only a single instance  then we can define a deadlockdetection algorithm that uses a variant of the resource-allocation graph  called a wait-for graph we obtain this graph from the resource-allocation graph by removing the resource nodes and collapsing the appropriate edges more precisely  an edge from pi to pi in a wait-for graph implies that process pz is waiting for process p1 to release a resource that p ; needs an edge pz  + pi exists iil a wait-for graph if and only if the corresponding resourceallocation graph contains two edges pz  + rq and rq  + pi for some resource rq for example  in figure 7.8  we present a resource-allocation graph and the corresponding wait-for graph as before  a deadlock exists in the system if and only if the wait-for graph contains a cycle to detect deadlocks  the system needs to maintain the wait-for graph and periodically invoke an algorithm that searches for a cycle in the graph an algorithm to detect a cycle in a graph requires an order of n2 operations  where n is the number of vertices in the graph 7.6.2 several instances of a resource type the wait-for graph scheme is not applicable to a resource-allocation system with multiple instances of each resource type we turn now to a deadlockdetection algorithm that is applicable to such a system the algorithm employs several time-varying data structures that are similar to those used in the banker 's algorithm  section 7.5.3   available a vector of length nz indicates the number of available resources of each type allocation ann x nz matrix defines the number of resources of each type currently allocated to each process 302 chapter 7  a   b  figure 7.8  a  resource-allocation graph  b  corresponding wait-for graph request an n x m matrix indicates the current request of each process if request  i   j  equals k  then process p ; is requesting k more instances of resource type rj the     relation between two vectors is defined as in section 7.5.3 to simplify notation  we again treat the rows in the matrices allocation and request as vectors ; we refer to them as allocation ; and request ;  the detection algorithm described here simply investigates every possible allocation sequence for the processes that remain to be completed compare this algorithm with the banker 's algorithm of section 7.5.3 let work and finish be vectors of length m and n  respectively initialize work = available fori = 0  1    n-1  if allocation ; # 0  then finish  i  = false ; otherwise  finish  i  = tme 2 find an index i such that both a finish  i  = = false b request ;     work if no such i exists  go to step 4 work = work + allocation ; finish  i  = true go to step 2 4 if finish  i  = = false for some i  0     i n  then the system is in a deadlocked state moreover  if finish  i  = = false  then process p ; is deadlocked this algorithm requires an order o m x n2 operations to detect whether the system is in a deadlocked state 7.6 303 you may wonder why we reclaim the resources of process p ;  in step 3  as soon as we determine that request ;  s work  in step 2b   we know that p ; is currently not involved in a deadlock  since request ;  s work   thus  we take an optimistic attitude and assume that p ; will require no more resources to complete its task ; it will thus soon return all currently allocated resources to the system if our assumption is incorrect  a deadlock may occur later that deadlock will be detected the next tince the deadlock-detection algorithm is invoked to illustrate this algorithm  we consider a system with five processes po through p4 and three resource types a  b  and c resource type a has seven instances  resource type b has two instances  and resource type c has six instances suppose that  at time t0  we have the following resource-allocation state  allocation request available abc abc abc po 0 1 0 000 000 pl 200 202 p2 303 000 p3 2 11 100 p4 002 002 we claim that the system is not in a deadlocked state indeed  if we execute our algorithm  we will find that the sequence po  p2  p3  plt p4 results in finish  i  = = true for all i suppose now that process p2 makes one additional request for an instance of type c the request matrix is modified as follows  request abc po 000 pl 202 p2 001 p3 100 p4 002 we claim that the system is now deadlocked although we can reclaim the resources held by process po  the number of available resources is not sufficient to fulfill the requests of the other processes thus  a deadlock exists  consisting of processes p1  p2  p3  and p4 7.6.3 detection-algorithm usage when should we invoke the detection algorithm the answer depends on two factors  1 how often is a deadlock likely to occur how many processes will be affected by deadlock when it happens 304 chapter 7 7.7 if deadlocks occur frequently  then the detection algorithm should be invoked frequently resources allocated to deadlocked processes will be idle until the deadlock can be broken in addition  the number of processes involved in the deadlock cycle may grow deadlocks occur only when some process makes a request that can not be granted immediately this request may be the final request that completes a chain of waiting processes in the extreme  then  we can invoke the deadlockdetection algorithm every time a request for allocation can not be granted immediately in this case  we can identify not only the deadlocked set of processes but also the specific process that caused the deadlock  in reality  each of the deadlocked processes is a link in the cycle in the resource graph  so all of them  jointly  caused the deadlock  if there are many different resource types  one request may create many cycles in the resource graph  each cycle completed by the most recent request and caused by the one identifiable process of course  invoking the deadlock-detection algorithm for every resource request will incur considerable overhead in computation time a less expensive alternative is simply to invoke the algorithm at defined intervals-for example  once per hour or whenever cpu utilization drops below 40 percent  a deadlock eventually cripples system throughput and causes cpu utilization to drop  if the detection algorithm is invoked at arbitrary points in time  the resource graph may contain many cycles in this case  we generally can not tell which of the many deadlocked processes caused the deadlock when a detection algorithm determines that a deadlock exists  several alternatives are available one possibility is to inform the operator that a deadlock has occurred and to let the operator deal with the deadlock manually another possibility is to let the system recover from the deadlock automatically there are two options for breaking a deadlock one is simply to abort one or more processes to break the circular wait the other is to preempt some resources from one or more of the deadlocked processes 7.7.1 process termination to eliminate deadlocks by aborting a process  we use one of two methods in both methods  the system reclaims all resources allocated to the terminated processes abort all deadlocked processes this method clearly will break the deadlock cycle  but at great expense ; the deadlocked processes may have computed for a long time  and the results of these partial computations must be discarded and probably will have to be recomputed later abort one process at a time until the deadlock cycle is eliminated this method incurs considerable overhead  since after each process is aborted  a deadlock-detection algorithnc rnust be invoked to determine whether any processes are still deadlocked 7.7 305 aborting a process may not be easy if the process was in the midst of updating a file  terminating it will leave that file in an incorrect state similarly  if the process was in the midst of printing data on a printer  the system must reset the printer to a correct state before printing the next job if the partial termination method is used  then we must determine which deadlocked process  or processes  should be terminated this determination is a policy decision  similar to cpu-scheduling decisions the question is basically an economic one ; we should abort those processes whose termination will incur the minimum cost unfortunately  the term minimum cost is not a precise one many factors may affect which process is chosen  including  1 what the priority of the process is 2 how long the process has computed and how much longer the process will compute before completing its designated task how many and what types of resources the process has used  for example  whether the resources are simple to preempt  how many more resources the process needs in order to complete 5 how many processes will need to be terminated whether the process is interactive or batch 7.7.2 resource preemption to eliminate deadlocks using resource preemption  we successively preempt some resources from processes and give these resources to other processes 1-m til the deadlock cycle is broken if preemption is required to deal with deadlocks  then three issues need to be addressed  selecting a victim which resources and which processes are to be preempted as in process termil ation  we must determine the order of preemption to minimize cost cost factors may include such parameters as the number of resources a deadlocked process is holding and the amount of time the process has thus far consumed during its execution rollback if we preempt a resource from a process  what should be done with that process clearly  it can not contil ue with its normal execution ; it is missing some needed resource we must roll back the process to some safe state and restart it from that state since  in general  it is difficult to determine what a safe state is  the simplest solution is a total rollback  abort the process and then restart it although it is more effective to roll back the process only as far as necessary to break the deadlock  this method requires the system to keep more information about the state of all running processes starvation how do we ensure that starvation will not occur that is  how can we guarantee that resources will not always be preempted from the same process 306 chapter 7 7.8 in a system where victim selection is based primarily on cost factors  it may happen that the same process is always picked as a victim as a result  this process never completes its designated task  a starvation situation that must be dealt with in any practical system clearly  we must ensure that a process can be picked as a victim only a  small  finite number of times the most common solution is to include the number of rollbacks in the cost factor a deadlocked state occurs when two or more processes are waiting indefinitely for an event that can be caused only by one of the waiting processes there are three principal methods for dealing with deadlocks  use some protocol to prevent or avoid deadlocks  ensuring that the system will never enter a deadlocked state allow the system to enter a deadlocked state  detect it  and then recover ignore the problem altogether and pretend that deadlocks never occur in the system the third solution is the one used by most operating systems  including unix and windows a deadlock can occur only if four necessary conditions hold simultaneously in the system  mutual exclusion  hold and wait  no preemption  and circular wait to prevent deadlocks  we can ensure that at least one of the necessary conditions never holds a method for avoiding deadlocks  rather than preventing them  requires that the operating system have a priori information about how each process will utilize system resources the banker 's algorithm  for example  requires a priori information about the maximunl number of each resource class that each process may request using this information  we can define a deadlockavoidance algorithm if a system does not employ a protocol to ensure that deadlocks will never occur  then a detection-and-recovery scheme may be employed a deadlockdetection algorithm must be invoked to detennine whether a deadlock has occurred if a deadlock is detected  the system must recover either by terminating some of the deadlocked processes or by preempting resources from some of the deadlocked processes where preemption is used to deal with deadlocks  three issues must be addressed  selecting a victim  rollback  and starvation in a system that selects victims for rollback primarily on the basis of cost factors  starvation may occur  and the selected process can never complete its designated task researchers have argued that none of the basic approaches alone is appropriate for the entire spectrum of resource-allocation problems in operating systems the basic approaches can be combined  however  allowing us to select an optimal approach for each class of resources in a system 307 7.1 a single-lane bridge connects the two vermont villages of north tunbridge and south tunbridge farmers in the two villages use this bridge to deliver their produce to the neighboring town the bridge can become deadlocked if a northbound and a southbound farmer get on the bridge at the same time  vermont farmers are stubborn and are unable to back up  using semaphores  design an algorithm that prevents deadlock initially  do not be concerned about starvation  the situation in which northbound farmers prevent southbound farmers from using the bridge  or vice versa   7.2 modify your solution to exercise 7.1 so that it is starvation-free 7.3 consider a system consisting of four resources of the same type that are shared by three processes  each of which needs at most two resources show that the system is deadlock free 7.4 consider the traffic deadlock depicted in figure 7.9 a show that the four necessary conditions for deadlock hold in this example b state a simple rule for avoiding deadlocks in this system 7.5 in a real computer system  neither the resources available nor the demands of processes for resources are consistent over long periods  months   resources break or are replaced  new processes come and go  and new resources are bought and added to the system if deadlock is controlled by the banker 's algorithm  which of the following changes figure 7.9 traffic deadlock for exercise 7.4 308 chapter 7 can be made safely  without introducing the possibility of deadlock   and under what circumstances a increase available  new resources added   b decrease available  resource permanently removed from system   c increase max for one process  the process needs or wants rnore resources than allowed   d decrease max for one process  the process decides it does not need that many resources   e increase the number of processes f decrease the number of processes 7.6 we can obtain the banker 's algorithm for a single resource type from the general banker 's algorithm simply by reducing the dimensionality of the various arrays by 1 show through an example that we can not implement the multiple-resource-type banker 's scheme by applying the sil1.gle-resource-type scheme to each resource type individually 7.7 consider the following resource-allocation policy requests for and releases of resources are allowed at any time if a request for resources can not be satisfied because the resources are not available  then we check any processes that are blocked waiting for resources if a blocked process has the desired resources  then these resources are taken away from it and are given to the requestmg process the vector of resources for which the blocked process is waiting is increased to include the resources that were taken away for example  consider a system with three resource types and the vector available initialized to  4,2,2   if process po asks for  2,2,1   it gets them if p1 asks for  1,0,1   it gets them then  if po asks for  0,0,1   it is blocked  resource not available   if p2 now asks for  2,0,0   it gets the available one  1,0,0  and one that was allocated to po  since po is blocked   po 's allocation vector goes down to  1,2,1   and its need vector goes up to  1,0,1   a can deadlock occur if you answer yes  give an example if you answer no  specify which necessary condition can not occur b can indefinite blocking occur explain your answer 7.8 a possible method for preventing deadlocks is to have a single  higherorder resource that must be requested before any other resource for example  if multiple threads attempt to access the synchronization objects a e  deadlock is possible  such synchronization objects may include mutexes  semaphores  condition variables  and the like  we can prevent the deadlock by adding a sixth object f whenever a thread wants to acquire the synchronization lock for any object a e  it must first acquire the lock for object f this solution is known as containment  the locks for objects a e are contained within the lock for object f compare this scheme with the circular-wait scheme of section 7.4.4 309 7.9 compare the circular-wait scheme with the various deadlock-avoidance schemes  like the banker 's algorithnc  with respect to the following issues  a runtime overheads b system throughput 7.10 consider the following snapshot of a system  allocation max available  abcd abcd abcd po 0012 0012 1520 pl 1000 1750 p2 1354 2356 p3 0632 0652 p4 0014 0656 answer the following questions using the banker 's algorithm  a what is the content of the matrix need b is the system in a safe state c if a request from process p1 arrives for  0,4,2,0   can the request be granted immediately 7.11 consider a system consisting of m resources of the same type being shared by n processes a process can request or release only one resource at a time show that the system is deadlock free if the following two conditions hold  a the maximum need of each process is between one resource and m resources b the sum of all maximum needs is less than m + n 7.12 consider a computer system that runs 5,000 jobs per month and has no deadlock-prevention or deadlock-avoidance scheme deadlocks occur about twice per month  and the operator must terminate and rerun about 10 jobs per deadlock each job is worth about $ 2  in cpu time   and the jobs terminated tend to be about half-done when they are aborted a systems programmer has estimated that a deadlock-avoidance algorithm  like the banker 's algorithm  could be installed in the system with an increase in the average execution time per job of about 10 percent since the machine currently has 30 percent idle time  all 5,000 jobs per month could still be run  although turnaround time would increase by about 20 percent on average a what are the arguments for installing the deadlock-avoidance algorithm b what are the arguments against installing the deadlock-avoidance algorithm 310 chapter 7 7.13 consider the deadlock situation that can occur in the diningphilosophers problem when the philosophers obtain the chopsticks one at a time discuss how the four necessary conditions for deadlock hold in this setting discuss how deadlocks could be avoided by eliminating any one of the four necessary conditions 7.14 what is the optimistic assumption made in the deadlock-detection algorithm how can this assumption be violated 7.15 consider the version of the dining-philosophers problem in which the chopsticks are placed at the center of the table and any two of them can be used by a philosopher assume that requests for chopsticks are made one at a time describe a simple rule for determining whether a particular request can be satisfied without causing deadlock given the current allocation of chopsticks to philosophers 7.16 is it possible to have a deadlock involving only a single process explain your answer 7.17 consider again the setting in the preceding question assume now that each philosopher requires three chopsticks to eat resource requests are still issued one at a time describe some simple rules for determining whether a particular request can be satisfied without causing deadlock given the current allocation of chopsticks to philosophers 7.18 in section 7.4.4  we describe a situation in which we prevent deadlock by ensuring that all locks are acquired in a certain order however  we also point out that deadlock is possible in this situation if two threads simultaneously invoke the transaction   function fix the transaction   function to prevent deadlocks 7.19 write a multithreaded program that implements the banker 's algorithm discussed in section 7.5.3 create n threads that request and release resources from the bank the banker will grant the request only if it leaves the system in a safe state you may write this program using either pthreads or win32 threads it is important that shared data be safe from concurrent access to ensure safe access to shared data  you can use mutex locks  which are available in both the pthreads and win32 apis the use of mutex locks in both of these libraries is described in the project entitled producer-consumer problem at the end of chapter 6 dijkstra  1965a  was one of the first and most influential contributors in the deadlock area holt  1972  was the first person to formalize the notion of deadlocks in terms of an allocation-graph model similar to the one presented in this chapter starvation was also covered by holt  1972   hyman  1985  provided the deadlock example from the kansas legislature a recent study of deadlock handling is provided in levine  2003   311 the various prevention algorithms were suggested by havender  1968   who devised the resource-ordering scheme for the ibm os/360 systen'l the banker 's algorithm for avoiding deadlocks was developed for a single resource type by dijkstra  1965a  and was extended to multiple resource types by habermam'l  1969   exercises 7.3 and 7.11 are from holt  1971   the deadlock-detection algorithm for multiple instances of a resource type  which is described in section 7.6.2  was presented by coffman et al  1971   bach  1987  describes how many of the algorithms in the traditional unix kernel handle deadlock solutions to deadlock problems in networks are discussed in works such as culler et al  1998  and rodeheffer and schroeder  1991   the witness lock-order verifier is presented in baldwin  2002   part four the main purpose of a computer system is to execute programs these programs  together with the data they access  must be at least partially in main memory during execution to improve both the utilization of the cpu and the speed of its response to users  a general-purpose computer must keep several processes in memory many memory-management schemes exist  reflecting various approaches  and the effectiveness of each algorithm depends on the situation selection of a memory-management scheme for a system depends on many factors  especially on the hardware design of the system most algorithms require hardware support 8.1 c in chapter 5  we showed how the cpu can be shared by a set of processes as a result of cpu scheduling  we can improve both the utilization of the cpu and the speed of the computer 's response to its users to realize this increase in performance  however  we must keep several processes in memory ; that is  we must share memory in this chapter  we discuss various ways to manage memory the memorymanagement algorithms vary from a primitive bare-machine approach to paging and segmentation strategies each approach has its own advantages and disadvantages selection of a memory-management method for a specific system depends on many factors  especially on the hardware design of the system as we shall see  many algorithms require hardware support  although recent designs have closely integrated the hardware and operating system to provide a detailed description of various ways of organizing memory hardware to discuss various memory-management techniques  including paging and segmentation to provide a detailed description of the intel pentium  which supports both pure segmentation and segmentation with paging as we saw in chapter 1  memory is central to the operation of a modern computer system memory consists of a large array of words or bytes  each with its own address the cpu fetches instructions from memory according to the value of the program counter these instructions may cause additional loading from and storing to specific memory addresses a typical instruction-execution cycle  for example  first fetches an instruction from memory the instruction is then decoded and may cause operands to be fetched from memory after the instruction has been executed on the 315 316 chapter 8 operands  results may be stored back in memory the mernory unit sees only a stream of memory addresses ; it does not know how they are generated  by the instruction counter  indexing  indirection  literal addresses  and so on  or what they are for  instructions or data   accordingly  we can ignore hozu a program generates a memory address we are interested only in the sequence of memory addresses generated by the running program we begin our discussion by covering several issues that are pertinent to the various techniques for managing memory this coverage includes an overview of basic hardware issues  the binding of symbolic memory addresses to actual physical addresses  and the distinction between logical and physical addresses we conclude the section with a discussion of dynamically loading and linking code and shared libraries 8.1.1 basic hardware main memory and the registers built into the processor itself are the only storage that the cpu can access directly there are machine instructions that take memory addresses as arguments  but none that take disk addresses therefore  any instructions in execution  and any data being used by the instructions  must be in one of these direct-access storage devices if the data are not in memory  they must be moved there before the cpu can operate on them registers that are built into the cpu are generally accessible within one cycle of the cpu clock most cpus can decode instructions and perform simple operations on register contents at the rate of one or more operations per clock tick the same can not be said of main memory  which is accessed via a transaction on the memory bus completing a memory access may take many cycles of the cpu clock in such cases  the processor normally needs to stall  since it does not have the data required to complete the instruction that it is executing this situation is intolerable because of the frequency of memory accesses the remedy is to add fast memory between the cpu and 0 operating system 256000 process 300040 i soa  lj.o i process base 420940 i 120 ! 1go i i  limit process 880000 1024000 figure 8.1 a base and a limit register define a logical address space 8.1 317 main memory a memory buffer used to accommodate a speed differential  called a is described in section 1.8.3 not only are we concerned with the relative speed of accessing physical memory  but we also must ensure correct operation to protect the operating system from access by user processes and  in addition  to protect user processes from one another this protection must be provided by the hardware it can be implemented in several ways  as we shall see throughout the chapter in this section  we outline one possible implementation we first need to make sure that each process has a separate memory space to do this  we need the ability to determine the range of legal addresses that the process may access and to ensure that the process can access only these legal addresses we can provide this protection by using two registers  usually a base and a limit  as illustrated in figure 8.1 the base holds the smallest legal physical memory address ; the specifies the size of the range for example  if the base register holds 300040 and the limit register is 120900  then the program can legally access all addresses from 300040 through 420939  inclusive   protection of memory space is accomplished by having the cpu hardware compare every address generated in user mode with the registers any attempt by a program executing in user mode to access operating-system memory or other users ' memory results in a trap to the operating system  which treats the attempt as a fatal error  figure 8.2   this scheme prevents a user program from  accidentally or deliberately  modifying the code or data structures of either the operating system or other users the base and limit registers can be loaded only by the operating system  which uses a special privileged instruction since privileged instructions can be executed only in kernel mode  and since only the operating system executes in kernel mode  only the operating system can load the base and limit registers this scheme allows the operating system to change the value of the registers but prevents user programs from changing the registers ' contents the operating system  executing in kernel mode  is given unrestricted access to both operating system memory and users ' memory this provision allows the operating system to load users ' programs into users ' memory  to yes no trap to operating system monitor-addressing error memory figure 8.2 hardware address protection with base and limit registers 318 chapter 8 dump out those programs in case of errors  to access and modify parameters of system calls  and so on 8.1.2 address binding usually  a program resides on a disk as a binary executable file to be executed  the program must be brought into memory and placed within a process depending on the memory management in use  the process may be moved between disk and memory during its execution the processes on the disk that are waiting to be brought into memory for execution form the the normal procedure is to select one of the processes in the input queue and to load that process into memory as the process is executed  it accesses instructions and data from memory eventually  the process terminates  and its memory space is declared available most systems allow a user process to reside in any part of the physical memory thus  although the address space of the computer starts at 00000  the first address of the user process need not be 00000 this approach affects the addresses that the user program can use in most cases  a user program will go through several steps-some of which may be optional-before bein.g executed  figure 8.3   addresses may be represented in different ways during these steps addresses in the source program are generally symbolic  such as count   a compiler will typically bind these symbolic addresses to relocatable addresses  such as 14 bytes from the beginning of this module   the lin.kage editor or loader will in turn bind the relocatable addresses to absolute addresses  such as 74014   each binding is a mapping from one address space to another classically  the binding of instructions and data to memory addresses can be done at any step along the way  compile time if you know at compile time where the process will reside in memory  then can be generated for example  if you krww that a user process will reside starting at location r  then the generated compiler code will start at that location and extend up from there if  at some later time  the starting location changes  then it will be necessary to recompile this code the ms-dos .com-format programs are bound at compile time load time if it is not known at compile time where the process will reside in memory  then the compiler must generate in this case  final binding is delayed until load time if the starting address changes  we need only reload the user code to incorporate this changed value execution time if the process can be moved during its execution from one memory segment to another  then binding must be delayed until run time special hardware must be available for this scheme to work  as will be discussed in section 8.1.3 most general-purpose operating systems 11se this method a major portion of this chapter is devoted to showing how these various bindings can be implemented effectively in a computer system and to discussing appropriate hardware support 8.1 compile time load time  execution time  run time  figure 8.3 multistep processing of a user program 8.1.3 logical versus physical address space an address generated by the cpu is commonly referred to as a 319 whereas an address seen by the memory unit-that is  the one loaded into the of the memory-is commonly referred to as a the compile-time and load-time address-binding methods generate identical logical and physical addresses however  the execution-time addressbinding scheme results in differing logical and addresses in this case  we usually refer to the logical address as a we use logical address and virtual address interchangeably in this text the set of all logical addresses generated by a program is a logical the set of all physical addresses corresponding to these logical addresses is a physical thus  in_ the execution-time address-binding scheme  the logical and physical address spaces differ the run-time mapping from virtual to physical addresses is done by a hardware device called the we can choose from many different methods to accomplish such mapping  as we discuss in 320 chapter 8 figure 8.4 dynamic relocation using a relocation register sections 8.3 through 8.7 for the time being  we illustrate this mapping with a simple mmu scheme that is a generalization of the base-register scheme described in section 8.1.1 the base register is now called a the value in the relocation register is added to every address generated by a user process at the time the address is sent to memory  see figure 8.4   for example  if the base is at 14000  then an attempt by the user to address location 0 is dynamically relocated to location 14000 ; an access to location 346 is mapped to location 14346 the ms-dos operating system running on the intel 80x86 family of processors used four relocation registers when loading and running processes the user program never sees the real physical addresses the program can create a pointer to location 346  store it in memory  manipulate it  and compare it with other addresses-all as the number 346 only when it is used as a memory address  in an indirect load or store  perhaps  is it relocated relative to the base register the user program deals with logical addresses the memory-mapping hardware converts logical addresses into physical addresses this form of execution-time binding was discussed in section 8.1.2 the final location of a referenced memory address is not determined until the reference is made we now have two different types of addresses  logical addresses  in the range 0 to max  and physical addresses  in the ranger + 0 tor + max for a base valuer   the user generates only logical addresses and thinks that the process runs in locations 0 to max the user program generates only logical addresses and thinks that the process runs in locations 0 to max however  these logical addresses must be mapped to physical addresses before they are used the concept of a logical address space that is bound to a separate physical address space is central to proper memory management 8.1.4 dynamic loading in our discussion so far  it has been necessary for the entire program and all data of a process to be in physical memory for the process to execute the size of a process has thus been limited to the size of physical memory to obtain better memory-space utilization  we can use dynamic with dynancic 8.1 321 loading  a routine is not loaded until it is called all routines are kept on disk in a relocatable load format the main program is loaded into memory and is executed when a routine needs to call another routine  the calling routine first checks to see whether the other routine has been loaded if it has not  the relocatable linking loader is called to load the desired routine into menwry and to update the program 's address tables to reflect this change then control is passed to the newly loaded routine the advantage of dynamic loading is that an unused routine is never loaded this method is particularly useful when large amounts of code are needed to handle infrequently occurring cases  such as error routines in this case  although the total program size may be large  the portion that is used  and hence loaded  may be much smaller dynamic loading does not require special support from the operating system it is the responsibility of the users to design their programs to take advantage of such a method operating systems may help the programmer  however  by providing library routines to implement dynamic loading 8.1.5 dynamic linking and shared libraries figure 8.3 also shows some operating systems support only linking  in system language libraries are treated like any other object module and are combined by the loader into the binary program image dynamic linking  in contrast  is similar to dynamic loading here  though  linking  rather than loading  is postponed until execution time this feature is usually used with system libraries  such as language subroutine libraries without this facility  each program on a system must include a copy of its language library  or at least the routines referenced by the program  in the executable image this requirement wastes both disk space and main memory with dynamic linking  a stub is included in the image for each libraryroutine reference the stub is a small piece of code that indicates how to locate the appropriate memory-resident library routine or how to load the library if the routine is not already present when the stub is executed  it checks to see whether the needed routine is already in memory if it is not  the program loads the routine into memory either way  the stub replaces itself with the address of the routine and executes the routine thus  the next time that particular code segment is reached  the library routine is executed directly  incurring no cost for dynamic linking under this scheme  all processes that use a language library execute only one copy of the library code this feature can be extended to library updates  such as bug fixes   a library may be replaced by a new version  and all programs that reference the library will automatically use the new version without dynamic linking  all such programs would need to be relinked to gain access to the new library so that programs will not accidentally execute new  incompatible versions of libraries  version information is included in both the program and the library more than one version of a library may be loaded into memory  and each program uses its version information to decide which copy of the library to use versions with minor changes retain the same version number  whereas versions with major changes increment the number thus  only programs that are compiled with the new library version are affected by any incompatible changes incorporated 322 chapter 8 8.2 in it other programs linked before the new library was installed will continue using the older library this system is also known as 'h  = ' unlike dynamic loading  dynamic linking generally requires help from the operating system if the processes in memory are protected from one another  then the operating system is the only entity that can check to see whether the needed routine is in another process 's memory space or that can allow multiple processes to access the same memory addresses we elaborate on this concept when we discuss paging in section 8.4.4 a process must be in memory to be executed a process  however  can be temporarily out of memory to a and then brought into memory for continued execution for example  assume a multiprogramming environment with a round-robin cpu-scheduling algorithm when a quantum expires  the memory manager will start to swap out the process that just finished and to swap another process into the memory space that has been freed  figure 8.5   in the meantime  the cpu scheduler will allocate a time slice to some other process in memory when each process finishes its quantum  it will be swapped with another process ideally  the memory manager can swap processes fast enough that some processes will be in memory  ready to execute  when the cpu scheduler wants to reschedule the cpu in addition  the quantum must be large enough to allow reasonable amounts of computing to be done between swaps a variant of this swapping policy is used for priority-based scheduling algorithms if a higher-priority process arrives and wants service  the memory manager can swap out the lower-priority process and then load and execute the higher-priority process when the higher-priority process finishes  the @ swap out @ swap in backing store main memory figure 8.5 swapping of two processes using a disk as a backing store 8.2 323 lower-priority process can be swapped back in and continued this variant of swapping is sometimes called roll normally  a process that is swapped out will be swapped back into the same memory space it occupied previously this restriction is dictated by the method of address binding if binding is done at assembly or load time  then the process can not be easily moved to a different location if execution-time binding is being used  however  then a process can be swapped into a different memory space  because the physical addresses are computed during execution time swapping requires a backing store the backing store is commonly a fast disk it must be large enough to accommodate copies of all memory images for all users  and it must provide direct access to these memory images the system maintains a consisting of all processes whose memory images are on the backing store or in memory and are ready to run whenever the cpu scheduler decides to execute a process  it calls the dispatcher the dispatcher checks to see whether the next process in the queue is in memory if it is not  and if there is no free memory region  the dispatcher swaps out a process currently in memory and swaps in the desired process it then reloads registers and transfers control to the selected process the context-switch time in such a swapping system is fairly high to get an idea of the context-switch time  let us assume that the user process is 100 mb in size and the backing store is a standard hard disk with a transfer rate of 50mb per second the actual transfer of the 100-mb process to or from main memory takes 100mb/50mb per second = 2 seconds assuming an average latency of 8 milliseconds  the swap time is 2008 milliseconds since we must both swap out and swap in  the total swap time is about 4016 milliseconds notice that the major part of the swap time is transfer time the total transfer time is directly proportional to the amount of memory swapped if we have a computer system with 4 gb of main memory and a resident operating system taking 1 gb  the maximum size of the user process is 3gb however  many user processes may be much smaller than this-say  100 mb a 100-mb process could be swapped out in 2 seconds  compared with the 60 seconds required for swapping 3 gb clearly  it would be useful to know exactly how much memory a user process is using  not simply how much it might be using then we would need to swap only what is actually used  reducing swap time for this method to be effective  the user must keep the system informed of any changes in memory requirements thus  a process with dynamic memory requirements will need to issue system calls  request memory and release memory  to inform the operating system of its changing memory needs swapping is constrained by other factors as well if we want to swap a process  we must be sure that it is completely idle of particular concern is any pending i/0 a process may be waiting for an i/0 operation when we want to swap that process to free up memory however  if the i/0 is asynchronously accessing the user memory for i/0 buffers  then the process can not be swapped assume that the i/0 operation is queued because the device is busy if we were to swap out process p1 and swap in process p2  the 324 chapter 8 8.3 i/0 operation might then attempt to use memory that now belongs to process p2  there are two main solutions to this problem  never swap a process with pending i/0  or execute i/0 operations only into operating-system buffers transfers between operating-system buffers and process memory then occur only when the process is swapped in the assumption  mentioned earlier  that swapping requires few  if any  head seeks needs further explanation we postpone discussing this issue until chapter 12  where secondary-storage structure is covered generally  swap space is allocated as a chunk of disk  separate from the file system  so that its use is as fast as possible currently  standard swapping is used in few systems it requires too much swapping time and provides too little execution time to be a reasonable memory-management solution modified versions of swapping  however  are found on many systems a modification of swapping is used in many versions of unix swapping is normally disabled but will start if many processes are running and are using a threshold amount of memory swapping is again halted when the load on the system is reduced memory management in unix is described fully in sections 21.7 and a.6 early pcs-which lacked the sophistication to implement more advanced memory-management methods-ran multiple large processes by using a modified version of swapping a prime example is the microsoft windows 3.1 operating system  which supports concurrent execution of processes in memory if a new process is loaded and there is insufficient main memory  an old process is swapped to disk this operating system does not provide full swapping  however  because the user  rather than the scheduler  decides when it is time to preempt one process for another any swapped-out process remains swapped out  and not executing  until the user selects that process to run subsequent versions of microsoft operating systems take advantage of the advanced mmu features now found in pcs we explore such features in section 8.4 and in chapter 9  where we cover virtual memory the main memory must accommodate both the operating system and the various user processes we therefore need to allocate main menlory in the most efficient way possible this section explains one common method  contiguous memory allocation the memory is usually divided into two partitions  one for the resident operating system and one for the user processes we can place the operating system in either low memory or high memory the major factor affecting this decision is the location of the interrupt vector since the interrupt vector is often in low memory  programmers usually place the operating system in low memory as well thus  in this text  we discuss only the situation in which the operating system resides in low memory the development of the other situation is similar we usually want several user processes to reside in memory at the same time we therefore need to consider how to allocate available memory to the processes that are in the input queue waiting to be brought into memory 8.3 325 in contiguous memory allocation  each process is contained in a single contiguous section of memory 8.3.1 memory mapping and protection before discussing memory allocation further  we must discuss the issue of memory mapping and protection we can provide these features by using a relocation register  as discussed in section 8.1.3  together with a limit register  as discussed in section 8.1.1 the relocation register contaitls the value of the smallest physical address ; the limit register contains the range of logical addresses  for example  relocation = 100040 and limit = 74600   with relocation and limit registers  each logical address must be less than the limit register ; the mmu maps the logical address dynamically by adding the value in the relocation register this mapped address is sent to memory  figure 8.6   when the cpu scheduler selects a process for execution  the dispatcher loads the relocation and limit registers with the correct values as part of the context switch because every address generated by a cpu is checked against these registers  we can protect both the operating system and the other users ' programs and data from being modified by this running process the relocation-register scheme provides an effective way to allow the operating system 's size to change dynamically this flexibility is desirable in many situations for example  the operating system contains code and buffer space for device drivers if a device driver  or other operating-system service  is not commonly used  we do not want to keep the code and data in memory  as we might be able to use that space for other purposes such code is sometimes called transient operating-system code ; it comes and goes as needed thus  using this code changes the size of the operating system during program execution 8.3.2 memory allocation now we are ready to turn to memory allocation one of the simplest methods for allocating memory is to divide memory into several fixed-sized each partition may contain exactly one process thus  the degree no trap  addressing error figure 8.6 hardware supportfor relocation and limit registers 326 chapter 8 of multiprogramming is bound by the number of partitions in this when a partition is free  a process is selected from the input queue and is loaded into the free partition when the process terminates  the partition becomes available for another process this method was originally used by the ibm os/360 operating system  called mft  ; it is no longer in use the method described next is a generalization of the fixed-partition scheme  called mvt  ; it is used primarily in batch environments many of the ideas presented here are also applicable to a time-sharing environment in which pure segmentation is used for memory management  section 8.6   in the scheme  the operating system keeps a table indicating which parts of memory are available and which are occupied initially  all memory is available for user processes and is considered one large block of available memory a eventually as you will see  memory contains a set of holes of various sizes as processes enter the system  they are put into an input queue the operating system takes into account the memory requirements of each process and the amount of available memory space in determining which processes are allocated memory when a process is allocated space  it is loaded into memory  and it can then compete for cpu time when a process terminates  it releases its memory which the operating system may then fill with another process from the input queue at any given time  then  we have a list of available block sizes and an input queue the operating system can order the input queue according to a scheduling algorithm memory is allocated to processes untit finally  the memory requirements of the next process can not be satisfied -that is  no available block of memory  or hole  is large enough to hold that process the operating system can then wait until a large enough block is available  or it can skip down the input queue to see whether the smaller memory requirements of some other process can be met in generat as mentioned  the memory blocks available comprise a set of holes of various sizes scattered throughout memory when a process arrives and needs memory  the system searches the set for a hole that is large enough for this process if the hole is too large  it is split into two parts one part is allocated to the arriving process ; the other is returned to the set of holes when a process terminates  it releases its block of memory  which is then placed back in the set of holes if the new hole is adjacent to other holes  these adjacent holes are merged to form one larger hole at this point  the system may need to check whether there are processes waiting for memory and whether this newly freed and recombined memory could satisfy the demands of any of these waiting processes this procedure is a particular instance of the general which concerns how to satisfy a request of size n from a there are many solutions to this problem the and strategies are the ones most commonly used to select a free hole from the set of available holes first fit allocate the first hole that is big enough searching can start either at the beginning of the set of holes or at the location where the previous first-fit search ended we can stop searching as soon as we find a free hole that is large enough 8.3 327 best fit allocate the smallest hole that is big enough we must search the entire list  unless the list is ordered by size this strategy produces the smallest leftover hole worst fit allocate the largest hole again  we must search the entire list  unless it is sorted by size this strategy produces the largest leftover hole  which may be more useful than the smaller leftover hole from a best-fit approach simulations have shown that both first fit and best fit are better than worst fit in terms of decreasing time and storage utilization neither first fit nor best fit is clearly better than the other in terms of storage utilization  but first fit is generally faster 8.3.3 fragmentation both the first-fit and best-fit strategies for memory allocation suffer from external as processes are loaded and removed from memory  the free memory space is broken into little pieces external fragmentation exists when there is enough total memory space to satisfy a request but the available spaces are not contiguous ; storage is fragmented into a large number of small holes this fragmentation problem can be severe in the worst case  we could have a block of free  or wasted  memory between every two processes if all these small pieces of memory were in one big free block instead  we might be able to run several more processes whether we are using the first-fit or best-fit strategy can affect the amount of fragmentation  first fit is better for some systems  whereas best fit is better for others  another factor is which end of a free block is allocated  which is the leftover piece-the one on the top or the one on the bottom  no matter which algorithm is used  however  external fragmentation will be a problem depending on the total amount of memory storage and the average process size  external fragmentation may be a minor or a major problem statistical analysis of first fit  for instance  reveals that  even with some optimization  given n allocated blocks  another 0.5 n blocks will be lost to fragmentation that is  one-third of memory may be unusable ! this property is known as the memory fragmentation can be internal as well as external consider a multiple-partition allocation scheme with a hole of 18,464 bytes suppose that the next process requests 18,462 bytes if we allocate exactly the requested block  we are left with a hole of 2 bytes the overhead to keep track of this hole will be substantially larger than the hole itself the general approach to avoiding this problem is to break the physical memory into fixed-sized blocks and allocate memory in units based on block size with this approach  the memory allocated to a process may be slightly larger than the requested memory the difference between these two numbers is internal memory that is internal to a partition one solution to the problem of external fragmentation is the goal is to shuffle the memory contents so as to place all free n'lemory together in one large block compaction is not always possible  however if relocation is static and is done at assembly or load time  compaction can not be done ; compaction is possible only if relocation is dynamic and is done at execution 328 chapter 8 8.4 time if addresses are relocated dynamically  relocation requires only moving the program and data and then changing the base register to reflect the new base address when compaction is possible  we must determine its cost the simplest compaction algorithm is to move all processes toward one end of memory ; all holes move in the other direction  producing one large hole of available memory this scheme can be expensive another possible solution to the external-fragmentation problem is to permit the logical address space of the processes to be noncontiguous  thus allowing a process to be allocated physical memory wherever such memory is available two complementary techniques achieve this solution  paging  section 8.4  and segmentation  section 8.6   these techniques can also be combined  section 8.7   is a memory-management scheme that permits the physical address space a process to be noncontiguous paging avoids external fragmentation and the need for compaction it also solves the considerable problem of fitting memory chunks of varying sizes onto the backin.g store ; most memorymanagement schemes used before the introduction of paging suffered from this problem the problem arises because  when some code fragments or data residing in main memory need to be swapped out  space must be fmmd on the backing store the backing store has the same fragmentation problems discussed in connection with main memory  but access is much slower  so compaction is impossible because of its advantages over earlier methods  paging in its various forms is used in most operating systems physical address foooo  0000 f1111  1111 page table figure 8.7 paging hardware 1---------1 physical memory 8.4 329 traditionally  support for paging has been handled by hardware however  recent designs have implemented paging by closely integrating the hardware and operating system  especially on 64-bit microprocessors 8.4.1 basic method the basic method for implementing paging involves breaking physical memory into fixed-sized blocks called harnes and breaking logical memory into blocks of the same size called when a process is to be executed  its pages are loaded into any available memory frames from their source  a file system or the backing store   the backing store is divided into fixed-sized blocks that are of the san1.e size as the memory frames the hardware support for paging is illustrated in figure 8.7 every address generated the cpu is divided into two parts  a  p  and a  the page number is used as an index into a the page table contains the base address of each page in physical memory this base address is combined with the page offset to define the physical memory address that is sent to the memory unit the paging model of memory is shown in figure 8.8 the page size  like the frame size  is defined by the hardware the size of a page is typically a power of 2  varying between 512 bytes and 16 mb per page  depending on the computer architecture the selection of a power of 2 as a page size makes the translation of a logical address into a page number and page offset particularly easy if the size of the logical address space is 2m  and a page size is 271 addressing units  bytes or wordst then the high-order m n bits of a logical address designate the page number  and the n low-order bits designate the page offset thus  the logical address is as follows  logical memory ~ w page table frame number physical memory figure 8.8 paging model of logical and physical memory 330 chapter 8 page number page offset d m -n n where p is an index into the page table and d is the displacement within the page as a concrete  although minuscule  example  consider the memory in figure 8.9 here  in the logical address  n = 2 and m = 4 using a page size of 4 bytes and a physical memory of 32 bytes  8 pages   we show how the user 's view of memory can be mapped into physical memory logical address 0 is page 0  offset 0 indexing into the page table  we find that page 0 is in frame 5 thus  logical address 0 maps to physical address 20  =  5 x 4  + 0   logical address 3  page 0  offset 3  maps to physical address 23  =  5 x 4  + 3   logical address 4 is page 1  offset 0 ; according to the page table  page 1 is mapped to frame 6 thus  logical address 4 maps to physical address 24  =  6 x 4  + o   logical address 13 maps to physical address 9 you may have noticed that paging itself is a form of dynamic relocation every logical address is bound by the paging hardware to some physical address using paging is similar to using a table of base  or relocation  registers  one for each frame of memory ~ m6 2 1 3 2 page table logical memory physical memory figure 8.9 paging example for a 32-byte memory with 4-byte pages 8.4 331 when we use a paging scheme  we have no external fragmentation  any free frame can be allocated to a process that needs it however  we may have some internal fragmentation notice that frames are allocated as units if the memory requirements of a process do not happen to coincide with page boundaries  the last frame allocated may not be completely full for example  if page size is 2,048 bytes  a process of 72,766 bytes will need 35 pages plus 1,086 bytes it will be allocated 36 frames  resulting in internal fragmentation of 2,048  1,086 = 962 bytes in the worst case  a process would need 11 pages plus 1 byte it would be allocated 11 + 1 frames  resulting in internal fragmentation of almost an entire frame if process size is independent of page size  we expect internal fragmentation to average one-half page per process this consideration suggests that small page sizes are desirable however  overhead is involved in each page-table entry  and this overhead is reduced as the size of the pages increases also  disk i/0 is more efficient when the amount data being transferred is larger  chapter 12   generally  page sizes have grown over time as processes  data sets  and main memory have become larger today  pages typically are between 4 kb and 8 kb in size  and some systems support even larger page sizes some cpus and kernels even support multiple page sizes for instance  solaris uses page sizes of 8 kb and 4 mb  depending on the data stored by the pages researchers are now developing support for variable on-the-fly page size usually  each page-table entry is 4 bytes long  but that size can vary as well a 32-bit entry can point to one of 232 physical page frames if frame size is 4 kb  then a system with 4-byte entries can address 244 bytes  or 16 tb  of physical memory when a process arrives in the system to be executed  its size  expressed in pages  is examined each page of the process needs one frame thus  if the process requires 11 pages  at least 11 frames must be available in memory if n frames are available  they are allocated to this arriving process the first page of the process is loaded injo one of the allocated frames  and the frame number is put in the page table for this process the next page is loaded into another frame  its frame number is put into the page table  and so on  figure 8.10   an important aspect of paging is the clear separation between the user 's view of memory and the actual physical memory the user program views memory as one single space  containing only this one program in fact  the user program is scattered throughout physical memory  which also holds other programs the difference between the user 's view of memory and the actual physical memory is reconciled by the address-translation hardware the logical addresses are translated into physical addresses this mapping is hidden from the user and is controlled by the operating system notice that the user process by definition is unable to access memory it does not own it has no way of addressing memory outside of its page table  and the table includes only those pages that the process owns since the operating system is managing physical memory  it must be aware of the allocation details of physical memory-which frames are allocated  which frames are available  how many total frames there are  and so on this information is generally kept in a data structure called a frame the frame table has one entry for each physical page frame  indicating whether the latter is free or allocated and  if it is allocated  to which page of which process or processes 332 chapter 8 free-frame list free-frame list 14 13 15 13 13 18 20 14 14 15 15 15 16 16 17 17 18 18 19 01 19 1 13 20 2 18 20 3.20 21 new-process page table 21  a   b  figure 8.10 free frames  a  before allocation and  b  after allocation in addition  the operating system must be aware that user processes operate in user space  and all logical addresses must be mapped to produce physical addresses if a user makes a system call  to do i/0  for example  and provides an address as a parameter  a buffe1 ~ for instance   that address must be mapped to produce the correct physical address the operating system maintains a copy of the page table for each process  just as it maintains a copy of the instruction counter and register contents this copy is used to translate logical addresses to physical addresses whenever the operating system must map a logical address to a physical address manually it is also used by the cpu dispatcher to define the hardware page table when a process is to be allocated the cpu paging therefore increases the context-switch time 8.4.2 hardware support each operating system has its own methods for storing page tables most allocate a page table for each process a pointer to the page table is stored with the other register values  like the instruction counter  in the process control block when the dispatcher is told to start a process  it must reload the user registers and define the correct hardware page-table values from the stored user page table the hardware implementation of the page table can be done in several in the simplest case  the page table is implemented as a set of dedicated these registers should be built with very high-speed logic to make the paging-address translation efficient every access to memory nlust go through the paging map  so efficiency is a major consideration the cpu dispatcher reloads these registers  just as it reloads the other registers instructions to load or modify the page-table registers are  of course  privileged  so that only the operating system can change the memory map the dec pdp-11 is an example of such an architecture the address consists of 16 bits  and the page size is 8 kb the page table thus consists of eight entries that are kept in fast registers 8.4 333 the use of registers for the page table is satisfactory if the page table is reasonably sncall  for example  256 entries   most contemporary computers  however  allow the page table to be very large  for example  1 million entries   for these machines  the use of fast registers to implement the page table is not feasible rather  the page table is kept in main memory  and a points to the page table changing page tables requires changing only this one register  substantially reducing context-switch time the problem with this approach is the time required to access a user memory location if we want to access location i  we must first index into the page table  using the value in the ptbr offset by the page number fori this task requires a memory access it provides us with the frame number  which is combined with the page offset to produce the actual address we can then access the desired place in memory with this scheme  two memory accesses are needed to access a byte  one for the page-table entry  one for the byte   thus  memory access is slowed by a factor of 2 this delay would be intolerable under most circumstances we might as well resort to swapping ! the standard solution to this problem is to use a special  small  fastlookup hardware cache  called a bc.1her the tlb is associative  high-speed memory each entry in the tlb consists of two parts  a key  or tag  and a value when the associative memory is presented with an item  the item is compared with all keys simultaneously if the item is found  the corresponding value field is returned the search is fast ; the hardware  however  is expensive typically  the number of entries in a tlb is small  often numbering between 64 and 1,024 the tlb is used with page tables in the following way the tlb contains only a few of the page-table entries when a logical address is generated by the cpu  its page number is presented to the tlb if the page number is found  its frame number is immediately available and is used to access memory the whole task may take less than 10 percent longer than it would if an unmapped memory reference were used if the page number is not in the tlb  known as a a memory reference to the page table must be made when the frame number is obtained  we can use it to access memory  figure 8.11   in addition  we add the page number and frame number to the tlb  so that they will be found quickly on the next reference if the tlb is already full of entries  the operating system must select one for replacement replacement policies range from least recently used  lru  to random furthermore  some tlbs allow certain entries to be meaning that they can not be removed from the tlb typically  tlb entries for kernel code are wired down some tlbs store in each tlb entry an asid uniquely identifies each process and is used to provide address-space protection for that process when the tlb attempts to resolve virtual page numbers  it ensures that the asid for the currently running process matches the asid associated with the virtual page if the asids do not match  the attempt is treated as a tlb miss in addition to providing address-space protection  an asid allows the tlb to contain entries for several different processes simultaneously if the tlb does not support separate asids  then every time a new table is selected  for instance  with each context switch   the tlb must  or erased  to ensure that the next executing process does not use the wrong translation information otherwise  the tlb could include old entries that 334 chapter 8 tlb hit tlb p tlb miss page table figure 8.11 paging hardware with tlb physical memory contain valid virtual addresses but have incorrect or invalid physical addresses left over from the previous process the percentage of times that a particular page number is found in the tlb is called the an 80-percent hit ratio  for example  means that we find the desired page number in the tlb 80 percent of the time if it takes 20 nanoseconds to search the tlb and 100 nanoseconds to access memory  then a mapped-memory access takes 120 nanoseconds when the page number is in the tlb if we fail to find the page number in the tlb  20 nanoseconds   then we must first access memory for the page table and frame number  100 nanoseconds  and then access the desired byte in memory  100 nanoseconds   for a total of 220 nanoseconds to find the effective we weight the case by its probability  effective access time = 0.80 x 120 + 0.20 x 220 = 140 nanoseconds in this example  we suffer a 40-percent slowdown in memory-access time  from 100 to 140 nanoseconds   for a 98-percent hit ratio  we have effective access time = 0.98 x 120 + 0.02 x 220 = 122 nanoseconds this increased hit rate produces only a 22 percent slowdown in access time we will further explore the impact of the hit ratio on the tlb in chapter 9 8.4 335 8.4.3 protection memory protection in a paged environment is accomplished by protection bits associated with each frame normally  these bits are kept in the page table one bit can define a page to be read-write or read-only every reference to memory goes through the page table to find the correct frame nuncber at the same time that the physical address is being computed  the protection bits can be checked to verify that no writes are being made to a read-only page an attempt to write to a read-only page causes a hardware trap to the operating system  or memory-protection violation   we can easily expand this approach to provide a finer level of protection we can create hardware to provide read-only  read-write  or execute-only protection ; or  by providing separate protection bits for each kind of access  we can allow any combination of these accesses illegal attempts will be trapped to the operating system one additional bit is generally attached to each entry in the page table  a bit when this bit is set to valid  the associated page is in the process 's logical address space and is thus a legal  or valid  page when the bit is set to invalid  the page is not in the process 's logical address space illegal addresses are trapped by use of the valid -invalid bit the operating system sets this bit for each page to allow or disallow access to the page suppose  for example  that in a system with a 14-bit address space  0 to 16383   we have a program that should use only addresses 0 to 10468 given a page size of 2 kb  we have the situation shown in figure 8.12 addresses in 0 frame number j valid-invalid bit 0 10,468 1 2,287 '-----'--'--' ' page n figure 8 i 2 valid  v  or invalid  i  bit in a page table 336 chapter 8 pages 0  1  2  3  4  and 5 are mapped normally through the page table any attempt to generate an address in pages 6 or 7  however  will find that the valid -invalid bit is set to invalid  and the computer will trap to flee operating system  invalid page reference   notice that this scheme has created a problem because the program extends only to address 10468  any reference beyond that address is illegal howeve1 ~ references to page 5 are classified as valid  so accesses to addresses up to 12287 are valid only the addresses from 12288 to 16383 are invalid this problem is a result of the 2-kb page size and reflects the internal fragmentation of paging rarely does a process use all its address range in fact many processes use only a small fraction of the address space available to them it would be wasteful in these cases to create a page table with entries for every page in the address range most of this table would be unused but would take up valuable memory space some systems provide hardware  in the form of a length to indicate the size of the page table value is checked against every logical address to verify that the address is in the valid range for the process failure of this test causes an error trap to the operating system 8.4.4 shared pages an advantage of paging is the possibility of sharing common code this consideration is particularly important in a time-sharing environment consider a system that supports 40 users  each of whom executes a text editor if the text editor consists of 150 kb of code and 50 kb of data space  we need 8,000 kb to support the 40 users if the code is  or pure however  it can be shared  as shown in figure 8.13 here we see a three-page editor-each page 50 kb in size  the large page size is used to simplify the figure  -being shared among three processes each process has its own data page reentrant code is non-self-modifying code  it never changes during execution thus  two or more processes can execute the same code at the same time each process has its own copy of registers and data storage to hold the data for the process 's execution the data for two different processes wilt of course  be different only one copy of the editor need be kept in physical memory each user 's page table maps onto the same physical copy of the editor  but data pages are mapped onto different frames thus  to support 40 users  we need only one copy of the editor  150 kb   plus 40 copies of the 50 kb of data space per user the total space required is now 2  50 kb instead of 8,000 kb-a significant savings other heavily used programs can also be shared -compilers  window systems  run-time libraries  database systems  and so on to be sharable  the code must be reentrant the read-only nature of shared code should not be left to the correctness of the code ; the operating system should enforce this property the sharing of memory among processes on a system is similar to the sharing of the address space of a task by threads  described in chapter 4 furthermore  recall that in chapter 3 we described shared memory as a method 8.5 ed 1  ed 2 ed 3 data .1 process p1 process p3 page table for p1 page table for p3 8.5 ed 1 ed 2 ed 3 data 2 process p2 0 data 1 2 data 3 3 ed 1 ed 2 ed 3  4 5 6 data 2 page table for p2 7 8 9 10 11 figure 8.13 sharing of code in a paging environment 337 of interprocess corrununication some operating systems implement shared memory using shared pages organizing memory according to pages provides numerous benefits in addition to allowing several processes to share the same physical pages we cover several other benefits in chapter 9 in this section  we explore some of the most common techniques for structuring the page table 8.5.1 hierarchical paging most modern computer systems support a large logical address space  232 to 264   in such an environment  the page table itself becomes excessively large for example  consider a system with a 32-bit logical address space if the page size in such a system is 4 kb  212   then a page table may consist of up to 1 million entries  232 /212   assuming that each entry consists of 4 bytes  each process may need up to 4mb of physical address space for the page table alone clearly  we would not want to allocate the page table contiguously in main memory one simple solution to this problem is to divide the page table into smaller pieces we can accomplish this division in several ways one way is to use a two-level paging algorithm  in which the page table itself is also paged  figure 8.14   for example  consider again the system with 338 chapter 8 0 page table memory figure 8.14 a two-level page-table scheme a 32-bit logical address space and a page size of 4 kb a logical address is divided into a page number consisting of 20 bits and a page offset consisting of 12 bits because we page the page table  the page number is further divided into a 10-bit page number and a 10-bit page offset thus  a logical address is as follows  page number page offset d 10 10 12 where p1 is an index into the outer page table and p2 is the displacement within the page of the outer page table the address-translation method for this architecture is shown in figure 8.15 because address translation works from the outer page table inward  this scheme is also known as a the vax architecture supports a variation of two-level paging the vax is a 32-bit machine with a page size of 512 bytes the logical address space of a process is divided into four equal sections  each of which consists of 230 bytes each section represents a different part of the logical address space of a process the first 2 high-order bits of the logical address designate the appropriate section the next 21 bits represent the logical page number of that section  and the final 9 bits represent an offset in the desired page by partitioning the page outer page table 8.5 figure 8 15 address translation for a two-level 32-bit paging architecture 339 table in this manner  the operating system can leave partitions unused until a process needs them an address on the vax architecture is as follows  section page offset s p d 2 21 9 where s designates the section number  p is an index into the page table  and d is the displacement within the page even when this scheme is used  the size of a one-level page table for a vax process using one section is 221 bits 4 bytes per entry = 8mb to further reduce main-memory use  the vax pages the user-process page tables for a system with a 64-bit logical address space  a two-level paging scheme is no longer appropriate to illustrate this point  let us suppose that the page size in such a system is 4 kb  212   in this case  the page table consists of up to 252 entries if we use a two-level paging scheme  then the iml.er page tables can conveniently be one page long  or contain 210 4-byte entries the addresses look like this  outer page inner page offset i  pl  i p2  i d 42 10 12 the outer page table consists of 242 entries  or 244 bytes the obvious way to avoid such a large table is to divide the outer page table into smaller pieces  this approach is also used on some 32-bit processors for added flexibility and efficiency  we can divide the outer page table in various ways we can page the outer page table  giving us a three-level paging scheme suppose that the outer page table is made up of standard-size pages  210 entries  or 212 bytes   in this case  a 64-bit address space is still daunting  2nd outer page outer page inner page offset i pr   p2 i p3 i d 32 10 10 12 the outer page table is sti11234 bytes in size 340 chapter 8 the next step would be a four-level paging scheme  where the second-level outer page table itself is also paged  and so forth the 64-bit ultrasparc would require seven levels of paging-a prohibitive number of memory accessesto translate each logical address you can see from this example why  for 64-bit architectures  hierarchical page tables are generally considered inappropriate 8.5.2 hashed page tables a common approach for handling address spaces larger than 32 bits is to use a with the hash value being the virtual page number each entry in the hash table contains a linked list of elements that hash to the same location  to handle collisions   each element consists of three fields   1  the virtual page number   2  the value of the mapped page frame  and  3  a pointer to the next element in the linked list the algorithm works as follows  the virtual page number in the virtual address is hashed into the hash table the virtual page number is compared with field 1 in the first element in the linked list if there is a match  the corresponding page frame  field 2  is used to form the desired physical address if there is no match  subsequent entries in the linked list are searched for a matching virtual page number this scheme is shown in figure 8.16 a variation of this scheme that is favorable for 64-bit address spaces has been proposed this variation uses which are similar to hashed page tables except that each entry in the hash table refers to several pages  such as 16  rather than a single page therefore  a single page-table entry can store the mappings for multiple physical-page frames clustered page tables are particularly useful for address spaces  where memory references are noncontiguous and scattered throughout the address space 8.5.3 inverted page tables usually  each process has an associated page table the page table has one entry for each page that the process is using  or one slot for each virtual hash table figure 8.16 hashed page table physical address physical memory 8.5 341 address  regardless of the latter 's validity   this table representation is a natural one  since processes reference pages through the pages ' virtual addresses the operating system must then translate this reference into a physical memory address since the table is sorted by virtual address  the operating system is able to calculate where in the table the associated physical address entry is located and to use that value directly one of the drawbacks of this method is that each page table may consist of millions of entries these tables may consume large amounts of physical memory just to keep track of how other physical memory is being used to solve this problem  we can use an page an inverted page table has one entry for each real page  or frame  of memory each entry consists of the virtual address of the page stored in that real memory location  with information about the process that owns the page thus  only one page table is in the system  and it has only one entry for each page of physical memory figure 8.17 shows the operation of an inverted page table compare it with figure 8.7  which depicts a standard page table in operation inverted page tables often require that an address-space identifier  section 8.4.2  be stored in each entry of the page table  since the table usually contains several different address spaces mapping physical memory storing the address-space identifier ensures that a logical page for a particular process is mapped to the corresponding physical page frame examples of systems using inverted page tables include the 64-bit ultrasparc and powerpc to illustrate this method  we describe a simplified version of the i11verted page table used in the ibm rt each virtual address in the system consists of a triple  process-id  page-number  offset  each inverted page-table entry is a pair process-id  page-number where the process-id assumes the role of the address-space identifier when a memory page table physical address figure 8.17 inverted page table physical memory 342 chapter 8 8.6 reference occurs  part of the virtual address  consisting of process-id  pagenumber  is presented to the memory subsystem the inverted page table is then searched for a match if a match is found-say  at entry i-then the physical address i  offset is generated if no match is found  then an illegal address access has been attempted although this scheme decreases the amount of memory needed to store each page table  it increases the amount of time needed to search the table when a page reference occurs because the inverted page table is sorted by physical address  but lookups occur on virtual addresses  the whole table might need to be searched for a match this search would take far too long to alleviate this problem  we use a hash table  as described in section 8.5.2  to limit the search to one-or at most a few-page-table entries of course  each access to the hash table adds a memory reference to the procedure  so one virtual memory reference requires at least two real memory reads-one for the hash-table entry and one for the page table  recall that the tlb is searched first  before the hash table is consulted  offering some performance improvement  systems that use inverted page tables have difficulty implementing shared memory shared memory is usually implemented as multiple virtual addresses  one for each process sharing the memory  that are mapped to one physical address this standard method can not be used with inverted page tables ; because there is only one virtual page entry for every physical page  one physical page can not have two  or more  shared virtual addresses a simple technique for addressing this issue is to allow the page table to contain only one mapping of a virtual address to the shared physical address this means that references to virtual addresses that are not mapped result in page faults an important aspect of memory management that became unavoidable with paging is the separation of the user 's view of memory from the actual physical memory as we have already seen  the user 's view of memory is not the same as the actual physical memory the user 's view is mapped onto physical memory this mapping allows differentiation between logical memory and physical memory 8.6.1 basic method do users think of memory as a linear array of bytes  some containing instructions and others containing data most people would say no rather  users prefer to view memory as a collection of variable-sized segments  with no necessary ordering among segments  figure 8.18   consider how you think of a program when you are writing it you think of it as a main program with a set of methods  procedures  or functions it may also include various data structures  objects  arrays  stacks  variables  and so on each of these modules or data elements is referred to by name you talk about the stack  the math library  the n1.ain program  without caring what addresses in memory these elements occupy you are not concerned with whether the stack is stored before or after the sqrt   function each of these segments is of variable length ; the length is intrinsically defined by subroutine symbol table  main program logical address 8.6 figure 8.18 user 's view of a program 343 the purpose of the segment in the program elements within a segment are identified by their offset from the begim1.ing of the segment  the first statement of the program  the seventh stack frame entry in the stack  the fifth instruction of the sqrt    and so on is a memory-management scheme that supports this user view of memory a logical address space is a collection of segments each segment has a name and a length the addresses specify both the segment name and the offset within the segment the user therefore specifies each address by two quantities  a segment name and an offset  contrast this scheme with the paging scheme  in which the user specifies only a single address  which is partitioned by the hardware into a page number and an offset  all invisible to the programmer  for simplicity of implementation  segments are numbered and are referred to by a segn lent number  rather than by a segment name thus  a logical address consists of a two tuple  segment-number  offset  normally  the user program is compiled  and the compiler automatically constructs segments reflecting the input program a c compiler might create separate segments for the following  the code global variables the heap  from which memory is allocated the stacks used by each thread the standard c library 344 chapter 8 no segment table yes trap  addressing error + figure 8.19 segmentation hardware physical memory libraries that are linked in during compile time might be assign.ed separate segments the loader would take all these segments and assign them segment numbers 8.6.2 hardware although the user can now refer to objects in the program by a two-dimensional address  the actual physical memory is still  of course  a one-dimensional sequence of bytes thus  we must define an implementation to map twodimensional user-defined addresses into one-dimensional physical addresses this mapping is effected by a each entry in the segment table has a segment base and a segment limit the segment base contains the startilcg physical address where the segment resides in memory  and the segment limit specifies the length of the segment the use of a segment table is illustrated in figure 8.19 a logical address consists of two parts  a segment number  s  and an offset into that segment  d the segment number is used as an index to the segment table the offset d of the logical address must be between 0 and the segment limit if it is not  we trap to the operating system  logical addressing attempt beyond end of segment   when an offset is legal  it is added to the segment base to produce the address in physical memory of the desired byte the segment table is thus essentially an array of base-limit register pairs as an example  consider the situation shown in figure 8.20 we have five segments numbered from 0 through 4 the segments are stored in physical memory as shown the segment table has a separate entry for each segment  giving the beginning address of the segment in physical memory  or base  and the length of that segment  or limit   for example  segment 2 is 400 bytes long and begins at location 4300 thus  a reference to byte 53 of segment 2 is mapped 8.7 subroutine segment o segment1 symbol table  segment 4 main program segment 2 logical address space 8.7 0 2 3 4 limit base 1000 1400 400 6300 400 4300 1100 3200 1000 4700 segment table figure 8.20 example of segmentation 14001---1 segment o 2400 3200 1-----1 segment 3 4300 1 ~ --1 4700 segment 2 segment 4 5700 f--------1 6300   s ~ gt \ 1e ! it 1 6700 physical memory 345 onto location 4300 + 53 = 4353 a reference to segment 3  byte 852  is mapped to 3200  the base of segment 3  + 852 = 4052 a reference to byte 1222 of segment 0 would result in a trap to the operating system  as this segment is only tooo bytes long both paging and segmentation have advantages and disadvantages in fact some architectures provide both in this section  we discuss the intel pentium architecture  which supports both pure segmentation and segmentation with paging we do not give a complete description of the memory-management structure of the pentium in this text rather  we present the major ideas on which it is based we conclude our discussion with an overview of linux address translation on pentium systems in pentium systems  the cpu generates logical addresses  which are given to the segmentation unit the segmentation unit produces a linear address for each logical address the linear address is then given to the paging unit  which in turn generates the physical address in main memory thus  the segmentation and paging units form the equivalent of the memory-management unit  mmu   this scheme is shown in figure 8.21 8.7.1 pentium segmentation the pentium architecture allows a segment to be as large as 4 gb  and the maximum number of segments per process is 16 k the logical-address space 346 chapter 8 i cpu i figure 8.21 logical to physical address translation in the pentium of a process is divided into two partitions the first partition consists of up to 8 k segments that are private to that process the second partition consists of up to 8 k segments that are shared all the processes information about the first partition is kept in the information about the second partition is kept in the each entry in the ldt and gdt consists of an 8-byte segment descriptor with detailed information about a particular segment  including the base location and limit of that segment the logical address is a pair  selector  offset   where the selector is a 16-bit number  g p 13 2 in which s designates the segment number  g indicates whether the segment is in the gdt or ldt  and p deals with protection the offset is a 32-bit number specifying the location of the byte  or word  within the segment in question the machine has six segment registers  allowing six segments to be addressed at any one time by a process it also has six 8-byte microprogram registers to hold the corresponding descriptors from either the ldt or gdt this cache lets the pentium avoid having to read the descriptor from memory for every memory reference the linear address on the pentium is 32 bits long and is formed as follows the segment register points to the appropriate entry in the ldt or gdt the base and limit information about the segment in question is used to generate a first  the limit is used to check for address validity if the address is not valid  a memory fault is generated  resulting in a trap to the operating system if it is valid  then the value of the offset is added to the value of the base  resulting in a 32-bit linear address this is shown in figure 8.22 in the following section  we discuss how the paging unit turns this linear address into a physical address 8.7.2 pentium paging the pentium architecture allows a page size of either 4 kb or 4 mb for 4-kb pages  the pentium uses a two-level paging schence in which the division of the 32-bit linear address is as follows  page number page offset d 10 10 12 the address-translation scheme for this architecture is similar to the scheme shown in figure 8.15 the intel pentium address translation is shown in more 8.7 347 logical address offset + 32-bit linear address figure 8.22 intel pentium segmentation detail in figure 8.23 the 10 high-order bits reference an entry in the outern'lost page table  which the pentium terms the page directory  the cr3 register points to the page directory for the current process  the page directory entry points to an inner page table that is indexed by the contents of the innermost 10 bits in the linear address finally  the low-order bits 0-11 refer to the offset in the 4-kb page pointed to in the page table one entry in the page directory is the page size flag  which-if setindicates that the size of the page frame is 4 mb and not the standard 4 kb if this flag is set  the page directory points directly to the 4-mb page frame  bypassing the inner page table ; and the 22 low-order bits in the linear address refer to the offset in the 4-mb page frame 31 cr3 registe r page directory page directory page directory  logical address  page table 22 21 l 1211 page table  i offset 31 22 21 offset j 4-kb page 4-mb page figure 8.23 paging in the pentium architecture 0 0 3l ! 8 chapter 8 to improve the efficiency of physical memory use  intel pentium page tables can be swapped to disk in this case  an invalid bit is used in the page directory entry to indicate whether the table to which the entry is pointing is in memory or on disk if the table is on disk  the operating system can use the other 31 bits to specify the disk location of the table ; the table then can be brought into memory on demand 8.7.3 linux on pentium systems as an illustration  consider the linux operating system running on the intel pentium architecture because linux is designed to run on a variety of processors many of which may provide only limited support for segmentationlinux does not rely on segmentation and uses it minimally on the pentium  linux uses only six segments  a segment for kernel code a segment for kernel data a segment for user code a segment for user data a task-state segment  tss  1i a default ldt segment the segments for user code and user data are shared by all processes running in user mode this is possible because all processes use the same logical address space and all segment descriptors are stored in the global descriptor table  gdt   furthermore  each process has its own task-state segment  tss   and the descriptor for this segment is stored in the gdt the tss is used to store the hardware context of each process during context switches the default ldt segment is normally shared by all processes and is usually not used however  if a process requires its own ldt  it can create one and use that instead of the default ldt as noted  each segment selector includes a 2-bit field for protection thus  the pentium allows four levels of protection of these four levels  limlx only recognizes two  user mode and kernel mode although the pentium uses a two-level paging model  linux is designed to run on a variety of hardware platforms  many of which are 64-bit platforms where two-level paging is not plausible therefore  linux has adopted a threelevel paging strategy that works well for both 32-bit and 64-bit architectures the linear address in linux is broken into the following four parts  global directory middle directory page table figure 8.24 highlights the three-level paging model in linux the number of bits in each part of the linear address varies according to architecture however  as described earlier in this section  the pentium architecture only uses a two-level paging model how  then  does linux apply 8.8 lglobal directory global directory cr3 __,.c__ ___ __l register 8.8  linear address  middle directory figure 8.24 three-level paging in linux offset page frame 349 its three-level model on the pentium in this situation  the size of the middle directory is zero bits  effectively bypassing the middle directory each task in linux has its own set of page tables and -just as in figure 8.23 -the cr3 register points to the global directory for the task currently executing during a context switch  the value of the cr3 register is saved and restored in the tss segments of the tasks involved in the context switch memory-management algorithms for multiprogrammed operating systems range from the simple single-user system approach to paged segmentation the most important determinant of the method used in a particular system is the hardware provided every memory address generated by the cpu must be checked for legality and possibly mapped to a physical address the checking can not be implemented  efficiently  in software hence  we are constrained by the hardware available the various memory-management algorithms  contiguous allocation  paging  segmentation  and combinations of paging and segmentation  differ in many aspects in comparing different memory-management strategies  we use the following considerations  hardware support a simple base register or a base-limit register pair is sufficient for the single and multiple-partition schemes  whereas paging and segmentation need mapping tables to define the address map performance as the memory-management algorithm becomes more complex  the time required to map a logical address to a physical address increases for the simple systems  we need only compare or add to the logical address-operations that are fast paging and segmentation can be as fast if the mapping table is implemented in fast registers if the table is 350 chapter 8 in memory  however  user memory accesses can be degraded substantially a tlb can reduce the performance degradation to an acceptable level fragmentation a multiprogrammed system will generally perform more efficiently if it has a higher level of multiprogramming for a given set of processes  we can increase the multiprogramming level only by packing more processes into memory to accomplish this task  we must reduce memory waste  or fragmentation systems with fixed-sized allocation units  such as the single-partition scheme and paging  suffer from internal fragmentation systems with variable-sized allocation units  such as the multiple-partition scheme and segmentation  suffer from external fragmentation relocation one solution to the external-fragmentation problem is compaction compaction involves shifting a program in memory in such a way that the program does not notice the change this consideration requires that logical addresses be relocated dynamically  at execution time if addresses are relocated only at load time  we can not compact storage swapping swapping can be added to any algorithm at intervals determined by the operating system  usually dictated by cpu-scheduling policies  processes are copied from main memory to a backing store and later are copied back to main memory this scheme allows more processes to be run than can be fit into memory at one time sharing another means of increasing the multiprogramming level is to share code and data among different users sharing generally requires that either paging or segmentation be used to provide small packets of information  pages or segments  that can be shared sharing is a means of running many processes with a limited amount of memory  but shared programs and data must be designed carefully protection if paging or segmentation is provided  different sections of a user program can be declared execute-only  read -only  or read-write this restriction is necessary with shared code or data and is generally useful in any case to provide simple run-time checks for common programming errors 8.1 explain the difference between internal and external fragmentation 8.2 compare the memory organization schemes of contiguous memory allocation  pure segmentation  and pure paging with respect to the following issues  a external fragmentation b internal fragmentation c ability to share code across processes 351 8.3 why are segmentation and paging sometimes combined into one scheme 8.4 most systems allow a program to allocate more memory to its address space during execution allocation of data in the heap segments of programs is an example of such allocated memory what is required to support dynamic memory allocation in the following schemes a contiguous memory allocation b pure segmentation c pure paging 8.5 consider the intel address-translation scheme shown in figure 8.22 a describe all the steps taken by the intel pentium in translatil g a logical address into a physical address b what are the advantages to the operating system of hardware that provides such complicated memory translation c are there any disadvantages to this address-translation system if so  what are they if not  why is this scheme not used by every manufacturer 8.6 what is the purpose of paging the page tables 8.7 explain why sharil g a reentrant module is easier when segmentation is used than when pure paging is used 8.8 on a system with paging  a process can not access memory that it does not own why how could the operating system allow access to other memory why should it or should it not 8.9 compare the segmented pagil g scheme with the hashed page table scheme for handling large address spaces under what circumstances is one scheme preferable to the other 8.10 consider a paging system with the page table stored in memory a if a memory reference takes 200 nanoseconds  how long does a paged memory reference take b if we add tlbs  and 75 percent of all page-table references are found in the tlbs  what is the effective memory reference time  assume that finding a page-table entry in the tlbs takes zero time  if the entry is there  352 chapter 8 8.11 compare paging with segmentation with respect to the amount of memory required by the address translation structures in order to convert virtual addresses to physical addresses 8.12 consider a system in which a program can be separated into two parts  code and data the cpu knows whether it wants an instruction  instruction fetch  or data  data fetch or store   therefore  two baselimit register pairs are provided  one for instructions and one for data the instruction base-limit register pair is automatically read-only  so programs can be shared among different users discuss the advantages and disadvantages of this scheme 8.13 consider the following process for generating binaries a compiler is used to generate the object code for individual modules  and a linkage editor is used to combine multiple object modules into a single program bilcary how does the linkage editor change the bindmg of instructions and data to memory addresses what information needs to be passed from the compiler to the linkage editor to facilitate the memory-binding tasks of the linkage editor 8.14 consider a logical address space of 64 pages of 1,024 words each  mapped onto a physical memory of 32 frames a how many bits are there in the logical address b how many bits are there in the physical address 8.15 consider the hierarchical paging scheme used by the vax architecture how many memory operations are performed when a user program executes a memory-load operation 8.16 given five memory partitions of 100 kb  500 kb  200 kb  300 kb  and 600 kb  ill order   how would the first-fit  best-fit  and worst-fit algorithms place processes of 212 kb  417 kb  112 kb  and 426 kb  in order  which algorithm makes the most efficient use of memory 8.17 describe a mechanism by which one segment could belong to the address space of two different processes 8.18 consider a computer system with a 32-bit logical address and 4-kb page size the system supports up to 512mb of physical memory how many entries are there in each of the following a a conventional single-level page table b an inverted page table 353 8.19 assuming a 1-kb page size  what are the page numbers and offsets for the following address references  provided as decimal numbers   a 2375 b 19366 c 30000 d 256 e 16385 8.20 program binaries in many systems are typically structured as follows code is stored starting with a small  fixed virtual address  such as 0 the code segment is followed by the data segment that is used for storing the program variables when the program starts executing  the stack is allocated at the other end of the virtual address space and is allowed to grow toward lower virtual addresses what is the significance of this structure for the following schemes a contiguous memory allocation b pure segmentation c pure paging 8.21 consider the following segment table  segment base length 0 219 600 1 2300 14 2 90 100 3 1327 580 4 1952 96 what are the physical addresses for the following logical addresses a 0,430 b 1,10 c 2,500 d 3,400 e 4,112 8.22 consider a logical address space of 32 pages with 1,024 words per page  mapped onto a physical memory of 16 frames a how many bits are required in the logical address b how many bits are required in the physical address 354 chapter 8 8.23 sharing segments among processes without requiring that they have the same segment number is possible in a dynamically linked segmentation system a define a system that allows static linking and sharing of segments without requiring that the segment numbers be the same b describe a paging scheme that allows pages to be shared without requiring that the page numbers be the same 8.24 assume that a system has a 32-bit virtual address with a 4-kb page size write a c program that is passed a virtual address  in decincal  on the command line and have it output the page number and offset for the given address as an example  your program would run as follows  ./a.out 19986 your program would output  the address 19986 contains  page number = 4 offset = 3602 writing this program will require using the appropriate data type to store 32 bits we encourage you to use unsigned data types as well dynamic storage allocation was discussed by knuth  1973   section 2.5   who found through simulation results that first fit is generally superior to best fit knuth  1973  also discussed the 50-percent rule the concept of paging can be credited to the designers of the atlas system  which has been described by kilburn et al  1961  and by howarth et al  1961   the concept of segmentation was first discussed by dennis  1965   paged segmentation was first supported in the ge 645  on which multics was originally implemented  organick  1972  and daley and dennis  1967    inverted page tables are discussed in an article about the ibm rt storage manager by chang and mergen  1988   address translation in software is covered in jacob and mudge  1997   hennessy and patterson  2002  explains the hardware aspects of tlbs  caches  and mmus talluri et al  1995  discusses page tables for 64-bit address spaces alternative approaches to enforcing memory protection are proposed and studied in wahbe et al  1993a   chase et al  1994   bershad et al  1995   and thorn  1997   dougan et al  1999  and jacob and mudge  2001  discuss 355 tedmiques for managing the tlb fang et al  2001  evaluate support for large pages tanenbaum  2001  discusses intel80386 paging memory management for several architectures-such as the pentiunl ii  powerpc  and ultrasparcare described by jacob and mudge  1998a   segmentation on lim1x systems is presented in bovet and cesati  2002   9.1 c er in chapter 8  we discussed various memory-management strategies used in computer systems all these strategies have the same goal  to keep many processes in memory simultaneously to allow multiprogramming however  they tend to require that an entire process be in memory before it can execute virtual memory is a tecrucique that allows the execution of processes that are not completely in memory one major advantage of this scheme is that programs can be larger than physical memory further  virtual memory abstracts main memory into an extremely large  uniform array of storage  separating logical memory as viewed by the user from physical memory this technique frees programmers from the concerns of memory-storage limitations virtual memory also allows processes to share files easily and to implement shared memory in addition  it provides an efficient mechanism for process creation virtual memory is not easy to implement  however  and may substantially decrease performance if it is used carelessly in this chapter  we discuss virtual memory in the form of demand paging and examine its complexity and cost to describe the benefits of a virtual memory system to explain the concepts of demand paging  page-replacement algorithms  and allocation of page frames to discuss the principles of the working-set model the memory-management algorithms outlined in chapter 8 are necessary because of one basic requirement  the instructions being executed must be in physical memory the first approach to meeting this requirement is to place the entire logical address space in physical memory dynamic loading can help to ease this restriction  but it generally requires special precautions and extra work by the programmer 357 358 chapter 9 the requirement that instructions m.ust be in physical memory to be executed seems both necessary and reasonable ; but it is also unfortunate  since it limits the size of a program to the size of physical memory in fact  an examination of real programs shows us that  in many cases  the entire program is not needed for instance  consider the following  programs often have code to handle unusual error conditions since these errors seldom  if ever  occur in practice  this code is almost never executed arrays,lists  and tables are often allocated more memory than they actually need an array may be declared 100 by 100 elements  even though it is seldom larger than 10 by 10 elements an assembler symbol table may have room for 3,000 symbols  although the average program has less than 200 symbols certain options and features of a program may be used rarely for instance  the routines on u.s government computers that balance the budget have not been used in many years even in those cases where the entire program is needed  it may not all be needed at the same time the ability to execute a program that is only partially in memory would confer many benefits  a program would no longer be constrained by the amount of physical memory that is available users would be able to write programs for an extremely large virtual address space  simplifying the programming task page 0 page 1 page 2 page v virtual memory memory map physical memory figure 9.1 diagram showing virtual memory that is larger than physical memory 9.1 359 because each user program could take less physical memory  more programs could be run at the sance time  with a corresponding increase in cpu utilization and throughput but with no increase in response time or turnaround time less i/o would be needed to load or swap user programs into memory  so each user program would run faster thus  running a program that is not entirely in memory would benefit both the system and the user involves the separation of logical memory as perceived by users from physical memory this separation allows an extremely large virtual memory to be provided for programmers when only a smaller physical memory is available  figure 9.1   virtual memory makes the task of programming much easier  because the programmer no longer needs to worry about the amount of physical memory available ; she can concentrate instead on the problem to be programmed the address space of a process refers to the logical  or virtual  view of how a process is stored in memory typically  this view is that a process begins at a certain logical address-say  address 0-and exists in contiguous memory  as shown in figure 9.2 recall from chapter 8  though  that in fact physical memory may be organized in page frames and that the physical page frames assigned to a process may not be contiguous it is up to the memorymanagement unit  mmu  to map logical pages to physical page frames in memory note in figure 9.2 that we allow for the heap to grow upward in memory as it is used for dynamic memory allocation similarly  we allow for the stack to grow downward in memory through successive function calls the large blank space  or hole  between the heap and the stack is part of the virtual address figure 9.2 virtual address space 360 chapter 9 space but will require actual physical pages only if the heap or stack grows virtual address spaces that include holes are known as sparse address spaces using a sparse address space is beneficial because the holes can be filled as the stack or heap segments grow or if we wish to dynam.ically link libraries  or possibly other shared objects  during program execution in addition to separating logical memory from physical memory  virtual memory allows files and memory to be shared by two or more processes through page sharing  section 8.4.4   this leads to the following benefits  system libraries can be shared by several processes through mapping of the shared object into a virtual address space although each process considers the shared libraries to be part of its virtual address space  the actual pages where the libraries reside in physical memory are shared by all the processes  figure 9.3   typically  a library is mapped read-only into the space of each process that is linked with it similarly  virtual memory enables processes to share memory recall from chapter 3 that two or more processes can communicate through the use of shared memory virtual memory allows one process to create a region of memory that it can share with another process processes sharing this region consider it part of their virtual address space  yet the actual physical pages of memory are shared  much as is illustrated in figure 9.3 virtual memory can allow pages to be shared during process creation with the fork   system calt thus speeding up process creation we further explore these-and other-benefits of virtual memory later in this chapter first though  we discuss implementing virtual memory through demand paging shared library shared pages shared library figure 9.3 shared library using virtual memory 9.2 9.2 361 consider how an executable program might be loaded from disk into n'lemory one option is to load the entire program in physical memory at program execution time however  a problent with this approach is that we may not initially need the entire program in memory suppose a program starts with a list of available options from which the user is to select loading the entire program into memory results in loading the executable code for all options  regardless of whether an option is ultimately selected by the user or not an alternative strategy is to load pages only as they are needed this technique is known as paging and is commonly used in virtual memory systems with demand-paged virtual memory  pages are only loaded when they are demanded during program execution ; pages that are never accessed are thus never loaded into physical memory a demand-paging system is similar to a paging system with swapping  figure 9.4  where processes reside in secondary memory  usually a disk   when we want to execute a process  we swap it into memory rather than swapping the entire process into memory  however  we use a a lazy swapper never swaps a page into memory unless that page will be needed since we are now viewing a process as a sequence of pages  rather than as one large contiguous address space  use of the term swapper is technically incorrect a swapper manipulates entire processes  whereas a is concerned with the individual pages of a process we thus use pager  rather than swapper  in connection with demand paging program a program b main memory swap out so 90100110 120130140150 swap in 16017 figure 9.4 transfer of a paged memory to contiguous disk space 362 chapter 9 9.2.1 basic concepts when a process is to be swapped in  the pager guesses which pages will be used before the process is swapped out again instead of swapping in a whole process  the pager brings only those pages into memory thus  it avoids reading into memory pages that will not be used anyway  decreasing the swap time and the amount of physical memory needed with this scheme  we need some form of hardware support to distinguish between the pages that are in memory and the pages that are on the disk the valid -invalid bit scheme described in section 8.4.3 can be used for this purpose this time  however  when this bit is set to valid/ ' the associated page is both legal and in n1.emory if the bit is set to invalid/ ' the page either is not valid  that is  not in the logical address space of the process  or is valid but is currently on the disk the page-table entry for a page that is brought into memory is set as usuat but the page-table entry for a page that is not currently in memory is either simply marked invalid or contains the address of the page on disk this situation is depicted in figure 9.5 notice that marking a page invalid will have no effect if the process never attempts to access that page hence  if we guess right and page in all and only those pages that are actually needed  the process will run exactly as though we had brought in all pages while the process executes and accesses pages that are execution proceeds normally 0 2 3 4 5 6 7 valid-invalid frame bit ' \  i 0 4 v logical memory physical memory dod d  1j   @ jtb  odd figure 9.5 page table when some pages are not in main memory operating system reference   ; \  page is on \   v backing store trap restart instruction page table reset page table physical memory 9.2 0 bring in missing page figure 9.6 steps in handling a page fault 363 but what happens if the process tries to access a page that was not brought into memory access to a page marked invalid causes a the paging hardware  in translating the address through the page table  will notice that the invalid bit is set  causing a trap to the operating system this trap is the result of the operating system 's failure to bring the desired page into memory the procedure for handling this page fault is straightforward  figure 9.6   we check an internal table  usually kept with the process control block  for this process to determine whether the reference was a valid or an invalid memory access if the reference was invalid  we terminate the process if it was valid  but we have not yet brought in that page  we now page it in we find a free frame  by taking one from the free-frame list  for example   we schedule a disk operation to read the desired page into the newly allocated frame when the disk read is complete  we modify the internal table kept with the process and the page table to indicate that the page is now in memory we restart the instruction that was interrupted by the trap the process can now access the page as though it had always been in memory in the extreme case  we can start executing a process with no pages in memory when the operating system sets the instruction pointer to the first 364 chapter 9 instruction of the process  which is on a non-memory-resident page  the process immediately faults for the page after this page is brought into memory  the process continues to execute  faulting as necessary until every page that it needs is in memory at that it can execute with no more faults this scheme is never bring a page into memory until it is required theoretically  some programs could access several new pages of memory with each instruction execution  one page for the instruction and many for data   possibly causing multiple page faults per instruction this situation would result in unacceptable system performance fortunately  analysis of running processes shows that this behavior is exceedingly unlikely programs tend to have described in section 9.6.1  which results in reasonable performance from demand paging the hardware to support demand paging is the same as the hardware for paging and swapping  page table this table has the ability to mark an entry invalid through a valid -invalid bit or a special value of protection bits secondary memory this memory holds those pages that are not present in main memory the secondary memory is usually a high-speed disk it is known as the swap device  and the section of disk used for this purpose is known as swap-space allocation is discussed in chapter 12 a crucial requirement for demand paging is the ability to restart any instruction after a page fault because we save the state  registers  condition code  instruction counter  of the interrupted process when the page fault occurs  we must be able to restart the process in exactly the same place and state  except that the desired page is now in memory and is accessible in most cases  this requirement is easy to meet a page fault may occur at any memory reference if the page fault occurs on the instruction fetch  we can restart by fetching the instruction again if a page fault occurs while we are fetching an operand  we must fetch and decode the instruction again and then fetch the operand as a worst-case example  consider a three-address instruction such as add the content of a to b  placing the result in c these are the steps to execute this instruction  fetch and decode the instruction  add   fetch a fetch b add a and b store the sum in c if we fault when we try to store inc  because c is in a page not currently in memory   we will have to get the desired page  bring it in  correct the page table  and restart the instruction the restart will require fetching the instruction again  decoding it again  fetching the two operands again  and 9.2 365 then adding again however  there is not much repeated work  less than one complete instruction   and the repetition is necessary only when a page fault occurs the major difficulty arises when one instruction may modify several different locations for example  consider the ibm system 360/370 mvc  move character  instruction  which can ncove up to 256 bytes from one location to another  possibly overlapping  location if either block  source or destination  straddles a page boundary  a page fault might occur after the move is partially done in addition  if the source and destination blocks overlap  the source block may have been modified  in which case we can not simply restart the instruction this problem can be solved in two different ways in one solution  the microcode computes and attempts to access both ends of both blocks if a page fault is going to occm ~ it will happen at this step  before anything is modified the move can then take place ; we know that no page fault can occur  since all the relevant pages are in memory the other solution uses temporary registers to hold the values of overwritten locations if there is a page fault  all the old values are written back into memory before the trap occurs this action restores memory to its state before the instruction was started  so that the instruction can be repeated this is by no means the only architectural problem resulting from adding paging to an existing architecture to allow demand paging  but it illustrates some of the difficulties involved paging is added between the cpu and the memory in a computer system it should be entirely transparent to the user process thus  people often assume that paging can be added to any system although this assumption is true for a non-demand-paging environment  where a page fault represents a fatal errm ~ it is not true where a page fault means only that an additional page must be brought into memory and the process restarted 9.2.2 performance of demand paging demand paging can significantly affect the performance of a computer system to see why  let 's compute the effective access time for a demand-paged memory for most computer systems  the memory-access time  denoted ma  ranges from 10 to 200 nanoseconds as long as we have no page faults  the effective access time is equal to the memory access time if  howeve1 ~ a page fault occurs  we must first read the relevant page from disk and then access the desired word let p be the probability of a page fault  0    ; p    ; 1   we would expect p to be close to zero-that is  we would expect to have only a few page faults the t'tp r ' ! nrr access is then effective access time =  1  p  x ma + p x page fault time to compute the effective access time  we must know how much time is needed to service a page fault a page fault causes the following sequence to occur  trap to the operating system save the user registers and process state 366 chapter 9 deterncine that the interrupt was a page fault check that the page reference was legal and determine the location of the page on the disk issue a read from the disk to a free frame  a wait in a queue for this device until the read request is serviced b wait for the device seek and/ or latency time c begin the transfer of the page to a free frame while waiting  allocate the cpu to some other user  cpu scheduling  optional   receive an interrupt from the disk i/0 subsystem  i/0 completed   save the registers and process state for the other user  if step 6 is executed   determine that the interrupt was from the disk correct the page table and other tables to show that the desired page is now in memory wait for the cpu to be allocated to this process again restore the user registers  process state  and new page table  and then resume the interrupted instruction not all of these steps are necessary in every case for example  we are assuming that  in step 6  the cpu is allocated to another process while the i/o occurs this arrangement allows multiprogramming to maintain cpu utilization but requires additional time to resume the page-fault service routine when the i/0 transfer is complete in any case  we are faced with tlu ee major components of the page-fault service time  service the page-fault interrupt read in the page restart the process the first and third tasks can be reduced  with careful coding  to several hundred instructions these tasks may take from 1 to 100 microseconds each the page-switch time  however  will probably be close to 8 milliseconds  a typical hard disk has an average latency of 3 milliseconds  a seek of 5 milliseconds  and a transfer time of 0.05 milliseconds thus  the total paging time is about 8 milliseconds  including hardware and software time  remember also that we are looking at only the device-service time if a queue of processes is waiting for the device  we have to add device-queueing time as we wait for the paging device to be free to service our request  increasing even more the time to swap with an average page-fault service time of 8 milliseconds and a memoryaccess time of 200 nanoseconds  the effective access time in nanoseconds is 3 9.3 effective access time =  1  p  x  200  + p  8 milliseconds  =  1 p  x 200 + p x 8,000,000 = 200 + 7,999,800 x p 367 we see  then  that the effective access time is directly proportional to the if one access out of 1,000 causes a page fault  the effective access time is 8.2 microseconds the computer will be slowed down by a factor of 40 because of demand paging ! if we want performance degradation to be less than 10 percent  we need 220 200 + 7,999,800 x p  20 7,999,800 x p  p 0.0000025 that is  to keep the slowdown due to paging at a reasonable level  we can allow fewer than one memory access out of 399,990 to page-fault in sum  it is important to keep the page-fault rate low in a demand-paging system otherwise  the effective access time increases  slowing process execution dramatically an additional aspect of demand paging is the handling and overall use of swap space disk i/0 to swap space is generally faster than that to the file system it is faster because swap space is allocated in much larger blocks  and file lookups and indirect allocation methods are not used  chapter 12   the system can therefore gain better paging throughput by copying an entire file image into the swap space at process startup and then performing demand paging from the swap space another option is to demand pages from the file system initially but to write the pages to swap space as they are replaced this approach will ensure that only needed pages are read from the file system but that all subsequent paging is done from swap space some systems attempt to limit the amount of swap space used through demand paging of binary files demand pages for such files are brought directly from the file system however  when page replacement is called for  these frames can simply be overwritten  because they are never modified   and the pages can be read in from the file system again if needed using this approach  the file system itself serves as the backing store howeve1 ~ swap space must still be used for pages not associated with a file ; these pages include the stack and heap for a process this method appears to be a good compromise and is used in several systems  including solaris and bsd unix in section 9 .2  we illustrated how a process can start quickly by merely demandpaging in the page containing the first instruction however  process creation using the fork   system call may initially bypass the need for demand paging by using a technique similar to page sharing  covered in section 8.4.4   this technique provides for rapid process creation and minimizes the number of new pages that must be allocated to the newly created process 368 chapter 9 physical figure 9.7 before process i modifies page c recall thatthe fork   system call creates a child process that is a duplicate of its parent traditionally  fork   worked by creating a copy of the parent 's address space for the child  duplicating the pages belonging to the parent however  considering that many child processes invoke the exec   system call immediately after creation  the copying of the parent 's address space may be unnecessary instead  we can use a technique known as which works by allowing the parent and child processes initially to share the same pages these shared pages are marked as copy-on-write pages  meaning that if either process writes to a shared page  a copy of the shared page is created copy-on-write is illustrated in figures 9.7 and figure 9.8  which show the contents of the physical memory before and after process 1 modifies page c for example  assume that the child process attempts to modify a page containing portions of the stack  with the pages set to be copy-on-write the operating system will create a copy of this page  nl.apping it to the address space of the child process the child process will then modify its copied page and not the page belonging to the parent process obviously  when the copy-on-write technique is used  only the pages that are modified by either process are copied ; all unmodified pages can be shared by the parent and child processes note  too  process1 physical memory figure 9.8 after process 1 modifies page c process2 9.4 9.4 369 that only pages that can be nwdified need be m ~ arked as copy-on-write pages that can not be modified  pages containing executable code  can be shared by the parent and child copy-on-write is a common technique used by several operating systems  including windows xp  linux  and solaris when it is determined that a page is going to be duplicated using copyon write  it is important to note the location from which the free page will be allocated many operating systems provide a of free pages for such requests these free pages are typically allocated when the stack or heap for a process must expand or when there are copy-on-write pages to be managed operating systems typically allocate these pages using a technique known as zem-fhl-on-den  1and zero-fill-on-demand pages have been zeroed-out before being allocated  thus erasing the previous contents several versions of unix  including solaris and linux  provide a variation ofthe fork   system call-vfork    for fori    that operates differently from fork   with copy-on-write with vfork    the parent process is suspended  and the child process uses the address space of the parent because vfork   does not use copy-on-write  if the child process changes any pages of the parent 's address space  the altered pages will be visible to the parent once it resumes therefore  vf ork   must be used with caution to ensure that the child process does not modify the address space of the parent vf or k   is intended to be used when the child process calls exec   immediately after creation because no copying of pages takes place  vf ork   is an extremely efficient method of process creation and is sometimes used to implement unix command-line shell interfaces in our earlier discussion of the page-fault rate  we assumed that each page faults at most once  when it is first referenced this representation is not strictly accurate  however if a process of ten pages actually uses only half of them  then demand paging saves the i/0 necessary to load the five pages that are never used we could also increase our degree of multiprogramming by running twice as many processes thus  if we had forty frames  we could run eight processes  rather than the four that could run if each required ten frames  five of which were never used   if we increase our degree of multiprogramming  we are memory if we run six processes  each of which is ten pages in size but uses only five pages  we have higher cpu utilization and throughput  ten frames to spare it is possible  however  that each of these processes  for a particular data set  may suddenly try to use all ten of its pages  resulting in a need for sixty frames when only forty are available further  consider that system memory is not used only for holding program pages buffers for i/ 0 also consume a considerable amount of memory this use can increase the strain on memory-placement algorithms deciding how much memory to allocate to i/0 and how much to program pages is a significant challenge some systems allocate a fixed percentage of memory for i/0 buffers  whereas others allow both user processes and the i/0 subsystem to compete for all system memory 370 chapter 9 valid-invalid pc   -_ = ' ~ ~ = =  ! came f il logical memory for user 1 page table for user 1 valid-invalid 0 frame ~ bi ~ r ~ v v ~ -------' ' 2 3 logical memory for user 2 page table for user 2 0 monitor 2 3 4 5 j 6 a 7 e physical memory figure 9.9 need for page replacement over-allocation of memory manifests itself as follows while a user process is executing  a page fault occurs the operating system determines where the desired page is residing on the disk but then finds that there are no free frames on the free-frame list ; all memory is in use  figure 9.9   the operating system has several options at this point it could terminate the user process however  demand paging is the operating system 's attempt to improve the computer system 's utilization and throughput users should not be aware that their processes are running on a paged system-paging should be logically transparent to the user so this option is not the best choice the operating system could instead swap out a process  freeing all its frames and reducing the level of multiprogramming this option is a good one in certain circumstances  and we consider it further in section 9.6 here  we discuss the most common solution  9.4.1 basic page replacement page replacement takes the following approach if no frame is free  we find one that is not currently being used and free it we can free a frame by writing its contents to swap space and changing the page table  and all other tables  to indicate that the page is no longer in memory  figure 9.10   we can now use the freed frame to hold the page for which the process faulted we modify the page-fault service routine to include page replacement  find the location of the desired page on the disk find a free frame  a if there is a free frame  use it 9.4 371 b if there is no free frame  use a page-replacement algorithnc to select a c write the victim frame to the disk ; change the page and frame tables accordingly read the desired page into the newly freed frame ; change the page and frame tables restart the user process notice that  if no frames are free  two page transfers  one out and one in  are required this situation effectively doubles the page-fault service time and increases the effective access time accordingly we can reduce this overhead by using a  or when this scheme is used  each page or frame has a modify bit associated with it in the hardware the modify bit for a page is set by the hardware whenever any word or byte in the page is written into  indicating that the page has been modified when we select a page for replacement  we examine its modify bit if the bit is set  we know that the page has been modified since it was read in from the disk in this case  we must write the page to the disk if the modify bit is not set  however  the page has not been modified since it was read into memory in this case  we need not write the memory page to the disk  it is already there this technique also applies to read-only pages  for example  pages of binary code   such pages can not be modified ; thus  they may be discarded when desired this scheme can significantly reduce the time required to service a page fault  since it reduces i/o time by one-half if the page has not been modified frame valid-invalid bit ' \  / physical memory figure 9.10 page replacement 372 chapter 9 page replacement is basic to demand paging it completes the separation between logical memory and physical memory with this mechanism  an enormous virtual memory can be provided for programn'lers on a smaller physical memory with no demand paging  user addresses are mapped into physical addresses  so the two sets of addresses can be different all the pages of a process still must be in physical memory  however with demand paging  the size of the logical address space is no longer constrained by physical memory if we have a user process of twenty pages  we can execute it in ten frames simply by using demand paging and using a replacement algorithm to find a free frame whenever necessary if a page that has been modified is to be replaced  its contents are copied to the disk a later reference to that page will cause a page fault at that time  the page will be brought back into memory  perhaps replacing some other page in the process we must solve two major problems to implement demand develop a algorithm and a ' ' '  ' ' l tceme ~ lu ~ ~ f ' ' ~ ~ ''h ' that is  if we have multiple processes in memory  we must decide how many frames to allocate to each process ; and when page replacement is required  we must select the frames that are to be replaced designing appropriate algorithms to solve these problems is an important task  because disk i/0 is so expensive even slight improvements in demand-paging methods yield large gains in system performance there are many different page-replacement algorithms every operating system probably has its own replacement scheme how do we select a particular replacement algorithm in general  we want the one with the lowest page-fault rate we evaluate an algorithm by running it on a particular string of memory references and computing the number of page faults the string of memory references is called a reference we can generate reference strings artificially  by using a random-number generator  for example   or we can trace a given system and record the address of each memory reference the latter choice produces a large number of data  on the order of 1 million addresses per second   to reduce the number of data  we use two facts first  for a given page size  and the page size is generally fixed by the hardware or system   we need to consider only the page number  rather than the entire address second  if we have a reference to a page p  then any references to page p that immediately follow will never cause a page fault page p will be in memory after the first reference  so the immediately following references will not fault for example  if we trace a particular process  we might record the following address sequence  0100,0432,0101,0612,0102,0103,0104,0101,0611,0102,0103  0104,0101,0610,0102,0103,0104,0101,0609,0102,0105 at 100 bytes per page  this sequence is reduced to the following reference string  1  4  1  6  1  6  1  6  1  6  1 9.4 373 16 g  14    j  ; 2 12 q  ol co 10 0 0 8 ' q  ..0 6 e    j c 4 2 2 3 4 5 6 number of frames figure 9.1 i graph of page faults versus number of frames to determine the number of page faults for a particular reference string and page-replacement algorithm  we also need to know the number of page frames available obviously  as the number of frames available increases  the number of page faults decreases for the reference stril'lg considered previously  for example  if we had three or more frames  we would have only three faultsone fault for the first reference to each page in contrast  with only one frame available  we would have a replacement with every reference  resulting in eleven faults in general  we expect a curve such as that in figure 9.11 as the number of frames increases  the number of page faults drops to some minimal level of course  adding physical memory increases the number of frames we next illustrate several page-replacement algorithms in doing so  we use the reference string for a memory with three frames 9.4.2 fifo page replacement the simplest page-replacement algorithm is a first-in  first-out  fifo  algorithm a fifo replacement algorithm associates with each page the time when that page was brought into memory when a page must be replaced  the oldest page is chosen notice that it is not strictly necessary to record the time when a page is brought in we can create a fifo queue to hold all pages in memory we replace the page at the head of the queue when a page is brought into memory  we insert it at the tail of the queue for our example reference string  our three frames are initially empty the first three references  7  0  1  cause page faults and are brought into these empty frames the next reference  2  replaces page 7  because page 7 was brought in first since 0 is the next reference and 0 is already in memory  we have no fault for this reference the first reference to 3 results in replacement of page 0  since 374 chapter 9 reference string 7 0 2 0 3 0 4 2 3 0 3 2 2 0 7 0 page frames figure 9.12 fifo page-replacement algorithm it is now first in line because of this replacement  the next reference  to 0  will fault page 1 is then replaced by page 0 this process continues as shown in figure 9.12 every time a fault occurs  we show which pages are in our three frames there are fifteen faults altogether the fifo page-replacement algorithm is easy to lmderstand and program however  its performance is not always good on the one hand  the page replaced may be an initialization module that was used a long time ago and is no longer needed on the other hand  it could contain a heavily used variable that was initialized early and is in constant use notice that  even if we select for replacement a page that is in active use  everything still works correctly after we replace an active page with a new one  a fault occurs almost immediately to retrieve the active page some other page must be replaced to bring the active page back into memory thus  a bad replacement choice increases the page-fault rate and slows process execution it does not  however  cause incorrect execution to illustrate the problems that are possible with a fifo page-replacement algorithm  we consider the following reference string  1  2  3  4  1  2  5  1  2  3  4  5 figure 9.13 shows the curve of page faults for this reference string versus the number of available frames notice that the number of faults for four frames  ten  is greater than the number of faults for three frames  nine  ! this most unexpected result is known as  for some page-replacement algorithms  the page-fault rate may increase as the number of allocated frames increases we would expect that giving more memory to a process would improve its performance in some early research  investigators noticed that this assumption was not always true belady 's anomaly was discovered as a result 9.4.3 optimal page replacement of belady 's anomaly was the search for an which has the lowest page-fault rate of all algorithms and will never suffer from belady 's anomaly such an algorithm does exist and has been called opt or min it is simply this  replace the page that will not be used for the longest period of time 9.4 375 16 ~    5 2 12 cj  moj 10 0 0 8 cj _o e 6    5 c 4 2 number of frames figure 9.13 page-fault curve for fifo replacement on a reference string use of this page-replacement algorithm guarantees the lowest possible pagefault rate for a fixed number of frames for example  on our sample reference string  the optimal page-replacement algorithm would yield nine page faults  as shown in figure 9.14 the first three references cause faults that fill the three empty frames the reference to page 2 replaces page 7  because page 7 will not be used until reference 18  whereas page 0 will be used at 5  and page 1 at 14 the reference to page 3 replaces page 1  as page 1 will be the last of the three pages in memory to be referenced again with only nine page faults  optimal replacement is much better than a fifo algorithm  which results in fifteen faults  if we ignore the first three  which all algorithms must suffer  then optimal replacement is twice as good as fifo replacement  irt fact  no replacement algorithm can process this reference string in three frames with fewer than nine faults unfortunately  the optimal page-replacement algorithm is difficult to implement  because it requires future knowledge of the reference string  we encountered a similar situation with the sjf cpu-schedulin.g algorithm in section 5.3.2  as a result  the optimal algorithm is used mainly for comparison studies for instance  it may be useful to know that  although a new algorithm reference string 7 0 2 0 3 0 4 2 3 0 3 2 2 0 7 0 page frames figure 9.14 optimal page-replacement algorithm 376 chapter 9 is not optimat it is within 12.3 percent of optimal at worst and within 4.7 percent on average 9.4.4 lru page replacement lf the optimal algorithm is not feasible  perhaps an approximation of the optimal algorithm is possible the key distinction between the fifo and opt algorithms  other than looking backward versus forward in time  is that the fifo algorithm uses the time when a page was brought into memory  whereas the opt algorithm uses the time when a page is to be used if we use the recent past as an approximation of the near future  then we can replace the that has not been used for the longest period of time this approach is the lru replacement associates with each page the time of that page 's last use when a page must be replaced  lru chooses the page that has not been used for the longest period of time we can think of this strategy as the optimal page-replacement algorithm looking backward in time  rather than forward  strangely  if we let sr be the reverse of a reference strings  then the page-fault rate for the opt algorithm on sis the same as the page-fault rate for the opt algorithm on sr similarly  the page-fault rate for the lru algorithm on sis the same as the page-fault rate for the lru algorithm on sr  the result of applying lru replacement to our example reference string is shown in figure 9.15 the lru algorithm produces twelve faults notice that the first five faults are the same as those for optimal replacement when the reference to page 4 occurs  however  lru replacement sees that  of the three frames in memory  page 2 was used least recently thus  the lru algorithm replaces page 2  not knowing that page 2 is about to be used when it then faults for page 2  the lru algorithm replaces page 3  since it is now the least recently used of the three pages in memory despite these problems  lru replacement with twelve faults is much better than fifo replacement with fifteen the lru policy is often used as a page-replacement algorithm and is considered to be good the major problem is how to implement lru replacement an lru page-replacement algorithm may require substantial hardware assistance the problem is to determine an order for the frames defined by the time of last use two implementations are feasible  counters in the simplest case  we associate with each page-table entry a time-of-use field and add to the cpu a logical clock or counter the clock is reference string 7 0 2 0 3 0 4 2 3 0 3 2 2 0 7 0 page frames figure 9.15 lru page-replacement algorithm 9.4 377 incremented for every memory reference whenever a reference to a page is made  the contents of the clock register are copied to the ti1ne-of-use field in the page-table entry for that page in this way  we always have the time of the last reference to each page we replace the page with the smallest time value this scheme requires a search of the page table to find the lru page and a write to memory  to the time-of-use field in the page table  for each memory access the times must also be m ~ aintained when page tables are changed  due to cpu scheduling   overflow of the clock must be considered stack another approach to implementing lru replacement is to keep a stack of page numbers whenever a page is referenced  it is removed from the stack and put on the top in this way  the most recently used page is always at the top of the stack and the least recently used page is always at the bottom  figure 9.16   because entries must be removed from the middle of the stack  it is best to implement this approach by using a doubly linked list with a head pointer and a tail pointer removing a page and putting it on the top of the stack then requires changing six pointers at worst each update is a little more expensive  but there is no search for a replacement ; the tail pointer points to the bottom of the stack  which is the lru page this approach is particularly appropriate for software or microcode implementations of lru replacement like optimal replacement  lru replacement does not suffer from belady 's both belong to a class of page-replacement algorithms  called si  ack that can never exhibit belady 's anomaly a stack algorithm is an algorithm for which it can be shown that the set of pages in memory for n frames is always a subset of the set of pages that would be in memory with n + 1 frames for lru replacement  the set of pages in memory would be the n most recently referenced pages if the number of frames is increased  these n pages will still be the most recently referenced and so will still be in memory note that neither implementation of lru would be conceivable without hardware assistance beyond the standard tlb registers the updating of the reference string 4 7 0 7 stack before a 0 2 stack after b 2 7 2 i l a b figure 9.16 use of a stack to record the most recent page references 378 chapter 9 clock fields or stack must be done for every memory reference if we were to use an interrupt for every reference to allow software to update such data structures  it would slow every memory reference by a factor of at least ten  hence slowing every user process by a factor of ten few systems could tolerate that level of overhead for memory management 9.4.5 lru-approximation page replacement few computer systems provide sufficient hardware support for true lru page replacement some systems provide no hardware support  and other pagereplacement algorithms  such as a fifo algorithm  must be used many systems provide some help  however  in the form of a the reference bit for a page is set by the hardware whenever that page is referenced  either a read or a write to any byte in the page   reference bits are associated with each entry in the page table initially  all bits are cleared  to 0  by the operating system as a user process executes  the bit associated with each page referenced is set  to 1  by the hardware after some time  we can determine which pages have been used and which have not been used by examining the reference bits  although we do not know the order of use this information is the basis for many page-replacement algorithms that approximate lru replacement 9.4.5.1 additional-reference-bits algorithm we can gain additional ordering information by recording the reference bits at regular intervals we can keep an 8-bit byte for each page in a table in memory at regular intervals  say  every 100 milliseconds   a timer interrupt transfers control to the operating system the operating system shifts the reference bit for each page into the high-order bit of its 8-bit byte  shifting the other bits right by 1 bit and discarding the low-order bit these 8-bit shift registers contain the history of page use for the last eight time periods if the shift register contains 00000000  for example  then the page has not been used for eight time periods ; a page that is used at least once in each period has a shift register value of 11111111 a page with a history register value of 11000100 has been used more recently than one with a value of 01110111 if we interpret these 8-bit bytes as unsigned integers  the page with the lowest number is the lru page  and it can be replaced notice that the numbers are not guaranteed to be unique  however we can either replace  swap out  all pages with the smallest value or use the fifo method to choose among them the number of bits of history included in the shift register can be varied  of course  and is selected  depending on the hardware available  to make the updating as fast as possible in the extreme case  the number can be reduced to zero  leaving only the reference bit itself this algorithm is called the 9.4.5.2 second-chance algorithm the basic algorithm of second-chance replacement is a fifo replacement algorithm when a page has been selected  however  we inspect its reference bit if the value is 0  we proceed to replace this page ; but if the reference bit is set to 1  we give the page a second chance and move on to select the next next victim 9.4 reference pages reference pages bits bits circular queue of pages circular queue of pages  a   b  figure 9.17 second-chance  clock  page-replacement algorithm 379 fifo page when a page gets a second chance  its reference bit is cleared  and its arrival time is reset to the current time thus  a page that is given a second chance will not be replaced until all other pages have been replaced  or given second chances   in addition  if a page is used often enough to keep its reference bit set  it will never be replaced one way to implement the second-chance algorithm  sometimes referred to as the clock algorithm  is as a circular queue a poi11ter  that is  a hand on the clock  indicates which page is to be replaced next when a frame is needed  the pointer advances until it finds a page with a 0 reference bit as it advances  it clears the reference bits  figure 9.17   once a victim page is found  the page is replaced  and the new page is inserted in the circular queue in that position notice that  in the worst case  when all bits are set  the pointer cycles through the whole queue  giving each page a second chance it clears all the reference bits before selecting the next page for replacement second-chance replacement degenerates to fifo replacement if all bits are set 9.4.5.3 enhanced second-chance algorithm we can enhance the second-chance algorithm by considering the reference bit and the modify bit  described in section 9.4.1  as an ordered pair with these two bits  we have the following four possible classes   0  0  neither recently used nor modified -best page to replace 380 chapter 9  0  1  not recently used hut modified-not quite as good  because the page will need to be written out before replacement  1  0  recently used but clean-probably will be used again soon  1  1  recently used and modified -probably will be used again soon  and the page will be need to be written out to disk before it can be replaced each page is in one of these four classes when page replacement is called for  we use the same scheme as in the clock algorithm ; but instead of examining whether the page to which we are pointing has the reference bit set to 1  we examine the class to which that page belongs we replace the first page encountered in the lowest nonempty class notice that we may have to scan the circular queue several times before we find a page to be replaced the major difference between this algorithm and the simpler clock algorithm is that here we give preference to those pages that have been modified to reduce the number of i/os required 9.4.6 counting-based page replacement there are many other algorithms that can be used for page replacement for example  we can keep a counter of the number of references that have been made to each page and develop the following two schemes the least frequently used  lfu  page-replacement algorithm requires that the page with the smallest count be replaced the reason for this selection is that an actively used page should have a large reference count a problem arises  however  when a page is used heavily during the initial phase of a process but then is never used again since it was used heavily  it has a large count and remains in memory even though it is no longer needed one solution is to shift the counts right by 1 bit at regular intervals  forming an exponentially decaying average usage count the most frequently used  mfu  page-replacement algorithm is based on the argument that the page with the smallest count was probably just brought in and has yet to be used as you might expect  neither mfu nor lfu replacement is common the implementation of these algorithms is expensive  and they do not approxin'late opt replacement well 9.4.7 page-buffering algorithms other procedures are often used in addition to a specific page-replacement algorithm for example  systems commonly keep a pool of free frames when a page fault occurs  a victim frame is chosen as before however  the desired page is read into a free frame from the pool before the victim is written out this procedure allows the process to restart as soon as possible  without waiting for the victim page to be written out when the victim is later written out  its frame is added to the free-frame pool 9.4 381 an expansion of this idea is to maintain a list of modified pages whenever the paging device is idle  a modified page is selected and is written to the disk its modify bit is then reset this scheme increases the probability that a page will be clean when it is selected for replacement and will not need to be written out another modification is to keep a pool of free frames but to remember which page was in each frame since the frame contents are not modified when a frame is written to the disk  the old page can be reused directly fronc the free-frame pool if it is needed before that frame is reused no i/o is needed in this case when a page fault occurs  we first check whether the desired page is in the free-frame pool if it is not  we must select a free frame and read into it this technique is used in the vax/vms system along with a fifo replacement algorithm when the fifo replacement algorithm mistakenly replaces a page that is still in active use  that page is quickly retrieved from the free-frame pool  and no i/o is necessary the free-frame buffer provides protection against the relatively poor  but sirnple  fifo replacement algorithm this method is necessary because the early versions of vax did not implement the reference bit correctly some versions of the unix system use this method in conjunction with the second-chance algorithm it can be a useful augmentation to any pagereplacement algorithm  to reduce the penalty incurred if the wrong victim page is selected 9.4.8 applications and page replacement in certain cases  applications accessing data through the operating system 's virtual memory perform worse than if the operating system provided no buffering at all a typical example is a database  which provides its own memory management and i/0 buffering applications like this understand their memory use and disk use better than does an operating system that is implementing algorithms for general-purpose use if the operating system is buffering i/0  and the application is doing so as well  then twice the memory is being used for a set of i/0 in another example  data warehouses frequently perform massive sequential disk reads  followed by computations and writes the lru algorithm would be removing old pages and preserving new ones  while the application would more likely be reading older pages than newer ones  as it starts its sequential reads again   here  mfu would actually be more efficient than lru because of such problems  some operating systems give special programs the ability to use a disk partition as a large sequential array of logical blocks  without any file-system data structures this array is sometimes called the raw disk  and i/o to this array is termed raw i/0 raw i/0 bypasses all the filesystem services  such as file i/0 demand paging  file locking  prefetching  space allocation  file names  and directories note that although certain applications are more efficient when implementing their own special-purpose storage services on a raw partition  most applications perform better when they use the regular file-system services 382 chapter 9 9.5 we turn next to the issue of allocation how do we allocate the fixed amount of free memory among the various processes if we have 93 free frames and two processes  how many frames does each process get the simplest case is the single-user system consider a single-user system with 128 kb of memory composed of pages 1 kb in size this system has 128 frames the operating system may take 35 kb  leaving 93 frames for the user process under pure demand paging  all 93 frames would initially be put on the free-frame list when a user process started execution  it would generate a sequence of page faults the first 93 page faults would all get free frames from the free-frame list when the free-frame list was exhausted  a page-replacement algorithm would be used to select one of the 93 in-memory pages to be replaced with the 94th  and so on when the process terminated  the 93 frames would once again be placed on the free-frame list there are many variations on this simple strategy we can require that the operating system allocate all its buffer and table space from the free-frame list when this space is not in use by the operating system  it can be used to support user paging we can try to keep three free frames reserved on the free-frame list at all times thus  when a page fault occurs  there is a free frame available to page into while the page swap is taking place  a replacement can be selected  which is then written to the disk as the user process continues to execute other variants are also possible  but the basic strategy is clear  the user process is allocated any free frame 9.5.1 minimum number of frames our strategies for the allocation of frames are constrained in various ways we can not  for example  allocate more than the total number of available frames  unless there is page sharing   we must also allocate at least a minimum number of frames here  we look more closely at the latter requirement one reason for allocating at least a minimum number of frames involves performance obviously  as the number of frames allocated to each process decreases  the page-fault rate increases  slowing process execution in addition  remember that when a page fault occurs before an executing ilcstruction is complete  the instruction must be restarted consequently we must have enough frames to hold all the different pages that any single ilcstruction can reference for example  consider a machine in which all memory-reference instructions may reference only one memory address in this case  we need at least one frame for the instruction and one frame for the mernory reference in addition  if one-level indirect addressing is allowed  for example  a load instruction on page 16 can refer to an address on page 0  which is an indirect reference to page 23   then paging requires at least three frames per process think about what might happen if a process had only two frames the minimum number of frames is defined by the computer architecture for example  the move instruction for the pdp-11 includes more than one word for some addressing modes  and thus the ilcstruction itself may straddle two pages in addition  each of its two operands may be indirect references  for a total of six frames another example is the ibm 370 mvc instruction since the 9.5 383 instruction is from storage location to storage location  it takes 6 bytes and can straddle two pages the block of characters to move and the area to which it is to be m.oved can each also straddle two pages this situation would require six frames the worst case occurs when the mvc instruction is the operand of an execute instruction that straddles a page boundary ; in this case  we need eight frames the worst-case scenario occurs in computer architectures that allow multiple levels of indirection  for example  each 16-bit word could contain a 15-bit address plus a 1-bit indirect indicator   theoretically  a simple load instruction could reference an indirect address that could reference an indirect address  on another page  that could also reference an indirect address  on yet another page   and so on  until every page in virtual memory had been touched thus  in the worst case  the entire virtual memory must be in physical memory to overcome this difficulty  we must place a limit on the levels of indirection  for example  limit an instruction to at most 16levels of indirection   when the first indirection occurs  a counter is set to 16 ; the counter is then decremented for each successive irtdirection for this instruction if the counter is decremented to 0  a trap occurs  excessive indirection   this limitation reduces the maximum number of memory references per instruction to 17  requiring the same number of frames whereas the minimum number of frames per process is defined by the architecture  the maximum number is defined by the amount of available physical memory in between  we are still left with significant choice in frame allocation 9.5.2 allocation algorithms the easiest way to split m frames among n processes is to give everyone an equal share  m/n frames for instance  if there are 93 frames and five processes  each process will get 18 frames the three leftover frames can be used as a free-frame buffer pool this scheme is called an alternative is to recognize that various processes will need differing amounts of memory consider a system with a 1-kb frame size if a small student process of 10 kb and an interactive database of 127 kb are the only two processes running in a system with 62 free frames  it does not make much sense to give each process 31 frames the student process does not need more than 10 frames  so the other 21 are  strictly speaking  wasted to solve this problem  we can use in which we allocate available memory to each process according to its size let the size of the virtual memory for process p ; be s ;  and define s = ls ;  then  if the total number of available frames is m  we allocate a ; frames to process p ;  where a ; is approximately a ; = s ; /s x m 384 chapter 9 of course  we must adjust each ai to be an integer that is greater than the ncinimum number of frames required by tl1e instruction set  with a sum not exceeding m with proportional allocation  we would split 62 frames between two processes  one of 10 pages and one of 127 pages  by allocating 4 frames and 57 frames  respectively  since 10/137 x 62 ~ 4  and 127/137 x 62 ~ 57 in this way  both processes share the available frames according to their needs  rather than equally in both equal and proportional allocation  of course  the allocation may vary according to the multiprogramming level if the multiprogramming level is increased  each process will lose some frames to provide the memory needed for the new process conversely  if the multiprogramming level decreases  the frames that were allocated to the departed process can be spread over the remaining processes notice that  with either equal or proportional allocation  a high-priority process is treated the same as a low-priority process by its definition  however  we may want to give the high-priority process more memory to speed its execution  to the detriment of low-priority processes one solution is to use a proportional allocation scheme wherein the ratio of frames depends not on the relative sizes of processes but rather on the priorities of processes or on a combination of size and priority 9.5.3 global versus local allocation another important factor in the way frames are allocated to the various processes is page replacement with multiple processes competing for frames  we can classify page-replacement algorithms into two broad categories  ; .no ' '-c'u ~ ' ' and local global replacement allows a process to a replacement frame from the set of all frames  even if that frame is currently allocated to some other process ; that is  one process can take a frame from another local replacement requires that each process select from only its own set of allocated frames for example  consider an allocation scheme wherein we allow high-priority processes to select frames from low-priority processes for replacement a process can select a replacement from among its own frames or the frames of any lower-priority process this approach allows a high-priority process to increase its frame allocation at the expense of a low-priority process with a local replacement strategy  the number of frames allocated to a process does not change with global replacement  a process may happen to select only frames allocated to other processes  thus increasing the number of frames allocated to it  assuming that other processes do not choose its frames for replacement   one problem with a global replacement algorithm is that a process can not control its own page-fault rate the set of pages in memory for a process depends not only on the paging behavior of that process but also on the paging behavior of other processes therefore  the same process may perform quite 9.5 385 differently  for example  taking 0.5 seconds for one execution and 10.3 seconds for the next execution  because of totally external circuntstances such is not the case with a local replacement algorithm under local replacement  the set of pages in memory for a process is affected by the paging behavior of only that process local replacement might hinder a process  however  by not making available to it other  less used pages of memory thus  global replacement generally results in greater system throughput and is therefore the more common method 9.5.4 non-uniform memory access thus far in our coverage of virtual memory  we have assumed that all main memory is created equal-or at least that it is accessed equally on many computer systems  that is not the case often  in systems with multiple cpus  section 1.3.2   a given cpu can access some sections of main memory faster than it can access others these performance differences are caused by how cpus and memory are interconnected in the system frequently  such a system is made up of several system boards  each containing multiple cpus and some memory the system boards are interconnected in various ways  ranging from system busses to high-speed network connections like infiniband as you might expect  the cpus on a particular board can access the memory on that board with less delay than they can access memory on other boards in the system systems in which memory access times vary significantly are known collectively as systems  and without exception  they are slower than systems in which memory and cpus are located on the same motherboard managing which page frames are stored at which locations can significantly affect performance in numa systems if we treat memory as uniform in such a system  cpus may wait significantly longer for memory access than if we modify memory allocation algorithms to take numa into account similar changes must be rnade to the scheduling system the goal of these changes is to have memory frames allocated as close as possible to the cpu on which the process is running the definition of close is with minimum latency  which typically means on the same system board as the cpu the algorithmic changes consist of having the scheduler track the last cpu on which each process ran if the scheduler tries to schedule each process onto its previous cpu  and the memory-management system tries to allocate frames for the process close to the cpu on which it is being scheduled  then improved cache hits and decreased memory access times will result the picture is more complicated once threads are added for example  a process with many running threads may end up with those threads scheduled on many different system boards how is the memory to be allocated in this case solaris solves the problem by creating an entity in the kernel each lgroup gathers together close cpus and memory in fact  there is a hierarchy of lgroups based on the amount of latency between the groups solaris tries to schedule all threads of a process and allocate all memory of a process within an lgroup if that is not possible  it picks nearby lgroups for the rest of the resources needed in this manner  overall memory latency is minimized  and cpu cache hit rates are maximized 386 chapter 9 9.6 if the number of frames allocated to a low-priority process falls below the minimum number required by the computer architecture  we must suspend that process 's execution we should then page out its remaining pages  freeing all its allocated frames this provision introduces a swap-in  swap-out level of intermediate cpu scheduling in fact  look at any process that does not have enough frames if the process does not have the num.ber of frames it needs to support pages in active use  it will quickly page-fault at this point  it must replace some page however  since all its pages are in active use  it must replace a page that will be needed again right away consequently  it quickly faults again  and again  and again  replacing pages that it must back in immediately this high paging activity is called a process is thrashing if it is spending more time paging than executing 9.6.1 cause of thrashing thrashing results in severe performance problems consider the following scenario  which is based on the actual behavior of early paging systems the operating system monitors cpu utilization if cpu utilization is too low  we increase the degree of multiprogramming by introducing a new process to the system a global page-replacement algorithm is used ; it replaces pages without regard to the process to which they belong now suppose that a process enters a new phase in its execution and needs more frames it starts faulting and taking frames away from other processes these processes need those pages  however  and so they also fault  taking frames from other processes these faulting processes must use the pagin.g device to swap pages in and out as they queue up for the paging device  the ready queue empties as processes wait for the paging device  cpu utilization decreases the cpu scheduler sees the decreasing cpu utilization and increases the degree of multiprogramming as a result the new process tries to get started by taking frames from running processes  causing more page faults and a longer queue for the paging device as a result  cpu utilization drops even further  and the cpu scheduler tries to increase the degree of multiprogramming even more thrashing has occurred  and system throughput plunges the pagefault rate increases tremendously as a result  the effective m.emory-access time increases no work is getting done  because the processes are spending all their time paging this phenomenon is illustrated in figure 9.18  in which cpu utilization is plotted against the degree of multiprogramming as the degree of multiprogramming increases  cpu utilization also ilccreases  although more slowly  until a maximum is reached if the degree of multiprogramming is increased even further  thrashing sets in  and cpu utilization drops sharply at this point  to increase cpu utilization and stop thrashing  we must decrease the degree of multiprogramming we can limit the effects of thrashing by using a  or with local replacement  if one process starts thrashing  it can not frames from another process and cause the latter to thrash as well however  the problem is not entirely solved if processes are 9.6 387 degree of multiprogramming figure 9.18 thrashing thrashing  they will be in the queue for the paging device most of the time the average service time for a page fault will increase because of the longer average queue for the paging device thus  the effective access time will increase even for a process that is not thrashing to prevent thtashing  we must provide a process with as many frames as it needs but how do we know how many frames it needs there are several teclmiques the working-set strategy  section 9.6.2  starts by looking at how frames a process is actually using this approach defines the locality of process execution the locality model states that  as a process executes  it moves from locality to locality a locality is a set of pages that are actively used together  figure 9.19   a program is generally composed of several different localities  which may overlap for example  when a function is called  it defines a new locality in this locality  memory references are made to the instructions of the function call  its local variables  and a subset of the global variables when we exit the function  the process leaves this locality  since the local variables and instructions of the function are no longer in active use we may return to this locality later thus  we see that localities are defined by the program structure and its data structures the locality model states that all programs will exhibit this basic memory reference structure note that the locality model is the unstated principle behind the caching discussions so far in this book if accesses to any types of data were random rather than patterned  caching would be useless suppose we allocate enough frames to a process to accommodate its current locality it will fault for the pages in its locality until all these pages are in memory ; then  it will not fault again until it changes localities if we do not allocate enough frames to accommodate the size of the current locality  the process will thrash  since it can not keep in memory all the pages that it is actively using 9.6.2 working-set model as mentioned  the is based on the assumption of locality this model uses a paramete1 ~ / '    to define the vrindovv the idea 388 chapter 9 32 ~ ~  ~ ~ = = ~ ~ ~ ~ ~ wl ~ ~ #  ~ ~  ~ ~ ~  \ jjl  jlli111 28  j   j   !  0 0  lj 26 i ' c 0 i e i  lj e execution time  figure 9.19 locality in a memory-reference pattern is to examine the most recent 6 references the set of pages in the most recent 6 page references is the  figure 9.20   if a page is in active use  it will be in the working set if it is no longer being used  it will drop from the working set 6 time units after its last reference thus  the working set is an approximation of the program 's locality for example  given the sequence of memory references shown in figure 9.20  if 6 = 10 memory references  then the working set at time t1 is  1  2  5  6  7   by time t2  the working set has changed to  3  4   the accuracy of the working set depends on the selection of 6 if 6 is too small  it will not encompass the entire locality ; if 6 is too large  it may overlap 9.6 page reference table    2 6 1 5 7 7 7 7 5 1 6 2 3 4 1 2 3 4 4 4 3 4 3 4 4 4 1 3 2 3 4 4 4 3 4 4 4  ~ ~ r ~ r t1 ws  t1  =  1 ,2,5,6,7  figure 9.20 working-set model 389 several localities in the extrem.e  if l is infinite  the working set is the set of pages touched during the process execution the most important property of the working set  then  is its size if we compute the working-set size  wss ;  for each process in the system  we can then consider that where dis the total demand for frames each process is actively using the pages in its working set thus  process i needs wss ; frames if the total demand is greater than the total number of available frames  d m   thrashing will occur  because some processes will not have enough frames once l has been selected  use of the working-set model is simple the operating system monitors the working set of each process and allocates to that working set enough frames to provide it with its working-set size if there are enough extra frames  another process can be initiated if the sum of the working-set sizes increases  exceeding the total number of available frames  the operating system selects a process to suspend the process 's pages are written out  swapped   and its frames are reallocated to other processes the suspended process can be restarted later this working-set strategy prevents thrashing while keeping the degree of multiprogramming as high as possible thus  it optimizes cpu utilization the difficulty with the working-set model is keeping track of the working set the working-set window is a moving window at each memory reference  a new reference appears at one end and the oldest reference drops off the other end a page is in the working set if it is referenced anywhere in the working-set window we can approximate the working-set model with a fixed-interval timer interrupt and a reference bit for example  assum.e that l equals 10,000 references and that we can cause a timer interrupt every 5,000 references when we get a timer interrupt  we copy and clear the reference-bit values for each page thus  if a page fault occurs  we can examine the current reference bit and two in-memory bits to determine whether a page was used within the last 10,000 to 15,000 references if it was used  at least one of these bits will be on if it has not been used  these bits will be off those pages with at least one bit on will be considered to be in the working set note that this arrangement is not entirely accurate  because we can not tell where  within an interval of 5,000  a reference occurred we can reduce the uncertainty by increasing the number of history bits and the frequency of interrupts  for example  10 bits and interrupts every 1,000 references   however  the cost to service these more frequent interrupts will be correspondingly higher 390 chapter 9 9.7 number of frames figure 9.21 page-fault frequency 9.6.3 page-fault frequency the working-set model is successful  and knowledge of the working set can be useful for prepaging  section 9.9.1   but it seems a clumsy way to control thrashilcg a strategy that uses the takes a more direct approach the specific problem is how to prevent thrashilcg thrashing has a high page-fault rate thus  we want to control the page-fault rate when it is too high  we know that the process needs more frames conversely  if the page-fault rate is too low  then the process may have too many frames we can establish upper and lower bounds on the desired page-fault rate  figure 9.21   if the actual page-fault rate exceeds the upper limit  we allocate the process another frame ; if the page-fault rate falls below the lower limit  we remove a frame from the process thus  we can directly measure and control the page-fault rate to prevent thrashing as with the working-set strategy  we may have to suspend a process if the page-fault rate ilccreases and no free frames are available  we must select some process and suspend it the freed frames are then distributed to processes with high page-fault rates consider a sequential read of a file on disk using the standard system calls open   ,read    and write    each file access requires a system call and disk access alternatively  we can use the virtual memory techniques discussed so far to treat file i/0 as routine memory accesses this approach  known as a file  allows a part of the virtual address space to be logically associated with the file as we shall see  this can lead to significant performance increases when performing i/0 9.7 391 working sets and page faultrates there is a directrelationship between the working set of a process and its page-fault rate typically as shown in figure 9.20  the working set ofa process changes pver time as references to data and code sections move from one locality to another assuming there is sufficient memory to store the working set of .a process  that is  the processis 11.ot thrashing   tbe page-fault rate of the process will transition between peaks and valleys over time this general behavior is shown in figure 9.22 page fault rate working set time figure 9.22 page fault rate over time a peak in the page-fault rate occurs when we begin demand-paging a new locality however  once the working set of this new locality is in memory  the page-fault rate falls when the process moves to a new working set  the page  fault rate rises toward a peak once again  returning to a lower rate once the new working set is loaded into memory the span oftime between the start of one peak and the start of thenext peak represents the transition from one working set to another 9.7.1 basic mechanism memory mapping a file is accomplished by mapping a disk block to a page  or pages  in memory initial access to the file proceeds through ordinary demand paging  resulting in a page fault however  a page-sized portion of the file is read from the file system into a physical page  some systems may opt to read in more than a page-sized chunk of memory at a time   subsequent reads and writes to the file are handled as routine memory accesses  thereby simplifying file access and usage by allowing the system to manipulate files through memory rather than incurring the overhead of using the read   and write   system calls similarly  as file l/0 is done in memory as opposed to using system calls that involve disk i/0  file access is much faster as well note that writes to the file mapped in memory are not necessarily imm.ediate  synchronous  writes to the file on disk some systems may choose to update the physical file when the operating system periodically checks 392 chapter 9 whether the page in memory has been modified when the file is closed  all the memory-mapped data are written back to disk and ren loved from the virtual memory of the process some operating systems provide memory mapping only through a specific system call and use the standard system calls to perform all other file i/0 however  some systems choose to memory-map a file regardless of whether the file was specified as memory-mapped let 's take solaris as an example if a file is specified as memory-mapped  using the mmap   system call   solaris maps the file into the address space of the process if a file is opened and accessed using ordinary system calls  such as open    read    and write    solaris still memory-maps the file ; however  the file is mapped to the kernel address space regardless of how the file is opened  then  solaris treats all file i/0 as memory-mapped  allowing file access to take place via the efficient memory subsystem multiple processes may be allowed to map the same file concurrently  to allow sharing of data writes by any of the processes modify the data in virtual memory and can be seen by all others that map the same section of the file given our earlier discussions of virtual memory  it should be clear how the sharing of memory-mapped sections of memory is implemented  the virtual memory map of each sharing process points to the same page of physical memory-the page that holds a copy of the disk block this memory sharing is illustrated in figure 9.23 the memory-mapping system calls can also support copy-on-write functionality  allowing processes to share a file in read-only mode but to have their own copies of any data they modify so that r i i i 1  r   ; i i 1 1 i -1 ii i i i i i j---r ' -rl..-r i i i i -r ' i i i i 1 -1 i i 1 _ i i i i i f +  =   .....c.c ~ ..-'---r ~   i i  .l i j i i i 1 i i i i i i i i i i l_ ~ i process a 1 1 1 virtual memory  ~ 1  disk file figure 9.23 memory-mapped files process b virtual memory 9.7 memory-mapped file figure 9.24 shared memory in windows using memory-mapped 1/0 393 access to the shared data is coordinated  the processes involved might use one of the mechanisms for achieving mutual exclusion described in chapter 6 in many ways  the sharing of memory-mapped files is similar to shared memory as described in section 3.4.1 not all systems use the same mechanism for both ; on unix and linux systems  for example  memory mapping is accomplished with the mmap   system call  whereas shared memory is achieved with the posix-compliant shmget   and shmat   systems calls  section 3.5.1   on windows nt  2000  and xp systems  howeve1 ~ shared memory is accomplished by memory mapping files on these systems  processes can communicate using shared memory by having the communicating processes memory-map the same file into their virtual address spaces the memorymapped file serves as the region of shared memory between the communicating processes  figure 9.24   in the following section  we illustrate support in the win32 api for shared memory using memory-mapped files 9.7.2 shared memory in the win32 api the general outline for creating a region of shared memory using memorymapped files in the win32 api involves first creating a file mapping for the file to be mapped and then establishing a view of the mapped file in a process 's virtual address space a second process can then open and create a view of the mapped file in its virtual address space the mapped file represents the shared-menwry object that will enable communication to take place between the processes we next illustrate these steps in more detail in this example  a producer process first creates a shared-memory object using the memory-mapping features available in the win32 api the producer then writes a message to shared m.emory after that  a consumer process opens a mapping to the shared-memory object and reads the message written by the consum.er to establish a memory-mapped file  a process first opens the file to be mapped with the createfile   function  which returns a handle to the opened file the process then creates a mapping of this file handle using the createfilemapping   function once the file mapping is established  the process then establishes a view of the mapped file in its virtual address space with the mapviewdffile   function the view of the mapped file represents the portion of the file being mapped in the virtual address space of the process 394 chapter 9 # include windows.h # include stdio.h int main  int argc  char argv      handle hfile  hmapfile ; lpvoid lpmapaddress ; hfile = createfile  temp.txt  //file name genericjread i generic_write  // read/write access 0  ii no sharing of the file null  //default security open_always  //open new or existing file file_attribute_normal  //routine file attributes null  ; //no file template hmapfile = createfilemapping  hfile  //file handle null  //default security pagejreadwrite  //read/write access to mapped pages 0  ii map entire file 0  text  sharedobject   ; //named shared memory object lpmapaddress = mapviewdffile  hmapfile  //mapped object handle filejmap_all_access  //read/write access 0  ii mapped view of entire file 0  0  ; ii write to shared memory sprintf  lpmapaddress  shared memory message  ; unmapviewoffile  lpmapaddress  ; closehandle  hfile  ; closehandle  hmapfile  ; figure 9.25 producer writing to shared memory using the win32 api -the entire file or only a portion of it may be mapped we illustrate this sequence in the program shown in figure 9 .25  we eliminate much of the error checking for code brevity  the call to createfilemapping   creates a named shared-memory object called sharedobj ect the consumer process will communicate using this shared-memory segment by creating a mapping to the same named object the producer then creates a view of the memory-mapped file in its virtual address space by passing the last three parameters the value 0  it indicates that the mapped view is the entire file it could instead have passed values specifying an offset and size  thus creating a view containing only a subsection of the file  it is important to note that the entire mapping may not be loaded # include windows.h # include stdio.h int main  int argc  char argv     handle hmapfile ; lpvoid lpmapaddress ; 9.7 395 hmapfile = openfilemapping  file_map_all_access  // r/w access false  //no inheritance  text  sharedobject   ; //name of mapped file object lpmapaddress = mapviewoffile  hmapfile  //mapped object handle filejmap_all_access  //read/write access 0  ii mapped view of entire file 0  0  ; ii read from shared memory printf  read message % s  lpmapaddress  ; unmapviewoffile  lpmapaddress  ; closehandle  hmapfile  ; figure 9.26 consumer reading from shared memory using the win32 api into memory when the mapping is established rather  the mapped file may be demand-paged  thus bringing pages into memory only as they are accessed  the mapviewoffile   fm1ction returns a pointer to the shared-memory object ; any accesses to this memory location are thus accesses to the memory-mapped file in this ii1stance  the producer process writes the message shared memory message to shared memory a program illustrating how the consumer process establishes a view of the named shared-memory object is shown in figure 9.26 this program is somewhat simpler than the one shown in figure 9.25  as all that is necessary is for the process to create a mapping to the existii1g named shared-memory object the consumer process must also create a view of the mapped file  just as the producer process did ii1 the program in figure 9.25 the consumer then reads from shared memory the message shared memory message thatwas written by the producer process finally  both processes remove the view of the mapped file with a call to unmapviewoffile    we provide a programming exercise at the end of this chapter using shared memory with memory mapping in the win32 api 9.7.3 memory-mapped i/0 in the case of i/0  as mentioned in section 1.2.1  each i/0 controller includes registers to hold commands and the data being transferred usually  special i/0 instructions allow data transfers between these registers and system memory 396 chapter 9 9.8 to allow more convenient access to i/0 devices1 many computer architectures provide in this case/ ranges of memory addresses are set aside and are mapped to the device registers reads and writes to these memory addresses cause the data to be transferred to and from the device registers this method is appropriate for devices that have fast response times/ such as video controllers in the ibm pc each location on the screen is mapped to a n1.emory location displaying text on the screen is almost as easy as writing the text into the appropriate memory-mapped locations memory-mapped i/o is also convenient for other devices/ such as the serial and parallel ports used to connect modems and printers to a computer the cpu transfers data through these kinds of devices by reading and writing a few device registers/ called an i/0 to send out a long string of bytes through a memory-mapped serial port1 the cpu writes one data byte to the data register and sets a bit in the control register to signal that the byte is available the device takes the data byte and then clears the bit in the control register to signal that it is ready for the next byte then the cpu can transfer the next byte if the cpu uses polling to watch the control bit/ constantly looping to see whether the device is ready/ this method of operation is called if the cpu does not poll the control bit/ but instead receives an interrupt when the device is ready for the next byte/ the data transfer is said to be when a process running in user rnode requests additional memory/ pages are allocated from the list of free page frames maintained by the kernel this list is typically populated using a page-replacement algorithm such as those discussed in section 9.4 and most likely contains free pages scattered throughout physical memory/ as explained earlier remember/ too/ that if a user process requests a single byte of memory/ internal fragmentation will result/ as the process will be granted an entire page frame kernel memory/ however1 is often allocated from a free-memory pool different from the list used to satisfy ordinary user-mode processes there are two primary reasons for this  the kernel requests memory for data structures of varying sizes  some of which are less than a page in size as a result1 the kernel must use memory conservatively and attempt to minimize waste due to fragmentation this is especially important because many operating systems do not subject kernel code or data to the paging system 2 pages allocated to user-mode processes do not necessarily have to be in contiguous physical memory however/ certain hardware devices interact directly with physical memory-without the benefit of a virtual memory interface-and consequently may require memory residing in physically contiguous pages in the following sections/ we examine two strategies for managing free memory that is assigned to kernel processes  the buddy system and slab allocation 9.8 397 9.8.1 buddy system tbe buddy system allocates memory from a fixed-size segment consisting of physically contiguous pages memory is allocated from this segment using a power-of-2 allocator  which satisfies requests in units sized as a power of 2  4 kb  8 kb  16 kb  and so forth   a request in units not appropriately sized is rounded up to the next highest power of 2 for example  if a request for 11 kb is made  it is satisfied with a 16-kb segment let 's consider a simple example assume the size of a memory segment is initially 256 kb and the kernel requests 21 kb of memory the segment is initially divided into two buddies-which we will call al and ar -each 128 kb in size one of these buddies is further divided into two 64-kb buddiesbland br however  the next-highest power of 2 from 21 kb is 32 kb so either bt or br is again divided into two 32-kb buddies  cl and cr one of these buddies is used to satisfy the 21-kb request this scheme is illustrated in figure 9.27  where cl is the segment allocated to the 21 kb request an advantage of the buddy system is how quickly adjacent buddies can be combined to form larger segments using a teclmique known as coalescing in figure 9.27  for example  when the kernel releases the cl unit it was allocated  the system can coalesce c l and c r into a 64-kb segment this segment  b l  can in turn be coalesced with its buddy b r to form a 128-kb segment ultimately  we can end up with the original256-kb segment the obvious drawback to the buddy system is that rounding up to the next highest power of 2 is very likely to cause fragmentation within allocated segments for example  a 33-kb request can only be satisfied with a 64 kb segment in fact  we can not guarantee that less than 50 percent of the allocated unit will be wasted due to internal fragmentation in the following section  we explore a memory allocation scheme where no space is lost due to fragmentation physically contiguous pages 256 kb figure 9.27 buddy system allocation 398 chapter 9 9.8.2 slab allocation a second strategy for allocating kernel memory is known as a is made up of one or nwre physically contiguous pages a consists of one or more slabs there is a single cache for each unique kernel data structure -for example  a separate cache for the data structure representing process descriptors  a separate cache for file objects  a separate cache for semaphores  and so forth each cache is populated with that are instantiations of the kernel data structure the cache represents for example  the cache representing semaphores stores instances of semaphore objects  the cache representing process descriptors stores instances of process descriptor objects  and so forth the relationship between slabs  caches  and objects is shown in figure 9.28 the figure shows two kernel objects 3 kb in size and three objects 7 kb in size these objects are stored in their respective caches the slab-allocation algorithm uses caches to store kernel objects when a cache is created  a number of objects-which are initially marked as free-are allocated to the cache the number of objects in the cache depends on the size of the associated slab for example  a 12-kb slab  made up of three continguous 4-kb pages  could store six 2-kb objects initially  all objects in the cache are marked as free when a new object for a kernel data structure is needed  the allocator can assign any free object from the cache to satisfy the request the object assigned from the cache is marked as used let 's consider a scenario in which the kernel requests memory from the slab allocator for an object representing a process descriptor in linux systems  a process descriptor is of the type struct task_struct  which requires approximately 1.7 kb of memory when the linux kernel creates a new task  it requests the necessary memory for the struct task_struct object from its cache the cache will fulfill the request using a struct task_struct object that has already been allocated in a slab and is marked as free in linux  a slab may be in one of three possible states  kernel objects slabs 3-kb objects 7-kb objects figure 9.28 slab allocation physically contiguous pages 9.9 9.9 full all objects in the slab are marked as used empty all objects in the slab are marked as free partial the slab consists of both used and free objects 399 the slab allocator first attempts to satisfy the request with a free object in a partial slab if none exist  a free object is assigned from an empty slab if no empty slabs are available  a new slab is allocated from contiguous physical pages and assigned to a cache ; memory for the object is allocated from this slab the slab allocator provides two main benefits  no memory is wasted due to fragmentation fragn entation is not an issue because each unique kernel data structure has an associated cache  and each cache is made up of one or more slabs that are divided into chunks the size of the objects being represented thus  when the kernel requests memory for an object  the slab allocator returns the exact amount of memory required to represent the object memory requests can be satisfied quickly the slab allocation scheme is thus particularly effective for mm aging memory when objects are frequently allocated and deallocated  as is often the case with requests from the kernel the act of allocating-and releasing-memory can be a time-consuming process however  objects are created in advance and thus can be quickly allocated from the cache furthermore  when the kernel has finished with an object and releases it  it is marked as free and returned to its cache  thus making it immediately available for subsequent requests fi om the kernel the slab allocator first appeared in the solaris 2.4 kernel because of its general-purpose nature  this allocator is now also used for certain user-mode memory requests in solaris linux originally used the buddy system ; however  beginning with version 2.2  the linux kernel adopted the slab allocator the major decisions that we make for a paging system are the selections of a replacement algorithm and an allocation policy  which we discussed earlier in this chapter there are many other considerations as well  and we discuss several of them here 9.9.1 prepaging an obvious property of pure demand paging is the large number of page faults that occur when a process is started this situation results from trying to get the initial locality into memory the same situation may arise at other times for instance  when a swapped-out process is restarted  all its are on the disk  and each must be brought in by its own page fault is an attempt to prevent this high level of initial paging the strategy is to bring into memory at 400 chapter 9 one tin1.e all the pages that will be needed some operating systerns-notably solaris-prepage the page frames for small files in a system using the working-set model  for example  we keep with each process a list of the pages in its working set if we must suspend a process  due to an i/0 wait or a lack of free frames   we remember the working set for that process when the process is to be resumed  because i/0 has finished or enough free frames have become available   we automatically bring back into memory its entire working set before restarting the process prepaging may offer an advantage in some cases the question is simply whether the cost of using prepaging is less than the cost of servicing the corresponding page faults it may well be the case that many of the pages brought back into memory by prepaging will not be used assume that s pages are prepaged and a fraction a of these s pages is actually used  0  '    a  '    1   the question is whether the cost of the s .a saved page faults is greater or less than the cost of prepaging s   1  a  unnecessary pages if a is close to 0  prepaging loses ; if a is close to 1  prepaging wins 9.9.2 page size the designers of an operating system for an existing machine seldom have a choice concerning the page size however  when new machines are being designed  a decision regarding the best page size must be made as you might expect  there is no single best page size rather  there is a set of factors that support various sizes page sizes are invariably powers of 2  generally ranging from 4,096  212  to 4,194,304  222  bytes how do we select a page size one concern is the size of the page table for a given virtual memory space  decreasing the page size increases the number of pages and hence the size of the page table for a virtual memory of 4 mb  222   for example  there would be 4,096 pages of 1,024 bytes but only 512 pages of 8,192 bytes because each active process must have its own copy of the page table  a large page size is desirable memory is better utilized with smaller pages  however if a process is allocated memory starting at location 00000 and continuing until it has as much as it needs  it probably will not end exactly on a page boundary thus  a part of the final page must be allocated  because pages are the units of allocation  but will be unused  creating internal fragmentation   assuming independence of process size and page size  we can expect that  on the average  half of the final page of each process will be wasted this loss is only 256 bytes for a page of 512 bytes but is 4,096 bytes for a page of 8,192 bytes to minimize internal fragmentation  then  we need a small page size another problem is the time required to read or write a page i/0 time is composed of seek  latency  and transfer times transfer time is proportional to the amount transferred  that is  the page size  -a fact that would seem to argue for a small page size howeve1 ~ as we shall see in section 12.1.1  latency and seek time normally dwarf transfer time at a transfer rate of 2 mb per second  it takes only 0.2 milliseconds to transfer 512 bytes latency time  though  is perhaps 8 milliseconds and seek time 20 milliseconds of the total i/0 time  28.2 milliseconds   therefore  only 1 percent is attributable to the actual transfer doubling the page size increases i/0 time to only 28.4 milliseconds it takes 28.4 milliseconds to read a single page of 1,024 bytes but 9.9 401 56.4 milliseconds to read the sam.e amount as two pages of 512 bytes each thus  a desire to minimize 1/0 time argues for a larger page size with a smaller page size  though  to tall /0 should be reduced  since locality will be improved a smaller page size allows each page to match program locality more accurately for example  consider a process 200 kb in size  of which only half  100 kb  is actually used in an execution if we have only one large page  we must bring in the entire page  a total of 200 kb transferred and allocated if instead we had pages of only 1 byte  then we could bring in only the 100 kb that are actually used  resulting in only 100 kb transferred and allocated with a smaller page size  we have better allowing us to isolate only the memory that is actually needed with a larger page size  we must allocate and transfer not only what is needed but also anything else that happens to be in the page  whether it is needed or not thus  a smaller page size should result in less i/0 and less total allocated memory but did you notice that with a page size of 1 byte  we would have a page fault for each byte a process of 200 kb that used only half of that memory would generate only one page fault with a page size of 200 kb but 102,400 page faults with a page size of 1 byte each page fault generates the large amount of overhead needed for processing the interrupt  saving registers  replacing a page  queueing for the paging device  and updating tables to minimize the number of page faults  we need to have a large page size other factors must be considered as well  such as the relationship between page size and sector size on the paging device   the problem has no best answer as we have seen  some factors  internal fragmentation  locality  argue for a small page size  whereas others  table size  i/0 time  argue for a large page size however  the historical trend is toward larger page sizes indeed  the first edition of operating system concepts  1983  used 4,096 bytes as the upper bound on page sizes  and this value was the most common page size in 1990 modern systems may now use much larger page sizes  as we will see in the following section 9.9.3 tlb reach in chapter 8  we introduced the of the tlb recall that the hit ratio for the tlb refers to the percentage of virtual address translations that are resolved in the tlb rather than the page table clearly  the hit ratio is related to the number of entries in the tlb  and the way to increase the hit ratio is by increasing the number of entries in the tlb this  however  does not come cheaply  as the associative memory used to construct the tlb is both expensive and power hungry related to the hit ratio is a similar metric  the the tlb reach refers to the amount of memory accessible from the tlb and is simply the number of entries multiplied by the page size ideally  the working set for a process is stored in the tlb if it is not  the process will spend a considerable amount of time resolving memory references in the page table rather than the tlb if we double the number of entries in the tlb  we double the tlb reach however  for some memory-intensive applications  this may still prove insufficient for storing the working set another approacl1 for increasing the tlb reach is to either increase the size of the page or provide multiple page sizes if we increase the page size-say  402 chapter 9 from 8 kb to 32 kb-we quadruple the tlb reach however  this may lead to an increase in fragmentation for some applications that do not require such a large page size as 32 kb alternatively  an operating system may provide several different page sizes for example  the ultrasparc supports page sizes of 8 kb  64 kb  512 kb  and 4mb of these available pages sizes  solaris uses both 8-kb and 4-mb page sizes and with a 64-entry tlb  the tlb reach for solaris ranges from 512 kb with 8-kb pages to 256mb with 4-mb pages for the majority of applications  the 8-kb page size is sufficient  although solaris maps the first 4 mb of kernel code and data with two 4-mb pages solaris also allows applications-such as databases-to take advantage of the large 4-mb page size providing support for multiple page sizes requires the operating system -not hardware-to manage the tlb for example  one of the fields in a tlb entry must indicate the size of the page frame corresponding to the tlb entry managing the tlb in software and not hardware comes at a cost in performance howeve1 ~ the increased hit ratio and tlb reach offset the performance costs indeed  recent trends indicate a move toward softwaremanaged tlbs and operating-system support for multiple page sizes the ultrasparc  mips  and alpha architectures employ software-managed tlbs the powerpc and pentium manage the tlb in hardware 9.9.4 inverted page tables section 8.5.3 introduced the concept of the inverted page table the purpose of this form of page management is to reduce the amount of physical memory needed to track virtual-to-physical address translations we accomplish this savings by creating a table that has one entry per page of physical memory  indexed by the pair process-id  page-number  because they keep information about which virtual memory page is stored in each physical frame  inverted page tables reduce the amount of physical memory needed to store this information however  the inverted page table no longer contains complete information about the logical address space of a process  and that information is required if a referenced page is not currently in memory demand paging requires this information to process page faults for the information to be available  an external page table  one per process  must be kept each such table looks like the traditional per-process page table and contains information on where each virtual page is located but do external page tables negate the utility of inverted page tables since these tables are referenced only when a page fault occurs  they do not need to be available quickly instead  they are themselves paged in and out of memory as necessary unfortunately  a page fault may now cause the virtual memory n1.anager to generate another page fault as it pages in the external page table it needs to locate the virtual page on the backing store this special case requires careful handling in the kernel and a delay in the page-lookup processing 9.9.5 program structure demand paging is designed to be transparent to the user program in many cases  the user is completely unaware of the paged nature of memory in other cases  however  system performance can be improved if the user  or compiler  has an awareness of the underlying demand paging 9.9 403 let 's look at a contrived but informative example assume that pages are 128 words in size consider a c program whose function is to initialize to 0 each element of a 128-by-128 array the following code is typical  inti  j ; int  128j  128j data ; for  j = 0 ; j 128 ; j + +  for  i = 0 ; i 128 ; i + +  data  ij  jj = 0 ; notice that the array is stored row major ; that is  the array is stored data  oj  oj  data  oj  1j   data  oj  127j  data  1j  oj  data  1j  1j   data  127j  127j for pages of 128 words  each row takes one page thus  the preceding code zeros one word in each page  then another word in each page  and so on if the operating system allocates fewer than 128 frames to the entire program  then its execution will result in 128 x 128 = 16,384 page faults in contrast  suppose we change the code to inti  j ; int  128j  128j data ; for  i = 0 ; i 128 ; i + +  for  j = 0 ; j 128 ; j + +  data  ij  jj = 0 ; this code zeros all the words on one page before starting the next page  reducing the number of page faults to 128 careful selection of data structures and programming structures can increase locality and hence lower the page-fault rate and the number of pages in the working set for example  a stack has good locality  since access is always made to the top a hash table  in contrast  is designed to scatter references  producing bad locality of course  locality of reference is just one measure of the efficiency of the use of a data structure other heavily weighted factors include search speed  total number of memory references  and total number of pages touched at a later stage  the compiler and loader can have a sigicificant effect on paging separating code and data and generating reentrant code means that code pages can be read-only and hence will never be modified clean pages do not have to be paged out to be replaced the loader can avoid placing routines across page boundaries  keeping each routine completely in one page routines that call each other many times can be packed into the same page this packaging is a variant of the bin-packing problem of operations research  try to pack the variable-sized load segments into the fixed-sized pages so that interpage references are minimized such an approach is particularly useful for large page sizes the choice of programming language can affect paging as well for example  c and c + + use pointers frequently  and pointers tend to randomize access to memory  thereby potentially diminishing a process 's locality some studies have shown that object-oriented programs also tend to have a poor locality of reference 404 chapter 9 9.9.6 1/0 interlock when demand paging is used  we sometimes need to allow some of the pages to be in n emory one such situation occurs when i/0 is done to or from user  virtual  memory l/0 is often implemented by a separate i/0 processor for example  a controller for a usb storage device is generally given the number of bytes to transfer and a memory address for the buffer  figure 9.29   when the transfer is complete  the cpu is interrupted we must be sure the following sequence of events does not occur  a process issues an i/0 request and is put in a queue for that i/o device meanwhile  the cpu is given to other processes these processes cause page faults ; and one of them  using a global replacement algorithm  replaces the page containing the memory buffer for the waiting process the pages are paged out some time later  when the i/o request advances to the head of the device queue  the i/o occurs to the specified address however  this frame is now being used for a different page belonging to another process there are two common solutions to this problem one solution is never to execute i/0 to user memory instead  data are always copied between system memory and user memory i/0 takes place only between system memory and the i/0 device to write a block on tape  we first copy the block to system memory and then write it to tape this extra copying may result in unacceptably high overhead another solution is to allow pages to be locked into memory here  a lock bit is associated with every frame if the frame is locked  it can not be selected for replacement under this approach  to write a block on tape  we lock into memory the pages containing the block the system can then continue as usual locked pages can not be replaced when the i/o is complete  the pages are unlocked figure 9.29 the reason why frames used for 1/0 must be in memory 9.10 9.10 405 lock bits are used in various situations frequently  some or all of the operating-system kernel is locked into memory  as many operating systems can not tolerate a page fault caused by the kernel another use for a lock bit involves normal page replacement consider the following sequence of events  a low-priority process faults selecting a replacement frame  the paging system reads the necessary page into memory ready to continue  the low-priority process enters the ready queue and waits for the cpu since it is a low-priority process  it may not be selected by the cpu scheduler for a time while the low-priority process waits  a high-priority process faults looking for a replacement  the paging system sees a page that is in memory but has not been referenced or modified  it is the page that the low-priority process just brought in this page looks like a perfect replacement  it is clean and will not need to be written out  and it apparently has not been used for a long time whether the high-priority process should be able to replace the low-priority process is a policy decision after all  we are simply delaying the low-priority process for the benefit of the high-priority process however  we are wasting the effort spent to bring in the page for the low-priority process if we decide to prevent replacement of a newly brought-in page until it can be used at least once  then we can use the lock bit to implement this mechanism when a page is selected for replacement  its lock bit is turned on ; it remains on until the faulting process is again dispatched using a lock bit can be dangerous  the lock bit may get turned on but never turned off should this situation occur  because of a bug in the operating system  for example   the locked frame becomes unusable on a single-user system  the overuse of locking would hurt only the user doing the locking multiuser systems must be less trusting of users for instance  solaris allows locking hints  but it is free to disregard these hints if the free-frame pool becomes too small or if an individual process requests that too many pages be locked in memory in this section  we describe how windows xp and solaris implement virtual memory 9.10.1 windows xp windows xp implements virtual memory using demand paging with clustering handles page faults by bringing in not only the faultil1.g page also several pages following the faulting page when a process is first created  it is assigned a working-set minimum and maximum the is the minimum number of pages the process is guaranteed to in memory if sufficient memory is available  a process may be assigned as many pages as its for most applications  the value of working-set minimum and working-set maximum is 50 and 345 pages  respectively  in some circumstances  a process may be allowed to exceed its working-set maximum  the virtual memory manager maintains a list of free page frames associated with this list is a threshold value that is used to 406 chapter 9 indicate whether sufficient free memory is available if a page fault occurs for a process that is below its working-set maximum  the virtual memory manager allocates a page from this list of free pages if a process that is at its working-set rnaximum incurs a page fault  it must select a page for replacement using a local page-replacement policy when the amount of free memory falls below the threshold  the virtual memory manager uses a tactic known as to restore the value above the threshold automatic working-set trimming works by evaluating the number of pages allocated to processes if a process has been allocated more pages than its working-set minimum  the virtual memory manager removes pages until the process reaches its working-set minimum a process that is at its working-set minimum may be allocated pages from the free-page-frame list once sufficient free memory is available the algorithm used to determine which page to remove from a working set depends on the type of processor on single-processor 80x86 systems  windows xp uses a variation of the clock algorithm discussed in section 9.4.5.2 on alpha and multiprocessor x86 systems  clearing the reference bit may require invalidatil g the entry in the translation look-aside buffer on other processors rather than incurring this overhead  windows xp uses a variation on the fifo algorithm discussed in section 9.4.2 9.10.2 solaris in solaris  when a thread incurs a page fault  the kernel assigns a page to the faulting thread from the list of free pages it maintains therefore  it is imperative that the kernel keep a sufficient amount of free memory available associated with this list of free pages is a parameter-zotsfree-that represents a threshold to begin paging the lotsfree parameter is typically set to 1/64 the size of the physical memory four times per second  the kernel checks whether the amount of free memory is less than lotsfree if the number of free pages falls below lotsfree  a process known as a pageout starts up the pageout process is similar to the second-chance algorithm described in section 9.4.5.2  except that it uses two hands while scanning pages  rather than one the pageout process works as follows  the front hand of the clock scans all pages in memory  setting the reference bit to 0 later  the back hand of the clock examines the reference bit for the pages in memory  appending each page whose reference bit is still set to 0 to the free list and writing to disk its contents if modified solaris maintains a cache list of pages that have been freed but have not yet been overwritten the free list contains frames that have invalid contents pages can be reclaimed from the cache list if they are accessed before being moved to the free list the pageout algorithm uses several parameters to control the rate at which pages are scam ed  known as the scanrate   the scanrate is expressed in pages per second and ranges from slowscan to fastscan when free memory falls below lotsfree  scanning occurs at slowscan pages per second and progresses to fastscan  depending on the amount of free memory available the default value of slowscan is 100 pages per second ; fasts can is typically set to the value  total physical pages  /2 pages per second  with a maximum of 8,192 pages per second this is shown in figure 9.30  withfastscan set to the maximum   the distance  in pages  between the hands of the clock is determil ed by a system parameter  handspread the amount of time between the front hand 's 9.11 8192 fastscan cll 7 c  1j u en 100 slowscan minfree desfree amount of free memory figure 9.30 solaris page scanner 9.11 407 lotsfree clearing a bit and the back hand 's investigating its value depends on the scanrate and the handspread if scam-ate is 100 pages per second and handspread is 1,024 pages  10 seconds can pass between the time a bit is set by the front hand and the time it is checked by the back hand however  because of the demands placed on the memory system  a scanrate of several thousand is not uncommon this means that the amount of time between clearing and investigating a bit is often a few seconds as mentioned above  the pageout process checks memory four times per second however  if free memory falls below desfree  figure 9.30   pageout will nm 100 times per second with the intention of keeping at least desfree free memory available if the pageout process is unable to keep the amount of free memory at desfree for a 30-second average  the kernel begins swapping processes  thereby freeing all pages allocated to swapped processes in general  the kernel looks for processes that have been idle for long periods of time if the system is unable to maintain the amount of free memory at minfree  the pageout process is called for every request for a new page recent releases of the solaris kernel have provided enhancements of the paging algorithm one such enhancement involves recognizing pages from shared libraries pages belonging to libraries that are being shared by several processes-even if they are eligible to be claimed by the scannerare skipped during the page-scanning process another enhancement concerns distinguishing pages that have been allocated to processes from pages allocated to regularfiles this is known as and is covered in section 11.6.2 it is desirable to be able to execute a process whose logical address space is larger than the available physical address space virtual memory is a technique 408 chapter 9 that enables us to map a large logical address space onto a smaller physical menlory virtual memory allows us to run extremely large processes and to raise the degree of multiprogramming  increasing cpu utilization further  it frees application programmers from worrying about memory availability in addition  with virtual memory  several processes can share system libraries and memory virtual memory also enables us to use an efficient type of process creation known as copy-on-write  wherein parent and child processes share actual pages of memory virtual memory is commonly implemented by demand paging pure demand paging never brings in a page until that page is referenced the first reference causes a page fault to the operating system the operating-system kernel consults an internal table to determine where the page is located on the backing store it then finds a free frame and reads the page in from the backing store the page table is updated to reflect this change  and the instruction that caused the page fault is restarted this approach allows a process to run even though its entire memory image is not in main memory at once as long as the page-fault rate is reasonably low  performance is acceptable we can use demand paging to reduce the number of frames allocated to a process this arrangement can increase the degree of multiprogramming  allowing more processes to be available for execution at one time  and-in theory  at least-the cpu utilization of the system it also allows processes to be run even though their memory requirements exceed the total available physical memory such processes run in virtual memory if total memory requirements exceed the capacity of physical memory  then it may be necessary to replace pages from memory to free frames for new pages various page-replacement algorithms are used fifo page replacement is easy to program but suffers from belady 's anomaly optimal page replacement requires future knowledge lru replacement is an approximation of optimal page replacement  but even it may be difficult to implement most page-replacement algorithms  such as the second-chance algorithm  are approximations of lru replacement in addition to a page-replacement algorithm  a frame-allocation policy is needed allocation can be fixed  suggesting local page replacement  or dynamic  suggesting global replacement the working-set model assumes that processes execute in localities the working set is the set of pages in the current locality accordingly  each process should be allocated enough frames for its current working set if a process does not have enough memory for its working set  it will thrash providing enough frames to each process to avoid thrashing may require process swapping and schedulil g most operating systems provide features for memory mappil1g files  thus allowing file i/0 to be treated as routine memory access the win32 api implements shared memory through memory mappil1g files kernel processes typically req1.1ire memory to be allocated using pages that are physically contiguous the buddy system allocates memory to kernel processes in units sized according to a power of 2  which often results in fragmentation slab allocators assign kernel data structures to caches associated with slabs  which are made up of one or more physically contiguous pages with slab allocation  no memory is wasted due to fragmentation  and memory requests can be satisfied quickly 409 in addition to reqmnng that we solve the major problems of page replacement and frame allocation  the proper design of a paging systern requires that we consider prep aging  page size  tlb reach  inverted page tables  program structure  i/0 interlock  and other issues 9.1 assume there is a 1,024-kb segment where memory is allocated using the buddy system using figure 9.27 as a guide  draw a tree illustrating how the following memory requests are allocated  request 240 bytes request 120 bytes request 60 bytes request 130 bytes next modify the tree for the followilcg releases of memory perform coalescing whenever possible  release 240 bytes release 60 bytes release 120 bytes 9.2 consider the page table for a system with 12-bit virtual and physical addresses with 256-byte pages the list of free page frames is d  e  f  that is  dis at the head of the list e is second  and f is last   410 chapter 9 convert the following virtual addresses to their equivalent physical addresses in hexadecimal all numbers are given in hexadecimal  a dash for a page frame indicates that the page is not in memory  9ef 111 700 off 9.3 a page-replacement algorithm should minimize the number of page faults we can achieve this minimization by distributing heavily used pages evenly over all of memory  rather than having them compete for a small number of page frames we can associate with each page frame a counter of the number of pages associated with that frame then  to replace a page  we can search for the page frame with the smallest counter a define a page-replacement algorithm using this basic idea specifically address these problems  i what is the initial value of the counters ii when are counters increased iii when are counters decreased 1v how is the page to be replaced selected b how many page faults occur for your algorithm for the following reference string with four page frames 1  2  3  4  5  3  4  1  6  7  8  7  8  9  7  8  9  5  4  5  4  2 c what is the minimum number of page faults for an optimal pagereplacement strategy for the reference string in part b with four page frames 9.4 consider a demand-paging system with the following time-measured utilizations  cpu utilization paging disk other i/0 devices 20 % 97.7 % 5 % for each of the following  say whether it will  or is likely to  improve cpu utilization explain your answers a install a faster cpu b install a bigger paging disk c increase the degree of multiprogramming d decrease the degree of multiprogramming 411 e install more main n1.enl0ry f install a faster hard disk or multiple controllers with multiple hard disks g add prepaging to the page-fetch algorithms h increase the page size 9.5 consider a demand-paged computer system where the degree of multiprogramming is currently fixed at four the system was recently measured to determine utilization of the cpu and the paging disk the results are one of the following alternatives for each case  what is happening can the degree of multiprogramming be increased to increase the cpu utilization is the paging helping a cpu utilization 13 percent ; disk utilization 97 percent b cpu utilization 87 percent ; disk utilization 3 percent c cpu utilization 13 percent ; disk utilization 3 percent 9.6 consider a demand-paging system with a paging disk that has an average access and transfer time of 20 milliseconds addresses are translated through a page table in main memory  with an access time of 1 microsecond per memory access thus  each memory reference through the page table takes two accesses to improve this time  we have added an associative memory that reduces access time to one memory reference if the page-table entry is in the associative memory assume that 80 percent of the accesses are in the associative memory and that  of those remaining  10 percent  or 2 percent of the total  cause page faults what is the effective memory access time 9.7 a simplified view of thread states is ready  running  and blocked  where a thread is either ready and waiting to be scheduled  is running on the processor  or is blocked  i.e is waiting for i/0  this is illustrated in figure 9.31 assuming a thread is in the running state  answer the following questions   be sure to explain your answer  a will the thread change state if it incurs a page fault if so  to what new state figure 9.31 thread state diagram for exercise 9.7 412 chapter 9 b will the thread change state if it generates a tlb miss that is resolved in the page table if so  to what new state c will the thread change state if an address reference is resolved in the page table if so  to what new state 9.8 discuss the hardware support required to support demand paging 9.9 consider the following page reference string  1  2  3  4  2  1  5  6  2  1  2  3  7  6  3  2  1  2  3  6 how many page faults would occur for the following replacement algorithms  assuming one  two  three  four  five  six  and seven frames remember that all frames are initially empty  so your first unique pages will cost one fault each lru replacement fifo replacement optimal replacement 9.10 consider a system that allocates pages of different sizes to its processes what are the advantages of such a paging scheme what modifications to the virtual memory system provide this functionality 9.11 discuss situations in which the most frequently used page-replacement algorithm generates fewer page faults than the least recently used page-replacement algorithm also discuss under what circumstances the opposite holds 9.12 under what circumstances do page faults occur describe the actions taken by the operating system when a page fault occurs 9.13 suppose that a machine provides instructions that can access memory locations using the one-level indirect addressing scheme what sequence of page faults is ilccurred when all of the pages of a program are currently nonresident and the first instruction of the program is an indirect memory-load operation what happens when the operating system is using a per-process frame allocation technique and only two pages are allocated to this process 9.14 consider a system that provides support for user-level and kernellevel threads the mapping in this system is one to one  there is a corresponding kernel thread for each user thread   does a multithreaded process consist of  a  a working set for the entire process or  b  a working set for each thread explain 413 9.15 what is the copy-on-write feature  and under what circumstances is it beneficial to use this feature what hardware support is required to implement this feature 9.16 consider the two-dimensional array a  int a     = new int  100   100  ; where a  oj  oj is at location 200 in a paged memory system with pages of size 200 a small process that manipulates the matrix resides in page 0  locations 0 to 199   thus  every instruction fetch will be from page 0 for three page frames  how many page faults are generated by the following array-initialization loops  using lru replacement and assuming that page frame 1 contains the process and the other two are initially empty a for  int j = 0 ; j 100 ; j + +  for  int i = 0 ; i 100 ; i + +  a  i   j  = 0 ; b for  int i = 0 ; i 100 ; i + +  for  int j = 0 ; j 100 ; j + +  a  i   j  = 0 ; 9.17 discuss situations in which the least frequently used page-replacement algorithm generates fewer page faults than the least recently used page-replacement algorithm also discuss under what circumstances the opposite holds 9.18 what is the cause of thrashing how does the system detect thrashing once it detects thrashing  what can the system do to eliminate this problem 9.19 assume that you are monitoring the rate at which the pointer in the clock algorithm  which indicates the candidate page for replacement  moves what can you say about the system if you notice the following behavior  a pointer is moving fast b pointer is moving slow 9.20 the vax/vms system uses a fifo replacement algorithm for resident pages and a free-frame pool of recently used pages assume that the free-frame pool is managed using the least recently used replacement policy answer the following questions  a if a page fault occurs and if the page does not exist in the free-frame pool  how is free space generated for the newly requested page 414 chapter 9 b if a page fault occurs and if the page exists in the free-frame pool  how is the resident page set and the free-france pool managed to make space for the requested page c what does the system degenerate to if the number of resident pages is set to one d what does the system degenerate to if the number of pages in the free-frame pool is zero 9.21 the slab-allocation algorithm uses a separate cache for each different object type assuming there is one cache per object type  explain why this scheme does n't scale well with multiple cpus what could be done to address this scalability issue 9.22 assume that we have a demand-paged memory the page table is held in registers it takes 8 milliseconds to service a page fault if an empty frame is available or if the replaced page is not modified and 20 milliseconds if the replaced page is modified memory-access time is 100 nanoseconds assume that the page to be replaced is modified 70 percent of the time what is the maximum acceptable page-fault rate for an effective access time of no more than 200 nanoseconds 9.23 segmentation is similar to paging but uses variable-sized pages define two segment-replacement algorithms based on fifo and lru pagereplacement schemes remember that since segments are not the same size  the segment that is chosen to be replaced may not be big enough to leave enough consecutive locations for the needed segment consider strategies for systems where segments cam ot be relocated and strategies for systems where they can 9.24 which of the following programming techniques and structures are good for a demand-paged environment which are not good explain your answers a stack b hashed symbol table c sequential search d binary search e pure code f vector operations a indirection b 9.25 when a page fault occurs  the process requesting the page must block while waiting for the page to be brought from disk into physical memory assume that there exists a process with five user-level threads and that the mapping of user threads to kernel threads is many to one if one user thread incurs a page fault while accessing its stack  would the other user user threads belonging to the same process also be affected by the page fault-that is  would they also have to wait for the faulting page to be brought into memory explain 415 9.26 consider a system that uses pure demand paging a when a process first starts execution  how would you characterize the page fault rate b once the working set for a process is loaded into memory  how would you characterize the page fault rate c assume that a process changes its locality and the size of the new working set is too large to be stored in available free memory identify some options system designers could choose from to handle this situation 9.27 assume that a program has just referenced an address in virtual memory describe a scenario in which each of the following can occur  if no such scenario can occur  explain why  tlb miss with no page fault tlb miss and page fault tlb hit and no page fault tlb hit and page fault 9.28 a certain computer provides its users with a virtual memory space of 232 bytes the computer has 218 bytes of physical memory the virtual memory is implemented by paging  and the page size is 4,096 bytes a user process generates the virtual address 11123456 explain how the system establishes the corresponding physical location distinguish between software and hardware operations 9.29 when virtual memory is implemented in a computing system  there are certain costs associated with the technique and certain benefits list the costs and the benefits is it possible for the costs to exceed the benefits if it is  what measures can be taken to ensure that this does not happen 9.30 give an example that illustrates the problem with restarting the move character instruction  mvc  on the ibm 360/370 when the source and destination regions are overlapping 9.31 consider the parameter 6 used to define the working-set window in the working-set model what is the effect of setting 6 to a small value on the page-fault frequency and the number of active  nonsuspended  processes currently executing in the system what is the effect when 6 is set to a very high value 9.32 is it possible for a process to have two working sets  one representing data and another representing code explain 9.33 suppose that your replacement policy  in a paged system  is to examine each page regularly and to discard that page if it has not been used since the last examination what would you gain and what would you lose by using this policy rather than lru or second-chance replacement 416 chapter 9 9.34 write a program that implements the fifo and lru page-replacement algorithms presented in this chapter first  generate a random pagereference string where page numbers range from 0 to 9 apply the random page-reference string to each algorithm  and record the number of page faults incurred by each algorithm implement the replacement algorithms so that the number of page frames can vary from 1 to 7 assume that demand paging is used 9.35 the catalan numbers are an integer sequence c11 that appear in treeenumeration problems the first catalan numbers for n = 1  2  3   are 1  2  5  14  42  132   a formula generating c11 is 1  2n   2n  ! ell =  n + 1   ; ; =  n + 1  ! n ! design two programs that communicate with shared memory using the win32 api as outlined in section 9.7.2 the producer process will generate the catalan sequence and write it to a shared memory object the consumer process will then read and output the sequence from shared memory in this instance  the producer process will be passed an integer parameter on the command line specifying how many catalan numbers to produce  for example  providing 5 on the command line means the producer process will generate the first five catalan numbers   demand paging was first used iil the atlas system  implemented on the manchester university muse computer around 1960  kilburn et al  1961    another early demand-paging system was multics  implemented on the ge 645 system  organick  1972    belady et al  1969  were the first researchers to observe that the fifo replacement strategy may produce the anomaly that bears belady 's name mattson et al  1970  demonstrated that stack algorithms are not subject to belady 's anomaly the optimal replacement algorithm was presented by belady  1966  and was proved to be optimal by mattson et al  1970   belady ' s optimal algorithm is for a fixed allocation ; prieve and fabry  1976  presented an optimal algorithm for situations in which the allocation can vary the enl lanced clock algorithm was discussed by carr and hennessy  1981   the working-set model was developed by denning  1968   discussions concerning the working-set model were presented by denning  1980   the scheme for monitoring the page-fault rate was developed by wulf  1969   who successfully applied this technique to the burroughs bssoo computer system wilson et al  1995  presented several algoritluns for dynamic memory allocation jolmstone and wilson  1998  described various memory-fragmentation 417 issues buddy system memory allocators were described in knowlton  1965l peterson and norman  1977   and purdom  jr and stigler  1970   bonwick  1994  discussed the slab allocator  and bonwick and adams  2001  extended the discussion to multiple processors other memory-fitting algorithms can be found in stephenson  1983   bays  1977   and brent  1989   a survey of memory-allocation strategies can be found in wilson et al  1995   solomon and russinovich  2000  and russinovich and solomon  2005  described how windows implements virtual memory mcdougall and mauro  2007  discussed virtual memory in solaris virtual memory techniques in linux and bsd were described by bovet and cesati  2002  and mckusick et al  1996   respectively ganapathy and schimmel  1998  and navarro et al  2002  discussed operating system support for multiple page sizes ortiz  2001  described virtual memory used in a real-time embedded operating system jacob and mudge  1998b  compared implementations of virtual memory in the mips  powerpc  and pentium architectures a companion article  jacob and mudge  1998a   described the hardware support necessary for implementation of virtual memory in six different architectures  including the ultrasparc part five since main memory is usually too small to accommodate all the data and programs permanently  the computer system must provide secondary storage to back up main memory modern computer systems use disks as the primary on-line storage medium for information  both programs and data   the file system provides the mechanism for on-line storage of and access to both data and programs residing on the disks a file is a collection of related information defined by its creator the files are mapped by the operating system onto physical devices files are normally organized into directories for ease of use the devices that attach to a computer vary in many aspects some devices transfer a character or a block of characters at a time some can be accessed only sequentially  others randomly some transfer data synchronously  others asynchronously some are dedicated  some shared they can be read-only or read-write they vary greatly in speed in many ways  they are also the slowest major component of the computer because of all this device variation  the operating system needs to provide a wide range of functionality to applications  to allow them to control all aspects of the devices one key goal of an operating system 's 1/0 subsystem is to provide the simplest interface possible to the rest of the system because devices are a performance bottleneck  another key is to optimize 1/0 for maximum concurrency 10.1 r for most users  the file system is the most visible aspect of an operating system it provides the mechanism for on-line storage of and access to both data and programs of the operating system and all the users of the computer system the file system consists of two distinct parts  a collection of files  each storing related data  and a directory structure  which organizes and provides information about all the files in the system file systems live on devices  which we explore fully irl the following chapters but touch upon here in this chapter  we consider the various aspects of files and the major directory structures we also discuss the semantics of sharing files among multiple processes  users  and computers finally  we discuss ways to handle file protection  necessary when we have multiple users and we want to control who may access files and how files may be accessed to explain the function of file systems to describe the interfaces to file systems to discuss file-system design tradeoffs  including access methods  file sharing  file locking  and directory structures to explore file-system protection computers can store information on various storage media  such as magnetic disks  magnetic tapes  and optical disks so that the computer system will be convenient to use  the operating system provides a uniform logical view of information storage the operating system abstracts from the physical properties of its storage devices to define a logical storage unit  the file files are mapped by the operating system onto physical devices these storage devices are usually nonvolatile  so the contents are persistent through power failures and system reboots 421 422 chapter 10 a file is a named collection of related information that is recorded on secondary storage from a user 's perspective  a file is the smallest allotment of logical secondary storage ; that is  data can not be written to secondary storage unless they are within a file commonly  files represent programs  both source and object forms  and data data files may be numeric  alphabetic  alphanumeric  or binary files may be free form  such as text files  or may be formatted rigidly in general  a file is a sequence of bits  bytes  lines  or records  the meaning of which is defined by the file 's creator and user the concept of a file is thus extremely general the information in a file is defined by its creator many different types of information may be stored in a file-source programs  object programs  executable programs  numeric data  text  payroll records  graphic images  sound recordings  and so on a file has a certain defined which depends on its type a text file is a sequence of characters organized into lines  and possibly pages   a source file is a sequence of subroutines and functions  each of which is further organized as declarations followed by executable statements an object file is a sequence of bytes organized in.to blocks nnderstandable by the system 's linker an executable file is a series of code sections that the loader can bring into memory and execute 10.1.1 file attributes a file is named  for the convenience of its human users  and is referred to by its name a name is usually a string of characters  such as example.c some systems differentiate between uppercase and lowercase characters in names  whereas other systems do not when a file is named  it becomes independent of the process  the user  and even the system that created it for instance  one user might create the file example.c  and another user might edit that file by specifying its name the file 's owner might write the file to a floppy disk  send it in an e-mail  or copy it across a network  and it could still be called example.c on the destination system a file 's attributes vary from one operating system to another but typically consist of these  name the symbolic file name is the only information kept in humanreadable form identifier this unique tag  usually a number  identifies the file within the file system ; it is the non-human-readable name for the file type this information is needed for systems that support different types of files location this information is a pointer to a device and to the location of the file on that device size the current size of the file  in bytes  words  or blocks  and possibly the maximum allowed size are included in this attribute protection access-control information determines who can do reading  writing  executing  and so on 10.1 423 time  date  and user identification this information may be kept for creation  last modification  and last use these data can be useful for protection  security  and usage monitoring the information about all files is kept in the directory structure  which also resides on secondary storage typically  a directory entry consists of the file 's name and its unique identifier the identifier in turn locates the other file attributes it may take more than a kilobyte to record this information for each file in a system with many files  the size of the directory itself may be megabytes because directories  like files  must be nonvolatile  they must be stored on the device and brought into memory piecemeal  as needed 10.1.2 file operations a file is an to define a file properly  we need to consider the operations that can be performed on files the operating system can provide system calls to create  write  read  reposition  delete  and truncate files let 's examine what the operating system must do to perform each of these six basic file operations it should then be easy to see how other similar operations  such as renaming a file  can be implemented creating a file two steps are necessary to create a file first  space in the file system must be found for the file we discuss how to allocate space for the file in chapter 11 second  an entry for the new file must be made in the directory writing a file to write a file  we make a system call specifying both the name of the file and the information to be written to the file given the name of the file  the system searches the directory to find the file 's location the system must keep a write pointer to the location in the file where the next write is to take place the write pointer must be updated whenever a write occurs reading a file to read from a file  we use a system call that specifies the name of the file and where  in memory  the next block of the file should be put again  the directory is searched for the associated entry  and the system needs to keep a read pointer to the location in the file where the next read is to take place once the read has taken place  the read pointer is updated because a process is usually either reading from or writing to a file  the current operation location can be kept as a per-process  both the read and write operations use this same pointer  saving space and reducing system complexity repositioning within a file the directory is searched for the appropriate entry  and the current-file-position pointer is repositioned to a given value repositioning within a file need not involve any actual i/0 this file operation is also kn.own as a file seek deleting a file to delete a file  we search the directory for the named file having found the associated directory entry  we release all file space  so that it can be reused by other files  and erase the directory entry 424 chapter 10 truncating a file the user may want to erase the contents of a file but keep its attributes rather than forcing the user to delete the file and then recreate it  this function allows all attributes to remain unchanged -except for file length-but lets the file be reset to length zero and its file space released these six basic operations comprise the minimal set of required file operations other common operations include appending new information to the end of an existing file and renaming an existing file these primitive operations can then be combined to perform other file operations for instance  we can create a copy of a file  or copy the file to another i/o device  such as a printer or a display  by creating a new file and then reading from the old and writing to the new we also want to have operations that allow a user to get and set the various attributes of a file for example  we may want to have operations that allow a user to determine the status of a file  such as the file 's length  and to set file attributes  such as the file 's owner most of the file operations mentioned involve searching the directory for the entry associated with the named file to avoid this constant searching  many systems require that an open   system call be made before a file is first used actively the operating system keeps a small table  called the containing information about all open files when a file operation is requested  the file is specified via an index into this table  so no searching is required when the file is no longer being actively used  it is closed by the process  and the operating system removes its entry from the open-file table create and delete are system calls that work with closed rather than open files some systems implicitly open a file when the first reference to it is made the file is automatically closed when the job or program that opened the file terminates most systems  however  require that the programmer open a file explicitly with the open   system call before that file can be used the open   operation takes a file name and searches the directory  copying the directory entry into the open-file table the open   call can also accept accessmode information-create  read-only  read-write  append-only  and so on this mode is checked against the file 's permissions if the request mode is allowed  the file is opened for the process the open   system call typically returns a pointer to the entry in the open-file table this pointer  not the actual file name  is used in all i/0 operations  avoiding any further searching and simplifying the system-call interface the implementation of the open   and close   operations is more complicated in an environment where several processes may open the file simultaneously this may occur in a system ~ where several different applications open the same file at the same time typically  the operating system uses two levels of internal tables  a per-process table and a system-wide table the perprocess table tracks all files that a process has open stored in this table is information regarding the use of the file by the process for instance  the current file pointer for each file is found here access rights to the file and accounting information can also be included each entry in the per-process table in turn points to a system-wide open-file table the system-wide table contains process-independent information  such as the location of the file on disk  access dates  and file size once a file has been opened by one process  the system-wide table includes an entry for the file 10.1 425 when another process executes an open   calt a new entry is simply added to the process 's open-file table pointing to the appropriate entry in the systemwide table typically  the open-file table also has an open count associated with each file to indicate how ncany processes have the file open each close   decreases this open count  and when the open count reaches zero  the file is no longer in use  and the file 's entry is removed from the open-file table in summary  several pieces of information are associated with an open file file pointer on systems that do not include a file offset as part of the read   and write   system calls  the systein must track the last readwrite location as a current-file-position pointer this pointer is unique to each process operating on the file and therefore must be kept separate from the on-disk file attributes file-open count as files are closed  the operating system must reuse its open-file table entries  or it could run out of space in the table because multiple processes may have opened a file  the system must wait for the last file to close before removing the open-file table entry the file-open counter tracks the number of opens and closes and reaches zero on the last close the system can then remove the entry disk location of the file most file operations require the system to modify data within the file the information needed to locate the file on disk is kept in memory so that the system does not have to read it from disk for each operation access rights each process opens a file in an access mode this information is stored on the per-process table so the operating system can allow or deny subsequent i/0 requests some operating systems provide facilities for locking an open file  or sections of a file   file locks allow one process to lock a file and prevent other processes from gaining access to it file locks are useful for files that are shared by several processes-for example  a system log file that can be accessed and modified by a number of processes in the system file locking in java in the java api  acquiring a lock requires firstobtaini  ng the f  i..lechannel fbr thefile to be locked the loc ; k   method of the filechannel is used to acquir  o the lock the api of the lock   method is filelock lock  l.ong begin  long end  l ; ooleqn shared  where begin and end are the h  ~ gi1iningand ending positions of the region being locked settingshared to true isfb ~ shared locks ; setting shared to false acquires the lock exclusively tice lock is released by invoking the release   of the filelock returned by the lock   operati n the program in figure 10.1 illusttates file locking in java  this program acquires two locks on thefilefile  txt the first half of.the file is acquired as an exclusive lock ~ the lock for the second half is a shared lock 426 chapter 10 file locks provide functionality similar to reader-writer locks  covered in section 6.6.2 a shared lock is akin to a reader lock in that several processes can acquire the lock concurrently an exclusive lock behaves like a writer lock ; only one process at a time can acquire such a lock it is important to note 10.1 427 that not au operating systems provide both types of locks ; some systems only provide exclusive file locking furthermore  operating systems may provide either mandatory or advisory file-locking mechanisms if a lock is n1.andatory  then once a process acquires an exclusive lock  the operating system will prevent any other process from accessing the locked file for example  assume a process acquires an exclusive lock on the file system .log if we attempt to open system .log from another process-for example  a text editor-the operating system will prevent access until the exclusive lock is released this occurs even if the text editor is not written explicitly to acquire the lock alternatively  if the lock is advisory  then the operating system will not prevent the text editor from acquiring access to system .log rather  the text editor must be written so that it manually acquires the lock before accessing the file in other words  if the locking scheme is mandatory  the operating system ensures locking integrity for advisory locking  it is up to software developers to ensure that locks are appropriately acquired and released as a general rule  windows operating systems adopt mandatory locking  and unix systems employ advisory locks the use of file locks requires the same precautions as ordinary process synchronization for example  programmers developing on systems with mandatory locking must be careful to hold exclusive file locks only while they are accessing the file ; otherwise  they will prevent other processes from accessing the file as well furthermore  some measures must be taken to ensure that two or more processes do not become involved in a deadlock while trying to acquire file locks 10.1.3 file types when we design a file system-indeed  an entire operating system-we always consider whether the operating system should recognize and support file types if an operating system recognizes the type of a file  it can then operate on the file in reasonable ways for example  a common mistake occurs when a user tries to print the binary-object form of a program this attempt normally produces garbage ; however  the attempt can succeed if the operating system has been told that the file is a binary-object program a common technique for implementing file types is to include the type as part of the file name the name is split into two parts-a name and an extension  usually separated by a period character  figure 10.2   in this way  the user and the operating system can tell from the name alone what the type of a file is for example  most operating systems allow users to specify a file name as a sequence of characters followed by a period and terminated by an extension of additional characters file name examples include resume.doc  server.java  and readerthread c the system uses the extension to indicate the type of the file and the type of operations that can be done on that file only a file with a .com  .exe  or .bat extension can be executed  for instance the .com and .exe files are two forms of binary executable files  whereas a .bat file is a containing  in ascii format  commands to the operating system ms-dos recognizes only a few extensions  but application programs also use extensions to indicate file types in which they are interested for example  assemblers expect source files to have an .asm extension  and the microsoft word word processor expects its files to 428 chapter 10 !   isnl ~ 1  f '  ~   j \  ir ~ i  tji ~  ~   '' ' r  ~ ~  r    ~ ; ,'u  ~ rt ~ tt ~ ~ ~   ~ \        ' ' ~   '   c executable exe  com  bin ready ~ to-run machineor none language program object obj  o compiled  machine language  not linked source code c  cc  java  pas  source code in various asm  a languages batch bat  sh commands to the command interpreter text txt  doc textual data  documents wo rdprocessor wp,tex  rtf  various wordcprocessor doc formats library lib  a  so  dll libraries o.troutines for .programmers print or view ps  pdf  jpg ascii or binary file in a format for printing or viewing archive arc  zip  .tar 1 related files grouped into .one file,sometimes compressed  for archiving or storage multimedia mpeg  mov  rm  binary file containing mp3  avi audio or a/v information figure 10.2 common file types end with a .doc extension these extensions are not required  so a user may specify a file without the extension  to save typing   and the application will look for a file with the given name and the extension it expects because these extensions are not supported by the operating system  they can be considered as hints to the applications that operate on them another example of the utility of file types comes from the tops-20 operating system if the user tries to execute an object program whose source file has been modified  or edited  since the object file was produced  the source file will be recompiled automatically this function ensures that the user always runs an up-to-date object file otherwise  the user could waste a significant amount of time executing the old object file for this function to be possible  the operating system must be able to discriminate the source file from the object file  to check the time that each file was created or last modified  and to determine the language of the source program  in order to use the correct compiler   consider  too  the mac os x operating system in this system  each file has a type  such as text  for text file  or appl  for application   each file also has a creator attribute containing the name of the program that created it this attribute is set by the operating system during the create   call  so its use is enforced and supported by the system for instance  a file produced by a word processor has the word processor 's name as its creator when the user opens that file  by double-clicking the mouse on the icon representing the file  10.1 429 the word processor is invoked automatically  and the file is loaded  ready to be edited the unix system uses a crude stored at the beginning of some files to indicate roughly the type of the file-executable program  batch file  or postscript file  and so on not all files have magic numbers  so system features can not be based solely on this information unix does not record the name of the creating program  either unix does allow file-nameextension hints  but these extensions are neither enforced nor depended on by the operating system ; they are meant mostly to aid users in determining what type of contents the file contains extensions can be used or ignored by a given application  but that is up to the application 's programmer 10.1.4 file structure file types also can be used to indicate the internal structure of the file as mentioned in section 10.1.3  source and object files have structures that match the expectations of the programs that read them further  certain files must conform to a required structure that is understood by the operating system for example  the operating system requires that an executable file have a specific structure so that it can determine where in memory to load the file and what the location of the first instruction is some operating systems extend this idea into a set of system-supported file structures  with sets of special operations for manipulating files with those structures for instance  dec 's vms operating system has a file system that supports three defined file structures this point brings us to one of the disadvantages of having the operating system support multiple file structures  the resulting size of the operating system is cumbersome if the operating system defines five different file structures  it needs to contain the code to support these file structures in addition  it may be necessary to define every file as one of the file types supported by the operating system when new applications require information structured in ways not supported by the operating system  severe problems may result for example  assume that a system supports two types of files  text files  composed of ascii characters separated by a carriage return and line feed  and executable binary files now  if we  as users  want to define an encrypted file to protect the contents from being read by unauthorized people  we may find neither file type to be appropriate the encrypted file is not ascii text lines but rather is  apparently  random bits although it may appear to be a binary file  it is not executable as a result  we may have to circumvent or misuse the operating system 's file-type mechanism or abandon our encryption scheme some operating systems impose  and support  a minimal number of file structures this approach has been adopted in unix  ms-dos  and others un1x considers each file to be a sequence of 8-bit bytes ; no interpretation of these bits is made by the operating systen'l this scheme provides maximum flexibility but little support each application program must include its own code to interpret an input file as to the appropriate structure however  all operating systems must support at least one structure-that of an executable file-so that the system is able to load and run programs the macintosh operating system also supports a minimal number of file structures it expects files to contain two parts  a and a 430 chapter 10 10.2 the resource fork contains information of interest to the user for instance  it holds the labels of any buttons displayed by the program a foreign user may want to re-label these buttons in his own language  and the macintosh operating system provides tools to allow modification of the data in the resource fork the data fork contains program code or data-the traditional file contents to accomplish the same task on a unix or ms-dos system  the programmer would need to change and recompile the source code  unless she created her own user-changeable data file clearly  it is useful for an operating system to support structures that will be used frequently and that will save the programmer substantial effort too few structures make programming inconvenient  whereas too many cause operating-system bloat and programmer confusion 10.1.5 internal file structure internally  locating an offset within a file can be complicated for the operating system disk systems typically have a well-defined block size determined by the size of a sector all disk i/0 is performed in units of one block  physical record   and all blocks are the same size it is unlikely that the physical record size will exactly match the length of the desired logical record logical records may even vary in length paddng a number of logical records into physical blocks is a common solution to this problem for example  the unix operating system defines all files to be simply streams of bytes each byte is individually addressable by its offset from the begi1ming  or end  of the file in this case  the logical record size is 1 byte the file system automatically packs and unpacks bytes into physical disk blockssay  512 bytes per block-as necessary the logical record size  physical block size  and packing technique determine how many logical records are in each physical block the packing can be done either by the user 's application program or by the operating system in either case  the file may be considered a sequence of blocks all the basic i/o functions operate in terms of blocks the conversion from logical records to physical blocks is a relatively simple software problem because disk space is always allocated in blocks  some portion of the last block of each file is generally wasted if each block were 512 bytes  for example  then a file of 1,949 bytes would be allocated four blocks  2,048 bytes  ; the last 99 bytes would be wasted the waste incurred to keep everything in units of blocks  instead of bytes  is all file systems suffer from internal fragmentation ; the larger the block size  the greater the internal fragmentation files store information when it is used  this information must be accessed and read into computer memory the information in the file can be accessed in several ways some systems provide only one access method for files other systems  such as those of ibm  support many access methods  and choosing the right one for a particular application is a major design problem 10.2 431 beginning current position end  ;        = = =  rewind ~ read or write ~ figure 10.3 sequential-access file 10.2.1 sequential access the simplest access method is  information in the file is processed in order  one record after the other this mode of access is by far the most common ; for example  editors and compilers usually access files in this fashion reads and writes make up the bulk of the operations on a file a read operation-read next-reads the next portion of the file and automatically advances a file pointer  which tracks the i/o location similarly  the write operation-write next-appends to the end of the file and advances to the end of the newly written material  the new end of file   such a file can be reset to the beginning ; and on some systems  a program may be able to skip forward or backward n records for some integer n-perhaps only for n = 1 sequential access  which is depicted in figure 10.3  is based on a tape model of a file and works as well on sequential-access devices as it does on random-access ones 10.2.2 direct access  or a file is made up of fixedlength that allow programs to read and write records rapidly in no particular order the direct-access method is based on a disk model of a file  since disks allow random access to any file block for direct access  the file is viewed as a numbered sequence of blocks or records thus  we may read block 14  then read block 53  and then write block 7 there are no restrictions on the order of reading or writing for a direct-access file direct-access files are of great use for immediate access to large amounts of information databases are often of this type when a query concerning a particular subject arrives  we compute which block contains the answer and then read that block directly to provide the desired information as a simple example  on an airline-reservation system  we might store all the information about a particular flight  for example  flight 713  in the block identified by the flight number thus  the number of available seats for flight 713 is stored in block 713 of the reservation file to store il1formation about a larger set such as people  we might compute a hash function on the people 's names or search a small in-ncemory index to determine a block to read and search for the direct-access method  the file operations must be modified to include the block number as a parameter thus  we have read n  where n is the block number  rather than read next  and write n rather than write next an alternative approach is to retain read next and write next  as with sequential 432 chapter 10 figure 10.4 simulation of sequential access on a direct-access file access  and to add an operation position file to n  where n is the block number then  to effect a read n  we would position to n and then read next the block number by the user to the operating system is normally a a relative block number is an index relative to the begirm.ing of the file thus  the first relative block of the file is 0  the next is 1  and so on  even though the absolute disk address may be 14703 for the first block and 3192 for the second the use of relative block numbers allows the operating system to decide where the file should be placed  called the allocation problem  as discussed in chapter 11  and helps to prevent the user from accessing portions of the file system that may not be part of her file some systems start their relative block numbers at 0 ; others start at 1 how  then  does the system satisfy a request for record nina file assuming we have a logical record length l  the request for record n is turned into an i/0 request for l bytes starting at location l  n  within the file  assuming the first record is n = 0   since logical records are of a fixed size  it is also easy to read  write  or delete a record not all operating systems support both sequential and direct access for files some systems allow only sequential file access ; others allow only direct access some systems require that a file be defined as sequential or direct when it is created ; such a file can be accessed only in a manner consistent with its declaration we can easily simulate sequential access on a direct-access file by simply keeping a variable cp that defines our current position  as shown in figure 10.4 simulating a direct-access file on a sequential-access file  however  is extremely inefficient and clumsy 10.2.3 other access methods other access methods can be built on top of a direct-access method these methods generally involve the construction of an index for the file the like an index in the back of a contains pointers to the various blocks to find a record in the file  we first search the index and then use the to access the file directly and to find the desired record for example  a retail-price file might list the universal codes  upcs  items  with the associated prices each record consists a 10-digit upc and a 6-digit price  a 16-byte record if our disk has 1,024 bytes per we can store 64 records per block a file of 120,000 records would occupy about 2,000 blocks  2 million bytes   by keeping the file sorted by upc  we can define an index consisting of the first upc in each block this index would have entries of 10 digits each  or 20,000 bytes  and thus could be kept in memory to 10.3 10.3 433 logical record last name number adams arthur asher sm ! th,jol  ir ! social ~ security  age  / e  smith .' '  / index file relative file figure 10.5 example of irdex and relative files find the price of a particular item  we can make a binary search of the index from this search  we learn exactly which block contains the desired record and access that block this structure allows us to search a large file doing little i/0 with large files  the index file itself may become too large to be kept in memory one solution is to create an index for the index file the primary index file would contain pointers to secondary index files  which would point to the actual data items for example  ibm 's indexed sequential-access method  isam  uses a small master index that points to disk blocks of a secondary index the secondary index blocks point to the actual file blocks the file is kept sorted on a defined key to find a particular item  we first make a binary search of the master index  which provides the block number of the secondary index this block is read in  and again a binary search is used to find the block containing the desired record finally  this block is searched sequentially in this way  any record can be located from its key by at most two direct-access reads figure 10.5 shows a similar situation as implemented by vms index and relative files next  we consider how to store files certainly  no general-purpose computer stores just one file there are typically thousand  millions  and even billions of files within a computer files are stored on random-access storage devices  including hard disks  optical disks  and solid state  memory-based  disks a storage device can be used in its entirety for a file system it can also be subdivided for finer-grained control for example  a disk can be into quarters  and each quarter can hold a file system storage devices can also be collected together into raid sets that provide protection from the failure of a single disk  as described in section 12.7   sometimes  disks are subdivided and also collected into raid sets partitioning is useful for limiting the sizes of individual file systems  putting multiple file-system types on the same device  or leaving part of the device available for other uses  such as swap space or unformatted  rz ; c  .v  disk 434 chapter 10 directory  directory partition a files disk 2 1-7 ~ ~ ~ disk 1 directory partition c files partition b files disk 3 figure 10.6 a typical file-system organization space partitions are also known as or  in the ibm world  a file system can be created on each of these parts of the disk any entity containing a file system is generally known as a the volume may be a subset of a device  a whole device  or multiple devices linked together into a raid set each volume can be thought of as a virtual disk volumes can also store multiple operating systems  allowing a system to boot and run more than one operating system each volume that contains a file system must also contain information about the files in the system this information is kept in entries in a or ~ the device directory  more commonly known simply as that records information -such as name  location  size  and type-for all files on that volume figure 10.6 shows a typical file-system organization 10.3.1 storage structure as we have just seen  a general-purpose computer system has multiple storage devices  and those devices can be sliced up into volumes that hold file systems computer systems may have zero or more file systems  and the file systems may be of varying types for example  a typical solaris system may have dozens of file systems of a dozen different types  as shown in the file system list in fig1-1re 10.7 in this book  we consider only general-purpose file systems it is worth noting  though  that there are many special-purpose file systems consider the types of file systems in the solaris example mentioned above  tmpfs-a temporary file system that is created in volatile main memory and has its contents erased if the system reboots or crashes objfs-a virtual file system  essentially an interface to the kernel that looks like a file system  that gives debuggers access to kernel symbols dfs-a virtual file system that maintains contract information to manage which processes start when the system boots and must continue to run during operation 10.3 435 i ufs /devices devfs /dev dev i system/ contract ctfs /proc proc /etc/mnttab mntfs i etc/ svc/volatile tmpfs i system/ object objfs /lib /libc.so.l lofs /dev/fd fd /var ufs /tmp tmpfs /var/run tmpfs /opt ufs /zpbge zfs i zpbge/backup zfs i export/home zfs /var/mail zfs /var/spool/inqueue zfs /zpbg zfs /zpbg/zones zfs figure 10.7 solaris file system lofs-a loop back file system that allows one file system to be accessed in place of another one prods-a virtual file system that presents information on all processes as a file system ufs  zfs-general-purpose file systems the file systems of computers  then  can be extensive even within a file system  it is useful to segregate files into groups and manage and act on those groups this organization involves the use of directories in the remainder of this section  we explore the topic of directory structure 10.3.2 directory overview the directory can be viewed as a symbol table that translates file names into their directory entries if we take such a view  we see that the directory itself can be organized in many ways we want to be able to insert entries  to delete entries  to search for a named entry  and to list all the entries in the directory in this section  we examine several schemes for defining the logical structure of the directory system when considering a particular directory structure  we need to keep in mind the operations that are to be performed on a directory  search for a file we need to be able to search a directory structure to find the entry for a particular file since files have symbolic names  and similar 436 chapter 10 names may indicate a relationship between files  we may want to be able to find all files whose names match a particular pattern create a file new files need to be created and added to the directory delete a file when a file is no longer needed  we want to be able to remove it from the directory list a directory we need to be able to list the files in a directory and the contents of the directory entry for each file in the list rename a file because the name of a file represents its contents to its users  we must be able to change the name when the contents or use of the file changes renaming a file may also allow its position within the directory structure to be changed traverse the file system we may wish to access every directory and every file within a directory structure for reliability  it is a good idea to save the contents and structure of the entire file system at regular intervals often  we do this by copyin.g all files to magn.etic tape this technique provides a backup copy in case of system failure in addition  if a file is no longer in use  the file can be copied to tape and the disk space of that file released for reuse by another file in the following sections  we describe the most common schemes for defining the logical structure of a directory 10.3.3 single-level directory the simplest directory structure is the single-level directory all files are contained in the same directory  which is easy to support and understand  figure 10.8   a single-level directory has significant limitations  however  when the number of files increases or when the system has more than one user since all files are in the same directory  they must have unique names if two users call their data file test  then the unique-name rule is violated for example  in one programming class  23 students called the program for their second assignment prog2 ; another 11 called it assign2 although file names are generally selected to reflect the content of the file  they are often limited in length  complicating the task of making file names unique the ms-dos operating system allows only 11-character file names ; unix  in contrast  allows 255 characters even a single user on a single-level directory may find it difficult to remember the names of all the files as the number of files increases it is not directory files figure 10.8 single-level directory 10.3 437 uncommon for a user to have hundreds of files on one computer system and an equal number of additional files on another system keeping track of so many files is a daunting task 10.3.4 two-level directory as we have seen  a single-level directory often leads to confusion of file names among different users the standard solution is to create a separate directory for each user in the two-level directory structure  each user has his own the ufds have similar structures  but each lists only the files of a single user w11en a user job starts or a user logs in  the system 's is searched the mfd is indexed by user name or account number  and each entry points to the ufd for that user  figure 10.9   when a user refers to a particular file  only his own ufd is searched thus  different users may have files with the same name  as long as all the file names within each ufd are unique to create a file for a user  the operating system searches only that user 's ufd to ascertain whether another file of that name exists to delete a file  the operating system confines its search to the local ufd ; thus  it can not accidentally delete another user 's file that has the same name the user directories themselves must be created and deleted as necessary a special system program is run with the appropriate user name and account information the program creates a new ufd and adds an entry for it to the mfd the execution of this program might be restricted to system administrators the allocation of disk space for user directories can be handled with the teduciques discussed in chapter 11 for files themselves although the two-level directory structure solves the name-collision problem  it still has disadvantages this structure effectively isolates one user from another isolation is an advantage when the users are completely independent but is a disadvantage when the users want to cooperate on some task and to access one another 's files some systems simply do not allow local user files to be accessed by other users if access is to be pennitted  one user must have the ability to name a file in another user 's directory to name a particular file lmiquely in a two-level directory  we must give both the user name and the file name a two-level directory can be thought of as a tree  or an inverted tree  of height 2 the root of the tree is the mfd its direct descendants are the ufds the descendants of user file directory figure i 0.9 two-level directory structure 438 chapter 10 the ufds are the files themselves the files are the leaves of the tree specifying a user name and a file name defines a path in the tree from the root  the mfd  to a leaf  the specified file   thus  a user name and a file name define a path name every file in the system has a path name to name a file uniquely  a user must know the path name of the file desired for example  if user a wishes to access her own test file named test  she can simply refer to test to access the file named test of user b  with directory-entry name userb   however  she might have to refer to /userb/test every system has its own syntax for naming files in directories other than the user 's own additional syntax is needed to specify the volume of a file for instance  in ms-dos a volume is specified by a letter followed by a colon thus  a file specification might be c  \ userb \ fest some systems go even further and separate the volume  directory name  and file name parts of the specification for instance  in vms  the file login.com might be specified as  u   sst.jdeck  login.com ; l  where u is the name of the volume  sst is the name of the directory  jdeck is the name of the subdirectory  and 1 is the version number other systems simply treat the volume name as part of the directory name the first name given is that of the volume  and the rest is the directory and file for instance  /u/pbg/test might specify volume u  directory pbg  and file test a special case of this situation occurs with the system files programs provided as part of the system -loaders  assemblers  compilers  utility routines  libraries  and so on-are generally defined as files when the appropriate commands are given to the operating system  these files are read by the loader and executed many command interpreters simply treat such a command as the name of a file to load and execute as the directory system is defined presently  this file name would be searched for in the current ufd one solution would be to copy the system files into each ufd however  copying all the system files would waste an enormous amount of space  if the system files require 5 mb  then supporting 12 users would require 5 x 12 = = 60 mb just for copies of the system files  the standard solution is to complicate the search procedure slightly a special user directory is defined to contain the system files  for example  user 0   whenever a file name is given to be loaded  the operating system first searches the local ufd if the file is found  it is used if it is not found  the system automatically searches the special user directory that contains the system files the sequence of directories searched when a file is named is called the  the search path can be extended to contain an unlimited list of directories to search when a command name is given this method is the one most used in unix and ms-dos systems can also be designed so that each user has his own search path 10.3.5 tree-structured directories once we have seen how to view a two-level directory as a two-level tree  the natural generalization is to extend the directory structure to a tree of arbitrary height  figure 10.10   this generalization allows users to create their own subdirectories and to organize their files accordingly a tree is the most common directory structure the tree has a root directory  and every file in the system has a unique path name 10.3 439 root ititi 0 0 figure i 0.10 tree-structured directory structure a directory  or subdirectory  contains a set of files or subdirectories a directory is simply another file  but it is treated in a special way all directories have the same internal format one bit in each directory entry defines the entry as a file  0  or as a subdirectory  1   special system calls are used to create and delete directories in normal use  each process has a current directory the should contain most of the files that are of current interest to the process when reference is made to a file  the current directory is searched if a file is needed that is not in the current directory  then the user usually must either specify a path name or change the current directory to be the directory holding that file to change directories  a system call is provided that takes a directory name as a parameter and uses it to redefine the current directory thus  the user can change his current directory whenever he desires from one change directory system call to the next  all open system calls search the current directory for the specified file note that the search path may or may not contain a special entry that stands for the current directory the initial current directory of the login shell of a user is designated when the user job starts or the user logs in the operating system searches the accounting file  or some other predefined location  to find an entry for this user  for accounting purposes   in the accounting file is a pointer to  or the name of  the user 's initial directory this pointer is copied to a local variable for this user that specifies the user 's initial current directory from that shell  other processes can be spawned the current directory of any subprocess is usually the current directory of the parent when it was spawned path names can be of two types  absolute and relative an begins at the root and follows a down to the specified file  giving the directory names on the path a defi11es a path from the current directory for example  in the tree-structured file system of figure 10.10  440 chapter 10 if the current directory is root/spell/mail  then the relative path nan e prt/jirst refers to the same file as does the absolute path name root/spell/mail/prt/jirst allowing a user to define her own subdirectories permits her to impose a structure on her files this structure might result in separate directories for files associated with different topics  for example  a subdirectory was created to hold the text of this book  or different forms of information  for example  the directory programs may contain source programs ; the directory bin may store all the binaries   an interesting policy decision in a tree-structured directory concerns how to handle the deletion of a directory if a directory is empty  its entry in the directory that contains it can simply be deleted however  suppose the directory to be deleted is not ernpty but contains several files or subdirectories one of two approaches can be taken some systems  such as ms-dos  will not delete a directory unless it is empty thus  to delete a directory  the user must first delete all the files in that directory if any subdirectories exist this procedure must be applied recursively to them  so that they can be deleted also this approach can result in a substantial amount of work an alternative approach  such as that taken by the unix rm command  is to provide an option  when a request is made to delete a directory  all that directory 's files and subdirectories are also to be deleted either approach is fairly easy to implement ; the choice is one of policy the latter policy is more convenient  but it is also more dangerous  because an entire directory structure can be removed with one command if that command is issued in error  a large number of files and directories will need to be restored  assuming a backup exists   with a tree-structured directory system  users can be allowed to access  in addition to their files  the files of other users for example  user b can access a file of user a by specifying its path names user b can specify either an absolute or a relative path name alternatively  user b can change her current directory to be user a 's directory and access the file by its file names a path to a file in a tree-struch1red directory can be longer than a path in a two-level directory to allow users to access programs without having to remember these long paths  the macintosh operating system automates the search for executable programs one method it uses is to maintain a file  called the desktop file  containing the metadata code and the name and location of all executable programs it has seen when a new hard disk is added to the system  or the network is accessed  the operating system traverses the directory structure  searching for executable programs on the device and recording the pertinent information this mechanism supports the double-dick execution functionality described previously a double-dick on a file causes its creatorattribute data to be read and the desktop file to be searched for a match once the match is found  the appropriate executable program is started with the clicked-on file as its input 10.3.6 acyclic-graph directories consider two programmers who are working on a joint project the files associated with that project can be stored in a subdirectory  separating them from other projects and files of the two programmers but since both programmers are equally responsible for the project  both want the subdirectory to be in 10.3 directory and disk structure 441 figure 10.11 acyclic-graph directory structure their own directories the common subdirectory should be shared a shared directory or file will exist in the file system in two  or more  places at once a tree structure prohibits the sharing of files or directories an acyclic graph -that is  a graph with no cycles-allows directories to share subdirectories and files  figure 10.11   the same file or subdirectory may be in two different directories the acyclic graph is a natural generalization of the tree-structured directory scheme it is important to note that a shared file  or directory  is not the same as two copies of the file with two copies  each programmer can view the copy rather than the original  but if one programmer changes the file  the changes will not appear in the other 's copy with a shared file  only one actual file exists  so any changes made by one person are immediately visible to the other sharing is particularly important for subdirectories ; a new file created by one person will automatically appear in all the shared subdirectories when people are working as a team  all the files they want to share can be put into one directory the ufd of each team member will contain this directory of shared files as a subdirectory even in the case of a single user  the user 's file organization may require that some file be placed in different subdirectories for example  a program written for a particular project should be both in the directory of all programs and in the directory for that project shared files and subdirectories can be implemented in several ways a common way  exemplified by many of the unix systems  is to create a new directory entry called a link a link is effectively a pointer to another file or subdirectory for example  a link may be implemented as an absolute or a relative path name when a reference to a file is made  we search the directory if the directory entry is marked as a link  then the name of the real file is included in the link information we resolve the link by using that path name to locate the real file links are easily identified by their format in the directory entry  or by having a special type on systems that support types  and are effectively 442 chapter 10 indirect pointers the operating system ignores these links when traversing directory trees to preserve the acyclic structure of the system another common approach to implementing shared files is simply to duplicate all information about them in both sharing directories thus  both entries are identical and equal consider the difference between this approach and the creation of a link the link is clearly different from the original directory entry ; thus  the two are not equal duplicate directory entries  however  make the original and the copy indistinguishable a major problem with duplicate directory entries is maintaining consistency when a file is modified an acyclic-graph directory structure is more flexible than is a simple tree structure  but it is also more complex several problems must be considered carefully a file may now have multiple absolute path names consequently  distinct file names may refer to the same file this situation is similar to the aliasing problem for programming languages if we are trying to traverse the entire file system-to find a file  to accumulate statistics on all files  or to copy all files to backup storage-this problem becomes significant  since we do not want to traverse shared structures more than once another problem involves deletion when can the space allocated to a shared file be deallocated and reused one possibility is to remove the file whenever anyone deletes it  but this action may leave dangling pointers to the now-nonexistent file worse  if the remaining file pointers contain actual disk addresses  and the space is subsequently reused for other files  these dangling pointers may point into the middle of other files in a system where sharing is implemented by symbolic links  this situation is somewhat easier to handle the deletion of a link need not affect the original file ; only the link is removed if the file entry itself is deleted  the space for the file is deallocated  leaving the links dangling we can search for these links and remove them as well  but unless a list of the associated links is kept with each file  this search can be expensive alternatively  we can leave the links until an attempt is made to use them at that time  we can determine that the file of the name given by the link does not exist and can fail to resolve the link name ; the access is treated just as with any other illegal file name  in this case  the system designer should consider carefully what to do when a file is deleted and another file of the same name is created  before a symbolic link to the original file is used  in the case of unix  symbolic links are left when a file is deleted  and it is up to the user to realize that the orig  llcal file is gone or has been replaced microsoft windows  all flavors  uses the same approach another approach to deletion is to preserve the file until all references to it are deleted to implement this approach  we must have some mechanism for determining that the last reference to the file has been deleted we could keep a list of all references to a file  directory entries or symbolic links   when a link or a copy of the directory entry is established  a new entry is added to the file-reference list when a link or directory entry is deleted  we remove its entry on the list the file is deleted when its file-reference list is empty the trouble with this approach is the variable and potentially large size of the file-reference list however  we really do not need to keep the entire list -we need to keep only a count of the number of references adding a new link or directory entry increments the reference count ; deleting a link or entry decrements the count when the count is 0  the file can be deleted ; there are no remaining references to it the unix operating system uses this approach 10.3 443 for nonsymbolic links  or keeping a reference count in the file information block  or inode ; see appendix a.7.2   by effectively prohibiting multiple references to directories  we maintain an acyclic-graph structure to avoid problems such as the ones just discussed  some systems do not allow shared directories or links for example  in ms-dos  the directory structure is a tree structure rather than an acyclic graph 10.3.7 general graph directory a serious problem with using an acyclic-graph structure is ensuring that there are no cycles if we start with a two-level directory and allow users to create subdirectories  a tree-structured directory results it should be fairly easy to see that simply adding new files and subdirectories to an existing tree-structured directory preserves the tree-structured nature howeve1 ~ when we add links  the tree structure is destroyed  resulting in a simple graph structure  figure 10.12   the primary advantage of an acyclic graph is the relative simplicity of the algorithms to traverse the graph and to determine when there are no more references to a file we want to avoid traversing shared sections of an acyclic graph twice  mainly for performance reasons if we have just searched a major shared subdirectory for a particular file without finding it  we want to avoid searching that subdirectory again ; the second search would be a waste of time if cycles are allowed to exist in the directory  we likewise want to avoid searching any component twice  for reasons of correctness as well as performance a poorly designed algorithm might result in an infinite loop continually searching through the cycle and never terminating one solution is to limit arbitrarily the number of directories that will be accessed during a search a similar problem exists when we are trying to determine when a file can be deleted with acyclic-graph directory structures  a value of 0 in the reference count means that there are no more references to the file or directory  figure 10.12 general graph directory 444 chapter 10 10.4 and the file can be deleted however  when cycles exist  the reference count may not be 0 even when it is no longer possible to refer to a directory or file this anomaly results from the possibility of self-referencing  or a cycle  in the directory structure in this case  we generally need to use a garbage-collection scheme to determine when the last reference has been deleted and the disk space can be reallocated garbage collection involves traversing the entire file system  marking everything that can be accessed then  a second pass collects everything that is not marked onto a list of free space  a similar marking procedure can be used to ensure that a traversal or search will cover everything in the file system once and only once  garbage collection for a disk-based file system  however  is extremely time consuming and is thus seldom attempted garbage collection is necessary only because of possible cycles in the graph thus  an acyclic-graph structure is much easier to work with the difficulty is to avoid cycles as new links are added to the structure how do we know when a new lir1k will complete a cycle there are algorithms to detect cycles in graphs ; however  they are computationally expensive  especially when the graph is on disk storage a simpler algorithm in the special case of directories and links is to bypass links during directory traversal cycles are avoided  and no extra overhead is incurred just as a file must be opened before it is used  a file system must be mounted before it can be available to processes on the system more specifically  the directory structure may be built out of multiple volumes  which must be mounted to make them available within the file-system name space the mount procedure is straightforward the operating system is given the name of the device and the location within the file structure where the file system is to be attached some operating systems require that a file system type be provided  while others inspect the structures of the device and determine the type of file system typically  a mount point is an empty directory for instance  on a unix system  a file system containing a user 's home directories might be mounted as /home ; then  to access the directory structure within that file system  we could precede the directory names with /home  as in /home/jane motmting that file system under /users would result in the path name /users/jane  which we could use to reach the same directory next  the operating system verifies that the device contains a valid file system it does so by asking the device driver to read the device directory and verifying that the directory has the expected format finally  the operating system notes in its directory structure that a file system is n1.ounted at the specified mount point this scheme enables the operating system to traverse its directory structure  switching among file systems  and even file systems of varying types  as appropriate to illustrate file mounting  consider the file system depicted in figure 10.13  where the triangles represent subtrees of directories that are of interest figure 10.13  a  shows an existing file system  while figure 10.13  b  shows an unmounted volume residing on /device/ds ! c at this point  only the files on the existing file system can be accessed figure 10.14 shows the effects of mounting 10.4 file-system mounting 445 bill  a   b  figure 10.13 file system  a  existing system  b  unmounted volume the volume residing on /device/dsk over /users if the volume is unmounted  the file system is restored to the situation depicted in figure 10.13 systems impose semantics to clarify functionality for example  a system may disallow a mount over a directory that contains files ; or it may make the mounted file system available at that directory and obscure the directory 's existing files until the file system is unmounted  terminating the use of the file system and allowing access to the original files in that directory as another example  a system may allow the same file system to be mounted repeatedly  at different mount points ; or it may only allow one mount per file system consider the actions of the classic macintosh operating system whenever the system encounters a disk for the first time  hard disks are found at boot time  and optical disks are seen when they are inserted into the drive   the macintosh operating system searches for a file system on the device if it finds one  it automatically mounts the file system at the root level  adding a folder icon on the screen labeled with the name of the file system  as stored in the i figure 10.14 mount point 446 chapter 10 10.5 device directory   the user is then able to click on the icon and thus display the newly mounted file system mac os x behaves much like bsd unix  on which it is based all file systems are mounted under the /volumes directory the mac os x gui hides this fact and shows the file systems as if they were all mounted at the root level the microsoft windows family of operating systems  95  98  nt  small 2000  2003  xp  vista  maintains an extended two-level directory structure  with devices and volumes assigned drive letters volumes have a general graph directory structure associated with the drive letter the path to a specific file takes the form of drive-letter  \ path \ to \ file the more recent versions of windows allow a file system to be mounted anywhere in the directory tree  just as unix does windows operating systems automatically discover all devices and mount all located file systems at boot time in some systems  like unix  the mount commands are explicit a system configuration file contains a list of devices and mount points for automatic mounting at boot time  but other mounts may be executed manually issues concerning file system mounting are further discussed in section 11.2.2 and in appendix a.7.5 in the previous sections  we explored the motivation for file sharing and some of the difficulties involved in allowing users to share files such file sharing is very desirable for users who want to collaborate and to reduce the effort required to achieve a computing goal therefore  user-oriented operating systems must accommodate the need to share files in spite of the inherent difficulties in this section  we examine more aspects of file sharing we begin by discussing general issues that arise when multiple users share files once multiple users are allowed to share files  the challenge is to extend sharing to multiple file systems  including remote file systems ; we discuss that challenge as well finally  we consider what to do about conflicting actions occurring on shared files for instance  if multiple users are writing to a file  should all the writes be allowed to occurf or should the operating system protect the users ' actions from one another 10.5.1 multiple users when an operating system accommodates multiple users  the issues of file sharing  file naming  and file protection become preeminent given a directory structure that allows files to be shared by users  the system must mediate the file sharing the system can either allow a user to access the files of other users by default or require that a user specifically grant access to the files these are the issues of access control and protection  which are covered in section 10.6 to implement sharing and protection  the system must maintain more file and directory attributes than are needed on a single-user system although many approaches have been taken to meet this requirement  most systems have evolved to use the concepts of file  or directory  owner  or user  and group the owner is the user who can change attributes and grant access and who has the most control over the file the group attribute defines a subset of users who 10.5 447 can share access to the file for example  the owner of a file on a unix system can issue all operations on a file  while members of the file 's group can execute one subset of those operations  and all other users can execute another subset of operations exactly which operations can be executed by group members and other users is definable by the file 's owner more details on permission attributes are included in the next section the owner and group ids of a given file  or directory  are stored with the other file attributes when a user requests an operation on a file  the user id can be compared with the owner attribute to determine if the requesting user is the owner of the file likewise  the group ids can be compared the result indicates which permissions are applicable the system then applies those permissions to the requested operation and allows or denies it many systems have multiple local file systems  including volumes of a single disk or multiple volumes on multiple attached disks in these cases  the id checking and permission matching are straightforward  once the file systems are mounted 10.5.2 remote file systems with the advent of networks  chapter 16   communication among remote computers became possible networking allows the sharing of resources spread across a campus or even around the world one obvious resource to share is data in the form of files through the evolution of network and file technology  remote file-sharing methods have changed the first implemented method involves manually transferring files between machines via programs like ftp the second major method uses a  dfs  in which remote directories are visible from a local machine in some ways  the third method  the is a reversion to the first a browser is needed to gain access to the remote files  and separate operations  essentially a wrapper for ftp  are used to transfer files ftp is used for both anonymous and authenticated access allows a user to transfer files without having an account on the remote system the world wide web uses anonymous file exchange almost exclusively dfs involves a much tighter integration between the machine that is accessing the remote files and the machine providing the files this integration adds complexity  which we describe in this section 10.5.2.1 the client-server model remote file systems allow a computer to mom1.t one or more file systems from one or more remote machines in this case  the machine containing the files is the server  and the machine seeking access to the files is the client the client-server relationship is common with networked machines generally  the server declares that a resource is available to clients and specifies exactly which resource  in this case  which files  and exactly which clients a server can serve multiple clients  and a client can use multiple servers  depending on the implementation details of a given client-server facility the server usually specifies the available files on a volume or directory level client identification is more difficult a client can be specified network name or other identifier  such as an ip address  but these can be 448 chapter 10 or imitated as a result of spoofing  an unauthorized client could be allowed access to the server more secure solutions include secure authentication of the client via encrypted keys unfortunately  with security come many challenges  including ensuring compatibility of the client and server  they must use the same encryption algorithms  and security of key exchanges  intercepted keys could again allow unauthorized access   because of the difficulty of solving these problems  unsecure authentication methods are most commonly used in the case of unix and its network file system  nfs   authentication takes place via the client networking information  by default in this scheme  the user 's ids on the client and server must match lf they do not  the server will be unable to determine access rights to files consider the example of a user who has an id of 1000 on the client and 2000 on the server a request from the client to the server for a specific file will not be handled appropriately  as the server will determine if user 1000 has access to the file rather than basing the determination on the real user id of 2000 access is thus granted or denied based on incorrect authentication information the server must trust the client to present the correct user id note that the nfs protocols allow many-to-many relationships that is  many servers can provide files to many clients in fact a given machine can be both a server to some nfs clients and a client of other nfs servers once the remote file system is mounted  file operation requests are sent on behalf of the user across the network to the server via the dfs protocol typically  a file-open request is sent along with the id of the requesting user the server then applies the standard access checks to determine if the user has credentials to access the file in the mode requested the request is either allowed or denied if it is allowed  a file handle is returned to the client application  and the application then can perform read  write  and other operations on the file the client closes the file when access is completed the operating system may apply semantics similar to those for a local file-system mount or may use different semantics 10.5.2.2 distributed information systems to make client-server systems easier to manage  also known as provide unified access to the information needed for remote computing the provides host-name-to-network-address translations for the entire internet  including the world wide web   before dns became widespread  files containing the same information were sent via e-mail or ftp between all networked hosts this methodology was not scalable dns is further discussed in section 16.5.1 other distributed information systems provide user name/password/user id/group id space for a distributed facility unix systems have employed a wide variety of distributed-information methods sun microsystems introduced yellow pages  since renamed or and most of the industry adopted its use it centralizes storage of user names  host names  printer information  and the like unfortunately  it uses unsecure authentication methods  including sending user passwords unencrypted  in clear text  and identifying hosts by ip address sun 's nis + is a much more secure replacement for nis but is also much more complicated and has not been widely adopted 10.5 449 network information is used in conjunction with user authentication  user name and password  to create a that the server uses to decide whether to allow or deny access to a requested file system for this authentication to be valid  the user names m.u.st match from machine to machine  as with nfs   microsoft uses two distributed naming structures to provide a single name space for users the older naming technology is the newer technology  available in windows xp and windows 2000  is once established  the distributed naming facility is used by all clients servers to authenticate users the industry is moving toward use of the as a secure distributed naming mechanism in fact  active is based on ldap sun microsystems includes ldap with the operating system and allows it to be employed for user authentication as well as system-wide retrieval of information  such as availability of printers conceivably  one distributed ldap directory could be used by an organization to store all user and resource information for all the organization 's computers the result would be for users  who would enter their authentication information once for access to all computers within the organization it would also ease system-administration efforts by combining  in one location  information that is currently scattered in various files on each system or in different distributed information services 10.5.2.3 failure modes local file systems can fail for a variety of reasons  including failure of the disk containing the file system  corruption of the directory structure or other disk-management information  collectively called disk-controller failure  cable failure  and host-adapter failure user or system-administrator failure can also cause files to be lost or entire directories or volumes to be deleted many of these failures will cause a host to crash and an error condition to be displayed  and human intervention will be required to repair the damage remote file systems have even more failure modes because of the complexity of network systems and the required interactions between remote machines  many more problems can interfere with the proper operation of remote file systems in the case of networks  the network can be interrupted between two hosts such interruptions can result from hardware failure  poor hardware configuration  or networking implementation issues although some networks have built-in resiliency  including multiple paths between hosts  many do not any single failure can thus interrupt the flow of dfs commands consider a client in the midst of using a remote file system it has files open from the remote host ; among other activities  it may be performing directory lookups to open files  reading or writing data to files  and closing files now consider a partitioning of the network  a crash of the server  or even a scheduled shutdown of the server suddenly  the remote file system is no longer reachable this scenario is rather common  so it would not be appropriate for the client system to act as it would if a local file system were lost rather  the system can either terminate all operations to the lost server or delay operations until the server is again reachable these failure semantics are defined and in plemented as part of the remote-file-system protocol termination of all operations can 450 chapter 10 result in users ' losing data-and patience thus  most dfs protocols either enforce or allow delaying of file-system operations to rencote hosts  with the hope that the remote host will become available again to implement this kind of recovery from failure  some kind of may be maintained on both the client and the server if both server and client maintain knowledge of their current activities and open files  then they can seamlessly recover from a failure in the situation where the server crashes but must recognize that it has remotely rnounted exported file systems and opened files  nfs takes a simple approach  implementing a dfs in essence  it assumes that a client request for a file read or write would not have occurred unless the file system had been remotely mounted and the file had been previously open the nfs protocol carries all the information needed to locate the appropriate file and perform the requested operation similarly  it does not track which clients have the exported volumes mounted  again assuming that if a request comes in  it must be legitimate while this stateless approach makes nfs resilient and rather easy to implement  it also makes it unsecure for example  forged read or write requests could be allowed by an nfs server even though the requisite mount request and permission check had not taken place these issues are addressed in the industry standard nfs version 4  in which nfs is made stateful to improve its security  performance  and functionality 10.5.3 consistency semantics represent an important criterion for evaluating any file system that supports file sharing these semantics specify how multiple users of a system are to access a shared file simultaneously in particular  they specify when modifications of data by one user will be observable by other users these semantics are typically implemented as code with the file system consistency semantics are directly related to the process-synchronization algorithms of chapter 6 however  the complex algorithms of that chapter tend not to be implemented in the case of file i/0 because of the great latencies and slow transfer rates of disks and networks for example  performing an atomic transaction to a remote disk could involve several network communications  several disk reads and writes  or both systems that attempt such a full set of functionalities tend to perform poorly a successful implementation of complex sharing semantics can be found in the andrew file system for the following discussion  we assume that a series of file accesses  that is  reads and writes  attempted by a user to the same file is always enclosed between the open   and close   operations the series of accesses between the open   and close   operations makes up a to illustrate the concept  we sketch several prominent examples of consistency semantics 10.5.3.1 unix semantics the unix file system  chapter 17  uses the following consistency semantics  writes to an open file by a user are visible immediately to other users who have this file open one mode of sharing allows users to share the pointer of current location into the file thus  the advancing of the pointer by one user affects all 10.6 10.6 451 sharing users here  a file has a single image that interleaves all accesses  regardless of their origin in the unix semantics  a file is associated with a single physical image that is accessed as an exclusive resource contention for this single image causes delays in user processes 10.5.3.2 session semantics the andrew file system  afs   chapter 17  uses the following consistency semantics  writes to an open file by a user are not visible immediately to other users that have the same file open once a file is closed  the changes made to it are visible only in sessions starting later already open instances of the file do not reflect these changes according to these semantics  a file may be associated temporarily with several  possibly different  images at the same time consequently  multiple users are allowed to perform both read and write accesses concurrently on their images of the file  without delay almost no constraints are enforced on scheduling accesses 10.5.3.3 immutable-shared-files semantics a unique approach is that of once a file is declared as shared by its creator  it cam1ot be modified an immutable ile has two key properties  its name may not be reused  and its contents may not be altered thus  the name of an immutable file signifies that the contents of the file are fixed the implementation of these semantics in a distributed system  chapter 17  is simple  because the sharing is disciplined  read-only   when information is stored in a computer system  we want to keep it safe from physical damage  the issue of reliability  and improper access  the issue of protection   reliability is generally provided by duplicate copies of files many computers have systems programs that automatically  or through computer-operator intervention  copy disk files to tape at regular intervals  once per day or week or month  to maintain a copy should a file system be accidentally destroyed file systems can be damaged by hardware problems  such as errors in reading or writing   power surges or failures  head crashes  dirt  temperature extremes  and vandalism files may be deleted accidentally bugs in the file-system software can also cause file contents to be lost reliability is covered in more detail in chapter 12 protection can be provided in many ways for a small single-user system  we might provide protection by physically removing the floppy disks and locking them in a desk drawer or file cabinet in a multiuser system  however  other mechanisms are needed 452 chapter 10 10.6.1 types of access the need to protect files is a direct result of the ability to access files systems that do not permit access to the files of other users do not need protection thus  we could provide complete protection by prohibiting access alternatively  we could provide free access with no protection both approaches are too extreme for general use what is needed is protection mechanisms provide controlled access by limitin.g the types of file access that can be made access is permitted or denied depending on several factors  one of which is the type of access requested several different types of operations may be controlled  read read from the file write write or rewrite the file execute load the file into memory and execute it append write new information at the end of the file delete delete the file and free its space for possible reuse list list the name and attributes of the file other operations  such as renaming  copying  and editing the file  may also be controlled for many systems  however  these higher-level fm1ctions may be implemented by a system program that makes lower-level system calls protection is provided at only the lower level for instance  copying a file may be implemented simply by a sequence of read requests in this case  a user with read access can also cause the file to be copied  printed  and so on many protection mechanisms have been proposed each has advantages and disadvantages and must be appropriate for its intended application a small computer system that is used by only a few members of a research group  for example  may not need the same types of protection as a large corporate computer that is used for research  finance  and personnel operations we discuss some approaches to protection in the following sections and present a more complete treatment in chapter 14 10.6.2 access control the most common approach to the protection problem is to make access dependent on the identity of the user different users may need different types of access to a file or directory the most general scheme to implement dependent access is to associate with each file and directory an  acju specifying user names and the types of access allowed for each user when a user requests access to a particular file  the operating system checks the access list associated with that file if that user is listed for the requested access  the access is allowed otherwise  a protection violation occurs  and the user job is denied access to the file this approach has the advantage of enabling complex access methodologies the main problem with access lists is their length if we want to allow everyone to read a file  we must list all users with read access this technique has two undesirable consequences  10.6 453 constructing such a list may be a tedious and unrewarding task  especially if we do not know in advance the list of users in the system the directory entry  previously of fixed size  now must be of variable size  resulting in more complicated space management these problems can be resolved by use of a condensed version of the access list to condense the length of the access-control list  many systems recognize three classifications of users in connection with each file  owner the user who created the file is the owner group a set of users who are sharing the file and need similar access is a group  or work group universe all other users in the system constitute the universe the most common recent approach is to combine access-control lists with the more general  and easier to implement  owner  group  and universe accesscontrol scheme just described for example  solaris 2.6 and beyond use the three categories of access by default but allow access-control lists to be added to specific files and directories when more fine-grained access control is desired to illustrate  consider a person  sara  who is writing a new book she has hired three graduate students  jim  dawn  and jill  to help with the project the text of the book is kept in a file named book the protection associated with this file is as follows  sara should be able to invoke all operations on the file jim  dawn  and jill should be able only to read and write the file ; they should not be allowed to delete the file all other users should be able to read  but not write  the file  sara is interested in letting as many people as possible read the text so that she can obtain feedback  to achieve such protection  we must create a new group-say  textwith members jim  dawn  and jill the name of the group  text  must then be associated with the file book  and the access rights must be set in accordance with the policy we have outlined now consider a visitor to whom sara would like to grant temporary access to chapter 1 the visitor can not be added to the text group because that would give him access to all chapters because a file can only be in one group  sara can not add another group to chapter 1 \ nith the addition of access-control-list functionality  though  the visitor can be added to the access control list of chapter 1 for this scheme to work properly  permissions and access lists must be controlled tightly this control can be accomplished in several ways for example  in the unix system  groups can be created and modified only by the manager of the facility  or by any superuser   thus  control is achieved through human interaction in the vms system  the owner of the file can create 454 chapter 10 and modify the access-control list access lists are discussed further in section 14.5.2 with the more limited protection classification  only three fields are needed to define protection often  each field is a collection of bits  and each bit either allows or prevents the access associated with it for example  the unix system defines three fields of 3 bits each -rwx  where r controls read access  w controls write access  and x controls execution a separate field is kept for the file owner  for the file 's group  and for all other users in this scheme  9 bits per file are needed to record protection information thus  for our example  the protection fields for the file book are as follows  for the owner sara  all bits are set ; for the group text  the rand w bits are set ; and for the universe  only the r bit is set one difficulty in combining approaches comes in the user interface users must be able to tell when the optional acl permissions are set on a file in the solaris example  a + appends the regular permissions  as in  f1l s'/stetvl  ji users  pbg-la.ptof \ users  permissions for gue  ; t full contml h-1odi ~ ,. f ; _e a.d g execute r.ead 'vi/rite spec  ia.l permissions a.llo w for specia.l permissions orfor advanced settings click .a.dva.nced .a.dva.nced figure 10.15 windows xp access-control list management 10.6 455 19 -rw-r--r + 1 jim staff 130 may 25 22  13 file1 a separate set of commands  setfacl and getfacl  is used to manage the acls windows xp users typically manage access-control lists via the cui figure 10.15 shows a file-permission window on windows xp 's ntfs file system in this example  user guest is specifically denied access to the file lo.tex another difficulty is assigning precedence when permission and acls conflict for example  if joe is in a file 's group  which has read permission  but the file has an acl granting joe read and write permission  should a write by joe be granted or denied solaris gives acls precedence  as they are more fine-grained and are not assigned by default   this follows the general rule that specificity should have priority 10.6.3 other protection approaches another approach to the protection problem is to associate a password with each file just as access to the computer system is often controlled by a password  access to each file can be controlled in the same way if the passwords are chosen randomly and changed often  this scheme may be effective in limiting access to a file the use of passwords has a few disadvantages  however first  the number of passwords that a user needs to remember may permissions in a unix system in the unix system  directory protection and file protection are handled similarly associated with each subdirectory are three fields-owner  group  and universe-each consisting of the three bits rwx thus  a user can list the content of a subdirectory only if the r bit is set in the appropriate field similarly  a user can change his current directory to another current directory  say  faa  only if the x bit associated with the faa subdirectory is set in the appropriate field a sample directory listing from a unix environment is shown in figure 10.16 the first field describes the protecti.on of the file or directory ad as the first character indicates a s11bdirectory also shown are the number of links to the file  the owner 's name  the group 's name  the size of the file in bytes  the date of last modification  and finally the file 's name  with optional extension   -rw-rw-r l pbg staff 31200 sep 30l  uo intro.ps drwx 5 pbg staff 512 jul 8 09.33 private/ drwxrwxr-x 2 pbg staff 512 jul8 09  35 doc/ drwxrwx 2 pbg student 512 aug 3 14  13 student-proj/ -rw-r--r 1 pbg staff 9423 feb 24 2003 program.c -rwxr-xr-x l pbg staff 20471 feb 24 2003 program drwx ~ -x--x 4 pbg faculty 512 jul 31 10  31 lib/ drwx 3 pbg staff 1024 aug 29 06  52 mail/ drwxrwxrwx 3 pbg staff 512 jul 8 09  35 test/ figure 10.16 a sample directory listing 456 chapter 10 10.7 become large  making the scheme impractical second  if only one password is used for all the files  then once it is discovered  all files are accessible ; protection is on an all-or-none basis some systems  for example  tops-20  allow a user to associate a password with a subdirectory  rather than with an individual file  to deal with this problem the ibmvm/cms operating system allows three passwords for a minidisk-one each for read  write  and nrultiwrite access some single-user operating systencs-such as ms-dos and versions of the macintosh operating system prior to mac os x-provide little in terms of file protection in scenarios where these older systems are now being placed on networks file sharing and communication  protection mechanisms must be into them designing a feature for a new operating system is almost always easier than adding a feature to an existing one such updates are usually less effective and are not seamless in a multilevel directory structure  we need to protect not only individual files but also collections of files in subdirectories ; that is  we need to provide a mechanism for directory protection the directory operations that must be protected are somewhat different from the file operations we want to control the creation and deletion of files in a directory in addition  we probably want to control whether a user can determine the existence of a file in a directory sometimes  knowledge of the existence and name of a file is significant in itself thus  listing the contents of a directory must be a protected operation similarly  if a path name refers to a file in a directory  the user must be allowed access to both the directory and the file in systems where files may have numerous path names  such as acyclic or general graphs   a given user may have different access rights to a particular file  depending on the path name used a file is an abstract data type defined and implemented by the operating system it is a sequence of logical records a logical record may be a byte  a line  of fixed or variable length   or a more complex data item the operating system may specifically support various record types or may leave that support to the application program the major task for the operating system is to map the logical file concept onto physical storage devices such as magnetic tape or disk since the physical record size of the device may not be the same as the logical record size  it may be necessary to order logical records into physical records again  this task may be supported by the operating system or left for the application program each device in a file system keeps a volume table of contents or a device directory listing the location of the files on the device in addition  it is useful to create directories to allow files to be organized a single-level directory in a multiuser system causes naming problems  since each must have a unique name a two-level directory solves this creating a separate directory for each users files the directory lists name and includes the file 's location on the disk  length  type  owner  time creation  time of last use  and so on the natural generalization of a two-level directory is a tree-structured directory a tree-structured directory allows a user to create subdirectories to organize files acyclic-graph directory structures enable users to share 457 subdirectories and files but complicate searching and deletion a general graph structure allows complete flexibility in the sharing of files and directories but sometimes requires garbage collection to recover unused disk space disks are segmented into one or more volumes/ each containing a file system or left raw file systems may be mounted into the system 's naming structures to make them available the naming scheme varies by operating system once mounted  the files within the volume are available for use file systems may be unmounted to disable access or for maintenance file sharing depends on the semantics provided by the system files may have multiple readers  multiple writers  or limits on sharing distributed file systems allow client hosts to mount volumes or directories from servers  as long as they can access each other across a network remote file systems present challenges in reliability  performance  and security distributed information systems maintain user/ host/ and access information so that clients and servers can share state information to ncanage use and access since files are the main information-storage mechanism in most computer systems  file protection is needed access to files can be controlled separately for each type of access-read  write  execute  append  delete  list directory  and so on file protection can be provided by access lists  passwords  or other techniques 10.1 some systems provide file sharing by maintaining a single copy of a file ; other systems maintain several copies  one for each of the users sharing the file discuss the relative merits of each approach 10.2 some systems automatically open a file when it is referenced for the first time and close the file when the job terminates discuss the advantages and disadvantages of this scheme compared with the more traditional one  where the user has to open and close the file explicitly 10.3 in some systems  a subdirectory can be read and written by an authorized user  just as ordinary files can be a describe the protection problems that could arise b suggest a scheme for dealing with each of these protection problems 10.4 do some systems keep track of the type of a file  while others leave it to the user and others simply do not implement multiple file types which system is better 10.5 consider a system that supports 5,000 users suppose that you want to allow 4,990 of these users to be able to access one file a howwould specify this protection scheme in unix b can you suggest another protection scheme that can be used more effectively for this purpose than the scheme provided by unix 458 chapter 10 10.6 what are the advantages and disadvantages of providing ncandatory locks instead of advisory locks whose usage is left to users ' discretion 10.7 explain the purpose of the open   and close   operations 10.8 the open-file table is used to maintain information about files that are currently open should the operating system maintain a separate table for each user or just maintain one table that contains references to files that are currently being accessed by all users if the same file is being accessed by two different programs or users  should there be separate entries in the open-file table 10.9 give an example of an application that could benefit from operatingsystem support for random access to indexed files 10.10 discuss the advantages and disadvantages of associating with remote file systems  stored on file servers  a set of failure semantics different from that associated with local file systems 10.11 could you simulate a multilevel directory structure with a single-level directory structure in which arbitrarily long names can be used if your answer is yes  explain how you can do so  and contrast this scheme with the multilevel directory scheme if your answer is no  explain what prevents your simulation 's success how would your answer change if file names were limited to seven characters 10.12 what are the implications of supporting unix consistency semantics for shared access for files stored on remote file systems 10.13 if the operating system knew that a certain application was going to access file data in a sequential manner  how could it exploit this information to improve performance 10.14 consider a file system in which a file can be deleted and its disk space reclaimed while links to that file still exist what problems may occur if a new file is created in the same storage area or with the same absolute path name how can these problems be avoided 10.15 discuss the advantages and disadvantages of supporting links to files that cross mount points  that is  the file link refers to a file that is stored in a different volume   10.16 what are the advantages and disadvantages of recording the name of the creating program with the file 's attributes  as is done in the macintosh operating system  general discussions concerning file systems are offered by grosshans  1986   golden and pechura  1986  describe the structure of microcomputer file systems database systems and their file structures are described in full in silberschatz et al  2001   a multilevel directory structure was first implemented on the multics system  organick  1972    most operating systems now implement multilevel 459 directory structures these include linux  bovet and cesati  2002    mac os x  http  / /www.apple.com/macosx/   solaris  mcdougall and mauro  2007    and all versions of windows  russinovich and solomon  2005    the network file system  nfs   designed by sun microsystems  allows directory structures to be spread across networked computer systems nfs is fully described in chapter 17 nfs version 4 is described in rfc3505  http  / /www.ietf.org/rfc/rfc3530.txt   general discussion of solaris file systems is found in the sun system administration guide  devices and file systems  http  / i docs sun com/ app i docs/ doc/817-5093   dns was first proposed by su  1982  and has gone through several revisions since  with mockapetris  1987  adding several major features eastlake  1999  has proposed security extensions to let dns hold security keys ldap  also known as x.509  is a derivative subset of the x.soo distributed directory protocol it was defined by yeong et al  1995  and has been implemented on many operating systems interesting research is ongoing in the area of file-system interfaces-in particular  on issues relating to file naming and attributes for example  the plan 9 operating system from bell laboratories  lucent technology  makes all objects look like file systems thus  to display a list of processes on a system  a user simply lists the contents of the /proc directory similarly  to display the time of day  a user need only type the file i dev i time 11.1 c as we saw in chapter 10  the file system provides the mechanism for on-line storage and access to file contents  including data and programs the file system resides permanently on secondary storage  which is designed to hold a large amount of data permanently this chapter is primarily concerned with issues surrounding file storage and access on the most common secondary-storage medium  the disk we explore ways to structure file use  to allocate disk space  to recover freed space  to track the locations of data  and to interface other parts of the operating system to secondary storage performance issues are considered throughout the chapter to describe the details of implementing local file systems and directory structures to describe the implementation of remote file systems to discuss block allocation and free-block algorithms and trade-offs disks provide the bulk of secondary storage on which a file system is maintained they have two characteristics that make them a convenient medium for storing multiple files  a disk can be rewritten in place ; it is possible to read a block from the disk  modify the block  and write it back into the sance place a disk can access directly any block of information it contains thus  it is simple to access any file either sequentially or randomly  and switching from one file to another requires only moving the read-write heads and waiting for the disk to rotate we discuss disk structure in great detail in chapter 12 461 462 chapter 11 to improve i/0 efficiency  i/0 transfers between memory and disk are performed in units of blocks each block has one or more sectors depending on the disk drive  sector size varies from 32 bytes to 4,096 bytes ; the usual size is 512 bytes provide efficient and convenient access to the disk by allowing data to be stored  located  and retrieved easily a file system poses two quite different design problems the first problem is defining how the file system should look to the user this task involves defining a file and its attributes  the operations allowed on a file  and the directory structure for organizing files the second problem is creating algorithms and data structures to map the logical file system onto the physical secondary-storage devices the file system itself is generally composed of many different levels the structure shown in figure 11.1 is an example of a layered design each level in the design uses the features of lower levels to create new features for use by higher levels the lowest level  the i/o control  consists of and interrupt handlers to transfer information between the main memory and the disk system a device driver can be thought of as a translator its input consists of high-level commands such as retrieve block 123 its output consists of lowlevel  hardware-specific instructions that are used by the hardware controller  which interfaces the i/0 device to the rest of the system the device driver usually writes specific bit patterns to special locations in the i/0 controller 's memory to tell the controller which device location to act on and what actions to take the details of device drivers and the i/o infrastructure are covered in chapter 13 the needs only to issue generic commands to the appropriate device driver to read and write physical blocks on the disk each physical block is identified by its numeric disk address  for example  drive 1  cylilcder 73  track 2  sector 10   this layer also manages the memory buffers and caches that hold various file-system  directory  and data blocks a block application programs ~ logical file system ~ file-organization module ~ basic file system ~ 1/0 control devices figure 11.1 layered file system 11.1 463 in the buffer is allocated before the transfer of a disk block can occur when the buffer is full  the buffer m ~ anager must find more buffer ncemory or free up buffer space to allow a requested i/o to complete caches are used to hold frequently used file-system metadata to improve performance  so managing their contents is critical for optimum system performance the knows about files and their logical blocks  as well as physical blocks by knowing the type of file allocation used and the location of the file  the file-organization module can translate logical block addresses to physical block addresses for the basic file system to transfer each file 's logical blocks are numbered from 0  or 1  through n since the physical blocks containing the data usually do not match the logical numbers  a translation is needed to locate each block the file-organization module also includes the free-space manager  which tracks unallocated blocks and provides these blocks to the file-organization module when requested finally  the f ! je manages metadata information metadata includes all of the file-system structure except the actual data  or contents of the files   the logical file system manages the directory structure to provide the fileorganization module with the information the latter needs  given a symbolic file name it maintains file structure via file-control blocks a flle-corttml  an in most unix file systems  contains information about the file  including ownership  permissions  and location of the file contents the logical file system is also responsible for protection and security  as discussed in chapters 10 and 14 when a layered structure is used for file-system implementation  duplication of code is minimized the i/o control and sometimes the basic file-system code can be used by multiple file systems each file system can then have its own logical file-system and file-organization modules unfortunately  layering can introduce more operating system overhead  which may result in decreased performance the use of layering  including the decision about how many layers to use and what each layer should do  is a major challenge in designing new systems many file systems are in use today most operating systems support more than one for example  most cd-roms are written in the iso 9660 format  a standard format agreed on by cd-rom manufacturers in addition to removable-media file systems  each operating system has one or more diskbased file systems unix uses the fee which is based on the berkeley fast file system  ffs   windows nt  2000  and xp support disk file-system formats of fat  fat32  and ntfs  or windows nt file system   as well as cd-rom  dvd  and floppy-disk file-system formats although linux supports over forty different file systerns  the standard linux file system is known as the with the most common versions being ext2 and ext3 there are also distributed file systems in which a file system on a server is mounted by one or more client computers across a network file-system research continues to be an active area of operating-system design and implementation coogle created its own file system to meet the company 's specific storage and retrieval needs another interesting project is the fuse file-system  which provides flexibility in file-system use by implementing and executing file systems as user-level rather than kernel-level code using fuse  a user can add a new file system to a variety of operating systems and can use that file system to manage her files 464 chapter 11 11.2 as was described in section 10.1.2  operating systems implement open   and close   systems calls for processes to request access to file contents in this section  we delve into the structures and operations used to implement file-system operations 11.2.1 overview several on-disk and in-memory structures are used to implement a file system these structures vary depending on the operating system and the file system  but some general principles apply on disk  the file system may contain information about how to boot an operating system stored there  the total number of blocks  the number and location of free blocks  the directory structure  and individual files many of these structures are detailed throughout the remainder of this chapter ; here  we describe them briefly  a  per volume  can contain information needed by the system to boot an operating system from that volume if the disk does not contain an operating system  this block can be empty it is typically the first block of a volume in ufs  it is called the b,jsck ; in ntfs  it is the  per volume  contains volume  or partition  details  such as the number of blocks in the partition  the size of the blocks  a free-block count and free-block pointers  and a free-fcb count and fcb pointers in ufs  this is called a in ntfs  it is stored in the a directory structure  per file system  is used to organize the files in ufs  this includes file names and associated inode numbers in ntfs  it is stored in the master file table a per-file fcb contains many details about the file it has a unique identifier number to allow association with a directory entry in ntfs  this information is actually stored within the master file table  which uses a relational database structure  with a row per file the in-memory in.formation is used for both file-system management and performance improvement via caching the data are loaded at mount time  updated during file-system operations  and discarded at dismount several types of structures may be included an in-memory volume contains information about each mounted an in-memory directory-structure cache holds the directory information of recently accessed directories  for directories at which volumes are mounted  it can contain a pointer to the volume table  the contains a copy of the fcb of each open file  as well as other information 11.2 465 file dates  create  access  write  file owner group  acl file data blocks or pointers to file data blocks figure 11.2 a typical file-control block the contains a pointer to the appropriate entry in the system-wide open-file table  as well as other information buffers hold file-system blocks when they are being read from disk or written to disk to create a new file  an application program calls the logical file system the logical file system knows the format of the directory structures to create a new file  it allocates a new fcb  alternatively  if the file-system implementation creates all fcbs at file-system creation time  an fcb is allocated from the set of free fcbs  the system then reads the appropriate directory into memory  updates it with the new file name and fcb  and writes it back to the disk a typical fcb is shown in figure 11.2 some operating systems  including unix  treat a directory exactly the same as a file-one with a type field indicating that it is a directory other operating systems  includii g windows nt  implement separate system calls for files and directories and treat directories as entities separate from files whatever the larger structural issues  the logical file system can call the file-organization module to map the directory i/0 into disk-block numbers  which are passed on to the basic file system and i/o control system now that a file has been created  it can be used for i/0 first  though  it must be opened the open   call passes a file name to the logical file system the open   system call first searches the system-wide open-file table to see if the file is already in use by another process if it is  a per-process open-file table entry is created pointing to the existing system-wide open-file table this algorithm can save substantial overhead if the file is not already open  the directory structure is searched for the given file name parts of the directory structure are usually cached in memory to speed directory operations once the file is found  the fcb is copied into a system-wide open-file table in memory this table not only stores the fcb but also tracks the number of processes that have the file open next  an entry is made in the per-process open-file table  with a pointer to the entry in the system-wide open-file table and some other fields these other fields may include a pointer to the current location in the file  for the next read   or write   operation  and the access mode in which the file is open the open   call returns a pointer to the appropriate entry in the per-process 466 chapter 11 user space user space kernel memory  a  kernel memory  b  ,  ---..,...  + -t-ilej d do secondary storage secondary storage figure 11.3 in-memory file-system structures  a  file open  b  file read file-system table all file operations are then performed via this pointer the file name may not be part of the open-file table  as the system has no use for it once the appropriate fcb is located on disk it could be cached  though  to save time on subsequent opens of the same file the name given to the entry varies unix systems refer to it as a windows refers to it as a when a process closes the file  the per-process table entry is removed  and the system-wide entry 's open count is decremented when all users that have opened the file close it  any updated metadata is copied back to the disk-based directory structure  and the system-wide open-file table entry is removed some systems complicate this scheme further by using the file system as an interface to other system aspects  such as networking for example  in ufs  the system-wide open-file table holds the inodes and other information for files and directories it also holds similar information for network connections and devices in this way  one mechanism can be used for multiple purposes the caching aspects of file-system structures should not be overlooked most systems keep all information about an open file  except for its actual data blocks  in memory the bsd unix system is typical in its use of caches wherever disk i/0 can be saved its average cache hit rate of 85 percent shows that these techniques are well worth implementing the bsd unix system is described fully in appendix a the operating structures of a file-system implementation are summarized in figure 11.3 11.2 467 11.2.2 partitions and mounting the layout of a disk can have many variations  depending on the operating system a disk can be sliced into multiple partitions  or a volume can span multiple partitions on multiple disks the former layout is discussed here  while the latter  which is more appropriately considered a form of raid  is covered in section 12.7 each partition can be either raw  containing no file system  or cooked  containing a file system is used where no file system is appropriate unix swap space can use a raw partition  for example  as it uses its own format on disk and does not use a file system likewise  some databases use raw disk and format the data to suit their needs raw disk can also hold information needed by disk raid systems  such as bit maps indicating which blocks are mirrored and which have changed and need to be mirrored similarly  raw disk can contain a miniature database holding raid configuration information  such as which disks are members of each raid set raw disk use is further discussed in section 12.5.1 boot information can be stored in a separate partition again  it has its own format  because at boot time the system does not have the file-system code loaded and therefore can not interpret the file-system format rather  boot information is usually a sequential series of blocks  loaded as an image into memory execution of the image starts at a predefined location  such as the first byte this in turn knows enough about the file-system structure to be able to find and load the kernel and start it executing it can contain more than the instructions for how to boot a specific operating system for instance  pcs and other systems can be multiple operating systems can be installed on such a system how does the system know which one to boot a boot loader that understands multiple file systems and multiple operating systems can occupy the boot space once loaded  it can boot one of the operating systems available on the disk the disk can have multiple partitions  each containing a different type of file system and a different operating system the which contains the operating-system kernel and sometimes other system files  is mounted at boot time other volumes can be automatically mounted at boot or manually mounted later  depending on the operating system as part of a successful mount operation  the operating system verifies that the device contains a valid file system it does so by asking the device driver to read the device directory and verifying that the directory has the expected format if the format is invalid  the partition must have its consistency checked and possibly corrected  either with or without user intervention finally  the operating system notes in its in-memory mount table that a file system is mounted  along with the type of the file system the details of this function depend on the operating system microsoft windows-based systems mount each volume in a separate name space  denoted by a letter and a colon to record that a file system is mounted at f   for example  the operating system places a pointer to the file system in a field of the device structure corresponding to f   when a process specifies the driver letter  the operating system finds the appropriate file-system pointer and traverses the directory structures on that device to find the specified file or directory later versions of windows can mount a file system at any point within the existing directory structure 468 chapter 11 on unix  file systems can be mounted at any directory mounting is implemented by setting a flag in the in-memory copy of the inode for that directory the flag indicates that the directory is a mount point a field then points to an entry in the mount table  indicating which device is mounted there the mount table entry contains a pointer to the superblock of the file system on that device this scheme enables the operating system to traverse its directory structure  switching seamlessly among file systems of varying types 11.2.3 virtual file systems the previous section m.akes it clear that modern operating systems must concurrently support multiple types of file systems but how does an operating system allow multiple types of file systems to be integrated into a directory structure and how can users seamlessly move between file-system types as they navigate the file-system space we now discuss some of these implementation details an obvious but suboptimal method of implementing multiple types of file systems is to write directory and file routines for each type instead  however  most operating systems  including unix  use object-oriented techniques to simplify  organize  and modularize the implementation the use of these methods allows very dissimilar file-system types to be implemented within the same structure  including network file systems  such as nfs users can access files that are contained within multiple file systems on the local disk or even on file systems available across the network data structures and procedures are used to isolate the basic systemcall functionality from the implementation details thus  the file-system implementation consists of three major layers  as depicted schematically in figure 11.4 the first layer is the file-system interface  based on the open    read    write    and close   calls and on file descriptors the second layer is called the layer the vfs layer serves two important functions  it separates file-system-generic operations from their implementation by defining a clean vfs interface several implementations for the vfs interface may coexist on the same machine  allowing transparent access to different types of file systems mounted locally it provides a mechanism for uniquely representing a file throughout a network the vfs is based on a file-representation structure  called a that contains a numerical designator for a network-wide unique file  unix inodes are unique within only a single file system  this network-wide uniqueness is required for support of network file systems the kernel maintains one vnode structure for each active node  file or directory   thus  the vfs distinguishes local files from remote ones  and local files are further distinguished according to their file-system types the vfs activates file-system-specific operations to handle local requests according to their file-system types and calls the nfs protocol procedures for remote requests file handles are constructed from the relevant vnodes and are passed as arguments to these procedures the layer implementing the 11.2 469 network figure 11.4 schematic view of a virtual file system file-system type or the remote-file-system protocol is the third layer of the architecture let 's briefly examine the vfs architecture in linux the four main object types defined by the linux vfs are  the inode object  which represents an individual file the file object  which represents an open file the superblock object  which represents an entire file system the dentry object  which represents an individual directory entry for each of these four object types  the vfs defines a set of operations that must be implemented every object of one of these types contains a pointer to a f1.mction table the function table lists the addresses of the actual functions that implement the defined operations for that particular object for example  an abbreviated api for some of the operations for the file object include  int open      -open a file ssize_t read      -read from a file ssize_t write      -write to a file int mmap    -memory-map a file an implementation of the file object for a specific file type is required to implement each function specified in the definition of the file object  the complete definition ofthe file object is specified in the struct f ile_operat ions  which is located in the file /usr/include/linux/fs .h  470 chapter 11 11.3 thus  the vfs software layer can perform an operation on one of these objects by calling the appropriate function from the object 's function table  without having to know in advance exactly what kind of object it is dealing with the vfs does not know  or care  whether an inode represents a disk file  a directory file  or a remote file the appropriate function for that file 's read   operation will always be at the same place in its function table  and the vfs software layer will call that function without caring how the data are actually read the selection of directory-allocation and directory-management algorithms significantly affects the efficiency  performance  and reliability of the file system in this section  we discuss the trade-offs involved in choosing one of these algorithms 11.3.1 linear list the simplest method of implementing a directory is to use a linear list of file names with pointers to the data blocks this method is simple to program but time-consuming to execute to create a new file  we must first search the directory to be sure that no existing file has the same name then  we add a new entry at the end of the directory to delete a file  we search the directory for the named file and then release the space allocated to it to reuse the directory entry  we can do one of several things we can mark the entry as unused  by assigning it a special name  such as an all-blank name  or with a used -unused bit in each entry   or we can attach it to a list of free directory entries a third alternative is to copy the last entry in the directory into the freed location and to decrease the length of the directory a linked list can also be used to decrease the time required to delete a file the real disadvantage of a linear list of directory entries is that finding a file requires a linear search directory information is used frequently  and users will notice if access to it is slow in fact  many operating systems implement a software cache to store the most recently used directory information a cache hit avoids the need to constantly reread the information from disk a sorted list allows a binary search and decreases the average search time however  the requirement that the list be kept sorted may complicate creating and deleting files  since we may have to move substantial amounts of directory information to maintain a sorted directory a more sophisticated tree data structure  such as a b-h ee  might help here an advantage of the sorted list is that a sorted directory listing can be produced without a separate sort step 11.3.2 hash table another data structure used for a file directory is a with this method  a linear list stores the directory entries  but a hash data structure is also used the hash table takes a value computed from the file name and returns a pointer to the file name in the linear list therefore  it can greatly decrease the directory search time insertion and deletion are also fairly straightforward  although some provision must be made for collisions-situations in which two file names hash to the same location 11.4 11.4 471 the major difficulties with a hash table are its generally fixed size and the dependence of the hash function on that size for example  assume that we make a linear-probing hash table that holds 64 entries the hash function converts file names into integers from 0 to 63  for instance  by using the remainder of a division by 64 if we later try to create a 65th file  we must enlarge the directory hash table-say  to 128 entries as a result  we need a new hash function that must map file narnes to the range 0 to 127  and we must reorganize the existing directory entries to reflect their new hash-function values alternatively  a chained-overflow hash table can be used each hash entry can be a linked list instead of an individual value  and we can resolve collisions by adding the new entry to the linked list lookups may be somewhat slowed  because searching for a name might require stepping through a linked list of colliding table entries still  this method is likely to be much faster than a linear search through the entire directory the direct-access nature of disks allows us flexibility in the implementation of files in almost every case  many files are stored on the same disk the main problem is how to allocate space to these files so that disk space is utilized effectively and files can be accessed quickly three major methods of allocating disk space are in wide use  contiguous  linked  and indexed each method has advantages and disadvantages some systems  such as data general 's rdos for its nova line of computers  support all three more commonly  a system uses one method for all files within a file-system type 11.4.1 contiguous allocation requires that each file occupy a set of contiguous blocks on disk disk addresses define a linear ordering on the disk with this ordering  assuming that only one job is accessil1.g the disk  accessing block b + 1 after block b normally requires no head movement when head movement is needed  from the last sector of one cylil1.der to the first sector of the next cylinder   the head need only move from one track to the next thus  the number of disk seeks required for accessing contiguously allocated files is minimal  as is seek time when a seek is finally needed the ibm vm/cms operatil1.g system uses contiguous allocation because it provides such good performance contiguous allocation of a file is defined by the disk address and length  in block units  of the first block if the file is n blocks long and starts at location b  then it occupies blocks b  b + 1  b + 2    b + n  1 the directory entry for each file indicates the address of the starting block and the length of the area allocated for this file  figure 11.5   accessing a file that has been allocated contiguously is easy for sequential access  the file system remembers the disk address of the last block referenced and  when necessary  reads the next block for direct access to block i of a file that starts at block b  we can immediately access block b + i thus  both sequential and direct access can be supported by contiguous allocation 472 chapter 11 directory file start length count 0 2 tr 14 3 mail 19 6 list 28 4 f 6 2 figure 1 i .5 contiguous allocation of disk space contiguous allocation has some problems  however one difficulty is finding space for a new file the system chosen to manage free space determines how this task is accomplished ; these management systems are discussed in section 11.5 any management system can be used  but some are slower than others the contiguous-allocation problem can be seen as a particular application of the general problem discussed in section 8.3  which involves to satisfy a request of size n from a list of free holes first fit and best fit are the most common strategies used to select a free hole from the set of available holes simulations have shown that both first fit and best fit are more efficient than worst fit in terms of both time and storage utilization neither first fit nor best fit is clearly best in terms of storage utilization  but first fit is generally faster all these algorithms suffer from the problem of as files are allocated and deleted  the free disk space is broken into pieces external fragmentation exists whenever free space is broken into chunks it becomes a problem when the largest contiguous chunk is insufficient for a request ; storage is fragncented into a number of holes  none of which is large enough to store the data depending on the total amount of disk storage and the average file size  external fragmentation may be a minor or a major problem one strategy for preventing loss of significant amounts of disk space to external fragmentation is to copy an entire file system onto another disk or tape the original disk is then freed completely  creating one large contiguous free space we then copy the files back onto the original disk by allocating contiguous space from this one large hole this scheme effectively all free space into one contiguous space  solving the fragmentation however  the cost of this compaction is time and it can be particularly severe for large hard disks that use contiguous allocation  where compacting all the space 11.4 473 may take hours and may be necessary on a weekly basis some systems require that this function be done with the file system unmounted during this normal system operation generally can not be permitted  so such compaction is avoided at all costs on production machines most modern systems that need defragmentation can perform it during normal system operations  but the performance penalty can be substantial another problem with contiguous allocation is determining how much space is needed for a file when the file is created  the total amount of space it will need must be found and allocated how does the creator  program or person  know the size of the file to be created in some cases  this detennination may be fairly simple  copying an existing file  for example  ; in general  however  the size of an output file may be difficult to estimate if we allocate too little space to a file  we may find that the file can not be extended especially with a best-fit allocation strategy  the space on both sides of the file may be in use hence  we can not make the file larger in place two possibilities then exist first  the user program can be terminated  with an appropriate error message the user must then allocate more space and run the program again these repeated runs may be costly to prevent them  the user will normally overestimate the amount of space needed  resulting in considerable wasted space the other possibility is to find a larger hole  copy the contents of the file to the new space  and release the previous space this series of actions can be repeated as long as space exists  although it can be time consuming however  the user need never be informed explicitly about what is happening ; the system continues despite the problem  although more and more slowly even if the total amount of space needed for a file is known in advance  preallocation may be inefficient a file that will grow slowly over a long period  months or years  must be allocated enough space for its final size  even though much of that space will be unused for a long time the file therefore has a large amount of internal fragmentation to minimize these drawbacks  some operating systems use a modified contiguous-allocation scheme here  a contiguous chunk of space is allocated initially ; then  if that amount proves not to be large enough  another chunk of contiguous space  known as an is added the location of a file 's blocks is then recorded as a location and a block count  plus a link to the first block of the next extent on some systems  the owner of the file can set the extent size  but this setting results in inefficiencies if the owner is incorrect internal fragm.entation can still be a problem if the extents are too large  and external fragmentation can become a problem as extents of varying sizes are allocated and deallocated the commercial veritas file system uses extents to optimize performance it is a high-performance replacement for the standard unix ufs 11.4.2 linked allocation solves all problems of contiguous allocation with linked allocation  each file is a linked list of disk blocks ; the disk blocks may be scattered anywhere on the disk the directory contains a pointer to the first and last blocks of the file for example  a file of five blocks might start at block 9 and continue at block 16  then block 1  then block 10  and finally block 25  figure 11.6   each block contains a pointer to the next block these pointers 474 chapter 11 directory 12 16 170180190 20021 ~ _20_ ~ 23_0-4 ~ 2402sc.51  260270 280290300310 figure i 1.6 linked allocation of disk space are not made available to the user thus  if each block is 512 bytes in size  and a disk address  the poileter  requires 4 bytes  then the user sees blocks of 508 bytes to create a new file  we simply create a new entry ile the directory with linked allocation  each directory entry has a pointer to the first disk block of the file this pointer is initialized to nil  the end-of-list pointer value  to signify an empty file the size field is also set to 0 a write to the file causes the free-space management system to filed a free block  and this new block is written to and is linked to the end of the file to read a file  we simply read blocks by following the pointers from block to block there is no external fragmentation with linked allocation  and any free block on the free-space list can be used to satisfy a request the size of a file need not be declared when that file is created a file can continue to grow as long as free blocks are available consequently  it is never necessary to compact disk space linked allocation does have disadvantages  however the major problem is that it can be used effectively only for sequential-access files to filed the ith block of a file  we must start at the begirueing of that file and follow the pointers rnetil we get to the ith block each access to a pointer requires a disk read  and some require a disk seek consequently  it is inefficient to support a direct-access capability for linked-allocation files another disadvantage is the space required for the pointers if a pointer requires 4 bytes out of a 512-byte block  then 0.78 percent of the disk is being used for pointers  rather than for information each file requires slightly more space than it would otherwise the usual solution to this problem is to collect blocks into multiples  called and to allocate clusters rather than blocks for instance  the file system may define a cluster as four blocks and operate on the disk only in cluster units pointers then use a much smaller percentage of the file 's disk space this method allows the logical-to-physical block mapping to remain simple 11.4 475 but improves disk throughput  because fewer disk-head seeks are required  and decreases the space needed for block allocation and free-list management the cost of this approach is an increase in internal fragmentation  because more space is wasted when a cluster is partially full than when a block is partially full clusters can be used to improve the disk-access time for many other algorithms as welt so they are used in most file systems yet another problem of linked allocation is reliability recall that the files are linked together by pointers scattered all over the disk  and consider what would happen if a pointer were lost or damaged a bug in the operating-system software or a disk hardware failure might result in picking up the wrong pointer this error could in turn result in linking into the free-space list or into another file one partial solution is to use doubly linked lists  and another is to store the file name and relative block number in each block ; however  these schemes require even more overhead for each file an important variation on linked allocation is the use of a  fat !  this simple but efficient method of disk-space allocation is used by the ms-dos and os/2 operating systems a section of disk at the beginning of each volume is set aside to contain the table the table has one entry for each disk block and is indexed by block number the fat is used in much the same way as a linked list the directory entry contains the block number of the first block of the file the table entry indexed by that block number contains the block number of the next block in the file this chain continues until it reaches the last block  which has a special end-of-file value as the table entry an unused block is indicated by a table value of 0 allocating a new block to a file is a simple matter of finding the first 0-valued table entry and replacing the previous end-of-file value with the address of the new block the 0 is then replaced with the end-of-file value an illustrative example is the fat structure shown in figure 11.7 for a file consisting of disk blocks 217  618  and 339 directory entry name start block 0 217 618 339  618 339 number of disk blocks -1 fat figure 11.7 file-allocation table 476 chapter 11 the fat allocation scheme can result in a significant number of disk head seeks  unless the fat is cached the disk head must move to the start of the volume to read the fat and find the location of the block in question  then move to the location of the block itself in the worst case  both moves occur for each of the blocks a benefit is that random-access time is improved  because the disk head can find the location of any block by reading the information in the fat 11.4.3 indexed allocation linked allocation solves the external-fragmentation and size-declaration problems of contiguous allocation however  in the absence of a fat  linked allocation can not support efficient direct access  since the pointers to the blocks are scattered with the blocks themselves all over the disk and must be retrieved in order solves this problem by bringil1.g all the pointers together into one location  the blo ; ct   each file has its own index block  which is an array of disk-block addresses the i th entry in the index block points to the i 111 block of the file the directory contains the address of the index block  figure 11.8   to find and read the i 1jz block  we use the pointer in the i 1lz index-block entry this scheme is similar to the paging scheme described il1 section 8.4 when the file is created  all pointers in the index block are set to nil when the ith block is first written  a block is obtained from the free-space manage1 ~ and its address is put in the ith index-block entry indexed allocation supports direct access  without suffering from external fragmentation  because any free block on the disk can satisfy a request for more space indexed allocation does suffer from wasted space  however the pointer overhead of the index block is generally greater than the pointer overhead of linked allocation consider a common case in which we have a file of only one or two blocks with linked allocation  we lose the space of only one pointer per directory file jeep 16 figure 11.8 indexed allocation of disk space 11.4 allocation methods 477 block with indexed allocation  an entire index block must be allocated  even if only one or two pointers will be non-nil this point raises the question of how large the index block should be every file must have an index block  so we want the index block to be as small as possible if the index block is too small  however  it will not be able to hold enough pointers for a large file  and a mechanism will have to be available to deal with this issue mechanisms for this purpose include the following  c linked scheme an index block is normally one disk block thus  it can be read and written directly by itself to allow for large files  we can link together several index blocks for example  an index block might contain a small header giving the name of the file and a set of the first 100 disk-block addresses the next address  the last word in the index block  is nil  for a small file  or is a pointer to another index block  for a large file   multilevel index a variant of linked representation uses a first-level index block to point to a set of second-level index blocks  which in tum point to the file blocks to access a block  the operating system uses the first-level index to find a second-level index block and then uses that block to find the desired data block this approach could be continued to a third or fourth level  depending on the desired maximum file size with 4,096-byte blocks  we could store 1,024 four-byte pointers in an index block two levels of indexes allow 1,048,576 data blocks and a file size of up to 4gb combined scheme another alternative  used in the ufs  is to keep the first  say  15 pointers of the index block in the file 's inode the first 12 of these pointers point to direct blocks ; that is  they contain addresses of blocks that contain data of the file thus  the data for small files  of no more than 12 blocks  do not need a separate index block if the block size is 4 kb  then up to 48 kb of data can be accessed directly the next three pointers point to indirect blocks the first points to a single indirect block  which is an index block containing not data but the addresses of blocks that do contain data the second points to a double indirect block  which contains the address of a block that contains the addresses of blocks that contain pointers to the actual data blocks the last pointer contains the address of a triple indirect block under this method  the number of blocks that can be allocated to a file exceeds the amount of space addressable by the four-byte file pointers used by many operating systems a 32-bit file pointer reaches only 232 bytes  or 4gb many unix implementations  including solaris and ibm 's aix  now support up to 64-bit file pointers pointers of this size allow files and file systems to be terabytes in size a unix inode is shown in figure 11.9 indexed-allocation schemes suffer from some of the same performance problems as does linked allocation specifically  the index blocks can be cached in memory  but the data blocks may be spread all over a volume 11.4.4 performance the allocation methods that we have discussed vary in their storage efficiency and data-block access times both are important criteria in selecting the proper method or methods for an operating system to implement 478 chapter 11 implementing file systems figure 11.9 the unix inode before selecting an allocation method  we need to determine how the systems will be used a system with mostly sequential access should not use the same method as a system with mostly random access for any type of access  contiguous allocation requires only one access to get a disk block since we can easily keep the initial address of the file in memory  we can calculate immediately the disk address of the ith block  or the next block  and read it directly for linked allocation  we can also keep the address of the next block in memory and read it directly this method is fine for sequential access ; for direct access  however  an access to the ith block might require i disk reads this problem indicates why linked allocation should not be used for an application requiring direct access as a result  some systems support direct-access files by using contiguous allocation and sequential-access files by using linked allocation for these systems  the type of access to be made must be declared when the file is created a file created for sequential access will be linked and can not be used for direct access a file created for direct access will be contiguous and can support both direct access and sequential access  but its maximum length must be declared when it is created in this case  the operating system must have appropriate data structures and algorithms to support both allocation methods files can be converted from one type to another by the creation of a new file of the desired type  into which the contents of the old file are copied the old file may then be deleted and the new file renamed indexed allocation is more complex if the index block is already in memory  then the access can be made directly however  keeping the index block in memory requires considerable space if this memory space is not available  then we may have to read first the index block and then the desired data block for a two-level index  two index-block reads might be necessary for an 11.5 11.5 479 extremely large file  accessing a block near the end of the file would require reading in all the index blocks before the needed data block finally could be read thus  the performance of indexed allocation depends on the index structure  on the size of the file  and on the position of the block desired some systems combine contiguous allocation with indexed allocation by using contiguous allocation for small files  up to three or four blocks  and automatically switching to an indexed allocation if the file grows large since most files are small  and contiguous allocation is efficient for small files  average performance can be quite good for instance  the version of the unix operating system from sun microsystems was changed in 1991 to improve performance in the file-system allocation algorithm the performance measurements indicated that the maximum disk throughput on a typical workstation  a 12-mips sparcstation1  took 50 percent of the cpu and produced a disk bandwidth of only 1.5 me per second to improve performance  sun made changes to allocate space in clusters of 56 kb whenever possible  56 kb was the maximum size of a dma transfer on sun systems at that time   this allocation reduced external fragmentation  and thus seek and latency times in addition  the disk-reading routines were optimized to read in these large clusters the inode structure was left unchanged as a result of these changes  plus the use of read-ahead and free-behind  discussed in section 11.6.2   25 percent less cpu was used  and throughput substantially improved many other optimizations are in use given the disparity between cpu speed and disk speed  it is not unreasonable to add thousands of extra instructions to the operating system to save just a few disk-head movements furthermore  this disparity is increasing over time  to the point where hundreds of thousands of instructions reasonably could be used to optimize head movements since disk space is limited  we need to reuse the space from deleted files for new files  if possible  write-once optical disks only allow one write to any given sector  and thus such reuse is not physically possible  to keep track of free disk space  the system maintains a the free-space list records all free disk blocks-those not allocated to some file or directory to create a file  we search the free-space list for the required amount of space and allocate that space to the new file this space is then removed from the free-space list when a file is deleted  its disk space is added to the free-space list the free-space list  despite its name  might not be implemented as a list  as we discuss next 11.5.1 bit vector frequently  the free-space list is implemented as a or each block is represented by 1 bit if the block is free  the bit is 1 ; if the block is allocated  the bit is 0 for example  consider a disk where blocks 2  3  4  5  8  9  10  11  12  13  17  18  25  26  and 27 are free and the rest of the blocks are allocated the free-space bit map would be 480 chapter 11 001111001111110001100000011100000  the main advantage of this approach is its relative simplicity and its efficiency in finding the first free block or n consecutive free blocks on the disk indeed  many computers supply bit-manipulation instructions that can be used effectively for that purpose for example  the intel family starting with the 80386 and the motorola family starting with the 68020 have instructions that return the offset in a word of the first bit with the value 1  these processors have powered pcs and macintosh systems  respectively   one technique for finding the first free block on a system that uses a bit-vector to allocate disk space is to sequentially check each word in the bit map to see whether that value is not 0  since a 0-valued word contains only 0 bits and represents a set of allocated blocks the first non-0 word is scanned for the first 1 bit  which is the location of the first free block the calculation of the block number is  number of bits per word  x  number of 0-value words  + offset of first 1 bit again  we see hardware features driving software functionality unfortunately  bit vectors are inefficient unless the entire vector is kept in main memory  and is written to disk occasionally for recovery needs   keeping it in main memory is possible for smaller disks but not necessarily for larger ones a 1.3-gb disk with 512-byte blocks would need a bit map of over 332 kb to track its free blocks  although clustering the blocks in groups of four reduces this number to around 83 kb per disk a 1-tb disk with 4-kb blocks requires 32 mb to store its bit map given that disk size constantly increases  the problem with bit vectors will continue to escalate a 1-pb file system would take a 32-gb bitmap just to manage its free space 11.5.2 linked list another approach to free-space management is to link together all the free disk blocks  keeping a pointer to the first free block in a special location on the disk and caching it in memory this first block contains a pointer to the next free disk block  and so on recall our earlier example  section 11.5.1   in which blocks 2  3  4  5  8  9  10  11  12  13  17  18  25  26  and 27 were free and the rest of the blocks were allocated in this situation  we would keep a pointer to block 2 as the first free block block 2 would contain a pointer to block 3  which would point to block 4  which would point to block 5  which would point to block 8  and so on  figure 11.10   this scheme is not efficient ; to traverse the list  we must read each block  which requires substantial i/0 time fortunately  however  traversing the free list is not a frequent action usually  the operating system simply needs a free block so that it can allocate that block to a file  so the first block in the free list is used the fat method incorporates free-block accounting into the allocation data structure no separate method is needed 11.5.3 grouping a modification of the free-list approach stores the addresses of n free blocks in the first free block the first n-1 of these blocks are actually free the last block contains the addresses of another n free blocks  and so on the addresses 11.5 481 figure 11.10 linked free-space list on disk of a large number of free blocks can now be found quickly  unlike the situation when the standard linked-list approach is used 11.5.4 counting another approach takes advantage of the fact that  generally  several contiguous blocks may be allocated or freed simultaneously  particularly when space is allocated with the contiguous-allocation algorithm or through clustering thus  rather than keeping a list of n free disk addresses  we can keep the address of the first free block and the number  n  of free contiguous blocks that follow the first block each entry in the free-space list then consists of a disk address and a count although each entry requires more space than would a simple disk address  the overall list is shorter  as long as the count is generally greater than 1 note that this method of tracking free space is similar to the extent method of allocating blocks these entries can be stored in a b-tree  rather than a linked list for efficient lookup  insertion  and deletion 11.5.5 space maps sun 's zfs file system was designed to encompass huge numbers of files  directories  and even file systems  in zfs  we can create file-system hierarchies   the resulting data structures could have been large and inefficient if they had not been designed and implemented properly on these scales  metadata i/0 can have a large performance impact conside1 ~ for example  that if the freespace list is implemented as a bit map  bit maps must be modified both when blocks are allocated and when they are freed freeing 1gb of data on a 1-tb disk could cause thousands of blocks of bit maps to be updated  because those data blocks could be scattered over the entire disk 482 chapter 11 11.6 zfs uses a combination of techniques in its free-space managem.ent algorithm to control the size of data structures and minimize the i/0 needed to manage those structures first  zfs creates to divide the space on the device into chucks of manageable size a given volume may contain hundreds of metaslabs each metaslab has an associated space map zfs uses the counting algorithm to store information about free blocks rather than write count structures to disk  it uses log-structured file system techniques to record them the space map is a log of all block activity  allocatil g and freemg   in time order  in countil g format when zfs decides to allocate or free space from a metaslab  it loads the associated space map into memory in a balanced-tree structure  for very efficient operation   indexed by offset  and replays the log into that structure the in-memory space map is then an accurate representation of the allocated and free space in the metaslab zfs also condenses the map as much as possible by combining contiguous free blocks into a sil gle entry finally  the free-space list is updated on disk as part of the transaction-oriented operations of zfs during the collection and sortmg phase  block requests can still occur  and zfs satisfies these requests from the log in essence  the log plus the balanced tree is the free list now that we have discussed various block-allocation and directorymanagement options  we can further consider their effect on performance and efficient disk use disks tend to represent a major bottleneck in system performance  since they are the slowest main computer component in this section  we discuss a variety of techniques used to improve the efficiency and performance of secondary storage 11.6.1 efficiency the efficient use of disk space depends heavily on the disk allocation and directory algorithms in use for instance  unix inodes are preallocated on a volume even an empty disk has a percentage of its space lost to inodes however  by preallocating the inodes and spreading them across the volume  we improve the file system 's performance this improved performance results from the unix allocation and free-space algorithms  which try to keep a file 's data blocks near that file 's inode block to reduce seek time as another example  let 's reconsider the clustermg scheme discussed in section 11.4  which aids in file-seek and file-transfer performance at the cost of internal fragmentation to reduce this fragmentation  bsd unix varies the cluster size as a file grows large clusters are used where they can be filled  and small clusters are used for small files and the last cluster of a file this system is described in appendix a the types of data normally kept in a file 's directory  or inode  entry also require consideration commonly  a last write date is recorded to supply information to the user and to determine whether the file needs to be backed up some systems also keep a last access date  so that a user can determine when the file was last read the result of keeping this information is that  whenever the file is read  a field in the directory structure must be written 11.6 483 to that means the block must be read into memory  a section changed  and the block written back out to disk  because operations on disks occur only in block  or cluster  chunks so any time a file is opened for reading  its directory entry must be read and written as well this requirement can be inefficient for frequently accessed files  so we must weigh its benefit against its performance cost when designing a file system generally  every data item associated with a file needs to be considered for its effect on efficiency and performance as an example  consider how efficiency is affected by the size of the pointers used to access data most systems use either 16 or 32-bit pointers throughout the operating system these pointer sizes limit the length of a file to either 216  64 kb  or 232 bytes  4 gb   some systems implement 64-bit pointers to increase this limit to 264 bytes  which is a very large number indeed however  64-bit pointers take more space to store and in turn make the allocation and free-space-management methods  linked lists  indexes  and so on  use more disk space one of the difficulties in choosing a pointer size  or indeed any fixed allocation size within an operating system  is planning for the effects of changing technology consider that the ibm pc xt had a 10-mb hard drive and an ms-dos file system that could support only 32 mb  each fat entry was 12 bits  pointing to an 8-kb cluster  as disk capacities increased  larger disks had to be split into 32-mb partitions  because the file system could not track blocks beyond 32mb as hard disks with capacities of over 100mb became common  the disk data structures and algorithms in ms-dos had to be modified to allow larger file systems  each fat entry was expanded to 16 bits and later to 32 bits  the initial file-system decisions were made for efficiency reasons ; however  with the advent of ms-dos version 4  millions of computer users were inconvenienced when they had to switch to the new  larger file system sun 's zfs file system uses 128-bit pointers  which theoretically should never need to be extended  the minimum mass of a device capable of storing 2128 bytes using atomic-level storage would be about 272 trillion kilograms  as another example  consider the evolution of sun 's solaris operating system originally  many data structures were of fixed length  allocated at system startup these structures included the process table and the open-file table when the process table became full  no more processes could be created when the file table became full  no more files could be opened the system would fail to provide services to users table sizes could be increased only by recompiling the kernel and rebooting the system since the release of solaris 2  almost all kernel structures have been allocated dynamically  eliminating these artificial limits on system performance of course  the algorithms that manipulate these tables are more complicated  and the operating system is a little slower because it must dynamically allocate and deallocate table entries ; but that price is the usual one for more general functionality 11.6.2 performance even after the basic file-system algorithms have been selected  we can still improve performance in several ways as will be discussed in chapter 13  most disk controllers include local memory to form an on-board cache that is large enough to store entire tracks at a time once a seek is performed  the track is read into the disk cache starting at the sector under the disk head 484 chapter 11 1/0 using read   and write   tile system figure 11.11 1/0 without a unified buffer cache  reducing latency time   the disk controller then transfers any sector requests to the operating system once blocks make it from the disk controller into main memory  the operating system may cache the blocks there some systems maintain a separate section of main memory for a where blocks are kept under the assumption that will be used again shortly other systems cache file data using a the page cache uses virtual memory techniques to cache file data as pages rather than as file-system-oriented blocks cachii lg file data using virtual addresses is far more efficient than caching through physical disk blocks  as accesses interface with virtual memory rather than the file system several systems-including solaris  linux  and windows nt  2000  and xp-use caching to cache both process pages and file data this is known as some versions of unix and linux provide a to illustrate the benefits of the unified buffer cache  consider the two alternatives for opening and accessing a file one approach is to use memory mapping  section 9.7  ; the second is to use the standard system calls read   and write    without a unified buffer cache  we have a situation similar to figure 11.11 here  the read   and write   system calls go through the buffer cache the memory-mapping call  however  requires using two caches-the page cache and the buffer cache a memory mapping proceeds by reading in disk blocks from the file system and storing them in the buffer cache because the virtual memory system does not interface with the buffer cache  the contents of the file in the buffer cache must be copied into the page cache this situation is known as and requires caching file-system data twice not only does it waste memory but it also wastes significant cpu and i/o cycles due to the extra data movement within system memory in addition  inconsistencies between the two caches can result in corrupt files in contrast  when a unified buffer cache is provided  both memory mapping and the read   and write   system calls use the same page cache this has the benefit of a voiding double 11.6 485 memory-mapped 1/0 buffer cache file system figure 11.12 1/0 using a unified buffer cache caching  and it allows the virtual memory system to manage file-system data the unified buffer cache is shown in figure 11.12 regardless of whether we are caching disk blocks or pages  or both   lru  section 9.4.4  seems a reasonable general-purpose algorithm for block or page replacement however  the evolution of the solaris page-caching algorithms reveals the difficulty in choosil1.g an algorithm solaris allows processes and the page cache to share unused memory versions earlier than solaris 2.5.1 made no distmction between allocatmg pages to a process and allocating them to the page cache as a result  a system performing many i/0 operations used most of the available memory for caching pages because of the high rates of i/0  the page scanner  section 9.10.2  reclaimed pages from processesrather than from the page cache-when free memory ran low solaris 2.6 and solaris 7 optionally implemented priority paging  in which the page scanner gives priority to process pages over the page cache solaris 8 applied a fixed limit to process pages and the file-system page cache  preventing either from forcing the other out of memory solaris 9 and 10 again changed the algorithms to maximize memory use and mmimize thrashing another issue that can affect the performance of i/0 is whether writes to the file system occur synchronously or asynchronously occur in the order in which the disk subsystem receives and the writes are not buffered thus  the calling routine must wait for the data to reach the disk drive before it can proceed in an the data are stored in the cache  and control returns to the caller asynchronous writes are done the majority of the time however  metadata writes  among others  can be synchronous operating systems frequently include a flag in the open system call to allow a process to request that writes be performed synchxonously for example  databases use this feature for atomic transactions  to assure that data reach stable storage in the required order some systems optimize their page cache by using different replacement algorithms  depending on the access type of the file a file being read or written sequentially should not have its pages replaced in lru order  because the most recently used page will be used last  or perhaps never again instead  sequential access can be optimized by techniques known as free-behind and read-ahead removes a page from the buffer as soon as the next 486 chapter 11 11.7 page is requested the previous are not likely to be used again and waste buffer space with a requested page and several subsequent pages are read and cached these pages are likely to be requested after the current page is processed retrieving these data from the disk in one transfer and caching them saves a considerable ancount of time one might think that a track cache on the controller would elincinate the need for read-ahead on a multiprogrammed system however  because of the high latency and overhead involved in making many small transfers from the track cache to main memory  performing a read-ahead remains beneficial the page cache  the file system  and the disk drivers have some interesting interactions when data are written to a disk file  the pages are buffered in the cache  and the disk driver sorts its output queue according to disk address these two actions allow the disk driver to minimize disk-head seeks and to write data at times optimized for disk rotation unless synchronous writes are required  a process writing to disk simply writes into the cache  and the system asynchronously writes the data to disk when convenient the user process sees very fast writes when data are read from a disk file  the block i/0 system does some read-ahead ; however  writes are much more nearly asynchronous than are reads thus  output to the disk through the file system is often faster than is input for large transfers  counter to intuition files and directories are kept both in main memory and on disk  and care must be taken to ensure that a system failure does not result in loss of data or in data inconsistency we deal with these issues in this section as well as how a system can recover from such a failure a system crash can cause inconsistencies among on-disk file-system data structures  such as directory structures  free-block pointers  and free fcb pointers many file systems apply changes to these structures in place a typical operation  such as creating a file  can involve many structural changes within the file system on the disk directory structures are modified  fcbs are allocated  data blocks are allocated  and the free counts for all of these blocks are decreased these changes can be interrupted by a crash  and inconsistencies among the structures can result for example  the free fcb count might indicate that an fcb had been allocated  but the directory structure might not point to the fcb compounding this problem is the caching that operating systems do to optimize i/0 performance some changes may go directly to disk  while others may be cached if the cached changes do not reach disk before a crash occurs  more corruption is possible in addition to crashes  bugs in file-system implementation  disk controllers  and even user applications can corrupt a file system file systems have varying methods to deal with corruption  depending on the file-system data structures and algorithms we deal with these issues next 11.7.1 consistency checking whatever the cause of corruption  a file system must first detect the problems and then correct them for detection  a scan of all the metadata on each file 11.7 487 system can confirm or deny the consistency of the systenl unfortunately  this scan can take minutes or hours and should occur every time the system boots alternatively  a file system can record its state within the file-system metadata at the start of any metadata change  a status bit is set to indicate that the metadata is in flux if all updates to the metadata complete successfully  the file system can clear that bit it however  the status bit remains set  a consistency checker is run the systems program such as f s ck in unix or chkdsk in windows-compares the data in the directory structure with the data blocks on disk and tries to fix any inconsistencies it finds the allocation and free-space-management algorithms dictate what types of problems the checker can find and how successful it will be in fixing them for instance  if linked allocation is used and there is a link from any block to its next block  then the entire file can be reconstructed from the data blocks  and the directory structure can be recreated in contrast the loss of a directory entry on an indexed allocation system can be disastrous  because the data blocks have no knowledge of one another for this reason  unix caches directory entries for reads ; but any write that results in space allocation  or other metadata changes  is done synchronously  before the corresponding data blocks are written of course  problems can still occur if a synchronous write is interrupted by a crash 11.7.2 log-structured file systems computer scientists often fin.d that algorithms and technologies origil1.ally used in one area are equally useful in other areas such is the case with the database log-based recovery algorithms described in section 6.9.2 these logging algorithms have been applied successfully to the of consistency '-.il'c' .ll the resulting implementations are known as  or file systems note that with the consistency-checking approach discussed in the preceding section  we essentially allow structures to break and repair them on recovery however  there are several problems with this approach one is that the inconsistency may be irreparable the consistency check may not be able to recover the structures  resulting in loss of files and even entire directories consistency checking can require human intervention to resolve conflicts  and that is inconvenient if no human is available the system can remain unavailable until the human tells it how to proceed consistency checking also takes system and clock time to check terabytes of data  hours of clock time may be required the solution to this problem is to apply log-based recovery techniques to file-system metadata updates both ntfs and the veritas file system use this method  and it is included in recent versions of ufs on solaris in fact it is becoming common on many operating systems fundamentally  all metadata changes are written each set of operations for performing a specific task is a the changes are written to this log  they are considered to be committed  and the system call can return to the user process  allowing it to continue execution meanwhile  these log entries are replayed across the actual filesystem structures as the changes are made  a pointer is updated to indicate 488 chapter 11 which actions have completed and which are still incomplete when an entire committed transaction is completed  it is removed from the log file  which is actually a circular buffer a cb  ul ; n writes to the end of its space and then continues at the beginning  overwriting older values as it goes we would not want the buffer to write over data that had not yet been saved  so that scenario is avoided the log may be in a separate section of the file system or even on a separate disk spindle it is more efficient  but more complex  to have it under separate read and write heads  thereby decreasing head contention and seek times if the system crashes  the log file will contain zero or more transactions any transactions it contains were not completed to the file system  even though they were committed by the operating system  so they must now be completed the transactions can be executed from the pointer until the work is complete so that the file-system structures remain consistent the only problem occurs when a transaction was aborted -that is  was not committed before the system crashed any changes from such a transaction that were applied to the file system must be undone  again preserving the consistency of the file system this recovery is all that is needed after a crash  elimil ating any problems with consistency checking a side benefit of using logging on disk metadata updates is that those updates proceed much faster than when they are applied directly to the on-disk data structures the reason for this improvement is found in the performance advantage of sequential i/0 over random i/0 the costly synchronous random meta data writes are turned into much less costly synchronous sequential writes to the log-structured file system 's loggil g area those changes in turn are replayed asynchronously via random writes to the appropriate structures the overall result is a significant gain in performance of metadata-oriented operations  such as file creation and deletion 11.7.3 other solutions another alternative to consistency checking is employed by network appliance 's wafl file system and sun 's zfs file system these systems never overwrite blocks with new data rather  a transaction writes all data and metadata changes to new blocks when the transaction is complete  the metadata structures that pointed to the old versions of these blocks are updated to point to the new blocks the file system can then remove the old pointers and the old blocks and make them available for reuse if the old pointers and blocks are kept  a is created ; the snapshot is a view of the file system before the last update took place this solution should require no consistency checking if the pointer update is done atomically wafl does have a consistency checke1 ~ however  so some failure scenarios can still cause metadata corruption  see 11.9 for details of the wafl file system  sun 's zfs takes an even more im ovative approach to disk consistency it never overwrites blocks  just as is the case with wafl however  zfs goes further and provides check-summing of all metadata and data blocks this solution  when combined with raid  assures that data are always correct zfs therefore has no consistency checker  more details on zfs are found in section 12.7.6  11.7 489 11.7.4 backup and restore magnetic disks sometimes fail  and care must be taken to ensure that the data lost in such a failure are not lost forever to this end  system programs can be used to data from disk to another storage device  such as a floppy disk  magnetic tape  optical disk  or other hard disk recovery from the loss of an individual file  or of an entire disk  may then be a matter of the data from backup to minimize the copying needed  we can use information from each file 's directory entry for instance  if the backup program knows when the last backup of a file was done  and the file 's last write date in the directory indicates that the file has not changed since that date  then the file does not need to be copied again a typical backup schedule may then be as follows  1 copy to a backup medium all files from the disk this is called a to another medium all files changed since day 1 this is an day 3 copy to another medium all files changed since day 2 day n copy to another medium all files changed since day n-1 then go back to day 1 the new cycle can have its backup written over the previous set or onto a new set of backup media in this manner  we can restore an entire disk by starting restores with the full backup and continuing through each of the incremental backups of course  the larger the value of n  the greater the number of media that must be read for a complete restore an added advantage of this backup cycle is that we can restore any file accidentally deleted during the cycle by retrieving the deleted file from the backup of the previous day the length of the cycle is a compromise between the amount of backup medium needed and the number of days back from which a restore can be done to decrease the number of tapes that must be read to do a restore  an option is to perform a full backup and then each day back up all files that have changed since the full backup in this way  a restore can be done via the most recent incremental backup and the full backup  with no other incremental backups needed the trade-off is that more files will be modified each day  so each successive incremental backup involves more files and more backup media a user ncay notice that a particular file is missing or corrupted long after the damage was done for this reason  we usually plan to take a full backup from time to time that will be saved forever it is a good idea to store these permanent backups far away from the regular backups to protect against hazard  such as a fire that destroys the computer and all the backups too and if the backup cycle reuses media  we must take care not to reuse the 490 chapter 11 11.8 media too many times-if the media wear out  it might not be possible to restore any data from the backups network file systems are commonplace they are typically integrated with the overall directory structure and interface of the client system nfs is a good example of a widely used  well-implemented client-server network file system here  we use it as an example to explore the implementation details of network file systems nfs is both an implementation and a specification of a software system for accessing remote files across lans  or even wans   nfs is part of onc +  which most unix vendors and some pc operating systems support the implementation described here is part of the solaris operating system  which is a modified version of unix svr4 running on sun workstations and other hardware it uses either the tcp or udp /ip protocol  depending on the interconnecting network   the specification and the implementation are intertwined in our description of nfs whenever detail is needed  we refer to the sun implementation ; whenever the description is general  it applies to the specification also there are multiple versions of nfs  with the latest being version 4 here  we describe version 3  as that is the one most commonly deployed 11.8.1 overview nfs views a set of interconnected workstations as a set of independent machines with independent file systems the goal is to allow some degree of sharing among these file systems  on explicit request  in a transparent manner sharing is based on a client-server relationship a machine may be  and often is  both a client and a server sharing is allowed between any pair of machines to ensure machine independence  sharing of a remote file system affects only the client machine and no other machine so that a remote directory will be accessible in a transparent manner from a particular machine-say  from ml-a client of that machine must first carry out a mount operation the semantics of the operation involve mounting a remote directory over a directory of a local file system once the mount operation is completed  the mounted directory looks like an integral subtree of the local file system  replacing the subtree descending from the local directory the local directory becomes the name of the root of the newly mounted directory specification of the remote directory as an argument for the mount operation is not done transparently ; the location  or host name  of the remote directory has to be provided however  fron l then on  users on machine ml can access files in the remote directory in a totally transparent manner to illustrate file mounting  consider the file system depicted in figure 11.13  where the triangles represent subtrees of directories that are of interest the figure shows three independent file systems of machines named u  51  and 52 at this point  on each machine  only the local files can be accessed figure 11.14  a  shows the effects of mounting 81  /usr/shared over u  /usr/local this figure depicts the view users on u have of their file system notice that after the mount is complete  they can access any file within the dirl directory 11.8 491 u  s1  s2  usr usr usr figure 11.13 three independent file systems using the prefix /usr /local/ dir1 the original directory /usr /local on that machine is no longer visible subject to access-rights accreditation  any file system  or any directory within a file system  can be mounted remotely on top of any local directory diskless workstations can even mount their own roots from servers cascading mounts are also permitted in some nfs implementations that is  a file system can be mounted over another file system that is remotely mounted  not local a machine is affected by only those mounts that it has itself invoked mounting a remote file system does not give the client access to other file systems that were  by chance  mounted over the former file system thus  the mount mechanism does not exhibit a transitivity property in figure 11.14  b   we illustrate cascading mounts the figure shows the result of mounting s2  /usr /dir2 over u  /usr/local/dir1  which is already remotely mounted from 51 users can access files within dir2 on u using the u  u   a   b  figure 11.14 mounting in nfs  a  mounts  b  cascading mounts 492 chapter 11 prefix /usr/local/dir1 if a shared file system is mounted over a user 's home directories on all machines in a network  the user can log into any workstation and get his honce environment this property permits one of the design goals of nfs was to operate in a heterogeneous environment of different machines  operating systems  and network architectures the nfs specification is independent of these media and thus encourages other implementations this independence is achieved through the use of rpc primitives built on top of an external data representation  xdr  protocol used between two implementation-independent interfaces hence  if the system consists of heterogeneous machines and file systems that are properly interfaced to nfs  file systems of different types can be mounted both locally and remotely the nfs specification distinguishes between the services provided by a mount mechanism and the actual remote-file-access services accordingly  two separate protocols are specified for these services  a mount protocol and a protocol for remote file accesses  the the protocols are specified as sets of rpcs these rpcs are the building blocks used to implement transparent remote file access 11.8.2 the mount protocol the establishes the initial logical connection between a server and a client in sun 's implementation  each machine has a server process  outside the kernel  performing the protocol functions a mount operation includes the name of the remote directory to be mounted and the name of the server machine storing it the mount request is mapped to the corresponding rpc and is forwarded to the mount server running on the specific server machine the server maintains an that specifies local file systems that it exports for mounting  along with names of machines that are permitted to mount them  in solaris  this list is the i etc/dfs/dfstab  which can be edited only by a superuser  the specification can also include access rights  such as read only to simplify the maintenance of export lists and mount tables  a distributed naming scheme can be used to hold this information and make it available to appropriate clients recall that any directory within an exported file system can be mounted remotely by an accredited machine a component unit is such a directory when the server receives a mount request that conforms to its export list  it returns to the client a file handle that serves as the key for further accesses to files within the mounted file system the file handle contains all the information that the server needs to distinguish an individual file it stores in unix terms  the file handle consists of a file-system identifier and an inode number to identify the exact mounted directory within the exported file system the server also maintains a list of the client machines and the corresponding currently mounted directories this list is used mainly for administrative purposes-for instance  for notifying all clients that the server is going down only through addition and deletion of entries in this list can the server state be affected by the mount protocol usually  a system has a static mounting preconfiguration that is established at boot time  i etc/vfstab in solaris  ; however  this layout can be modified in 11.8 493 addition to the actual mount procedure  the mount protocol includes several other procedures  such as unmount and return export list 11.8.3 the nfs protocol the nfs protocol provides a set of rpcs for remote file operations the procedures support the following operations  searching for a file within a directory reading a set of directory entries manipulating links and directories accessing file attributes reading and writing files these procedures can be invoked only after a file handle for the remotely mounted directory has been established the omission of open   and close   operations is intentional a prominent feature of nfs servers is that they are stateless servers do not maintain information about their clients from one access to another no parallels to unix 's open-files table or file structures exist on the server side consequently  each request has to provide a full set of arguments  including a unique file identifier and an absolute offset inside the file for the appropriate operations the resulting design is robust ; no special measures need be taken to recover a server after a crash file operations must be idempotent for this purpose every nfs request has a sequence number  allowing the server to determine if a request is duplicated or if any are missing maintaining the list of clients that we mentioned seems to violate the statelessness of the server howeve1 ~ this list is not essential for the correct operation of the client or the server  and hence it does not need to be restored after a server crash consequently  it might include inconsistent data and is treated as only a hint a further implication of the stateless-server philosophy and a result of the synchrony of an rpc is that modified data  including indirection and status blocks  must be committed to the server 's disk before results are returned to the client that is  a client can cache write blocks  but when it flushes them to the server  it assumes that they have reached the server 's disks the server must write all nfs data synchronously thus  a server crash and recovery will be invisible to a client ; all blocks that the server is managing for the client will be intact the consequent performance penalty can be large  because the advantages of caching are lost performance can be increased using storage with its own nonvolatile cache  usually battery-backed-up memory   the disk controller ackiwwledges the disk write when the write is stored in the nonvolatile cache in essence  the host sees a very fast synchronous write these blocks remain intact even after system crash and are written from this stable storage to disk periodically a single nfs write procedure call is guaranteed to be atomic and is not intermixed with other write calls to the same file the nfs protocol  however  does not provide concurrency-control mechanisms a write   system call may 494 chapter 11 client server figure 11.15 schematic view of the nfs architecture be broken down into several rpc writes  because each nfs write or read call can contain up to 8 kb of data and udp packets are limited to 1,500 bytes as a result  two users writing to the same remote file may get their data intermixed the claim is that  because lock management is inherently stateful  a service outside the nfs should provide locking  and solaris does   users are advised to coordinate access to shared files using mechanisms outside the scope of nfs nfs is integrated into the operating system via a vfs as an illustration of the architecture  let 's trace how an operation on an already open remote file is handled  follow the example in figure 11.15   the client initiates the operation with a regular system call the operating-system layer maps this call to a vfs operation on the appropriate vnode the vfs layer identifies the file as a remote one and invokes the appropriate nfs procedure an rpc call is made to the nfs service layer at the remote server this call is reinjected to the vfs layer on the remote system  which finds that it is local and invokes the appropriate file-system operation this path is retraced to return the result an advantage of this architecture is that the client and the server are identical ; thus  a machine may be a client  or a server  or both the actual service on each server is performed by kernel threads 11.8.4 path-name translation in nfs involves the parsing of a path name such as /usr/local/dir1/file txt into separate directory entries  or components   1  usr   2  local  and  3  dir1 path-name translation is done by breaking the path into component names and perform.ing a separate nfs lookup call for every pair of component name and directory vnode once a n10unt point is crossed  every component lookup causes a separate rpc to the server this 11.8 495 expensive path-name-traversal scheme is needed  since the layout of each client 's logical name space is unique  dictated by the mounts the client has performed it would be itluch more efficient to hand a server a path name and receive a target vnode once a mount point is encountered at any point  however  there might be another mount point for the particular client of whicb the stateless server is unaware so that lookup is fast  a directory-name-lookup cache on the client side holds the vnodes for remote directory names this cache speeds up references to files with the same initial path name the directory cache is discarded when attributes returned from the server do not match the attributes of the cached vnode recall that mounting a remote file system on top of another already mounted remote file system  a cascading mount  is allowed in some implementations of nfs however  a server can not act as an intermediary between a client and another server instead  a client must establish a direct client-server com1ection with the second server by directly mounting the desired directory when a client has a cascading mount  more than one server can be involved in a path-name traversal however  each component lookup is performed between the original client and some server therefore  when a client does a lookup on a directory on which the server has mounted a file system  the client sees the underlying directory instead of the mounted directory 11.8.5 remote operations with the exception of opening and closing files  there is almost a one-to-one correspondence between the regular unix system calls for file operations and the nfs protocol rpcs thus  a remote file operation can be translated directly to the corresponding rpc conceptually  nfs adheres to the remote-service paradigm ; but in practice  buffering and caching techniques are employed for the sake of performance no direct correspondence exists between a remote operation and an rpc instead  file blocks and file attributes are fetched by the rpcs and are cached locally future remote operations use the cached data  subject to consistency constraints there are two caches  the file-attribute  inode-infonnation  cache and the file-blocks cache when a file is opened  the kernel checks with the remote server to determine whether to fetch or revalidate the cached attributes the cached file blocks are used only if the corresponding cached attributes are up to date the attribute cache is updated whenever new attributes arrive from the server cached attributes are  by default  discarded after 60 seconds both read-ahead and delayed-write techniques are used between the server and the client clients do not free delayed-write blocks until the server confirms that the data have been written to disk delayed-write is retained even when a file is opened concurrently  in conflicting modes hence  unix semantics  section 10.5.3.1  are not preserved tuning the system for performance makes it difficult to characterize the consistency semantics of nfs new files created on a machine may not be visible elsewhere for 30 seconds furthermore  writes to a file at one site may or may not be visible at other sites that have this file open for reading new opens of a file observe only the changes that have already been flushed to the server thus  nfs provides neither strict emulation of unix semantics nor the 496 chapter 11 11.9 session sen antics of andrew  section 10.5.3.2  .ln spite of these drawbacks  the utility and good performance of the mechanism make it the most widely used multi-vendor-distributed system in operation disk i/o has a huge impact on system performance as a result  file-system design and implementation command quite a lot of attention from system designers some file systems are general purpose  in that they can provide reasonable performance and functionality for a wide variety of file sizes  file types  and i/0 loads others are optimized for specific tasks in an attempt to provide better performance in those areas than general-purpose file systems the wafl file system from network appliance is an example of this sort of optimization wafl  the write-anywhere file layout  is a powerful  elegant file system optimized for random writes wafl is used exclusively on network file servers produced by network appliance and so is meant for use as a distributed file system it can provide files to clients via the nfs  cifs  ftp  and http protocols  although it was designed just for nfs and cifs when many clients use these protocols to talk to a file server  the server may see a very large demand for random reads and an even larger demand for random writes the nfs and cifs protocols cache data from read operations  so writes are of the greatest concern to file-server creators wafl is used on file servers that include an nvram cache for writes the wafl designers took advantage of running on a specific architecture to optimize the file system for random i/0  with a stable-storage cache in front ease of use is one of the guiding principles of wafl  because it is designed to be used in an appliance its creators also designed it to include a new snapshot functionality that creates multiple read-only copies of the file system at different points in time  as we shall see the file system is similar to the berkeley fast file system  with many modifications it is block-based and uses inodes to describe files each inode contains 16 pointers to blocks  or indirect blocks  belonging to the file described by the inode each file system has a root inode all of the metadata lives in files  all inodes are in one file  the free-block map in another  and the free-inode root inode 1 free blotk map i figure 11.16 the wafl file layout 11.9 497 map in a third  as shown in figure 11.16 because these are standard files  the data blocks are not limited in location and can be placed anywhere if a file system is expanded by addition of disks  the lengths of the metadata files are automatically expanded by the file systen  thus  a wafl file system is a tree of blocks with the root inode as its base to take a snapshot  wafl creates a copy of the root inode any file or metadata updates after that go to new blocks rather than overwriting their existing blocks the new root inode points to metadata and data changed as a result of these writes meanwhile  the snapshot  the old root inode  still points to the old blocks  which have not been updated it therefore provides access to the file system just as it was at the instant the snapshot was made-and takes very little disk space to do so ! in essence  the extra disk space occupied by a snapshot consists of just the blocks that have been modified since the snapshot was taken an important change from more standard file systems is that the free-block map has more than one bit per block it is a bitmap with a bit set for each snapshot that is using the block when all snapshots that have been using the block are deleted  the bit map for that block is all zeros  and the block is free to be reused used blocks are never overwritten  so writes are very fast  because a write can occur at the free block nearest the current head location there are many other performance optimizations in wafl as well many snapshots can exist simultaneously  so one can be taken each hour of the day and each day of the month a user with access to these snapshots can access files as they were at any of the times the snapshots were taken the snapshot facility is also useful for backups  testing  versioning  and so on wafl 's snapshot facility is very efficient in that it does not even require that copy-on-write copies of each data block be taken before the block is modified other file systems provide snapshots  but frequently with less efficiency wafl snapshots are depicted in figure 11.17 newer versions of wafl actually allow read-write snapshots  known as ,.hj ' ' ' clones are also efficient  using the same techniques as shapshots in this case  a read-only snapshot captures the state of the file system  and a clone refers back to that read-only snapshot any writes to the clone are stored in new blocks  and the clone 's pointers are updated to refer to the new blocks the original snapshot is unmodified  still giving a view into the file system as it was before the clone was updated clones can also be promoted to replace the original file system ; this involves throwing out all of the old pointers and any associated old blocks clones are useful for testing and upgrades  as the original version is left untouched and the clone deleted when the test is done or if the upgrade fails another feature that naturally falls from the wafl file system implementation is the duplication and synchronization of a set of data over a network to another system first  a snapshot of a wafl file system is duplicated to another system when another snapshot is taken on the source system  it is relatively easy to update the remote system just by sending over all blocks contained in the new snapshot these blocks are the ones that have changed between the times the two snapshots were taken the remote system adds these blocks to the file system and updates its pointers  and the new system then is a duplicate of the source system as of the time of the second snapshot repeating this process maintains the remote system as a nearly up-to-date copy of the first 498 chapter 11 11.10  a  before a snapshot  b  after a snapshot  before any blocks change  c  after block d has changed to o  figure 11.17 snapshots in wafl system such replication is used for disaster recovery should the first system be destroyed  most of its data are available for use on the remote system finally  we should note that sun 's zfs file system supports similarly efficient snapshots  clones  and replication the file system resides permanently on secondary storage  which is designed to hold a large amount of data permanently the most common secondary-storage medium is the disk physical disks may be segmented into partitions to control media use and to allow multiple  possibly varying  file systems on a single spindle these file systems are mounted onto a logical file system architecture to make then available for use file systems are often implemented in a layered or modular structure the lower levels deal with the physical properties of storage devices upper levels deal with symbolic file names and logical properties of files intermediate levels map the logical file concepts into physical device properties any file-system type can have different structures and algorithms a vfs layer allows the upper layers to deal with each file-system type uniformly even 499 remote file systems can be integrated into the system 's directory structure and acted on by standard system calls via the vfs interface the various files can be allocated space on the disk in three ways  through contiguous  linked  or indexed allocation contiguous allocation can suffer from external fragmentation direct access is very inefficient with linked allocation indexed allocation may require substantial overhead for its index block these algorithms can be optimized in many ways contiguous space can be enlarged through extents to increase flexibility and to decrease external fragmentation indexed allocation can be done in clusters of multiple blocks to increase throughput and to reduce the number of index entries needed indexing in large clusters is similar to contiguous allocation with extents free-space allocation methods also influence the efficiency of disk-space use  the performance of the file system  and the reliability of secondary storage the methods used include bit vectors and linked lists optimizations include grouping  countilcg  and the fat  which places the linked list in one contiguous area directory-management routines must consider efficiency  performance  and reliability a hash table is a commonly used method  as it is fast and efficient unfortunately  damage to the table or a system crash can result in inconsistency between the directory information and the disk 's contents a consistency checker can be used to repair the damage operating-system backup tools allow disk data to be copied to tape  enabling the user to recover from data or even disk loss due to hardware failure  operating system bug  or user error network file systems  such as nfs  use client-server methodology to allow users to access files and directories from remote machines as if they were on local file systems system calls on the client are translated into network protocols and retranslated into file-system operations on the server networking and multiple-client access create challenges in the areas of data consistency and performance due to the fundamental role that file systems play in system operation  their performance and reliability are crucial techniques such as log structures and cachirtg help improve performance  while log structures and raid improve reliability the wafl file system is an example of optimization of performance to match a specific i/o load 11.1 in what situations would using memory as a ram disk be more useful than using it as a disk cache 11.2 consider a file systenc that uses a modifed contiguous-allocation scheme with support for extents a file is a collection of extents  with each extent corresponding to a contiguous set of blocks a key issue in such systems is the degree of variability in the size of the 500 chapter 11 extents what are the advantages and disadvantages of the following schemes a all extents are of the same size  and the size is predetermined b extents can be of any size and are allocated dynamically c extents can be of a few fixed sizes  and these sizes are predetermined 11.3 some file systems allow disk storage to be allocated at different levels of granularity for instance  a file system could allocate 4 kb of disk space as a single 4-kb block or as eight 512-byte blocks how could we take advantage of this flexibility to improve performance what modifications would have to be made to the free-space management scheme in order to support this feature 11.4 what are the advantages of the variant of linked allocation that uses a fat to chain together the blocks of a file 11.5 consider a file currently consisting of 100 blocks assume that the filecontrol block  and the index block  in the case of indexed allocation  is already in memory calculate how many disk i/0 operations are required for contiguous  linked  and indexed  single-level  allocation strategies  if  for one block  the following conditions hold in the contiguous-allocation case  assume that there is no room to grow at the beginning but there is room to grow at the end also assume that the block information to be added is stored in memory a the block is added at the beginning b the block is added in the middle c the block is added at the end d the block is removed from the beginning e the block is removed from the middle f the block is removed from the end 11.6 consider a file system that uses inodes to represent files disk blocks are 8 kb in size  and a pointer to a disk block requires 4 bytes this file system has 12 direct disk blocks  as well as single  double  and triple indirect disk blocks what is the maximum size of a file that can be stored in this file system 11.7 assume that in a particular augmentation of a reinote-file-access protocol  each client maintains a name cache that caches translations from file names to corresponding file handles what issues should we take into account in implementing the name cache 11.8 consider the following backup scheme  day 1 copy to a backup medium all files from the disk day 2 copy to another m.edium all files changed since day 1 day 3 copy to another medium all files changed since day 1 501 this differs from the schedule given in section 11.7.4 by having all subsequent backups copy all files modified since the first full backup what are the benefits of this system over the one in section 11.7.4 what are the drawbacks are restore operations made easier or more difficult explain your answer 11.9 why must the bit map for file allocation be kept on mass storage  rather than in main memory 11.10 consider a file system on a disk that has both logical and physical block sizes of 512 bytes assume that the information about each file is already in memory for each of the three allocation strategies  contiguous  linked  and indexed   answer these questions  a how is the logical-to-physical address mapping accomplished in this system  for the indexed allocation  assume that a file is always less than 512 blocks long  b if we are currently at logical block 10  the last block accessed was block 10  and want to access logical block 4  how many physical blocks must be read from the disk 11.11 why is it advantageous to the user for an operating system to dynamically allocate its internal tables what are the penalties to the operating system for doing so 11.12 explain why logging metadata updates ensures recovery of a file system after a file-system crash 11.13 fragmentation on a storage device can be eliminated by recompaction of the information typical disk devices do not have relocation or base registers  such as those used when memory is to be compacted   so how can we relocate files give three reasons why recompacting and relocation of files are often avoided 11.14 consider a system where free space is kept in a free-space list a suppose that the pointer to the free-space list is lost can the system reconstruct the free-space list explain your answer b consider a file system similar to the one used by unix with indexed allocation how many disk i/0 operations might be 502 chapter 11 required to read the contents of a small local file at /a/b/c assume that none of the disk blocks is currently being cached c suggest a scheme to ensure that the pointer is never lost as a result of memory failure 11.15 one problem with contiguous allocation is that the user must preallocate enough space for each file if the file grows to be larger than the space allocated for it  special actions must be taken one solution to this problem is to define a file structure consisting of an initial contiguous area  of a specified size   if this area is filled  the operating system automatically defines an overflow area that is linked to the initial contiguous area if the overflow area is filled  another overflow area is allocated compare this implementation of a file with the standard contiguous and linked implementations 11.16 discuss how performance optimizations for file systems might result in difficulties in maintaining the consistency of the systems in the event of com.puter crashes the ms-dos fat system is explained in norton and wilton  1988   and the os/2 description can be found in iacobucci  1988   these operating systems use the intel 8086 cpus  intel  1985b   intel  1985a   intel  1986   and intel  1990    ibm allocation methods are described in deitel  1990   the internals of the bsd unl ' system are covered in full in mckusick et al  1996   mcvoy and kleiman  1991  discusses optimizations of these methods made in solaris the coogle file system is described in ghemawat et al  2003   fuse can be found workloads in distributed file systems are examined in baker et al  1991   ousterhout  1991  discusses the role of distributed state in networked file systems log-structured designs for networked file systems are proposed in hartman and ousterhout  1995  and thekkath et al  1997   nfs and the unix file system  ufs  are described in vahalia  1996  and mauro and mcdougall  2007   the windows nt file system  ntfs  is explained in solomon  1998   the ext2 file system used in linux is described in bovet and cesati  2002  and the wafl file system in hitz et al  1995   zfs documentation can be found at http  / /www.opensolaris.org/ os/ community /zfs/ docs 12.1 the file system can be viewed logically as consisting of three parts in chapter 10  we examined the user and programmer interface to the file system in chapter 11  we described the internal data structures and algorithms used by the operating system to implement this interface in this chapter  we discuss the lowest level of the file system  the secondary and tertiary storage structures we first describe the physical structure of magenetic disks and magnetic tapes we then describe disk-scheduling algorithms  which schedule the order of disk i/ os to improve performance next  we discuss disk formatting and management of boot blocks  damaged blocks  and swap space we then examine secondary storage structure  covering disk reliability and stablestorage implementation we conclude with a brief description of tertiary storage devices and the problems that arise when an operating system uses tertiary storage to describe the physical structure of secondary and tertiary storage devices and its effects on the uses of the devices to explain the performance characteristics of mass-storage devices to discuss operating-system services provided for mass storage  including raid and hsm in this section  we present a general overview of the physical structure of secondary and tertiary storage devices 12.1.1 magnetic disks provide the bulk of secondary storage for modern computer systems conceptually  disks are relatively simple  figure 12.1   each disk platter has a flat circular shape  like a cd common platter diameters range 505 506 chapter 12 arm assembly rotation figure 12.1 moving-head disk mechanism from 1.8 to 5.25 inches the two surfaces of a platter are covered with a magnetic material we store information by recording it magnetically on the platters a read -write head flies just above each surface of every platter the heads are attached to a that moves all the heads as a unit the surface of a platter is logically divided into circular which are subdivided into the set of tracks that are at one arm position makes up a there may be thousands of concentric cylinders in a disk drive  and each track may contain hundreds of sectors the storage capacity of common disk drives is measured iil gigabytes when the disk is in use  a drive motor spins it at high speed most drives rotate 60 to 200 times per second disk speed has two parts the is the rate at which data flow between the drive and the computer the sometimes called the consists of the time necessary to move the disk arm to the desired cylinder  called the and the time necessary for the desired sector to rotate to the disk head  called the typical disks can transfer several megabytes of data per second  and they seek times and rotational latencies of several milliseconds because the disk head flies on an extremely thin cushion of air  measured in microns   there is a danger that the head will make contact with the disk surface although the disk platters are coated with a thin protective laye1 ~ the head will sometimes damage the magnetic surface this accident is called a a head crash normally can not be repaired ; the entire disk must be replaced a disk can be allowing different disks to be mounted as needed removable magnetic disks generally consist of one platter  held in a plastic case to prevent damage while not in the disk drive are inexpensive removable magnetic disks that have a soft plastic case containing a flexible platter the head of a floppy-disk drive generally sits directly on the disk 12.1 507 disk transfer rates as with many aspects of computingf published performance numbers for disks are not the same as real-world performance numbers stated transfer rates are always lower than for example the transfer rate may be the rate at which bits can be read from the magnetic media by the disk head  but that is different from the rate at which blocks are delivered to the operating system surface  so the drive is designed to rotate more slowly than a hard-disk drive to reduce the wear on the disk surface the storage capacity of a floppy disk is typically only 1.44mb or so removable disks are available that work much like normal hard disks and have capacities measured in gigabytes a disk drive is attached to a computer by a set of wires called an several kinds of buses are available  including buses the data transfers on a bus are carried out by special electronic processors called the is the controller at the computer end of the bus a is built into each disk drive to perform a disk i/0 operation  the computer places a command into the host controller  typically using memory-mapped i/0 portsf as described in section 9.7.3 the host controller then sends the command via messages to the disk controller  and the disk controller operates the disk-drive hardware to carry out the command disk controllers usually have a built-in cache data transfer at the disk drive happens between the cache and the disk surface  and data transfer to the host  at fast electronic speeds  occurs between the cache and the host controller 12.1.2 magnetic tapes was used as an early secondary-storage medium although it is relatively permanent and can hold large quantities of dataf its access time is slow compared with that of main memory and magnetic disk in addition  random access to magnetic tape is about a thousand times slower than random access to magnetic disk  so tapes are not very useful for secondary storage tapes are used mainly for backup  for storage of infrequently used information  and as a medium for transferring information from one system to another a tape is kept in a spool and is wound or rewound past a read-write head moving to the correct spot on a tape can take minutes  but once positioned  tape drives can write data at speeds comparable to disk drives tape capacities vary greatly  depending on the particular kind of tape drive typically  they store from 20gb to 200gb some have built-in compression that can more than double the effective storage tapes and their drivers are usually categorized by width  includil1.g 4  8f and 19 millimeters and 1/4 and 1/2 inch some are named according to technology  such as lt0-2 and sdlt tape storage is further described in section 12.9 508 chapter 12 12.2 fire wire refers to an interface designed for connecting peripheral devices such as hard drives  dvd drives  and digital video cameras to a computer system fire wire was first developed by apple computer and became the ieee 1394 standard in 1995 the originalfirewire standard provided bandwidth up to 400 megabits per second recently  a new standardfirewire 2-has emerged and is identified by the ieee 1394b standard firewire 2 provides double the data rate of the original firewire-800 megabits per second modern disk drives are addressed as large one-dimensional arrays of where the logical block is the smallest unit of transfer the size of a logical block is usually 512 bytes  although some disks can be to have a different logical block size  such as 1,024 bytes this option is described in section 12.5.1 the one-dimensional array of logical blocks is mapped onto the sectors of the disk sequentially sector 0 is the first sector of the first track on the outermost cylinder the mapping proceeds in order through that track  then through the rest of the tracks in that cylinder  and then through the rest of the cylinders from outermost to innermost by using this mapping  we can -at least in theory-convert a logical block number into an old-style disk address that consists of a cylinder number  a track number within that cylinder  and a sector number within that track in practice  it is difficult to perform this translation  for two reasons first  most disks have some defective sectors  but the mapping hides this by substituting spare sectors from elsewhere on the disk second  the number of sectors per track is not a constant on smne drives let 's look more closely at the second reason on media that use the density of bits per track is uniform the farther a track is from the center of the disk  the greater its length  so the more sectors it can hold as we move from outer zones to inner zones  the number of sectors per track decreases tracks in the outermost zone typically hold 40 percent more sectors than do tracks in the innermost zone the drive increases its rotation speed as the head moves from the outer to the inner tracks to keep the same rate of data moving under the head this method is used in cd-rom and dvd-rom drives alternatively  the disk rotation speed can stay constant ; in this case  the density of bits decreases from inner tracks to outer tracks to keep the data rate constant this method is used in hard disks and is known as the number of sectors per track has been increasing as disk technology improves  and the outer zone of a disk usually has several hundred sectors per track similarly  the number of cylinders per disk has been increasing ; large disks have tens of thousands of cylinders 12.3 12.3 509 computers access disk storage in two ways one way is via i/o ports  or this is common on small systems the other way is via a remote host in a distributed file system ; this is referred to as 12.3.1 host-attached storage host-attached storage is storage accessed through local i/0 ports these ports use several technologies the typical desktop pc uses an i/0 bus architecture called ide or ata this architecture supports a maximum of two drives per i/0 bus a newer  similar protocol that has simplified cabling is sata high-end workstations and servers generally use more sophisticated i/0 architectures  such as scsi and fiber charmel  fc   scsi is a bus architecture its physical medium is usually a ribbon cable with a large number of conductors  typically 50 or 68   the scsi protocol supports a maximum of 16 devices per bus generally  the devices include one controller card in the host  the and up to 15 storage devices  the to.rgr    ts   a scsi disk is a common scsi target  but the protocol provides the ability to address up to 8 in each scsi target a typical use of logical unit addressing is to commands to components of a raid array or components of a removable media library  such as a cd jukebox sendil g commands to the media-changer mechanism or to one of the drives   fc is a high-speed serial architecture that can operate over optical fiber or over a four-conductor copper cable it has two variants one is a large switched fabric having a 24-bit address space this variant is expected to dominate in the future and is the basis of  sjld '  ; s   discussed in section 12.3.3 because of the large space and the switched nature of the communication  multiple hosts and storage devices can attach to the fabric  allowing great flexibility in i/0 communication the other fc variant is an that can address 126 devices  drives and controllers   a wide variety of storage devices are suitable for use as host-attached storage among these are hard disk drives  raid arrays  and cd  dvd  and tape drives the i/0 commands that initiate data transfers to a host-attached storage device are reads and writes of logical data blocks directed to specifically identified storage units  such as bus id  scsi id  and target logical unit   12.3.2 network-attached storage a network-attached storage  nas  device is a special-purpose storage system that is accessed remotely over a data network  figure 12.2   clients access network-attached storage via a remote-procedure-call interface such as nfs for unix systems or cifs for windows machines the remote procedure calls  rpcs  are carried via tcp or udp over an ip network-usually the same local-area network  lan  that carries all data traffic to the clients the networkattached storage unit is usually implemented as a raid array with software that implements the rpc interface it is easiest to thil k of nas as simply another storage-access protocol for example  rather than using a scsi device driver and scsi protocols to access storage  a system using nas would use rpc over tcp /ip 510 chapter 12 12.4 lan/wan figure 12.2 network-attached storage network-attached storage provides a convenient way for all the computers on a lan to share a pool of storage with the same ease of naming and access enjoyed with local host-attached storage however  it tends to be less efficient and have lower performance than some direct-attached storage options is the latest network-attached storage protocol in essence  it uses the ip network protocol to carry the scsi protocol thus  networks-rather than scsi cables-can be used as the interconnects between hosts and their storage as a result  hosts can treat their storage as if it were directly attached  even if the storage is distant from the host 12.3.3 storage-area network one drawback of network-attached storage systems is that the storage i/o operations consume bandwidth on the data network  thereby increasing the latency of network communication this problem can be particularly acute in large client-server installations-the communication between servers and clients competes for bandwidth with the communication among servers and storage devices a storage-area network  san  is a private network  using storage protocols rather than networking protocols  connecting servers and storage units  as shown in figure 12.3 the power of a san lies in its flexibility multiple hosts and multiple storage arrays can attach to the same san  and storage can be dynamically allocated to hosts a san switch allows or prohibits access between the hosts and the storage as one example  if a host is running low on disk space  the san can be configured to allocate more storage to that host sans make it possible for clusters of servers to share the same storage and for storage arrays to include multiple direct host com1.ections sans typically have more ports  and less expensive ports  than storage arrays fc is the most common san interconnect  although the simplicity of iscsi is increasing its use an emerging alternative is a special-purpose bus architecture named infiniband  which provides hardware and software support for highspeed interconnection networks for servers and storage units one of the responsibilities of the operating system is to use the hardware efficiently for the disk drives  meeting this responsibility entails having 12.4 511 figure 12.3 storage-area network fast access time and large disk bandwidth the access time has two major components  also see section 12.1.1   the is the time for the disk arm to move the heads to the cylinder containing the desired sector the is the additional time for the disk to rotate the desired sector to the disk head the disk is the total number of bytes transferred  divided by the total time between the first request for service and the completion of the last transfer we can improve both the access time and the bandwidth by managing the order in which disk i/o requests are serviced whenever a process needs i/0 to or from the disk  it issues a system call to the operating system the request specifies several pieces of information  whether this operation is input or output what the disk address for the transfer is what the memory address for the transfer is what the number of sectors to be transferred is if the desired disk drive and controller are available  the request can be serviced immediately if the drive or controller is busy  any new requests for service will be placed in the queue of pending requests for that drive for a multiprogramming system with many processes  the disk queue may often have several pending requests thus  when one request is completed  the operating system chooses which pending request to service next how does the operating system make this choice any one of several disk-scheduling algorithms can be used  and we discuss them next 12.4.1 fcfs scheduling the simplest form of disk scheduling is  of course  the first-come  first-served  fcfs  algorithm this algorithm is intrinsically fair  but it generally does not provide the fastest service consider  for example  a disk queue with requests for i/0 to blocks on cylinders 98  183  37  122  14  124  65  67  512 chapter 12 queue = 98  183,37,122  14,124,65,67 head starts at 53 0 14 37 536567 98 122124 figure 12.4 fcfs disk scheduling 183199 in that order if the disk head is initially at cylinder 53  it will first move from 53 to 98  then to 183  37  122  14  124  65  and finally to 67  for a total head movement of 640 cylinders this schedule is diagrammed in figure 12.4 the wild swing from 122 to 14 and then back to 124 illustrates the problem with this schedule if the requests for cylinders 37 and 14 could be serviced together  before or after the requests for 122 and 124  the total head movement could be decreased substantially  and performance could be thereby improved 12.4.2 sstf scheduling it seems reasonable to service all the requests close to the current head position before moving the head far to service other this assumption is the basis for the the sstf algorithm selects the request with the least seek time from the current head position since seek time increases with the number of cylinders traversed by the head  sstf chooses the pending request closest to the current head position for our example request queue  the closest request to the initial head position  53  is at cylinder 65 once we are at cylinder 65  the next closest request is at cylinder 67 from there  the request at cylinder 37 is closer than the one at 98  so 37 is served next continuing  we service the request at cylinder 14  then 98  122  124  and finally 183  figure 12.5   this scheduling method results in a total head movement of only 236 cylinders-little more than one-third of the distance needed for fcfs scheduling of this request queue clearly  this algorithm gives a substantial improvement in performance sstf scheduling is essentially a form of shortest-job-first  sjf  scheduling ; and like sjf scheduling  it may cause starvation of some requests remember that requests may arrive at any time suppose that we have two requests in the queue  for cylinders 14 and 186  and while the request from 14 is being serviced  a new request near 14 arrives this new request will be serviced next  making the request at 186 wait while this request is being serviced  another request close to 14 could arrive in theory  a continual stream of requests near one another could cause the request for cylinder 186 to wait indefinitely 12.4 queue = 98  183  37  122  14  124  65  67 head starts at 53 0 14 37 536567 98 122124 figure 12.5 sstf disk scheduling 513 183199 this scenario becomes increasingly likely as the pending-request queue grows longer although the sstf algorithm is a substantial improvement over the fcfs algorithm  it is not optimal in the example  we can do better by moving the head from 53 to 37  even though the latter is not closest  and then to 14  before turning around to service 65  67  98  122  124  and 183 this strategy reduces the total head movement to 208 cylinders 12.4.3 scan scheduling in the toward the end  servicing requests as it reaches each cylinder  until it gets to the other end of the disk at the other end  the direction of head movement is reversed  and servicing continues the head continuously scans back and forth across the disk the scan algorithm is sometimes called the since the disk arm behaves just like an elevator in a building  first servicing all the requests going up and then reversing to service requests the other way let 's return to our example to illustrate before applying scan to schedule the requests on cylinders 98  183,37  122  14  124  65  and 67  we need to know the direction of head movement in addition to the head 's current position assuming that the disk arm is moving toward 0 and that the initial head position is again 53  the head will next service 37 and then 14 at cylinder 0  the arm will reverse and will move toward the other end of the disk  servicil lg the requests at 65  67  98  122  124  and 183  figure 12.6   if a request arrives in the queue just in front of the head  it will be serviced almost immediately ; a request arriving just behind the head will have to wait until the arm moves to the end of the disk  reverses direction  and comes back assuming a uniform distribution of requests for cylinders  consider the density of requests when the head reaches one end and reverses direction at this point  relatively few requests are immediately in front of the head  since these cylinders have recently been serviced the heaviest density of requests 514 chapter 12 queue = 98  183,37,122  14,124,65,67 head starts at 53 0 14 37 536567 98 122124 figure 12.6 scan disk scheduling 183199 is at the other end of the disk these requests have also waited the longest so why not go there first that is the idea of the next algorithm 12.4.4 c-scan scheduling is a variant of scan designed to provide a more uniform wait time like scan  c-scan moves the head from one end of the disk to the other  servicing requests along the way when the head reaches the other end  however  it immediately returns to the beginning of the disk without servicing any requests on the return trip  figure 12.7   the c-scan scheduling algorithm essentially treats the cylinders as a circular list that wraps around from the final cylinder to the first one queue = 98  183  37  122  14  124  65  67 head starts at 53 0 1 4 37 53 65 67 98 1 22 1 24 figure 12.7 c-scan disk scheduling 183199 12.4 queue = 98  183  37  122  14  124  65  67 head starts at 53 0 14 37 536567 98 122124 figure 12.8 c-look disk scheduling 12.4.5 look scheduling 515 183199 as we described themf both scan and c-scan move the disk arm across the full width of the disk in practicef neither algorithm is often implemented this way more commonlyf the arm goes only as far as the final request in each direction then  it reverses direction immediatelyf without going all the way to the end of the disk versions of scan and c-scan that follow this pattern are called and because they look for a request before continuing to move in a given direction  figure 12.8   12.4.6 selection of a disk-scheduling algorithm given so many disk-scheduling algorithmsf how do we choose the best one sstf is common and has a natural appeal because it increases performance over fcfs scan and c-scan perform better for systems that place a heavy load on the diskf because they are less likely to cause a starvation problem for any particular list of requestsf we can define an optimal order of retrievat but the computation needed to find an optimal schedule may not justify the savings over sstf or scan with any scheduling algoritlunf howeverf performance depends heavily on the number and types of requests for instance  suppose that the queue usually has just one outstanding request thenf all scheduling algorithms behave the samef because they have only one choice of where to move the disk head  they all behave like fcfs scheduling requests for disk service can be greatly influenced by the file-allocation method a program reading a contiguously allocated file will generate several requests that are close together on the disk  resulting in limited head movement a linked or indexed fik in contrastf may include blocks that are widely scattered on the diskf resulting in greater head movement the location of directories and index blocks is also important since every file must be opened to be usedf and opening a file requires searching the directory structuref the directories will be accessed frequently suppose that a directory entry is on the first cylinder and a filef s data are on the final cylinder in this casef the disk head has to move the entire width of the disk if the directory 516 chapter 12 12.5 entry were on the middle cylinder  the head would have to move only one-half the width caching the directories and index blocks in main memory can also help to reduce disk-arm movement particularly for read requests because of these complexities  the disk-scheduling algorithm should be written as a separate module of the operating system  so that it can be replaced with a different algorithm if necessary either sstf or look is a reasonable choice for the default algorithm the scheduling algorithms described here consider only the seek distances for modern disks  the rotational latency can be nearly as large as the average seek time it is difficult for the operating system to schedule for improved rotational latency  though  because modern disks do not disclose the physical location of logical blocks disk manufacturers have been alleviating this problem by implementing disk-scheduling algorithms in the controller hardware built into the disk drive if the operating system sends a batch of requests to the controller  the controller can queue them and then schedule them to improve both the seek time and the rotational latency if i/o performance were the only consideration  the operating system would gladly turn over the responsibility of disk scheduling to the disk hardware in practice  however  the operating system may have other constraints on the service order for requests for instance  demand paging may take priority over application i/0  and writes are more urgent than reads if the cache is running out of free pages also  it may be desirable to guarantee the order of a set of disk writes to make the file system robust in the face of system crashes consider what could happen if the operating system allocated a disk page to a file and the application wrote data into that page before the operating system had a chance to flush the modified inode and free-space list back to disk to accommodate such requirements  an operating system may choose to do its own disk scheduling and to spoon-feed the requests to the disk controller  one by one  for some types of i/0 the operating system is responsible for several other aspects of disk management  too here we discuss disk initialization  booting from disk  and bad-block recovery 12.5.1 disk formatting a new magnetic disk is a blank slate  it is just a platter of a magnetic recording material before a disk can store data  it must be divided into sectors that the disk controller can read and write this process is called or low-level formatting fills the disk with a special data structure for each sector the data structure for a sector typically consists of a header  a data area  usually 512 bytes in size   and a trailer the header and trailer contain information used by the disk controller  such as a sector number and an  when the controller writes a sector of data during normal i/0  the ecc is updated with a value calculated from all the bytes in the data area when the sector is read  the ecc is recalculated and compared with the stored value if the stored and calculated numbers are different  this 12.5 517 mismatch indicates that the data area of the sector has become corrupted and that the disk sector may be bad  section 12.5.3   the ecc is an error-correcting code because it contains enough information  if only a few bits of data have been corrupted  to enable the controller to identify which bits have changed and calculate what their correct values should be it then reports a recoverable  the controller automatically does the ecc processing whenever a sector is read or written most hard disks are low-level-formatted at the factory as a part of the manufacturing process this formatting enables the manufacturer to test the disk and to initialize the mapping from logical block numbers to defect-free sectors on the disk for many hard disks  when the disk controller is instructed to low-level-format the disk  it can also be told how many bytes of data space to leave between the header and trailer of all sectors it is usually possible to choose among a few sizes  such as 256,512  and 1,024 bytes formatting a disk with a larger sector size means that fewer sectors can fit on each track ; but it also means that fewer headers and trailers are written on each track and more space is available for user data some operating systems can handle only a sector size of 512 bytes before it can use a disk to hold files  the operating system still needs to record its own data structures on the disk it does so in two steps the first step is to the disk into one or more groups of cylinders the operatiltg system can treat each partition as though it were a separate disk for instance  one partition can hold a copy of the operating system 's executable code  while another holds user files the second step is icgicz ; i or creation of a file system in this step  the operating system stores the iltitial file-system data structures onto the disk these data structures may include maps of free and allocated space  a fat or inodes  and an initial empty directory to increase efficiency  most file systems group blocks together into larger chunks  frequently called disk i/0 is done via blocks  but file system ii 0 is done via clusters  effectively assuring that ii 0 has more sequential-access and fewer random-access characteristics some operating systems give special programs the ability to use a disk partition as a large sequential array of logical blocks  without any file-system data structures this array is sometimes called the raw disk  and ii 0 to this array is termed raw l/0 for example  some database systems prefer raw iio because it enables them to control the exact disk location where each database record is stored raw l/0 bypasses all the file-system services  such as the buffer cache  file locking  prefetching  space allocation  file names  and directories we can make certain applications more efficient by allowing them to implement their own special-purpose storage services on a raw partition  but most applications perform better when they use the regular file-system services 12.5.2 boot block for a computer to start running-for instance  when it is powered up or rebooted -it must have an initial program to run this initial bootstrap program tends to be simple it initializes all aspects of the system  from cpu registers to device controllers and the contents of main memory  and then starts the operating system to do its job  the bootstrap program finds the 518 chapter 12 operating-system kernel on disk  loads that kernel into memory  and jumps to an initial address to begin the operating-system execution for most computers  the bootstrap is stored in this location is convenient  because rom needs no initialization and is at a fixed location that the processor can start executing when powered up or reset and  since rom is read only  it can not be infected by a computer virus the problem is that changing this bootstrap code requires changing the rom hardware chips for this reason  most systems store a tiny bootstrap loader program in the boot rom whose only job is to bring in a full bootstrap program from disk the full bootstrap program can be changed easily  a new version is simply written onto the disk the full bootstrap program is stored in the boot blocks at a fixed location on the disk a disk that has a boot partition is called a or the code in the boot rom instructs the disk controller to read the boot blocks into memory  no device drivers are loaded at this point  and then starts executing that code the full bootstrap program is more sophisticated than the bootstrap loader in the boot rom ; it is able to load the entire operating system from a non-fixed location on disk and to start the operating system ruru1ing even so  the full bootstrap code may be small let 's consider as an example the boot process in windows 2000 the windows 2000 system places its boot code in the first sector on the hard disk  which it terms the or furthermore  windows 2000 allows a hard disk to be divided into one or more partitions ; one partition  identified as the contains the operating system and device drivers bootil1g begins in a windows 2000 system by running code that is resident in the system 's rom memory this code directs the system to read the boot code from the mbr in addition to containing boot code  the mbr contains a table listing the partitions for the hard disk and a flag indicating which partition the system is to be booted from  as illustrated in figure 12.9 once the system identifies the boot partition  it reads the first sector from that partition  which is called the and contilmes with the remainder of the boot process  which includes loading the various subsystems and system services mbr partition 1 partition 2 partition 3 partition 4 boot code partition table boot partition figure 12.9 booting from disk in windows 2000 12.5 519 12.5.3 bad blocks because disks have moving parts and small tolerances  recall that the disk head flies just above the disk surface   they are prone to failure sometimes the failure is complete ; in this case  the disk needs to be replaced and its contents restored from backup media to the new disk more frequently  one or more sectors become defective most disks even con'le from the factory with depending on the disk and controller in use  these blocks are handled in a variety of ways on simple disks  such as some disks with ide controllers  bad blocks are handled manually for instance  the ms-dos format command performs logical formatting and  as a part of the process  scans the disk to find bad blocks if format finds a bad block  it writes a special value into the corresponding fat entry to tell the allocation routines not to use that block if blocks go bad during normal operation  a special program  such as chkdsk  must be run manually to search for the bad blocks and to lock them away data that resided on the bad blocks usually are lost more sophisticated disks  such as the scsi disks used in high-end pcs and most workstations and servers  are smarter about bad-block recovery the controller maintains a list of bad blocks on the disk the list is initialized during the low-level formatting at the factory and is updated over the life of the disk low-level formatting also sets aside spare sectors not visible to the operating system the controller can be told to replace each bad sector logically with one of the spare sectors this scheme is known as or a typical bad-sector transaction might be as follows  the operating system tries to read logical block 87 the controller calculates the ecc and finds that the sector is bad it reports this finding to the operating system the next time the system is rebooted  a special command is run to tell the scsi controller to replace the bad sector with a spare after that  whenever the system requests logical block 87  the request is translated into the replacement sector 's address by the controller note that such a redirection by the controller could invalidate any optimization by the operating system 's disk-scheduling algorithm ! for this reason  most disks are formatted to provide a few spare sectors in each cylinder and a spare cylinder as well when a bad block is remapped  the controller uses a spare sector from the same cylinder  if possible as an alternative to sector some controllers can be instructed to replace a bad block by here is an example  suppose that logical block 17 becomes defective and the first available spare follows sector 202 then  sector slipping remaps all the sectors front 17 to 202  moving them all down one spot that is  sector 202 is copied into the spare  then sector 201 into 202  then 200 into 201  and so on  until sector 18 is copied into sector 19 slipping the sectors in this way frees up the space of sector 18  so sector 17 can be mapped to it the replacement of a bad block generally is not totally automatic because the data in the bad block are usually lost soft errors may trigger a process in 520 chapter 12 12.6 which a copy of the block data is made and the block is spared or slipped an unrecoverable howeverf results in lost data whatever file was using th.at block must be repaired  for instancef by restoration from a backup tape  f and that requires manual intervention swapping was first presented in section 8.2f where we discussed moving entire processes between disk and main memory swapping in that setting occurs when the amount of physical memory reaches a critically low point and processes are moved from memory to swap space to free available memory in practicef very few modern operating systems implement swapping in this fashion rathel ~ systems now combine swapping with virtual memory techniques  chapter 9  and swap pagesf not necessarily entire processes in fact some systems now use the terms swapping and paging interchangeablyf reflecting the merging of these two concepts is another low-level task of the operating system virtual memory uses disk space as an extension of main memory since disk access is much slower than memory accessf using swap space significantly decreases system performance the main goal for the design and implementation of swap space is to provide the best throughput for the virtual memory system in this sectionf we discuss how swap space is usedf where swap space is located on diskf and how swap space is managed 12.6.1 swap-space use swap space is used in various ways by different operating systemsf depending on the memory-management algorithms in use for instancef systems that implement swapping may use swap space to hold an entire process imagef including the code and data segments paging systems may simply store pages that have been pushed out of main memory the amount of swap space needed on a system can therefore vary from a few megabytes of disk space to gigabytesf depending on the amow1.t of physical memoryf the amount of virtual memory it is backingf and the way in which the virtual memory is used note that it may be safer to overestimate than to underestimate the amount of swap space requiredf because if a system runs out of swap space it may be forced to abort processes or may crash entirely overestimation wastes disk space that could otherwise be used for filesf but it does no other harm some systems recommend the amount to be set aside for swap space solarisf for examplef suggests setting swap space equal to the amount by which virtual memory exceeds pageable physical memory in the past linux has suggested setting swap space to double the amount of physical memoryf although most linux systems now use considerably less swap space in factf there is currently much debate in the linux community about whether to set aside swap space at all ! some operating systems-including linux-allow the use of multiple swap spaces these swap spaces are usually put on separate disks so that the load placed on the i/0 system by paging and swapping can be spread over the systemfs i/o devices 12.6 521 12.6.2 swap-space location a swap space can reside in one of two places  it can be carved out of the normal file system  or it can be in a separate disk partition if the swap space is simply a large file within the file system  normal file-system routines can be used to create it  name it and allocate its space this approach  though easy to implement is inefficient navigating the directory structure and the diskallocation data structures takes time and  possibly  extra disk accesses external fragmentation can greatly increase swapping times by forcing multiple seeks during reading or writing of a process image we can improve performance by caching the block location information in physical memory and by using special tools to allocate physically contiguous blocks for the swap file  but the cost of traversing the file-system data structures remains alternatively  swap space can be created in a separate partition no file system or directory structure is placed in this space rather  a separate swap-space storage manager is used to allocate and deallocate the blocks from the raw partition this manager uses algorithms optimized for speed rather than for storage efficiency  because swap space is accessed much more frequently than file systems  when it is used   internal fragmentation may increase  but this trade-off is acceptable because the life of data in the swap space generally is much shorter than that of files in the file system since swap space is reinitialized at boot time  any fragmentation is short-lived the raw-partition approach creates a fixed amount of swap space during disk partitioning adding more swap space requires either repartitioning the disk  which involves moving the other file-system partitions or destroying them and restoring them from backup  or adding another swap space elsewhere some operating systems are flexible and can swap both in raw partitions and in file-system space linux is an example  the policy and implementation are separate  allowing the machine 's administrator to decide which type of swapping to use the trade-off is between the convenience of allocation and management in the file system and the performance of swapping in raw partitions 12.6.3 swap-space management  an example we can illustrate how swap space is used by following the evolution of swapping and paging in various unix systems the traditional unix kernel started with an implementation of swapping that copied entire processes between contiguous disk regions and memory unix later evolved to a combination of swapping and paging as pagiltg hardware became available in solaris 1  sunos   the designers changed standard unix methods to improve efficiency and reflect technological developments when a process executes  text-segment pages containing code are brought in from the file system  accessed in main memory  and thrown away if selected for pageout it is more efficient to reread a page from the file system than to write it to swap space and then reread it from there swap space is only used as a backing store for pages of memory  which includes memory allocated for the stack  heap  and uninitialized data of a process more changes were made in later versions of solaris the biggest change is that solaris now allocates swap space only when a page is forced out of physical memory  rather than when the virtual memory page is first created 522 chapter 12 12.7 swap partition or swap file swap map 1---------swap area--------1 page i slot -1 l ~  ~ ---_l __ _l _ ~ figure 12.10 the data structures for swapping on linux systems this scheme gives better performance on modern computers  which have more physical memory than older systems and tend to page less linux is similar to solaris in that swap space is only used for anonymous memory or for regions of memory shared by several processes linux allows one or more swap areas to be established a swap area may be in either a swap file on a regular file system or a raw-swap-space partition each swap area consists of a series of 4-kb which are used to hold swapped pages associated with each swap area is a .u1.2.p-an array of integer counters  each corresponding to a page slot in the swap area if the value of a counter is 0  the corresponding page slot is available values greater than 0 indicate that the page slot is occupied by a swapped page the value of the counter ij.l.dicates the number of mappings to the swapped page ; for example  a value of 3 indicates that the swapped page is mapped to three different processes  which can occur if the swapped page is storing a region of memory shared by three processes   the data structures for swapping on linux systems are shown in figure 12.10 disk drives have continued to get smaller and cheaper  so it is now economically feasible to attach many disks to a computer system having a large number of disks in a system presents opportunities for improving the rate at which data can be read or written  if the disks are operated in parallel furthermore  this setup offers the potential for improving the reliability of data storage  because redundant information can be stored on multiple disks thus  failure of one disk does not lead to loss of data a of disk-organization techniques  collectively called disks  raids   are commonly used to address the performance and reliability issues in the past  raids composed of small  cheap disks were viewed as a cost-effective alternative to large  expensive disks ; today  raids are used for their higher reliability and higher data-transfer rate  rather than for economic reasons hence  the i in raid  which once stood for inexpensive/ ' now stands for ij.l.dependent 12.7.1 improvement of reliability via redundancy let us first consider the reliability of raids the chance that some disk out of a set of n disks will fail is much higher than the chance that a specific single 12.7 523 structuring raid raid storage can be structured in a variety of ways for example  a system can have disks directly attached to its buses in this case  the operating system or system software can implement raid flmctionality alternatively  an intelligent host controller can control multiple attached disks and can implement raid on those disks in hardware finally  a  or can be used a raid array is a standalone unit with its own controller  cache  usually   and disks it is attached to the host via one or more standard ata scsi or fc controllers this common setup allows any operating system and software without raid functionality to have raid-protected disks it is even used on systems that do have raid software layers because of its simplicity and flexibility disk will fail suppose that the of a single disk is 100,000 hours then the mean time to failure of some disk in an array of 100 disks will be 100,000/100 = 1,000 hours  or 41.66 days  which is not long at all ! if we store only one copy of the data  then each disk failure will result in loss of a significant amount of data -and such a high rate of data loss is unacceptable the solution to the problem of reliability is to introduce  we store extra information that is not normally needed but that can be used in the event of failure of a disk to rebuild the lost information thus  even if a disk fails  data are not lost the simplest  but most expensive  approach to introducing redundancy is to duplicate every disk this technique is called with mirroring  a logical disk consists of two physical disks  and every write is carried out on both disks the result is called a mirrored volume if one of the disks in the volume fails  the data can be read from the other data will be lost only if the second disk fails before the first failed disk is replaced the mean time to failure of a mirrored volume-where failure is the loss of data depends on two factors one is the mean time to failure of the individual disks the other is the which is the time it takes  on average  to replace a failed disk and to restore the data on it suppose that the failures of the two disks are that is  the failure of one disk is not connected to the failure of the other then  if the mean time to failure of a single disk is 100,000 hours and the mean time to repair is 10 hours  the of a mirrored disk system is 100  0002 /  2 10  = 500 106 hours  or 57,000 years ! you should be aware that the assumption of independence of disk failures is not valid power failures and natural disasters  such as earthquakes  fires  and floods  may result in damage to both disks at the same time also  manufacturing defects in a batch of disks can cause correlated failures as disks age  the probability of failure grows  increasing the chance that a second disk will fail while the first is being repaired in spite of all these considerations  however  n1.irrored-disk systems offer much higher reliability than do singledisk systems power failures are a particular source of concern  since they occur far more frequently than do natural disasters even with mirroring of disks  if writes are 524 chapter 12 in progress to the same block in both disks  and power fails before both blocks are fully written  the two blocks can be in an inconsistent state one solution to this is to write one copy first then the next another is to add a cache to the raid array this write-back cache is protected from data loss during power failures  so the write can be considered complete at that point  assuming the nvram has some kind of error protection and correction  such as ecc or mirroring 12.7.2 improvement in performance via parallelism now let 's consider how parallel access to multiple disks improves performance with disk mirroring  the rate at which read requests can be handled is doubled  since read requests can be sent to either disk  as long as both disks in a pair are functionat as is almost always the case   the transfer rate of each read is the same as in a single-disk system  but the number of reads per unit time has doubled with multiple disks  we can improve the transfer rate as well  or instead  by striping data across the disks in its simplest form  consists of the bits of each byte across multiple disks ; such striping is called for example  if we have an array of eight disks  we write bit i of each byte to disk i the array of eight disks can be treated as a single disk with sectors that are eight times the normal size and  more important that have eight times the access rate in such an organization  every disk participates in every access  read or write  ; so the number of accesses that can be processed per second is about the same as on a single disk  but each access can read eight times as many data in the same time as on a single disk bit-level striping can be generalized to include a number of disks that either is a multiple of 8 or divides 8 for example  if we use an array of four disks  bits i and 4 + i of each go to disk i further  striping need not occur at the bit level in for instance  blocks of a file are striped across multiple disks ; with n disks  block i of a file goes to disk  i mod n  + 1 other levels of striping  such as bytes of a sector or sectors of a block  also are possible block-level striping is the most common parallelism in a disk system  as achieved through striping  has two main goals  increase the throughput of multiple small accesses  that is  page accesses  by load balancing reduce the response time of large accesses 12.7.3 raid levels mirroring provides high reliability  but it is expensive striping provides high data-transfer rates  but it does not improve reliability numerous schemes to provide redundancy at lower cost by using disk striping combined with parity bits  which we describe next  l1.ave been proposed these schemes have different cost-performance trade-offs and are classified according to levels called we describe the various levels here ; figure 12.11 shows them pictorially  in the figure  p indicates error-correcting bits  and c 12.7 raid structure 525  a  raid 0  non-redundant striping  b  raid 1  mirrored disks  c  raid 2  memory-style error-correcting codes  d  raid 3  bit-interleaved parity  e  raid 4  block-interleaved parity  f  raid 5  block-interleaved distributed parity  g  raid 6  p + q redundancy figure 12.11 raid levels indicates a second copy of the data   in all cases depicted in the figure  four disks ' worth of data are stored  and the extra disks are used to store redundant information for failure recovery raid level 0 raid level 0 refers to disk arrays with striping at the level of blocks but without any redundancy  such as mirroring or parity bits   as shown in figure 12.1l  a   raid ievell raid level1 refers to disk mirroring figure 12.1l  b  shows a mirrored organization ' raid level2 raid level2 is also known as memory-style error-correctingcode  ecc  organization memory systems have long detected certain errors by using parity bits each byte in a memory system may have a parity bit associated with it that records whether the number of bits in the byte set to 1 is even  parity = 0  or odd  parity = 1   if one of the bits in the 526 chapter 12 byte is damaged  either a 1 becomes a 0  or a 0 becomes an the parity of the byte changes and thus does not match the stored parity similarly  if the stored parity bit is damaged  it does not match the computed parity thus  all single-bit errors are detected by the menwry system  error-correcting schemes store two or more extra bits and can reconstruct the data if a single bit is damaged the idea of ecc can be used directly in disk arrays via striping of bytes across disks for example  the first bit of each byte can be stored in disk 1  the second bit in disk 2  and so on until the eighth bit is stored in disk 8 ; the error-correction bits are stored in further disks this scheme is shown pictorially in figure 12.1l  c   where the disks labeled p store the error-correction bits if one of the disks fails  the remaining bits of the byte and the associated error-correction bits can be read from other disks and used to reconstruct the damaged data note that raid level 2 requires only three disks ' overhead for four disks of data  unlike raid level 1  which requires four disks ' overhead raid level 3 raid level 3  or improves on level 2 by taking into account the fact that  unlike memory systems  disk controllers can detect whether a sector has been read correctly  so a single parity bit can be used for error correction as well as for detection the idea is as follows  if one of the sectors is damaged  we know exactly which sector it is  and we can figure out whether any bit in the sector is a 1 or a 0 by computing the parity of the corresponding bits from sectors in the other disks if the parity of the remaining bits is equal to the stored parity  the missing bit is 0 ; otherwise  it is 1 raid level3 is as good as level 2 but is less expensive in the number of extra disks required  it has only a one-disk overhead   so level 2 is not used in practice this scheme is shown pictorially in figure 12.1l  d   raid level 3 has two advantages over level 1 first  the storage overhead is reduced because only one parity disk is needed for several regular disks  whereas one mirror disk is needed for every disk in level1 second  since reads and writes of a byte are spread out over multiple disks with n-way striping of data  the transfer rate for reading or writing a single block is n times as fast as with raid level 1 on the negative side  raid level3 supports fewer l/os per second  since every disk has to participate in every i/0 request a further performance problem with raid 3-and with all paritybased raid levels-is the expense of computing and writing the parity this overhead results in significantly slower writes than with non-parity raid arrays to moderate this performance penalty  many raid storage arrays include a hardware controller with dedicated parity hardware this controller offloads the parity computation from the cpu to the array the array has an nvram cache as well  to store the blocks while the parity is computed and to buffer the writes from the controller to the spindles this combination can make parity raid almost as fast as non-parity in fact  a caching array doing parity raid can outperform a non-caching non-parity raid raid level 4 raid level4  or uses block-level striping  as in raid 0  and in addition keeps a parity block on a separate disk for corresponding blocks from n other disks this scheme is 12.7 527 diagramed in figure 12.1l  e   if one of the disks fails  the parity block can be used with the corresponding blocks from the other disks to restore the blocks of the failed disk a block read accesses only one disk  allowing other requests to be processed by the other disks thus  the data-transfer rate for each access is slowe1 ~ but multiple read accesses can proceed in parallel  leading to a higher overall i/0 rate the transfer rates for large reads are high  since all the disks can be read in parallel ; large writes also have high transfer rates  since the data and parity can be written in parallel small independent writes can not be performed in parallel an operatingsystem write of data smaller than a block requires that the block be read  modified with the new data  and written back the parity block has to be updated as well this is known as the syti  .e  thus  a single write requires four disk accesses  two to read the two old blocks and two to write the two new blocks wafl  chapter 11  uses raid level4 because this raid level allows disks to be added to a raid set seamlessly if the added disks are initialized with blocks containing all zeros  then the parity value does not change  and the raid set is still correct raid levels raid levels  or  differs from level 4 by spreading data and parity among all n + 1 disks  rather than storing data in n disks and parity in one disk for each block  one of the disks stores the parity  and the others store data for example  with an array of five disks  the parity for the nth block is stored in disk  n mod 5  + 1 ; the nth blocks of the other four disks store actual data for that block this setup is shown in figure 12.11  f   where the ps are distributed across all the disks a parity block can not store parity for blocks in the same disk  because a disk failure would result in loss of data as well as of parity  and hence the loss would not be recoverable by spreading the parity across all the disks in the set  raid 5 avoids potential overuse of a single parity disk  which can occur with raid 4 raid 5 is the most common parity raid system raid level 6 raid level 6  also called the is much like raid level 5 but stores extra redundant information to guard against disk failures instead of parity  error-correcting codes such as the are used in the scheme shown in figure 12.11  g   2 bits of redundant data are stored for every 4 bits of datacompared with 1 parity bit in level 5-and the system can tolerate two disk failures raid levels 0 + 1 and 1 + 0 raid level 0 + 1 refers to a combination of raid levels 0 and 1 raid 0 provides the performance  while raid 1 provides the reliability generally  this level provides better performance than raid 5 it is common in enviromnents where both performance and reliability are important unfortunately  like raid 1  it doubles the number of disks needed for storage  so it is also relatively expensive in raid 0 + 1  a set of disks are striped  and then the stripe is mirrored to another  equivalent stripe 528 chapter 12 stripe a  raid 0 + 1 with a single disk failure ua mirror b  raid 1 + 0 with a single disk failure figure 12.12 raid 0 + 1 and 1 + 0 another raid option that is becoming available commercially is raid level 1 + 0  in which disks are mirrored in pairs and then the resulti.j.l.g mirrored pairs are striped this scheme has some theoretical advantages over raid 0 + 1 for example  if a single disk fails in raid 0 + 1  an entire stripe is inaccessible  leaving only the other stripe available with a failure in raid 1 + 0  a single disk is unavailable  but the disk that mirrors it is still available  as are all the rest of the disks  figure 12.12   numerous variations have been proposed to the basic raid schemes described here as a result  some confusion may exist about the exact definitions of the different raid levels the implementation of raid is another area of variation consider the following layers at which raid can be implemented volume-management software can implement raid within the kernel or at the system software layer in this case  the storage hardware can provide a minimum of features and still be part of a full raid solution parity raid is fairly slow when implemented in software  so typically raid 0  1  or 0 + 1 is used raid can be implemented in the host bus-adapter  hba  hardware only the disks directly connected to the hba can be part of a given raid set this solution is low in cost but not very flexible 12.7 529 raid can be implemented in the hardware of the storage array the storage array can create raid sets of various levels and can even slice these sets into smaller volumes  which are then presented to the operating system the operating system need only implement the file system on each of the volumes arrays can have multiple connections available or can be part of a san  allowing multiple hosts to take advantage of the array 's features raid can be implemented in the san interconnect layer by disk virtualization devices in this case  a device sits between the hosts and the storage it accepts commands from the servers and manages access to the storage it could provide mirroring  for example  by writing each block to two separate storage devices other features  such as and replication  can be implemented at each of these levels as well involves the automatic duplication of writes between separate sites for redundancy and disaster recovery replication can be synchronous or asynchronous in synchronous replication  each block must be written locally and remotely before the write is considered complete  whereas in asynchronous replication  the writes are grouped together and written periodically asynchronous replication can result in data loss if the primary site fails  but it is faster and has no distance limitations the implementation of these features differs depending on the layer at which raid is implemented for example  if raid is implemented in software  then each host may need to carry out and manage its own replication if replication is implemented in the storage array or in the san intercom1ect  however  then whatever the host operating system or its features  the host 's data can be replicated one other aspect of most raid implementations is a hot spare disk or disks a is not used for data but is configured to be used as a replacement in case disk failure for instance  a hot spare can be used to rebuild a mirrored pair should one of the disks in the pair fail in this way  the raid level can be reestablished automatically  without waiting for the failed disk to be replaced allocating more than one hot spare allows more than one failure to be repaired without human intervention 12.7.4 selecting a raid level given the many choices they have  how do system designers choose a raid level one consideration is rebuild performance if a disk fails  the time needed to rebuild its data can be significant this may be an important factor if a continuous supply of data is required  as it is in high-performance or interactive database systems furthermore  rebuild performance influences the mean time to failure rebuild performance varies with the raid level used rebuilding is easiest or raid level1  since data can be copied from another disk ; for the other levels  we need to access all the other disks in the array to rebuild data in a failed disk rebuild times can be hours for raid 5 rebuilds of large disk sets raid level 0 is used in high-performance applications where data loss is not critical raid level1 is popular for applications that require high reliability with fast recovery raid 0 + 1 and 1 + 0 are used where both performance and reliability are important-for example  for small databases due to raid 1 's 530 chapter 12 the inserv storage array im1ovation  in an effort to provide better  faster  and less expensive solutions  frequently blurs the lines that separated previous technologies consider the inserv storage array from 3par unlike most other storage arrays  inserv does not require that a set of disks be configured at a specific raid level rather  each disk is broken into 256-mb chunklets ram is then applied at the chunklet level a disk can thus participate in multiple and various raid levels as its chunklets are used for multiple volumes inserv also provides snapshots similar to those created by the wafl file system the format of inserv snapshots can be read-write as well as readonly  allowing multiple hosts to mount copies of a given file system without needing their own copies of the entire file system any changes a host makes in its own copy are copy-on-write and so are not reflected in the other copies a further innovation is  some file systems do not expand or shrink on these systems  the original size is the only size  and any change requires copying data an administrator can configure inserv to provide a host with a large amount of logical storage that initially occupies only a small amount of physical storage as the host starts using the storage  unused disks are allocated to the host  up to the original logical level the host thus can believe that it has a large fixed storage space  create its file systems there  and so on disks can be added or removed from the file system by inserv without the file systems noticing the change this feature can reduce the number of drives needed by hosts  or at least delay the purchase of disks until they are really needed high space overhead  raid levels is often preferred for storing large volumes of data level6 is not supported currently by many raid implementations  but it should offer better reliability than levels raid system designers and administrators of storage have to make several other decisions as well for example  how many disks should be in a given raid set how many bits should be protected by each parity bit if more disks are in an array  data-transfer rates are higher  but the system is more expensive if more bits are protected by a parity bit  the space overhead due to parity bits is lower  but the chance that a second disk will fail before the first failed disk is repaired is greater  and that will result in data loss 12.7.5 extensions the concepts of raid have been generalized to other storage devices  including arrays of tapes  and even to the broadcast of data over wireless systems when applied to arrays of tapes  raid structures are able to recover data even if one of the tapes in an array is damaged when applied to broadcast of data  a block of data is split into short units and is broadcast along with a parity unit ; if one of the units is not received for any reason  it can be reconstructed from the other units comrnonly  tape-drive robots containing multiple tape drives will stripe data across all the drives to increase throughput and decrease backup time 12.7 531 12.7.6 problems with raid unfortunately  raid does not always assure that data are available for the operating system and its users a pointer to a file could be wrong  for example  or pointers within the file structure could be wrong incomplete writes  if not properly recovered  could result in corrupt data some other process could accidentally write over a file system 's structures  too raid protects against physical media errors  but not other hardware and software errors as large as is the landscape of software and hardware bugs  that is how numerous are the potential perils for data on a system the solaris zfs file system takes an innovative approach to solving these problems through the use of  a technique which is used to verify the integrity of data zfs maintains internal checksums of all blocks  including data and metadata these checksums are not kept with the block that is being checksummed rathel ~ they are stored with the pointer to that block  see figure 12.13  consider an inode with pointers to its data within the inode is the checksum of each block of data if there is a problem with the data  the checksum will be incorrect and the file system will know about it if the data are mirrored  and there is a block with a correct checksum and one with an incorrect checksum  zfs will automatically update the bad block with the good one similarly  the directory entry that points to the inode has a checksum for the inode any problem in the inode is detected when the directory is accessed this checksumming takes places throughout all zfs structures  providing a much higher level of consistency  error detection  and error correction than is found in raid disk sets or standard file systems the extra overhead that is created by the checksum calculation and extra block read-modify-write cycles is not noticeable because the overall performance of zfs is very fast another issue with most raid implementations is lack of flexibility consider a storage array with twenty disks divided into four sets of five disks each set of five disks is a raid level 5 set as a result  there are four separate data 1 figure 12.13 zfs checksums all metadata and data 532 chapter 12 volumes  each holding a file system but what if one file system is too large to fit on a five-disk raid level 5 set and what if another file system needs very little space if such factors are known ahead of time  then the disks and volumes can be properly allocated very frequently  however  disk use and requirements change over time even if the storage array allowed the entire set of twenty disks to be created as one large raid set other issues could arise several volumes of various sizes could be built on the set but some volume managers do not allow us to change a volume 's size in that case  we would be left with the same issue described above-mismatched file-system sizes some volume n lanagers allow size changes  but some file systems do not allow for file-system growth or shrinkage the volumes could change sizes  but the file systems would need to be recreated to take advantage of those changes zfs combines file-system management and volume management into a unit providing greater functionality than the traditional separation of those functions allows disks  or partitions of disks  are gathered together via raid sets into of storage a pool can hold one or more zfs file systems the entire pool 's free space is available to all file systems within that pool zfs uses the memory model of malloc and free to allocate and release storage for each file system as blocks are used and freed within the file system as a result there are no artificial limits on storage use and no need to relocate file systems between volumes or resize volumes zfs provides quotas to limit the size of a file system and reservations to assure that a file system can grow by a specified amount  but those variables may be changed by the file system owner at any time figure 12.14  a  depicts traditional volumes and file systems  and figure 12.14  b  shows the zfs model i fs i ~  a  traditional volumes and file systems  b  zfs and pooled storage figure 12.14  a  traditional volumes and file systems  b  a zfs pool and file systems 12.8 12.8 533 in chapter 6  we introduced the write-ahead log  which requires the availability of stable storage by definition  information residing in stable storage is never lost to implement such storage  we need to replicate the required information on multiple storage devices  usually disks  with independent failure modes we also need to coordinate the writing of updates in a way that guarantees that a failure during an update will not leave all the copies in a damaged state and that  when we are recovering from a failure  we can force all copies to a consistent and correct value  even if another failure occurs during the recovery in this section  we discuss how to meet these needs a disk write results in one of three outcomes  successful completion the data were written correctly on disk partial failure a failure occurred in the midst of transfer  so only some of the sectors were written with the new data  and the sector being written during the failure may have been corrupted total failure the failure occurred before the disk write started  so the previous data values on the disk remain intact whenever a failure occurs during writing of a block  the system needs to detect it and invoke a recovery procedure to restore the block to a consistent state to do that  the system must maintain two physical blocks for each logical block an output operation is executed as follows  write the information onto the first physical block when the first write completes successfully  write the same injormation onto the second physical block declare the operation complete only after the second write completes successfully during recovery from a failure  each pair of physical blocks is examined if both are the same and no detectable error exists  then no further action is necessary if one block contains a detectable error  then we replace its contents with the value of the other block if neither block contains a detectable error  but the blocks differ in content  then we replace the content of the first block with that of the second this recovery procedure ensures that a write to stable storage either succeeds completely or results in no change we can extend this procedure easily to allow the use of an arbitrarily large number of copies of each block of stable storage although having a large number of copies further reduces the probability of a failure  it is usually reasonable to simulate stable storage with only two copies the data in stable storage are guaranteed to be safe unless a failure destroys all the copies because waiting for disk writes to complete  synchronous i/o  is time consuming  many storage arrays add nvram as a cache since the memory is nonvolatile  it usually has battery power to back up the unit 's power   it can be trusted to store the data en route to the disks it is thus considered part of 534 chapter 12 12.9 the stable storage writes to it are much faster than to disk  so performance is greatly improved would you buy a dvd or cd player that had one disk sealed inside of course not you expect to use a dvd or cd player with many relatively inexpensive disks on a computer as well  using many inexpensive cartridges with one drive lowers the overall cost low cost is the defining characteristic of tertiary storage  which we discuss in this section 12.9.1 tertiary-storage devices because cost is so important  in practice  tertiary storage is built with the most common examples are floppy disks  tapes  and read-only  write-once  and rewritable cds and dvds many any other kinds of tertiarystorage devices are available as well  including removable devices that store data in flash memory and interact with the computer system via a usb interface 12.9.1.1 removable disks removable disks are one kind of tertiary storage floppy disks are an example of removable magnetic disks they are made from a thin  flexible disk coated with magnetic material and enclosed in a protective plastic case although common floppy disks can hold only about 1 mb  similar technology is used for removable magnetic disks that hold more than 1 gb removable magnetic disks can be nearly as fast as hard disks  although the recording stuface is at greater risk of from scratches a is another kind of removable disk it records data on a rigid platter coated with magnetic material  but the recording technology is quite different from that for a magnetic disk the magneto-optic head flies much farther from the disk surface than a magnetic disk head does  and the magnetic material is covered with a thick protective layer of plastic or glass this arrangement makes the disk much more resistant to head crashes the magneto-optic disk drive has a coil that produces a magnetic field ; at room temperature  the field is too large and too weak to magnetize a bit on the disk to write a bit  the disk head flashes a laser beam at the disk surface the laser is aimed at a tiny spot where a bit is to be written the laser heats this spot  which makes the spot susceptible to the magnetic field now the large  weak magnetic field can record a tiny bit the magneto-optic head is too far from the disk surface to read the data by detecting the tiny magnetic fields in the way that the head of a hard disk does instead  the drive reads a bit using a property of laser light called the when a laser beam is bounced off of a magnetic spot  the polarization of the laser beam is rotated clockwise or counterclockwise  dependin ~ g on the orientation of the magnetic field this rotation is what the head detects to read a bit another category of removable disk is the optical disks do not use magnetism at all instead  they use special materials that can be altered by laser light to have relatively dark or bright spots one exarnple of optical-disk 12.9 535 technology is the which is coated with a material that can freeze into either a crystalline or an amorphous state the crystalline state is more transparent  and hence a laser beam is brighter when it passes through the ltlaterial and bounces off the reflective layer the phase-change drive uses laser light at three different powers  low power to read data  medium power to erase the disk by melting and refreezing the recording medium into the crystalline state  and high power to melt the medium into the amorphous state to write to the disk the most common examples of this technology are the re-recordable cd-rw and dvd-rw the kinds of disks just described can be used over and over they are called in contrast  can be written only once an old way to make a worm disk is to manufacture a thin aluminum film sandwiched between two glass or plastic platters to write a bit  the drive uses a laser light to burn a small hole through the aluminum this burning can not be reversed although it is possible to destroy the information on a worm disk by burning holes everywhere  it is virtually impossible to alter data on the disk  because holes can only be added  and the ecc code associated with each sector is likely to detect such additions worm disks are considered durable and reliable because the metal layer is safely encapsulated between the protective glass or plastic platters and magnetic fields can not damage the recording a newer write-once technology records on an organic polymer dye instead of an aluminum layer ; the dye absorbs laser light to form marks this technology is used in the recordable cd-r and dvd-r read-oniv such as cd-rom and dvd-rom  come from the factory with the data prerecorded they use technology similar to that of worm disks  although the bits are pressed  not burned   and they are very durable most removable disks are slower than their nonremovable counterparts the writing process is slower  as are rotation and sometimes seek time 12.9.1.2 tapes magnetic tape is another type of removable medium as a general rule  a tape holds more data than an optical or magnetic disk cartridge tape drives and disk drives have similar transfer rates but random access to tape is much slower than a disk seek  because it requires a fast-forward or rewind operation that takes tens of seconds or even minutes although a typical tape drive is more expensive than a typical disk drive  the price of a tape cartridge is lower than the price of the equivalent capacity of magnetic disks so tape is an economical medium for purposes that do not require fast random access tapes are commonly used to hold backup copies of disk data they are also used in large supercomputer centers to hold the enornwus volumes of data used in scientific research and by large commercial enterprises large tape installations typically use robotic tape changers that move tapes between tape drives and storage slots in a tape library these mechanisms give the computer automated access to many tape cartridges a robotic tape library can lower the overall cost of data storage a diskresident file that will not be needed for a while can be to tape  where the cost per gigabyte is lower ; if the file is needed in the future  the computer can it back into disk storage for active use a robotic tape library is 536 chapter 12 sometimes called storage  since it is between the high performance of on-line magnetic disks and the low cost of off-line tapes sitting on shelves in a storage room 12.9.1.3 future technology in the future  other storage technologies may become important sometimes old technologies are used in new ways  as economics change or the technologies evolve for example  solid-state disks  or are growing in importance and becoming more common simply described  an ssd is a disk that is used like a hard drive depending on the memory technology used  it can be volatile or nonvolatile the memory technology also affects performance nonvolatile ssds have the same characteristics as traditional hard disks but can be more reliable because they have no moving parts and faster because they have no seek time or latency in addition  they use less energy however  they are more expensive per megabyte than traditional hard disks  have lower capacity than the larger hard disks  and may have shorter life-spans than hard disks ; so their uses are limited in one example  ssds are being used in storage arrays to hold metadata which requires high-performance such as the journal of a journaling file system ssds are also being added to notebook computers to make them smaller  faster  and more energy efficient another promising storage technology  bologt ; ;  phk uses laser light to record holographic photographs on special media we can think of a hologram as a three-dimensional array of pixels each pixel represents one bit  0 for black or 1 for white and all the pixels in a hologram are transferred in one flash of laser light  so the data transfer rate is extremely high with continued development  holographic storage may become commercially viable another technology under active research is based on  iv ! e \ 1s   the idea is to apply the fabrication technologies that produce electronic chips to the manufacture of small datastorage machines one proposal calls for the fabrication of an array of 10,000 tiny disk heads  with a square centimeter of magnetic storage material suspended above the array when the storage material is moved lengthwise over the heads  each head accesses its own linear track of data on the material the storage material can be shifted sideways slightly to enable all the heads to access their next track although it remains to be seen whether this technology can be successful  it may provide a nonvolatile data-storage technology that is faster than magnetic disk and cheaper than semiconductor dram whether the storage medium is a removable magnetic disk  a dvd  or a magnetic tape  the operating system needs to provide several capabilities to use removable media for data storage these capabilities are discussed in section 12.9.2 12.9.2 operating-system support two major jobs of an operating system are to manage physical devices and to present a virtual machine abstraction to applications in this chapter  we have seen that  for hard disks  the operating system provides two abstractions one is the raw device  which is just an array of data blocks the other is a file system for a file system on a magnetic disk  the operating system queues and 12.9 537 schedules the interleaved requests from several applications now  we shall see how the operating system does its job when the storage media are removable 12.9.2.1 application interface most operating systems can handle removable disks almost exactly as they do fixed disks when a blank cartridge is inserted into the drive  or mounted   the cartridge must be formatted  and then an empty file system is generated on the disk this file system is used just like a file system on a hard disk tapes are often handled differently the operating system usually presents a tape as a raw storage medium an application does not open a file on the tape ; it opens the whole tape drive as a raw device usually  the tape drive is then reserved for the exclusive use of that application until the application exits or closes the tape device this exclusivity makes sense  because random access on a tape can take tens of seconds  or even a few minutes  so interleaving random accesses to tapes from more than one application would be likely to cause thrashing when the tape drive is presented as a raw device  the operating system does not provide file-system services the application must decide how to use the array of blocks for instance  a program that backs up a hard disk to tape might store a list of file names and sizes at the beginning of the tape and then copy the data of the files to the tape in that order it is easy to see the problems that can arise from this way of using tape since every application makes up its own rules for how to organize a tape  a tape full of data can generally be used only by the program that created it for instance  even if we know that a backup tape contains a list of file names and file sizes followed by the file data  we will still find it difficult to use the tape how exactly are the file names stored are the file sizes in binary or ascii form are the files written one per block  or are they all concatenated in one tremendously long string of bytes we do not even know the block size on the tape  because this variable is generally one that can be chosen separately for each block written for a disk drive  the basic operations are read    write    and seek    tape drives have a different set of basic operations instead of seek    a tape drive uses the locate   operation the tape locate   operation is more precise than the disk seek   operation  because it positions the tape to a specific logical block  rather than an entire track locating to block 0 is the same as rewinding the tape for most kinds of tape drives  it is possible to locate to any block that has been written on a tape in a partly filled tape  however  it is not possible to locate into the empty space beyond the written area  because most tape drives do not manage their physical space in the same way disk drives do for a disk drive  the sectors have a fixed size  and the formatting process must be used to place empty sectors in their final positions before any data can be written most tape drives have a variable block size  and the size of each block is detern ined on the fly  when that block is written if an area of defective tape is encountered during writing  the bad area is skipped and the block is written again this operation explains why it is not possible to locate into the empty space beyond the written area -the positions and numbers of the logical blocks have not yet been detennined 538 chapter 12 most tape drives have a read_position   operation that returns the logical block number where the tape head is currently located many tape drives also support a space   operation for relative motion so  for example  the operation space  -2  would locate backward over two logical blocks for most kinds of tape drives  writing a block has the side effect of logically erasing everything beyond the position of the write in practice  this side effect means that most tape drives are append-only devices  because updating a block in the middle of the tape also effectively erases everything beyond that block the tape drive implements this appending by placing an end-of-tape  eot  mark after a block that is written the drive refuses to locate past the eot mark  but it is possible to locate to the eot and then start writing doing so overwrites the old eot mark and places a new one at the end of the new blocks just written in principle  a file system can be implemented on a tape but many of the file-system data structures and algorithms would be different from those used for disks  because of the append-only property of tape 12.9.2.2 file naming another question that the operating system needs to handle is how to name files on removable media for a fixed disk  naming is not difficult on a pc  the file name consists of a drive letter followed by a path name in unix  the file name does not contain a drive letter  but the molmt table enables the operating system to discover on what drive the file is located if the disk is removable  however  knowing what drive contained the cartridge at some time in the past does not mean knowing how to find the file if every removable cartridge in the world had a different serial number  the name of a file on a removable device could be prefixed with the serial number  but to ensure that no two serial numbers are the same would require each one to be about 12 digits in length who could remember the names of her files if she had to memorize a 12-digit serial number for each one the problem becomes even more difficult when we want to write data on a removable cartridge on one computer and then use the cartridge in another computer if both machines are of the same type and have the same kind of removable drive  the only difficulty is knowing the contents and data layout on the cartridge but if the machines or drives are different  many additional problems can arise even if the drives are compatible  different computers may store bytes in different orders and may use different encodings for binary numbers and even for letters  such as ascii on pcs versus ebcdic on mainframes   today 's operating systems generally leave the name-space problem unsolved for removable media and depend on applications and users to figure out how to access and interpret the data fortunately  a few kinds of removable media are so well standardized that all computers use them the same way one example is the cd music cds use a universal format that is understood by any cd drive data cds are available in only a few different formats  so it is usual for a cd drive and the operating-system device driver to be programmed to handle all the comn1on formats dvd fonnats are also well standardized 12.9 539 12.9.2.3 hierarchical storage management a ju enables the computer to change the removable cartridge in a tape or disk drive without human assistance two major uses of this technology are for backups and hierarchical storage systems the use of a jukebox for backups is simple  when one cartridge becomes full  the computer instructs the jukebox to switch to the next cartridge some jukeboxes hold tens of drives and thousands of cartridges  with robotic arms managing the movement of tapes to the drives a hierarchical storage system extends the storage hierarchy beyond primary memory and secondary storage  that is  magnetic disk  to incorporate tertiary storage tertiary storage is usually implemented as a jukebox of tapes or removable disks this level of the storage hierarchy is larger  cheaper  and slower although the virtual memory system can be extended in a straightforward manner to tertiary storage  this extension is rarely carried out in practice the reason is that a retrieval from a jukebox can take tens of seconds or even minutes  and such a long delay is intolerable for demand paging and for other forms of virtual memory use the usual way to incorporate tertiary storage is to extend the file system small and frequently used files remain on magnetic disk  while large and old files that are not actively used are archived to the jukebox in some file-archiving systems  the directory entry for the file continues to exist  but the contents of the file no longer occupy space in secondary storage if an application tries to open the file  the open   system call is suspended until the file contents can be staged in from tertiary storage when the contents are again available from magnetic disk  the open   operation returns control to the application  which proceeds to use the disk-resident copy of the data today  is usually found in installations that have large volumes of data that are used seldom  sporadically  current work in hsm includes extending it to provide full here  data move from disk to tape and back to disk  as needed  but are deleted on a schedule or according to policy for example  some sites save e-mail for seven years but want to be sure that at the end of seven years it is destroyed at that point  the data might be on disk  hsm tape  and backup tape ilm centralizes knowledge of where the data are so that policies can be applied across all these locations 12.9.3 performance issues as with any component of the operating system  the three most important aspects of tertiary-storage performance are speed  reliability  and cost 12.9.3.1 speed the speed of tertiary storage has two aspects  bandwidth and latency we measure the bandwidth in bytes per second the ~ 'l ' is the average data rate during a transfer-that is  the number of bytes divided by the transfer time the calculates the average over the entire l/0 time  including the time for seek   or locate   and any 540 chapter 12 cartridge-switching time in a jukebox in essence  the sustained bandwidth is the rate at which the data stream actually flows  and the effective bandwidth is the overall data rate provided by the drive the bandwidth of a drive is generally understood to mean the sustained bandwidth for removable disks  tlce bandwidth ranges from a few megabytes per second for the slowest to over 40 mb per second for the fastest tapes have a similar range of bandwidths  from a few megabytes per second to over 30mb per second the second aspect of speed is the  by this performance measure  disks are much faster than tapes disk storage is essentially twodimensional all the bits are out in the open a disk access simply moves the ann to the selected cylinder and waits for the rotational latency  which may take less than 5 milliseconds by contrast  tape storage is three-dimensional at any time  a small portion of the tape is accessible to the head  whereas most of the bits are buried below hundreds or thousands of layers of tape wound on the reel a random access on tape requires winding the tape reels until the selected block reaches the tape head  which can take tens or hundreds of seconds so we can generally say that random access within a tape cartridge is more than a thousand times slower than random access on disk if a jukebox is involved  the access latency can be significantly higher for a removable disk to be changed  the drive must stop spinning  then the robotic arm must switch the disk cartridges  and then the drive must spin up the new cartridge this operation takes several seconds-about a hundred times longer than the random-access time within one disk so switching disks in a jukebox incurs a relatively high performance penalty for tapes  the robotic-ann time is about the same as for disks but for tapes to be switched  the old tape generally must rewind before it can be ejected  and that operation can take as long as 4 minutes and  after a new tape is loaded into the drive  many seconds can be required for the drive to calibrate itself to the tape and to prepare for i/0 although a slow tape jukebox can have a tape-switch time of 1 or 2 minutes  this time is not enormously greater than the random-access time within one tape to generalize  we can say that random access in a disk jukebox has a latency of tens of seconds  whereas random access in a tape jukebox has a latency of hundreds of seconds ; switching tapes is expensive  but switching disks is not we must be careful not to overgeneralize  though some expensive tape jukeboxes can rewind  eject  load a new tape  and fast-forward to a random item of data all in less than 30 seconds if we pay attention to only the performance of the drives in a jukebox  the bandwidth and latency seem reasonable but if we focus our attention on the cartridges instead  we find a terrible bottleneck consider first the bandwidth the bandwidth-to-storage-capacity ratio of a robotic library is much less favorable than that of a fixed disk to read all the data stored on a large hard disk could take about an hour to read all the data stored in a large tape library could take years the situation with respect to access latency is nearly as bad to illustrate  if 100 requests are queued for a disk drive  the average waiting time will be about a second if 100 requests are queued for a tape library  the average waiting time could be over an hour the low cost of tertiary storage results from having many cheap cartridges share a few expensive drives but a removable library is best devoted to the storage of 12.9 541 infrequently used data  because the library can satisfy only a relatively small number of i/0 requests per hour 12.9.3.2 reliability although we often think good pe1jormance means high speed  another important aspect of performance is reliability if we try to read some data and are unable to do so because of a drive or media failure  for all practical purposes the access time is infinitely long and the bandwidth is infinitely small so it is important to understand the reliability of removable media removable n1.ag  netic disks are somewhat less reliable than are fixed hard disks  because they are more likely to be exposed to harmful environmental conditions such as dust  large changes in temperature and humidity  and mechanical forces such as shock and bending optical disks are considered very reliable  because the layer that stores the bits is protected by a transparent plastic or glass layer the reliability of magnetic tape varies widely  depending on the kind of drive some inexpensive drives wear out tapes after a few dozen uses ; other drives are gentle enough to allow millions of reuses by comparison with a magnetic-disk head  the head in a magnetic-tape drive is a weak spot a disk head flies above the media  but a tape head is in close contact with the tape the scrubbing action of the tape can wear out the head after a few thousands or tens of thousands of hours in summary  we can say that a fixed-disk drive is likely to be more reliable than a removable-disk or tape drive  and an optical disk is likely to be more reliable than a magnetic disk or tape but a fixed magnetic disk has one weakness a head crash in a hard disk generally destroys the data  whereas the failure of a tape drive or optical-disk drive often leaves the data cartridge unharmed 12.9.3.3 cost storage cost is another important factor here is a concrete example of how removable media may lower the overall storage cost suppose that a hard disk that holds x gb has a price of $ 200 ; of this amom1.t  $ 190 is for the housing  motor  and controller  and $ 10 is for the magnetic platters the storage cost for this disk is $ 200/ x per gigabyte now  suppose that we can manufacture the platters in a removable cartridge for one drive and 10 cartridges  the total price is $ 190 + $ 100  and the capacity is lox gb  so the storage cost is $ 291 x per gigabyte even if it is a little more expensive to make a removable cartridge  the cost per gigabyte of removable storage may well be lower than the cost per gigabyte of a hard disk  because the expense of one drive is averaged with the low price of many removable cartridges figures 12.15  12.16  and 12.17 show cost trends per megabyte for dram memory  magnetic hard disks  and tape drives the prices in the graphs are the lowest prices found in advertisements in various computer magazines and on the world wide web at the end of each year these prices reflect the smallcomputer marketplace of the readership of these magazines  where prices are low by comparison with the mainframe and minicomputer markets in the case of tape  the price is for a drive with one tape the overall cost of tape storage becomes much lower as more tapes are purchased for use with the drive  542 chapter 12 co   ; ; ; &  5 160 80 40 20 10 1.2 0.8 0.4 64 kb 32 128mb 512mb 2gb 0 02 --'-c-19 = '  ,8.,.-2 -1 98-cc4  -19 = '  c8 = -6 -1 ~ 9 '     88---,19 = '  c9 = -o -1c  '91 '     92  1 c  '  99-4 -c  c19l96---,19c  '  9 = -8 -2.,-,o '     oo  2c  '  oo = -2 -2      0l.04     2c  '  oo = -6   '2oos year figure 12 15 price per megabyte of dram  from 1981 to 2008 because the price of a tape is a small fraction of the price of the drive however  in a huge tape library containing thousands of cartridges  the storage cost is dominated by the cost of the tape cartridges as of 2004  the cost per gb of tape cartridges was around $ .40 as figure 12.15 shows  the cost of dram fluctuates widely in the period from 1981 to 2004  we can see three price crashes  around 1981  1989  and 1996  as excess production caused a glut in the marketplace we can also see two periods  around 1987 and 1993  where shortages in the marketplace caused sigrtificant price increases in the case of hard disks  figure 12.16   the price decline has been steadier tape-drive prices also fell steadily up to 1997  figure 12.17   since 1997  the price per gigabyte of inexpensive tape drives has ceased its dramatic fall  although the price of mid-range tape technology  such as dat /dds  has continued to fall and is now approaching that of the co   ; ; ; ' 100 50 20 5 2 0.5 0.2 0.05 0.02 0.004 0.001 0.0005 0.0002 10 20 1982 1984 1986 120 1.2 2 1988 1990 1992 19 gb gb gb 1994 1996 1998 2000 2002 2004 2006 2008 year figure 12.16 price per megabyte of magnetic hard disk  from 1981 to 2008 12.10 oj ~ 12.10 40 20 8 60 120 1.2 0.5 0.1 72gb 0.025 320gb 0.01 320gb 0.0051 ~ 9c-c84-1  l98  -6 -19  '  -88c---cc19 ~ 90c  c19 ~ 92  --c-c19l94-c-c19l96,---.,-c19'cc98-2,-jooc-c0--c2,-j.00,-,-2  ~   .,j = ~ 2008 year figure 12.17 price per megabyte of a tape drive  from 1984 to 2008 543 in.expensive drives tape-drive prices are not shown for years prior to 1984  because  as mentioned  the magazines used in tracking prices are targeted to the small-computer marketplace  and tape drives were not widely used with small computers prior to 1984 we can see from these graphs that the cost of storage has fallen dramatically by comparing the graphs  we can also see that the price of disk storage has plummeted relative to the price of dram and tape the price per megabyte of magnetic disk storage improved by more than four orders of magnitude from 1981 to 2004  whereas the corresponding improvement for main memory was only three orders of magnitude main memory today is more expensive than disk storage by a factor of 100 the price per megabyte dropped much more rapidly for disk drives than for tape drives as well in fact  the price per megabyte of a magnetic disk drive is approaching that of a tape cartridge without the tape drive consequently  small and medium-sized tape libraries have a higher storage cost than disk systems with equivalent capacity the dramatic fall in disk prices has largely rendered tertiary storage obsolete we no longer have any tertiary storage technology that is orders of magnitude less expensive than magnetic disk it appears that the revival of tertiary storage must await a revolutionary technology breakthrough meanwhile  tape storage will find its use mostly limited to purposes such as backups of disk drives and archival storage in enormous tape libraries that greatly exceed the practical storage capacity of large disk farms disk drives are the major secondary-storage i/0 devices on most computers most secondary storage devices are either magnetic disks or n1.agnetic tapes modern disk drives are structured as large one-dimensional arrays of logical disk blocks generally  these logical blocks are 512 bytes in size disks may be attached to a computer system in one of two ways   1  through the local i/0 ports on the host computer or  2  through a network cmmection 544 chapter 12 requests for disk i/0 are generated by the file system and by the virtual memory system each request specifies the address on the disk to be referenced  in the form of a logical block number disk-schedliling algorithms can improve the effective bandwidth  the average response time  and the variance in response time algorithms such as sstf  scan  c-scan  look  and c-look are designed to make such improvements through strategies for disk-queue ordering performance can be harmed by external fragmentation some systems have utilities that scan the file system to identify fragmented files ; they then move blocks around to decrease the fragmentation defragmenting a badly fragmented file system can significantly improve performance  but the systenc may have reduced performance while the defragmentation is in progress sophisticated file systems  such as the unix fast file system  incorporate many strategies to control fragmentation during space allocation so that disk reorganization is not needed the operating system manages the disk blocks first  a disk must be lowlevel formatted to create the sectors on the raw hardware-new disks usually come preformatted then  the disk is partitioned  file systems are created  and boot blocks are allocated to store the system 's bootstrap program finally  when a block is corrupted  the system must have a way to lock out that block or to replace it logically with a spare because an efficient swap space is a key to good performance  systems usually bypass the file system and use raw disk access for paging i/0 some systems dedicate a raw disk partition to swap space  and others use a file within the file system instead still other systems allow the user or system administrator to make the decision by providing both options because of the amount of storage required on large systems  disks are frequently made redundant via raid algorithms these algorithms allow more than one disk to be used for a given operation and allow continued operation and even automatic recovery in the face of a disk failure raid algorithms are organized into different levels ; each level provides some combination of reliability and high transfer rates the write-ahead log scheme requires the availability of stable storage to implement such storage  we need to replicate the needed information on multiple nonvolatile storage devices  usually disks  with independent failure modes we also need to update the information in a controlled manner to ensure that we can recover the stable data after any failure during data transfer or recovery tertiary storage is built from disk and tape drives that use removable media many different technologies are available  including magnetic tape  removable magnetic and magneto-optic disks  and optical disks for removable disks  the operating system generally provides the full services of a file-system interface  including space management and requestqueue scheduling for many operating systems  the name of a file on a removable cartridge is a combination of a drive name and a file name within that drive this convention is simpler but potentially more confusing than is using a name that identifies a specific cartridge for tapes  the operating system generally provides only a raw interface many operating systems have no built-in support for jukeboxes jukebox 545 support can be provided by a device driver or by a privileged application designed for backups or for hsm three important aspects of performance are bandwidth  latency  and reliability many bandwidths are available for both disks and tapes  but the random-access latency for a tape is generally much greater than that for a disk switching cartridges in a jukebox is also relatively slow because a jukebox has a low ratio of drives to cartridges  reading a large fraction of the data in a jukebox can take a long time optical media  which protect the sensitive layer with a transparent coating  are generally more robust than magnetic media  which are more likely to expose the magnetic material to physical damage lastly  the cost of storage has decreased greatly in the past two decades  most notably for disk storage 12.1 what would be the effects on cost and performance if tape storage had the same areal density as disk storage  areal density is the number of gigabits per square inch  12.2 it is sometimes said that tape is a sequential-access medium  whereas a magnetic disk is a random-access medium in fact the suitability of a storage device for random access depends on the transfer size the term streaming transfer rate denotes the rate for a data transfer that is underway  excluding the effect of access latency by contrast  the effective transfer rate is the ratio of total bytes per total seconds  including overhead time such as access latency suppose that  in a computer  the level-2 cache has an access latency of 8 nanoseconds and a streaming transfer rate of 800 megabytes per second  the main memory has an access latency of 60 nanoseconds and a streaming transfer rate of 80 megabytes per second  the magnetic disk has an access latency of 15 milliseconds and a streaming transfer rate of 5 megabytes per second  and a tape drive has an access latency of 60 seconds and a streaming transfer rate of 2 megabytes per seconds a random access causes the effective transfer rate of a device to decrease  because no data are transferred during the access time for the disk described  what is the effective transfer rate if an average access is followed by a streaming transfer of  1  512 bytes   2  8 kilobytes   3  1 megabyte  and  4  16 megabytes b the utilization of a device is the ratio of effective transfer rate to streaming transfer rate calculate the utilization of the disk drive for each of the four transfer sizes given in part a c suppose that a utilization of 25 percent  or higher  is considered acceptable using the performance figures given  compute the smallest transfer size for disk that gives acceptable utilization 546 chapter 12 d complete the following sentence  a disk is a random-access device for transfers larger than ______ bytes and is a sequentialaccess device for s1naller transfers e compute the minimum transfer sizes that give acceptable utilization for cache  memory  and tape f when is a tape a random-access device  and when is it a sequential-access device 12.3 the reliability of a hard-disk drive is typically described in terms of a quantity called mean time between failures  mtbf   although this quantity is called a time  the mtbf actually is measured in drive-hours per failure a if a system contains 1,000 disk drives  each of which has a 750,000 hour mtbf  which of the following best describes how often a drive failure will occur in that disk farm  once per thousand years  once per century  once per decade  once per year  once per month  once per week  once per day  once per hour  once per minute  or once per second b mortality statistics indicate that  on the average  a u.s resident has about 1 chance in 1,000 of dying between the ages of 20 and 21 deduce the mtbf hours for 20-year-olds convert this figure from hours to years what does this mtbf tell you about the expected lifetime of a 20-year-old c the manufacturer guarantees a 1-million-hour mtbf for a certain model of disk drive what can you conclude about the number of years for which one of these drives is under warranty 12.4 discuss how an operating system could maintain a free-space list for a tape-resident file system assume that the tape technology is append-only and that it uses eot marks and locate  space  and read position commands as described in section 12.9.2.1 12.5 imagine that a holographic storage drive has been invented the drive costs $ 10,000 and has an average access time of 40 milliseconds it uses a $ 100 cartridge the size of a cd this cartridge holds 40,000 images  and each image is a square black-and-white picture with a resolution of 6  000 x 6  000 pixels  each pixel stores 1 bit   the drive can read or write one picture in 1 millisecond answer the following questions a what would be some good uses for this device b how would this device affect the l/0 performance of a computing system c what kinds of storage devices  if any  would become obsolete as a result of the invention of this device 547 12.6 the term fast wide scsi-ii denotes a scsi bus that operates at a data rate of 20 megabytes per second when it moves a packet of bytes between the host and a device suppose that a fast wide scsi-ii disk drive spins at 7,200 rpm  has a sector size of 512 bytes  and holds 160 sectors per track a estimate the sustained transfer rate of this drive in megabytes per second b suppose that the drive has 7,000 cylinders  20 tracks per cylinde1 ~ a head-switch time  from one platter to another  of 0.5 millisecond  and an adjacent-cylinder seek time of 2 milliseconds use this additional information to give an accurate estimate of the sustained transfer rate for a huge transfer c suppose that the average seek time for the drive is 8 milliseconds estimate the i/0 operations per second and the effective transfer rate for a random-access workload that reads individual sectors that are scattered across the disk d calculate the random-access i/0 operations per second and transfer rate for i/0 sizes of 4 kilobytes  8 kilobytes  and 64 kilobytes e if multiple requests are in the queue  a scheduling algorithm such as scan should be able to reduce the average seek distance suppose that a random-access workload is reading 8-kilobyte pages  the average queue length is 10  and the scheduling algorithm reduces the average seek time to 3 milliseconds now calculate the i/0 operations per second and the effective transfer rate of the drive 12.7 compare the performance of write operations achieved by a raid level 5 organization with that achieved by a raid level1 organization 12.8 suppose that a disk drive has 5,000 cylinders  numbered 0 to 4999 the drive is currently serving a request at cylinder 143  and the previous request was at cylinder 125 the queue of pending requests  in fifo order  is  86,1470,913,1774,948,1509,1022,1750,130 starting from the current head position  what is the total distance  in cylinders  that the disk arm moves to satisfy all the pending requests for each of the following disk-scheduling algorithms a fcfs b sstf 548 chapter 12 c scan d look e c-scan f c-look 12.9 elementary physics states that when an object is subjected to a constant acceleration a  the relationship between distance d and time t is given by d = ~ at2  suppose that  during a seek  the disk in exercise 12.8 accelerates the disk arm at a constant rate for the first half of the seek  then decelerates the disk arm at the same rate for the second half of the seek assume that the disk can perform a seek to an adjacent cylinder in 1 n lillisecond and a full-stroke seek over all 5,000 cylinders in 18 milliseconds a the distance of a seek is the number of cylinders that the head moves explain why the seek time is proportional to the square root of the seek distance b write an equation for the seek time as a function of the seek distance this equation should be of the form t = x + y ~  where t is the time in milliseconds and l is the seek distance in cylinders c calculate the total seek time for each of the schedules in exercise 12.8 determine which schedule is the fastest  has the smallest total seek time   d the percentage speedup is the time saved divided by the original time what is the percentage speedup of the fastest schedule over fcfs 12.10 the accelerating seek described in exercise 12.9 is typical of hard-disk drives by contrast  floppy disks  and many hard disks manufactured before the mid-1980s  typically seek at a fixed rate suppose that the disk in exercise 12.9 has a constant-rate seek rather than a constantacceleration seek  so the seek time is of the form t = x + yl  where t is the time in milliseconds and l is the seek distance suppose that the time to seek to an adjacent cylinder is 1 millisecond  as before  and the time to seek to each additional cylinder is 0.5 milliseconds a write an equation for this seek time as a function of the seek distance b using this seek-time function  calculate the total seek time or each of the schedules in exercise 12.8 is your answer the same as the one or exercise 12.9  c  549 c what is the percentage speedup of the fastest scb.edule over fcfs in this case 12.11 suppose that the disk in exercise 12.9 rotates at 7,200 rpm a what is the average rotational latency of this disk drive b what seek distance can be covered in the tim.e that you found or part a 12.12 suppose that a one-sided 5.25-inch optical-disk cartridge has an areal density of 1 gigabit per square inch further suppose that a magnetic tape has an areal density of 20 megabits per square inch and is 1/2 inch wide and 1,800 feet long calculate an estimate of the storage capacities of these two kinds of storage media suppose that an optical tape exists that has the same physical size as the magnetic tape but the same storage density as the optical disk what volume of data could the optical tape hold what would be a marketable price for the optical tape if the magnetic tape cost $ 25 12.13 write a program that simulates the disk-scheduling algorithms discussed in section 12.4 12.14 why is rotational latency usually not considered in disk scheduling how would you modify sstf  scan  and c-scan to include latency optimization 12.15 remapping bad blocks by sector sparing or sector slipping can influence perfonnance suppose that the drive in exercise 12.6 has a total of 100 bad sectors at random locations and that each bad sector is mapped to a spare that is located on a different track within the same cylinder estimate the number of i/0 operations per second and the effective transfer rate for a random-access workload consisting of 8 kilobyte reads  assuming a queue length of 1  that is  the choice of scheduling algorithm is not a factor   what is the effect of a bad sector on performance 12.16 discuss the relative advantages and disadvantages of sector sparing and sector slipping 12.17 compare the performance of c-scan and scan scheduling  assuming a uniform distribution of requests consider the average response time  the time between the arrival of a request and the completion of that request 's service   the variation in response time  and the effective 550 chapter 12 bandwidth how does performance depend on the relative sizes of seek time and rotational latency 12.18 none of the disk-scheduling disciplines  except fcfs  is truly fair  starvation may occur   a explain why this assertion is true b describe a way to modify algorithms such as scan to ensure fairness c explain why fairness is an important goal in a time-sharing system d give three or more examples of circumstances in which it is important that the operating system be unfair in serving i/o requests 12.19 consider a raid level 5 organization comprising five disks  with the parity for sets of four blocks on four disks stored on the fifth disk how many blocks are accessed in order to perform the following a a write of one block of data b a write of seven continuous blocks of data 12.20 the operating system generally treats removable disks as shared file systems but assigns a tape drive to only one application at a time give three reasons that could explain this difference in treatment of disks and tapes describe the additional features that an operating system would need to support shared file-system access to a tape jukebox would the applications sharing the tape jukebox need any special properties  or could they use the files as though the files were disk-resident explain your answer 12.21 how would use of a ram disk affect your selection of a disk-scheduling algorithm what factors would you need to consider do the same considerations apply to hard-disk scheduling  given that the file system stores recently used blocks in a buffer cache in main memory 12.22 you can use simple estimates to compare the cost and performance of a terabyte storage system made entirely from disks with one that incorporates tertiary storage suppose that each magnetic disk holds 10gb  costs $ 1,000  transfers 5mb per second  and has an average access latency of 15 milliseconds also suppose that a tape library costs $ 10 per gigabyte  transfers 10 mb per second  and has an average access latency of 20 seconds compute the total cost  the maximum total data rate  and the average waiting time for a pure disk system if you make 551 any assumptions about the workload  describe and justify them now  suppose that 5 percent of the data are frequently used  so they must reside on disk  but the other 95 percent are archived in the tape library further suppose that the disk system handles 95 percent of the requests and the library handles the other 5 percent what are the total cost  the maximum total data rate  and the average waiting time for this hierarchical storage system 12.23 assume that you have a mixed configuration comprising disks organized as raid levell and raid levels disks assume that the system has flexibility in deciding which disk organization to use for storing a particular file which files should be stored in the raid level 1 disks and which in the raid levels disks in order to optimize performance 12.24 what are the tradeoffs involved in rereading code pages from the file system versus using swap space to store them 12.25 requests are not usually uniformly distributed for example  we can expect a cylinder containing the file-system fat or inodes to be accessed more frequently than a cylinder containing only files suppose you know that 50 percent of the requests are for a small  fixed number of cylinders a would any of the scheduling algorithms discussed in this chapter be particularly good for this case explain your answer b propose a disk-scheduling algorithm that gives even better performance by taking advantage of this hot spot on the disk c file systems typically fil1.d data blocks via an indirection table  such as a fat in dos or inodes in unix describe one or more ways to take advantage of this indirection to improve disk performance 12.26 discuss the reasons why the operating system might require accurate information on how blocks are stored on a disk how could the operating system improve file system performance with this knowledge 12.27 in a disk jukebox  what would be the effect of having more open files than the number of drives in the jukebox 12.28 compare the throughput achieved by a raid levels organization with that achieved by a raid levell organization for the following  a read operations on single blocks b read operations on multiple contiguous blocks 552 chapter 12 12.29 could a raid level 1 organization achieve better performance for read requests than a raid level 0 organization  with nonredundant striping of data  if so  how discussions of redundant arrays of independent disks  raids  are presented by patterson et al  1988  and in the detailed survey of chen et al  1994   disk-system architectures for high-performance computing are discussed by katz et al  1989   enhancements to raid systems are discussed in wilkes et al  1996  and yu et al  2000   teorey and pinkerton  1972  present an early comparative analysis of disk-scheduling algorithms they use simulations that model a disk for which seek time is linear in the number of cylinders crossed for this disk look is a good choice for queue lengths below 140  and c-look is good for queue lengths above 100 king  1990  describes ways to improve the seek time by moving the disk ann when the disk is otherwise idle seltzer et al  1990  and jacobson and wilkes  1991  describe disk-scheduling algorithms that consider rotational latency in addition to seek time scheduling optimizations that exploit disk idle times are discussed in lumb et al  2000   worthington et al  1994  discuss disk performance and show the negligible performance impact of defect management the placement of hot data to improve seek times has been considered by ruemmler and wilkes  1991  and akyurek and salen'l  1993   ruemmler and wilkes  1994  describe an accurate performance model for a modern disk drive worthington et al  1995  tell how to determine low-level disk properties such as the zone structure  and this work is further advanced by schindler and gregory  1999   disk power management issues are discussed in douglis et al  1994l douglis et al  1995l greenawalt  1994l and golding et al  1995   the i/0 size and randomness of the workload has a considerable influence on disk performance ousterhout et al  1985  and ruemmler and wilkes  1993  report numerous interesting workload characteristics  including that most files are smalt most newly created files are deleted soon thereafter  most files that are opened for reading are read sequentially in their entirety  and most seeks are short mckusick et al  1984  describe the berkeley fast file system  ffs   which uses many sophisticated techniques to obtain good performance for a wide variety of workloads mcvoy and kleiman  1991  discuss further improvements to the basic ffs quinlan  1991  describes how to implement a file system on worm storage with a magnetic disk cache ; richards  1990  discusses a file-system approach to tertiary storage maher et al  1994  give an overview of the integration of distributed file systems and tertiary storage the concept of a storage hierarchy has been studied for more than thirty years for instance  a 1970 paper by mattson et al  1970  describes a mathematical approach to predicting the performance of a storage hierarchy alt  1993  describes the accommodation of removable storage in a commercial operating system  and miller and katz  1993  describe the characteristics of tertiary-storage access in a supercomputing environment benjamin  1990  gives an overview of the massive storage requirements for the eosdis project at nasa management and use of network-attached disks and programmable 553 disks are discussed in gibson et al  1997b t gibson et al  1997at riedel et al  1998t and lee and thekkath  1996   holographic storage technology is the subject of an article by psaltis and mok  1995  ; a collection of papers on this topic dating from 1963 has been assembled by sincerbox  1994   asthana and finkelstein  1995  describe several emerging storage technologies  including holographic storage  optical tape  and electron trapping toigo  2000  gives an in-depth description of modern disk technology and several potential future storage technologies 13.1 r the two main jobs of a computer are i/0 and processing in many cases  the main job is i/0  and the processing is merely incidental for instance  when we browse a web page or edit a file  our immediate interest is to read or enter some information  not to compute an answer the role of the operating system in computer i/0 is to manage and control i/0 operations and i/0 devices although related topics appear in other chapters  here we bring together the pieces to paint a complete picture of i/0 first  we describe the basics of i/o hardware  because the nature of the hardware interface places constraints on the internal facilities of the operating system next  we discuss the i/0 services provided by the operating system and the embodiment of these services in the application i/0 interface then  we explain how the operating system bridges the gap between the hardware interface and the application interface we also discuss the unix system v streams mechanism  which enables an application to assemble pipelines of driver code dynamically finally  we discuss the performance aspects of i/o and the principles of operating-system design that improve i/0 performance to explore the structure of an operating system 's 1/0 subsystem to discuss the principles and complexities of 110 hardware to explain the performance aspects of 110 hardware and software the control of devices connected to the computer is a major concern of operating-system designers because i/o devices vary so widely in their function and speed  consider a mouse  a hard disk  and a cd-rom jukebox   varied methods are needed to control them these methods form the i/0 subsystem of the kernet which separates the rest of the kernel from the complexities of managing i/0 devices 555 556 chapter 13 13.2 i/o-device technology exhibits two conflicting trends on the one hand  we see increasing standardization of software and hardware interfaces this trend helps 11s to incorporate improved device generations into existing computers and operating systems on the other hand  we see an increasingly broad variety of 1/0 devices some new devices are so unlike previous devices that it is a challenge to incorporate them into our computers and operating systems this challenge is met by a combination of hardware and software techniques the basic i/0 hardware elements  such as ports  buses  and device controllers  accommodate a wide variety of i/0 devices to encapsulate the details and oddities of different devices  the kernel of an operating system is structured to use device-driver modules the present a uniform deviceaccess interface to the i/0 subsystem  much as system calls provide a standard interface between the application and the operating system computers operate a great many kinds of devices most fit into the general categories of storage devices  disks  tapes   transmission devices  network cards  modems   and human-interface devices  screen  keyboard  mouse   other devices are more specialized  s11ch as those involved in the steering of a military fighter jet or a space shuttle in these aircraft  a human gives input to the flight computer via a joystick and foot pedals  and the computer sends output commands that cause motors to move rudders  flaps  and thrusters despite the incredible variety of i/0 devices  though  we need only a few concepts to understand how the devices are attached and how the software can control the hardware a device communicates with a computer system by sending signals over a cable or even through the air the device communicates with the machine via a connection point  or example  a serial port if devices use a common set of wires  the connection is called a bus a is a set of wires and a rigidly defined protocol that specifies a set of messages that can be sent on the wires in terms of the electronics  the messages are conveyed by patterns of electrical voltages applied to the wires with defined timings when device a has a cable that plugs into device b  and device b has a cable that plugs into device c  and device c plugs into a port on the computer  this arrangement is called a a daisy chain usually operates as a bus buses are used widely in computer architecture and vary in their signaling methods  speed  throughput  and connection methods a typical pc bus structure appears in figure 13.1 this figure shows a  the common pc system bus  that connects the processor-memory subsystem to the fast devices and an that connects relatively slow devices  such as the keyboard and serial and usb ports in the upper-right portion of the figure  four disks are c01mected together on a scsi bus plugged into a scsi controller other common buses used to interconnect main parts of a computer include with up to 4.3 gb ;  pcie   with throughput up with throughput up to 20 gb is a collection of electronics that can operate a port  a bus  or a device a serial-port controller is a simple device controller it is a single chip  or portion of a chip  in the computer that controls the signals on the 13.2 557 figure 13.1 a typical pc bus structure wires of a serial port by contrast  a scsi bus controller is not simple because the scsi protocol is complex  the scsi bus controller is often implemented as a separate circuit board  or a that plugs into the computer it typically contains a processor  microcode  and some private memory to enable it to process the scsi protocol messages some devices have their own built-in controllers if you look at a disk drive  you will see a circuit board attached to one side this board is the disk controller it implements the disk side of the protocol for some kind of com1ection-scsi or ata  for instance it has microcode and a processor to do many tasks  such as bad-sector mapping  prefetching  buffering  and caching how can the processor give commands and data to a controller to accomplish an i/0 transfer the short answer is that the controller has one or more registers for data and control signals the processor communicates with the controller by reading and writing bit patterns in these registers one way in which this communication can occur is through the use of special i/0 instructions that specify the transfer of a byte or word to an i/0 port address the i/0 instruction triggers bus lines to select the proper device and to move bits into or out of a device register alternatively  the device controller can support in this case  the device-control registers are mapped into the address space of the processor the cpu executes i/0 requests using the standard data-transfer instructions to read and write the device-control registers some systems use both techniques for instance  pcs use i/0 instructions to control some devices and memory-mapped i/0 to control others figure 13.2 shows the usual i/o port addresses for pcs the graphics controller has i/o ports for basic control operations  but the controller has a large memory558 chapter 13 000-00f dma controller 020-021 interrupt controller 040-043 timer 200-20f game controller 2f8-2ff serial port  secondary  320-32f hard-disk controller 378-37f parallel port 3d0-3df graphics controller 3f0-3f7 diskette-drive controller 3f8-3ff serial port  primary  figure 13.2 device 1/0 port locations on pcs  partial   mapped region to hold screen contents the process sends output to the screen by writing data into the memory-mapped region the controller generates the screen image based on the contents of this memory this technique is simple to use moreover  writing millions of bytes to the graphics memory is faster than issuing millions of i/0 instructions but the ease of writing to a memory-mapped i/0 controller is offset by a disadvantage because a common type of software fault is a write through an incorrect pointer to an unintended region of memory  a memory-mapped device register is vulnerable to accidental modification of course  protected memory helps to reduce this risk an i/0 port typically consists of four registers  called the  1  status   2  control   3  data-in  and  4  data-out registers the the is read by the host to get input is written by the host to send output the contains bits that can be read by the host these bits indicate states  such as whether the current command has completed  whether a byte is available to be read from the data-in register  and whether a device error has occurred the can be written by the host to start a command or to change the nlode of a device for instance  a certain bit in the control register of a serial port chooses between full-duplex and half-duplex communication  another bit enables parity checking  a third bit sets the word length to 7 or 8 bits  and other bits select one of the speeds supported by the serial port the data registers are typically 1 to 4 bytes in size some controllers have fifo chips that can hold several bytes of input or output data to expand the capacity of the controller beyond the size of the data register a fifo chip can hold a small burst of data until the device or host is able to receive those data 13.2 559 13.2.1 polling the complete protocol for interaction between the host and a controller can be intricate  but the basic handshaking notion is simple we explain handshaking with an example assume that 2 bits are used to coordinate the producer-consumer relationship between the controller and the host the controller indicates its state through the busy bit in the status register  recall that to set a bit means to write a 1 into the bit and to clear a bit means to write a 0 into it  the controller sets the busy bit when it is busy working and clears the busy bit when it is ready to accept the next comm.and the host signals its wishes via the command-ready bit in the command register the host sets the command-ready bit when a command is available for the controller to execute for this example  the host writes output through a port  coordinating with the controller by handshaking as follows the host repeatedly reads the busy bit until that bit becomes clear the host sets the write bit in the command register and writes a byte into the data-out register the host sets the command-ready bit when the controller notices that the command-ready bit is set  it sets the busy bit the controller reads the command register and sees the write command it reads the data-out register to get the byte and does the i/o to the device the controller clears the command-ready bit  clears the error bit in the status register to indicate that the device i/o succeeded  and clears the busy bit to indicate that it is finished this loop is repeated for each byte in step 1  the host is or it is in a loop  reading the status register over and over until the busy bit becomes clear if the controller and device are fast  this method is a reasonable one but if the wait may be long  the host should probably switch to another task how  then  does the host know when the controller has become idle for some devices  the host must service the device quickly  or data will be lost for instance  when data are streaming in on a serial port or from a keyboard  the small buffer on the controller will overflow and data will be lost if the host waits too long before returning to read the bytes in many computer architectures  three cpu-instruction cycles are sufficient to poll a device  read a device register  logical-and to extract a status bit  and branch if not zero clearly  the basic polling operation is efficient but polling becomes inefficient when it is attempted repeatedly yet rarely finds a device to be ready for service  while other useful cpu processing remains undone in such instances  it may be more efficient to arrange for the hardware controller to notify the cpu when the device becomes ready for service  rather than to require the cpu to poll repeatedly for an i/0 completion the hardware mechanism that enables a device to notify the cpu is called an 560 chapter 13 7 cpu device driver initiates 1/0 cpu executing checks for interrupts between instructions cpu resumes processing of interrupted task 1/0 controller 4 figure 13.3 interrupt-driven 1/0 cycle 13.2.2 interrupts the basic interrupt mechanism works as follows the cpu hardware has a wire called the that the cpu senses after executing every instruction when the cpu detects that a controller has asserted a signal on the line  the cpu performs a state save and jumps to the at a fixed address in memory the interrupt handler determines the cause of the interrupt  performs the necessary processing  performs a state restore  and executes a return from interrupt instruction to return the cpu to the execution state prior to the interrupt we say that the device controller raises an interrupt by asserting a signal on the interrupt request line  the cpu catches the interrupt and dispatches it to the interrupt handler  and the handler clears the interrupt by servicing the device figure 13.3 summarizes the interrupt-driven i/0 cycle this basic interrupt mechanism enables the cpu to respond to an asynchronous event  as when a device controller becomes ready for service in a modern operating system  however  we need nlore sophisticated interrupthandling features we need the ability to defer interrupt handling during critical processing 13.2 561 we need an efficient way to dispatch to the proper interrupt handler for a device without first polling all the devices to see which one raised the interrupt we need multilevel interrupts  so that the operating system can distinguish between high and low-priority interrupts and can respond with the appropriate degree of urgency in modern computer hardware  these three features are provided by the cpu and by the most cpus have two interrupt request lines one is the ' ' ' ' which is reserved for events such as unrecoverable memory errors the second interrupt line is it can be turned off by the cpu before the execution of critical instruction sequences that must not be interrupted the maskable interrupt is used by device controllers to request service the interrupt mechanism accepts an number that selects a specific interrupt-handling routine from a small set in most architectures  this address is an offset in a table called the  this vector contains the memory addresses of specialized interrupt handlers the purpose of a vectored interrupt mechanism is to reduce the need for a single interrupt handler to search all possible sources of interrupts to determine which one needs service in practice  however  computers have more devices  and  hence  interrupt handlers  than they have address elements in the interrupt vector a common way to solve this problem is to use the technique of interrupt chaining  in which each element in the interrupt vector points to the head of a list of interrupt handlers when an il1.terrupt is raised  the handlers on the corresponding list are called one by one  until one is found that can service the request this structure is a compromise between the overhead of a huge interrupt table and the inefficiency of dispatching to a single interrupt handler figure 13.4 illustrates the design of theinterruptvector for the intel pentium processor the events from 0 to 31  which are nonmaskable  are used to signal various error conditions the events from 32 to 255  which are maskable  are used for purposes such as device-generated interrupts the interrupt mechanism also implements a system of this mechanism enables the cpu to defer the handling of low-priority interrupts without maskii1.g off all interrupts and makes it possible for a high-priority interrupt to preempt the execution of a low-priority interrupt a modern operating system interacts with the interrupt mechanism in several ways at boot time  the operating system probes the hardware buses to determine what devices are present and installs the corresponding interrupt handlers into the interrupt vector during i/0  the various device controllers raise interrupts when they are ready for service these interrupts signify that output has cornpleted  or that input data are available  or that a failure has been detected the interrupt mechanism is also used to handle a wide variety of such as dividing by zero  accessing a protected or nonexistent memory address  or attempting to execute a privileged instruction from user mode the events that trigger interrupts have a common property  they are occurrences that induce the cpu to execute an urgent self-contained routine an operating system has other good uses for an efficient hardware and software mechanism that saves a small amount of processor state and then 562 chapter 13 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 i 18 19-31 32-255 breakpoint into-detected overflow bound range exception invalid opcode device not available double fault coprocessor segment overrun  reserved  invalid task state segment segment not present stack fault general protection page fault  intel reserved  do not use  floating-point error alignment check machine check  intel reserved  do not use  maskable interrupts figure i3.4 intel pentium processor event-vector table calls a privileged routine in the kernel for example  many operating systems use the interrupt mechanism for virtual memory paging a page fault is an exception that raises an interrupt the interrupt suspends the current process and jumps to the page-fault handler in the kernel this handler saves the state of the process  moves the process to the wait queue  performs page-cache management  schedules an i/0 operation to fetch the page  schedules another process to resume execution  and then returns from the interrupt another example is found in the implementation of system calls usually  a program uses library calls to issue system calls the library routines check the arguments given by the application  build a data structure to convey the arguments to the kernel  and then execute a special instruction called a or  this instruction has an operand that identifies the desired kernel service when a process executes the trap instruction  the interrupt hardware saves the state of the user code  switches to supervisor mode  and dispatches to the kernel routine that implements the requested service the trap is given a relatively low interrupt priority compared with those assigned to device interrupts-executilcg a system call on behalf of an application is less urgent than servicing a device controller before its fifo queue overflows and loses data interrupts can also be used to manage the flow of control within the kernel for example  consider the processing required to complete a disk read one step is to copy data from kernel space to the user buffer this copying is time consuming but not urgent-it should not block other high-priority interrupt 13.2 563 handling another step is to start the next pending l/0 for that disk drive this step has higher priority if the disks are to be used efficiently  we need to start the next i/o as soon as the previous one completes consequently  a pair of interrupt handlers implen ents the kernel code that completes a disk read the high-priority handler records the l/0 status  clears the device interrupt  starts the next pending i/0  and raises a low-priority interrupt to complete the work later  when the cpu is not occupied with high-priority work  the low-priority interrupt will be dispatched the corresponding handler completes the userlevel i/0 by copying data from kernel buffers to the application space and then calling the scheduler to place the application on the ready queue a threaded kernel architecture is well suited to implement multiple interrupt priorities and to enforce the precedence of interrupt handling over background processing in kernel and application routines we illustrate this point with the solaris kernel in solaris  interrupt handlers are executed as kernel threads a range of high priorities is reserved for these threads these priorities give interrupt handlers precedence over application code and kernel housekeeping and implement the priority relationships among interrupt handlers the priorities cause the solaris thread scheduler to preempt lowpriority interrupt handlers in favor of higher-priority ones  and the threaded implementation enables multiprocessor hardware to run several interrupt handlers concurrently we describe the interrupt architecture of windows xp and unix in chapter 22 and appendix a  respectively in summa  r  y  interrupts are used throughout modern operating systems to handle asynchronous events and to trap to supervisor-mode routines in the kernel to enable the most urgent work to be done first  modern computers use a system of interrupt priorities device controllers  hardware faults  and system calls all raise interrupts to trigger kernel routines because interrupts are used so heavily for time-sensitive processing  efficient interrupt handling is required for good system performance 13.2.3 direct memory access for a device that does large transfers  such as a disk drive  it seems wasteful to use an expensive general-purpose processor to watch status bits and to feed data into a controller register one byte at a time-a process termed many computers avoid burdening the main cpu with pio by offloading some of this work to a special-purpose processor called a to initiate a dma transfer  the host writes a dma command block into memory this block contains a pointer to the source of a transfer  a pointer to the destination of the transfer  and a count of the number of bytes to be transferred the cpu writes the address of this command block to the dma controller  then goes on with other work the dma controller proceeds to operate the memory bus directly  placing addresses on the bus to perform transfers without the help of the main cpu a simple dma controller is a standard component in pcs  and for the pc usually contain their own high-speed dma hardware handshaking between the dma controller and the device controller is performed via a pair of wires called dma-request and dma-acknowledge the device controller places a signal on the dma-request wire when a word of data is available for transfer this signal causes the dma controller to seize 564 chapter 13 the memory bus  place the desired address on the memory-address wires  and place a signal on the dl \ iia -acknowledge wire when the device controller receives the dma-acknowledge signat it transfers the word of data to memory and removes the dma-request signal when the entire transfer is finished  the dma controller interrupts the cpu this process is depicted in figure 13.5 when the dma controller seizes the memory bus  the cpu is momentarily prevented from accessing main memory although it can still access data items in its primary and secondary caches although this can slow down the cpu computation  offloading the data-transfer work to a dma controller generally improves the total system performance some computer architectures use physical memory addresses for dma  but others perform mercwry using virtual addresses that undergo translation to physical addresses dvma can perform a transfer between two memory-mapped devices without the intervention of the cpu or the use of main memory on protected-mode kernels  the operating system generally prevents processes from issuing device commands directly this discipline protects data from access-control violations and also protects the system from erroneous use of device controllers that could cause a system crash instead  the operating system exports functions that a sufficiently privileged process can use to access low-level operations on the underlying hardware on kernels without memory protection  processes can access device controllers directly this direct access can be used to achieve high performance/ since it can avoid kernel communication  context switches  and layers of kernelsoftware unfortunately  it interferes with system security and stability the trend in general-purpose operating systems is to protect memory and devices so that the system can try to guard against erroneous or malicious applications 5 dma controller transfers bytes to buffer x  increasing memory address and decreasing c until c = 0 1 device driver is told to transfer disk data to buffer at address x 2 device driver tells l  ' '  ~ i ' ' ~ --' ' disk controller to transfer c bytes from disk to buffer at address x 6 when c = 0  dma interrupts cpu to signal transfer completion 1  2.'.c ~ li.ip  2i  .2-j rc-c ~ .,.,---j' = ,._ ~ 3 disk controller initiates dma transfer c'g ! ,or  tt_ront ; r ' i 4 disk controller sends each byte to dma controller figure 13.5 steps in a dma transfer 13.3 13.3 565 13.2.4 1/0 hardware summary although the hardware aspects of i/0 are complex when considered at the level of detail of electronics-hardware design  the concepts that we have just described are sufficient to enable us to understand many i/0 features of operating systen s let 's review the main concepts  a bus a controller an i/0 port and its registers the handshaking relationship between the host and a device controller the execution of this handshaking in a polling loop or via interrupts the offloading of this work to a dma controller for large transfers we gave a basic example of the handshaking that takes place between a device controller and the host earlier in this section in reality  the wide variety of available devices poses a problem for operating-system implementers each kind of device has its own set of capabilities  control-bit definitions  and protocols for interacting with the host-and they are all different how can the operating system be designed so that we can attach new devices to the computer without rewriting the operating system and when the devices vary so widely  how can the operating system give a convenient  uniform i/0 interface to applications we address those questions next in this section  we discuss structuring techniques and interfaces for the operating system that enable i/0 devices to be treated in a standard  uniform way we explain  for instance  how an application can open a file on a disk without knowing what kind of disk it is and how new disks and other devices can be added to a cmnputer without disruption of the operating system like other complex software-engineering problems  the approach here involves abstraction  encapsulation  and software layering specifically  we can abstract away the detailed differences in i/0 devices by identifying a few general kinds each kind is accessed through a standardized set of functions-an the differences are encapsulated in kernel modules called device drivers that internally are custom-tailored to specific devices but that export one of the standard interfaces figure 13.6 illustrates how the i/o-related portions of the kernel are structured in software layers the purpose of the device-driver layer is to hide the differences among device controllers from the i/o subsystem of the kernel  much as the i/0 system calls encapsulate the behavior of devices in a few generic classes that hide hardware differences from applications making the i/0 subsystem independent of the hardware simplifies the job of the operating-system developer it also benefits the hardware manufacturers they either design new devices to be compatible with an existing host controller interface  such as scsi-2   or they write device drivers to interface the new hardware to popular 566 chapter 13 figure 13.6 a kernel i/o structure operating systems thus  we can attach new peripherals to a computer without waiting for the operating-system vendor to develop support code unfortm1ately for device-hardware manufacturers  each type of operating system has its own standards for the device-driver interface a given device may ship with multiple device drivers-for instance  drivers for ms-dos  windows 95/98  windows nt/2000  and solaris devices vary on many dimensions  as illustrated in figure 13.7 character-stream or block a character-stream device transfers bytes one by one  whereas a block device transfers a block of bytes as a unit sequential or random access a sequential device transfers data in a fixed order determined by the device  whereas the user of a random-access device can instruct the device to seek to any of the available data storage locations synchronous or asynchronous a synchronous device performs data transfers with predictable response times an asynchronous device exhibits irregular or unpredictable response times sharable or dedicated a sharable device can be used concurrently by several processes or threads ; a dedicated device can not speed of operation device speeds range from a few bytes per second to a few gigabytes per second read -write  read only  or write only some devices perform both input and output  but others support only one data transfer direction access method transfer schedule i/o direction 13.3 synchronous asynchronous dedicated sharable latency seek time transfer rate delay between operations read only write only read-write tape keyboard tape keyboard cd-rom figure i 3 7 characteristics of 1/0 devices 567 for the purpose of application access  many of these differences are hidden by the operating system  and the devices are grouped into a few conventional types the resulting styles of device access have been found to be useful and broadly applicable although the exact system calls may differ across operating systems  the device categories are fairly standard the major access conventions include block i/0  character-stream i/0  memory-mapped file access  and network sockets operating systems also provide special system calls to access a few additional devices  such as a time-of-day clock and a timer some operating systems provide a set of system calls for graphical display  video  and audio devices most operating systems also have an  or that transparently passes arbitrary conunands from an application to a device driver in unix  this system call is ioctl    for i/0 control   the ioctl   system call enables an application to access any functionality that can be implernented by any device driver  without the need to invent a new system call the ioctl   system call has three arguments the first is a file descriptor that connects the application to the driver by referring to a hardware device managed by that driver the second is an integer that selects one of the commands implemented in the driver the third is a pointer to an arbitrary data structure in memory that enables the application and driver to communicate any necessary control information or data 13.3.1 block and character devices the captures all the aspects necessary for accessing disk drives and other block-oriented devices the device is expected to understand commands such as read   and write   ; if it is a random-access device  it is also expected to have a seek   command to specify which block to transfer next 568 chapter 13 applications normally access such a device through a file-system interface we can see that read    write    and seek 0 capture the essen.tial behaviors of block-storage devices  so that applications are insulated from the low-level differences among those devices the operating system itself  as well as special applications such as databasemanagement systems  may prefer to access a block device as a simple linear array of blocks this mode of access is sometimes called if the application performs its own buffering  then using a file systen1 would cause extra  unneeded buffering likewise  if an application provides its own locking of file blocks or regions  then any operating-system locking services would be redundant at the least and contradictory at the worst to avoid these conflicts  raw-device access passes control of the device directly to the application  letting the operating system step out of the way unfortunately  no operating-system services are then performed on this device a compromise that is becoming common is for the operating system to allow a mode of operation on a file that disables buffering and locking in the unix world  this is called memory-mapped file access can be layered on top of block-device drivers rather than offering read and write operations  a memory-mapped interface provides access to disk storage via an array of bytes in main memory the system call that maps a file into memory returns the virtual memory address that contains a copy of the file the actual data transfers are performed only when needed to satisfy access to the memory image because the transfers are handled by the same mechanism as that used for demand-paged virtual memory access  memory-mapped i/o is efficient memory mapping is also convenient for programmers-access to a memory-mapped file is as simple as reading from and writing to memory operating systems that offer virtual memory commonly use the mapping interface for kernel services for instance  to execute a program  the operating system maps the executable into memory and then transfers control to the entry address of the executable the mapping interface is also commonly used for kernel access to swap space on disk a keyboard is an example of a device that is accessed through a the basic system calls in this interface enable an application to get   or put   one character on top of this interface  libraries can be built that offer line-at-a-time access  with buffering and editing services  for example  when a user types a backspace  the preceding character is removed from the input stream   this style of access is convenient for input devices such as keyboards  mice  and modems that produce data for input spontaneously -that is  at times that cam1.ot necessarily be predicted by the application this access style is also good for output devices such as printers and audio boards  which naturally fit the concept of a linear stream of bytes 13.3.2 network devices because the performance and addressing characteristics of network i/0 differ significantly from those of disk i/0  most operating systems provide a network i/o interface that is different from the read   -write   -seek   interface used for disks one interface available in many operating systerns  including unix and windows nt  is the network interface think of a wall socket for electricity  any electrical appliance can be plugged in by analogy  the system calls in the socket interface enable an application 13.3 569 to create a socket  to connect a local socket to a remote address  which plugs this application into a socket created by another application   to listen for any remote application to plug into the local socket  and to send and receive packets over the connection to support the implementation of servers  the socket interface also provides a function called select   that manages a set of sockets a call to select   returns information about which sockets have a packet waiting to be received and which sockets have room to accept a packet to be sent the use of select   eliminates the polling and busy waiting that would otherwise be necessary for network i/0 these functions encapsulate the essential behaviors of networks  greatly facilitating the creation of distributed applications that can use any underlying network hardware and protocol stack many other approaches to interprocess communication and network communication have been implemented for instance  windows nt provides one interface to the network interface card and a second interface to the network protocols  appendix c.6   in unix  which has a long history as a proving ground for network technology  we find half-duplex pipes  full-duplex fifos  full-duplex streams  message queues  and sockets information on unix networking is given in appendix a.9 13.3.3 clocks and timers most computers have hardware clocks and timers that provide three basic functions  give the current time give the elapsed time set a timer to trigger operation x at time t these functions are used heavily by the operating system  as well as by timesensitive applications unfortunately  the system calls that implement these functions are not standardized across operating systems the hardware to measure elapsed time and to trigger operations is called a  it can be set to wait a certain amount of time generate an interrupt  and it can be set to do this once or to repeat the process to generate periodic interrupts the scheduler uses this mechanism to generate an interrupt that will preempt a process at the end of its time slice the disk i/o subsystem uses it to invoke the periodic flushing of dirty cache buffers to disk  and the network subsystem uses it to cancel operations that are proceeding too slowly because of network congestion or failures the operating system may also provide an interface for user processes to use timers the operating system can support more timer requests than the number of timer hardware chan11els by simulating virtual clocks to do so  the kernel  or the timer device driver  maintains a list of interrupts wanted by its own routines and by user requests  sorted in earliest-time-first order it sets the timer for the earliest tince when the timer interrupts  the kernel signals the requester and reloads the timer with the next earliest time on many computers  the interrupt rate generated by the hardware clock is between 18 and 60 ticks per second this resolution is coarse  since a modern computer can execute hundreds of millions of instructions per second the 570 chapter 13 precision of triggers is limited by the coarse resolution of the timer  together with the overhead of maintaining virtual clocks furthermore  if the timer ticks are used to maintain the system time-of-day clock  the system clock can drift in most computers  the hardware clock is constructed from a highfrequency counter in some computers  the value of this counter can be read from a device register  in which case the counter can be considered a highresolution clock although this clock does not generate interrupts  it offers accurate measurements of time intervals 13.3.4 blocking and nonblocking 1/0 another aspect of the system-call interface relates to the choice between blocking i/0 and nonblocking i/0 when an application issues a system call  the execution of the application is suspended the application is moved from the operating system 's run queue to a wait queue after the system call completes  the application is moved back to the run queue  where it is eligible to resume execution when it resumes execution  it will receive the values returned by the system call the physical actions performed by i/0 devices are generally asynchronous-they take a varying or unpredictable amount of time nevertheless  most operating systems use blocking system calls for the application interface  because blocking application code is easier to understand than nonblocking application code some user-level processes need i/0 one example is a user interface that receives keyboard and mouse input while processing and displaying data on the screen another example is a video application that reads frames from a file on disk while simultaneously decompressing and displaying the output on the display one way an application writer can overlap execution with i/0 is to write a multithreaded application some threads can perform blocking system calls  while others continue executing the solaris developers used this technique to implement a user-level library for asynchronous i/0  freeing the application writer from that task some operating systems provide nonblocking i/0 system calls a nonblocking call does not halt the execution of the application for an extended time h1.stead  it returns quickly  with a return value that indicates how many bytes were transferred an alternative to a nonblocking system call is an asynchronous system call an asynchronous call returns immediately  without waiting for the i/0 to complete the application continues to execute its code the completion of the i/0 at some future time is communicated to the application  either through the setting of some variable in the address space of the application or through the triggering of a signal or software interrupt or a call-back routine that is executed outside the linear control flow of the application the difference between nonblocking and asynchronous system calls is that a nonblocking read   returns immediately with whatever data are available-the full number of bytes requested  fewer  or none at all an asynchronous read   call requests a transfer that will be performed in its entirety but will complete at some future time these two i/0 methods are shown in figure 13.8 a good example of nonblocking behavior is the select   system call for network sockets this system call takes an argument that specifies a maximum waiting time by setting it to 0  an application can poll for network activity 13.4 13.4 571 kernel user user kernel  a   b  figure 13.8 two 1/0 methods   a  synchronous and  b  asynchronous without blocking but using select   introduces extra overhead  because the select   call only checks whether i/0 is possible for a data transfer  select   must be followed by some kind of read   or write   command a variation on this approach  fotmd in mach  is a blocking multiple-read call it specifies desired reads for several devices in one system call and returns as soon as any one of them completes kernels provide many services related to i/0 several services-scheduling  buffering  caching  spooling  device reservation  and error handlil1.g-are provided by the kernel 's i/0 subsystem and build on the hardware and devicedriver infrastructure the i/o subsystem is also responsible for protectil1.g itself from errant processes and malicious users 13.4.1 1/0 scheduling to schedule a set of i/o requests means to determine a good order in which to execute them the order in which applications issue system calls rarely is the best choice scheduling can improve overall system performance  can share device access fairly among processes  and can reduce the average waiting time for i/0 to complete here is a simple example to illustrate suppose that a disk arm is near the begilming of a disk and that three applications issue blocking read calls to that disk application 1 requests a block near the end of the disk  application 2 requests one near the beginning  and application 3 requests one in the middle of the disk the operating system can reduce the distance that the disk ann travels by serving the applications in the order 2  3  1 rearrangil1.g the order of service in this way is the essence of i/0 scheduling operating-system developers implement scheduling by maintaining a wait queue of requests for each device when an application issues a blocking i/0 system call  the request is placed on the queue for that device the i/0 scheduler rearranges the order of the queue to improve the overall system efficiency and the average response time experienced by applications the operating 572 chapter 13 figure 13.9 device-status table system may also try to be fair  so that no one application receives especially poor service  or it may give priority service for delay-sensitive requests for instance  requests from the virtual memory subsystem may take priority over application requests several scheduling algorithms for disk i/0 are detailed in section 12.4 when a kernel supports asynchronous i/0  it must be able to keep track of many i/0 requests at the same time for this purpose  the operating system might attach the wait queue to a  able the kernel manages this table  which contains an entry for each i/0 device  as shown in figure 13.9 each table entry indicates the device 's type  address  and state  not functioning  idle  or busy   if the device is busy with a request  the type of request and other parameters will be stored in the table entry for that device one way in which the i/0 subsystem improves the efficiency of the computer is by scheduling i/0 operations another way is by using storage space in main memory or on disk via teclul.iques called buffering  caching  and spooling 13.4.2 buffering a is a memory area that stores data being transferred between two devices or between a device and an application buffering is done for three reasons one reason is to cope with a speed mismatch between the producer and consumer of a data stream suppose  for example  that a file is being received via modem for storage on the hard disk the modem is about a thousand times slower than the hard disk so a buffer is created in main mernory to accumulate the bytes received from the modem when an entire buffer of data has arrived  the buffer can be written to disk in a single operation since the disk write is not instantaneous and the modem still needs a place to store additional incoming data  two buffers are used after the modem fills the first buffer  the disk write is requested the modem then starts to fill the second buffer while the first buffer is written to disk by the time the modem has filled 13.4 573 the second buffer  the disk write from the first one should have completed  so the modem can switch back to the first buffer while the disk writes the second one this decouples the producer of data from the consun1.er  thus relaxing timing requirements between them the need for this decoupling is illustrated in figure 13.10  which lists the enormous differences in device speeds for typical computer hardware a second use of buffering is to provide adaptations for devices that have different data-transfer sizes such disparities are especially common in computer networking  where buffers are used widely for fragmentation and reassembly of messages at the sending side  a large message is fragmented into small network packets the packets are sent over the network  and the receiving side places them in a reassembly buffer to form an image of the source data a third use of buffering is to support copy semantics for application i/0 an example will clarify the meaning of copy semantics suppose that an application has a buffer of data that it wishes to write to disk it calls the write   systemcalt providing a pointer to the buffer and an integer specifying the number of bytes to write after the system call returns  what happens if the application changes the contents of the buffer with the version of the data written to disk is guaranteed to be version at the time of the application system calt independent of any subsequent changes in the application 's buffer a simple way in which the operating system can guarantee copy semantics is for the write   system call to copy the application i system bus hype  ~ ransport  32,pair  ~ ~ ~ iii ~ ~ ~ ~ ~ ~ ~ ~ i pci ~ xpress 2.0  32  i lnfi ! l.i band  qdr ; .1 2x  0.00001 0.001 0.1 10 1000 100000 1 efigure 13.10 sun enterprise 6000 device-transfer rates  logarithmic   574 chapter 13 data into a kernel buffer before returning control to the application the disk write is performed from the kernel buffer  so that subsequent changes to the application buffer have no effect copying of data between kernel buffers and application data space is common in operating systems  despite the overhead that this operation introduces  because of the clean semantics the same effect can be obtained more efficiently by clever use of virtual memory mapping and copy-on-write page protection 13.4.3 caching a is a region of fast memory that holds copies of data access to the cached copy is more efficient than access to the original for instance  the instructions of the currently running process are stored on disk  cached ilc physical memory  and copied again ill the cpu 's secondary and primary caches the difference between a buffer and a cache is that a buffer may hold the only existing copy of a data item  whereas a cache  by definition  holds a copy on faster storage of an item that resides elsewhere caching and buffering are distinct functions  but sometinces a region of memory can be used for both purposes for illstance  to preserve copy semantics and to enable efficient scheduling of disk i/0  the operating system uses buffers in maill memory to hold disk data these buffers are also used as a cache  to improve the i/o efficiency for files that are shared by applications or that are being written and reread rapidly when the kernel receives a file i/0 request  the kernel first accesses the buffer cache to see whether that region of the file is already available in main memory if it is  a physical disk i/o can be avoided or deferred also  disk writes are accumulated ill the buffer cache for several seconds  so that large transfers are gathered to allow efficient write schedules this strategy of delayilcg writes to improve i/o efficiency is discussed  in the context of remote file access  ill section 17.3 13.4.4 spooling and device reservation a is a buffer that holds output for a device  such as a printer  that can not accept ilcterleaved data streams although a prillter can serve only one job at a time  several applications may wish to print their output concurrently  without having their output mixed together the operating system solves this problem by intercepting all output to the printer each application 's output is spooled to a separate disk file when an application finishes printing  the spooling system queues the correspondilcg spool file for output to the printer the spooling system copies the queued spool files to the printer one at a time in some operating systems  spooling is managed by a system daemon process in others  it is handled by an in-kernel thread in either case  the operating system provides a control interface that enables users and system administrators to display the queue  remove unwanted jobs before those jobs print  suspend printing while the printer is serviced  and so on some devices  such as tape drives and printers  can not usefully multiplex the i/0 requests of multiple concurrent applications spooling is one way operating systems can coordinate concurrent output another way to deal with concurrent device access is to provide explicit facilities for coordination some operating systems  including vms  provide support for exclusive device access by enabling a process to allocate an idle device and to deallocate that device 13.4 575 when it is no longer needed other operating systems enforce a limit of one open file handle to such a device many operating systems provide functions that enable processes to coordinate exclusive access among then'lselves for instance  windows nt provides system calls to wait until a device object becomes available it also has a parameter to the open   system call that declares the types of access to be permitted to other concurrent threads on these systems  it is up to the applications to avoid deadlock 13.4.5 error handling an operating system that uses protected memory can guard against many kinds of hardware and application errors  so that a complete system failure is not the usual result of each minor mechanical glitch devices and i/0 transfers can fail in many ways  either for transient reasons  as when a network becomes overloaded  or for permanent reasons  as when a disk controller becomes defective operating systems can often compensate effectively for transient failures for instance  a disk read   failure results in a read   retry  and a network send   error results in a res end    if the protocol so specifies unfortunately  if an important component experiences a permanent failure  the operating system is unlikely to recover as a general rule  an i/0 system call will return one bit of information about the status of the call  signifying either success or failure in the unix operating system  an additional integer variable named errno is used to return an error code-one of about a hundred values-indicating the general nature of the failure  for example  argument out of range  bad pointer  or file not open   by contrast  some hardware can provide highly detailed error information  although many current operating systems are not designed to convey this information to the application for instance  a failure of a scsi device is reported by the scsi protocol in three levels of detail  a key that identifies the general nature of the failure  such as a hardware error or an illegal request ; an that states the category of failure  such as a bad command parameter or a self-test failure ; and an 'x ' l '.l that gives even more detail  such as which command parameter was in error or which hardware subsystem failed its self-test further  many scsi devices maintain internal pages of error-log information that can be requested by the host-but seldom are 13.4.6 1/0 protection errors are closely related to the issue of protection a user process may accidentally or purposely attempt to disrupt the normal operation of a systern by attempting to issue illegal i/0 instructions we can use various mechanisms to ensure that such disruptions cam'lot take place in the system to prevent users from performing illegal i/0  we define all i/0 instructions to be privileged instructions thus  users can not issue i/o instructions directly ; they must do it through the operating system to do i/0  a user program executes a system call to request that the operating system perform i/0 on its behalf  figure 13.11   the operating system  executing in monitor mode  checks that the request is valid and  if it is  does the i/0 requested the operating system then returns to the user 576 chapter 13 cd trap to monitor kernel perform 1/0 return to user user program figure 13.1 1 use of a system call to perform 1/0 in addition  any memory-mapped and i/o port memory locations must be protected from user access by the memory-protection system note that a kernel can not simply deny all user access most graphics games and video editing and playback software need direct access to memory-mapped graphics controller memory to speed the performance of the graphics  for example the kernel might in this case provide a locking mechanism to allow a section of graphics memory  representing a window on screen  to be allocated to one process at a time 13.4.7 kernel data structures the kernel needs to keep state information about the use of i/0 components it does so through a variety of in-kernel data structures  such as the open-file table structure from section 11.1 the kernel uses many similar structures to track network connections  character-device communications  and other i/0 activities unix provides file-system access to a variety of entities  such as user files  raw devices  and the address spaces of processes although each of these entities supports a read   operation  the semantics differ for instance  to read a user file  the kernel needs to probe the buffer cache before deciding whether to perform a disk i/0 to read a raw disk  the kernel needs to ensure that the request size is a multiple of the disk sector size and is aligned on a sector boundary to read a process image  it is merely necessary to copy data from memory unix encapsulates these differences within a uniform structure by using an object-oriented teclucique the open-file record  shown in 13.4 577 system-wide open-file table 1 ;  ;       ;  ' ; t ; . ' file-system record 1  ~  s  1 ~ if ~   inode pointer +   ;       ' pointer to read and write functions i      i  ; .l  pointer to select function ; ;  ;    ;  ~ ti ~ ~ ~ ~ ~ ple pointer to ioctl function file descriptor .,_   ;  \ ' '  ' ' pointer to close function n   .r              r ; i ~ ~ ~  ~ ~ ~ kl  f user-process memory networking  socket  record i  ' ~ 1 ~ t ~ 6 ~ ! v'.' pointer to network info + f   ;   pointer to read and write.functions     ~   pointer to select function pointer to ioctl function pointer to close f..un ction  kernel memory figure 13 12 unix 1/0 kernel structure figure 13.12  contains a dispatch table that holds pointers to the appropriate routines  depending on the type of file some operating systems use object-oriented methods even more extensively for instance  windows nt uses a message-passing implementation for i/0 an i/0 request is converted into a message that is sent through the kernel to the ii 0 manager and then to the device driver  each of which may change the message contents for output  the message contains the data to be written for input  the message contains a buffer to receive the data the message-passing approach can add overhead  by comparison with procedural techniques that use shared data structures  but it simplifies the structure and design of the i/0 system and adds flexibility 13.4.8 kernel i/o subsystem summary in summary  the i/0 subsystem coordinates an extensive collection of services that are available to applications and to other parts of the kernel the i/0 subsystenc supervises these procedures  management of the name space for files and devices access control to files and devices operation control  for example  a modem can not seek    file-system space allocation device allocation 578 chapter 13 13.5 buffering  caching  and spooling i/0 scheduling device-status monitoring  error handling  and failure recovery device-driver configuration and initialization the upper levels of the i/o subsystem access devices via the uniform interface provided by the device drivers earlier  we described the handshaking between a device driver and a device controller  but we did not explain how the operating system connects an application request to a set of network wires or to a specific disk sector consider  for example  reading a file from disk the application refers to the data by a file name within a disk  the file system maps from the file name through the file-system directories to obtain the space allocation of the file for instance  in ms-dos  the name maps to a number that indicates an entry in the file-access table  and that table entry tells which disk blocks are allocated to the file in unix  the name maps to an inode number  and the corresponding inode contains the space-allocation information but how is the connection made from the file name to the disk controller  the hardware port address or the memory-mapped controller registers  one method is that used by ms-dos  a relatively simple operating system the first part of an ms-dos file name  preceding the colon  is a string that identifies a specific hardware device for example  c  is the first part of every file name on the primary hard disk the fact that c  represents the primary hard disk is built into the operating system ; c  is mapped to a specific port address through a device table because of the colon separator  the device name space is separate from the file-system name space this separation makes it easy for the operating system to associate extra functionality with each device for instance  it is easy to invoke spooling on any files written to the printer if  instead  the device name space is incorporated in the regular file-system name space  as it is in unix  the normal file-system name services are provided automatically if the file system provides ownership and access control to all file names  then devices have owners and access control since files are stored on devices  such an interface provides access to the i/o system at two levels names can be used to access the devices themselves or to access the files stored on the devices unix represents device names in the regular file-system name space unlike an ms-dos file name  which has a colon separator  a unix path name has no clear separation of the device portion in fact  no part of the path name is the name of a device unix has a that associates prefixes of path names with specific device names to resolve a path name  unix looks up the name in the mount table to find the longest ncatchilcg prefix ; the corresponding entry in the mount table gives the device name this device name also has the form of a name in the file-system name space when unix looks up this name in the file-system directory structures  it finds not an inode number but a major  13.5 579 minor device number the m.ajor device number identifies a device driver that should be called to handle l/0 to this device the minor device number is passed to the device driver to index into a device table the corresponding device-table entry gives the port address or the memory-mapped address of the device controller modern operating systems obtain significant flexibility from the multiple stages of lookup tables in the path between a request and a physical device controller the mechanisms that pass requests between applications and drivers are general thus  we can introduce new devices and drivers into a computer without recompiling the kernel in fact  some operating systems have the ability to load device drivers on demand at boot time  the system first probes the hardware buses to determine what devices are present ; it then loads in the necessary drivers  either immediately or when first required by an i/0 request we next describe the typical life cycle of a blocking read request  as depicted in figure 13.13 the figure suggests that an i/0 operation requires a great many steps that together consume a tremendous number of cpu cycles a process issues a blocking read   system call to a file descriptor of a file that has been opened previously the system-call code in the kernel checks the parameters for correctness in the case of input  if the data are already available irl the buffer cache  the data are returned to the process  and the i/o request is completed otherwise  a physical i/0 must be performed the process is removed from the run queue and is placed on the wait queue for the device  and the i/0 request is scheduled eventually  the i/0 subsystem sends the request to the device driver depending on the operating system  the request is sent via a subroutine call or an in-kernel message the device driver allocates kernel buffer space to receive the data and schedules the i/0 eventually  the driver sends commands to the device controller by writing into the device-control registers the device controller operates the device hardware to perform the data transfer the driver may poll for status and data  or it may have set up a dma transfer into kernel memory we assume that the transfer is managed by a dma controller  which generates an interrupt when the transfer completes the correct interrupt handler receives the interrupt via the interruptvector table  stores any necessary data  signals the device driver  and returns from the interrupt the device driver receives the signal  determines which i/0 request has completed  determines the request 's status  and signals the kernel i/0 subsystem that the request has been completed the kernel transfers data or return codes to the address space of the requesting process and moves the process from the wait queue back to the ready queue 580 chapter 13 13.6 system call device-controller commands user process kernel 1/0 subsystem kernel 1/0 subsystem device driver interrupt handler device controller return from system call interrupt ~ -------tim_e ~  ~   figure 13.13 the life cycle of an 1/0 request moving the process to the ready queue unblocks the process when the scheduler assigns the process to the cpu  the process resumes execution at the completion of the system call unix system v has an interesting mechanism  called that enables an application to assemble pipelines of driver code dynamically a stream is a full-duplex connection between a device driver and a user-level process it consists of a that interfaces with the user process  a  id that controls the device  and zero or more between the stream user process 13.6 i streams modules _j figure 13.14 the streams structure 581 head and the driver end each of these components contains a pair of queues -a read queue and a write queue message passing is used to transfer data between queues the streams structure is shown in figure 13.14 modules provide the functionality of streams processing ; they are pushed onto a stream by use of the ioctl   system call for examplef a process can open a serial-port device via a stream and can push on a module to handle input editing because messages are exchanged between queues in adjacent modules  a queue in one module may overflow an adjacent queue to prevent this from occurring  a queue may support without flow control  a queue accepts all messages and immediately sends them on to the queue in the adjacent module without buffering them a queue supporting flow control buffers messages and does not accept messages without sufficient buffer space ; this process involves exchanges of control messages between queues in adjacent modules a user process writes data to a device using either the write   orputmsg   system call the write   system call writes raw data to the stream  whereas putmsg   allows the user process to specify a message regardless of the system call used by the user process  the stream head copies the data into a message and delivers it to the queue for the next module in line this copying of messages continues until the message is copied to the driver end and hence the device similarly  the user process reads data from the stream head using either the read   or getmsg   system call if read   is used  the stream head gets a message from its adjacent queue and returns ordinary data  an unstructured byte stream  to the process if getmsg   is used  a message is returned to the process 582 chapter 13 13.7 streams i/0 is asynchronous  or nonblocking  except when the user process communicates with the stream ~ head when writing to the stream  the user process will block  assuming the next queue uses flow controt until there is room to copy the message likewise  the user process will block when reading from the stream ~ until data are available as mentioned  the driver end-like the stream head and modules-has a read and write queue however  the driver end must respond to interrupts  such as one triggered when a frame is ready to be read from a network unlike the stream head  which may block if it is unable to copy a message to the next queue in line  the driver end must handle all incoming data drivers must support flow control as well however  if a device 's buffer is fult the device typically resorts to dropping incoming messages consider a network card whose input buffer is full the network card must simply drop further messages until there is ample buffer space to store incoming messages the benefit of using streams is that it provides a framework for a modular and incremental approach to writing device drivers and network protocols modules may be used by different streams and hence by different devices for example  a networking module may be used by both an ethernet network card and a 802.11 wireless network card furthermore  rather than treating character-device i/o as an unstructured byte stream  streams allows support for message boundaries and control information when communicating between modules most unix variants support streams  and it is the preferred method for writing protocols and device drivers for example  system v unix and solaris implement the socket mechanism using streams i/ 0 is a major factor in system performance it places heavy demands on the cpu to execute device-driver code and to schedule processes fairly and efficiently as they block and unblock the resulting context switches stress the cpu and its hardware caches i/o also exposes any inefficiencies in the interrupt-handling mechanisms in the kernel in addition  i/o loads down the memory bus during data copies between controllers and physical memory and again durilcg copies between kernel buffers and application data space coping gracefully with all these demands is one of the major concerns of a computer architect although modern computers can handle many thousands of interrupts per second  interrupt handling is a relatively expensive task each interrupt causes the system to perform a state change  to execute the interrupt handler  and then to restore state programmed i/0 can be more efficient than internjpt-driven i/0  if the number of cycles spent in busy waiting is not excessive an i/0 completion typically unblocks a process  leading to the full overhead of a context switch network traffic can also cause a high context-switch rate consider  for instance  a remote login from one machine to another each character typed on the local machine must be transported to the remote machine on the local machine  the character is typed ; a keyboard interrupt is generated ; and the character is passed through the interrupt handler to the device driver  to the kernet and then to the user process the user process issues a network i/o system call to send the character to the remote machine the character then 13.7 583 flows into the local kernel  through the network layers that construct a network packet  and into the network device driver the network device driver transfers the packet to the network controller  which sends the character and generates an interrupt the interrupt is passed back up through the kernel to cause the network l/0 system call to complete now  the remote system 's network hardware receives the packet  and an interrupt is generated the character is unpacked from the network protocols and is given to the appropriate network daemon the network daemon identifies which remote login session is involved and passes the packet to the appropriate subdaemon for that session throughout this flow  there are context switches and state switches  figure 13.15   usually  the receiver echoes the character back to the sender ; that approach doubles the work to eliminate the context switches involved in moving each character between daemons and the kernel  the solaris developers reimplemented the daemon using in-kernel threads sun estimates that this improvement sending system receiving system figure 13.15 lntercomputer communications 584 chapter 13 increased the maximum number of network logins from a few hundred to a few thousand on a large server other systems use separate for terminal i/0 to reduce the interrupt burden on the main cpu for instance  a can multiplex the traffic from hundreds of remote terminals into one port on a large computer an is a dedicated  special-purpose cpu found in mainframes and in other high-end systems the job o a channel is to offload i/0 work from the main cpu the idea is that the cham1.els keep the data flowing smoothly  while the main cpu remains free to process the data like the device controllers and dma controllers found in smaller computers  a channel can process more general and sophisticated programs  so channels can be tuned for particular workloads we can employ several principles to improve the efficiency of i/0  reduce the number of context switches reduce the number of times that data must be copied in memory while passing between device and application reduce the frequency of interrupts by using large transfers  smart controllers  and polling  if busy waiting can be minimized   increase concurrency by using dma-knowledgeable controllers or channels to offload simple data copying from the cpu move processing primitives into hardware  to allow their operation in device controllers to be concurrent with cpu and bus operation balance cpu  memory subsystem  bus  and i/o performance  because an overload in any one area will cause idleness in others i/0 devices vary greatly in complexity for instance  a mouse is simple the mouse movements and button clicks are converted into numeric values that are passed from hardware  through the mouse device driver  to the application by contrast  the functionality provided by the windows nt disk device driver is complex it not only manages individual disks but also implements raid arrays  section 12.7   to do so  it converts an application 's read or write request into a coordinated set of disk i/0 operations moreover  it implements sophisticated error-handling and data-recovery algorithms and takes many steps to optimize disk performance where should the i/0 functionality be implemented -in the device hardware  in the device driver  or in application software sometimes we observe the progression depicted in figure 13.16 initially  we implement experimental i/0 algorithms at the application level  because application code is f1exible and application bugs are unlikely to cause system crashes furthermore  by developing code at the application level  we avoid the need to reboot or reload device drivers after every change to the code an application-level implementation can be inefficient  however  because of the overhead o context switches and because the application can not take advantage of internal kernel data structures and 13.8 13.8 585 device code  hardware  figure 13.16 device functionality progression kernel functionality  such as efficient in-kernel messaging  threading  and locking   when an application-level algorithm has demonstrated its worth  we may reimplement it in the kernel this can improve performance  but the development effort is more challenging  because an operating-system kernel is a large  complex software system moreover  an in-kernel implementation must be thoroughly debugged to avoid data corruption and system crashes the highest performance may be obtained through a specialized implementation in hardware  either in the device or in the controller the disadvantages of a hardware implementation include the difficulty and expense of making further improvements or of fixing bugs  the increased development time  months rather than days   and the decreased flexibility for instance  a hardware raid controller may not provide any means for the kernel to influence the order or location of individual block reads and writes  even if the kernel has special information about the workload that would enable it to improve the i/0 performance the basic hardware elements involved in i/0 are buses  device controllers  and the devices themselves the work of moving data between devices and main memory is perform.ed by the cpu as programmed i/0 or is offloaded to a dma controller the kernel module that controls a device is a device driver the system-call interface provided to applications is designed to handle several basic categories of hardware  including block devices  character devices  memory-mapped files  network sockets  and programmed interval timers the system calls usually block the processes that issue them  but nonblocking and 586 chapter 13 asynchronous calls are used by the kernel itself and by applications that must not sleep while waiting for an i/0 operation to complete the kernel 's i/o subsystem provides num.erous services among these are i/0 scheduling  buffering  caching  spooling  device reservation  and error handling another service  name translation  makes the connections between hardware devices and the symbolic file names used by applications it involves several levels of mapping that translate from character-string names  to specific device drivers and device addresses  and then to physical addresses of ii 0 ports or bus controllers this mapping may occur within the file-system name space  as it does in unix  or in a separate device name space  as it does in ms-dos streams is an implementation and methodology that provides a framework for a modular and incremental approach to writing device drivers and network protocols through streams  drivers can be stacked  with data passing through them sequentially and bidirectionally for processing i/o system calls are costly in terms of cpu consumption because of the many layers of software between a physical device and an application these layers imply overhead from several sources  context switching to cross the kernel 's protection boundary  signal and interrupt handling to service the i/0 devices  and the load on the cpu and memory system to copy data between kernel buffers and application space 13.1 write  in pseudocode  an implementation of virtual clocks  including the queueing and management of timer requests for the kernel and applications assume that the hardware provides three timer channels 13.2 what are the advantages and disadvantages of supporting memorymapped i/0 to device control registers 13.3 typically  at the completion of a device i/0  a single interrupt is raised and appropriately handled by the host processor in certain settings  however  the code that is to be executed at the completion of the i/0 can be broken into two separate pieces the first piece executes immediately after the i/0 completes and schedules a second interrupt for the remaining piece of code to be executed at a later time what is the purpose of using this strategy in the design of interrupt handlers 13.4 why might a system use interrupt-driven i/0 to manage a single serial port and polling i/0 to manage a front-end processor  such as a termii1.al concentrator 13.5 what are the various kinds of performance overhead associated with servicing an interrupt 13.6 unix coordinates the activities of the kernel i/0 components by manipulating shared in-kernel data structures  whereas windows nt uses object-oriented message passing between kernel i/o components discuss three pros and three cons of each approach 587 13.7 in most multiprogrammed systems  user programs access memory through virtual addresses  while the operating system uses raw physical addresses to access men10ry what are the implications of this design for the initiation of i/0 operations by the user program and their execution by the operating system 13.8 polling for an i/0 completion can waste a large number of cpu cycles if the processor iterates a busy-waiting loop many times before the i/0 completes but if the i/0 device is ready for service  polling can be much more efficient than is catching and dispatching an interrupt describe a hybrid strategy that combines polling  sleeping  and interrupts for i/0 device service for each of these three strategies  pure polling  pure interrupts  hybrid   describe a computing environment in which that strategy is more efficient than is either of the others 13.9 consider the following i/0 scenarios on a single-user pc  a a mouse used with a graphical user interface b a tape drive on a multitasking operating system  with no device preallocation available  c a disk drive containing user files d a graphics card with direct bus connection  accessible through memory-mapped i/0 for each of these scenarios  would you design the operating system to use buffering  spooling  caching  or a combin_ation would you use polled i/o or interrupt-driven i/0 give reasons for your choices 13.10 the example of handshaking in section 13.2 used 2 bits  a busy bit and a command-ready bit is it possible to implement this handshaking with only 1 bit if it is  describe the protocol if it is not  explain why 1 bit is insufficient 13.11 discuss the advantages and disadvantages of guaranteeing reliable transfer of data between modules in the streams abstraction 13.12 some dma controllers support direct virtual memory access  where the targets of i/0 operations are specified as virtual addresses and a translation from virtual to physical address is performed during the dma how does this design complicate the design of the dma controller what are the advantages of providing such functionality 13.13 why is it important to scale up system-bus and device speeds as cpu speed increases 13.14 when multiple interrupts from different devices appear at about the same time  a priority scheme could be used to determine the order in which the interrupts would be serviced discuss what issues need to be considered in assigning priorities to different interrupts 588 chapter 13 13.15 describe three circumstances under which blocking ii 0 should be used describe three circumstances under which nonblocking i/0 should be used why not just implement nonblocking i/0 and have processes busy-wait until their devices are ready vahalia  1996  provides a good overview of i/o and networking in unix leffler et al  1989  detail the i/o structures and methods employed in bsd unix milenkovic  1987  discusses the complexity of i/0 methods and implementation the use and programming of the various interprocesscommunication and network protocols in unix are explored in stevens  1992   brain  1996  documents the windows nt application interface the i/o implementation in the sample minix operating system is described in tanenbaum and woodhull  1997   custer  1994  includes detailed information on the nt message-passing implementation of i/0 for details of hardware-level ii 0 handling and memory-mapping functionality  processor reference manuals  motorola  1993  and intel  1993   are among the best sources hennessy and patterson  2002  describe multiprocessor systems and cache-consistency issues tanenbaum  1990  describes hardware i/0 design at a low level  and sargent and shoemaker  1995  provide a programmer 's guide to low-level pc hardware and software the ibm pc device i/o address map is given in ibm  1983   the march 1994 issue of ieee computer is devoted to i/0 hardware and software raga  1993  provides a good discussion of streams part six protection mechanisms control access to a system by limiting the types of file access permitted to users in addition  protection must ensure that only processes that have gained proper authorization from the operating system can operate on memory segments  the cpu  and other resources protection is provided by a mechanism that controls the access of programs  processes  or users to the resources defined by a computer system this mechanism must provide a means for specifying the controls to be imposed  together with a means of enforcing them security ensures the authentication of system users to protect the integrity of the information stored in the system  both data and code   as well as the physical resources of the computer system the security system prevents unauthorized access  malicious destruction 01 alteration of data  and accidental introduction of inconsistency 14.1 chapter the processes in an operating system must be protected from one another 's activities to provide such protection  we can use various mechanisms to ensure that only processes that have gained proper authorization from the operating system can operate on the files  memory segments  cpu  and other resources of a system protection refers to a mechanism for controlling the access of programs  processes  or users to the resources defined by a computer system this mechanism must provide a means for specifying the controls to be imposed  together with a means of enforcement we distinguish between protection and security  which is a measure of confidence that the integrity of a system and its data will be preserved in this chapter  we focus on protection security assurance is a much broader topic  and we address it in chapter 15 to discuss the goals and principles of protection in a modern computer system to explain how protection domains  combined with an access matrix  are used to specify the resources a process may access to examine capability and language-based protection systems as computer systems have become more sophisticated and pervasive in their applications  the need to protect their integrity has also grown protection was originally conceived as an adjunct to multiprogramming operating systems  so that untrustworthy users might safely share a common logical name space  such as a directory of files  or share a common physical name space  such as memory modern protection concepts have evolved to increase the reliability of any complex system that makes use of shared resources we need to provide protection for several reasons the most obvious is the need to prevent the mischievous  intentional violation of an access restriction 591 592 chapter 14 14.2 by a user of more general importance  however  is the need to ensure that each program component active in a system uses system resources only in ways consistent with stated policies this requirement is an absolute one for a reliable system protection can improve reliability by detecting latent errors at the interfaces between component subsystems early detection of interface errors can often prevent contamination of a healthy subsystem by a malfunctioning subsystem also  an unprotected resource can not defend against use  or misuse  by an unauthorized or incompetent user a protection-oriented system provides means to distinguish between authorized and unauthorized usage the role of protection in a computer system is to provide a mechanism for the enforcement of the policies governing resource use these policies can be established in a variety of ways some are fixed in the design of the system  while others are formulated by the management of a system still others are defined by the individual users to protect their own files and programs a protection system must have the flexibility to enforce a variety of policies policies for resource use may vary by application  and they may change over time for these reasons  protection is no longer the concern solely of the designer of an operating system the application programmer needs to use protection mechanisms as well  to guard resources created and supported by an application subsystem against misuse in this chapter  we describe the protection mechanisms the operating system should provide  but application designers can use them as well in designing their own protection software note that mechanisms are distinct from policies mechanisms determine how something will be done ; policies decide what will be done the separation of policy and mechanism is important for flexibility policies are likely to change from place to place or time to time in the worst case  every change in policy would require a change in the underlying mechanism using general mechanisms enables us to avoid such a situation frequently  a guiding principle can be used throughout a project  such as the design of an operating system following this principle simplifies design decisions and keeps the system consistent and easy to understand a key  time-tested guiding principle for protection is the it dictates that programs  users  and even systems be given just enough privileges to perform their tasks consider the analogy of a security guard with a passkey if this key allows the guard into just the public areas that she guards  then misuse of the key will result in minimal damage if  however  the passkey allows access to all areas  then damage from its being lost  stolen  misused  copied  or otherwise compromised will be much greater an operating system following the principle of least privilege implements its features  programs  system calls  and data structures so that failure or compromise of a component does the minimum damage and allows the n1inimum damage to be done the overflow of a buffer in a system daemon might cause the daemon process to fail  for example  but should not allow the execution of code from the daemon process 's stack that would enable a remote 14.3 14.3 593 user to gain maximum privileges and access to the entire system  as happens too often today   such an operating system also provides system calls and services that allow applications to be written with fine-grained access controls it provides mechanisms to enable privileges when they are needed and to disable them when they are not needed also beneficial is the creation of audit trails for all privileged function access the audit trail allows the prograrnmer  systems administrator  or law-enforcement officer to trace all protection and security activities on the system managing users with the principle of least privilege entails creating a separate account for each user  with just the privileges that the user needs an operator who needs to mount tapes and back up files on the system has access to just those commands and files needed to accomplish the job some systems implement role-based access control  rbac  to provide this functionality computers implemented in a computing facility under the principle of least privilege can be limited to running specific services  accessing specific remote hosts via specific services  and doing so during specific times typically  these restrictions are implemented through enabling or disabling each service and through using access control lists  as described in sections 10.6.2 and 14.6 the principle of least privilege can help produce a more secure computing environment unfortunately  it frequently does not for example  windows 2000 has a complex protection scheme at its core and yet has many security holes by comparison  solaris is considered relatively secure  even though it is a variant of unix  which historically was designed with little protection in mind one reason for the difference may be that windows 2000 has more lines of code and more services than solaris and thus has more to secure and protect another reason could be that the protection scheme in windows 2000 is irtcomplete or protects the wrong aspects of the operating system  leaving other areas vulnerable a computer system is a collection of processes and objects by objects  we mean both  such as the cpu  memory segments  printers  disks  and tape drives  and  such as files  programs  and semaphores   each object has a unique name that differentiates it from all other objects in the system  and each can be accessed only through well-defined and meaningful operations objects are essentially abstract data types the operations that are possible may depend on the object for example  on a cpu  we can only execute memory segments can be read and written  whereas a cd-rom or dvd-rom can only be read tape drives can be read  written  and rewound data files can be created  opened  read  written  closed  and deleted ; program files can be read  written  executed  and deleted a process should be allowed to access only those resources for which it has authorization furthermore  at any time  a process should be able to access only those reso1jrces that it currently reqllires to complete its task this second requirement  conunonly referred to as the need-to-know principle  is useful in limiting the amount of damage a faulty process can cause in the system for example  when process p invokes procedure a    the procedure should be 594 chapter14 allowed to access only its own variables and the formal parameters passed to it ; it should not be able to access all the variables of process p similarly  consider the case in which process p invokes a compiler to compile a particular file the compiler should not be able to access files arbitrarily but should have access only to a well-defined subset of files  such as the source file  listing file  and so on  related to the file to be compiled conversely  the compiler may have private files used for accounting or optimization purposes that process p should not be able to access the need-to-know principle is similar to the principle of least privilege discussed in section 14.2 in that the goals of protection are to minimize the risks of possible security violations 14.3.1 domain structure to facilitate the scheme just described  a process operates within a which specifies the resources that the process may access each domain defines a set of objects and the types of operations that may be invoked on each object the ability to execute an operation on an object is an a domain is a collection of access rights  each of which is an ordered pair object-name  rights-set  for example  if domain d has the access right file f   read  write   then a process executing in domain d can both read and write file f ; it can not  however  perform any other operation on that object domains do not need to be disjoint ; they may share access rights for example  in figure 14.1  we have three domains  d1  d2  and d3  the access right 0 4   print  is shared by d2 and d3  implying that a process executing in either of these two domains can print object 0 4  note that a process must be executing in domain d1 to read and write object 0 1  while only processes in domain d3 may execute object 0 1 the association between a process and a domain may be either if the set of resources available to the process is fixed throughout the process 's lifetime  or as might be expected  establishing dynamic protection domains is more complicated than establishing static protection domains if the association between processes and domains is fixed  and we want to adhere to the need-to-know principle  then a mechanism must be available to change the content of a domain the reason stems from the fact that a process may execute in two different phases and may  for example  need read access in one phase and write access in another if a domain is static  we must define the domain to include both read and write access however  this arrangement provides more rights than are needed in each of the two phases  since we have read access in the phase where we need only write access  and vice versa thus  the need-to-know principle is violated we must allow the contents of 0 3   read  write  0 1  read  write  0 2   execute  0 2   write  01   execute  0 3   read  figure 14.1 system with three protection domains 14.3 595 a domain to be modified so that the domain always reflects the n1inimum necessary access rights if the association is dynamic  a mechanism is available to allow enabling the process to switch from one domain to another we may also want to allow the content of a domain to be changed if we can not change the content of a domain  we can provide the same effect by creating a new domain with the changed content and switching to that new domain when we want to change the domain content a domain can be realized in a variety of ways  each user may be a domain in this case  the set of objects that can be accessed depends on the identity of the user domain switching occurs when the user is changed -generally when one user logs out and another user logs in each process may be a domain in this case  the set of objects that can be accessed depends on the identity of the process domain switching occurs when one process sends a message to another process and then waits for a response each procedure may be a domain in this case  the set of objects that can be accessed corresponds to the local variables defined within the procedure domain switching occurs when a procedure call is made we discuss domain switching in greater detail in section 14.4 consider the standard dual-mode  monitor-user mode  model of operating-system execution when a process executes in monitor mode  it can execute privileged instructions and thus gain complete control of the computer system in contrast  when a process executes in user mode  it can invoke only nonprivileged instructions consequently  it can execute only within its predefined memory space these two modes protect the operating system  executing in monitor domain  from the user processes  executing in user domain   in a multiprogrammed operating system  two protection domains are insufficient  since users also want to be protected from one another therefore  a more elaborate scheme is needed we illustrate such a scheme by examining two influential operating systems-unix and multics -to see how they implement these concepts 14.3.2 an example  unix in the unix operating system  a domain is associated with the user switching the domain corresponds to changing the user identification temporarily this change is accomplished tbough the file system as follows an owner identification and a domain bit  known as the setuid bit  are associated with each file when the setuid bit is on  and a user executes that file  the user id is set to that of the owner of the file ; when the bit is off  however  the user id does not change for example  when a user a  that is  a user with userid = a  starts executing a file owned by b  whose associated domain bit is off  the userid of the process is set to a when the setuid bit is on  the userid is set to that of the owner of the file  b when the process exits  this temporary userid change ends 596 chapter 14 other methods are used to change domains in operating systems in which user ids are used for domain definition  because almost all systems need to provide such a mechanism this mechanism is used when an otherwise privileged facility needs to be made available to the general user population for instance  it might be desirable to allow users to access a network without letting them write their own networking programs in such a case  on a unix system  the setuid bit on a networking program would be set  causing the user ld to change when the program was run the user ld would change to that of a user with network access privilege  such as root  the most powerful user id   one problem with this method is that if a user manages to create a file with user id root and with its setuid bit on  that user can become root and do anything and everything on the system the setuid mechanism is discussed further in appendix a an alternative to this method used in other operating systems is to place privileged programs in a special directory the operating system would be designed to change the user ld of any program run from this directory  either to the equivalent of root or to the user ld of the owner of the directory this eliminates one security problem with setuid programs in which crackers create and hide such programs for later use  using obscure file or directory names   this method is less flexible than that used in unix  however even more restrictive  and thus more protective  are systems that simply do not allow a change of user id in these instances  special techniques must be used to allow users access to privileged facilities for instance  a may be started at boot time and run as a special user id users then run a separate program  which sends requests to this process whenever they need to use the facility this method is used by the tops-20 operating system in any of these systems  great care must be taken in writing privileged programs any oversight can result in a total lack of protection on the system generally  these programs are the first to be attacked by people trying to break into a system ; unfortunately  the attackers are frequently successful for example  security has been breached on many unix systems because of the setuid feature we discuss security in chapter 15 14.3.3 an example  mul tics in the multics system  the protection domains are organized hierarchically into a ring structure each ring corresponds to a single domain  figure 14.2   the rings are numbered from 0 to 7 let d ; and dj be any two domain rings if j i  then d ; is a subset of dj that is  a process executing in domain dj has more privileges than does a process executing in domain d ;  a process executing in domain do has the most privileges if only two rings exist  this scheme is equivalent to the monitor-user n1ode of execution  where monitor mode corresponds to do and user mode corresponds to d1 multics has a segmented address space ; each segment is a file  and each segment is associated with one of the rings a segm.ent description includes an entry that identifies the ring number in addition  it includes three access bits to control reading  writing  and execution the association between segments and rings is a policy decision with which we are not concerned here a current-ring-number counter is associated with each process  identifying the ring in which the process is executing currently when a process is executing 14.3 597 figure 14.2 multics ring structure in ring i  it cmmot access a segment associated with ring j  j i   it can access a segment associated with ring k  k    i   the type of access  however  is restricted according to the access bits associated with that segment domain switching in multics occurs when a process crosses from one ring to another by calling a procedure in a different ring obviously  this switch must be done in a controlled mmmer ; otherwise  a process could start executing in ring 0  and no protection would be provided to allow controlled domain switching  we modify the ring field of the segment descriptor to include the following  access bracket a pair of integers  bl and b2  such that bl   =  b2 limit an integer b3 such that b3 b2 list of gates identifies the entry points  or may be called at which the segments if a process executing in ring i calls a procedure  or segncent  with access bracket  bl,b2   then the call is allowed if bl   =  i   =  b2  and the current ring number of the process remains i otherwise  a trap to the operating system occurs  and the situation is handled as follows  if i bl  then the call is allowed to occur  because we have a transfer to a ring  or domain  with fewer privileges however  if parameters are passed that refer to segments in a lower ring  that is  segments not accessible to the called procedure   then these segments must be copied into an area that can be accessed by the called procedure if i b2  then the call is allowed to occur only if b3 is greater than or equal to i and the call has been directed to one of the designated entry points in the list of gates this scheme allows processes with limited access rights to call procedures in lower rings that have more access rights  but only in a carefully controlled mmmer 598 chapter 14 14.4 the main disadvantage of the ring  or hierarchical  structure is that it does not allow us to enforce the need-to-know principle in particular  if an object must be accessible in domain 0 j but not accessible in domain oi  then we must have j i but this requirement means that every segment accessible in oi is also accessible in 0 1 the multics protection system is generally more complex and less efficient than are those used in current operating systems if protection interferes with the ease of use of the system or significantly decreases system performance  then its use must be weighed carefully against the purpose of the system for instance  we would want to have a complex protection system on a computer used by a university to process students ' grades and also used by students for classwork a similar protection system would not be suited to a computer being used for number crunching  in which performance is of utmost importance we would prefer to separate the mechanism from the protection policy  allowing the same system to have complex or simple protection depending on the needs of its users to separate mechanism from policy  we require a more general model of protection our model of protection can be viewed abstractly as a matrix  called an the rows of the access matrix represent domains  and the columns represent objects each entry in the matrix consists of a set of access rights because the column defines objects explicitly  we can omit the object name from the access right the entry access  i,j  defines the set of operations that a process executing in domain oi can invoke on object oj to illustrate these concepts  we consider the access matrix shown in figure 14.3 there are four domains and four objects-three files  f1  f2  f3  and one laser printer a process executing in domain 0 1 can read files f1 and f3  a process executing in domain 0 4 has the same privileges as one executing in domain 0 1 ; but in addition  it can also write onto files f1 and f3  note that the laser printer can be accessed only by a process executing in domain 0 2 the access-matrix scheme provides us with the mechanism for specifying a variety of policies the mechanism consists of implementing the access 01 read read 02 print 03 read execute 04 read read write write figure 14.3 access matrix 14.4 599 matrix and ensuring that the semantic properties we have outlined hold more specifically  we must ensure that a process executing in domain n can access only those objects specified in row  and then only as allowed by the access-matrix entries the access matrix can implement policy decisions concerning protection the policy decisions involve which rights should be included in the  i,j  th entry we must also decide the domain in which each process executes this last policy is usually decided by the operating system the users normally decide the contents of the access-matrix entries when a user creates a new object oi  the column oi is added to the access matrix with the appropriate initialization entries  as dictated by the creator the user may decide to enter some rights in some entries in cohum1 j and other rights in other entries  as needed the access matrix provides an appropriate mechanism for defining and implementing strict control for both the static and dynamic association between processes and domains when we switch a process from one domain to another  we are executing an operation  switch  on an object  the domain   we can control domain switching by including domains among the objects of the access matrix similarly  when we change the content of the access matrix  we are performing an operation on an object  the access matrix again  we can control these changes by including the access matrix itself as an object actually  since each entry in the access matrix may be modified individually  we must consider each entry in the access matrix as an object to be protected now  we need to consider only the operations possible on these new objects  domains and the access matrix  and decide how we want processes to be able to execute these operations processes should be able to switch from one domain to another switching from domain d ; to domain di is allowed if and only if the access right switch e access  i,j   thus  in figure 14.4  a process executing in domain d2 can switch to domain d3 or to domain d4  a process in domain d4 can switch to d1  and one in domain d1 can switch to d2 allowing controlled change in the contents of the access-matrix entries requires three additional operations  copy  owner  and control we examine these operations next 01 read read switch 02 print switch switch 03 read execute 04 read read switch write write figure 14.4 access matrix of figure 14.3 with domains as objects 600 chapter 14  a  execute read execute execute read  b  figure 14.5 access matrix with copy rights the ability to copy an access right from one domain  or row  of the access matrix to another is denoted by an asterisk   appended to the access right the copy right allows the access right to be copied only within the colurrm  that is  for the object  for which the right is defined for example  in figure 14.5  a   a process executing in domain d2 can copy the read operation into any entry associated with file f2  hence  the access matrix of figure 14.5  a  can be modified to the access matrix shown in figure 14.5  b   this scheme has two variants  a right is copied from access  i  j  to access  jc  j  ; it is then removed from access  i  j   this action is a transfer of a right  rather than a copy propagation of the copy right may be limited that is  when the right r is copied from access  i,j  to access  lc,j   only the right r  not r  is created a process executing in domain d ~ r can not further copy the right r a system may select only one of these three copy rights  or it may provide all three by identifying them as separate rights  copy  transfer  and limited copy we also need a mechanism to allow addition of new rights and removal of some rights the owner right controls these operations if access  i  j  includes the owner right  then a process executing in domain di can add and remove any right in any entry in column j for example  in figure 14.6  a   domain d1 is the owner of f1 and thus can add and delete any valid right in column f1 similarly  domain d2 is the owner of f2 and f3 and thus can add and remove any valid right within these two columns thus  the access matrix of figure 14.6  a  can be modified to the access matrix shown in figure 14.6  b   14.4 601 01 owner write execute read read 02 owner owner write 03 execute  a  01 owner write execute owner read 02 read owner write write 03 write write  b  figure 14.6 access matrix with owner rights the copy and owner rights allow a process to change the entries in a column a mechanism is also needed to change the entries in a row the control right is applicable only to domain objects if access  i  j  includes the control right  then a process executing in domain di can remove any access right from row j for example  suppose that  in figure 14.4  we include the control right in access  d2  d4   then  a process executil1.g in domain d2 could modify domai11 d4  as shown in figure 14.7 read read switch print switch switch control read execute write write switch figure 14.7 modified access matrix of figure 14.4 602 chapter 14 14.5 the copy and owner rights provide us with a mechanism to limit the propagation of access rights however  they do not give us the appropriate tools for preventing the propagation  or disclosure  of information the problem of guaranteeing that no information initially held in an object can migrate outside of its execution environment is called the  this problem is in general unsolvable  see the bibliographical notes at the end of the chapter   these operations on the domains and the access matrix are not in themselves important  but they illustrate the ability of the access-matrix model to allow the implementation and control of dynamic protection requirements new objects and new domains can be created dynamically and included in the access-matrix model however  we have shown only that the basic mechanism exists ; system designers and users must make the policy decisions concerning which domains are to have access to which objects in which ways how can the access matrix be implemented effectively in general  the matrix will be sparse ; that is  most of the entries will be empty although datastructure techniques are available for representing sparse matrices  they are not particularly useful for this application  because of the way in which the protection facility is used here  we first describe several methods of implementing the access matrix and then compare the methods 14.5.1 global table the simplest implementation of the access matrix is a global table consisting of a set of ordered triples domain  object  rights-set  whenever an operation m is executed on an object oj within domain d ;  the global table is searched for a triple d ;  0 1  r ~ c  with me r ~ c if this triple is found  the operation is allowed to continue ; otherwise  an exception  or error  condition is raised this implementation suffers from several drawbacks the table is usually large and thus can not be kept in main memory  so additional i/0 is needed virtual memory techniques are often used for managing this table in addition  it is difficult to take advantage of special groupings of objects or domains for example  if everyone can read a particular object  this object must have a separate entry in every domain 14.5.2 access lists for objects each column in the access matrix can be implemented as an access list for one object  as described in section 10.6.2 obviously  the empty entries can be discarded the resulting list for each object consists of ordered pairs domain  rights-set  which define all domains with a nonempty set of access rights for that object this approach can be extended easily to define a list plus a default set of access rights when an operation m on an object oi is attempted in domain d ;  we search the access list for object 0 i  looking for an entry d ;  r1c with me rjc if the entry is found  we allow the operation ; if it is not  we check the default set if m is in the default set  we allow the access otherwise  access is 14.5 603 denied  and an exception condition occurs for efficiency  we may check the default set first and then search the access list 14.5.3 capability lists for domains rather than associating the columns of the access matrix with the objects as access lists  we can associate each row with its domain a ltst for a domain is a list of objects together with the operations allowed on tbose objects an object is often represented by its physical name or address  called a to execute operation m on object 0 1  the process executes the operation m  specifying the capability  or pointer  for object 0 j as a parameter simple of the capability means that access is allowed the capability list is associated with a domain  but it is never directly accessible to a process executing in that domain rather  the capability list is itself a protected object  maintained by the operating system and accessed by the user only indirectly capability-based protection relies on the fact that the capabilities are never allowed to migrate into any address space directly accessible by a user process  where they could be modified   if all capabilities are secure  the object they protect is also secure against unauthorized access capabilities were originally proposed as a kind of secure pointer  to meet the need for resource protection that was foreseen as multiprogrammed computer systems came of age the idea of an inherently protected pointer provides a fom1dation for protection that can be extended up to the applications level to provide inherent protection  we must distinguish capabilities from other kinds of objects  and they must be interpreted by an abstract machine on which higher-level programs run capabilities are usually distinguished from other data in one of two ways  each object has a to denote whether it is a capability or accessible data the tags themselves must not be directly accessible by an application program hardware or firmware support may be used to enforce this restriction although only one bit is necessary to distinguish between capabilities and other objects  more bits are often used this extension allows all objects to be tagged with their types by the hardware thus  the hardware can distinguish integers  floating-point numbers  pointers  booleans  characters  instructions  capabilities  and uninitialized values by their tags alternatively  the address space associated with a program can be split into two parts one part is accessible to the program and contains the program 's normal data and instructions the other part  containing the capability list  is accessible only by the operating system a segmented memory space  section 8.6  is useful to support this approach several capability-based protection systems have been developed ; we describe them briefly in section 14.8 the mach operating system also uses a version of capability-based protection ; it is described in appendix b 604 chapter 14 14.5.4 a lock-key mechanism the t ' is a compromise between access lists and capability lists each object has a list of unique bit patterns  called similarly  each domain has a list of unique bit patterns  called a process executing in a domain can access an object only if that domain has a key that matches one of the locks of the object as with capability lists  the list of keys for a domain must be managed by the operating system on behalf of the domain users are not allowed to examine or modify the list of keys  or locks  directly 14.5.5 comparison as you might expect choosing a technique for implementing an access matrix involves various trade-offs using a global table is simple ; however  the table can be quite large and often can not take advantage of special groupings of objects or domains access lists correspond directly to the needs of users when a user creates an object he can specify which domains can access the object as well as what operations are allowed however  because access-rights information for a particular domain is not localized  determining the set of access rights for each domain is difficult in addition  every access to the object must be checked  requiring a search of the access list in a large system with long access lists  this search can be time consuming capability lists do not correspond directly to the needs of users ; they are usefut however  for localizing information for a given process the process attempting access must present a capability for that access then  the protection system needs only to verify that the capability is valid revocation of capabilities  however  may be inefficient  section 14.7   the lock-key mechanism  as mentioned  is a compromise between access lists and capability lists the mechanism can be both effective and flexible  depending on the length of the keys the keys can be passed freely from domain to domain in addition  access privileges can be effectively revoked by the simple technique of changing some of the locks associated with the object  section 14.7   most systems use a combination of access lists and capabilities when a process first tries to access an object  the access list is searched if access is denied  an exception condition occurs otherwise  a capability is created and attached to the process additional references use the capability to demonstrate swiftly that access is allowed after the last access  the capability is destroyed this strategy is used in the multics system and in the cal system as an example of how such a strategy works  consider a file system in which each file has an associated access list when a process opens a file  the directory structure is searched to find the file  access permission is checked  and buffers are allocated all this information is recorded in a new entry in a file table associated with the process the operation returns an index into this table for the newly opened file all operations on the file are made by specification of the index into the file table the entry in the file table then points to the file and its buffers when the file is closed  the file-table entry is deleted since the file table is maintained by the operating system  the user carmot accidentally corrupt it thus  the user can access only those files that have been opened 14.6 14.6 605 since access is checked when the file is opened  protection is ensured this strategy is used in the unix system the right to access must still be checked or1 each access  and the file-table entry has a capability only for the allowed operations if a file is opened for reading  then a capability for read access is placed in the file-table entry if an attempt is made to write onto the file  the system identifies this protection violation by com.paring the requested operation with the capability in the file-table entry in section 10.6.2  we described how access controls can be used on files within a file system each file and directory are assigned an owner  a group  or possibly a list of users  and for each of those entities  access-control information is assigned a similar function can be added to other aspects of a computer system a good example of this is found in solaris 10 solaris 10 advances the protection available in the sun microsystems operating system by explicitly adding the principle of least privilege via this facility revolves around privileges a privilege is the right to execute a system call or to use an option within that system call  such as opening a file with write access   privileges can be assigned to processes,limiting them to exactly the access they need to perform their work privileges and programs can also be assigned to users are assigned roles or can take roles based on passwords to the roles in this way a user can take a role that enables a privilege  allowing the user to run a program to accomplish a specific task  as depicted in figure 14.8 this implementation of privileges decreases the security risk associated with superusers and setuid programs user1 executes with role 1 privileges ~ figure 14.8 role-based access control in solaris 10 606 chapter 14 14.7 notice that this facility is similar to the access matrix described in section 14.4 this relationship is further explored in the exercises at the end of the chapter in a dynamic protection system  we may sometimes need to revoke access rights to objects shared by different users various questions about revocation may arise  immediate versus delayed does revocation occur immediately  or is it delayed if revocation is delayed  can we find out when it will take place selective versus general when an access right to an object is revoked  does it affect all the users who have an access right to that object  or can we specify a select group of users whose access rights should be revoked partial versus total can a subset of the rights associated with an object be revoked  or must we revoke all access rights for this object temporary versus permanent can access be revoked permanently  that is  the revoked access right will never again be available   or can access be revoked and later be obtained again with an access-list scheme  revocation is easy the access list is searched for any access rights to be revoked  and they are deleted from the list revocation is immediate and can be general or selective  total or partial  and permanent or temporary capabilities  howeve1 ~ present a much more difficult revocation problem  as mentioned earlier since the capabilities are distributed throughout the system  we must find them before we can revoke them schemes that implement revocation for capabilities include the following  reacquisition periodically  capabilities are deleted from each domain if a process wants to use a capability  it may find that that capability has been deleted the process may then try to reacquire the capability if access has been revoked  the process will not be able to reacquire the capability back-pointers a list of pointers is maintained with each object  pointing to all capabilities associated with that object when revocation is required  we can follow these pointers  changing the capabilities as necessary this scheme was adopted in the multics system it is quite general  but its implementation is costly indirection the capabilities point indirectly  not directly  to the objects each capability points to a unique entry in a global table  which in turn points to the object we implement revocation by searching the global table for the desired entry and deleting it then  when an access is attempted  the capability is found to point to an illegal table entry table entries can be reused for other capabilities without difficulty  since both the capability and the table entry contain the unique name of the object the object for a 14.8 14.8 607 capability and its table entry must match this scheme was adopted in the cal system it does not allow selective revocation keys a key is a unique bit pattern that can be associated with a capability this key is defined when the capability is created  and it can be neither modified nor inspected by the process that owns the capability a is associated with each object ; it can be defined or replaced with the set-key operation when a capability is created  the current value of the master key is associated with the capability when the capability is exercised  its key is compared with the master key if the keys match  the operation is allowed to continue ; otherwise  an exception condition is raised revocation replaces the master key with a new value via the set-key operation  invalidating all previous capabilities for this object this scheme does not allow selective revocation  since only one master key is associated with each object if we associate a list of keys with each object  then selective revocation can be implemented finally  we can group all keys into one global table of keys a capability is valid only if its key matches some key in the global table we implement revocation by removing the matching key from the table with this scheme  a key can be associated with several objects  and several keys can be associated with each object  providing maximum flexibility in key-based schemes  the operations of defining keys  inserting them into lists  and deleting them from lists should not be available to all users in particular  it would be reasonable to allow only the owner of an object to set the keys for that object this choice  however  is a policy decision that the protection system can implement but should not define in this section  we survey two capability-based protection systems these systems differ in their complexity and in the types of policies that can be implemented on them neither system is widely used  but both provide interesting proving grounds for protection theories 14.8.1 an example  hydra hydra is a capability-based protection system that provides considerable flexibility the system implements a fixed set of possible access rights  including such basic forms of access as the right to read  write  or execute a memory segment in addition  a user  of the protection system  can declare other rights the interpretation of user-defined rights is performed solely by the user 's program  but the system provides access protection for the use of these rights  as well as for the use of system-defined rights these facilities constitute a significant development in protection technology operations on objects are defined procedurally the procedures that implement such operations are themselves a form of object  and they are accessed indirectly by capabilities the names of user-defined procedures must be identified to the protection system if it is to deal with objects of the userdefined type when the definition of an object is made krtown to hydra  the names of operations on the type become auxiliary rights 608 chapter 14 can be described in a capability for an instance of the type for a process to perform an operation on a typed object  the capability it holds for that object must contain the name of the operation being invoked among its auxiliary rights this restriction enables discrin lination of access rights to be made on an instance-by-instance and process-by-process basis hydra also provides   ' ! fnrwk ;  j1o  l this scheme allows a procedure to be certified as to act on a formal parameter of a specified type on behalf of any process that holds a right to execute the procedure the rights held by a trustworthy procedure are independent oc and may exceed  the rights held by the calling process however  such a procedure must not be regarded as universally trustworthy  the procedure is not allowed to act on other types  for instance   and the trustworthiness must not be extended to any other procedures or program segments that might be executed by a process amplification allows implementation procedures access to the representation variables of an abstract data type if a process holds a capability to a typed object a  for instance  this capability may include an auxiliary right to invoke some operation p but does not include any of the so-called kernel rights  such as read  write  or execute  on the segment that represents a such a capability gives a process a means of indirect access  through the operation p  to the representation of a  but only for specific purposes when a process invokes the operation p on an object a  howeve1 ~ the capability for access to a may be amplified as control passes to the code body of p this amplification may be necessary to allow p the right to access the storage segment representing a so as to implement the operation that p defines on the abstract data type the code body of p may be allowed to read or to write to the segment of a directly  even though the calling process cmmot on return from p the capability for a is restored to its originat unamplified state this case is a typical one in which the rights held by a process for access to a protected segment must change dynamically  depending on the task to be performed the dynamic adjustment of rights is performed to guarantee consistency of a programmer-defined abstraction amplification of rights can be stated explicitly in the declaration of an abstract type to the hydra operating system when a user passes an object as an argument to a procedure  we may need to ensure that the procedure can not modify the object we can implement this restriction readily by passing an access right that does not have the modification  write  right howeve1 ~ if amplification may occur  the right to modify may be reinstated thus  the user-protection requirement can be circumvented in generat of course  a user may trust that a procedure performs its task correctly this assumption is not always correct however  because of hardware or software errors hydra solves this problem by restricting amplifications the procedure-call mechanism of hydra was designed as a direct solution to the problem of mutually suspicious subsystems this problem is defined as follows suppose that a program is provided that can be invoked as a service by a number of different users  for example  a sort routine  a compile1 ~ a game   when users invoke this service program  they take the risk that the program will malfunction and will either damage the given data or retain some access right to the data to be used  without authority  later similarly  the service program may have som.e private files  for accounting purposes  14.8 609 for example  that should not be accessed directly by the calling user program hydra provides mechanisms for directly dealing with this problem a hydra subsystem is built on top of its protection kernel and may require protection of its own components a subsystem interacts with the kernel through calls on a set of kernel-defined primitives that define access rights to resources defined by the subsystenl the subsystem designer can define policies for use of these resources by user processes  but the policies are enforceable by use of the standard access protection afforded by the capability system programmers can make direct use of the protection system after acquainting themselves with its features in the appropriate reference rnanual hydra provides a large library of system-defined procedures that can be called by user programs programmers can explicitly incorporate calls on these system procedures into their program code or can use a program translator that has been interfaced to hydra 14.8.2 an example  cambridge cap system a different approach to capability-based protection has been taken in the design of the cambridge cap system cap 's capability system is simpler and superficially less powerful than that of hydra however  closer examination shows that it  too  can be used to provide secure protection of user-defined objects cap has two kinds of capabilities the ordinary kind is called a  it can be used to provide access to objects  but the only rights provided are the standard read  write  and execute of the individual storage segments associated with the object data capabilities are interpreted by microcode in the cap machine the second kind of capability is the so-called which is protected  but not interpreted  by the cap microcode it is interpreted by a protected  that is  privileged  procedure  which may be written by an application programmer as part of a subsystem a particular kind of rights amplification is associated with a protected procedure when executing the code body of such a procedure  a process temporarily acquires the right to read or write the contents of a software capability itself this specific kind of rights amplification corresponds to an implementation of the seal and unseal primitives on capabilities of course  this privilege is still subject to type verification to ensure that only software capabilities for a specified abstract type are passed to any such procedure universal trust is not placed in any code other than the cap machine 's microcode  see bibliographical notes for references  the interpretation of a software capability is left completely to the subsystem  through the protected procedures it contains this scheme allows a variety of protection policies to be implemented although programmers can define their own protected procedures  any of which might be incorrect   the security of the overall system can not be compromised the basic protection system will not allow an unverified  user-defined  protected procedure access to any storage segments  or capabilities  that do not belong to the protection environment in which it resides the most serious consequence of an insecure protected procedure is a protection breakdown of the subsystem for which that procedure has responsibility 610 chapter 14 14.9 the designers of the cap system have noted that the use of software capabilities allowed them to realize considerable economies in formulating and implementing protection policies commensurate with the requirements of abstract resources however  subsystem designers who want to make use of this facility can not simply study a reference manual  as is the case with hydra instead  they must learn the principles and techniques of protection  since the system provides them with no library of procedures to the degree that protection is provided in existing computer systems  it is usually achieved through an operating-system kernel  which acts as a security agent to inspect and validate each attempt to access a protected resource since comprehensive access validation may be a source of considerable overhead  either we must give it hardware support to reduce the cost of each validation or we must allow the system designer to compromise the goals of protection satisfying all these goals is difficult if the flexibility to implement protection policies is restricted by the support mechanisms provided or if protection environments are made larger than necessary to secure greater operational efficiency as operating systems have become more complex  and particularly as they have attempted to provide higher-level user interfaces  the goals of protection have become much more refined the designers of protection systems have drawn heavily on ideas that originated in programming languages and especially on the concepts of abstract data types and objects protection systems are now concerned not only with the identity of a resource to which access is attempted but also with the functional nature of that access in the newest protection systems  concern for the function to be invoked extends beyond a set of system-defined functions  such as standard file-access methods  to include functions that may be user-defined as well policies for resource use may also vary  depending on the application  and they may be subject to change over time for these reasons  protection can no longer be considered a matter of concern only to the designer of an operating system it should also be available as a tool for use by the application designe1 ~ so that resources of an applications subsystem can be guarded against tampering or the influence of an error 14.9.1 compiler-based enforcement at this point  programming languages enter the picture specifying the desired control of access to a shared resource in a system is making a declarative statement about the resource this kind of statement can be integrated into a language by an extension of its typing facility when protection is declared along with data typing  the designer of each subsystem can specify its requirements for protection  as well as its need for use of other resources in a system such a specification should be given directly as a program is composed  and in the language in which the program itself is stated this approach has several significant advantages  14.9 611 protection needs are simply declared  rather than programmed as a sequence of calls on procedures of an operating system protection requirements can be stated independently of the facilities provided by a particular operating system the means for enforcement need not be provided by the designer of a subsystem a declarative notation is natural because access privileges are closely related to the linguistic concept of data type a variety of techniques can be provided by a programming-language implementation to enforce protection  but any of these must depend on some degree of support from an underlying machine and its operating system for example  suppose a language is used to generate code to run on the cambridge cap system on this system  every storage reference made on the underlying hardware occurs indirectly through a capability this restriction prevents any process from accessing a resource outside of its protection environment at any time however  a program may impose arbitrary restrictions on how a resource can be used during execution of a particular code segment we can implement such restrictions most readily by usin.g the software capabilities provided by cap a language implementation might provide standard protected procedures to interpret software capabilities that would realize the protection policies that could be specified in the language this scheme puts policy specification at the disposal of the programmers  while freeing them from implementing its enforcement even if a system does not provide a protection kernel as powerful as those of hydra or cap  mechanisms are still available for implementing protection specifications given in a programming language the principal distinction is that the security of this protection will not be as great as that supported by a protection kernel  because the mechanism must rely on more assumptions about the operational state of the system a compiler can separate references for which it can certify that no protection violation could occur from those for which a violation might be possible  and it can treat them differently the security provided by this form of protection rests on the assumption that the code generated by the compiler will not be modified prior to or during its execution what  then  are the relative merits of enforcement based solely on a kernel  as opposed to enforcement provided largely by a compiler security enforcement by a kernel provides a greater degree of security of the protection system itself than does the generation of protectionchecking code by a compiler in a compiler-supported scheme  security rests on correctness of the translator  on some underlying mechanism of storage management that protects the segments from which compiled code is executed  and  ultimately  on the security of files from which a program is loaded some of these considerations also apply to a softwaresupported protection kernel  but to a lesser degree  since the kernel may reside in fixed physical storage segments and may be loaded only from a designated file with a tagged-capability system  in which all address 612 chapter 14 computation is performed either by hardware or by a fixed microprogram  even greater security is possible hardware-supported protection is also relatively immune to protection violations that might occur as a result of either hardware or system software malfunction flexibility there are limits to the flexibility of a protection kernel in implementing a user-defined policy  although it may supply adequate facilities for the system to provide enforcement of its own policies with a programming language  protection policy can be declared and enforcem.ent provided as needed by an implementation if a language does not provide sufficient flexibility  it can be extended or replaced with less disturbance of a system in service than would be caused by the modification of an operating-system kernel efficiency the greatest efficiency is obtained when enforcement of protection is supported directly by hardware  or microcode   insofar as software support is required  language-based enforcement has the advantage that static access enforcement can be verified off-line at compile time also  since an intelligent compiler can tailor the enforcement mechanism to meet the specified need  the fixed overhead of kernel calls can often be avoided in summary  the specification of protection in a programming language allows the high-level description of policies for the allocation and use of resources a language implementation can provide software for protection enforcement when automatic hardware-supported checking is unavailable in addition  it can interpret protection specifications to generate calls on whatever protection system is provided by the hardware and the operating system one way of making protection available to the application program is through the use of a software capability that could be used as an object of computation inherent in this concept is the idea that certain program components might have the privilege of creating or examining these software capabilities a capability-creating program would be able to execute a primitive operation that would seal a data structure  rendering the latter 's contents inaccessible to any program components that did not hold either the seal or the unseal privilege such components might copy the data structure or pass its address to other program components  but they could not gain access to its contents the reason for introducing such software capabilities is to bring a protection mechanism into the programming language the only problem with the concept as proposed is that the use of the seal and unseal operations takes a procedural approach to specifying protection a nonprocedural or declarative notation seems a preferable way to make protection available to the application programmer what is needed is a safe  dynamic access-control mechanism for distributing capabilities to system resources among user processes to contribute to the overall reliability of a system  the access-control mechanism should be safe to use to be useful in practice  it should also be reasonably efficient this requirement has led to the development of a number of language constructs that allow the programmer to declare various restrictions on the use of a specific managed resource  see the bibliographical notes for appropriate references  these constructs provide mechanisms for three functions  14.9 613 distributing capabilities safely and efficiently among customer processes in particular  mechanisms ensure that a user process will use the managed resource only if it was granted a capability to that resource specifying the type of operations that a particular process may invoke on an allocated resource  for example  a reader of a file should be allowed only to read the file  whereas a writer should be able both to read and to write   it should not be necessary to grant the same set of rights to every user process  and it should be impossible for a process to enlarge its set of access rights  except with the authorization of the access-control mechanism specifying the order in which a particular process may invoke the various operations of a resource  for example  a file must be opened before it can be read   it should be possible to give two processes different restrictions on the order in which they can invoke the operations of the allocated resource the incorporation of protection concepts into programming languages  as a practical tool for system design  is in its infancy protection will likely become a matter of greater concern to the designers of new systems with distributed architectures and increasingly stringent requirements on data security then the importance of suitable language notations in which to express protection requirements will be recognized more widely 14.9.2 protection in java because java was designed to run in a distributed environment  the java virtual machine-or jvm-has many built-in protection mechanisms java programs are composed of each of which is a collection of data fields and functions  called that operate on those fields the jvm loads a class in response to a request to create instances  or objects  of that class one of the most novel and useful features ofj ava is its support for dynamically loading untrusted classes over a network and for executing mutually distrusting classes within the same jvm because of these capabilities of java  protection is a paramount concern classes running in the same jvm may be from different sources and may not be equally trusted as a result  enforcing protection at the granularity of the jvm process is insufficient intuitively  whether a request to open a file should be allowed will generally depend on which class has requested the open the operating system lacks this knowledge thus  such protection decisions are handled within the jvm when the jvm loads a class  it assigns the class to a protection domain that gives the permissions of that class the protection domain to which the class is assigned depends on the url from which the class was loaded and any digital signatures on the class file  digital signatures are covered in section 15.4.1.3  a configurable policy file determines the permissions granted to the domain  and its classes   for example  classes loaded from a trusted server might be placed in a protection domain that allows them to access files in the user 's home directory  whereas classes loaded from an untrusted server might have no file access permissions at all 614 chapter 14 it can be complicated for the jvm to determine what class is responsible for a request to access a protected resource accesses are often performed indirectly  through system libraries or other classes for example  consider a class that is not allowed to open network connections it could call a system library to request the load of the contents of a url the jvm must decide whether or not to open a network connection for this request but which class should be used to determine if the connection should be allowed  the application or the system library the philosophy adopted in java is to require the library class to explicitly permit a network corucection more generally  in order to access a protected resource  some method in the calling sequence that resulted in the request must explicitly assert the privilege to access the resource by doing so  this method takes responsibility for the request ; presumably  it will also perform whatever checks are necessary to ensure the safety of the request of course  not every method is allowed to assert a privilege ; a method can assert a privilege only if its class is in a protection domain that is itself allowed to exercise the privilege this implementation approach is called every thread in the jvm has an associated stack of its ongoing invocations when a caller may not be trusted  a method executes an access request within a dopri vileged block to perform the access to a protected resource directly or indirectly dopri vileged   is a static method in the accesscontroller class that is passed a class with a run   method to invoke when the dopri vileged block is entered  the stack frame for this method is annotated to indicate this fact then  the contents of the block are executed when an access to a protected resource is subsequently requested  either by this method or a method it calls  a call to checkpermissions   is used to invoke stack inspection to determine if the request should be allowed the inspection examines stack frames on the calling thread 's stack  starting from the most recently added frame and working toward the oldest if a stack frame is first found that has the dopri vileged   annotation  then checkpermissions   returns immediately and silently  allowing the access if a stack frame is first found for which access is disallowed based on the protection domain of the method 's class  then checkpermissions   throws an accesscontrolexception if the stack inspection exhausts the stack without finding either type of frame  then whether access is allowed depends on the implementation  for example  some implementations of the jvm may allow access  while other implementations may disallow it   stack inspection is illustrated in figure 14.9 here  the gui   method of a class in the untrusted applet protection domain performs two operations  first a get   and then an open    the former is an invocation of the get   method of a class in the url loader protection domain  which is permitted to open   sessions to sites in the lucent com domain  in particular a proxy server proxy .lucent com for retrieving urls for this reason  the untrusted applet 's get   invocation will succeed  the checkpermissions   call in the networking library encounters the stack frame of the get   method  which performed its open   in a dopri vileged block however  the untrusted applet 's open   invocation will result in an exception  because the checkpermissions   call finds no dopri vileged annotation before encountering the stack frame of the gui   method 14.10 protection domain  socket permission  class  none gui  get  uri  ; open  addr  ; 14.10 .lucent.com  80  connect get  url u   doprivileged  open  'proxy.lucent.com  80 '  ;  request u from proxy figure 14.9 stack inspection 615 any open  addr a   checkpermission  a  connect  ; connect  a  ; of course  for stack inspection to work  a program must be unable to modify the annotations on its own stack frame or to do other manipulations of stack inspection this is one of the most important differences between java and many other languages  including c + +   a java program can not directly access memory ; it can manipulate only an object for which it has a reference references can not be forged  and the manipulations are made only through well-defined interfaces compliance is enforced through a sophisticated collection of load-time and run-time checks as a result  an object can not manipulate its run-time stack  because it camlot get a reference to the stack or other components of the protection system more generally  java 's load-time and run-time checks enforce of java classes type safety ensures that classes can not treat integers as pointers  write past the end of an array  or otherwise access memory in arbitrary ways rather  a program can access an object only via the methods defined on that object by its class this is the f01mdation of java protection  since it enables a class to effectively  and protect its data and methods from other classes loaded in the same jvm for example  a variable can be defined as private so that only the class that contains it can access it or protected so that it can be accessed only by the class that contains it  subclasses of that class  or classes in the same package type safety ensures that these restrictions can be enforced computer systems contain many objects  and they need to be protected from misuse objects may be hardware  such as memory  cpu time  and i/0 devices  or software  such as files  programs  and semaphores   an access right is permission to perform an operation on an object a domain is a set of access rights processes execute in domains and may use any of the access rights in the domain to access and manipulate objects during its lifetime  a process may be either bound to a protection domain or allowed to switch from one domain to another 616 chapter 14 the access matrix is a general model of protection that provides a mechanisnc for protection without imposing a particular protection policy on the system or its users the separation of policy and mechanism is an important design property the access matrix is sparse it is normally implemented either as access lists associated with each object or as capability lists associated with each domain we can include dynamic protection in the access-matrix model by considering domains and the access matrix itself as objects revocation of access rights in a dynamic protection model is typically easier to implement with an access-list scheme than with a capability list real systems are much more limited than the general model and tend to provide protection only for files unix is representative  providing read  write  and execution protection separately for the owner  group  and general public for each file multics uses a ring structure in addition to file access hydra  the cambridge cap system  and mach are capability systems that extend protection to user-defined software objects solaris 10 implements the principle of least privilege via role-based access controt a form of the access matrix language-based protection provides finer-grained arbitration of requests and privileges than the operating system is able to provide for example  a single java jvm can run several threads  each in a different protection class it enforces the resource requests through sophisticated stack inspection and via the type safety of the language 14.1 consider a computer system in which computer games can be played by students only between 10 p.m and 6 a.m  by faculty members between 5 p.m and 8 a.m  and by the computer center staff at all times suggest a scheme for implementing this policy efficiently 14.2 the rc 4000 system  among others  has defined a tree of processes  called a process tree  such that all the descendants of a process can be given resources  objects  and access rights by their ancestors only thus  a descendant can never have the ability to do anything that its ancestors can not do the root of the tree is the operating system  which has the ability to do anything assume the set of access rights is represented by an access matrix  a a  x,y  defines the access rights of process x to object y if xis a descendant of z  what is the relationship between a  x,y  and a  z,y  for an arbitrary object y 14.3 how are the access-matrix facility and the role-based access-control facility similar how do they differ 14.4 discuss the need for rights amplification in hydra how does this practice compare with the cross-ring calls in a ring-protection scheme 617 14.5 explain why a capability-based system such as hydra provides greater flexibility than the ring-protection scheme in enforcing protection policies 14.6 consider the ring-protection scheme in multics if we were to implement the system calls of a typical operating system and store them in a segment associated with ring 0  what should be the values stored in the ring field of the segment descriptor what happens during a system call when a process executing in a higher-numbered ring invokes a procedure in ring 0 14.7 discuss the strengths and weaknesses of implementing an access matrix using capabilities that are associated with domains 14.8 discuss the strengths and weaknesses of implementing an access matrix using access lists that are associated with objects 14.9 the access-control matrix can be used to determine whether a process can switch from  say  domain a to domain b and enjoy the access privileges of domain b is this approach equivalent to including the access privileges of domain b in those of domain a 14.10 how can systems that implement the principle of least privilege still have protection failures that lead to security violations 14.11 how does the principle of least privilege aid in the creation of protection systems 14.12 what protection problems may arise if a shared stack is used for parameter passing 14.13 if all the access rights to an object are deleted  the object can no longer be accessed at this point the object should also be deleted  and the space it occupies should be returned to the system suggest an efficient implementation of this scheme 14.14 discuss which of the following systems allow module designers to enforce the need-to-know principle a the multics ring-protection scheme b hydra 's capabilities c jvm 's stack-inspection scheme 618 chapter 14 14.15 consider a computing environment where a unique number is associated with each process and each object in the system suppose that we allow a process with number n to access an object with number m only if n m what type of protection structure do we have 14.16 what is the need-to-know principle why is it important for a protection system to adhere to this principle 14.17 what hardware features does a computer system need for efficient capability manipulation can these features be used for memory protection 14.18 describe how the java protection model would be compromised if a java program were allowed to directly alter the annotations of its stack frame 14.19 a burroughs b7000/b6000 mcp file can be tagged as sensitive data when such a file is deleted  its storage area is overwritten by some random bits for what purpose would such a scheme be useful the access-matrix model of protection between domains and objects was developed by lampson  1969  and lampson  1971   popek  1974  and saltzer and schroeder  1975  provided excellent surveys on the subject of protection harrison et al  1976  used a formal version of this model to enable them to prove properties of a protection system mathematically the concept of a capability evolved from iliffe 's and jodeit 's codewords  which were implemented in the rice university computer  iliffe and jodeit  1962    the term capability was introduced by dennis and horn  1966   the hydra system was described by wulf et al  1981   the cap system was described by needham and walker  1977   organick  1972  discussed the multics ring-protection system revocation was discussed by redell and fabry  1974   cohen and jefferson  1975   and ekanadham and bernstein  1979   the principle of separation of policy and mechanism was advocated by the designer of hydra  levin et al  1975    the confinement problem was first discussed by lampson  1973  and was further examined by lipner  1975   the use of higher-level languages for specifying access control was suggested first by morris  1973   who proposed the use of the seal and unseal operations discussed in section 14.9 kieburtz and silberschatz  1978   kieburtz and silberschatz  1983   and mcgraw and andrews  1979  proposed various language constructs for dealing with general dynamic-resource-management schemes jones and liskov  1978  considered how a static access-control scheme can be incorporated in a programming language that supports abstract data types the use of minimal operating-system support to enforce protection was advocated by the exokernel project  ganger et al  2002   kaashoek et al  1997    15.1 protection  as we discussed in chapter 14  is strictly an internal problem  how do we provide controlled access to programs and data stored in a computer system on the other hand  requires not only an adequate protection system but also consideration of the external environment within which the system operates a protection system is ineffective if user authentication is compromised or a program is run by an unauthorized user computer resources must be guarded against unauthorized access  malicious destruction or alteration  and accidental introduction of inconsistency these resources include information stored in the system  both data and code   as well as the cpu  memory  disks  tapes  and networking that are the computer in this chapter  we start by examining ways in which resources may be accidentally or purposely misused we then explore a key security enabler -cryptography finally  we look at mechanisms to guard against or detect attacks to discuss security threats and attacks to explain the fundamentals of encryption  authentication  and hashing to examine the uses of cryptography in computing to describe various countermeasures to security attacks in many applications  ensuring the security of the computer system is worth considerable effort large commercial systems containing payroll or other financial data are inviting targets to thieves systems that contain data pertaining to corporate operations may be of interest to unscrupulous competitors furthermore  loss of such data  whether by accident or fraud  can seriously impair the ability of the corporation to function in chapter 14  we discussed mechanisms that the operating system can provide  with appropriate aid from the hardware  that allow users to protect 621 622 chapter 15 their resources  including programs and data these mechanisms work well only as long as the users conform to the intended use of and access to these resources we say that a system is if its resources are used and accessed as intended under all circumstances unfortunately total security can not be achieved nonetheless  we must have mechanisms to make security breaches a rare occurrence  rather than the norm security violations  or misuse  of the system can be categorized as intentional  malicious  or accidental it is easier to protect against accidental misuse than against malicious misuse for the most part protection mechanisms are the core of protection from accidents the following list includes several forms of accidental and malicious security violations we should note that in our discussion of security  we use the terms intruder and cracker for those attempting to breach security in addition  a is the potential for a security violation  such as the discovery of a vulnerability  whereas an is the attempt to break security breach of confidentiality this type of violation involves 1mauthorized reading of data  or theft of information   typically  a breach of confidentiality is the goal of an intruder capturing secret data from a system or a data stream  such as credit-card information or identity information for identity theft  can result directly in money for the intruder breach of integrity this violation involves unauthorized modification of data such attacks can  for example  result in passing of liability to an innocent party or modification of the source code of an important commercial application breach of availability this violation involves unauthorized destruction of data some crackers would rather wreak havoc and gain status or bragging rights than gain financially web-site defacement is a common example of this type of security breach theft of service this violation involves unauthorized use of resources for example  an intruder  or intrusion program  may install a daemon on a system that acts as a file server denial of service this violation involves preventing legitimate use of the system or attacks are sometimes accidental the original internet worm turned into a dos attack when a bug failed to delay its rapid spread we discuss dos attacks further in section 15.3.3 attackers use several standard methods in their attempts to breach security the most common is in which one participant in a communication pretends to be someone  another host or another person   by masquerading  attackers breach the correctness of identification ; they can then gain access that they would not normally be allowed or escalate their privileges-obtain privileges to which they would not normally be entitled another common attack is to replay a captured exchange of data a consists of the malicious or fraudulent repeat of a valid data transmission sometimes the replay comprises the entire attackfor example  in a repeat of a to transfer money but frequently it is done along with again to escalate privileges consider 15.1 623 normal attacker masquerading attacker man-in-the-middle attacker figure 15.1 standard security attacks the damage that could be done if a request for authentication had a legitimate user 's information with an unauthorized user 's yet another kind of attack is the in which an attacker sits in the data flow of a communication  masquerading as the sender to the receiver  and vice versa in a network communication  a man-in-the-middle attack may be preceded by a in which an active communication session is intercepted several attack methods are depicted in figure 15.1 as we have already suggested  absolute protection of the system from malicious abuse is not possible  but the cost to the perpetrator can be made sufficiently high to deter most intruders in some cases  such as a denial-ofservice attack  it is preferable to prevent the attack but sufficient to detect the attack so that cmmtermeasures can be taken to protect a system  we must take security measures at four levels  physical the site or sites containing the computer systems must be physically secured against armed or surreptitious entry by intruders both the machine rooms and the terminals or workstations that have access to the machines must be secured 624 chapter 15 must be done carefully to assure that only access to the system even authorized users/ to let others use their access  in exchange they may also be tricked into allowing one type of social-engineering attack here  a legitimate-looking e-mail or web page misleads a user into entering confidential information another teclucique is human authorization appropriate users have however  may be for a bribe  for access via is a general term for attempting to gather information in order to gain unauthorized access to the computer  by looking through trash  finding phone books  or finding notes containing passwords  for example   these security problems are management and personnel issues  not problems pertaining to operating systems operating system the system must protect itself from accidental or purposeful security breaches a runaway process could constitute an accidental denial-of-service attack a query to a service could reveal passwords a stack overflow could the launching of an unauthorized process list of possible breaches is almost endless network much computer data in modern systems travels over private leased lines  shared lines like the internet  wireless connections  or dial-up lines intercepting these data could be just as harmful as breaking into a computer ; and interruption communications could constitute a remote denial-of-service attack  diminishing users ' use of and trust in the system at the first two levels must be inaintained if operating-system is to be ensured a weakness at a high security  physical or allows circumvention of strict low-level  operating-system  security measures the old adage that a chal.'l is as weak as its weakest link is true of system security all these aspects must be addressed for to be maintained to allow the more is as intruders countermeasures are created and deployed this causes intruders to become more in their attacks for incidents include the use of to section tools needed to block the in the remainder of this 15 15.2 625 ways  ranging from passwords for authentication through guarding against viruses to detecting intrusions we start with an exploration of security threats processes  along with the kernel  are the only means of accomplishing work on a computer therefore  writing a program that creates a breach of security  or causing a normal process to change its behavior and create a breach  is a common goal of crackers in fact even most nonprogram security events have as their goal causing a program threat for example  while it is useful to log in to a without authorization  it is quite a lot more useful to leave behind a daemon that provides information or allows easy access even if the original exploit is blocked in this section  we describe common methods which programs cause security breaches note that there is considerable variation in the naming conventions of security holes and that we use the most common or descriptive terms 15.2.1 horse systems have mechanisms for allowing programs written by users to be executed by other users if these programs are executed in a domain that provides the access rights of the executing user  the other users may misuse these rights a text-editor program  for example  may include code to search the file to be edited for certairl keywords if any are found  the entire file may be to a special area accessible to the creator of the text editor a code segment that misuses its environment is called a long search paths  as are common on unix systems  exacerbate the trojanhorse the search path lists the set of directories to search when an program name is given the is searched for a file of that name  and the file is executed all the directories in such a search path must be secure  or a horse could be slipped into the user 's and executed consider the use of the  character in a search path the  to include the current directory in the search if a user has  in her search has set her current to a friend 's directory  and enters the name of a normal system commanct be executed from the friend 's instead the program would run the user 's to do anything that the user is allowed to the user 's instance horse is a that emulates a what 626 chapter 15 such as the control-alt-delete conlbination used by all modern windows operating systems another variation on the trojan horse is spyware sometimes accompanies a program that the user has chosen to install most frequently  it comes along with freeware or shareware programs  but sometimes it is included with commercial software the goal of spyware is to download ads to display on the user 's system  create when certain sites are visited  or capture information from the user 's system and return it to a central site this latter is an example of a general category of attacks known as in which surreptitious communication occurs for example  the installation of an innocuous-seeming program on a windows system could result in the loading of a spyware daemon the spyware could contact a central site  be given a message and a list of recipient addresses  and deliver the spam message to those users from the windows machine this process continues until the user discovers the spyware frequently  the spyware is not discovered in 2004  it was estimated that 80 percent of spam was being delivered by this method this theft of service is not even considered a crime in most countries ! spyware is a micro example of a macro problem  violation of the principle of least privilege under most circumstances  a user of an operating system does not need to install network daemons such daemons are installed via two mistakes first  a user may run with more privileges than necessary  for example  as the administrator   allowing programs that she runs to have more access to the system than is necessary this is a case of human error-a common security weakness second  an operating system may allow by default more privileges than a normal user needs this is a case of poor operating-system design decisions an operating system  and  indeed  software in general  should allow fine-grained control of access and security  but it must also be easy to manage and understand inconvenient or inadequate security measures are bound to be circumvented  causing an overall weakening of the security they were designed to implement 15.2.2 trap door the designer of a program or system might leave a hole in the software that only he is capable of using this type of security breach  or was shown in the movie war games for instance  the code inight check a specific user id or password  and it might circumvent normal security procedures programmers have been arrested for embezzling from banks by including rounding errors in their code and having the occasional half-cent credited to their accounts this account credititrg can add up to a large amount of money  considering the number of transactions that a large bank executes a clever trap door could be included in a compiler the compiler could generate standard object code as well as a trap door  regardless of the source code being compiled this activity is particularly nefarious  since a search of the source code of the program will not reveal any problems only the source code of the compiler would contain the information trap doors pose a difficult problem because  to detect them  we have to analyze all the source code for all components of a system given that software systems may consist of millions of lines of code  this analysis is not done frequently  and frequently it is not done at all ! 15.2 627 15.2.3 logic bomb consider a program that initiates a security incident only under certain circltmstances it would be hard to detect because under normal operations  there would be no security hole however  when a predefined set of parameters were met  the security hole would be created this scenario is known as a a programmer  for example  might write code to detect whether was still employed ; if that check failed  a daemon could be spawned to allow remote access  or code could be launched to cause damage to the site 15.2.4 stack and buffer overflow the stack or buffer-overflow attack is the most common way for an attacker outside the system  on a network or dial-up connection  to gain unauthorized access to the target system an authorized user of the system may also use this exploit for privilege escalation essentially  the attack exploits a bug in a program the bug can be a simple case of poor programming  in which the programmer neglected to code bounds checking on an input field in this case  the attacker sends more data than the program was expecting by using trial and error  or by examining the source code of the attacked program if it is available  the attacker determines the vulnerability and writes a program to do the following  overflow an input field  command-line argument  or input buffer-for example  on a network daemon-wl.til it writes into the stack overwrite the current return address on the stack with the address of the exploit code loaded in step 3 write a simple set of code for the next space in the stack that includes the commands that the attacker wishes to execute-for instance  spawn a shell the result of this attack program 's execution will be a root shell or other privileged command execution for instance  if a web-page form expects a user name to be entered into a field  the attacker could send the user name  plus extra characters to overflow the buffer and reach the stack  plus a new return address to load onto the stack  plus the code the attacker wants to run when the buffer-reading subroutine returns from execution  the return address is the exploit code  and the code is run let 's look at a buffer-overflow exploit in more detail consider the simple c program in fig1-1re 15.2 this program creates a character array size buffer_size and copies the contents of the parameter provided on the command line-argv  1   as long as the size of this parameter is less than buffer_size  we need one byte to store the null terminator   this program works properly but consider what happens if the parameter provided on the command line is longer than buffer_size in this scenario  the strcpy   function will begin copying from argv  1  until it encounters a null terminator  \ 0  or until the program crashes thus  this program suffers from a potential problem in which copied data overflow the buffer array 628 chapter 15 # include stdio.h # define buffer_size 256 int main  int argc  char argv      char buffer  buffer_size  ; if  argc 2  return -1 ; else  strcpy  buffer,argv  1   ; return 0 ;  figure 15.2 c program with buffer-overflow condition note that a careful programmer could have performed bounds checking on the size of argv  1  by using the strncpy   function rather than strcpy    replacing the line strcpy  buffer  argv  1   ; with strncpy  buffer  argv  1   sizeof  buffer  -1  ; .unfortunately  good bounds checking is the exception rather than the norm furthermore  lack of bounds checking is not the only possible cause of the behavior of the program in figure 15.2 the program could instead have been carefully designed to compromise the integrity of the system we now consider the possible security vulnerabilities of a buffer overflow when a function is invoked in a typical computer architecture  the variables defined locally to the function  sometimes known as automatic variables   the parameters passed to the function  and the address to which control returns once the function exits are stored in a stack frame the layout for a typical stack frame is shown in figure 15.3 examining the stack frame from top to bottom  we first see the parameters passed to the function  followed by any automatic variables declared in the function we next see the frame pointer  which is the address of the beginning of the stack frame finally  we have the return bottom ~ frame pointer grows top figure 15.3 the layout for a typical stack frame 15.2 629 address  which specifies where to return control once the function exits the frame pointer must be saved on the stack  as the value of the stack pointer can vary during the function call ; the saved frame pointer allows relative access to parameters and automatic variables given this standard memory layout  a cracker could execute a bufferoverflow attack i  ter goal is to replace the return address in the stack frame so that it now points to the code segment containing the attacking program the programmer first writes a short code segment such as the following  # include stdio.h int main  int argc  char argv     execvp  ' ' \ bin \ sh' '  ' ' \ bin \ sh' '  null  ; return 0 ; using the execvp   system call  this code segment creates a shell process if the program being attacked runs with system-wide permissions  this newly created shell will gain complete access to the system of course  the code segment could do anything allowed by the privileges of the attacked process this code segment is then compiled so that the assembly language instructions can be modified the primary modification is to remove unnecessary features in the code  thereby reducing the code size so that it can fit into a stack frame this assembled code fragment is now a binary sequence that will be at the heart of the attack refer again to the program shown in figure 15.2 let 's assume that when the main   function is called in that program  the stack frame appears as shown in figure 15.4  a   using a debugger  the programmer then finds the copied  a   b  figure 15.4 hypothetical stack frame for figure 15.2   a  before and  b  after 630 chapter 15 address of buffer  0  in the stack that address is the location of the code the attacker wants executed the binary sequence is appended with the necessary amount of no-op instructions  for no-operation  to fill the stack frame up to the location of the return address ; and the location of buffer  0   the new return address  is added the attack is complete when the attacker gives this constructed binary sequence as input to the process the process then copies the binary sequence from argv  1  to position buffer  0  in the stack frame now  when control returns from main    instead of returning to the location specified by the old value of the return address  we return to the modified shell code  which runs with the access rights of the attacked process ! figure 15.4  b  contains the modified shell code there are many ways to exploit potential buffer-overflow problems in this example  we considered the possibility that the program being attackedthe code shown in figure 15.2-ran with system-wide permissions however  the code segment that runs once the value of the return address has been modified might perform any type of malicious act  such as deleting files  opening network ports for further exploitation  and so on this example buffer-overflow attack reveals that considerable knowledge and programming skill are needed to recognize exploitable code and then to exploit it unfortunately  it does not take great programmers to launch security attacks rather  one cracker can determine the bug and then write an exploit anyone with rudimentary computer skills and access to the exploita so-called then try to launch the attack at target systems the buffer-overflow attack is especially pernicious because it can be run between systems and can travel over allowed communication channels such attacks can occur within protocols that are expected to be used to communicate with the target machine  and they can therefore be hard to detect and prevent they can even bypass the security added by firewalls  section 15.7   one solution to this problem is for the cpu to have a feature that disallows execution of code in a stack section of memory recent versions of sun 's sparc chip include this setting  and recent versions of solaris enable it the return address of the overflowed routine can still be modified ; but when the return address is within the stack and the code there attempts to execute  an exception is generated  and the program is halted with an error recent versions of amd and intel x86 chips include the nx feature to prevent this type of attack the use of the feature is supported in several x86 operating systems  including linux and windows xp sp2 the hardware implementation involves the use of a new bit in the page tables of the cpus this bit marks the associated page as nonexecutable  so that instructions can not be read from it and executed as this feature becomes prevalent  buffer-overflow attacks should greatly diminish 15.2.5 viruses another form of program threat is a a virus is a fragment of code embedded in a legitimate program viruses are self-replicating and are designed to infect other programs they can wreak havoc in a system by modifying or destroying files and causing system crashes and program malfunctions as with most penetration attacks  viruses are very specific to architectures  operating systems  and applications viruses are a particular problem for users of 15.2 631 pcs unix and other multiuser operating systems generally are not susceptible to viruses because the executable programs are protected from writing by the operating system even if a virus does infect such a progran  1  its powers usually are limited because other aspects of the system are protected viruses are usually borne via e-mail  with spam the most comrnon vector they can also spread when users download viral programs internet file-sharing services or exchange infected disks another common form of virus transmission uses microsoft office files  such as microsoft word documents these documents can contain macros visual basic programs  that programs in the office suite powerpoint  and excel  will execute automatically because these programs run under the user 's own account  the macros can run largely unconstrained  for example  deleting user files at will   commonly  the virus will also e-mail itself to others in the user 's contact list here is a code sample that shows the simplicity of writing a visual basic macro that a virus could use to format the hard drive of a windows computer as soon as the file containing the macro was opened  sub autoopen   dim ofs set ofs = createobject  ' 'scripting.filesystemobject' '  vs = shell  ' 'c  command.com /k format c  ' ',vbhide  end sub how do viruses work once a virus reaches a target machine  a program known as a inserts the virus into the system the virus dropper is usually a trojan horse  executed for other reasons but installing the virus as its core activity once installed  the virus may do any one of a number of things there are literally thousands of viruses  but they fall into several main categories note that many viruses belong to more than one category file a standard file virus infects a system by appending itself to a file it changes the start of the program so that execution jumps to its code after it executes  it returns control to the program so that its execution is not noticed file viruses are sometimes known as parasitic viruses  as they leave no full files behind and leave the host program still functional boot a boot virus infects the boot sector of the system  executing every time the system is booted and before the operating system is loaded it watches for other boatable media  that is  floppy disks  and infects them these viruses are also known as memory viruses  because they do not appear in the file system figure 15.5 shows how a boot virus works macro most viruses are written in a low-levellanguage  such as assembly or c macro viruses are written in a high-level language  such as visual basic these viruses are triggered when a program capable of executing the macro is run for example  a macro virus could be contained in a spreadsheet file source code a source code virus looks for source code and modifies it to include the virus and to help spread the virus 632 chapter 15 figure i 5.5 a boot-sector computer virus polymorphic a polymorphic virus changes each time it is installed to avoid detection by antivirus software the changes do not affect the virus 's functionality but rather change the virus 's signature a a pattern that cili'i be used to identify a virus  typically a series make up the virus code encrypted an encrypted virus includes decryption code along with the encrypted virus  again to avoid detection the virus first decrypts and then executes stealth this tricky virus attempts to avoid detection by modifying parts of the system that could be used to detect it for example  it could modify the read system call so that if the file it has modified is read  the original form of the code is returned rather than the infected code tunneling this virus attempts to bypass detection by an anti virus scanner by installing itself in the interrupt-handler chain similar viruses install themselves in device drivers 15.3 15.3 633 multipartite a virus of this type is able to infect nmltiple parts of a system  including boot sectors  memory  and files this makes it difficult to detect and contain armored an armored virus is coded to ncake it hard for antivirus researchers to unravel and understand it can also be compressed to avoid detection and disinfection in addition  virus droppers and other full files that are part of a virus infestation are frequently hidden via file attributes or unviewable file names this vast variety of viruses is likely to continue to grow in fact  in 2004 a new and widespread virus was detected it exploited three separate bugs for its operation this virus started by infecting hundreds of windows servers  including many trusted sites  running microsoft internet information server  iis   any vulnerable microsoft explorer web browser visiting those sites received a browser virus with any download the browser virus installed several back-door programs  including a which records all things entered on the keyboard  including and credit-card numbers   it also installed a daemon to allow unlimited remote access by an intruder and another that allowed an intruder to route spam through the infected desktop computer generally  viruses are the most disruptive security attacks ; and because they are effective  they will continue to be written and to spread the active debates within the computing community is whether a jth'-yhcn ; _ in which many systems run the same hardware  operating system  and/ or application software  is increasing the threat of and damage caused by security intrusions this monoculture supposedly consists of microsoft products  and part of the debate concerns whether such a monoculture even exists today program threats typically use a breakdown in the protection mechanisms of a system to attack programs in contrast  system and network threats involve the abuse of services and network comcections system and network threats create a situation in which operating-system resources and user files are inisused sometimes a system and network attack is used to launch a program attack  and vice versa the more an operating system is-the more services it has enabled and the more functions it allows-the more likely it is that a is available to exploit increasingly  operating systems strive to be for example  solaris 10 moved from a model in which many services  ftp  telnet  and others  were enabled by default when the system was installed to a model in which almost all services are disabled at installation time and must specifically be enabled system administrators such changes reduce the system 's set of ways in which an attacker can to break into the system in the remainder of this section  we discuss some examples of system and network threats  including worms  port scamcing  and denial-of-service attacks it is important to note that masquerading and replay attacks are also 634 chapter 15 commonly launched over netvvorks between systems in fact  these attacks are more effective and harder to counter when multiple systems are involved for example  within a computer  the operating system usually can determine the sender and receiver of a message even if the sender changes to the id of someone else  there may be a record of that id change when multiple systems are involved  especially systems controlled by attackers  then such tracing is much more difficult in general  we can say that sharing secrets  to prove identity and as keys to encryption  is required for authentication and encryption  and sharing secrets is easier in environments  such as a single operating system  in which secure sharing methods exist these methods include shared memory and interprocess comnmnications creating secure communication and authentication is discussed in sections 15.4 and 15.5 15.3.1 worms a is a process that uses the mechanism to ravage system performance the worm spawns copies of itself  using up system resources and perhaps locking out all other processes on computer networks  worms are particularly potent  since they may reproduce themselves among systems and thus shut down an entire network such an event occurred in 1988 to unix systems on the internet  causing the loss of system and system-administrator time worth millions of dollars at the close of the workday on november 2  1988  robert tappan morris  jr  a first-year cornell graduate student  unleashed a worm program on one or more hosts corm.ected to the internet targeting sun microsystems ' sun 3 workstations and vax computers running variants of version 4 bsd unix  the worm quickly spread over great distances ; within a few hours of its release  it had consumed system resources to the point of bringing down the infected machines although robert morris designed the self-replicating program for rapid reproduction and distribution  some of the features of the unix networking environment provided the means to propagate the worm throughout the system it is likely that morris chose for in.itial infection an internet host left open for and accessible to outside users from there  the worm program exploited flaws in the unix operating system 's security routines and took advantage of unix utilities that simplify resource sharing in local-area networks to gain unauthorized access to thousands of other connected sites morris 's methods of attack are outlined next the worm was made up of two programs  a  also called a or program and the main program ll.c  the grappling hook consisted of 99 lines of c code compiled and run on each machine it accessed once established on the computer system under attack  the grappling hook connected to the machine where it originated and uploaded a copy of the main worm onto the hooked system  figure 15.6   the main program proceeded to search for other machines to which the newly infected system could connect easily in these actions  morris exploited the unix networking utility rsh for easy remote task execution by setting up special files that list host-login name pairs  users can omit entering a password each time they access a remote account on the paired list the worm searched these special files for site names 15.3 635 rsh attack finger attack sendmail attack worm sent target system infected system figure 15.6 the morris internet worm that would allow remote execution without a password where remote shells were established  the worm program was uploaded and began executing anew the attack via remote access was one of three infection methods built into the worm the other two methods involved operating-system bugs in the unix finger and sendmail programs the finger utility functions as an electronic telephone directory ; the command finger user-name hostname returns a person 's real and login names along with other information that the user may have provided  such as office and home address and telephone number  research plan  or clever quotation finger runs as a background process  or daemon  at each bsd site and responds to queries throughout the internet the worm executed a buffer-overflow attack on finger the program queried finger with a 536-byte string crafted to exceed the buffer allocated for input and to overwrite the stack frame instead of returning to the main routine where it resided before morris 's calt the finger daemon was routed to a procedure within the invading 536-byte string now residing on the stack the new procedure executed /bin/ sh  which  if successful  gave the worm a remote shell on the machine under attack the bug exploited in sendmail also involved using a daemon process for malicious entry sendmail sends  receives  and routes electronic mail debugging code in the utility permits testers to verify and display the state of the ncail system the debugging option was useful to system administrators and was often left on morris included in his attack arsenal a call to debug that -instead of specifying a user address  as would be normal in testing-issued a set of cornmands that mailed and executed a copy of the grappling-hook program once in place  the main worm systematically attempted to discover user passwords it began by trying simple cases of no password or passwords constructed of account-user-name combinations  then used comparisons with an internal dictionary of 432 favorite password choices  and then went to the 636 chapter 15 final stage of trying each word in the standard unix on-line dictionary as a possible password this elaborate and efficient three-stage password-cracking algorithm enabled the worm to gain access to other user accounts on the infected system the wontt then searched for rsh data files in these newly broken accounts and used them as described previously to gain access to user accounts on remote systems with each new access  the worm program searched for already active copies of itself if it found one  the new copy exited  except in every seventh instance had the worm exited on all duplicate sightings  it might have remained undetected allowing every seventh duplicate to proceed  possibly to confound efforts to stop its spread baiting with fake worms  created a wholesale infestation of sun and vax systems on the internet the very features of the unix network environment that assisted il l the worm 's propagation also helped to stop its advance ease of electronic communication  mechanisms to copy source and binary files to remote machines  and access to both source code and human expertise allowed cooperative efforts to develop solutions quickly by the evening of the next day  november 3  methods of halting the invading program were circulated to system administrators via the internet within days  specific software patches for the exploited security flaws were available why did morris unleash the worm the action has been characterized as both a harmless prank gone awry and a serious criminal offense based on the complexity of the attack  it is unlikely that the worm 's release or the scope of its spread was unintentional the worm program took elaborate steps to cover its tracks and to repel efforts to stop its spread yet the program contained no code aimed at damaging or destroying the systems on which it ran the author clearly had the expertise to include such commands ; in fact  data structures were present in the bootstrap code that could have been used to transfer trojan-horse or virus programs the behavior of the program may lead to interesting observations  but it does not provide a sound basis for inferring motive what is not open to speculation  however  is the legal outcome  a federal court convicted morris and handed down a sentence of three years ' probation  400 hours of community service ; and a $ 10,000 fine morris 's legal costs probably exceeded $ 100,000 security experts continue to evaluate methods to decrease or eliminate worms a more recent event ; though  shows that worms are still a fact of life on the internet it also shows that as the internet grows  the damage that even harmless worms can do also grows and can be significant this example occurred during august 2003 the fifth version of the sobig worm  more properly known as w32.sobig.f @ mm  was released by persons at this time unknown it was the fastest-spreading worm released to date  at its peak mfecting hundreds of thousands of computers and one in seventeen e-mail messages on the internet it clogged e-mail inboxes  slowed networks  and took a huge number of hours to clean up sobig.f was launched by being uploaded to a pornography newsgroup via an account created with a stolen credit card it was disguised as a photo the virus targeted microsoft windows systems and used its own smtp engine to e-mail itself to all the addresses found on an infected system it used a variety of subject lines to help avoid detection  including thank you ! your details,' ' and re  approved it also used a random address on the host as the from  15.3 637 address  making it difficult to determine from the message which machine was the infected source sobig.f included an attachment for the target e-mail reader to click on  again with a variety of names if this payload was executed  it stored a program called winppr32.exe in the default windows directory  along with a text file it also modified the windows registry the code included in the attachment was also programmed to periodically attempt to connect to one of twenty servers and download and execute a program from them fortunately  the servers were disabled before the code could be downloaded the content of the program from these servers has not yet been determined if the code was malevolent  untold damage to a vast number of machines could have resulted 15.3.2 port scanning port scanning is not an attack but rather a means for a cracker to detect a system 's vulnerabilities to attack port scanning typically is automated  involving a tool that attempts to create a tcp lip connection to a specific port or a range of ports for example  suppose there is a known vulnerability  or bug  in sendmail a cracker could launch a port scanner to try to connect  say  to port 25 of a particular system or to a range of systems if the connection was successful  the cracker  or tool  could attempt to communicate with the answering service to determine if the service was indeed sendmail and  if so  if it was the version with the bug now imagine a tool in which each bug of every service of every operath g system was encoded the tool could attempt to connect to every port of one or nwre systems for every service that answered  it could try to use each known bug frequently  the bugs are buffer overflows  allowing the creation of a privileged command shell on the system from there  of course  the cracker could install trojan horses  back-door programs  and so on there is no such tool  but there are tools that perform subsets of that functionality for example  nma it has a database of bugs and their exploits it can scan a range of systems  determine the services running on those systems  and attempt to attack all appropriate bugs it generates reports about the results it does not perform the final step of exploiting the found bugs  but a knowledgeable cracker or a script kiddie could because port scans are detectable  section 15.6.3   they frequently are launched from such systems are previously compromised  independent systems that are serving their owners while being used for nefarious purposes  including denial-of-service attacks and spam relay zombies make crackers particularly difficult to prosecute because determining the source of the attack and the person that launched it is challenging this is one of many reasons for securing inconsequential systems  not just systems containing valuable information or services 638 chapter 15 15.3.3 denial of service as mentioned earlier  denial-of-service attacks are aimed not at gaming information or stealing resources but rather at disrupting legitimate use of a system or facility most such attacks involve systems that the attacker has not penetrated indeed  launching an attack that prevents legitimate use is frequently easier than breaking into a machine or facility denial-of-service attacks are generally network based they fall into two categories attacks in the first category use so many facility resources that  in essence  no useful work can be done for example  a web-site click could download a java applet that proceeds to use all available cpu time or to pop up windows infinitely the second category involves disrupting the network of the facility there have been several successful denial-of-service attacks of this kind against major web sites these attacks result from abuse of some of the fundamental functionality of tcp lip for instance  if the attacker sends the part of the protocol that says i want to start a tcp connection  but never follows with the standard the connection is now complete  the result can be partially started tcp sessions if enough of these sessions are launched  they can eat up all the network resources of the system  disabling any further legitimate tcp connections such attacks  which can last hours or days  have caused partial or full failure of attempts to use the target facility the attacks are usually stopped at the network level until the operating systems can be updated to reduce their vulnerability generally  it is impossible to prevent denial-of-service attacks the attacks use the same mechanisms as normal even more difficult to prevent and resolve are these attacks are launched from multiple sites at once  toward a common target  typically by zombies ddos attacks have become more comncon and are sometimes associated with blackmail attempts a site comes under attack  and the attackers offer to halt the attack in exchange for money sometimes a site does not even know it is under attack it can be difficult to determine whether a system slowdown is an attack or just a surge in system use consider that a successful advertising campaign that greatly increases traffic to a site could be considered a ddos there are other interesting aspects of dos attacks for example  if an authentication algorithm locks an account for a period of time after several incorrect attempts to access the account  then an attacker could cause all authentication to be blocked by purposely making incorrect attempts to access all accounts similarly  a firewall that automatically blocks certain kinds of traffic could be induced to block that traffic when it should not these examples suggest that programmers and systems managers need to fully understand the algorithms and technologies they are deploying finally  computer science classes are notorious sources of accidental system dos attacks consider the first programming exercises in which students learn to create subprocesses or threads a common bug involves spawning subprocesses infinitely the system 's free memory and cpu resources do n't stand a chance there are many defenses against computer attacks  running the gamut from methodology to technology the broadest tool available to system designers 15.4 639 and users is cryptography in this section  we discuss the details of cryptography and its use in computer security in an isolated computer  the operating system can reliably determine the sender and recipient of ali interprocess communication  since it controls all communication channels in the computer in a network of computers  the situation is quite different a networked computer receives bits from the wire with no immediate and reliable way of determining what machine or application sent those bits similarly  the computer sends bits onto the network with no of knowing who might eventually receive them commonly  network addresses are used to infer the potential senders and receivers of network messages network packets arrive with a source address  such as an ip address and when a computer sends a message  it names the intended receiver by specifying a destination address however  for applications where security matters  we are asking for trouble if we assume that the source or destination address of a packet reliably determines who sent or received that packet a rogue computer can send a message with a falsified source address  and numerous computers other than the  one specified by the destination address can  and typically do  receive a packet for example  all of the routers on the way to the destination will receive the packet  too how  then  is an operating system to decide whether to grant a request when it can not trust the named source of the request and how is it supposed to provide protection for a request or data when it can not determine who will receive the response or message contents it sends over the network it is generally considered infeasible to build a network of any scale in which the source and destination addresses of packets can be trusted in this sense therefore  the only alternative is somehow to eliminate the need to trust the network this is the job of cryptography abstractly  ~ ' ' ~ .,-ro.rnron ~ used to constrain the potential senders and/ or receivers of a message cryptography is based on secrets called that are selectively distributed to computers in a network and used to process messages cryptography enables a recipient of a message to verify that the message was created by some computer possessing a certain key-the key is the source of the message similarly  a sender can encode its message so that only a computer with a certain key can decode the message  so that the key becomes the destination unlike network addresses  however  keys are designed so that it is not computationally feasible to derive them from the messages they were used to generate or from any other public information thus  they provide a much more trustworthy means of constraining senders and receivers of messages note that cryptography is a field of study unto itself  with large and small complexities and subtleties here  we explore the most important aspects of the parts of cryptography that pertain to operating systems 15.4.1 encryption because it solves a wide variety of communication security problems  is used frequently in many aspects of modern computing encryption a means for constraining the possible receivers of a message an encryption algorithm enables the sender of a message to ensure that only a computer possessing a certain key can read the message encryption of messages is an ancient practice  of course  and there have been many encryption algorithms  640 chapter 15 dating back to ancient times in this section  we describe important modern encryption principles and algorithms figure 15.7 shows an example of two users communicating securely over an insecure channel we refer to this figure throughout the section note that the key exchange can take place directly between the two parties or via a trusted third party  that is  a certificate authority   as discussed in section 15.4.1.4 an encryption algorithm consists of the following components  a set k of keys a set m of messages a set c of ciphertexts a function e  k  +  m  + c   thatis  for each k e k  e  k  is a function for generating ciphertexts from messages both e and e  lc  for any k should be efficiently computable functions a function d  i  +  c  + m   thatis  for eachlc e i  d  k  is a function for generating messages from ciphertexts both d and d  lc  for any k should be efficiently computable functions an encryption algorithm must provide this essential property  given a ciphertext c e c a computer can compute m such that e  lc   m  = c only if it possesses write ----1 rnessage rn 1 i ii 12 0 ~  z ~ -x ~  read 1 message ml figure i5.7 a secure communication over an insecure medium 15.4 641 d  lc   thus  a computer holding d  lc  can decrypt ciphertexts to the plaintexts used to produce them  but a computer not holding d  lc  can not decrypt ciphertexts since ciphertexts are generally exposed  for example  sent on a network   it is important that it be infeasible to derive d  lc  from the ciphertexts there are two main types of encryption algorithms  symmetric and asymmetric we discuss both types in the following sections 15.4.1.1 symmetric encryption in a the same key is used to encrypt and to decrypt that is  e can be from d  lc   and vice versa therefore  the secrecy of e  lc  must be protected to the same extent as that of d  lc   for the past several decades  the most commonly used symmetric encryption algorithm in the united states for civilian applications has been the adopted by the national institute of stantechxwlogy  nist   des works by taking a 64-bit value and a 56-bit key and performing a series of transformations these transformations are based on substitution and permutation operations  as is generally the case for symmetric encryption transformations some of the transformations are in that their algorithms are hidden in fact  these so-called s-boxes are classified by the united states government messages longer than 64 bits are broken into 64-bit chunks because des works on a chunk of bits at a time  is known as a cipher if the same key is used for encrypting an extended anwunt of data  it becomes vulnerable to attack consider  for example  that the same source block would result in the same ciphertext if the same key and encryption algorithm were used therefore  the chunks are not just encrypted but also exclusive-or'ed  xored  with the ciphertext block before encryption this is known as des is now considered insecure for many applications because its keys can be exhaustively searched with moderate computing resources rather than giving up on des  though  nist created a modification called in which the des algorithm is repeated three times  two encryptions and one decryption  on the same plaintext usli'lg two or three keys-for example  c = e  k3   d  lc2   e  k1   m     when three keys are used  the effective key length is 168 bits triple des is in widespread use today in 2001  nist a new encryption algorithm  called the to replace des aes is another symmetric block cipher it can use key lengths of 128  192  and 256 bits and works on 128-bit blocks it works by performing 10 to 14 rounds of transformations on a matrix formed from a block generally  the algorithm is compact and efficient several other symmetric block encryption algorithms in use today bear mentioning the algorithm is fast compact  and easy to implement it can use a variable key length of up to 256 bits and works on 128-bit blocks can vary in key length  number of transformations  and block size because it uses only basic computational operations  it can run on a wide variety of crus is perhaps the most common stream cipher a is designed to encrypt and decrypt a stream of bytes or bits rather than a block this is useful when the length of a communication would make a block cipher too slow the key is input into a pseudo-random-bit generator  which is an 642 chapter 15 algorithm that attempts to produce random bits the output of the generator when fed a key is a keystream a is an infinite set of keys that can be used for the input plaintext stream rc4 is used in encrypting steams of data  such as in wep  the wireless lan protocol lt is also used in communications between web browsers and web servers  as we discuss below unfortunately  rc4 as used in wep  ieee standard 802.11  has been found to be breakable in a reasonable amount of con1.puter time in fact rc4 itself has vulnerabilities 15.4.1.2 asymmetric encryption in an there are different encryption and decryption keys here  we one such algorithm  known as rsa after the names of its inventors  rivest  shamir  and adleman   the rsa cipher is a block-cipher public-key algorithm and is the most widely used asymmetrical algorithm asymmetrical algorithms based on elliptical curves are gaining ground  however  because the key length of such an algorithm can be shorter for the same amount of cryptographic strength it is computationally infeasible to derive d  kd  n  from e  lee  n   and so e  ke  n  need not be kept secret and can be widely disseminated ; thus  e  lee  n   or just ice  is the and d  kd  n   or just led  is the n is the write 'i messlge 69/ 0 isl l m ~ encryption_... 695 mod 91 key k5.91 ~  + ' gl ~ gl  j c uc m m  f  .r     ~ 0 ' 1      n read ~ figure 15.8 encryption and decryption using rsa asymmetric cryptography 15.4 643 product of two large  randomly chosen prime numbers p and q  for example  p andq are512bitseach   theencryptionalgorithmis e  kc  n   rn  = mk  mod n  where icc satisfies leekd mod  p -1   q -1  = 1 the decryption algorithm is then d  kd  n   c  = ckd mod n an example using small values is shown in figure 15.8 in this example  we make p = 7 and q = 13 we then calculate n = 7 13 = 91 and  p-1   q -1  = 72 we next select kc relatively prime to 72 and 72  yielding 5 finally  we calculate kd such that kekrt mod 72 = 1  yielding 29 we now have our keys  the public key  lee  n = 5  91  and the private key  led  n = 29  91 encrypting the message 69 with the public key results in the message 62  which is then decoded by the receiver via the private key the use of asymmetric encryption begins with the publication of the public key of the destination for bidirectional communication  the source also must publish its public key publication can be as simple as handing over an electronic copy of the key  or it can be more complex the private key  or secret key  must be jealously guarded  as anyone holding that key can decrypt any message created by the matching public key we should note that the seemingly small difference in key use between asymmetric and symmetric cryptography is quite large in practice asymmetric cryptography is based on mathematical functions rather than transformations  inaking it much more computationally expensive to execute it is much faster for a computer to encode and decode ciphertext by using the usual symmetric algorithms than by using asymmetric algorithms why  then  use an asymmetric algorithm in truth  these algorithms are not used for generalpurpose encryption of large amounts of data however  they are used not only for encryption of small amounts of data but also for authentication  confidentiality  and key distribution  as we show in the following sections 15.4.1.3 authentication we have seen that encryption offers a way of constraining the set of possible receivers of a message constraining the set of potential senders of a message is called authentication is thus complementary to encryption in fact  sometimes their functions overlap consider that an encrypted message can also prove the identity of the sender for example  if d  kd  n   e  ke n   m   produces a valid message  then we know that the creator of the message must hold ke authentication is also useful for proving that a message has not been modified in this section  we discuss authentication as a constraint on possible receivers of a message note that this sort of authentication is similar to but distinct from user authentication  which we discuss in section 15.5 an authentication algorithm consists of the following components  a set k of keys a set m of messages a set a of authenticators a functions  k    m + a   that is  for each k e k  s  k  is a function for generating authenticators from messages both sand s  k  for any k should be efficiently computable functions 644 chapter 15 a function v     +  m x a +  true  false    that is  for each lc e k  v  lc  is a function for verifying authenticators on messages both v and v  lc  for any lc should be efficiently computable functions the critical property that an authentication algorithm must possess is this  for a message m  a computer can generate an authenticator a e a such that v  lc   m  a  = true only if it possesses s  lc   thus  a computer holding s  lc  can generate authenticators on messages so that any computer possessing v  lc  can verify them however  a computer not holding s  lc  can not generate authenticators on messages that can be verified using v  lc   since authenticators are generally exposed  for example  sent on a network with the messages themselves   it must not be feasible to derive s  lc  from the authenticators just as there are two types of encryption algorithms  there are two main varieties of authentication algorithms the first in understanding these algorithms is to explore hash functions a h  m  creates a small  fixed-sized block of data  known as a or from a message m hash functions work by taking a message in n-bit blocks and processing the blocks to produce an n-bit hash h must be collision resistant on m-that is  it must be infeasible to find an 1111 # m such that h  m  = h  n/   now  if h  m  = h  m1   we know that m m1 -that is  we know that the message has not been modified common message-digest functions include which produces a 128-bit hash  and which outputs a 160-bit hash message digests are useful for detecting changed messages but are not useful as authenticators for example  h  m  can be sent along with a message ; but if his known  then someone could modify m and recompute h  m   and the message modification would not be detected therefore  an authentication algorithm takes the message digest and encrypts it the first main type of authentication algorithm uses symmetric encryption in a a cryptographic checksum is generated from message using a secret key knowledge of v  lc  and knowledge of s  lc  are equivalent  one can be derived from the other  so lc must be kept secret a simple example of a mac defines s  lc   m  = f  k  h  m    where f is a function that is one-way on its first argument  that is  k cam10t be derived from f  k  h  m     because of the collision resistance in the hash function  we are reasonably assured that no other message could create the same mac a suitable verification algorithm is then v  lc   m  a  =  j  lc  m  = a   note that k is needed to compute both s  lc  and v  lc   so anyone able to compute one can compute the other the second main type of authentication algorithm is a and the authenticators thus produced are called in a digital-signature algorithm  it is computationally to derive s  ks  from v  lcv  ; in particular  vis a one-way function thus  kv is the public key and lc5 is the private key consider as an example the rsa digital-signature algorithm it is similar to the rsa encryption algorithm  but the key use is reversed the digital signature of a message is derived by computing s  lcs   m  = h  m  s mod n the key /c5 again is a pair  d  n   where n is the product of two large  randomly chosen prime numbers p and q the verification algorithm is then v  kv   m  a  =  ak mod n = h  m    where kv satisfies lc   c5 mod  p  1   q  1  = 1 15.4 645 lf encryption can prove the identity of the sender of a m ~ essage  then why do we need separate authentication algorithms there are three primary reasons authentication algorithms generally require fewer computations  with the notable exception of h.sa digital signatures   over large amounts of plaintext  this efficiency can make a huge difference in resource use and the time needed to authenticate a message the authenticator of a message is almost always shorter than the message and its ciphertext this improves space use and transmission time efficiency sometimes  we want authentication but not confidentiality for example  a company could provide a software patch and could sign that patch to prove that it came from the company and that it has n't been modified authentication is a component of many aspects of security for example  it is the core of which supplies proof that an entity performed an action a typical example of nonrepudiation involves the filling out of electronic forms as an alternative to the signing of paper contracts nonrepudiation assures that a person filling out an electronic form can not deny that he did so 15.4.1.4 key distribution certainly  a good part of the battle between cryptographers  those inventing ciphers  and cryptanalysts  those trying to break them  involves keys with symmetric algorithms  both parties need the key  and no one else should have it the delivery of the symmetric key is a huge challenge sometimes it is performed cut-or-band -say  via a paper document or a conversation these methods do not scale well  however also consider the key-management challenge suppose a user wanted to communicate with n other users privately that user would need n keys and  for more security  would need to change those keys frequently these are the very reasons for efforts to create asymmetric key algorithms not only can the keys be exchanged in public  but a given user needs only one private key  no matter how many other people she wants to communicate with there is still the matter of managing a public key for each party to be communicated with  but since public keys need not be secured  simple storage can be used for that unfortunately  even the distribution of public keys requires some care consider the man-in-the-middle attack shown in figure 15.9 here  the person who wants to receive an encrypted message sends out his public key  but an attacker also sends her bad public key  which matches her private key   the person who wants to send the encrypted message knows no better and so uses the bad key to encrypt the message the attacker then happily decrypts it the problem is one of authentication-what we need is proof of who  or what  owns a public one to solve that problem involves the use of digital certificates a is a public key digitally signed by a trusted party the trusted party receives proof of identification from some entity 646 chapter 15 encryption __ key kbad attacker decryption key kd __ co ' -.u ' c ; aq cd 0 _ decryption ... key kbad .,......_read  + message m i figure 15.9 a man-in-the-middle attack on asymmetric cryptography and certifies that the public key we can trust the certifier these have their public keys i.j.l.cluded within web browsers  and other consumers of certificates  before they are distributed the certificate authorities can then vouch for other authorities  digitally signing the public keys of these other authorities   and so on  creating a web of trust the certificates can be distributed in a standard x.509 digital certificate format that can be parsed by computer this scheme is used for secure web communication  as we discuss in section 15.4.3 15.4.2 implementation of cryptography network protocols are typically organized in each layer acting as a client of the one below it that is  when one protocol generates a message to send to its protocol peer on another machine  it hands its message to the protocol below it in the network-protocol stack for delivery to its peer on that machine for example  in an ip network  tcp  a transport-layer protocol  acts as a client of ip  a network-layer protocol   tcp packets are passed down to ip for delivery to the tcp peer at the other end of the tcp connection ip encapsulates the tcp 15.4 647 packet in an ip packet  which it similarly passes down to the data-link layer to be transmitted across the network to its ip peer on the destination computer this ip peer then delivers the tcp up to the tcp peer on that machine all in all  the which has been almost universally adopted as a model for data networking  defines seven such protocol layers  you will read more about the iso model of networking in chapter 16 ; figure 16.6 shows a diagram of the model  cryptography can be inserted at almost any layer in the iso model ssl  section 15.4.3   for example  provides security at the transport layer networklayer security generally has been standardized on which defines ip packet formats that allow the insertion of authenticators and the encryption of packet contents it uses symmetric encryption and uses the protocol for key ipsec is becoming widely used as the basis for in which all traffic between two ipsec endpoints is encrypted to make a private network out of one that may otherwise be public numerous protocols also have been developed for use by applications  but then the applications themselves must be coded to implement security where is cryptographic protection best placed in a protocol stack in general  there is no definitive answer on the one hand  more protocols benefit from protections placed lower in the stack for example  since ip packets encapsulate tcp packets  encryption of ip packets  using ipsec  for example  also hides the contents of the encapsulated tcp packets similarly  authenticators on ip packets detect the modification of contaii1.ed tcp header information on the other hand  protection at lower layers in the protocol stack may give insufficient protection to higher-layer protocols for example  an application server that runs over ipsec might be able to authenticate the client computers from which requests are received however  to authenticate a user at a client computer  the server may need to use an application-level protocol-for example  the user may be required to type a password also consider the problem of e-mail e-mail delivered via the industry standard smtp protocol is stored and forwarded  frequently multiple times  before it is delivered each of these transmissions could go over a secure or an insecure network for e-mail to be secure  the e-mail message needs to be encrypted so that its security is independent of the transports that carry it 15.4.3 an example  ssl ssl 3.0 is a cryptographic protocol that enables two computers to corrumjj1icate securely-that is  so that each can limit the sender and receiver of to the other it is perhaps the most commonly used cryptographic on the internet today  since it is the standard protocol by which web communicate securely with web servers for completeness  we should note that ssl was designed by netscape and that it evolved into the industry standard tls protocol in this discussion  we use ssl to mean both ssl and tls ssl is a complex protocol with many options here  we present only a single variation of it  and even then in a very simplified and abstract form  so as to maintain focus on its use of cryptographic primitives what we are about to see is a complex dance in which asymmetric cryptography is used so that a client and a server can establish a secure  -cey that can be used for symmetric encryption of the session between the two-all of this while 648 chapter 15 avoiding man-in-the-middle and replay attacks for added cryptographic strength  the session keys are forgotten once a session is completed another communication between the two will generation of new session keys the ssl protocol is initiated by a c to communicate securely with a prior to the protocol 's use  the server s is assumed to have obtained a certificate  denoted cert  from certification authority ca this certificate is a structure containing the following  various attributes attrs of the server  such as its unique distinguished name and its common  dns  name the identity of a public encryption algorithm e   for the server the public key kc of this server a validity interval interval durirtg which the certificate should be considered valid a digital signature a on the above information made by theca-that is  a = s  kca    attrs  e  ke   interval   in addition  prior to the protocol 's use  the client is presumed to have obtained the public verification algorithm v  kca  for ca in the case of the web  the user 's browser is shipped from its vendor containing the verification algorithms and public keys of certain certification authorities the user can add or delete these for certification authorities as she chooses when c connects to s 1 it sends a 28-byte random value nc to the server  which responds with a random value n5 of its own  plus its certificate cert5 the client verifies that v  kca    attrs  e  lee   interval   a  = true and that the current time is in the validity interval interval if both of these tests are satisfied  the server has proved its identity then the client generates a random 46-byte and sends cpms = e  ks   pms  to the server the server recovers pms = d  kd   cpms   now both the client and the server are in possession of nc  n5  and pms  and each can cmnpute a shared 48-byte l ' '' ' c f  nc  715  pms   where f is a one-way and collision-resistant function server and client can compute ms  since only they know pms dependence of ms on nc and n5 ensures that ms is a fresh value-that is  a session key that has not been used in a previous communication at this point  the client and the server both compute the keys the ms  a symmetric encryption key k ~  ypt for encrypting messages from to the server client a symmetric encryption to the client lc ~ rypt for encrypting messages from the server a mac generation jc ~ ac generating authenticators on from the client to the server a mac generation k ~ ~ ac for generating authenticators on from the server to the to send a message m to the server  the client sends 15.5 15.5 649 upon receiving c  the server recovers  m a  = d  jc ~ pt   c  and accepts m if v  lc ~ ac   m  a  = true similarly  to send a message m to the client  the server sends and the client recovers and accepts m if v  k ~ ac   m  a  = true this protocol enables the server to limit the recipients of its messages to the client that generated pms and to limit the senders of the messages it accepts to that same client similarly  the client can limit the recipients of the messages it sends and the senders of the messages it accepts to the party that knows s  kd   that is  the party that can decrypt cpms   in many applications  such as web transactions  the client needs to verify the identity of the party that knows s  lcd   this is one purpose of the certificate cert5 ; in particular  the attrs field contains information that the client can use to determine the identityfor example  the domain name-of the server with which it is communicating for applications in which the server also needs information about the client  ssl supports an option by which a client can send a certificate to the server in addition to its use on the internet  ssl is being used for a wide variety of tasks for example  ipsec vpns now have a competitor in ssl vpns ipsec is good for point-to-point encryption of traffic-say  between two company offices ssl vpns are more flexible but not as efficient  so they might be used between an individual employee working remotely and the corporate office our earlier discussion of authentication involves messages and sessions but what about users if a system can not authenticate a user  then authenticating that a message can'le from that user is thus  a major security problem for operating systems is the protection system depends on the ability to identify the programs and processes currently executing  which in turn depends on the ability to identify each user the system users identify themselves how do we determine a user 's is authentic generally  user authentication is based on one or more things  the user 's possession of something  a or card   the user 's of something user identifier and password   an attribute retina or signature   15.5.1 passwords the most comm.on when the user a user is the use of user id or account name  she 650 chapter 15 is asked for a password i the user-supplied password matches the password stored in the system  the system assumes that the account is being accessed by the owner of that account passwords are often used to protect objects in the computer system  in the absence of more complete protection schemes they can be considered a special case of either keys or capabilities for instance  a password may be associated with each resource  such as a file   whenever a request is made to use the resource  the password nmst be given if the password is correct  access is granted different passwords may be associated with different access rights for example  different passwords may be used for reading files  appending files  and updating files in practice  most systems require only one password for a user to gain full rights although more passwords theoretically would be more secure  such systems tend not to be implemented due to the classic trade-off between security and convenience if security makes something inconvenient then the security is frequently bypassed or otherwise circumvented 15.5.2 password vulnerabilities passwords are extremely common because they are easy to understand and use unfortunately  passwords can often be guessed  accidentally exposed  sniffed  or illegally transferred from an authorized user to an unauthorized one  as we show next there are two common ways to guess a password one way is for the intruder  either human or program  to know the user or to have information about the user all too frequently  people use obvious information  such as the names of their cats or spouses  as their passwords the other way is to use brute force  trying enumeration-or all possible combinations of valid password characters  letters  numbers  and punctuation on some systems  -until the password is found short passwords are especially vulnerable to this method for example  a four-character password provides only 10,000 variations on average  guessing 5,000 times would produce a correct hit a program that could try a password every millisecond would take only about 5 seconds to guess a four-character password enumeration is less successful where systems allow longer passwords that include both uppercase and lowercase letters  along with numbers and all punctuation characters of course  users must take advantage of the large password space and must not  for example  use only lowercase letters in addition to being guessed  passwords can be exposed as a result of visual or electronic monitoring an intruder can look over the shoulder of a user when the user is logging iil and can learn the password easily by watching the keyboard alternatively  anyone with access to the network on which a computer resides can seamlessly add a network monitor  allowing him to watch all data being transferred on the network including user ids and passwords encrypting the data stream containing the password solves this problem even such a system could have passwords stolen  however for example  if a file is used to contain the passwords  it could be copied for off-system analysis or consider a trojan-horse program installed on the system that captures every keystroke before sending it on to the application 15.5 651 exposure is a particularly severe problem if the password is written down where it can be read or lost as we shall see  some systems force users to select hard-to-remember or long passwords  which may cause a user to record the password or to reuse it as a result  such systems provide much less security than systems that allow users to select easy passwords ! the final type of password compromise  illegal transfer  is the result of human nature most computer installations have a rule that forbids users to share accounts this rule is sometimes implemented for accounting reasons but is often aimed at improving security for instance  suppose one user id is shared by several users  and a security breach occurs from that user id it is impossible to know who was using the id at the time the break occurred or even whether the user was an authorized one with one user per user id  any user can be questioned directly about use of the account ; in addition  the user might notice something different about the account and detect the break-in sometimes  users break account-sharing rules to help friends or to circumvent accounting  and this behavior can result in a system 's being accessed by unauthorized users -possibly harmful ones passwords can be either generated by the system or selected by a user system-generated passwords may be difficult to remember  and thus users may write them down as mentioned  however  user-selected passwords are often easy to guess  the user 's name or favorite car  for example   some systems will check a proposed password for ease of guessing or cracking before accepting it at some sites  administrators occasionally check user passwords and notify a user if his password is easy to guess some systems also age passwords  forcing users to change their passwords at regular intervals  every three months  for instance   this method is not foolproof either  because users can easily toggle between two passwords the solution  as implemented on some systems  is to record a password history for each user for instance  the system could record the last n passwords and not allow their reuse several variants on these simple password schemes can be used for example  the password can be changed more frequently in the extren  1e  the password is changed from session to session a new password is selected  either by the system or by the user  at the end of each session  and that password must be used for the next session in such a case  even if a password is misused  it can be used only once when the legitimate user tries to use a now-invalid password at the next session  he discovers the security violation steps can then be taken to repair the breached security 15.5.3 encrypted passwords one problem with all these approaches is the difficulty of keeping the password secret within the computer how can the system store a password securely yet allow its use for authentication when the user presents her password the unix system uses encryption to avoid the necessity of keeping its password list secret each user has a password the system contains a function that is extremely difficult-the designers hope impossible-to invert but is simple to compute that is  given a value x  it is easy to compute the function value f  x   given a function value j  x   however  it is impossible to compute x this function is used to encode all passwords only encoded passwords are stored when a user presents a password  it is encoded and compared against the 652 chapter 15 stored encoded password even if the stored encoded password is seen  it cam1ot be decoded  so the password can not be determined thus  the password file does not need to be kept secret the functionf  x  is typically an encryption algorithm that has been designed and tested rigorously the flaw in this method is that the system no longer has control over the passwords although the passwords are encrypted  anyone with a copy of the password file can run fast encryption routines against it-encrypting each word in a dictionary  for instance  and comparing the results against the passwords if the user has selected a password that is also a word in the dictionary  the password is cracked on sufficiently fast computers  or even on clusters of slow computers  such a comparison may take only a few hours furthermore  because unix systems use a well-known encryption algorithm  a cracker might keep a cache of passwords that have been cracked previously for these reasons  new versions of unix store the encrypted password entries in a file readable only by the the programs that compare a presented password to the stored password run setuid to root ; so they can read this file  but other users can not they also include a salt  or recorded random number  in the encryption algorithm the salt is added to the password to ensure that if two plaintext passwords are the same  they result in different ciphertexts another weakness in the unix password methods is that many unix systems treat only the first eight characters as significant it is therefore extremely important for users to take advantage of the available password space to avoid the dictionary encryption method  some systems disallow the use of dictionary words as passwords a good technique is to generate your password by using the first letter of each word of an easily remembered phrase using both upper and lower characters with a number or punctuation mark thrown in for good measure for example  the phrase my mother 's name is katherine might yield the password mmn.isk !  the password is hard to crack but easy for the user to remember 15.5.4 one-time passwords to avoid the problems of password sniffing and shoulder surfing  a system could use a set of paired when a session begins  the system randomly selects and presents one part of a password pair ; the user must supply the other part in this system  the user is challenged and must with the correct answer to that challenge this approach can be generalized to the use of an algorithm as a password the algorithm might be an integer function  for example the system selects a random integer and presents it to the user the user applies a function and replies with the correct result the system also applies the function if the two results match  access is allowed such algorithmic passwords are not susceptible to reuse ; that is  a user can type in a password  and no entity intercepting that password will be able to reuse it in this scheme  the system and the user share a secret the secret is never transmitted over a medium that allows exposure rather  the secret is used as input to the function  along with a shared seed a is a random number or alphanumeric sequence the seed is the authentication challenge from the computer the secret and the seed are used as input to the function f  secret  seed   the result of this function is transmitted as the password to the 15.5 653 computer becallse the computer also knows the secret and the seed  it can perform the same computation if the results match  the user is authenticated the next time the user needs to be authenticated  another seed is generated  and the same ensue this time  the password is different in this system  the password is different in each instance anyone capturing the password from one session and trying to reuse it in another session will fail one-time passwords are among the only ways to prevent improper authentication clue to password exposure one-time password systems are implemented in various ways commercial implementations  such as securid  use hardware calculators most of these calculators are shaped like a credit card  a key-chain dangle  or a usb device ; they include a display and may or may not also have a keypad some use the current time as the random seed others the user to enter the shared secret  also known as a or on the keypad the display then shows the one-time password the use of both a one-time password generator and a pin is one form of n ! jn  ' ' ' ' two different types of components are needed in this case two-factor authentication offers far better authentication protection than single-factor authentication another variation on one-time passwords uses a or which is a list of single-use passwords each password on the list is used once and then is crossed out or erased the commonly used s/key system uses either a software calculator or a code book based on these calculations as a source of one-time passwords of course  the user must protect his code book 15.5.5 biometrics yet another variation on the use of passwords for authentication involves the use of biometric measures palm or hand-readers are commonly used to secure physical access-for example  access to a data center these readers match stored parameters against what is being read from hand-reader pads the parameters can include a temperature map  as well as finger length  finger width  and line patterns these devices are currently too large and expensive to be used for normal computer authentication fingerprint readers have become accurate and cost-effective and should become more common in the future these devices read finger ridge patterns and convert them into a sequence of numbers over time  they can store a set of sequences to adjust for the location of the finger on the reading pad and other factors software can then scan a finger on the pad and compare its features with these stored sequences to determine if they match of course  multiple users can have profiles stored  and the scanner can differentiate among them a very accurate two-factor authentication scheme can result from requiring a password as well as a user name and fingerprint scan if this information is encrypted in transit  the system can be very resistant to spoofing or replay attack is better still consider how strong authentication can be with a usb device that must be plugged into the system  a pin  and a fingerprint scan except for the user 's having to place her finger on a pad and plug the usb into the system  this authentication method is no less convenient 654 chapter 15 15.6 that using normal passwords recall  though  that strong authentication by itself is not sufficient to guarantee the id of the user an authenticated session can still be hijacked if it is not encrypted just as there are myriad threats to system and network security  there are many security solutions the solutions run the gamut from improved user education  through technology  to bug-free software most security professionals subscribe to the theory of which states that more layers of defense are better than fewer layers of course  this theory applies to any kind of security consider the security of a house without a door lock  with a door lock  and with a lock and an alarm in this section  we look at the major methods  tools  and techniques that can be used to improve resistance to threats 15.6.1 security policy toward improving the security of any aspect of computing is to have a  policies vary widely but generally include a statement of what is being secured for example  a policy might state that all outsideaccessible applications must have a code review before being deployed  or that users should not share their passwords  or that all connection points between a company and the outside must have port scans nm every six months without a policy in place  it is impossible for users and administrators to know what is permissible  what is required  and what is not allowed the policy is a road map to security  and if a site is trying to move from less secure to more secure  it needs a map to know how to get there once the security policy is in place  the people it affects should know it well it should be their guide the policy should also be a that is reviewed and updated periodically to ensure that it is still pertinent and still followed 15.6.2 vulnerability assessment how can we determine whether a security policy has been correctly implemented the best way is to execute a vulnerability assessment such assessments can cover broad ground  from social engineering through risk assessment to port scans rlsl for example  endeavors to value the assets of the entity in question  a program  a management team  a system  or a facility  and determine the odds that a security incident will affect the entity and decrease its value when the odds of suffering a loss and the amount of the potential loss are known  a value can be placed on trying to secure the entity the core activity of most vulnerability assessments is a '., ~ '''--in which the entity is scanned for known vulnerabilities because this book is concerned with operating systems and the software that runs on them  we concentrate on those aspects of vulnerability assessment vulnerability scans typically are done at times when computer use is relatively low  to minimize their impact when appropriate  they are done on 15.6 655 test systems rather than production systems  because they can induce unhappy behavior from the target systems or network devices a scan within an individual system can check a variety of aspects of the system  short or easy-to-guess passwords unauthorized privileged programs  such as setuid programs unauthorized programs in system directories unexpectedly long-running processes improper directory protections on user and system directories improper protections on system data files  such as the password file  device drivers  or the operating-system kernel itself dangerous entries in the program search path  for example  the trojan horse discussed in section 15.2.1  changes to system programs detected with checksum values unexpected or hidden network daemons any problems found by a security scan can be either fixed automatically or reported to the managers of the system networked computers are much more susceptible to security attacks than are standalone systems rather than attacks from a known set of access points  such as directly connected terminals  we face attacks from an unknown and large set of access points-a potentially severe security problem to a lesser extent  systems connected to telephone lines via modems are also more exposed in fact  the u.s government considers a system to be only as secure as its most far-reaching connection for instance  a top-secret system may be accessed only from within a building also considered top-secret the system loses its topsecret rating if any form of communication call occur outside that environment some government facilities take extreme security precautions the connectors that plug a terminal into the secure computer are locked in a safe in the office when the terminal is not in use a person must have proper id to gain access to the building and her office  must know a physical lock combination  and must know authentication information for the computer itself to gain access to the computer-an example of multifactor authentication unfortunately for systems administrators and computer-security professionals  it is frequently impossible to lock a machine in a room and disallow all remote access for instance  the internet network currently connects millions of computers it is becoming a mission-critical  indispensable resource for many companies and individuals if you consider the internet a club  then  as in any club with millions of members  there are many good members and some bad members the bad members have many tools they can use to attempt to gain access to the interconnected computers  just as morris did with his worm vulnerability scans can be applied to networks to address some of the problems with network security the scans search a network for ports that respond to a request if services are enabled that should not be  access to them can be blocked  or they can be disabled the scans then determine the details of 656 chapter 15 the application listening on that port and try to determine if it has any known vulnerabilities testing those vulnerabilities can determine if the system is ncisconfigured or lacks needed patches finally though  consider the use of port scanners in the hands of a cracker rather than someone trying to improve security these tools could help crackers find vulnerabilities to attack  fortunately  it is possible to detect port scans through anomaly detection  as we discuss next  it is a general challenge to security that the same tools can be used for good and for harm in fact  some people advocate stating that no tools should be written to test security  because such tools can be used to find  and exploit  security holes others believe that this approach to security is not a valid one  pointing out  for example  that crackers could write their own tools it seems reasonable that security through obscurity be considered one of the layers of security only so long as it is not the only layer for example  a company could publish its entire network configuration ; but keeping that information secret makes it harder for intruders to know what to attack or to determine what might be detected even here  though  a company assuming that such information will remain a secret has a false sense of security 15.6.3 intrusion detection and facilities is intimately linked to intrusion detection as its name suggests  strives to detect attempted or successful intrusions into computer systems and to initiate appropriate responses to the intrusions intrusion detection encompasses a wide array of techniques that vary on a number of axes  including the following  the time at which detection occurs detection can occur in real time  while the intrusion is occurring  or after the fact the types of inputs examined to detect intrusive activity these may include user-shell commands  process system calls  and network packet headers or contents some forms of intrusion might be detected only by correlating information from several such sources the range of response capabilities simple forms of response include alerting an administrator to the potential intrusion or somehow halting the potentially intrusive activity-for example  killing a process engaged in such activity in a sophisticated fonn of response ; a system might transparently divert an intruder 's activity to a false resource exposed to the attacker the resource appears real to the attacker and enables the system to monitor and gain information about the attack these degrees of freedom in the design space for detecting intrusions have a wide range of solutions  known as and ids systems raise an alarm when an intrusion is detected  while idp systems act as routers  passing traffic unless an intrusion is detected  at which point that traffic is blocked   but just what constitutes an intrusion defining a suitable specification of intrusion turns out to be quite difficult  and thus automatic idss and idps today settle for one of two less ambitious approaches in the first  called system input or network traffic is examined for 15.6 657 specific behavior patterns  or known to indicate attacks a simple example of signature-based detection is scanning network packets for the string /etc/passwd/ targeted for a unix systenl another example is virus-detection software  which scans binaries or network packets for lmown viruses the second approach  typically called attempts through various techniques to detect anomalous behavior within computer systen s of course  not all anomalous system activity indicates an intrusion  but the presumption is that intrusions often induce anomalous behavior an example of anomaly detection is monitoring system calls of a daemon process to detect whether the system-call behavior deviates from normal patterns  possibly indicating that a buffer overflow has been exploited in the daemon to corrupt its behavior another example is monitoring shell commands to detect anomalous commands for a given user or detecting an anomalous login time for a user  either of which may indicate that an attacker has succeeded in gaining access to that user 's account signature-based detection and anomaly detection can be viewed as two sides of the same coin  signature-based detection attempts to characterize dangerous behaviors and to detect when one of these behaviors occurs  whereas anomaly detection attempts to characterize normal  or non dangerous  behaviors and to detect when something other than these behaviors occurs these different approaches yield idss and idps with very different properties  however in particular  anomaly detection can find previously unknown methods of intrusion  so-called signature-based detection  in contrast  will identify only known attacks that can be codified in a recognizable pattern thus  new attacks that were not contemplated when the signatures were generated will evade signature-based detection this problem is well known to vendors of virus-detection software  who must release new signatures with great frequency as new viruses are detected manually anomaly detection is not necessarily superior to signature-based detection  however indeed  a significant challenge for systems that attempt anomaly detection is to benchmark normal system behavior accurately if the system has already been penetrated when it is benchmarked  then the intrusive activity may be included in the normal benchmark even if the system is benchinarked cleanly  without influence from intrusive behaviorf the benchmark must give a fairly complete picture of normal behavior otherwise  the number of  false alarms  orf worse   missed intrusions  will be excessive to illustrate the impact of even a marginally high rate of false alarms  consider an installation consisting of a hundred unix workstations from which security-relevant events are recorded for purposes of intrusion detection a small installation such as this could easily generate a million audit records per day only one or two might be worthy of an administrator 's investigation if we suppose  optimistically  that each actual attack is reflected in ten audit recordsf we can roughly compute the rate of occurrence of audit records reflecting truly intrusive activity as follows  2 intrusions  10  recor s mtrus10n 0.00002 658 chapter 15 interpreting this as a probability of occurrence of intrusive records/ ' we denote it as p  i  ; that is  event i is the occurrence of a record reflecting truly intrusive behavior since p  i  = 0.00002  we also know that p  ~ i  = 1-p  i  = 0.99998 now we let a denote the raising of an alarm by an ids an accurate ids should maximize both p  i la  and p  ~ i i ~ a  -that is  the probabilities that an alarm indicates an intrusion and that no alarm indicates no intrusion focusil g on p  i i a  for the moment  we can compute it using p  iia  p  i  p  aii  p  i  p  aii  + p  ~ i  p  ai ~ i  0.00002 p  aii  0.00002 p  aii  + 0.99998 p  ai ~ i  now consider the impact ofthe false-alarm rate p  ai ~ i  on p  iia   even with a very good true-alarm rate of p  ail  = 0.8  a seemingly good falsealarm rate of p  ai ~ i  = 0.0001 yields p  iia  ~ 0.14 that is  fewer than one ill every seven alarms indicates a real intrusion ! in systems where a security administrator ilwestigates each alarm  a high rate of false alarms-called a christmas tree effect -is exceedingly wasteful and will quickly teach the admilcistrator to ignore alarms this example illustrates a general principle for idss and idps  for usability  they must offer an extremely low false-alarm rate achieving a sufficiently low false-alarm rate is an especially serious challenge for anomaly-detection systems  as mentioned  because of the difficulties of adequately benchmarking normal system behavior however  research contil ues to improve anomalydetection techniques intrusion detection software is evolving to implement signatures  anomaly algorithms  and other algorithms and to combine the results to arrive at a more accurate anomaly-detection rate 15.6.4 virus protection as we have seen  viruses can and do wreak havoc on systems protection from viruses thus is an important security concern antivirus programs are often used to provide this protection some of these programs are effective against only particular known viruses they work by searching all the programs on a system for the specific pattern of instructions known to make up the virus when they find a known pattern  they remove the instructions  the program antivirus programs may have catalogs of thousands of viruses for which they search both viruses and antivirus software continue to become more sophisticated some viruses modify themselves as they infect other software to avoid the basic pattern-match approach of antivirus programs antivirus programs ill turn now look for families of patterns rather than a single pattern to identify a virus in fact  some antivirus programs implement a variety of detection algorithms they can decompress compressed viruses before checking for a signature some also look for process anomalies a process opening an executable file for writing is suspicious  for example  unless it is a compiler another popular teducique is to run a program in a which is a controlled or emulated 15.6 659 the tripwire file system an example of an anomaly-detection tool is the checking tool for unix  developed at purdue university tripwire operates on the premise that many intrusions result in modification of system directories and files for example  an attacker might modify the system programs  perhaps inserting copies with trojan horses  or might insert new programs into directories commonly found in user-shell search paths or an intruder might remove system log files to cover his tracks tripwire is a tool to monitor file systems for added  deleted  or changed files and to alert system administrators to these modifications the operation of tripwire is controlled by a configurationfile tw.config that enumerates the directories and files to be monitored for changes  deletions  or additions each entry in this configuration file includes a selection mask to specify the file attributes  inode attributes  that will be monitored for changes for example  the selection mask might specify that a file 's permissions be monitored but its access time be ignored in addition  the selection mask can instruct thatthe file be monitored for changes monitoring the hash of a file for changes is as good as monitoring the file itselt but storing hashes of files requires far less room than copying the files themselves when run initially  tripwire takes as input the tw.config file and computes a sign.ature for each file or directory consisting of its monitored attributes  inode attributes and hash values   these signatures are stored in a database when run subsequently  tripwire inputs both tw.config and the previously stored database  recomputes the signature for each file or directory named in tw.conf ig  and compares this signature with the signature  if any  in the previously compl.j-ted database events reported to an administrator include any monitored file or directory whose signature differs from that in the database  a changed file   any file or directory in a monitored directory for which a signature does not exist in the database  an added file   and any signature in the database for which the corresponding file or directory no longer exists  a deleted file   although effective for a wide class of attacks  tripwire does have limitations perhaps the most obvious is the need to protect the tripwire program and its associated files  especially the database file  from unauthorized modification for this reason  tripwire and its associated files should be stored on some tamper-proof medium  such as a write-protected disk or a secure server where logins can be tightly controlled unforhm.ately  this makes it less convenient to update the database after authorized updates to monitored directories and files a second limitation is that some security-relevant files-for example  system log files-are supposed to change over time  and tripwire does not provide a way to distinguish between an authorized and an unauthorized change so  for example  an attack that modifies  without deleting  a system log that would normally change anyway would escape tripwire 's detection capabilities the best tripwire can do in this case is to detectcertain obvious inconsistencies  for example  a shrinking log file   free and commercial versions of tripwire are available from 660 chapter 15 section of the system the antivirus software analyzes the behavior of the code in the sandbox before letting it run unmonitored some antivirus programs also put up a complete shield rather than just scanning files within a file system they search boot sectors  menlory  inbound and outbound e-mail  files as they are downloaded  files on removable devices or media  and so on the best protection against computer viruses is prevention  or the practice of purchasing unopened software from vendors and avoiding free or pirated copies from public sources or disk exchange offer the safest route to preventing infection however  even new copies of legitimate software applications are not immune to virus infection  in a few cases  disgruntled employees of a software company have infected the master copies of software programs to do economic harm to the company for macro viruses  one defense is to exchange microsoft word documents in an alternative file format called unlike the native word format rtf does not include the capability to attach macros another defense is to avoid opening any e-mail attachments from unknown users unfortunately  history has shown that e-mail vulnerabilities appear as fast as they are fixed for example  in 2000  the love bug virus became very widespread by traveling in e-mail messages that pretended to be love notes sent by friends of the receivers once a receiver opened the attached visual basic script  the virus propagated by sending itself to the first addresses in the receiver 's e-mail contact list fortunately  except for clogging e-mail systems and users ' inboxes  it was relatively harmless it did  however  effectively negate the defensive strategy of opening attachments only from people known to the receiver a more effective defense method is to avoid opening any e-mail attachment that contains executable code some companies now enforce this as policy by removing all incoming attachments to e-mail messages another safeguard  although it does not prevent infection  does permit early detection a user must begin by completely reformatting the hard disk  especially the boot sector  which is often targeted for viral attack only secure software is uploaded  and a signature of each program is taken via a secure message-digest computation the resulting filename and associated messagedigest list must then be kept free from unauthorized access periodically  or each time a program is run  the operating system recomputes the signature and compares it with the signature on the original list ; any differences serve as a warning of possible infection this technique can be combined with others for example  a high-overhead antivirus scan  such as a sandbox  can be used ; and if a program passes the test  a signature can be created for it if the signatures match the next time the program is run  it does not need to be virus-scanned again 15.6.5 auditing  accounting  and logging auditing  accounting  and logging can decrease system performance  but they are useful in several areas  including security logging can be general or specific all system-call executions can be logged for analysis of program behavior  or misbehavior   more typically  suspicious events are logged authentication failures and authorization failures can tell us quite a lot about break-in attempts 15 15.7 661 accounting is another potential tool in a security administrator 's kit it can be used to find performance changes  which in tum can reveal security problems one of the early unix computer break-ins was detected by cliff stoll when he was exam5ning accounting logs and spotted an anomaly we turn next to the question of how a trusted computer can be connected safely to an untrustworthy network one solution is the use of a firewall to separate trusted and unh usted systems a is a computer  appliance  or router that sits between the trusted and the untrusted a network firewall limits network access between the two and monitors and logs all connections it can also limit coru1.ections based on source or destination address  source or destination port  or direction of the connection for instance  web servers use http to communicate with web browsers a firewall therefore may allow only http to pass from all hosts outside the firewall to the web server within the firewall the morris internet worm used the finger protocol to break into computers  so finger would not be allowed to pass  for example in fact  a network firewall can separate a network into multiple domains a common implementation has the internet as the untrusted domain ; a semitrusted and semisecure network  called the as another domain ; and a company 's computers as a third domain  figure 15.10   coru1.ections are allowed from the internet to the dmz computers and from the company computers to the internet but are not allowed from the internet or dmz computers to the company computers optionally  controlled commurucations may be allowed between the dmz and one company computer or more for instance  a web server on the dmz may need to query a database server on the corporate network with a firewall  however  access is contained  and any dmz systems that are broken into still are unable to access the company computers internet internet access from company 's computers r---------i company computers access between dmz and company 's computers figure 15.10 domain separation via firewall 662 chapter 15 15.8 of course  a firewall itself must be secure and attack-proof ; otherwise  its ability to secure connections can be compromised furthermore  firewalls do not prevent attacks that or travel within protocols or com1ections that the firewall allows a buffer-overflow attack to a web server will not be stopped by the firewall  for example  because the http connection is allowed ; it is the contents of the http connection that house the attack likewise  denialof service attacks can affect firewalls as much as any other machines another vulnerability of firewalls is in which an unauthorized host pretends to be an authorized host by meeting some authorization criterion for example  if a firewall rule allows a connection from a host and identifies that host by its ip address  then another host could send packets using that same address and be allowed through the firewall in addition to the most common network firewalls  there are other  newer kinds of firewalls  each with its pros and cons a is a software layer either included with the operating system or added as an application rather than limiting communication between security domains  it limits communication to  and possibly from  a given host a user could add a personal firewall to her pc so that a trojan horse would be denied access to the network to which the pc is connected  for example an prex-y understands the protocols that applications speak across the network for example  smtp is used for mail transfer an application proxy accepts a com1ection just as an smtp server would and then initiates a connection to the original destination smtp server it can monitor the traffic as it forwards the message  watching for and disabling illegal commands  attempts to exploit bugs  and so on some firewalls are designed for one specific protocol an for example  has the specific purpose of analyzing xml traffic and blocking disallowed or malformed xml sit between applications and the kernel  monitoring system-call execution for example  in solaris 10  the least privilege feature implements a list of more than fifty system calls that processes may or may not be allowed to make a process that does not need to spawn other processes can have that ability taken away  for instance the u.s department of defense trusted computer system evaluation criteria specify four security classifications in systems  a  b  c  and d this specification is widely used to determine the security of a facility and to model security solutions  so we explore it here the lowest-level classification is division d  or minimal protection division d includes only one class and is used for systems that have failed to meet the requirements of any of the other security classes for instance  ms-dos and windows 3.1 are in division d division c  the next level of security  provides discretionary protection and accountability of users and their actions through the use of audit capabilities division c has two levels  c1 and c2 a c1-class system incorporates some form of controls that allow users to protect private information and to keep other users from accidentally reading or destroying their data a c1 environment is one in which cooperating users access data at the same levels of sensitivity most versions of unix are c1 class 15.8 663 the total of all protection systems within a computer system  hardware  software  firmware  that correctly enforce a security policy is known as a the tcb of a cl system controls access between users and files by allowing the user to specify and control sharing of objects by named individuals or defined groups in addition  the tcb requires that the users identify themselves before they start any activities that the tcb is expected to mediate this identification is accomplished via a protected mechanism or password ; the tcb protects the authentication data so that they are inaccessible to unauthorized users a c2-class system adds an individual-level access control to the requirements of a cl system for example  access rights of a file can be specified to the level of a single individual in addition  the system adrninistrator can selectively audit the actions of any one or more users based on individual identity the tcb also protects itself from modification of its code or data structures in addition  no information produced by a prior user is available to another user who accesses a storage object that has been released back to the system some speciat secure versions of unix have been certified at the c2 level division-b mandatory-protection systems have all the properties of a classc2 system ; in addition  they attach a sensitivity label to each object the bl-class tcb maintains the security label of each object in the system ; the label is used for decisions pertaining to mandatory access control for example  a user at the confidential level could not access a file at the more sensitive secret level the tcb also denotes the sensitivity level at the top and bottom of each page of any human-readable output in addition to the normal user-namepassword authentication information  the tcb also maintains the clearance and authorizations of individual users and will support at least two levels of security these levels are hierarchicat so that a user may access any objects that carry sensitivity labels equal to or lower than his security clearance for example  a secret-level user could access a file at the confidential level in the absence of other access controls processes are also isolated through the use of distinct address spaces a b2-class system extends the sensitivity labels to each system resource  such as storage objects physical devices are assigned minimum and maximum security levels that the system uses to enforce constraints imposed by the physical environments in which the devices are located in addition  a b2 system supports covert channels and the auditing of events that could lead to the exploitation of a covert channel a b3-class system allows the creation of access-control lists that denote users or groups not granted access to a given named object the tcb also contains a mechanism to monitor events that may indicate a violation of security policy the mechanism notifies the security administrator and  if necessary  terminates the event in the least disruptive manner the highest-level classification is division a architecturally  a class-al system is functionally equivalent to a b3 system  but it uses formal design specifications and verification techniques  granting a high degree of assurance that the tcb has been implemented correctly a system beyond class al might be designed and developed in a trusted facility by trusted personnel the use of a tcb merely ensures that the system can enforce aspects of a security policy ; the tcb does not specify what the policy should be typically  664 chapter 15 15.9 a given computing environment develops a security policy for and has the plan by a security agency  such as the national computer security center certain computing environments may require other certification  such as that supplied by tempest  which guards against electronic eavesdropping for example  a tempest-certified system has terminals that are shielded to prevent electromagnetic fields from escaping this shielding ensures that equipment outside the room or building where the terminal is housed camwt detect what information is being displayed by the terminal microsoft windows xp is a general-purpose operating system designed to support a variety of security features and methods in this section  we examine features that windows xp uses to perform security functions for more information and background on wilcdows xp  see chapter 22 the windows xp security model is based on the notion of windows xp allows the creation of any number of user accounts  which can be grouped in any manner access to system objects can then be permitted or denied as desired users are identified to the system by a unique security id when a user logs on  windows xp creates a that includes the security id for the user  security ids for any groups of which the user is a member  and a list of any special privileges that the user has examples of special privileges include backing up files and directories  shutting down the compute1 ~ logging on interactively  and changing the system clock every process that windows xp runs on behalf of a user will receive a copy of the access token the system uses the security ids in the access token to permit or deny access to system objects whenever the use1 ~ or a process on behalf of the user  attempts to access the object authentication of a user account is typically accomplished via a user name and password  although the modular design of windows xp allows the development of custom authentication packages for example  a retinal  or eye  scanner might be used to verify that the user is who she says she is windows xp uses the idea of a subject to ensure that programs run by a user do not get greater access to the system than the user is authorized to have a is used to track and manage permissions for each program that a user runs ; it is composed of the user 's access token and the program acting on behalf of the user since windows xp operates with a client-server model  two classes of subjects are used to control access  simple subjects and server subjects an example of a is the typical application program that a user executes after she logs on simple subject is assigned a based on the security access token of the user a is a process implemented as a protected server that uses the security context of the client when acting on the client 's behalf as mentioned in section 15.7  auditing is a useful security technique windows xp has bl1ilt-in auditing that allows many common security threats to be monitored examples include failure auditing for login and logoff events to detect random password break-ins  success auditing for login and logoff events to detect login activity at strange hours  success and failure write-access auditing for executable files to track a virus outbreak  and success and failure auditing for file access to detect access to sensitive files 15.10 15.10 665 security attributes of an object in windows xp are described by a the security descriptor contains the security id of the owner  who can change the access permissions   a group security id used the posix subsystem  a discretionary access-control list that identifies users or groups are allowed  and which are not allowed  access  and a system access-control list that controls which auditing messages the system will generate for example  the security descriptor of the file foo.bar might have owner avi and this discretionary access-control list  a vi -all access group cs-read-write access user cliff-no access in addition  it might have a system access-control list of audit writes by everyone an access-control list is composed of access-control entries that contain the security id of the individual and an access mask that defines all possible actions on the object  with a value of accessallowed or accessdenied for each action files in windows xp may have the following access types  readdata  writedata,appenddata  execute,readextendedattribute  writeextendedattribute  readattributes  and wri teattributes we can see how this allows a fine degree of control over access to objects windows xp classifies objects as either container objects or noncontainer objects such as directories  can logically contain other objects by default  an object is created within a container object  the new object inherits permissions from the parent object similarly  if the user copies a file from one directory to a new directory  the file will inherit the permissions of the destination directory inherit no other permissions furthermore  if a permission is changed on a directory  the new permissions do not automatically apply to existing files and subdirectories ; the user may explicitly apply them if she so desires the system administrator can prohibit printilig to a printer on the system for all or part of a day and can use the windows xp performance monitor to help her spot approaching problems in general  windows xp does a good job of providing features to help ensure a secure computing environment many of these features are not enabled by default  however  which may be one reason for the myriad security breaches on windows xp systems another reason is the vast number of services windows xp starts at system boot tiine and the number of applications that typically are installed on a windows xp system for a real multiuser environment  the system administrator should formulate a security plan and implement it  using the features that windows xp provides and other security tools protection is an internal problem security  in contrast  must consider both the computer system and the environment-people  buildings  businesses  valuable objects  and threats-within which the system is used 666 chapter 15 the data stored in the computer system must be protected from unauthorized access  malicious destruction or alteration  and accidental introduction of inconsistency it is easier to protect against accidental loss of data consistency than to protect against malicious access to the data absolute protection of the information stored in a computer system from malicious abuse is not possible ; but the cost to the perpetrator can be made sufficiently high to deter most  if not all  attempts to access that information without proper authority several types of attacks can be launched against programs and agaitlst individual computers or the masses stack and buffer-overflow techniques allow successful attackers to change their level of system access viruses and worms are self-perpetuating  sometimes infecting thousands of computers denial-of-service attacks prevent legitimate use of target systems encryption limits the domain of receivers of data  while authentication limits the domain of senders encryption is used to provide confidentiality of data being stored or transferred symmetric encryption requires a shared key  while asymn'letric encryption provides a public key and a private key authentication  when combined with hashing  can prove that data have not been changed user authentication methods are used to identify legitimate users of a system in addition to standard user-name and password protection  several authentication methods are used one-time passwords  for example  change from session to session to avoid replay attacks two-factor authentication requires two forms of authentication  such as a hardware calculator with an activation pin multifactor authentication uses three or more forms these methods greatly decrease the chance of authentication forgery methods of preventing or detecting security incidents include intrusiondetection systems  antivirus software  auditing and logging of system events  monitoring of system software changes  system-call monitoring  and firewalls 15.1 argue for or against the judicial sentence handed down against robert morris  jr  for his creation and execution of the internet worm discussed in section 15.3.1 15.2 discuss a means by which managers of systems connected to the internet could design their systems to limit or eliminate the damage done by worms what are the drawbacks of making the change that you suggest 15.3 what commonly used computer programs are prone to man-in-themiddle attacks discuss solutions for preventing this form of attack 15.4 the unix program cops scans a given system for possible security holes and alerts the user to possible problems what are two potential hazards of using such a system for security how can these problems be limited or eliminated 667 15.5 make a list of six security concerns for a bank 's computer system for each item on your list  state whether this concern relates to physicat human  or operating-system ~ security 15.6 an experimental addition to unix allows a user to connect a program to a file the watchdog is invoked whenever a program requests access to the file the watchdog then either grants or denies access to the file discuss two pros and two cons of using watchdogs for security 15.7 discuss how the asymmetric encryption algorithm can be used to achieve the following goals a authentication  the receiver knows that only the sender could have generated the message b secrecy  only the receiver can decrypt the message c authentication and secrecy  only the receiver can decrypt the message  and the receiver knows that only the sender could have generated the message 15.8 why does n't d  lce  n   e  /cd  n   m   provide authentication of the sender to what uses can such an encryption be put 15.9 consider a system that generates 10 million audit records per day also assume that there are on average 10 attacks per day on this system and that each such attack is reflected in 20 records if the intrusion-detection system has a true-alarm rate of 0.6 and a false-alarm rate of 0.0005  what percentage of alarms generated by the system correspond to real intrusions 15.10 what is the purpose of using a salt along with the user-provided password where should the salt be stored  and how should it be used general discussions concerning security are given by hsiao et al  1979l landwehr  1981   deru  1ing  1982   pfleeger and pfleeger  2003   tanenbaum 2003  and russell and gangemi  1991   also of general interest is the text by lobel  1986   computer networking is discussed in kurose and ross  2005   issues concernin ~ g the design and verification of secure systems are discussed by rushby  1981  and by silverman  1983   a security kernel for a multiprocessor microcomputer is described by schell  1983   a distributed secure system is described by rushby and randell  1983   morris and thompson  1979  discuss password security morshedian  1986  presents methods to fight password pirates password authentication 668 chapter 15 with insecure communications is considered by lamport  1981   the issue of password cracking is examined by seely  1989   cmnputer break-ins are discussed by lehmann  1987  and by reid  1987   issues related to trusting computer programs are discussed in thompson  1984   discussions concerning unix security are offered by grampp and morris  1984 l wood and kochan  1985   farrow  1986b   farrow  1986a   filipski and hanko  1986   hecht et al  1988   kramer  1988   and garfinkel et al  2003   bershad and pinkerton  1988  present the watchdog extension to bsd unix the cops security-scanning package for unix was written by farmer at purdue university it is available to users on the internet via the ftp program from host ftp.uu.net in directory /pub i security i cops spafford  1989  presents a detailed technical discussion of the internet worm the spafford article appears with three others in a special section on the morris internet worm in communications of the acm  volume 32  number 6  june 1989   security problems associated with the tcp /ip protocol suite are described in bellovin  1989   the mechanisms commonly used to prevent such attacks are discussed in cheswick et al  2003   another approach to protecting networks from insider attacks is to secure topology or route discovery kent et al  2000   hu et al  2002   zapata and asokan  2002   and hu and perrig  2004  present solutions for secure routing savage et al  2000  examine the distributed denialof service attack and propose ip trace-back solutions to address the problem perlman  1988  proposes an approach to diagnose faults when the network contains malicious routers information about viruses and worms can be found at http  / /www.viruslist.com  as well as in ludwig  1998  and ludwig  2002   other web sites containing up-to-date security information diffie and hellman  1976  and diffie and hellman  1979  were the first researchers to propose the use of the public-key encryption scheme the algorithm presented in section 15.4.1 is based on the public-key encryption scheme ; it was developed by rivest et al  1978   lempel  1979   simmons  1979   denning and demting  1979   gifford  1982   denning  1982   ahituv et al  1987   schneier  1996   and stallings  2003  explore the use of cryptography in computer systems discussions concerning protection of digital signatures are offered by akl  1983   davies  1983   denning  1983   and denning  1984   the u.s government is  of course  concerned about security the department of defense trusted computer system evaluation criteria  dod  1985    known also as the orange book  describes a set of security levels and the features that an operating system must have to qualify for each security rating reading it is a good starting point for understanding security concerns the microsoft windows nt workstation resource kit  microsoft  1996   describes the security inodel of nt and how to use that model the rsa algorithm is presented in rivest et al  1978   information about nist 's aes activities can be found at http  / /home.netscape.com/eng/ssl3/ in 1999  ssl 3.0 was modified slightly and presented in an ietf request for comments  rfc  under the name tls the example in section 15.6.3 illustrating the impact of false-alarm rate on the effectiveness of idss is based on axelsson  1999   the description of tripwire in section 15.6.5 is based on kim and spafford  1993   research into system-call-based anomaly detection is described in forrest et al  1996   part seven a distributed system is a collection of processors that do not share memory or a clock instead  each processor has its own local memory  and the processors communicate with one another through communication lines such as local-area or wide-area networks the processors in a distributed system vary in size and function such systems may include small handheld or real-time devices  personal computers  workstations  and large mainframe computer systems a distributed file system is a file-service system whose users  servers  and storage devices are dispersed among the sites of a distributed system accordingly  service activity has to be carried out across the network ; instead of a single centralized data repository  there are multiple independent storage devices the benefits of a distributed system include giving users access to the resources maintained by the system and thereby speeding up computation and improving data availability and reliability because a system is distributed  however  it must provide mechanisms for process synchronization and communication  for dealing with the deadlock problem  and for handling failures that are not encountered in a centralized system 16.1 a distributed system is a collection of processors that do not share memory or a clock instead  each processor has its own local memory the processors communicate with one another through various communication networks  such as high-speed buses or telephone lines in this chapter  we discuss the general structure of distributed systems and the networks that interconnect them we contrast the main differences in operating-system design between these systems and centralized systems in chapter 17  we go on to discuss distributed file systems then  i11 chapter 18  we describe the methods necessary for distributed operating systems to coordinate their actions to provide a high-level overview of distributed systems and the networks that interconnect them to discuss the general structure of distributed operating systems a is a collection of loosely coupled processors interconnected by a communication network from the point of view of a specific processor in a distributed system  the rest of the processors and their respective resources are remote  whereas its own resources are local the processors in a distributed system may vary in size and function they may include small microprocessors  workstations  minicomputers  and large general-purpose cornputer systems these processors are referred to by a number of names  such as sites  nodes  computers  machines  and hosts  depending on the context in which they are mentioned we mainly use site to indicate the location of a machine and host to refer to a specific system at a site generally  one host at one site  the server  has a resource that another host at another site  the client  or user   would like to use a general structure of a distributed system is shown in figure 16.1 673 674 chapter 16 site a site c network communication site b figure 16.1 a distributed system d d d d l resources l there are four major reasons for building distributed systems  resource sharing  computation speedup  reliability  and communication in this section  we briefly discuss each of them 16.1.1 resource sharing if a number of different sites  with different capabilities  are connected to one another  then a user at one site may be able to use the resources available at another for example  a user at site a may be using a laser printer located at site b meanwhile  a user at b may access a file that resides at a in general  in a distributed system provides mechanisms for sharing files at remote sites  processing information in a distributed database  printing files at remote sites  using remote specialized hardware devices  such as a high-speed array processor   and performing other operations 16.1.2 computation speedup if a particular computation can be partitioned into subcomputations that can run concurrently  then a distributed system allows us to distribute the subcomputations among the various sites ; the subcomputations can be run concurrently and thus provide in addition  if a particular site is currently overloaded with jobs  some of them can be moved to other  lightly loaded sites this movement of jobs is called automated load sharing  in which the distributed operating system automatically moves jobs  is not yet comnlon in commercial systems 16.1.3 reliability if one site fails in a distributed system  the remammg sites can continue operating  giving the system better reliability if the system is composed of multiple large autonomous installations  that is  general-purpose computers   the failure of one of them should not affect the rest if  however  the system 16.2 16.2 675 is composed of sncall machines  each of which is responsible for some crucial system function  such as tenninal character i/0 or the file system   then a single failure may halt the operation of the whole system in general  with enough redundancy  in both hardware and data   the system can continue operation  even if some of its sites have failed the failure of a site must be detected by the system  and appropriate action may be needed to recover from the failure the system must no longer use the services of that site in addition  if the function of the failed site can be taken over by another site  the system must ensure that the transfer of function occurs correctly finally  when the failed site recovers or is repaired  mechanisms must be available to integrate it back into the system smoothly as we shall see in chapters 17 and 18  these actions present difficult problems that have many possible solutions 16.1.4 communication when several sites are connected to one another by a communication network  users at the various sites have the opportunity to exchange information at a low level  are passed between systems  much as messages are passed between processes in the single-computer message system discussed in section 3.4 given message passing  all the higher-level flmctionality found in standalone systems can be expanded to encompass the distributed system such functions include file transfer  login  mail  and remote procedure calls  rpcs   the advantage of a distributed system is that these functions can be carried out over great distances two people at geographically distant sites can collaborate on a project  for example by transferring the files of the project  logging in to each other 's remote systems to run programs  and exchanging mail to coordinate the work  users minimize the limitations inherent in longdistance work we wrote this book by collaborating in such a manner the advantages of distributed systems have resulted in an industry-wide trend toward dovmslzing many companies are replacing their mainframes with networks of workstations or personal computers companies get a bigger bang for the buck  that is  better functionality for the cost   more flexibility in locating resources and expanding facilities  better user interfaces  and easier maintenance in this section  we describe the two general categories of network-oriented operating systems  network operating systems and distributed operating systems network operating systems are simpler to implement but generally more difficult for users to access and utilize than are distributed operating systems  which provide more features 16.2.1 network operating systems a operating provides an environment in which users  who are aware of the multiplicity of machines  can access remote resources by either 676 chapter 16 logging in to the appropriate remote machine or transferring data from the remote machine to their own machines 16.2.1.1 remote login an important function of a network operating system is to allow users to log in remotely the internet provides the telnet facility for this p1.npose to illustrate this facility  lets suppose that a user at westminster college wishes to compute on cs.yale.edu  a computer that is located at yale university to do so  the user must have a valid account on that machine to log in remotely  the user issues the command telnet cs.yale.edu this command results in the formation of a socket connection between the local machine at westminster college and the cs.yale.edu computer after this connection has been established  the networking software creates a transparent  bidirectional link so that all characters entered by the user are sent to a process on cs.yale.edu and all the output from that process is sent back to the user the process on the remote machine asks the user for a login name and a password once the correct information has been received  the process acts as a proxy for the use1 ~ who can compute on the remote machine just as any local user can 16.2.1.2 remote file transfer another major function of a network operating system is to provide a mechanism for remote file transfer from one machine to another in such an enviromnent  each computer maintains its own local file system if a user at one site  say  cs.uvm.edu  wants to access a file located on another computer  say  cs.yale.edu   then the file must be copied explicitly from the computer at yale to the computer at the university of vermont the internet provides a mechanism for such a transfer with the file transfer protocol  ftp  program suppose that a user on cs.uvm.edu wants to copy a java program server java that resides on cs.yale.edu the user must first invoke the ftp program by executing ftp cs.yale.edu the program then asks the user for a login name and a password once the correct information has been received  the user must connect to the subdirectory where the file server java resides and then copy the file by executing get server java in this scheme  the file location is not transparent to the user ; users must know exactly where each file is moreover  there is no real file sharing  because a user can only copy a file from one site to another thus  several copies of the same file may exist  resulting in a waste of space tn addition  if these copies are modified  the vario-us copies will be inconsistent 16.2 677 notice that  in our example  the user at the university of vermont must have login permission on cs.yale.edu ptp also provides a way to allow a user who does not have an account on the yale computer to copy files remotely this remote copying is accomplished through the anonymous ft'p method  which works as follows the file to be copied  that is  server java  must be placed in a special subdirectory  say  jtp  with the protection set to allow the public to read the file a user who wishes to copy the file uses the ftp command as before when the user is asked for the login nan'le  the user supplies the name anonymous and an arbitrary password once anonymous login is accomplished  care must be taken by the system to ensure that this partially authorized user does not access inappropriate files generally  the user is allowed to access only those files that are in the directory tree of user anonymous any files placed here are accessible to any anonymous users  subject to the usual file-protection scheme used on that machine anonymous users  however  cam'lot access files outside of this directory tree implementation of the ftp mechanism is similar to telnet implementation a daemon on the remote site watches for requests to coru'lect to the system 's ptp port login authentication is accomplished  and the user is allowed to execute commands remotely unlike the telnet daemon  which executes any command for the user  the ptp daemon responds only to a predefined set of file-related commands these include the following  get-transfer a file from the remote machine to the local machine put-transfer from the local machine to the remote machine ls or dir-list files in the current directory on the remote machine cd -change the current directory on the remote machine there are also various commands to change transfer modes  for binary or ascii files  and to determine connection status an important point about telnet and ptp is that they require the user to change paradigms ptp requires the user to know a command set entirely different from the normal operating-system commands telnet requires a smaller shift  the user must know appropriate commands on the remote system for instance  a user on a windows machine who teh'lets to a unix machine must switch to unix commands for the duration of the telnet session facilities are more convenient for users if they do not require the use of a different set of commands distributed operating systems are designed to address this problem 16.2.2 distributed operating systems in a distributed operating system  users access remote resources in the same way they access local resources data and process migration from one site to another is under the control of the distributed operating system 16.2.2.1 data migration suppose a user on site a wants to access data  such as a file  that reside at site b the system can transfer the data by one of two basic methods one approach 678 chapter 16 to is to transfer the entire file to site a from that point on  all access to the file is local when the user no longer needs access to the file  a copy of the file  if it has been modified  is sent back to site b even if only a modest change has been made to a large file  all the data must be transferred this mechanism can be thought of as an automated ftp system this approach was used in the andrew file system  as we discuss in chapter 17  but it was found to be too inefficient the other approach is to transfer to site a only those portions of the file that are actually necessary for the immediate task if another portion is required later  another transfer will take place when the user no longer wants to access the file  any part of it that has been modified must be sent back to site b  note the similarity to demand paging  the sun microsystems network file system  nfs  protocol uses this method  chapter 17   as do newer versions of andrew the microsoft smb protocol  running on top of either tcp /ip or the microsoft netbeui protocol  also allows file sharing over a network smb is described in appendix c.6.1 clearly  if only a small part of a large file is being accessed  the latter approach is preferable if significant portions of the file are being accessed  however  it is more efficient to copy the entire file in both methods  data migration includes more than the mere transfer of data from one site to another the system must also perform various data translations if the two sites involved are not directly compatible  for instance  if they use different character-code representations or represent integers with a different number or order of bits   16.2.2.2 computation migration in some circumstances  we may want to transfer the computation  rather than the data  across the system ; this approach is called for example  consider a job that needs to access various large files that reside at different sites  to obtain a summary of those files it would be more efficient to access the files at the sites where they reside and return the desired results to the site that il itiated the computation generally  if the time to transfer the data is longer than the time to execute the remote cmmnand  the remote command should be used such a computation can be carried out in different ways suppose that process p wants to access a file at site a access to the file is carried out at site a and could be il itiated by an rpc an rpc uses a  udp on the internet  to execute a routine on a remote system  section 3.6.2   process p invokes a predefilced procedure at site a the procedure executes appropriately and then returns the results to p alternatively process p can send a message to site a the operatil g system at site a then creates a new process q whose function is to carry out the designated task when process q completes its execution  it sends the needed result back to p via the message system in this scheme  process p may execute concurrently with process q ; in fact  it may have several processes running concurrently on several sites either method could be used to access several files residing at various sites one rpc might result in the ilwocation of another rpc or even in the transfer of messages to another site similarly  process q could  duril g the course of its 16.3 16.3 679 execution  send a message to another site  which in turn would create another process this process might either send a message back to q or repeat the cycle 16.2.2.3 process migration a logical extension of computation migration is na  ' ' ~ c process is submitted for execution  it is not always executed at it is initiated the entire process  or parts of it  may be executed at different sites this scheme may be used for several reasons  load balancing the processes  or subprocesses  may be distributed across the network to even the workload computation speedup if a single process can be divided into a number of subprocesses that can run concurrently on different sites  then the total process turnaround time can be reduced hardware preference the process may have characteristics that make it more suitable for execution on some specialized processor  such as matrix inversion on an array processor  rather than on a microprocessor software preference the process may require software that is available at only a particular site  and either the software can not be moved  or it is less expensive to move the process data access just as in computation migration  if the data being used in the computation are numerous  it may be more efficient to have a process run remotely than to transfer all the data we use two complementary techniques to move processes in a computer network in the first  the system can attempt to hide the fact that the process has migrated from the client this scheme has the advantage that the user does not need to code her program explicitly to accomplish the migration this method is usually employed for achieving load balancing and computation speedup among homogeneous systems  as they do not need user input to help them execute programs remotely the other approach is to allow  or require  the user to specify explicitly how the process should migrate this method is usually employed when the process must be moved to satisfy a hardware or software preference you have probably realized that the web has many aspects of a distributedcomputing environment certainly it provides data migration  between a web server and a web client   it also provides computation migration for instance  a web client could trigger a database operation on a web server finally  with java  it provides a form of process migration  java applets are sent from the server to the client  where they are executed a network operating system provides most of these features  but a distributed operating system makes them seamless and easily accessible the result is a powerful and easy-to-use facility-one of the reasons for the huge growth of the world wide web there are basically two types of networks  and the main difference between the two is the way in 680 chapter 16 which they are geographically distributed local-area networks are composed of processors distributed over small areas  such as a single building or a number of adjacent buildings   whereas wide-area networks are composed of a number of autonomous processors distributed over a large area  such as the united states   these differences imply major variations in the speed and reliability of the communications networks  and they are reflected in the distributed operating-system design 16.3.1 local-area networks local-area networks emerged in the early 1970s as a substitute for large mainframe computer systems for many enterprises  it is more economical to have a number of small computers  each with its own self-contained applications  than to have a single large system because each small computer is likely to need a full complement of peripheral devices  such as disks and printers   and because some form of data sharing is likely to occur in a single enterprise  it was a natural step to connect these small systems into a network lans  as mentioned  are usually designed to cover a small geographical area  such as a single building or a few adjacent buildings  and are generally used in an office environment all the sites in such systems are close to one another  so the communication links tend to have a higher speed and lower error rate than do their cou.rjerparts in wide-area networks high-quality  expensive  cables are needed to attain this higher speed and reliability it is also possible to use the cable exclusively for data network traffic over longer distances  the cost of using high-quality cable is enormous  and the exclusive use of the cable tends to be prohibitively expensive the most conunon links in a local-area network are twisted-pair and fiberoptic cabling the most common configurations are multiaccess bus  ring  and star networks communication speeds range from 1 megabit per second  for networks such as appletalk  infrared  and the new bluetooth local radio network  to 1 gigabit per second for ethernet ten megabits per second is the speed of requires a higher-quality cable but runs at 100 m ~ egabits per second and is common also growing is the use of optical-fiber-based fddi networking the fddi network is token-based and runs at over 100 megabits per second a typical lan may consist of a number of different computers  from mainframes to laptops or pdas   various shared peripheral devices  such as laser printers and magnetic-tape drives   and one or more gateways  specialized processors  that provide access to other networks  figure 16.2   an ethernet scheme is commonly used to construct lans an ethernet network has no central controller  because it is a multiaccess bus  so new hosts can be added easily to the network the ethernet protocol is defined by the ieee 802.3 standard there has been significant growth in using the wireless spectrum for designing local-area networks wireless  or wifi  networks allow constructing a network using only a wireless router for transmitting signals between hosts each host has a wireless adapter networking card which allows it to join and use the wireless network however  where ethernet systems often run at 100 megabits per second  wifi networks typically run at slower speeds there are 16.3 681 workstation workstation workstation printer laptop file server figure 16.2 local-area network several ieee standards for wireless networks  802.11g can theoretically run at 54 megabits per second  although ilc practice data rates are often less than half that amount the recent 802.11n standard provides theoretically much higher data rates than 802.11g  although in actual practice 802.11n networks have typical data rates of around 75 megabits per second data rates of wireless networks are heavily influenced by the distance between the wireless router and the host as well as interference in the wireless spectrum wireless networks often have a physical advantage over wired ethernet networks as no cabling needs to be run to connect communicatilcg hosts as a result  wireless networks are popular in homes as well as public areas such as libraries and internet cafes 16.3.2 wide-area networks wide-area networks emerged in the late 1960s  mainly as an academic research project to provide efficient communication among sites  allowing hardware and software to be shared conveniently and economically by a wide community of users the first wan to be designed and developed was the arpanet begun in 1968  the arpanet has grown from a four-site experimental network to a worldwide network of networks  the internet  comprising millions of computer systems because the sites in a wan are physically distributed over a large geographical area  the communication links are  by default  relatively slow and unreliable typical links are telephone lines  leased  dedicated data  lines  microwave links  and satellite channels these communication links are controlled by special  figure 16.3   which are responsible for defilcing the interface through which the sites communicate over the network  as well as for transferring information among the various sites 682 chapter 16 communication subsystem h h netwot k host communication processor figure 16.3 communication processors in a wide-area network for example  the internet wan enables hosts at geographically separated sites to communicate with one another the host computers typically differ from one another in type  speed  word length  operatil1.g system  and so on hosts are generally on lans  which are  in turn  connected to the internet via regional networks the regional networks  such as nsfnet il1 the northeast united states  are interlinked with  section 16.5.2  to form the worldwide network connections between networks frequently use a telephone-system service called t1  which provides a transfer rate of 1.544 megabits per second over a leased line for sites requiring faster internet access  tls are collected into multiple-t1 units that work in parallel to provide more throughput for instance  a t3 is composed of 28 t1 connections and has a transfer rate of 45 megabits per second the routers control the path each message takes through the net this routing may be either dynamic  to increase commmlication efficiency  or static  to reduce security risks or to allow communication charges to be computed other wans use standard telephone lines as their primary means of communication are devices that accept digital data from the computer side and convert it to the analog signals that the telephone system uses a modem at the destination site converts the analog signal back to digital form  and the destination receives the data the unix news network  uucp  allows systems to communicate with each other at predetermined times  via modems  to exchange messages the messages are then routed to other nearby systems and in this way either are propagated to all hosts on the network  public messages  or are transferred to specific destinations  private messages   wans are generally slower than lans ; their transmission rates range from 1,200 bits 16.4 16.4 683 per second to over 1 megabit per second uucp has been superseded by ppp  the point-to-point protocol ppp functions over modem coru1ections  allowing home computers to be fully connected to the internet the sites in a distributed system can be connected physically in a variety of ways each configuration has advantages and disadvantages we can compare the configurations by using the following criteria  installation cost the cost of physically linking the sites in the system communication cost the cost in time and money to send a message from site a to site b availability the extent to which data can be accessed despite the failure of some links or sites the various topologies are depicted in figure 16.4 as graphs whose nodes correspond to sites an edge from node a to node b corresponds to a direct communication link between the two sites in a fully connected network  each site is directly connected to every other site however  the number of links grows as the square of the number of sites  resulting in a huge installation cost therefore  fully connected networks are impractical in any large system in a pc  ntially direct links exist between some-but not all-pairs of sites hence  the installation cost of such a configuration is lower than that of the fully connected network however  if two sites a and b are not directly connected  messages from one to the other must be through a sequence of communication links this requirement results in a higher communication cost if a communication link fails  messages that would have been transmitted across the link must be rerouted in some cases  another route through the network may be found  so that the messages are able to reach their destination in other cases  a failure may mean that no connection exists between some pair  or pairs  of sites when a system is split into two  or more  unconnected subsystems  it is partitioned under this definition  a subsystem  or partition  may consist of a single node the various partially connected network types include tree-structured networks  ring networks  and star networks  as shown in figure 16.4 these types have different failure characteristics and installation and communication costs installation and communication costs are relatively low for a treestructured network however  the failure of a single link in such a network can result in the network 's becoming partitioned in a ring network  at least two links must fail for partition to occur thus  the ring network has a higher degree of availability than does a tree-structured network however  the communication cost is high  since a message may have to cross a large number of links in a star network  the failure of a single link results in a network partition  but one of the partitions has only a single site such a partition can be treated as a single-site failure the star network also has a low communication cost  since each site is at most two links away from every other site howeve1 ~ 684 chapter 16 16.5 fully connected network partially connected network b d f tree-structured network star network f ring network figure 16.4 network topology if the central site fails  all the sites in the system become disconnected from one another now that we have discussed the physical aspects of networking  we turn to the internal workings the designer of a communication network must address five basic issues  naming and name resolution how do two processes locate each other to communicate routing strategies how are messages sent through the network packet strategies are packets sent individually or as a sequence connection strategies how do two processes send a sequence of messages 16.5 685 contention how do we resolve conflicting demands for the network 's lise  given that it is a shared resource in the following sections  we elaborate on each of these issues 16.5.1 naming and name resolution the first component of network communication is the naming o the systems in the network for a process at site a to exchange information with a process at site b  each must be able to specify the other within a computer system  each process has a process identifier  and messages may be addressed with the process identifier beca use networked systems share no memory  however  a host within the system initially has no knowledge about the processes on other hosts to solve this problem  processes on remote systems are generally identified by the pair host name  identifier  where host name is a name unique within the network and identifier may be a process identifier or other unique number within that host a host name is usually an alphanumeric identifier  rather than a number  to make it easier for users to specify for instance  site a might have hosts named homer  marge  bart  and lisa bart is certainly easier to remember than is 12814831100 names are convenient for humans to use  but computers prefer numbers for speed and simplicity for this reason  there must be a mechanism to !  '  the host name into a that describes the destination system to the networking hardware this mechanism is similar to the name-to-address binding that occurs during program compilation  linking  loading  and execution  chapter 8   in the case of host names  two possibilities exist first  every host may have a data file containing the names and addresses of all the other hosts reachable on the network  similar to binding at compile time   the problem with this model is that adding or removing a host from the network requires updati.n.g the data files on all the hosts the alternative is to distribute the information among systems on the network the network must then use a protocol to distribute and retrieve the information this scheme is like execution-time binding the first method was the one originally used on the internet ; as the internet rnarr-,,or it became untenable  so the second method  the domain-name ' ~ ' ' is now in use dns specifies the naming structure of the hosts  as well as name-to-address resolution hosts on the internet are logically addressed with multipart names known as ip addresses the parts of an ip address progress frorn the most specific to the most general part  with periods separating the fields for instance  bob.cs.brown.edu refers to host bob in the depattment of science at brown university within the top-level domain edu domains include com for commercial sites and for organizations  as well as connected to the for systems the resolves in reverse order each a a process on a a name and returns the address of the name server as the final the name server for the host in host-id is returned for a made communicate with bob.cs.brown.edu would result in 686 chapter 16 the kernel of system a issues a request to the name server for the edu domain  asking for the address of the name server for brown.edu the name server for the edu domain must be at a known address  so that it can be queried the edu nance server returns the address of the host on which the brown.edu name server resides the kernel on system a then queries the name server at this address and asks about cs.brown.edu an address is returned ; and a request to that address for bob.cs.brown.edu now  finally  returns an host-id for that host  for example  128.148.31.100   this protocol may seem inefficient  but local caches are usually kept by each name server to speed the process for example  the edu name server would have brown.edu in its cache and would inform system a that it could resolve two portions of the name  returning a pointer to the cs.brown.edu name server of course  the contents of these caches must be refreshed over time in case the name server is moved or its address changes in fact  this service is so important that many optimizations have occurred in the protocol  as well as many safeguards consider what would happen if the primary edu name server crashed it is possible that no edu hosts would be able to have their addresses resolved  making them all lmreachable ! the solution is to use secondary  back-up name servers that duplicate the contents of the primary servers before the domain-name service was introduced  all hosts on the internet needed to have copies of a file that contained the names and addresses of each host on the network all changes to this file had to be registered at one site  host sri-nic   and periodically all hosts had to copy the updated file from sri-nic to be able to contact new systems or find hosts whose addresses had changed under the domain-name service  each name-server site is responsible for updating the host information for that domain for instance  any host changes at brown university are the responsibility of the name server for brown.edu and need not be reported anywhere else dns lookups will automatically retrieve the updated information because they will contact brown.edu directly within domains  there can be autonomous subdomains to further distribute the responsibility for host-name and host-id changes java provides the necessary api to design a program that maps ip names to ip addresses the program shown in figure 16.5 is passed an ip name  such as bob.cs.brown.edu  on the command line and either outputs the ip address of the host or returns a message indicating that the host name could not be resolved an inetaddress is a java class representing an ip name or address the static method getbyname   belonging to the inetaddress class is passed a string representation of an ip name  and it returns the corresponding inetaddress the program then invokes the gethostaddress   method  which internally uses dns to look up the ip address of the designated host generally  the operating system is responsible for accepting from its processes a message destined for host name  identifier and for transferring that message to the appropriate host the kernel on the destination host is then responsible for transferring the message to the process named by the identifier this exchange is by no means trivial ; it is described in section 16.5.4 16.5 i usage  java dnslookup ip name i.e java dnslookup www.wiley.com i public class dnslookup   public static void main  string   args   inetaddress hostaddress ; try   hostaddress = inetaddress.getbyname  args  o   ; system.out.println  hostaddress.gethostaddress    ; catch  unknownhostexception uhe     system err println  unknown host  + args  0   ; figure 16.5 java program illustrating a dns lookup 16.5.2 routing strategies 687 when a process at site a wants to communicate with a process at site b  how is the message sent if there is only one physical path from a to b  such as in a star or tree-structured network   the message must be sent through that path however  if there are multiple physical paths from a to b  then several routing options exist each site has a indicating the alternative paths that can be used to send a message to other sites the table may include information about the speed and cost of the various communication paths  and it may be updated as necessary  either manually or via programs that exchange routing information the three most common routing schemes are td ~ .-i  ja  and fixed routing a path from a to b is specified in advance and does not change unless a hardware failure disables it usually  the shortest path is chosen  so that communication costs are minimized virtual routing a path from a to b is fixed for the duration of one different sessions involving messages from a to b may use different paths a session could be as short as a file transfer or as long as a remote-login period dynamic routing the path used to send a message from site a to site b is chosen only when the message is sent because the decision is made dynamically  separate messages may be assigned different paths site a will make a decision to send the message to site c ; c  in turn  will decide to send it to sited  and so on eventually  a site will deliver the message to b usually  a site sends a message to another site on whatever link is the least used at that particular time there are tradeoffs among these three schem.es fixed routing can not adapt to link failures or load changes in other words  if a path has been established 688 chapter 16 between a and b  the messages must be sent along this path  even if the path is down or is used more heavily than another possible path we can partially remedy this problem by using virtual routing and can avoid it completely by using dynamic routing fixed routing and virtual routing ensure that ncessages from a to b will be delivered in the order in which they were sent in dynamic routing  messages may arrive out of order we can remedy this problem by appending a sequence number to each message dynamic routing is the most complicated to set up and run ; however  it is the best way to manage routing in complicated environments unix provides both fixed routing for use on hosts within simple networks and dynamic routing for complicated network environments it is also possible to mix the two within a site  the hosts may just need to know how to reach the system that connects the local network to other networks  such as company-wide networks or the internet   such a node is known as a each individual host has a static route to the gateway  although the gateway itself uses dynamic routing to reach any host on the rest of the network a router is the entity within the computer network responsible for routing messages a router can be a host computer with routing software or a special-purpose device either way  a router must have at least two network cmmections  or else it would have nowhere to route messages a router decides whether any given message needs to be passed from the network on which it is received to any other network connected to the router it makes this determination by examining the destination internet address of the message the router checks its tables to determine the location of the destination host  or at least of the network to which it will send the message toward the destination host in the case of static routing  this table is changed only by manual update  a new file is loaded onto the router   with dynamic routing  a is used between routers to inform them of network changes and to allow them to update their routing tables automatically gateways and routers typically are dedicated hardware devices that run code out of firmware 16.5.3 packet strategies messages generally vary in length to simplify the system design  we commonly implement communication with fixed-length messages called or a communication incplemented in one packet can be sent to its destination in a a connectionless message can be in which case the sender has no guarantee that  and can not tell whether  the packet reached its destination alternatively  the packet can be usually  in this case  a packet is returned from the destination indicating that the packet arrived  of course  the return packet could be lost along the way  if a message is too long to fit within one packet  or if the packets need to how back and forth between the two communicators  a connection is established to allow the reliable exchange of multiple packets 16.5.4 connection strategies c ~ uuc'' ' ~ u are able to reach their destinations  processes can institute to exchange information pairs of processes that want to communicate over the network can be connected in a number of ways 16.5 689 the three most common schemes are and circuit switching if two processes want to con1municate  a permanent physical link is established between them tl1is link is allocated for the duration of the communication session  and no other process can use that link during this period  even if the two processes are not actively communicating for a while   this scheme is similar to that used in the telephone system once a communication line has been opened between two parties  that is  party a calls party b   no one else can use this circuit until the communication is terminated explicitly  for example  when the parties hang up   message switching if two processes want to communicate  a temporary link is established for the duration of one message transfer physical links are allocated dynamically among correspondents as needed and are allocated for only short periods each message is a block of data with system information-such as the source  the destination  and errorcorrection codes  ecc  -that allows the communication network to deliver the message to the destination correctly this scheme is similar to the post-office mailing system each letter is a message that contains both the destination address and source  return  address many messages  from different users  can be shipped over the same link packet switching one logical message may have to be divided into a number of packets each packet may be sent to its destination separately  and each therefore must include a source and a destination address with its data furthermore  the various packets may take different paths through the network the packets must be reassembled into messages as they arrive note that it is not harmful for data to be broken into packets  possibly routed separately  and reassembled at the destination breaking up an audio signal  say  a telephone communication   in contrast  could cause great confusion if it was not done carefully there are obvious tradeoffs among these schemes circuit switching requires substantial set-up time and may waste network bandwidth  but it incurs less overhead for shipping each message conversely  message and packet switching require less set-up time but incur more overhead per message also  in packet switching  each message must be divided into packets and later reassembled packet switching is the method most commonly used on data networks because it makes the best use of network bandwidth 16.5.5 contention depending on the network topology  a link may cmmect more than two sites in the computer network  and several of these sites may want to transmit information over a link simultaneously this situation occurs mainly in a ring or multiaccess bus network in this case  the transmitted information may become scrambled if it does  it must be discarded ; and the sites must be notified about the problem so that they can retransmit the information if no special provisions are made  this situation may be repeated  resulting in degraded performance 690 chapter 16 16.6 several techniques have been developed to avoid repeated collisions  including collision detection and token passing csma/cd before transmitting a message over a link  a site must listen to determine whether another message is currently being transmitted over that link ; this technique is called -uvith  if the link is free  the site can start transmitting otherwise  it must wait  and continue to listen  until the link is free if two or more sites begin transmitting at exactly the same time  each thinking that no other site is using the link   then they will register a and will stop transmitting each site will try again after some random time interval the main problem with this approach is that  when the system is very busy  many collisions may occur  and thus performance may be degraded nevertheless  csma/cd has been used successfully in the ethernet system  the most common local area network system one strategy for limiting the number of collisions is to limit the number of hosts per ethernet network adding more hosts to a congested network could result in poor network throughput as systems get faster  they are able to send more packets per time segment as a result  the number of systems per ethernet network generally is decreasing so that networking performance is kept reasonable token passing a unique message type  known as a continuously circulates in the system  usually a ring structure   a site that wants to transmit information must wait until the token arrives it then removes the token from the ring and begins to transmit its messages when the site completes its round of message passing  it retransmits the token this action  in turn  allows another site to receive and remove the token and to start its message transmission if the token gets lost  the system must detect the loss and generate a new token it usually does that by declaring an to choose a unique site where a new token will be generated later  in section 18.6  we present one election algorithm a token-passing scheme has been adopted by the ibm and hp i apollo systems the benefit of a token-passing network is that performance is constant adding new sites to a network may lengthen the waiting time for a token  but it will not cause a large performance decrease  as may happen on ethernet on lightly loaded networks  however  ethernet is more efficient  because systems can send messages at any time when we are designing a communication network  we must deal with the inherent complexity of coordinating asynchronous operations communicating in a potentially slow and error-prone environment in addition  the systems on the network must agree on a protocol or a set of protocols for determining host names  locating hosts on the network  establishing connections  and so on we can simplify the design problem  and related implementation  by partitioning the problem into multiple layers each layer on one system communicates with the equivalent layer on other systems typically  each layer has its own protocols  and communication takes place between peer layers 16.6 691 network environment iso environment real systems environment figure 16.6 two computers communicating via the iso network model using a specific protocol the protocols may be implemented in hardware or software for instance  figure 16.6 shows the logical communications between two computers  with the three lowest-level layers implemented in hardware following the international standards organization  iso   we refer to the layers as follows  physical layer the physical layer is responsible for handling both the mechanical and the electrical details of the physical transmission of a bit stream at the physical layer  the communicating systems must agree on the electrical representation of a binary 0 and 1  so that when data are sent as a stream of electrical signals  the receiver is able to interpret the data properly as binary data this layer is implemented in the hardware of the networking device data-link layer the data-link layer is responsible for handlingfi'ames  or fixed-length parts of packets  including any error detection and recovery that occurs in the physical layer network layer the network layer is responsible for providing connecti01cs and for routing packets in the communication network  including handling the addresses of outgoing packets  decoding the addresses of incoming packets  and maintaining routing information for proper response to changing load levels routers work at this layer transport layer the transport layer is responsible for low-level access to the network and for transfer of messages between clients  including partitioning messages into packets  maintaining packet order  controlling flow  and generating physical addresses session layer the session layer is responsible for implementing sessions  or process-to-process communication protocols typically  these protocols are the actual communications for remote logins and for file and mail transfers 692 chapter 16 presentation layer the presentation layer is responsible for resolving the differences in formats among the various sites in the network  including character conversions and half duplex-full duplex modes  character echoing   application layer the application layer is responsible for interacting directly with users this layer deals with file transfe1 ~ remote-login protocols  and electronic mail  as well as with schemas for distributed databases figure 16.7 summarizes the set of cooperating protocols-showing the physical flow of data as mentioned  logically each layer of a protocol stack communicates with the equivalent layer on other systems but physically  a message starts at or above the application layer and end-user application process distributed information transfer-syntax negotiation data-representation transformations dialog and synchronization control for application entities network-independent message-interchange service j end-to ~ end message transfer  connection management  error control  fragmentation  flow control  network routing  addressing  call set-up and clearing application layer presentation layer session layer transport layer network layer data-link control  framing  data transparency  error control  link layer mechanical and electrical networkcinterface connections physical connection to network termination equipment physical layer 16.7 the iso protocol stack 16.6 data-link -layer header network-layer header transport-layer header f-------1 session-layer header f-------1 presentation layer f-------1 application layer message l_ _ _____j data-link -layer trailer figure 16.8 an iso network message 693 is passed through each lower level in turn each layer may modify the message and il1.clude message-header data for the equivalent layer on the receiving side ultimately  the message reaches the data-network layer and is transferred as one or more packets  figure 16.8   the data-lil1.k layer of the target system receives these data  and the message is moved up through the protocol stack ; it is analyzed  modified  and stripped of headers as it progresses it fu1.ally reaches the application layer for use by the receiving process the iso model formalizes some of the earlier work done in network protocols but was developed in the late 1970s and is currently not in widespread use perhaps the most widely adopted protocol stack is the tcp /ip model  which has been adopted by virtually all internet sites the tcp /ip protocol stack has fewer layers than does the iso model theoretically  because it combilles several functions ill each layer  it is more difficult to implement but more efficient than iso networking the relationship between the iso and tcp /ip models is shown in figure 16.9 the tcp /ip application layer identifies several protocols ill widespread use ill the internet  illcluding http  ftp  telnet  dns  and smtp the transport layer identifies the unreliable  connectionless user datagram protocol  udp  and the reliable  connection-oriented transmission control protocol  tcp   the internet protocol  ip  is responsible for routing ip datagrams through the internet the tcp /ip model does not formally identify a link or physical laye1 ~ allowing tcp /ip traffic to run across any physical network in section 16.9  we consider the tcp /ip model running over an ethernet network security should be a concern in the design and implementation of any modern communication protocol both strong authentication and encryption are needed for secure communication strong authentication ensures that the sender and receiver of a communication are who or what they are supposed to be encryption protects the contents of the communication from eavesdropping weak authentication and clear-text communication are still very common  however  for a variety of reasons when most of the 694 chapter 16 16.7 iso presentation session physical tcp/ip http  dns  telnet smtp  ftp not defined not defined tcp-udp not defined not defined figure 16.9 the iso and tcp/ip protocol stacks common protocols were designed  security was frequently less important than performance  simplicity  and efficiency strong authentication requires a multistep handshake protocol or authentication devices  adding complexity to a protocol modern cpus can efficiently perform encryption  and systems frequently offload encryption to separate cryptography processors  so system performance is not compromised longdistance communication can be made secure by authenticating the endpoints and encrypting the stream of packets in a virtual private network  as discussed in 15.4.2 lan communication remains unencrypted at most sites  but protocols such as nfs version 4  which includes strong native authentication and encryption  should help improve even lan security a distributed system may suffer from various types of hardware failure the failure of a link  the failure of a site  and the loss of a message are the most common types to ensure that the system is robust  we must detect any of these failures  reconfigure the system so that computation can continue  and recover when a site or a link is repaired 16.7.1 failure detection in an environment with no shared memory  we are generally unable to differentiate among link failure  site failure  and message loss we can usually detect only that one of these failures has occurred once a failure has been 16.7 695 detected  appropriate action must be taken what action is appropriate depends on the particular application to detect link and site failure  we use a procedure suppose that sites a and b have a direct physical link between them  at fixed intervals  the sites send each other an j-am-up m.essage if site a does not receive this message within a predetermined time period  it can assume that site b has failed  that the link between a and b has failed  or that the message from b has been lost at this point  site a has two choices it can wait for another time period to receive an j-am-up message from b  or it can send an are-you-up message to b if time goes by and site a still has not received an j-am-up message  or if site a has sent an are-you-up message and has not received a reply  the procedure can be repeated again  the only conclusion that site a can draw safely is that some type of failure has occurred site a can try to differentiate between link failure and site failure by sending an are-you-up message to b by another route  if one exists   if and when b receives this message  it immediately replies positively this positive reply tells a that b is up and that the failure is in the direct link between them since we do not know in advance how long it will take the message to travel from a to b and back  we must use a at the time a sends the are-you-up message  it specifies a time interval during which it is willing to wait for the reply from b if a receives the reply message within that time interval  then it can safely conclude that b is up if not  however  that is  if a time-out occurs   then a may conclude only that one or more of the following sih1ations has occurred  site b is down the direct link  if one exists  from a to b is down the alternative path from a to b is down the message has been lost site a can not  however  determine which of these events has occurred 16.7.2 reconfiguration suppose that site a has discovered  through the mechanism described in the previous section  that a failure has occurred it must then initiate a procedure that will allow the system to reconfigure and to continue its normal mode of operation if a direct link from a to b has failed  this information must be broadcast to every site in the system  so that the various routing tables can be updated accordingly if the system believes that a site has failed  because that site can be reached no longer   then all sites in the system must be so notified  so that they will no longer attempt to use the services of the failed site the failure of a site that serves as a central coordinator for some activity  such as deadlock detection  requires the election of a new coordinator similarly  if the failed 696 chapter 16 site is part of a logical ring  then a new logical ring must be constructed note that  if the site has not failed  that is  if it is up but camwt be reached   then we may have the undesirable situation in which two sites serve as the coordinator when the network is partitioned  the two coordinators  each for its own partition  may initiate conflicting actions for example  if the coordinators are responsible for implementing mutual exclusion  we may have a situation in which two processes are executing simultaneously in their critical sections 16.7.3 recovery from failure when a failed link or site is repaired  it must be integrated into the system gracefully and smoothly suppose that a link between a and b has failed wlcen it is repaired  both a and b must be notified we can accomplish this notification by continuously repeating the handshaking procedure described in section 16.7.1 suppose that site b has failed wlcen it recovers  it must notify all other sites that it is up again site b then may have to receive information from the other sites to update its local tables ; for example  it may need routing-table information  a list of sites that are down  or mcdelivered messages and mail if the site has not failed but simply could not be reached  then this information is still required 16.7.4 fault tolerance a distributed system must tolerate a certain level of failure and continue to function normally when faced with various types of failures making a facility fault tolerant starts at the protocol level  as described above  but continues through all aspects of the system we use the term fault tolerance in a broad sense communication faults  machine failures  of type fail-stop where the machine stops before performing an erroneous operation that is visible to other processors   storage-device crashes  and decays of storage media should all be tolerated to some extent a should continue to function  perhaps in a degraded form  when faced with such failures the degradation can be in performance  in functionality  or in both it should be proportional  however  to the failures that caused it a system that grinds to a halt when only one of its components fails is certainly not fault tolerant unfortunately  fault tolerance can be difficult and expensive to implement at the network layer  multiple redundant communication paths and network devices such as switches and routers are needed to avoid a cmnmunication failure a storage failure can cause loss of the operating system  applications  or data storage units can include redundant hardware components that automatically take over from each other in case of failure in addition  raid systems can ensure continued access to the data even in the event of one or more disk failures  section 12.7   a system failure without redundancy can cause an application or an entire facility to stop operation the inost simple system failure involves a system running only stateless applications these applications can be restarted without 16.8 16.8 697 compromising the operation ; so as long as the applications can run on more than one computer  node   operation can continue such a facility is commonly known as a because it is computation-centric in contrast  systems involve running applications that access and modify shared data as a result  data-centric computing facilities are more difficult to make fault tolerant they failure-monitoring software and special infrastructure for instance  such as veritas cluster and sun cluster include two or more computers and a set of shared disks any given application can be stored on the computers or on the shared disk  but the data must be stored on the shared disk the running application 's node has exclusive access to the application 's data on disk the application is monitored by the cluster software  and if it fails it is automatically restarted if it camwt be restarted  or if the entire computer fails  the node 's exclusive access to the application 's data is terminated and is granted to another node in the cluster the application is restarted on that new node the application loses whatever state information was in the failed system 's memory but can continue based on whatever state it last wrote to the shared disk from a user 's point of view  a service was interrupted and then restarted  possibly with some data missing specific applications may improve on this functionality by implementing lock management along with clustering with lock management  section 18.4.1   the application can run on multiple nodes and can use the same data on shared disks concurrently clustered databases frequently implement this functionality if anode fails  transactions can continue on other nodes  and users notice no interruption of service  as long as the client is able to automatically locate the other nodes in the cluster any noncommitted transactions on the failed node are lost  but again  client applications can be designed to retry noncommitted transactions if they detect a failure of their database node making the multiplicity of processors and storage devices to the users has been a key challenge to many designers ideally  a distributed system should look to its users like a conventional  centralized system the user interface of a transparent distributed system should not distinguish between local and remote resources that is  users should be able to access remote resources as though these resources were local  and the distributed system should be responsible for locating the resources and for arranging for the appropriate interaction another aspect of transparency is user mobility it would be convenient to allow users to log into any machine in the system rather than forcing them to use a specific machine a transparent distributed system facilitates user mobility by bringiicg over the user 's environment  for example  home directory  to wherever he logs in both the andrew file system from cmu and project athena from mit provide this functionality on a large scale ; nfs can provide it on a smaller scale still another issue is l ;  -the capability of a system to adapt to increased service load systems have bounded resources and can become completely saturated under increased load for example  with respect to a file 698 chapter 16 system  saturation occurs either when a server 's cpu runs at a high utilization rate or when disks are almost full scalability is a relative property  but it can be measured accurately a scalable system reacts more gracefully to increased load than does a nonscalable one first  its performance degrades more moderately ; and second  its resources reach a saturated state later even perfect design can not accommodate an ever-growing load adding new resources might solve the problem  but it might generate additional indirect load on other resources  for example  adding machines to a distributed system can clog the network and increase service loads   even worse  expanding the system can call for expensive design modifications a scalable system should have the potential to grow without these problems in a distributed system  the ability to scale up gracefully is of special importance  since expanding the network by adding new machines or interconnecting two networks is commonplace in short  a scalable design should withstand high service load  accommodate growth of the user community  and enable simple integration of added resources scalability is related to fault tolerance  discussed earlier a heavily loaded component can become paralyzed and behave like a faulty component also  shifting the load from a faulty component to that component 's backup can saturate the latter generally  having spare resources is essential for ensuring reliability as well as for handling peak loads gracefully an inherent advantage of a distributed system is a potential for fault tolerance and scalability because of the multiplicity of resources however  inappropriate design can obscure this potential fault-tolerance and scalability considerations call for a design demonstrating distribution of control and data very large-scale distributed systems  to a great extent  are still only theoretical no magic guidelines ensure the scalability of a system it is easier to point out why current designs are not scalable we next discuss several designs that pose problems and propose possible solutions  all in the context of scalability one principle for designing very large-scale systems is that the service demand from any component of the system should be bounded by a constant that is independent of the number of nodes in the system any service mechanism whose load demand is proportional to the size of the system is destined to become clogged once the system grows beyond a certain size adding more resources will not alleviate such a problem the capacity of this mechanism simply limits the growth of the system another principle concerns centralization central control schemes and central resources should not be used to build scalable  and fault-tolerant  systems examples of centralized entities are central authentication servers  central naming servers  and central file servers centralization is a form of functional asyrrunetry among machines constituting the system the ideal alternative is a functionally symmetric configuration ; that is  all the component machines have an equal role in the operation of the system  and hence each machine has some degree of autonomy practically  it is virtually impossible to comply with such a principle for instance  incorporating diskless machines violates functional symmetry  since the workstations depend on a central disk however  autonomy and symmetry are important goals to which we should aspire deciding on the process structure of the server is a major problem in the design of any service servers are supposed to operate efficiently in peak 16.9 16.9 699 periods  when hundreds of active clients need to be served simultaneously a single-process server is certainly not a good choice  since whenever a request necessitates disk i/0  the whole service will be blocked assigning a process for each client is a better choice ; however  the expense of frequent context switches between the processes must be considered a related problem occurs because all the server processes need to share information one of the best solutions for the server architecture is the use of lightweight processes  or threads  which we discuss in chapter 4 we can think of a group of lightweight processes as multiple threads of control associated with some shared resources usually  a lightweight process is not bound to a particular client instead  it serves single requests of different clients scheduling of threads can be preemptive or nonpreemptive if threads are allowed to run to completion  nonpreemptive   then their shared data do not need to be protected explicitly otherwise  some explicit locking mechanism must be used clearly  some form of lightweight-process scheme is essential if servers are to be scalable we now return to the name-resolution issue raised in section 16.5.1 and examine its operation with respect to the tcf /if protocol stack on the internet we consider the processing needed to transfer a packet between hosts on different ethernet networks in a tcf /if network  every host has a name and an associated if address  or host-id   both of these strings must be unique ; and so that the name space can be managed  they are segmented the name is hierarchical  as explained in section 16.5.1   describing the host name and then the organization with which the host is associated the host-id is split into a network number and a host number the proportion of the split varies  depending on the size of the network once the internet adrninistrators assign a network number  the site with that number is free to assign host-ids the sending system checks its routing tables to locate a router to send the frame on its way the routers use the network part of the host-id to transfer the packet from its source network to the destination network the destination system then receives the packet the packet may be a complete message  or it may just be a component of a message  with more packets needed before the message can be reassembled and passed to the tcf /udf layer for transmission to the destination process now we know how a packet moves from its source network to its destination within a network  how does a packet move from sender  host or router  to receiver ethernet device has a unique byte number  called the assigned to it for addressing two devices on a lan communicate with each other only with this number if a system needs to send data to another system  the networking software generates an containing the if address of the destination system this packet is to all other systems on that ethernet network a broadcast uses a special network address  usually  the maximum address  to signal that all hosts should receive and process the packet the 700 chapter 16 broadcast is not re-sent by gateways  so only systems on the local network receive it only the system whose ip address matches the ip address of the arp request responds and sends back its mac address to the system that initiated the query for efficiency  the host caches the ip-mac address pair in an internal table the cache entries are so that an entry is eventually removed from the cache if an access to that system is not required within a given time in this way  hosts that are removed from a network are eventually forgotten for added performance  arp entries for heavily used hosts may be hardwired in the arp cache once an ethernet device has announced its host-id and address  communication can begin a process may specify the name of a host with which to communicate networking software takes that name and determines the ip address of the target  using a dns lookup the message is passed from the application laye1 ~ through the software layers  and to the hardware layer at the hardware layer  the packet  or packets  has the ethernet address at its start ; a trailer indicates the end of the packet and contains a for detection of packet damage  figure 16.10   the packet is placed on the network by the ethernet device the data section of the packet may contain some or all of the data of the original message  but it may also contain some of the upper-level headers that compose the message in other words  all parts of the original message must be sent from source to destination  and all headers above the 802.3layer  data-link layer  are included as data in the ethernet packets if the destination is on the same local network as the source  the system can look in its arp cache  find the ethernet address of the host  and place the packet on the wire the destination ethernet device then sees its address in the packet and reads in the packet passing it up the protocol stack if the destination system is on a network different from that of the source  the source system finds an appropriate router on its network and sends the packet there routers then pass the packet along the wan 1-mtil it reaches its bytes 7 2 or 6 2 or 6 2 0-1500 0-46 4 pt.e ~ ~ n1bh  l ~ s.tartfc1ft  r = ccll et  1 each byte pattern 1010101 o data pattern 10101011 ethernet address or broadcast ethernet address length in bytes message data message must be 63 bytes long for error detection figure i 6.10 an ethernet packet 16.10 701 destination network the router that connects the destination network checks its arp cache  finds the ethernet number of the destination  and sends the packet to that host through all of these transfers  the data-link-layer header may change as the ethernet address of the next router in the chain is used  but the other headers of the packet remain the same until the packet is received and processed by the protocol stack and finally passed to the receiving process by the kernel a distributed system is a collection of processors that do not share memory or a clock instead  each processor has its own local memory  and the processors communicate with one another through various communication lines  such as high-speed buses and telephone lines the processors in a distributed system vary in size and function they may include small microprocessors  workstations  minicomputers  and large general-purpose computer systems the processors in the system are connected through a communication network  which can be configured in a number of ways the network may be fully or partially connected it may be a tree  a star  a ring  or a multiaccess bus the communication-network design must include routing and com1ection strategies  and it must solve the problems of contention and security a distributed system provides the user with access to the resources the system provides access to a shared resource can be provided by data migration  computation migration  or process migration protocol stacks  as specified by network layering models  massage the message  adding information to it to ensure that it reaches its destination a naming system  such as dns  must be used to translate from a host name to a network address  and another protocol  such as arp  may be needed to translate the network number to a network device address  an ethernet address  for instance   if systems are located on separate networks  routers are needed to pass packets from source network to destination network a distributed system may suffer from various types of hardware failure for a distributed system to be fault tolerant  it must detect hardware failures and reconfigure the system when the failure is repaired  the system must be reconfigured again 16.1 what are the advantages of using dedicated hardware devices for routers and gateways what are the disadvantages of using these devices compared with using general-purpose computers 16.2 why would it be a bad idea for gateways to pass broadcast packets between networks what would be the advantages of doing so 16.3 consider a network layer that senses collisions and retransmits immediately on detection of a collision what problems could arise with this strategy how could they be rectified 702 chapter 16 16.4 even though the iso model of networking specifies seven layers of functionality  most computer systems use fewer layers to implement a network why do they use fewer layers what problems could the use of fewer layers cause 16.5 the lower layers of the iso network model provide datagram service  with no delivery guarantees for messages a transport-layer protocol such as tcp is used to provide reliability discuss the advantages and disadvantages of supporting reliable message delivery at the lowest possible layer 16.6 what are the advantages and the disadvantages of making the computer network transparent to the user 16.7 under what circumstances is a token-passing network more effective than an ethernet network 16.8 process migration within a heterogeneous network is usually impossible  given the differences in architectures and operating systems describe a method for process migration across different architectures running  a the same operating system b different operating systems 16.9 contrast the various network topologies in terms of the following attributes  a reliability b available bandwidth for concurrent communications c installation cost d load balance in routing responsibilities 16.10 how does using a dynamic routing strategy affect application behavior for what type of applications is it beneficial to use virtual routing instead of dynamic routing 16.11 the original http protocol used tcp /ip as the underlying network protocol for each page  graphic  or applet  a separate tcp session was constructed  used  and torn down because of the overhead of building and destroying tcp lip connections  performance problems resulted from this implementation method would using udp rather than tcp be a good alternative what other changes could you make to improve http performance 16.12 what are the advantages and disadvantages of using circuit switching for what kinds of applications is circuit switching a viable strategy 16.13 in what ways is using a name server better than using static host tables what problems or complications are associated with name servers what methods could you use to decrease the amount of traffic name servers generate to satisfy translation requests 703 16.14 of what use is an address-resolution protocol why is it better to use such a protocol than to make each host read each packet to determine that packet 's destination does a token-passing network need such a protocol explain your answer 16.15 what is the difference between computation migration and process migration which is easier to implement  and why 16.16 run the program shown in figure 16.5 and determine the ip addresses of the following host names  www.wiley.com www.cs.yale.edu www.apple.com www.westminstercollege.edu www.ietf.org 16.17 to build a robust distributed system  you must know what kinds of failures can occur a list three possible types of failure in a distributed system b specify which of the entries in your list also are applicable to a centralized system 16.18 explain why doubling the speed of the systems on an ethernet segment may result in decreased network performance what changes could help solve this problem 16.19 name servers are organized in a hierarchical manner what is the purpose of using a hierarchical organization 16.20 consider a distributed system with two sites  a and b consider whether site a can distinguish among the following  a b goes down b the link between a and b goes down c b is extremely overloaded  and its response time is 100 times longer than normal what implications does your answer have for recovery in distributed systems tanenbaum  2003   stallings  2000a   and kurose and ross  2005  provide general overviews of computer networks williams  2001  covers computer networking from a computer-architecture viewpoint the internet and its protocols are described in comer  1999  and comer  2000   coverage of tcp /ip can be found in stevens  1994  and stevens  1995   704 chapter 16 unix network programming is described thoroughly in stevens  1997  and stevens  1998   discussions concerning distributed operating-system structures have been offered by coulouris et al  2001  and tanenbaum and van steen  2002   load balancing and load sharing are discussed by i-iarchol-balter and downey  1997  and vee and i-isu  2000   i-iarish and owens  1999  describes load-balancing dns servers process migration is discussed by jul et al  1988   douglis and ousterhout  1991   han and ghosh  1998   and milojicic et al  2000   issues relating to a distributed virtual machine for distributed systems are examined in sirer et al  1999   17.1 in the previous chapter  we discussed network construction and the low-level protocols needed to transfer between systems now we examine one use of this infrastructure a is a distributed implementation of the classical time-sharing of a file system  where multiple users share files and storage resources  chapter 11   the purpose of a dfs is to support the same kind of sharing when the files are physically dispersed among the sites of a distributed system in this chapter  we describe how a dfs can be designed and implemented first  we discuss common concepts on which dfss are based then  we illustrate our concepts by examining one influential dfs-the andrew file system  afs   to explain the naming mechanism that provides location transparency and independence to describe the various methods for accessing distributed files to contrast stateful and stateless distributed file servers to show how replication of files on different machines in a distributed file system is a useful redundancy for improving availability to introduce the andrew file system  afs  as an example of a distributed file system as we noted in the preceding chapter  a distributed system is a collection of loosely coupled computers interconnected by a communication network these computers can share physically dispersed files by using a distributed file system  dfs   in this chapter  we use the term dfs to mean distributed file systems in general  not the commercial transarc dfs product ; we refer to the latter as transarc dfs also  nfs refers to nfs version 3  unless otherwise noted 705 706 chapter 17 to explain the structure of a dfs  we need to define the terms service  server  and client a is a software entity running on one or more machines and providing a particular type of function to clients a is the service software running on a single machine a is a process that can invoke a service using a set of operations that form its sometimes a lower-level interface is defined for the actual cross-machine interaction ; it is the using this terminology  we say that a file system provides file services to clients a client interface for a file service is formed by a set of primitive file operations  such as create a file  delete a file  read from a file  and write to a file the primary hardware concponent that a file server controls is a set of local secondary-storage devices  usually  magnetic disks  on which files are stored and from which they are retrieved according to the clients ' requests a dfs is a file system whose clients  servers  and storage devices are dispersed among the machines of a distributed system accordingly  service activity has to be carried out across the network instead of a single centralized data repository  the system frequently has multiple and independent storage devices as you will see  the concrete configuration and implementation of a dfs may vary from system to system in some configurations  servers run on dedicated machines ; in others  a machine can be both a server and a client a dfs can be implemented as part of a distributed operating system or  alternatively  by a software layer whose task is to manage the communication between conventional operating systems and file systems the distinctive features of a dfs are the multiplicity and autonomy of clients and servers in the system ideally  a dfs should appear to its clients to be a conventional  centralized file system the multiplicity and dispersion of its servers and storage devices should be made invisible that is  the client interface of a dfs should not distinguish between local and remote files it is up to the dfs to locate the files and to arrange for the transport of the data a dfs facilitates user mobility by bringing a user 's environment  that is  home directory  to wherever the user logs in the most important performance measure of a dfs is the amount of time needed to satisfy service requests in conventional systems  this time consists of disk-access time and a small amount of cpu-processing time in a dfs  however  a remote access has the additional overhead attributed to the distributed structure this overhead includes the time to deliver the request to a server  as well as the time to get the response across the network back to the client for each direction  in addition to the transfer of the information  there is the cpu overhead of running the communication protocol software the performance of a dfs can be viewed as another dimension of the dfs 's transparency that is  the performance of an ideal dfs would be comparable to that of a conventional file system the fact that a dfs manages a set of dispersed storage devices is the dfs ' s key distinguishing feature the overall storage space managed by a dfs is composed of different and remotely located smaller storage spaces usually  these constituent storage spaces correspond to sets of files a cmnpm1.c  nt is the smallest set of files that can be stored on a single machine  independently from other units all files belonging to the same component unit must reside in the same location 17.2 17.2 707 is a mapping between logical and physical objects for instance  users deal with logical data objects represented by file nances  whereas the system manipulates physical blocks of data stored on disk tracks usually  a user refers to a file by a textual name the latter is mapped to a lower-level numerical identifier that in turn is mapped to disk blocks this multilevel mapping provides users with an abstraction of a file that hides the details of how and where on the disk the file is stored in a transparent dfs  a new dimension is added to the abstraction  that of hiding where in the network the file is located in a conventional file system  the range of the naming mapping is an address within a disk in a dfs  this range is expanded to include the specific machine on whose disk the file is stored going one step further with the concept of treating files as abstractions leads to the possibility of given a file name  the mapping returns a set of the locations of this file 's replicas in this abstraction  both the existence of multiple copies and their locations are hidden 17.2.1 naming structures we need to differentiate two related notions regarding name mappings in a dfs   the name of a file does not reveal any hint of the file 's physical storage location 'j ' ~ ' '''  ' ' ' ' the name of a file does not need to be changed when the file 's physical storage location changes both definitions relate to the level of naming discussed previously  since files have different names at different levels  that is  user-level textual names and system-level numerical identifiers   a location-independent naming scheme is a dynamic mapping  since it can map the same file name to different locations at two different times therefore  location independence is a stronger property than is location transparency in practice  most of the current dfss provide a static  location-transparent mapping for user-level names these systems  however  do not support that is  changing the location of a file automatically is impossible hence  the notion of location independence is irrelevant for these systems files are associated permanently with a specific set of disk blocks files and disks can be moved between machines manually  but file migration implies an automatic  operating-system-initiated action only afs and a few experimental file systems support location independence and file mobility afs supports file mobility mainly for administrative purposes a protocol provides migration of afs component units to satisfy high-level user requests  without changing either the user-level names or the low-level names of the corresponding files a few aspects can further differentiate location independence and static location transparency  divorce of data from location  as exhibited by location independence  provides a better abstraction for files a file name should denote the file 's 708 chapter 17 most significant attributes  which are its contents ratber than its location location-independent files can be viewed as logical data containers that are not attached to a specific storage location if only static location transparency is supported  the file name still denotes a specific  although hidden  set of physical disk blocks static location transparency provides users with a convenient way to share data users can share remote files by simply naming the files in a locationtransparent manner  as though the files were local nevertheless  sharing the storage space is cumbersome  because logical names are still statically attached to physical storage devices location independence promotes sharing the storage space itself  as well as the data objects when files can be mobilized  the overall  system-wide storage space looks like a single virtual resource a possible benefit of such a view is the ability to balance the utilization of disks across the system location independence separates the naming hierarchy from the storagedevices hierarchy and from the intercomputer structure by contrast  if static location transparency is used  although names are transparent   we can easily expose the correspondence between component units and machines the machines are configured in a pattern similar to the naming structure this configuration may restrict the architecture of the system um1.ecessarily and conflict with other considerations a server in charge of a root directory is an example of a structure that is dictated by the naming hierarchy and contradicts decentralization guidelines once the separation of name and location has been completed  clients can access files residing on remote server systems in fact  these clients may be and rely on servers to provide all files  including the operatingsystem kernel special protocols are needed for the boot sequence  however consider the problem of getting the kernel to a diskless workstation the diskless workstation has no kernel  so it cam1.ot use the dfs code to retrieve the kernel instead  a special boot protocol  stored in read-only memory  rom  on the client  is invoked it enables networking and retrieves only one special file  the kernel or boot code  from a fixed location once the kernel is copied over the network and loaded  its dfs makes all the other operating-system files available the advantages of diskless clients are many  including lower cost  because the client machines require no disks  and greater convenience  when an operating-system upgrade occurs  only the server needs to be modified   the disadvantages are the added complexity of the boot protocols and the performance loss resulting from the use of a network rather than a local disk the current trend is for clients to use both local disks and remote file servers operating systems and networking software are stored locally ; file systems containing user data-and possibly applications-are stored on remote file systems some client systems may store commonly used applications  such as word processors and web browsers  on the local file system as well other  less commonly used applications may be from the remote file server to the client on demand the main reason for providing clients with local file systems rather than pure diskless systems is that disk drives are rapidly increasing in capacity and decreasing in cost  with new generations appearing every year or so the same can not be said for networks  which evolve every few years 17.2 709 overall  systems are growing more quickly than are networks  so extra work is needed to limit network access to improve system throughput 17.2.2 naming schemes there are three main approaches to naming schemes in a dfs in the simplest approach  a file is identified by some combination of its host name and local name  which guarantees a unique system-wide name in ibis  for instance  a file is identified uniquely by the name host  local-name  where local-name is a unix-like path this naming scheme is neither location transparent nor location independent nevertheless  the same file operations can be used for both local and remote files the dfs is structured as a collection of isolated component units  each of which is an entire conventional file system in this first approach  component 1-mits remain isolated  although means are provided to refer to a remote file we do not consider this scheme any further in this text the second approach was popularized by sun 's network file system  nfs nfs is the file-system component of onc +  a networking package supported by many unix vendors nfs provides a means to attach remote directories to local directories  thus giving the appearance of a coherent directory tree early nfs versions allowed only previously mmmted remote directories to be accessed transparently with the advent of the feature  mounts are done on demand  based on a table of mount points and file-structure names components are integrated to support transparent sharing  although this integration is limited and is not uniform  because each machine may attach different remote directories to its tree the resulting structure is versatile we can achieve total integration of the component file systems by using the third approach here  a single global name structure spans all the files in the system ideally  the composed file-system structure is the same as the structure of a conventional file system in practice  however  the many special files  for example  unix device files and machine-specific binary directories  make this goal difficult to attain to evaluate naming structures  we look at their the most complex and most difficult-to-maintain structure is the nfs structure because any rem.ote directory can be attached anywhere onto the local directory tree  the resulting hierarchy can be highly m1structured if a server becomes unavailable  some arbitrary set of directories on different machines becomes unavailable in addition  a separate accreditation mechanism controls which machine is allowed to attach which directory to its tree thus  a user might be able to access a remote directory tree on one client but be denied access on another client 17.2.3 implementation techniques implementation of transparent naming requires a provision for the mapping of a file naine to the associated location to keep this mapping manageable  we must aggregate sets of files into component units and provide the mapping on a component-unit basis rather than on a single-file basis this aggregation serves administrative purposes as well unix-like systems use the hierarchical directory tree to provide name-to-location mapping and to aggregate files recursively into directories 710 chapter 17 17.3 to enhance the availability of the crucial mapping information  we can use replication  local caching  or both as we noted  location independence means that the mapping changes over time ; hence  replicating the mapping makes a simple yet consistent update of this information impossible a teclllcique to overcome this obstacle is to introduce low-level me textual file names are mapped to lower-level file identifiers that indicate to which component unit the file belongs these identifiers are still location independent they can be replicated and cached freely without being invalidated by migration of component units the inevitable price is the need for a second level of mapping  which maps component units to locations and needs a simple yet consistent update mechanism implementing unix-like directory trees using these low-levet location-independent identifiers makes the whole hierarchy invariant under component-unit migration the only aspect that does change is the component-unit location mapping a common way to implement low-level identifiers is to use structured names these names are bit strings that usually have two parts the first part identifies the component unit to which the file belongs ; the second part identifies the particular file within the unit variants with more parts are possible the invariant of structured names  however  is that individual parts of the name are unique at all times only within the context of the rest of the parts we can obtain uniqueness at all times by taking care not to reuse a name that is still in use  by adding sufficiently more bits  this method is used in afs   or by using a timestamp as one part of the name  as done in apollo domain   another way to view this process is that we are taking a location-transparent system  such as ibis  and adding another level of abstraction to produce a location-independent naming scheme aggregating files into component units and using lower-level locationindependent file identifiers are techniques exemplified in afs consider a user who requests access to a remote file the server storing the file has been located by the nanling scheme  and now the actual data transfer must take place one way to achieve this transfer is through a whereby requests for accesses are delivered to the server  the server machine performs the accesses  and their results are forwarded back to the user one of the most common ways of implementing remote service is the remote procedure call  rpc  paradigm  which we discussed in chapter 3 a direct analogy exists between disk-access methods in conventional file systems and the remote-service method in a dfs  using the remote-service method is analogous to performing a disk access for each access request to ensure reasonable performance of a remote-service mechanism  we can use a form of caching in conventional file systems  the rationale for caching is to reduce disk i/0  thereby increasing performance   whereas in dfss  the goal is to reduce both network traffic and disk i/0 in the following discussion  we describe the implementation of caching in a dfs and contrast it with the basic remote-service paradigm 17.3 711 17.3.1 basic caching scheme the concept of caching is simple if the data needed to satisfy the access request are not already cached  then a copy of those data is brought from the server to the client system accesses are performed on the cached copy the idea is to retain recently accessed disk blocks in the cache  so that repeated accesses to the same information can be handled locally  without additional network traffic a replacement policy  for example  the least-recently-used algorithm  keeps the cache size bounded no direct correspondence exists between accesses and traffic to the server files are still identified with one master copy residing at the server machine  but copies  or parts  of the file are scattered in different caches when a cached copy is modified  the changes need to be reflected on the master copy to preserve the relevant consistency semantics the problem of keeping the cached copies consistent with the master file is the which we discuss in section 17.3.4 dfs caching could just as easily be called  it acts sincilarly to demand-paged virtual memory  except that the backing store usually is not a local disk but rather a remote server nfs allows the swap space to be mounted remotely  so it actually can implement virtual memory over a network  notwithstanding the resulting performance penalty the granularity of the cached data in a dfs can vary from blocks of a file to an entire file usually  more data are cached than are needed to satisfy a single access  so that many accesses can be served by the cached data this procedure is much like diskread-ahead  section 11.6.2   afs caches files in large chunks  64 kb   the other systems discussed in this chapter support caching of individual blocks driven by client demand increasing the caching unit increases the hit ratio  but it also increases the miss penalty  because each miss requires more data to be transferred it increases the potential for consistency problems as well selecting the unit of caching involves considering parameters such as the network transfer unit and the rpc protocol service unit  if an rpc protocol is used   the network transfer unit  for ethernet  a packet  is about 1.5 kb  so larger units of cached data need to be disassembled for delivery and reassembled on reception block size and total cache size are obviously of importance for blockcaching schemes in unix-like systems  common block sizes are 4 kb and 8 kb for large caches  over 1mb   large block sizes  over 8 kb  are beneficial for smaller caches  large block sizes are less beneficial because they result in fewer blocks in the cache and a lower hit ratio 17.3.2 cache location where should the cached data be stored-on disk or in main memory disk caches have one clear advantage over main-memory caches  they are reliable modifications to cached data are lost in a crash if the cache is kept in volatile memory moreove1 ~ if the cached data are kept on disk  they are still there during recovery  and there is no need to fetch them again main-memory caches have several advantages of their own  however  main-memory caches permit workstations to be diskless data can be accessed more quickly from a cache in main memory than from one on a disk 712 chapter 17 technology is moving toward larger and less expensive memory the resulting performance speedup is predicted to outweigh the advantages of disk caches the server caches  used to speed up disk i/0  will be in main memory regardless of where user caches are located ; if we use main-memory caches on the user machine  too  we can build a single caching nl.echanism for use by both servers and users many remote-access implementations can be thought of as hybrids of caching and remote service in nfs  for instance  the implementation is based on remote service but is augmented with client and server-side memory caching for performance similarly sprite 's implementation is based on caching ; but under certain circumstances  a remote-service method is adopted thus  to evaluate the two methods  we must evaluate the degree to which either method is emphasized the nfs protocol and most implementations do not provide disk caching recent solaris implementations ofnfs  solaris 2.6 and beyond  include a clientside disk-caching option  the  file system once the nfs client reads blocks of a file from the serve1 ~ it caches them in memory as well as on disk if the memory copy is flushed  or even if the system reboots  the disk cache is referenced if a needed block is neither in memory nor in the cachefs disk cache  an rpc is sent to the server to retrieve the block  and the block is written into the disk cache as well as stored in the memory cache for client use 17.3.3 cache-update policy the policy used to write modified data blocks back to the server 's master copy has a critical effect on the performance and reliability the simplest policy is to write data to disk as soon as they are placed in any cache the advantage of a is reliability  little information is lost when a client system crashes however  this policy requires each write access to wait until the information is sent to the server  so it causes poor write performance caching with write-through is equivalent to using remote service for write accesses and exploiting caching for read accesses an alternative is the also known as where we delay updates to the master copy modifications are written to the cache and then are written through to the server at a later time this policy has two advantages over write-through first  because writes are made to the cache  write accesses complete much more quickly second  data may be overwritten before they are written back  in which case only the last update needs to be written at all unfortunately  delayed-write schemes introduce reliability problems  since unwritten data are lost whenever a user machine crashes variations of the delayed-write policy differ in when modified data blocks are flushed to the server one alternative is to flush a block when it is about to be ejected from the client 's cache this option can result in good performance  but some blocks can reside in the client 's cache a long time before they are written back to the server a compromise between this alternative and the write-through policy is to scan the cache at regular intervals and to flush blocks that have been modified since the most recent scan  just as unix scans 17.3 713 nfs server network workstation figure 17.1 cachefs and its use of caching its local cache sprite uses this policy with a 30-second interval nfs uses the policy for file data  but once a write is issued to the server durilcg a cache flush  the write must reach the server 's disk before it is considered complete nfs treats meta data  directory data and file-attribute data  differently any metadata changes are issued synchronously to the server thus  file-structure loss and directory-structure corruption are avoided when a client or the server crashes for nfs with cachefs  writes are also written to the local disk cache area when they are written to the server  to keep all copies consistent thus  nfs with cachefs improves performance over standard nfs on a read request with a cachefs cache hit but decreases performance for read or write requests with a cache miss as with all caches  it is vital to have a high cache hit rate to gain performance figure 17.1 shows how cachefs uses write-through and write-back caching yet another variation on delayed write is to write data back to the server when the file is closed this is used in afs in the case of files that are open for short periods or are modified rarely  this policy does not significantly reduce network traffic in addition  the write-on-close policy requires the closing process to delay while the file is written through  which reduces the performance advantages of delayed writes for files that are open for long periods and are modified frequently  however  the performance advantages of this policy over delayed write with more frequent flushing are apparent 17.3.4 consistency a client machine is faced with the problem of deciding whether a locally cached copy of the data is consistent with the master copy  and hence can be used   if 714 chapter 17 the client machine determines that its cached data are out of date  accesses can no longer be served by those cached data an up-to-date copy of the data needs to be cached there are two approaches to verifying the validity of cached data  client-initiated approach the client initiates a validity check  in which it contacts the server and checks whether the local data are consistent with the master copy the frequency of the validity checking is the crux of this approach and determines the resulting consistency semantics it can range from a check before every access to a check only on first access to a file  on file open  basically   every access coupled with a validity check is delayed  compared with an access served immediately by the cache alternatively  checks can be initiated at fixed time intervals depending on its frequency  the validity check can load both the network and the server server-initiated approach the server records  for each client  the files  or parts of files  that it caches when the server detects a potential inconsistency  it must react a potential for inconsistency occurs when two different clients in conflicting modes cache a file if unix semantics  section 10.5.3  is implemented  we can resolve the potential inconsistency by having the server play an active role the server must be notified whenever a file is opened  and the intended mode  read or write  must be indicated for every open the server can then act when it detects that a file has been opened simultaneously in conflicting modes by disabling caching for that particular file actually  disabling caching results in switching to a remote-service mode of operation 17.3.5 a comparison of caching and remote service essentially  the choice between caching and remote service trades off potentially increased performance with decreased simplicity we evaluate this tradeoff by listing the advantages and disadvantages of the two methods  when caching is used  the local cache can handle a substantial number of the remote accesses efficiently capitalizing on locality in file-access patterns makes caching even more attractive thus  most of the remote accesses will be served as fast as will local ones moreover  servers are contacted only occasionally  rather than for each access consequently  server load and network traffic are reduced  and the potential for scalability is enhanced by contrast  when the remote-service method is used  every remote access is handled across the network the penalty in network traffic  server load  and performance is obvious total network overhead is lower for transmitting big chunks of data  as is done in caching  than for transmitting series of responses to specific requests  as in the remote-service method   furthermore  disk-access routines on the server may be better optimized if it is known that requests will always be for large  contiguous segments of data rather than for random disk blocks the cache-consistency problem is the major drawback of caching when access patterns exhibit infrequent writes  caching is superior however  17.4 17.4 715 when writes are frequent  the mechanisms employed to overcome the consistency problem incur substantial overhead in terms of performance  network traffic  and server load so that caching will confer a benefit  execution should be carried out on machines that have either local disks or large main memories remote access on diskless  small-memory-capacity machines should be done through the remote-service method in caching  since data are transferred en masse between the server and the client  rather than in response to the specific needs of a file operation  the lower-level intermachine interface is different from the upper-level user interface the remote-service paradigm  in contrast  is just an extension of the local file-system interface across the network thus  the intermachine interface mirrors the user interface there are two approaches for storing server-side information when a client accesses remote files  either the server tracks each file being accessed by each client  or it simply provides blocks as they are requested by the client without knowledge of how those blocks are used in the former case  the service provided is stateful ; in the latter case  it is stateless the typical scenario involving a is as follows  a client must perform an open   operation on a file before accessing that file the server fetches information about the file from its disk  stores it in its memory  and gives the client a connection identifier that is unique to the client and the open file  in unix terms  the server fetches the inode and gives the client a file descriptor  which serves as an index to an in-core table of inodes  this identifier is used for subsequent accesses ru1.til the session ends a stateful service is characterized as a connection between the client and the server during a session either on closing the file or through a garbage-collection mechanism  the server must reclaim the main-memory space used by clients that are no longer active the key point regarding fault tolerance in a stateful service approach is that the server keeps main-memory information about its clients afs is a stateful file service a avoids state information by making each request self-contained that is  each request identifies the file and the position in the file  for read and write accesses  in full the server does not need to keep a table of open files in main memory  although it usually does so for efficiency reasons moreover  there is no need to establish and terminate a com1.ection through open   and close   operations they are totally redundant since each file operation stands on its own and is not considered part of a session a client process would open a file  and that open would not result in the sending of a remote message reads and writes would take place as remote messages  or cache lookups   the final close by the client would again result in only a local operation nfs is a stateless file service the advantage of a stateful over a stateless service is increased performance file information is cached in main memory and can be accessed easily via the connection identifier  thereby saving disk accesses in addition  a stateful 716 chapter 17 5 server knows whether a file is open for sequential access and can therefore read ahead the next blocks stateless servers cmmot do so  since they have no knowledge of the purpose of the client 's requests the distinction between stateful and stateless service becomes more evident when we consider the effects of a crash that occurs during a service activity a stateful server loses all its volatile state in a crash ensuring the graceful recovery of such a server involves restoring this state  usually by a recovery protocol based on a dialog with clients less graceful recovery requires that the operations that were underway when the crash occurred be aborted a different problem is caused by client failures the server needs to become aware of such failures so that it can reclaim space allocated to record the state of crashed client processes this phenomenon is sometimes referred to as a stateless computer server avoids these problems  silcce a newly reincarnated server can respond to a self-contained request without any difficulty therefore  the effects of server failures and recovery are almost unnoticeable there is no difference between a slow server and a recovering server from a client 's point of view the client keeps retransmitting its request if it receives no response the penalty for using the robust stateless service is longer request messages and slower processing of requests  since there is no in-core i,_'lformation to speed the processing in addition  stateless service imposes additional constraints on the design of the dfs first  since each request identifies the target file  a uniform  system-wide  low-level naming scheme should be used translating remote to local names for each request would cause even slower processing of the requests second  since clients retransmit requests for file operations  these operations must be idempotent ; that is  each operation must have the same effect and return the same output if executed several times consecutively self-contained read and write accesses are idempotent  as long as they use an absolute byte count to indicate the position withilc the file they access and do not rely on an incremental offset  as is done in unix read   and write   system calls   however  we must be careful when implementing destructive operations  such as deleting a file  to make them idempotent  too in some environments  a stateful service is a necessity if the server employs the server-initiated method for cache validation  it camcot provide stateless service  since it maintains a record of which files are cached by which clients the way unix uses file descriptors and implicit offsets is inherently stateful servers must mailctain tables to map the file descriptors to inodes and must store the current offset within a file this requirement is why nfs  which employs a stateless service  does not use file descriptors and does include an explicit offset in every access replication of files on different machines in a distributed file system is a useful redundancy for improving availability multimachine replication can benefit performance too  selecting a nearby replica to serve an access request results in shorter service time the basic requirement of a replication scheme is that different replicas of the same file reside on failure-independent machines that is  the availability 17.5 717 nfsv4 ourcov,era.geofnfs thus .far has 0p1y considered version 3  orv3  nf  the mostrecentnps standard is version 4  v 4   and it differs fundanrentaljy from pr ~ vious versimi.s jhe most significant ch9nge is that the protocol is now stqteful,meaping tha  tthesetv ~ er maintains the.state ofthe client session from the  time the r ~ j1lote file is 0pt   ned untij itis closed th.t1s  thenfs protocol now provides open   ar ; td c1o $ e   operations ; previous v  ersions of nfs  v \ thich are stateless  .proyide .np such operations furthen   ore  preytous \ cersions specify s ~ parate protocols or j  lounting remote fil ~ systews and for lockii1g remote files  v4 provides ali of these features under l phlgle prqtocol in patticular ; the 1nrnmt protocol was elimin ; 1.ted  allowing 1 \  fs to work with network fitewalls  the nj.ount protocol was a notorimis security hole in nps implem,entations ~         .additionally  v4 has enhan edthe ability of.dients jo cache file data local  y  this feature ; i ~ prov ~ s tne performapc ~ of the disttil  mt  3d file system  a.s plients are able to reso ~ ve more file a9cesses from the loc ~ l c ~ che rgther thanh ; lvingto gpjhroughthe s ~ !  ver '   4 ali   ws diel   tsjo req  c ! estfile jocks from s ~ rvers as we 'll .if the  senr ~ r grqrct ~  the request the client maintains the loci ,tmtil it is released or its lea.st = expires  clier ~ ts ar,e als   permitted to r ~ new ex ~ stil'tg least = s  traditiora11y1 unix ~ ba ~ ~ d  systems .provide advisory jile locking  whereas windows operatirg systen1 ~ use mandat   rylockil1g to allov \ t l ' \  ps towm  wellwithnon-unixsyste1fts  v4t1qw.p.rovides mandatory locking as vvell the new lockinga11d caching mec ~ anisms are based on the concept   f d ~ legahon  whe ~ eby the server delegates responsibilities for a file 's lockand contents to .the client thatrequested tnel   ckcrhat delegated client maintains in cache the .current version of .the file  and  other clients can  ask that deleg21ted client for lock access  a1i.d filt = confentsuntifthe del  egated client reli11quishesth ~ lock andde ~ egation              finally  whereas.preyiousversionb   f npsarebasedon the udj network ptqtocol  \  4 is based on .tcp,whioh allows itto betteraclj \ lstto varying traffic loads on thel  etwork  peleg2lting these responsibilities to cliel ! cts reduces the foado11the s.eryet and.i ~ proves.cache.coherency of one replica is not affected by the availability of the rest of the replicas this obvious requirement implies that replication management is inherently a location-opaque activity provisions for placing a replica on a particular machine must be available it is desirable to hide the details of replication from users mapping a replicated file name to a particular replica is the task of the naming scheme the existence of replicas should be invisible to higher levels at lower levels  however  the replicas must be distinguished from one another by different lower-level names another transparency requirement is providing replication control at higher levels replication control includes determination of the degree of replication and of the placement of replicas under certain circumstances  we may want to expose these details to users locus  for instance  provides users and system administrators with mechanisms to control the replication scheme 718 chapter 17 17.6 the main problem ~ associated with replicas is updating from a user 's point of view  replicas of a file denote the same logical entity  and thus an update to any replica must be reflected on all other replicas more precisely  the relevant consistency sen1antics must be preserved when accesses to replicas are viewed as virtual accesses to the replicas ' logical files if consistency is not of primary incportance  it can be sacrificed for availability and performance in this fundamental tradeoff in the area of fault tolerance  the choice is between preserving consistency at all costs  thereby creating a potential for indefinite blocking  and sacrificing consistency under some  we hope  rare  circumstances for the sake of guaranteed progress locus  for example  employs replication extensively and sacrifices consistency in the case of network partition for the sake of availability of files for read and write accesses ibis uses a variation of the primary-copy approach the domain of the name mapping is a pair primary-replica-identifier  local-replica-identifier  if no local replica exists  a special value is used thus  the mapping is relative to a machine if the local replica is the primary one  the pair contains two identical identifiers ibis supports demand replication  an automatic replication-control policy similar to whole-file caching under demand replication  reading of a nonlocal replica causes it to be cached locally  thereby generating a new nonprimary replica updates are performed only on the primary copy and cause all other replicas to be invalidated through the sending of appropriate messages atomic and serialized invalidation of all nonprimary replicas is not guaranteed hence  a stale replica may be considered valid to satisfy remote write accesses  we migrate the primary copy to the requesting machine andrew is a distributed computing environment designed and implemented at carnegie mellon university the andrew file system  afs  constitutes the underlying information-sharing mechanism among clients of the environment the transarc corporation took over development of afs and then was purchased by ibm ibm has since produced several commercial implementations of afs afs was subsequently chosen as the dfs for an industry coalition ; the result was part of the distributed computing environment  dce  from the osf organization in 2000  ibm 's transarc lab announced that afs would be an open-source product  termed openafs  available under the ibm public license  and transarc dfs was canceled as a commercial product openafs is available under most commercial versions of unix as well as linux and microsoft windows systems many unix vendors  as well as microsoft  support the dce system and its dfs  which is based on afs  and work is ongoing to make dce a cross-platform  universally accepted dfs as afs and transarc dfs are very similar ~ we describe afs throughout this section  unless transarc dfs is named specifically afs seeks to solve many of the problems of the simpler dfss  such as nfs  and is arguably the most feature-rich nonexperimental dfs it features a uniform name space  location-independent file sharing  client-side caching with cache consistency  and secure authentication via kerberos it also includes server-side caching in the form of replicas  with high availability through automatic switchover to a replica if the source server is unavailable one of the 17.6 719 most formidable attributes of afs is scalability  the andrew system is targeted to span over 5,000 workstations between afs and transarc dfs  there are hundreds of implementations worldwide 17.6.1 overview afs distinguishes between client machines  sometimes referred to as workstations  and dedicated server machines servers and clients originally ran only 4.2 bsd unix  but afs has been ported to many operating systems the clients and servers are interconnected by a network of lans or wans clients are presented with a partitioned space of file names  a and a dedicated servers  collectively called vice after the name of the software they run  present the shared name space to the clients as a homogeneous  identical  and location-transparent file hierarchy the local name space is the root file system of a workstation  from which the shared name space descends workstations run the virtue protocol to communicate with vice  and each is required to have a local disk where it stores its local name space servers collectively are responsible for the storage and management of the shared name space the local name space is small  is distinct for each workstation  and contains system programs essential for autonomous operation and better performance also local are temporary files and files that the workstation owner  for privacy reasons  explicitly wants to store locally viewed at a finer granularity  clients and servers are structured in clusters interconnected by a wan each cluster consists of a collection of workstations on a lan and a representative of vice called a and each cluster is com1.ected to the wan by a router the decomposition into clusters is done primarily to address the problem of scale for optimal performance  workstations should use the server on their own cluster most of the time  thereby making cross-cluster file references relatively infrequent the file-system architecture is also based on considerations of scale the basic heuristic is to offload work from the servers to the clients  in light of experience indicating that server cpu speed is the system 's bottleneck following this heuristic  the key mechanism for remote file operations is to cache files in large chunks  64 kb   this feature reduces file-open latency and allows reads and writes to be directed to the cached copy without frequently involving the servers briefly  here are a few additional issues in the design of afs  client mobility clients are able to access any file in the shared name space from any workstation a client may notice some initial performance degradation due to the caching of files when accessil g files a workstation other than the usual one security the vice interface is considered the boundary of trustworthiness  because no client programs are executed on vice machines authentication and secure-transmission functions are provided as part of a connectionbased communication package based on the rpc paradigm after mutual authentication  a vice server and a client communicate via encrypted messages encryption is performed by hardware devices or  more slowly  720 chapter 17 in software information about clients and groups is stored in a protection database replicated at each server protection afs provides for protecting directories and the regular unlxbits for file protection the access list ncay contain information about those users allowed to access a directory  as well as information about those users not allowed to access it thus  it is simple to specify that everyone except  say  jim can access a directory afs supports the access types read  write  lookup  insert  administer  lock  and delete heterogeneity defining a clear interface to vice is a key for integration of diverse workstation hardware and operating systems so that heterogeneity is facilitated  some files in the local /bin directory are symbolic links pointing to machine-specific executable files residing in vice 17.6.2 the shared name space afs 's shared name space is made up of component units called the volumes are unusually small component units typically  they are associated with the files of a single client few volumes reside within a single disk partition  and they may grow  up to a quota  and shrink in size conceptually  volumes are glued together by a mechanism similar to the unix m01mt mechanism however  the granularity difference is significant  since in unix only an entire disk partition  containing a file system  can be mounted volumes are a key administrative unit and play a vital role in identifying and locating an individual file a vice file or directory is identified by a low-level identifier called a fid each afs directory entry maps a path-name component to a fid a fid is 96 bits long and has three equal-length components  a volume number  a vnode number  and a uniquifier the vnode number is used as an index into an array containing the inodes of files in a single volume the allows reuse of vnode numbers  thereby keeping certain data structures compact fids are location transparent ; therefore  file movements from server to server do not invalidate cached directory contents location information is kept on a volume basis in a replicated on each server a client can identify the location of every volume in the system by querying this database the aggregation of files into volumes makes it possible to keep the location database at a manageable size to balance the available disk space and utilization of servers  volumes need to be migrated among disk partitions and servers when a volume is shipped to its new location  its original server is left with temporary forwarding information  so that the location database need not be updated synchronously while the volume is being transferred  the original server can still handle updates  which are shipped later to the new server at some point  the volume is briefly disabled so that the recent modifications can be processed ; then  the new volume becomes available again at the new site the volume-movement operation is atomic ; if either server crashes  the operation is aborted read-only replication at the granularity of an entire volume is supported for system-executable files and for seldom-updated files in the upper levels of the vice name space the volume-location database specifies the server 17.6 721 contammg the only read-write copy of a volume and a list of read-only replication sites 17.6.3 file operations and consistency semantics the fundamental architectural principle in afs is the caching of entire files from servers accordingly  a client workstation interacts with vice servers only during opening and closing of files  and even this interaction is not always necessary reading and writing files do not cause remote interaction  in contrast to the remote-service n lethod   this key distinction has far-reaching ramifications for performance  as well as for semantics of file operations the operating system on each workstation intercepts file-system calls and forwards them to a client-level process on that workstation this process  called venus  caches files from vice when they are opened and stores modified copies of files back on the servers from which they came when they are closed venus may contact vice only when a file is opened or closed ; reading and writing of individual bytes of a file are performed directly on the cached copy and bypass venus as a result  writes at some sites are not visible immediately at other sites caching is further exploited for future opens of the cached file venus assumes that cached entries  files or directories  are valid unless notified otherwise therefore  venus does not need to contact vice on a file open to validate the cached copy the mechanism to support this policy  called callback  dramatically reduces the number of cache-validation requests received by servers it works as follows when a client caches a file or a directory  the server updates its state information to record this caching we say that the client has a callback on that file the server notifies the client before allowing another client to modify the file in such a case  we say that the server removes the callback on the file for the former client a client can use a cached file for open purposes only when the file has a callback if a client closes a file after modifying it  all other clients caching this file lose their callbacks therefore  when these clients open the file later  they have to get the new version from the server readin.g and writing bytes of a file are done directly by the kernel without venus 's intervention on the cached copy venus regains control when the file is closed if the file has been modified locally  it updates the file on the appropriate server thus  the only occasions on which venus contacts vice servers are on opens of files that either are not in the cache or have had their callback revoked and on closes of locally modified files basically  afs implements session semantics the only exceptions are file operations other than the primitive read and write  such as protection changes at the directory level   which are visible everywhere on the network immediately after the operation completes in spite of the callback mechanism  a small amount of cached validation traffic is still present  usually to replace callbacks lost because of machine or network failures when a workstation is rebooted  venus considers all cached files and directories suspect  and it generates a cache-validation request for the first use of each such entry the callback mechanism forces each server to maintain callback information and each client to maintain validity information if the amount of callback 722 chapter 17 information maintained by a server is excessive  the server can break callbacks and reclaim some storage by unilaterally notifying clients and revoking the validity of their cached files if the callback state maintained by venus gets out of sync with the corresponding state maintained by the servers  some inconsistency may result venus also caches contents of directories and syncbolic links  for pathname translation each component in the path name is fetched  and a callback is established for it if it is not already cached or if the client does not have a callback on it venus does lookups on the fetched directories locally  using fids no requests are forwarded from one server to another at the end of a path-name traversal  all the intermediate directories and the target file are in the cache with callbacks on them future open calls to this file will involve no network communication at all  unless a callback is broken on a component of the path name the only exception to the caching policy is a modification to a directory that is made directly on the server responsible for that directory for reasons of integrity the vice interface has well-defined operations for such purposes venus reflects the changes in its cached copy to avoid re-fetching the directory 17.6.4 implementation client processes are interfaced to a unix kernel with the usual set of system calls the kernel is modified slightly to detect references to vice files in the relevant operations and to forward the requests to the client-level venus process at the workstation venus carries out path-name translation component by component  as described above it has a mapping cache that associates volumes to server locations in order to avoid server interrogation for an already known volume location if a volume is not present in this cache  venus contacts any server to which it already has a comcection  requests the location information  and enters that information into the mapping cache unless venus already has a connection to the server  it establishes a new comcection it then uses this connection to fetch the file or directory connection establishment is needed for authentication and security purposes when a target file is found and cached  a copy is created on the local disk venus then returns to the kernel  which opens the cached copy and returns its handle to the client process the unix file system is used as a low-level storage system for both afs servers and clients the client cache is a local directory on the workstation 's disk within this directory are files whose names are placeholders for cache entries both venus and server processes access unix files directly by the latter 's inodes to avoid the expensive path-name-to-inode translation routine  namei   because the internal inode interface is not visible to client-level processes  both venus and server processes are client-level processes   an appropriate set of additional system calls was added dfs uses its own journaling file system to improve performance and reliability over ufs venus manages two separate caches  one for status and the other for data it uses a simple least-recently-used  lru  algorithm to keep each of them bounded in size when a file is flushed from the cache  venus notifies the appropriate server to remove the callback for this file the status cache is kept in virtual memory to allow rapid servicing of stat    file-status-returning  17.7 17.7 723 system calls the data cache is resident on the local disk  but the unix i/0 buffering mechanism does some caching of disk blocks in memory that is transparent to venus a single client-level process on each file server services all file req1.1ests from clients this process uses a lightweight-process package with non-preemptible scheduling to service many client requests concurrently the rpc package is integrated with the lightweight-process package  thereby allowing the file server to concurrently make or service one rpc per lightweight process the rpc package is built on top of a low-level datagram abstraction whole-file transfer is implemented as a side effect of the rpc calls one rpc connection exists per client  but there is no a priori binding of lightweight processes to these connections instead  a pool of lightweight processes services client requests on all connections the use of a single multithreaded server process allows the caching of data structures needed to service requests on the negative side  a crash of a single server process has the disastrous effect of paralyzing this particular server a dfs is a file-service system whose clients  servers  and storage devices are dispersed among the sites of a distributed system accordingly  service activity has to be carried out across the network ; instead of a single centralized data repository  there are multiple independent storage devices ideally  a dfs should look to its clients like a conventional  centralized file system the multiplicity and dispersion of its servers and storage devices should be made transparent that is  the client interface of a dfs should not distinguish between local and remote files it is up to the dfs to locate the files and to arrange for the transport of the data a transparent dfs facilitates client mobility by bringing the client 's environment to the site where the client logs in there are several approaches to naming schemes in a dfs in the simplest approach  files are named by some combination of their host name and local name  which guarantees a unique system-wide name another approach  popularized by nfs  provides a means to attach remote directories to local directories  thus giving the appearance of a coherent directory tree requests to access a remote file are usually handled by two complementary methods with remote service  requests for accesses are delivered to the server the server machine performs the accesses  and their results are forwarded back to the client with caching  if the data needed to satisfy the access request are not already cached  then a copy of the data is brought from the server to the client accesses are performed on the cached copy the idea is to retain recently accessed disk blocks in the cache  so that repeated accesses to the same information can be handled locally  without additional network traffic a replacement policy is used to keep the cache size bounded the problem of keeping the cached copies consistent with the master file is the cache-consistency problem there are two approaches to server-side information either the server tracks each file the client accesses  or it simply provides blocks as the client requests them without knowledge of their use these approaches are the stateful versus stateless service paradigms 724 chapter 17 replication of files on different machines is a useful redundancy for improving availability multimachine replication can benefit performance  too  since selecting a nearby replica to serve an access request results in shorter service time afs is a feature-rich dfs characterized by location independence and location transparency it also imposes significant consistency semantics caching and replication are used to improve performance 17.1 discuss whether afs and nfs provide the following   a  location transparency and  b  location independence 17.2 discuss whether clients in the following systems can obtain inconsistent or stale data from the file server and  if so  under what scenarios this could occur a afs b sprite c nfs 17.3 consider afs  which is a stateful distributed file system what actions need to be performed to recover from a server crash in order to preserve the consistency guaranteed by the system 17.4 discuss the advantages and disadvantages of path-name translation in which the client ships the entire path to the server requesting a translation for the entire path name of the file 17.5 under what circumstances would a client prefer a locationtransparent dfs under what circumstances would she prefer a location-independent dfs discuss the reasons for these preferences 17.6 v'lhich of the example dfss discussed in this chapter would handle a large  multiclient database application most efficiently explain your answer 17.7 what are the benefits of mapping objects into virtual memory  as apollo domain does what are the drawbacks 17.8 compare and contrast the teclmiques of caching disk blocks locally  on a client system  and remotely  on a server 17.9 what aspects of a distributed system would you select for a system running on a totally reliable network 725 consistency and recovery control for replicated files are examined by davcev and burkhard  1985   management of replicated files in a unix environncent is covered by brereton  1986  and purdin et al  1987   wah  1984  discusses the issue of file placement on distributed computer systems a detailed survey of mainly centralized file servers appears in svobodova  1984   sun 's network file system  nfs  is described by callaghan  2000  and sandberg et al  1985   the afs system is discussed by morris et al  1986   howard et al  1988   and satyanarayanan  1990   information about openafs is available from many different and interesting dfss are not covered in detail in this text  including unix united  sprite  and locus unix united is described by brownbridge et al  1982   the locus system is discussed by popek and walker  1985   the sprite system is described by ousterhout et al  1988  and nelson et al  1988   distributed file systems for mobile storage devices are discussed in kistler and satyanarayanan  1992  and sobti et al  2004   considerable research has also been performed on cluster-based distributed file systems  anderson et al  1995   lee and thekkath  1996   thekkath et al  1997   and anderson et al  2000    distributed storage systems for large-scale  wide-area settings are presented in dabek et al  2001  and kubiatowicz et al  2000   18.1 in chapter 6  we described various mechanisms that allow processes to synchronize their actions we also discussed a number of schemes to ensure the atomicity of a transaction that executes either in isolation or concurrently with other transactions in chapter 7  we described various methods that an operating system can use to deal with the deadlock problem in this chapter  we examine how centralized synchronization mechanisms can be extended to a distributed environment we also discuss methods for handling deadlocks in a distributed system to describe various methods for achieving mutual exclusion in a distributed system to explain how atomic transactions can be implemented in a distributed system to show how some of the concurrency-control schemes discussed in chapter 6 can be modified for use in a distributed environment to present schemes for handling deadlock prevention  deadlock avoidance  and deadlock detection in a distributed system in a centralized system  we can always determine the order in which two events occurred  since the system has a single common memory and clock many applications may require us to determine order for example  in a resource-allocation scheme  we specify that a resource can be used only aft-er the resource has been granted a distributed system  however  has no common memory and no common clock therefore  it is sometimes impossible to say which of two events occurred first the happened-before relation  discussed next  is only a partial ordering of the events in distributed systems since 727 728 chapter 18 the ability to define a total ordering is crucial in many applications  we present a distributed algorithm for extending the happened-before relation to a consistent total ordering of all the events in the system 18.1.1 the happened-before relation since we are considering only sequential processes  all events executed in a single process are totally ordered also  by the law of causality  a message can be received only after it has been sent therefore  we can define the happenedbefore relation  denoted by  +  on a set of events as follows  assuming that sending and receiving a message constitutes an event   1 if a and bare events in the same process  and a was executed before b  then a  + b if a is the event of sending a message by one process and b is the event of receiving that message by another process  then a + b 3 if a + band b + c  then a + c since an event can not happen before itself  the  + relation is an irreflexive partial ordering if two events  a and b  are not related by the  + relation  that is  a did not happen before b  and b did not happen before a   then we say that these two events were executed in this case  neither event can causally affect the other if  however  a  + b  it is possible for event a to affect event b causally a space-time diagram  such as that in figure 18.1  can best illustrate the definitions of concurrency and happened-before the horizontal direction represents space  that is  different processes   and the vertical direction represents time the labeled vertical lines denote processes  or processors   the labeled dots denote events a wavy line denotes a message sent from one process to another events are concurrent if and only if no path exists between them for example  these are some of the events related by the happened-before relation il figure 18.1  p1  + q2 ro  + q4 q3 + r4 p1  + q4  sil ce p1  + q2 and q2  + q4  these are some of the concurrent events in the system  qo and p2 ro and q3 ro and p3 q3 and p3 we can not know which of two concurrent events  such as qo and p2  happened first however  since neither event can affect the other  there is no way for one of them to know whether the other has occurred yet   it is not important which 18.1 729 p 0 figure 18.1 relative time for three concurrent processes happened first it is important only that any processes that care about the order of two concurrent events agree on son1.e order 18.1.2 implementation to determine that an event a happened before an event b  we need either a common clock or a set of perfectly synchronized clocks since neither of these is available in a distributed system  we must defil e the happened-before relation without the use of physical clocks first we associate with each system event a we can then define the requirement  for every pair of events a and b  if a + b  then the timestamp of a is less than the timestamp of b  below  we will see that the converse need not be true  how do we enforce the global ordering requirement in a distributed environment we define within each process pi a logical lci the logical clock can be implemented as a simple counter incremented between any two successive events executed within a process since the logical clock has a increasing value  it assigns a unique number to every event  and if an event a occurs before event bin process pi  then lci  a  lc  b   the timestamp for an event is the value of the logical clock for that event this scheme ensures that for any two events in the same process the global orderil g requirement is met unfortunately  this scheme does not ensure that the global ordering requirement is met across processes to illustrate the problem  consider two processes p1 and p2 that communicate with each other suppose that p1 sends a message to p2  event a  with lc1  a  = 200  and p2 receives the message  event b  with lc2  b  = 195  because the processor for p2 is slower than the processor for p1  its logical clock ticks more slowly   this situation violates our requirement  since a + b but the timestamp of a is greater than the timestamp of b to resolve this difficulty  we require a process to advance its logical clock when it receives a message whose timestamp is greater than the current value of its logical clock in particulm ~ if process pi receives a message  event b  with timestamp t and lc  b     ; t  tlcenit should advance its clock so that lci  b  = t + 1 thus  in our example  when p2 receives the message from p1  it will advance its logical clock so that lc2  b  = 201 730 chapter 18 18.2 finally  to realize a total ordering  we need only observe that  with our timestamp-ordering scheme  if the timestamps of two events  a and b  are the same  then the events are concurrent in this case  we may use process identity numbers to break ties and to create a total ordering the use of tirnestamps is further discussed in section 18.4.2 in this section  we present a number of different algorithms for implementing mutual exclusion in a distributed environment we assume that the system consists of n processes  each of which resides at a different processor to simplify our discussion  we assume that processes are numbered uniquely from 1 to n and that a one-to-one mapping exists between processes and processors  that is  each process has its own processor   18.2.1 centralized approach in a centralized approach to providing mutual exclusion  one of the processes in the system is chosen to coordinate the entry to the critical section each process that wants to invoke mutual exclusion sends a request message to the coordinator when the process receives a reply message from the coordinator  it can enter its critical section after exiting its critical section  the process sends a release message to the coordinator and proceeds with its execution on receiving a request message  the coordinator checks to see whether some other process is in its critical section if no process is in its critical section  the coordinator immediately sends back a reply message otherwise  the request is queued when the coordinator receives a release message  it removes one of the request messages from the queue  in accordance with some scheduling algorithm  and sends a reply message to the requesting process it should be clear that this algorithm ensures mutual exclusion in addition  if the scheduling policy within the coordinator is fair-such as first-come  firstserved  fcfs  scheduling-no starvation can occur this scheme requires three messages per critical-section entry  a reques  a reply  and a release if the coordinator process fails  then a new process must take its place in section 18.6  we describe some algorithms for electing a unique new coordinator once a new coordinator has been elected  it must poll all the processes in the system to reconstruct its request queue once the queue has been constructed  the computation can resume 18.2.2 fully distributed approach if we want to distribute the decision making across the entire system  then the solution is far more complicated one approach  described next  uses an algorithm based on the event-ordering scheme described in section 18.1 when a process g wants to enter its critical section  it generates a new timestamp  ts  and sends the message request  p ;  ts  to all processes in the system  including itself   on receiving a request message  a process may reply immediately  that is  send a reply message back to p ;   or it may defer sending a reply back  because it is already in its critical section  for example   a process that has received a reply message from all other processes in the system can 18.2 731 enter its critical section  queueing incmning requests and deferring them after exiting its critical section  the process sends reply messages to all its deferred requests the decision whether process pi replies immediately to a request  pj  ts  message or defers its reply is based on three factors  if process pi is in its critical section  then it defers its reply to pj if process pi does not want to enter its critical section  then it sends a reply immediately to p j if process p ; wants to enter its critical section but has not yet entered it  then it compares its own request timestamp with the timestamp of the incoming request made by process pj if its own request timestamp is greater than that of the incoming request  then it sends a reply immediately to pj  pj asked first   otherwise  the reply is deferred this algorithm exhibits the following desirable behavior  mutual exclusion is obtained freedom from deadlock is ensured freedom from starvation is ensured  since entry to the critical section is scheduled according to the timestamp ordering the timestamp ordering ensures that processes are served in fcfs order the number of messages per critical-section entry is 2 x  n-1   this number represents the minimum number of required messages per critical-section entry when processes act independently and concurrently to illustrate how the algorithm functions  we consider a system consisting of processes p1  p2  and p3  suppose that processes p1 and p3 want to enter their critical sections process p1 then sends a message request  p1  timestamp = 10  to processes p2 and p3  while process p3 sends a message request  p3  timestamp = 4  to processes p1 and p2   the timestamps 4 and 10 were obtained from the logical clocks described in section 18.1  when process p2 receives these request messages  it replies immediately when process p1 receives the request from process p3  it replies immediately  since the timestamp  10  on its own request message is greater than the timestamp  4  for process p3 when process p3 receives the request message from process p1  it defers its reply  since the timestamp  4  on its request message is less than the timestamp  10  for the message from process p1  on receiving replies from both process p1 and process p2  process p3 can enter its critical section after exiting its critical section  process p3 sends a reply to process p1  which can then enter its critical section because this scheme requires the participation of all the processes in the system  it has three undesirable consequences  the processes need to know the identity of all other processes in the system when a new process joins the group of processes participating in the mutual-exclusion algorithm  the following actions need to be taken  732 chapter 18 a the process must receive the names of all the other processes in the gro11p b the name of the new process must be distributed to all the other processes in the group this task is not as trivial as it may seem  since some request and reply messages may be circulating in the system when the new process joins the group the interested reader is referred to the bibliographical notes at the end of the chapter if one process fails  then the entire scheme collapses we can resolve this difficulty by continuously monitoring the state of all processes in the system if one process fails  then all other processes are notified  so that they will no longer send request messages to the failed process when a process recovers  it must initiate the procedure that allows it to rejoin the group processes that have not entered their critical section must pause frequently to assure other processes that they intend to enter the critical section because of these difficulties  this protocol is best suited for small  stable sets of cooperating processes 18.2.3 token-passing approach another method of providilcg mutual exclusion is to circulate a token among the processes ilc the system a is a special type of message that is passed from process to process possession of the token entitles the holder to enter the critical section since there is only a single token  only one process can be in its critical section at a time we assume that the processes in the system are logically organized ilc a the physical communication network need not be a ring as long as the processes are connected to one another  it is possible to implement a logical ring to implement mutual exclusion  we pass the token around the ring when a process receives the token  it may enter its critical section  keeping the token after the process exits its critical section  the token is passed around again if the process receiving the token does not want to enter its critical section  it passes the token to its neighbor this scheme is similar to algorithm 1 in chapter 6  but a token is substituted for a shared variable if the ring is unidirectional  freedom from starvation is ensured the number of messages required to implement mutual exclusion may vary from one message per entry  in the case of high contention  that is  every process wants to enter its critical section   to an infinite number of messages  in the case of low contention  that is  no process wants to enter its critical section   two types of failure must be considered first  if the token is lost an election must be called to generate a new token second  if a process fails  a new logical ring must be established in section 18.6  we present an election algorithm ; others are possible the development of an algorithm for reconstructing the ring is left to you in exercise 18.4 18.3 18.3 733 in chapter 6  we introduced the concept of an atomic transaction  which is a program unit that must be executed  that is  either all the operations associated with it are executed to completion  or none are performed when we are dealing with a distributed system  ensuring the atomicity of a transaction becomes much more complicated than in a centralized system this difficulty occurs because several sites may be participating in the execution of a single transaction the failure of one of these sites  or the failure of a communication link connecting the sites  may result in erroneous computations ensuring that the execution of transactions in the distributed system preserves atomicity is the function of the each site has its own local transaction coordinator  which is responsible for coordinating the execution of all the transactions initiated at that site for each such transaction  the coordinator is responsible for the following  starting the execution of the transaction breaking the transaction into a number of subtransactions and distributing these subtransactions to the appropriate sites for execution coordinating the termination of the transaction  which may result in the transactions being committed at all sites or aborted at all sites we assume that each local site maintains a log for recovery purposes 18.3.1 the two-phase commit protocol for atomicity to be ensured  all the sites in which a transaction t has executed must agree on the final outcome of the execution t must either commit at all sites  or it must abort at all sites to ensure this property  the transaction coordinator of t must execute a among the simplest and most widely used commit protocols is the which we discuss next assume that t is a transaction initiated at site s ; and that the transaction coordinator at s ; is c ;  when t completes its execution-that is  when all the sites at which t has executed inform c ; that t has completed -then c starts the 2pc protocol phase 1 c adds the record prepare t to the log and forces the record onto stable storage it then sends a prepare  t  message to all the sites at which t has executed on receiving the message  the transaction manager at each of these sites determines whether it is willing to commit its portion oft if the answer is no  it adds a record no t to the log  and then it responds by sending an abort  t  message to c ;  if the answer is yes  it adds a record ready t to the log and forces all the log records corresponding to t onto stable storage the transaction manager then replies with a ready  t  message to c phase 2 when c ; has received responses to the prepare  t  message from all the sites  or when a pre-specified interval of time has elapsed since the prepare  t  message was sent out  c ; can determine whether the transaction 734 chapter 18 t can be committed or aborted transaction t can be committed if c ; has received a ready  t  m ~ essage from all the participating sites otherwise  transaction t must be aborted depending on the verdict  either a record commit t or a record abort t is added to the log and forced onto stable storage at this point  the fate of the transaction has been sealed following this  the coordinator sends either a commit  t  or an abort  t  message to all participating sites when a site receives that message  it records the message in the log a site at which t has executed can unconditionally abort tat any time prior to its sending the message ready  t  to the coordinator the ready  t  message is  in effect  a promise by a site to follow the coordinator 's order to commit tor to abort t a site can make such a promise only when the needed information is stored in stable storage otherwise  if the site crashes after sending ready  t   it may be unable to make good on its promise since unanimity is required to commit a transaction  the fate of t is sealed as soon as at least one site responds with abort  t   note that the coordinator site 5 ; can decide unilaterally to abort t  as it is one of the sites at which t has executed the final verdict regarding t is determined at the time the coordinator writes that verdict  commit or abort  to the log and forces it to stable storage in some implementations of the 2pc protocol  a site sends an acknowledge  t  message to the coordinator at the end of the second phase of the protocol when the coordinator has received the acknowledge  t  message from all the sites  it adds the record complete t to the log 18.3.2 failure handling in 2pc we now examine in detail how 2pc responds to various types of failures as we shall see  one major disadvantage of the 2pc protocol is that coordinator failure may result in blocking  and a decision either to commit or to abort t may have to be postponed until the coordinator recovers 18.3.2.1 failure of a participating site when a participating site s ~ c recovers from a failure  it must examine its log to determine the fate of those transactions that were in the midst of execution when the failure occurred suppose that t is one such transaction how will s ~ c deal with t we consider each of the possible alternatives  the log contain ~ s a commit t record in this case  the site executes redo  t   the log contains an abort t record in this case  the site executes undo  t   the log contains a ready t record in this case  the site must consult c ; to determine the fate oft if c ; is up  it tells s ~ c whether t committed or aborted in the former case  it executes redo  t  ; in the latter case  it executes undo  t   if c ; is down  s ~ c must try to find out the fate of t from other sites it does so by sending a query-status  t  message to all the sites in the system on receiving such a message  a site must consult 18.3 735 its log to determine whether t has executed there and  if so  whether t committed or aborted it then notifies s  about this outcome if no site has the appropriate information  that is  whether t corrnni tted or aborted   then s ~ c can neither abort nor commit t the decision concerning t is postponed until s1c can obtain the needed information thus  s ~ c must periodically resend the query-status  t  message to the other sites it does so until a site responds with the needed information the site at which c ; resides always has this inforn1.ation the log contains no control records  abort  commit  ready  concerning t the absence of control records implies that s1c failed before responding to the prepare  t  message from c ;  since the failure of s1c means that it could not have sent such a response  by our algorithm  c ; must have aborted t hence  s1c must execute undo  t   18.3.2.2 failure of the coordinator if the coordinator fails in the midst of the execution of the commit protocol for transaction t  then the participating sites must decide the fate oft we shall see that  in certain cases  the participating sites can not decide whether to commit or abort t  and therefore these sites must wait for the recovery of the failed coordinator if an active site contains a commit t record in its log  then t must be committed if an active site contains an abort t record in its log  then t must be aborted if some active site does not contain a ready t record in its log  then the failed coordinator c ; can not have decided to commit t we can draw this conclusion because a site that does not have a ready t record in its log can not have sent a ready  t  message to c ;  however  the coordinator may have decided to abort t rather than wait for c ; to recover  it is preferable to abort tin this case if none of the preceding cases holds  then all the active sites must have a ready t record in their logs  but no additional control records  such as abort t or commit t   since the coordinator has failed  it is impossible to determine whether a decision has been made-or  if so  what that decision is-until the coordinator recovers thus  the active sites must wait for c to recover as long as the fate oft remains in doubt  t may continue to hold system resources for example  if locking is used  t may hold locks on data at active sites such a situation is undesirable because hours or days may pass before c ; is again active during this time  other transactions may be forced to wait for t as a result  data are unavailable not only on the failed site  c ;  but on active sites as well the amount of unavailable data increases as the downtime of c ; grows this situation is called the blocking problem  because t is blocked pending the recovery of site c ;  736 chapter 18 18.4 18.3.2.3 failure of the network when a link fails  the messages in the process of being routed through the link do not arrive at their destinations intact from the viewpoint of the sites connected throughout that link  the other sites appear to have failed thus  our previous schemes apply here as well when a number of links fail  the network may partition in this case  two possibilities exist the coordinator and all its participants may remain in one partition ; in this case  the failure has no effect on the commit protocol alternatively  the coordinator and its participants may belong to several partitions ; in this case  messages between the participant and the coordinator are lost  reducing the case to a link failure we move next to the issue of concurrency control in this section  we show how certain of the concurrency-control schemes discussed in chapter 6 can be modified for use in a distributed environment the transaction manager of a distributed database system manages the execution of those transactions  or subtransactions  that access data stored in a local site each such transaction may be either a local transaction  that is  a transaction that executes only at that site  or part of a global transaction  that is  a transaction that executes at several sites   each transaction manager is responsible for maintaining a log for recovery purposes and for participating in an appropriate concurrency-control scheme to coordinate the concurrent execution of the transactions executing at that site as we shall see  the concurrency schemes described in chapter 6 need to be modified to accommodate the distribution of transactions 18.4.1 locking protocols the two-phase locking protocols described in chapter 6 can be used in a distributed environment the only change needed is iil the way the lock manager is implemented here  we present several possible schemes the first deals with the case where no data replication is allowed the others apply to the more general case where data can be replicated in several sites as in chapter 6  we assume the existence of the shared and 18.4.1.1 nonreplicated scheme if no data are replicated in the system  then the locking schemes described in section 6.9 can be applied as follows  each site maintains a local lock manager whose function is to administer the lock and unlock requests for those data items stored in that site when a transaction wishes to lock data item qat site s ;  it simply sends a message to the lock manager at site s ; requesting a lock  in a particular lock mode   if data item q is locked in an incompatible mode  then the request is delayed until that request can be granted once it has been determined that the lock request can be granted  the lock n1.anager sends a message back to the initiator indicating that the lock request has been granted 18.4 737 this scheme has the advantage of simple implementation it requires two message transfers for handling lock requests and one ncessage transfer for handling unlock requests however  deadlock handling is more complex since the lock and unlock requests are no longer made at a single site  the various deadlock-handling algorithms discussed in chapter 7 must be modified ; these modifications are discussed in section 18.5 18.4.1.2 single-coordinator approach several concurrency-control schemes can be used in systems that allow data replication under the single-coordinator approach  the system maintains a single lock manager that resides in a single chosen site-say  si all lock and unlock requests are made at site si when a transaction needs to lock a data item  it sends a lock request to si the lock manager determines whether the lock can be granted immediately if so  it sends a message to that effect to the site at which the lock request was initiated otherwise  the request is delayed until it can be granted ; and at that time  a message is sent to the site at which the lock request was initiated the transaction can read the data item from any one of the sites at which a replica of the data item resides in the case of a write operation  all the sites where a replica of the data item resides must be involved in the writing the scheme has the following advantages  simple implementation this scheme requires two messages for handling lock requests and one message for handling lmlock requests simple deadlock handling since all lock and unlock requests are made at one site  the deadlock-handling algorithms discussed in chapter 7 can be applied directly to this environment the disadvantages of the scheme include the following  bottleneck the site si becomes a bottleneck  since all requests must be processed there vulnerability if the site si fails  the concurrency controller is lost either processing must stop or a recovery scheme must be used a compromise between these advantages and disadvantages can be achieved through a in which the lockmanager function is distributed over several sites each lock manager administers the lock and unlock requests for a subset of the data items this distribution reduces the degree to which the coordinator is a bottleneck  but it complicates deadlock handling  since the lock and unlock requests are not made at a single site 18.4.1.3 majority protocol the majority protocol is a modification of the nonreplicated data scheme presented earlier the system maintains a lock manager at each site each manager controls the locks for all the data or replicas of data stored at that site when a transaction wishes to lock a data item q that is replicated inn different 738 chapter 18 sites  it must send a lock request to nlore than one-half of then sites in which q is stored each lock manager detern1.ines whether the lock can be granted immediately  as far as it is concerned   as before  the response is delayed until the request can be granted the transaction does not operate on q until it has successfully obtained a lock on a majority of the replicas of q this scheme deals with replicated data in a decentralized manner  thus avoiding the drawbacks of central control however  it suffers from its own disadvantages  implementation the majority protocol is more complicated to implement than the previous schemes it requires 2  n/2 + 1  messages for handling lock requests and  n/2 + 1  messages for handling unlock requests deadlock handling since the lock and unlock requests are not made at one site  the deadlock-handling algorithms must be modified  section 18.5   in addition  a deadlock can occur even if only one data item is being locked to illustrate  consider a system with four sites and full replication suppose that transactions t1 and t2 wish to lock data item q in exclusive mode transaction t1 may succeed in locking q at sites 51 and 53  while transaction t2 may succeed in locking q at sites 52 and 54 each then must wait to acquire the third lock  and hence a deadlock has occurred 18.4.1.4 biased protocol the biased protocol is similar to the majority protocol the difference is that requests for shared locks are given more favorable treatment than are requests for exclusive locks the system maintains a lock manager at each site each manager manages the locks for all the data items stored at that site shared and exclusive locks are handled differently shared locks when a transaction needs to lock data item q  it simply requests a lock on q from the lock manager at one site containing a replica of q exclusive locks when a transaction needs to lock data item q  it requests a lock on q from the lock manager at each site containing a replica of q as before  the response to the request is delayed until the request can be granted the scheme has the advantage of imposing less overhead on read operations than does the majority protocol this advantage is especially significant in common cases in which the frequency of reads is much greater than the frequency of writes however  the additional overhead on writes is a disadvantage furthermore  the biased protocol shares the majority protocol 's disadvantage of complexity in handling deadlock 18.4.1.5 primary copy yet another alternative is to choose one of the replicas as the primary copy thus  for each data item q  the primary copy of q must reside in precisely one site  which we call the primary site of q when a transaction needs to lock a data 18.4 739 item q  it requests a lock at the primary site of q again  the response to the request is delayed until the request can be granted this scheme enables us to handle concurrency control for replicated data in much the same way as for unreplicated data implementation of the method is simple however  if the primary site of q fails  q is inaccessible even though other sites containing a replica may be accessible 18.4.2 timestamping the principal idea behind the timestamping scheme discussed in section 6.9 is that each transaction is given a unique timestamp  which is used to decide the serialization order our first task  then  in generalizing the centralized scheme to a distributed scheme is to develop a method for generating unique timestamps our previous protocols can then be applied directly to the nonreplicated environment 18.4.2.1 generation of unique timestamps two primary methods are used to generate unique timestamps ; one is centralized  and one is distributed in the centralized scheme  a single site is chosen for distributing the timestamps the site can use a logical counter or its own local clock for this purpose in the distributed scheme  each site generates a local unique timestamp using either a logical counter or the local clock the global unique timestamp is obtained by concatenation of the local unique timestamp with the site identifier  which must also be unique  figure 18.2   the order of concatenation is important ! we use the site identifier in the least sign.ificant position to ensure that the global timestamps generated in one site are not always greater than those generated in another site compare this technique for generating unique timestamps with the one presented in section 18.1.2 for generating unique na1nes we may still have a problem if one site generates local timestamps at a faster rate than do other sites in such a case  the fast site 's logical counter will be larger than those of other sites therefore  all timestamps generated by the fast site will be larger than those generated by other sites a mechanism is needed to ensure that local timestamps are generated fairly across the system to accomplish the fair generation of timestamps  we define within each site si a logical clock  lc   which generates the local timestamp  see section 18.1.2   to ensure that the various logical clocks are synchronized  we require that a site local unique timestamp site identifier ll 0 global unique identifier figure 18.2 generation of unique timestamps 740 chapter 18 18.5 si advance its logical clock whenever a transaction ~ with timestarnp x,y visits that site and xis greater than the current value of lc in this case  site si advances its logical clock to the value x + 1 if the system clock is used to generate timestamps  then timestamps are assigned fairly  provided that no site has a system clock that runs fast or slow since clocks may not be perfectly accurate  a technique similar to that used for logical clocks must be used to ensure that no clock gets far ahead or far behind another clock 18.4.2.2 timestamp-ordering scheme the basic timestamp scheme introduced in section 6.9.4.3 can be extended in a straightforward mam'ler to a distributed system as in the centralized case  cascading rollbacks may result if no mechanism is used to prevent a transaction from reading a data item value that is not yet committed to eliminate cascading rollbacks  we can con bine the basic timestamp scheme of section 6.9 with the 2pc protocol of section 18.3 to obtain a protocol that ensures serializability with no cascading rollbacks we leave the development of such an algorithm to you the basic timestamp scheme just described suffers from the undesirable property that conflicts between transactions are resolved through rollbacks  rather than through waits to alleviate this problem  we can buffer the various read and write operations  that is  delay them  until a time when we are assured that these operations can take place without causing aborts a read  x  operation by ~ must be delayed if there exists a transaction tj that will perform a wri te  x  operation but has not yet done so and ts  tj  ts  ~   similarly  a wri te  x  operation by ~ must be delayed if there exists a transaction tj that will perform either a read  x  or a wri te  x  operation and ts  tj  ts  t ;   various methods are available for this property one such method  called the scheme  requires each site to maintain a read queue and a write queue consisting of all the read and write requests that are to be executed at the site and that must be delayed to preserve the property just described we shall not present the scheme here again  we leave the development of the algorithm to you the deadlock-prevention  deadlock-avoidance  and deadlock-detection algorithms presented in chapter 7 can be extended so that they can be used in a distributed system in this section  we describe several of these distributed algorithms 18.5.1 deadlock prevention and avoidance the deadlock-prevention and deadlock-avoidance algorithms presented in chapter 7 can be used in a distributed system  provided that appropriate modifications are made for example  we can use the resource-ordering deadlock-prevention technique by simply defining a global ordering among the system resources that is  all resources in the entire system are assigned unique numbers  and a process may request a resource  at any processor  with 18.5 741 unique nl1mber i only if it is not holding a resource with a unique number greater than i similarly  we can use the banker 's algorithm in a distributed systen'l by designating one of the processes in the system  the banker  as the process that maintains the information necessary to carry out the banker 's algorithm every resource request must be channeled through the banker the global resource-ordering deadlock-prevention scheme is simple to implement in a distributed environment and requires little overhead the banker 's algorithm can also be implemented easily  but it may require too much overhead the banker may become a bottleneck  since the number of messages to and from the banker may be large thus  the banker 's scheme does not seem to be of practical use in a distributed system we turn next to a new deadlock-prevention scheme based on a timestampordering approach with resource preemption although this approach can handle any deadlock situation that may arise in a distributed system  for simplicity we consider only the case of a single instance of each resource type to control the preemption  we assign a unique priority number to each process these numbers are used to decide whether a process p ; should wait for a process pj for example  we can let p ; wait for pj if p ; has a priority higher than that of pj ; otherwise  p ; is rolled back this scheme prevents deadlocks because  for every edge p ;  + pi in the wait-for graph  p ; has a higher priority than p ;  thus  a cycle can not exist one difficulty with this scheme is the possibility of starvation some processes with extremely low priorities may always be rolled back this difficulty can be avoided through the use of timestamps each process in the system is assigned a unique timestamp when it is created two complementary deadlock-prevention schemes using timestamps have been proposed  1 the wait-die scheme this approach is based on a nonpreemptive teclmique when process p ; requests a resource currently held by pj  p ; is allowed to wait only if it has a smaller timestamp than does pj  that is  p ; is older than pj   otherwise  p ; is rolled back  dies   for example  suppose that processes p1  p2  and p3 have timestamps 5  10  and 15  respectively if p1 requests a resource held by p2  p1 will wait if p3 requests a resource held by p2  p3 will be rolled back the wound-wait scheme this approach is based on a preemptive technique and is a counterpart to the wait-die approach when process p ; requests a resource currently held by pj  p ; is allowed to wait only if it has a larger timestamp than does pi  that is  p ; is younger than pi   otherwise  pi is rolled back  pj is wounded by p ;   returning to our previous example  with processes p1  p2  and p3  if p1 requests a resource held by p2  then the resource will be preempted from p2  and p2 will be rolled back if p3 requests a resource held by p2  then p3 will wait both schemes can avoid starvation provided that  when a process is rolled back  it is not assigned a new timestamp since timestamps always increase  a process that is rolled back will eventually have the smallest timestamp thus  it will not be rolled back again there are  however  significant differences in the way the two schemes operate 742 chapter 18 in the wait-die scheme  an older process must wait for a younger one to release its resource thus  the older the process gets  the more it tends to wait by contrast  in the wound-wait scheme  an older process never waits for a younger process in the wait-die scheme  if a process pi dies and is rolled back because it has requested a resource held by process pj  then pi n ay reissue the same sequence of requests when it is restarted if the resource is still held by pj  then pi will die again thus  pi may die several times before acquiring the needed resource contrast this series of events with what happens in the wound-wait scheme process pi is wounded and rolled back because pi has requested a resource it holds when pi is restarted and requests the resource now being held by pi  pi waits thus  fewer rollbacks occur in the wound-wait scheme the major problem with both schemes is that unnecessary rollbacks may occur 18.5.2 deadlock detection the deadlock-prevention algorithm may preempt resources even if no deadlock has occurred to prevent um ecessary preemptions  we can use a deadlockdetection algorithm we construct a wait-for graph describing the resourceallocation state since we are assuming only a single resource of each type  a cycle in the wait-for graph represents a deadlock the main problem in a distributed system is deciding how to maintain the wait-for graph we illustrate this problem by describing several common techniques to deal with this issue these schemes require each site to keep a local wait-for graph the nodes of the graph correspond to all the processes  local as well as nonlocal  currently holding or requesting any of the resources local to that site for example  in figure 18.3 we have a system consisting of two sites  each maintaining its local wait-for graph note that processes p2 and p3 appear in both graphs  indicating that the processes have requested resources at both sites these local wait-for graphs are constructed in the usual manner for local processes and resources when a process pi in site 51 needs a resource held by process pj in site 52  a request message is sent by pi to site 52  the edge pi  + pj is then inserted in the local wait-for graph of site 52  figure 18.3 two local wait-for graphs 18.5 743 figure 18.4 global wait-for graph for figure 18.3 clearly  if any local wait-for graph has a cycle  deadlock has occurred the fact that we find no cycles in any of the local wait-for graphs does not mean that there are no deadlocks  however to illustrate this problem  we consider the system depicted in figure 18.3 each wait-for graph is acyclic ; nevertheless  a deadlock exists in the system to prove that a deadlock has not occurred  we must show that the of all local graphs is acyclic the graph  figure 18.4  that we obtain from the union of the two wait-for graphs of figure 18.3 does indeed contain a cycle  implying that the system is in a deadlocked state a number of methods are available to organize the wait-for graph in a distributed system we describe several common schemes here 18.5.2.1 centralized approach in the centralized approach  a global wait-for graph is constructed as the tmion of all the local wait-for graphs it is maintained in a single process  the since there is communication delay in the system  we must distinguish between two types of wait-for graphs the real graph describes the real but unknown state of the system at any point in time  as would be seen by an ormliscient observer the constructed graph is an approximation generated by the coordinator during the execution of its algorithm the constructed graph must be generated so that  whenever the detection algorithm is invoked  the reported results are correct by correct we mean the following  if a deadlock exists  then it is reported properly if a deadlock is reported  then the system is indeed in a deadlocked state as we shall show  it is not easy to construct such correct algorithms the wait-for graph may be constructed at three different points in time  whenever a new edge is inserted in or removed from one of the local wait-for graphs periodically  when a number of changes have occurred in a wait-for graph whenever the deadlock-detection coordinator needs to invoke the cycledetection algorithm when the deadlock-detection algorithm is invoked  the coordinator searches its global graph if a cycle is found  a victim is selected to be rolled back the 744 chapter 18 coordinator must notify all the sites that a particular process has been selected as victiitl the sites  in turn  roll back the victim process next  we consider each of the three graph-construction options listed above with option 1  whenever an edge is either inserted in or removed from a local graph  the local site must also send a message to the coordinator to notify it of this modification on receiving such a message/ the coordinator updates its global graph alternatively  option 2  1 a site can send a number of such changes in a single message periodically returning to our previous example/ the coordinator process will maintain the global wait-for graph as depicted in figure 18.4 when site 52 inserts the edge p3  + p4 in its local wait-for graph/ it also sends a message to the coordinator similarly/ when site 51 deletes the edge p5  + p1 because p1 has released a resource that was requested by ps 1 an appropriate message is sent to the coordinator note that no matter which of these two options is used/ unnecessary rollbacks may occur  as a result of two situations  1 false may exist in the global wait-for graph to illustrate this point we consider a snapshot of the system as depicted in figure 18.5 suppose that p2 releases the resource it is holding in site 511 resulting in the deletion of the edge p1  + p2 in site 51 process p2 then requests a resource held by p3 at site 521 resulting in the addition of the edge p2  + p3 in site 52 if the insert p2  + p3 message from site 52 arrives before the delete p1  + p2 message from site 51/ the coordinator may discover the false cycle p1  + p2  + p3  + p1 after the insert  but before the delete   deadlock recovery may be initiated  although no deadlock has occurred 2 unnecessary rollbacks may also result when a deadlock has indeed occurred and a victim has been picked1 but at the same time one of the processes has been aborted for reasons unrelated to the deadlock  as when a process has exceeded its allocated time   for example/ suppose that site 51 in figure 18.3 decides to abort p2 at the same time/ the coordinator has discovered a cycle and picked p3 as a victim both p2 and p3 are now rolled back1 although only p2 needed to be rolled back now consider a centralized deadlock-detection algorithm using option 3 that detects all deadlocks that actually occur and does not detect false deadlocks to avoid the report of false deadlocks/ we require that requests coordinator figure 18.5 local and global wait-for graphs 18.5 745 from different sites be appended with unique identifiers  or timestamps   when process p ;  at site 511 requests a resource from pj  at site 52  a request message with timestamp t5 is sent the edge p ; ~ p1 with the label t5 is inserted in the local wait-for graph of 51 this edge is inserted in the local wait-for graph of site 52 only if site 52 has received the request message and can not immediately grant the requested resource a request from p ; to p1 in the same site is handled in the usual manner ; no timestamps are associated with the edge p ; ~ pi the detection algorithm is as follows  the controller sends an initiating message to each site in the system on receiving this message  a site sends its local wait-for graph to the coordinator each of these wait-for graphs contains all the local information the site has about the state of the real graph the graph reflects an instantaneous state of the site  but it is not synchronized with respect to any other site when the controller has received a reply from each site  it constructs a graph as follows  a the constructed graph contains a vertex for every process in the system b the graph has an edge p ; ~ p1 if and only if there is an edge p ; ~ p1 in one of the wait-for graphs or an edge p ; ~ pj with some label t5 in more than one wait-for graph if the constructed graph contains a cycle  then the system is in a deadlocked state if the constructed graph does not contain a cycle  then the system was not in a deadlocked state when the detection algorithm was invoked as result of the initiating messages sent by the coordinator  in step 1   18.5.2.2 fully distributed approach in the all controllers share equally the responsibility for detecting deadlock every site constructs a waitfor graph that represents a part of the total graph  depending on the dynamic behavior of the system the idea is that  if a deadlock exists  a cycle will appear in at least one of the partial graphs we present one such algorithm  which involves construction of partial graphs in every site each site maintains its own local wait-for graph a local wait-for graph in this scheme differs from the one described earlier in that we add one additional node pex to the graph an arc p ; ~ pex exists in the graph if p ; is waiting for a data item in another site being held by any process similarly  an arc pex ~ pi exists in the graph if a process at another site is waiting to acquire a resource currently being held by p1 in this local site to illustrate this situation  we consider again the two local wait-for graphs of figure 18.3 the addition of the node pex in both graphs results in the local wait-for graphs shown in figure 18.6 if a local wait-for graph contains a cycle that does not involve node pm then the system is in a deadlocked state if  however  a local graph contains a cycle involving pex  then this implies the possibility of a deadlock 746 chapter 18 figure 18.6 augmented local wait-for graphs from figure 18.3 to ascertain whether a deadlock does exist  we must invoke a distributed deadlock -detection algorithm suppose that  at site si  the local wait-for graph contains a cycle involving node pex this cycle must be of the form which indicates that process pk  in site si is waiting to acquire a data item located in some other site-say  sj on discovering this cycle  site si sends to site si a deadlock-detection message containing information about that cycle when site si receives this deadlock-detection message  it updates its local wait-for graph with the new information then it searches the newly constructed wait-for graph for a cycle not involving pex if one exists  a deadlock is found  and an appropriate recovery scheme is iiwoked if a cycle involving pex is discovered  then si transmits a deadlock-detection message to the appropriate site-say  sk site sic ! in return  repeats the procedure thus  after a finite number of rounds  either a deadlock is discovered or the deadlock-detection computation halts to illustrate this procedure  we consider the local wait-for graphs of figure 18.6 suppose that site 51 discovers the cycle since p3 is waiting to acquire a data item in site 52  a deadlock-detection message describing that cycle is transmitted from site 51 to site 52  when site 52 receives this message  it updates its local wait-for graph  obtaining the wait-for graph of figure 18.7 this graph contains the cycle which does not include node pex therefore  the system is in a deadlocked state  and an appropriate recovery scheme must be invoked note that the outcome would be the same if site 52 discovered the cycle first in its local wait-for graph and sent the deadlock-detection message to site 51 in the worst case  both sites will discover the cycle at about the same time  and two deadlock-detection messages will be sent  one by 51 to 52 and another by s2 to 51 this situation results in unnecessary message transfer and overhead in updating the two local wait-for graphs and searching for cycles in both graphs 18.6 18.6 747 figure 18.7 augmented local wait-for graph in site 5.z of figure 18.6 to reduce message traffic  we assign to each process pi a unique identifier  which we denote id  pi   when site sk discovers that its local wait-for graph contains a cycle involving node pex of the form it sends a deadlock-detection message to another site only if id  pi j id  pr j otherwise  site s1c continues its normal execution  leaving the burden of initiating the deadlock-detection algorithm to some other site to illustrate this scheme  we consider again the wait-for graphs maintained at sites 51 and 52 as shown in figure 18.6 suppose that suppose both sites discover these local cycles at about the same time the cycle in site 51 is of the form since id  p3  id  p2   site 51 does not send a deadlock-detection message to site 52  the cycle in site 52 is of the form since id  p2  id  p3   site 52 does send a deadlock-detection message to site 51  which  on receiving the message  updates its local wait-for graph site s1 then searches for a cycle in the graph and discovers that the system is in a deadlocked state as we pointed out in section 18.3  many distributed algorithms employ a coordinator process that performs functions needed by the other processes in 748 chapter 18 the systenc these functions include enforcing mutual exclusion  maintaining a global wait-for graph for deadlock detection  replacing a lost token  and controlling an input or output device in the system if the coordinator process fails due to the failure of the site at which it resides  the system can continue execution only by restarting a new copy of the coordinator on some other site the algorithms that determine where a new copy of the coordinator should be restarted are called election algorithms assume that a unique priority number is associated with each active process in the system for ease of notation  we assume that the priority number of process g is i to simplify our discussion  we assume a one-to-one correspondence between processes and sites and thus refer to both as processes the coordinator is always the process with the largest priority number hence  when a coordinator fails  the algorithm must elect that active process with the largest priority number this number must be sent to each active process in the system in addition  the algorithm must provide a mechanism for a recovered process to identify the current coordinator in this section  we present examples of election algorithms for two different configurations of distributed systems the first algorithm applies to systems where every process can send a message to every other process in the system the second algorithm applies to systems organized as a ring  logically or physically   both algorithms require n2 messages for an election  where n is the number of processes in the system we assume that a process that has failed knows on recovery that it has indeed failed and thus takes appropriate actions to rejoin the set of active processes 18.6.1 the bully algorithm suppose that process p ; sends a request that is not answered by the coordinator within a time interval t in this situation  it is assumed that the coordinator has failed  and p ; tries to elect itself as the new coordinator this task is completed through the following algorithm process p ; sends an election message to every process with a higher priority number process p ; then waits for a time interval t for an answer from any one of these processes if no response is received within time t  p ; assumes that all processes with numbers greater than i have failed and elects itself the new coordinator process p ; restarts a new copy of the coordinator and sends a message to inform all active processes with priority numbers less than i that p ; is the new coordinator howeve1 ~ if an answer is received  p ; begins a time interval t '  waiting to receive a message informing it that a process with a higher priority number has been elected  that is  some other process is electing itself coordinator and should report the results within time t  if no message is received within t '  then the process with a higher number is assumed to have failed  and process p ; should restart the algorithm if p ; is not the coordinator  then  at any time during execution  p ; may receive one of the following two messages from process pr p1 is the new coordinator  j i   process p ;  in turn  records this information 18.6 749 pi has started an election  j i   process p ; sends a response to pi and begins its own election algorithm  provided that p ; has not already initiated such an election the process that completes its algorithm has the highest number and is elected as the coordinator it has sent its number to all active processes with lower numbers after a failed process recovers  it immediately begins execution of the same algorithm if there are no active processes with higher numbers  the recovered process forces all processes with lower numbers to let it become the coordinator process  even if there is a currently active coordinator with a lower number for this reason  the algorithm is termed the we can demonstrate the operation of the algorithm with a simple example of a system consisting of processes p1 through p4  the operations are as follows  all processes are active ; p4 is the coordinator process p1 and p4 fail p2 determines that p4 has failed by sending a request that is not answered within time t p2 then begins its election algorithm by sending a request to p3  p3 receives the request  responds to p2  and begins its own algorithm by sending an election request to p4  p2 receives p3 's response and begins waiting for an interval t' p4 does not respond within an interval t  so p3 elects itself the new coordinator and sends the number 3 to p2 and p1  p1 does not receive the number  since it has failed  later  when p1 recovers  it sends an election request to p2  p3  and p4  p2 and p3 respond to p1 and begin their own election algorithms p3 will again be elected  through the same events as before finally  p4 recovers and notifies p1  p2  and p3 that it is the current coordinator  p4 sends no election requests  since it is the process with the highest number in the system  18.6.2 the ring algorithm the assumes that the links between processes are unidirectional and that each process sends its messages to the neighbor on the right the main data structure used by the algorithm is the lisi   a list that contains the priority numbers of all active processes in the system when the algorithm ends ; each process maintains its own active list the algorithm works as follows  if process p ; detects a coordinator failure  it creates a new active list that is initially empty it then sends a message elect  i  to its neighbor on the right and adds the number i to its active list 750 chapter 18 18.7 if pi receives a message elect  j  from the process on the left  it must respond in one of three ways  a if this is the first elect message it has seen or sent  pi creates a new active list with the numbers i and j it then sends the message elect  i   followed by the message elect  j   b ifi    f j-thatis,ifthe message received does not contain pi'snumber -then pi adds j to its active list and forwards the message to its neighbor on the right c if i = j-that is  if pi receives the message elect  i  -then the active list for pi now contains the numbers of all the active processes in the system process g can now determine the largest number in the active list to identify the new coordinator process this algorithm does not specify how a recovering process determines the number of the current coordinator process one solution requires a recovering process to send an inquiry message this message is forwarded around the ring to the current coordinator  which in turn sends a reply containing its number for a system to be reliable  we need a mechanism that allows a set of processes to agree on a common value such an agreement may not take place  for several reasons first  the communication medium may be faulty  resulting in lost or garbled messages second  the processes themselves may be faulty  resultilcg lie unpredictable process behavior the best we can hope for in this case is that processes fail in a clean way  stopping their execution without deviating from their normal execution pattern in the worst case  processes may send garbled or incorrect messages to other processes or even collaborate with other failed processes in an attempt to the integrity of the system the provides an analogy for this situation several divisions byzantine army  each commanded by its own general  surround an enemy camp the byzantine generals must reach agreement on whether or not to attack the enemy at dawn it is crucial that all generals agree  since an attack by only some of the divisions would result lie defeat the various divisions are geographically dispersed  and the generals can communicate with one another only via messengers who run from camp to camp the generals may not be able to reach agreement for at least two major reasons  messengers may get caught by the enemy and thus may be unable to deliver their messages this situation corresponds to unreliable communication in a computer system and is discussed further in section 18.7.1 generals may be traitors  trying to prevent the loyal generals from reaching an agreement this situation corresponds to faulty processes lie a computer system and is discussed further in section 18.7.2 18.7 751 18.7.1 unreliable communications let us first assume that  if processes fail  they do so in a clean way and that the communication medium is unreliable suppose that process p ; at site 51  which has sent a message to process pj at site 52  needs to know whether p1 has received the message so that it can decide how to proceed with its computation for example  p ; may decide to compute a functionfoo if pj has received its message or to compute a function boo if p1 has not received the message  because of some hardware failure   to detect failures  we can use a similar to the one described in section 16.7.1 when p ; sends out a message  it also specifies a time interval during which it is willing to wait for an acknowledgment message from pi when pi receives the message  it immediately sends an acknowledgment to p ;  if p ; receives the acknowledgment message within the specified time interval  it can safely conclude that p1 has received its message if  however  a time-out occurs  then p ; needs to retransmit its message and wait for an acknowledgment this procedure continues until p ; either gets the acknowledgment message back or is notified by the system that site 52 is down in the first case  it will compute foo ; in the latter case  it will compute boo note that  if these are the only two viable alternatives  p ; must wait until it has been notified that one of the situations has occurred suppose now that p7 also needs to know that p ; has received its acknowledgment message  so that it can decide how to proceed with its computation for example  p1 may want to compute foo only if it is assured that p ; got its acknowledgment in other words  p ; and pi will compute foo if and only if both have agreed on it it turns out that  in the presence of failure  it is not possible to accomplish this task more precisely  it is not possible in a distributed environment for processes p ; and p1 to agree completely on their respective states to prove this claim  let us suppose that a minimal sequence of message transfers exists such that  after the messages have been delivered  both processes agree to compute foo let m ' be the last message sent by p ; to pj since g does not know whether its message will arrive at pj  since the message may be lost due to a failure   p ; will execute foo regardless of the outcome of the message delivery thus  m ' could be removed from the sequence without affecting the decision procedure hence  the original sequence was not minimal  contradicting our assumption and showing that there is no sequence the processes can never be sure that both will compute foo 18.7.2 faulty processes now let us assume that the communication medium is reliable but that processes can fail in unpredictable ways consider a system of n processes  of which no more than m are faulty suppose that each process p ; has some private value of v ;  we wish to devise an algorithm that allows each nonfaulty process p ; to construct a vector x ; =  a ; ,l  a ; ,2    a ;  11  such that the following conditions exist  1 if pi is a nonfaulty process  then a ; ,i = vi if p ; and p1 are both nonfaulty processes  then x ; = x1 752 chapter 18 18.8 there are many solutions to this problem  and they share the following properties  a correct algorithm can be devised only if n 3 x m + 1 the worst-case delay for reaching agreement is proportionate tom + 1 message-passing delays the number of messages required for reaching agreement is large no single process is trustworthy  so all processes must collect all information and make their own decisions rather than presenting a general solution  which would be complicated  we present an algorithm for the simple case where m = 1 and n = 4 the algorithm requires two rounds of information exchange  each process sends its private value to the other three processes each process sends the information it has obtained in the first round to all other processes a faulty process obviously may refuse to send messages in this case  a nonfaulty process can choose an arbitrary value and pretend that the value was sent by the faulty process once these two rounds are completed  a nonfaulty process pi can construct its vector x ; =  ai,l  a,2  ai,3  a4  as follows  ai = v ;  for j -1 i  if at least two of the three values reported for process pj  in the two rounds of exchange  agree  then the majority value is used to set the value of ai,f ' otherwise  a default value-say  nil-is used to set the value of ai,f ' in a distributed system with no common memory and no common clock  it is sometimes impossible to determine the exact order in which two events occur the happened-before relation is only a partial ordering of the events in a distributed system timestamps can be used to provide a consistent event ordering mutual exclusion in a distributed environment can be implemented in a variety of ways in a centralized approach  one of the processes in the system is chosen to coordinate the entry to the critical section in the fully distributed approach  the decision making is distributed across the entire system a distributed algorithm  which is applicable to ring-structured networks  is the token-passing approach for atomicity to be ensured  all the sites in which a transaction t has executed must agree on the final outcome of the execution t either commits at all sites or aborts at all sites to ensure this property  the transaction coordinator 753 of t must execute a commit protocol the most widely used commit protocol is the 2pc protocol the various concurrency-control schemes that can be used in a centralized system can be modified for use in a distributed environment in the case of locking protocols  we need only change the way the lock manager is implemented in the case of timestamping and validation schemes  the only change needed is the development of a mechanism for generating unique global timestamps the mechanism can either concatenate a local timestamp with the site identification or advance local clocks whenever a message arrives that has a larger timestamp the primary method for dealing with deadlocks in a distributed environment is deadlock detection the main problem is deciding how to maintain the wait-for graph methods for organizing the wait-for graph include a centralized approach and a fully distributed approach some distributed algorithms require the use of a coordinator if the coordinator fails because of the failure of the site at which it resides  the system can continue execution only by restarting a new copy of the coordinator on some other site it can do so by maintaining a backup coordinator that is ready to assume responsibility if the coordinator fails another approach is to choose the new coordinator after the coordinator has failed the algorithms that determine where a new copy of the coordinator should be restarted are called election algorithms two algorithms  the bully algorithm and the ring algorithm  can be used to elect a new coordinator in case of failures 18.1 why is deadlock detection much more expensive in a distributed environment than in a centralized environment 18.2 consider a hierarchical deadlock-detection algorithm in which the global wait-for graph is distributed over a number of different controllers  which are organized in a tree each non-leaf controller maintains a wait-for graph that contains relevant information from the graphs of the controllers in the subtree below it in particular  let sa  sp  and sc be controllers such that sc is the lowest common ancestor of sa and sp  sc must be unique  since we are dealing with a tree   suppose that node ti appears in the local wait-for graph of controllers sa and sp then ~ must also appear in the local wait-for graph of controller sc every controller in the path from sc to sa every controller in the path from sc to sp in addition  if ~ and tj appear in the wait-for graph of controller so and there exists a path from ti to ti in the wait-for graph of one of the children of s0  then an edge ~  + tj must be in the wait-for graph of so show that  if a cycle exists in any of the wait-for graphs  then the system is deadlocked 754 chapter 18 18.3 your company is building a comp1.1ter network  and you are asked to develop a scheme for dealing with the deadlock problem a would you use a deadlock-detection scheme or a deadlockprevention scheme b if you used a deadlock-prevention scheme  which one would you use explain your choice c if you used a deadlock-detection scheme  which one would you use explain your choice 18.4 derive an election algorithm for bidirectional rings that is more efficient than the one presented in this chapter how many messages are needed for n processes 18.5 your company is building a computer network  and you are asked to write an algorithm for achieving distributed mutual exclusion which scheme will you use explain your choice 18.6 consider the following failure model for faulty processors processors follow the prescribed protocol but may fail at unexpected points in time when processors fail  they simply stop functioning and do not continue to participate in the distributed system given such a failure model  design an algorithm for reaching agreement among a set of processors discuss the conditions under which agreement could be reached 18.7 under what circumstances does the wait-die scheme perform better than the wound-wait scheme for granting resources to concurrently executing transactions 18.8 consider a failure that occurs during 2pc for a transaction for each possible failure  explain how 2pc ensures transaction atomicity despite the failure 18.9 the logical clock timestamp scheme presented in this chapter provides the following guarantee  if event a happens before event b  then the timestamp of a is smaller than the timestamp of b note  however  that we cam1.0t order two events based only on their timestamps the fact that an event c has a timestamp that is smaller than the timestamp of event d does not necessarily mean that event c happened before event d ; c and d could be concurrent events in the system discuss ways in which the logical clock timestamp scheme could be extended to distinguish concurrent events from events that can be ordered by the happened-before relation the distributed algorithm for extending the happened-before relation to a consistent total ordering of all the events in the system was developed by lamport  1978b   further discussions of using logical time to characterize the behavior of distributed systems can be found in fidge  1991   raynal and 755 singhal  1996   babaoglu and marzullo  1993   schwarz and mattern  1994   and mattern  1988   the first general algorithm for implementing mutual exclusion in a distributed environment was also developed by lamport  1978b   lamport 's scheme requires 3 x  n  1  messages per critical-section entry subsequently  ricart and agrawala  1981  proposed a distributed algorithm that requires only 2 x  n 1  messages their algorithm is presented in section 18.2.2 a squareroot algorithm for distributed mutual exclusion is described by maekawa  1985   the token-passing algorithm for rilcg-structured systems presented ilc section 18.2.3 was developed by lann  1977   carvalho and roucairol  1983  discusses mutual exclusion in computer networks  and agrawal and abbadi  1991  describes an efficient and fault-tolerant solution of distributed mutual exclusion a simple taxonomy for distributed mutual-exclusion algorithms is presented by raynal  1991   the issue of distributed synchronization is discussed by reed and kanodia  1979   shared-memory environment   lamport  1978b   lamport  1978a   and sclmeider  1982   totally disjoint processes   a distributed solution to the dilling-philosophers problem is presented by chang  1980   the 2pc protocol was developed by lampson and sturgis  1976  and gray  1978   two modified versions of 2pc  called presume commit and presume abort  reduce the overhead of 2pc by defilling default assumptions regarding the fate of transactions  mohan and lindsay  1983    papers dealing with the problems of implementillg the transaction concept in a distributed database were presented by gray  1981   traiger et al  1982   and spector and schwarz  1983   bernsteill et al  1987  offer comprehensive discussions of distributed concurrency control rosenkrantz et al  1978  report on the timestamp distributed deadlock-prevention algorithm the fully distributed deadlock-detection scheme presented ill section 18.5.2 was developed by obermarck  1982   the hierarchical deadlock-detection scheme of exercise 18.1 appears in menasce and muntz  1979   knapp  1987  and singhal  1989  offer surveys of deadlock detection in distributed systems deadlocks can also be detected by takilcg global snapshots of a distributed system  as discussed ill chandy and lamport  1985   the byzantine generals problem is discussed by lamport et al  1982  and pease et al  1980   the bully algorithm is presented by garcia-molina  1982   and the election algorithm for a ring-structured system was written by larue  1977   part eight our coverage of operating-system issues thus far has focused mainly on general-purpose computing systems there are  however  specialpurpose systems with requirements different from those of many of the systems we have described a real-time system is a computer system that requires not only that computed results be correct but also that the results be produced within a specified deadline period results produced after the deadline has passed-even if correct-may be of no real value for such systems  many traditional operating-system scheduling algorithms must be modified to meet the stringent timing deadlines a multimedia system must be able to handle not only conventional data  such as text files  programs  and word-processing documents  but also multimedia data multimedia data consist of continuous-media data  audio and video  as well as conventional data continuous-media data-such as frames of video-must be delivered according to certain time restrictions  for example  30 frames per second   the demands of handling continuous-media data require significant changes in operatingsystem structure  most notably in memory  disk  and network management 19.1 r our coverage of operating-system issues thus far has focused mainly on general-purpose computing systems  for example  desktop and server systems   we now turn our attention to real-time computing systems the requirements of real-time systems differ from those of many of the systems we have described  largely because real-time systems must produce results within certain time limits in this chapter  we provide an overview of real-time computer systems and describe how real-time operating systems must be constructed to meet the stringent timing requirements of these systems to explain the timing requirements of real-time systems to distinguish between hard and soft real-time systems to discuss the defining characteristics of real-time systems to describe scheduling algorithms for hard real-time systems a real-time system is a computer system that requires not only that the computing results be correct but also that the results be produced within a specified deadline period results produced after the deadline has passedeven if correct-may be of no real value to illustrate  consider an autonomous robot that delivers mail in an office complex if its vision-control system identifies a wall after the robot has walked into it  despite correctly identifying the wall  the system has not met its requirement contrast this timing requirement with the much less strict demands of other systems in an interactive desktop computer system  it is desirable to provide a quick response time to the interactive user  but it is not mandatory to do so some systems -such as a batch-processing system-m.ay have no timing requirements whatsoever real-time systems executing on traditional computer hardware are used in a wide range of applications in addition  many real-time systems are 759 760 chapter 19 19.2 embedded in specialized devices  such as ordinary home appliances  for example  microwave ovens and dishwashers   consumer digital devices  for exarnple  cameras and mp3 players   and communication devices  for example  cellular telephones and blackberry handheld devices   they are also present in larger entities  such as automobiles and airplanes an embedded system is a computing device that is part of a larger system in which the presence of a computing device is often not obvious to the user to illustrate  consider an embedded system for controlling a home dishwasher the embedded system may allow various options for scheduling the operation of the dishwasher-the water temperature  the type of cleaning  light or heavy   even a timer indicating when the dishwasher is to start most likely  the user of the dishwasher is unaware that there is in fact a computer embedded in the appliance as another example  consider an embedded system controlling antilock brakes in an automobile each wheel in the automobile has a sensor detecting how much sliding and traction are occurring  and each sensor continually sends its data to the system controller taking the results from these sensors  the controller tells the braking mechanism in each wheel how much braking pressure to apply again  to the user  in this instance  the driver of the automobile   the presence of an embedded computer system may not be apparent it is important to note  however  that not all embedded systems are real-time for example  an embedded system controlling a home furnace may have no real-time requirements whatsoever some real-time systems are identified as safety-critical systems in a safety-critical system  incorrect operation-usually due to a missed deadline -results in some sort of catastrophe examples of safety-critical systems include weapons systems  antilock brake systems  flight-management systems  and health-related embedded systems  such as pacemakers in these scenarios  the real-time system must respond to events by the specified deadlines ; otherwise  serious injury-or worse-might occur however  a significant majority of embedded systems do not qualify as safety-critical  including fax machines  microwave ovens  wristwatches  and networking devices such as switches and routers for these devices  missing deadline requirements results in nothing more than perhaps an unhappy user real-time computing is of two types  hard and soft a hard real-time system has the most stringent requirements  guaranteeing that critical realtime tasks be completed within their deadlines safety-critical systems are typically hard real-time systems a soft real-time system is less restrictive  simply providing that a critical real-time task will receive priority over other tasks and that it will retain that priority until it completes many commercial operating systems-as well as linux-provide soft real-time support in this section  we explore the characteristics of real-time systems and address issues related to designing both soft and hard real-time operating systencs the following characteristics are typical of many real-time systems  single purpose small size inexpensively mass-produced specific timing requirements 19.2 we next examine each of these characteristics 761 unlike pcs  which are put to many uses  a real-time system typically serves only a single purpose  such as controlling antilock brakes or delivering music on an mp3 player it is unlikely that a real-time system controlling an airliner 's navigation system will also play dvds ! the design of a real-time operating system reflects its single-purpose nature and is often quite simple many real-time systems exist in environments where physical space is constrained consider the amount of space available in a wristwatch or a microwave oven-it is considerably less than what is available in a desktop computer as a result of space constraints  most real-time systems lack both the cpu processing power and the amount of memory available in standard desktop pcs whereas most contemporary desktop and server systems use 32 or 64-bit processors  many real-time systems run on 8 or 16-bit processors similarly  a desktop pc might have several gigabytes of physical memory  whereas a real-time system might have less than a megabyte we refer to the amount of memory required to run the operating system and its applications as the footprint of a system because the amount of memory is limited  most real-time operating systems must have small footprints next  consider where many real-time systems are implemented  they are often found in home appliances and consumer devices devices such as digital cameras  microwave ovens  and thermostats are mass-produced in very costconscious environments thus  the microprocessors for real-time systems must also be inexpensively mass-produced one technique for reducing the cost of an embedded controller is to use an alternative technique for organizing the components of the computer system rather than organizing the computer around the structure shown in figure 19.1  where buses provide the mechanism intercom1ectin.g individual components  many embedded system controllers use a strategy known as here  the cpu  memory  including cache   memorymouse keyboard printer figure 19.1 bus-oriented organization 762 chapter 19 19.3 management-unit  mmu   and any attached peripheral ports  such as usb ports  are contained in a single integrated circuit the soc strategy is typically less expensive than the bus-oriented organization of figure 19.1 we turn now to the final characteristic identified above for real-time systems  specific timing requirements it is  in fact  the defining characteristic of such systems accordingly  the primary task of both hard and soft real-time operating systems is to support the timing requirements of real-time tasks  and the remainder of this chapter focuses on this issue real-time operating systems meet tim.ing requirements by using scheduling algorithms that give real-time processes the highest schedulil1.g priorities furthermore  schedulers must ensure that the priority of a real-time task does not degrade over time another technique for addressing timing requirements is by minimizing the response time to events such as il1.terrupts in this section  we discuss the features necessary for designing an operating system that supports real-time processes before we begin  though  let 's consider what is typically not needed for a real-time system we begin by examining several features provided in many of the operatil1.g systems discussed so far in this text  including linux  unix  and the various versions of windows these systems typically provide support for the following  a variety of peripheral devices  such as graphical displays  cd drives  and dvd drives protection and security mechanisms multiple users supporting these features often results in a sophisticated -and large-kernel for example  windows xp has over forty million lines of source code in contrast  a typical real-time operating system usually has a very simple design  often written in thousands rather than millions of lines of source code we would not expect these simple systems to il1.clude the features listed above but why do n't real-time systems provide these features  which are crucial to standard desktop and server systems there are several reasons  but three are most prominent first  because most real-time systems serve a single purpose  they simply do not require many of the features found il1 a desktop pc consider a digital wristwatch  it obviously has no need to support a disk drive or dvd  let alone virtual memory furthermore  a typical real-time system does not include the notion of a user the system simply supports a small number of tasks  which often await input from hardware devices  sensors  vision identification  and so forth   second  the features supported by standard desktop operating systems are impossible to provide without fast processors and large amounts of memory both of these are unavailable in real-time systems due to space constraints  as explained earlier in addition  many realtime systems lack sufficient space to support peripheral disk drives or graphical displays  although some systems may support file systems using nonvolatile memory  nvram   third  supporting features common in standard desktop 19.3 p = l relocation register r p physical memory figure 19.2 address translation in real-time systems 763 computing environments would greatly increase the cost of real-time systems  which could make such systems economically impractical additional considerations arise when we consider virtual memory in a real-time system providing virtual memory features as described in chapter 9 requires that the system include a memory-management unit  mmu  for translating logical to physical addresses however  mmus typically increase the cost and power consumption of the system in addition  the time required to translate logical addresses to physical addresses-especially in the case of a translation look-aside buffer  tlb  miss-may be prohibitive in a hard real-time environment in the following discussion  we examine several approaches for translating addresses in real-time systems figure 19.2 illustrates three different strategies for managing address translation available to designers of real-time operating systems in this scenario  the cpu generates logical address l  which must be mapped to physical address p the first approach is to bypass logical addresses and have the cpu generate physical addresses directly this teclmique-kn.own as real-addressing mode-does not employ virtual memory techniques and is effectively stating that p equals l one problem with real-addressil1.g mode is the absence of memory protection between processes real-addressing mode may also require that programmers specify the physical location where their programs are loaded into memory however  the benefit of this approach is that the system is quite fast  as no time is spent on address translation real-addressing mode is quite common in embedded systems with hard real-time constraints in fact  some real-time operating systems rum1.ing on microprocessors containing an mmu actually disable the mmu to gain the performance benefit of referencing physical addresses directly a second strategy for translating addresses is to use an approach similar to the dynamic relocation register shown in figure 8.4 in this scenario  a relocation register r is set to the memory location where a program is loaded the physical address p is generated by adding the contents of the relocation register r to l some real-time systems configure the mmu to perform this way the obvious benefit of this strategy is that the mmu can easily translate logical addresses to physical addresses using p = l + r however  this system still suffers from a lack of memory protection between processes 764 chapter 19 19.4 the last approach is for the real-time system to provide full virtual memory functionality as described in chapter 9 in this instance  address translation takes place via page tables and a translation look-aside buffer  or tlb in addition to allowing a program to be loaded at any memory location  this strategy also provides memory protection between processes for systems without attached disk drives  demand paging and swapping may not be possible however  systems may provide such features using nvram flash memory the lynxos and on core systems are examples of real-time operating systems providing full support for virtual memory keeping in mind the many possible variations  we now identify the features necessary for implementing a real-time operating system this list is by no means absolute ; some systems provide more features than we list below  while other systems provide fewer preemptive  priority-based scheduling preemptive kernel minimized latency one notable feature we omit from this list is networking support however  deciding whether to support networking protocols such as tcp /ip is simple  if the real-time system must be connected to a network  the operating system must provide networking capabilities for example  a system that gathers real-time data and transmits it to a server must obviously include networking features alternatively  a self-contained embedded system requiring no interaction with other computer systems has no obvious networking requirencent in the remainder of this section  we examine the basic requirements listed above and identify how they can be implemented in a real-time operating system 19.4.1 priority-based scheduling the most important feature of a real-time operating system is to respond immediately to a real-time process as soon as that process requires the cpu as a result  the scheduler for a real-time operating system must support a priority-based algorithm with preemption recall that priority-based scheduling algorithms assign each process a priority based on its importance ; more important tasks are assigned higher priorities than those deemed less important if the scheduler also supports preemption  a process currently running on the cpu will be preempted if a higher-priority process becomes available to run preemptive  priority-based scheduling algorithms are discussed in detail in chapter 5  where we also present examples of the soft real-time scheduling features of the solaris  windows xp  and linux operating systems each of these systems assigns real-time processes the highest scheduling priority for 19.4 765 example  windows xp has 32 different priority levels ; the highest levelspriority values 16 to 31-are reserved for real-time processes solaris and linux have similar prioritization schemes note  however  that providing a preemptive  priority-based scheduler only guarantees soft real-time functionality hard real-time systems must further guarantee that real-time tasks will be serviced in accord with their deadline requirem ~ ents  and making such guarantees may require additional scheduling features in section 19.5  we cover scheduling algorithms appropriate for hard real-time systems 19.4.2 preemptive kernels nonpreemptive kernels disallow preemption of a process running in kernel mode ; a kernel-mode process will run until it exits kernel mode  blocks  or voluntarily yields control of the cpu in contrast  a preemptive kernel allows the preemption of a task running in kernel mode designing preemptive kernels can be quite difficult  and traditional user-oriented applications such as spreadsheets  word processors  and web browsers typically do not require such quick response times as a result  some commercial desktop operating systems-such as windows xp-are nonpreemptive however  to meet the timing requirements of real-time systems-in particular  hard real-time systems-preemptive kernels are mandatory otherwise  a real-time task might have to wait an arbitrarily long period of time while another task was active in the kernel there are various strategies for making a kernel preemptible one approach is to insert preemption points in long-duration system calls a preemption point checks to see whether a high-priority process needs to be nm if so  a context switch takes place then  when the high-priority process terminates  the interrupted process continues with the system call preemption points can be placed only at safe locations in the kernel-that is  only where kernel data structures are not being modified a second strategy for making a kernel preemptible is through the use of synchronization mechanisms  discussed in chapter 6 with this method  the kernel can always be preemptible  because any kernel data being updated are protected from modification by the high-priority process 19.4.3 minimizing latency consider the event-driven nature of a real-time system the system is typically waiting for an event in real time to occur events may arise either in software -as when a timer expires-or in hardware-as when a remote-controlled vehicle detects that it is approaching an obstruction when an event occurs  the system must respond to and service it as quickly as possible we refer to event latency as the amount of time that elapses from when an event occurs to when it is serviced  figure 19.3   usually  different events have different latency requirements for example  the latency requirement for an antilock brake system might be three to five milliseconds  meaning that from the time a wheel first detects that it is sliding  the system controlling the antilock brakes has three to five milliseconds to respond to and control the situation any response that takes longer might result in the automobile 's veering out of control in contrast  an embedded 766 chapter 19 event e first occurs event latency real-time system responds to e time figure 19.3 event latency system controlling radar in an airliner might tolerate a latency period of several seconds two types of latencies affect the performance of real-time systems  interrupt latency dispatch latency interrupt latency refers to the period of time from the arrival of an interrupt at the cpu to the start of the routine that services the interrupt when an interrupt occursf the operating system must first complete the instruction it is executing and determine the type of interrupt that occurred it must then save the state of the current process before servicing the interrupt using the specific interrupt service routine  isr   the total time required to perform these tasks is the interrupt latency  figure 19.4   obviouslyf it is crucial for real-time task t running interrupt 1 determine or interrupt type o'witocontiex t interrupt latency time i ish i figure 19.4 interrupt latency 19.4 767 operating systems to minimize interrupt latency to ensure that real-time tasks receive immediate attention one important factor contributing to interrupt latency is the amomrt of time interrupts may be disabled while kernel data structures are being updated real-time operating systems require that interrupts be disabled for very short periods of time howeve1 ~ for hard real-time systems  interrupt latency must not only be minimized  it must in fact be bounded to guarantee the deterministic behavior required of hard real-time kernels the amount of time required for the schedulil g dispatcher to stop one process and start another is known as dispatch latency providing real-time tasks with immediate access to the cpu mandates that real-time operating systems minimize this latency the most effective technique for keeping dispatch latency low is to provide preemptive kernels in figure 19.5  we diagram the makeup of dispatch latency the conflict phase of dispatch latency has two components  preemption of any process running in the kernel release by low-priority processes of resources needed by a high-priority process as an example  in solaris  the dispatch latency with preemption disabled is over a hundred milliseconds with preemption enabled  it is reduced to less than a millisecond one issue that can affect dispatch latency arises when a higher-priority process needs to read or modify kernel data that are currently beil g accessed by a lower-priority process-or a chain of lower-priority processes as kernel event response to event ~ --------response interval + ~ process made interrupt available processing ! +  dispatch latency ----1 ~ _.... time figure 19.5 dispatch latency real-time process execution 768 chapter 19 19.5 data are typically protected with a lock  the higher-priority process will have to wait or a lower-priority one to finish with the resource the situation becomes more complicated i the lower-priority process is preempted in favor of yet another process with a higher priority as an example  assume we have three processes  l  m  and h  whose priorities follow the order l m h also assume that process h requires resource r  which is currently being accessed by process l ordinarily  process h would wait for l to finish using resource r howevet ~ now suppose that process m becomes runnable  thereby preempting process l indirectly  a process with a lower priority -process m-has affected how long process h must wait for l to relinquish resource r this problem  known as priority inversion  can be solved by use of the priority-inheritance protocol according to this protocol  all processes that are accessing resources needed by a higher-priority process inherit the higher priority until they are finished with the resources in question when they are finished  their priorities revert to their original values in the example above  a priority-inheritance protocol allows process l to temporarily inherit the priority of process h  thereby preventing process m from preempting its execution when process l has finished using resource r  it relinquishes its inherited priority from h and assumes its original priority as resource r is now available  process h -not m -will run next our coverage of scheduling so far has focused primarily on soft real-time systems as mentioned  though  scheduling for such systems provides no guarantee on when a critical process will be scheduled ; it guarantees only that the process will be given preference over noncritical processes hard real-time systems have stricter requirements a task must be serviced by its deadline ; service after the deadline has expired is the same as no service at all we now consider scheduling for hard real-time systems before we proceed with the details of the individual schedulers  however  we must define certain characteristics of the processes that are to be scheduled first  the processes are considered periodic that is  they require the cpu at constant intervals  periods   each periodic process has a fixed processing timet once it acquires the cpu  a deadline d by which time it must be serviced by the cpu  and a period p the relationship of the processing time  the deadline  and the period can be expressed as 0    ; t    ; d    ; p the rate of a periodic task is 1 i p figure 19.6 p d period1 d period2 d ~ i lc = j figure 19.6 periodic task time period3 19.5 769 illustrates the execution of a periodic process over time schedulers can take advantage of this relationship and assign priorities according to the deadline or rate requirements of a periodic process what is unusual about this form of scheduling is that a process may have to announce its deadline requirements to the scheduler then  using a technique known as an admission-control algorithm  the scheduler either admits the process  guaranteeing that the process will complete on time  or rejects the request as impossible if it can not guarantee that the task will be serviced by its deadline in the following sections  we explore scheduling algorithms that address the deadline requirements of hard real-time systems 19.5.1 rate-monotonic scheduling the rate-monotonic scheduling algorithm schedules periodic tasks using a static priority policy with preemption if a lower-priority process is running and a higher-priority process becomes available to run  it will preempt the lower-priority process upon entering the system  each periodic task is assigned a priority inversely based on its period the shorter the period  the higher the priority ; the longer the period  the lower the priority the rationale behind this policy is to assign a higher priority to tasks that require the cpu more often furthermore  rate-monotonic scheduling assumes that the processing time of a periodic process is the same for each cpu burst that is  every time a process acquires the cpu  the duration of its cpu burst is the same let 's consider an example we have two processes p1 and p2 the periods for p1 and p2 are 50 and 100  respectively-that is  pl = 50 and p2 = 100 the processil1.g times are t1 = 20 for p1 and t2 = 35 for p2  the deadline for each process requires that it complete its cpu burst by the start of its next period we must first ask ourselves whether it is possible to schedule these tasks so that each meets its deadlines if we measure the cpu utilization of a process pi as the ratio of its burst to its period -ti i pi -the cpu utilization of p1 is 20j50 = 0.40 and that of p2 is 35/100 = 0.35  for a total cpu utilization of 75 percent therefore  it seems we can schedule these tasks in such a way that both meet their deadlines and still leave the cpu with available cycles first  suppose we assign p2 a higher priority than p1 the execution of p1 and p2 is shown in figure 19.7 as we can see  p2 starts execution first and completes at time 35 at this point  p1 starts ; it completes its cpu burst at time 55 however  the first deadline for p1 was at time 50  so the scheduler has caused p1 to miss its deadline now suppose we use rate-monotonic scheduling  in which we assign p1 a higher priority than p2  since the period of p1 is shorter than that of p2 deadlines 0 1 0 20 30 40 50 60 70 80 90 1 00 11 0 120 figure 19.7 scheduling of tasks when p2 has a higher priority than p1  770 chapter 19 deadlines 0 1 0 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 figure '19.8 rate-monotonic scheduling the execution of these processes is shown in figure 19.8 p1 starts first and completes its cpu burst at time 20  thereby meeting its first deadline p2 starts running at this point and runs until time 50 at this time  it is preempted by p1  although it still has 5 milliseconds remaining in its cpu burst p1 completes its cpu burst at time 70  at which point the scheduler resumes p2 p2 completes its cpu burst at time 75  also meeting its first deadline the system is idle until time 100  when p1 is scheduled again rate-monotonic scheduling is considered optimal in that if a set of processes can not be scheduled by this algorithm  it can not be scheduled by any other algorithm that assigns static priorities let 's next examine a set of processes that can not be scheduled using the rate-monotonic algorithm assume that process p1 has a period of p1 = 50 and a cpu burst of t1 = 25 for p2  the corresponding values are p2 = 80 and t2 = 35 rate-monotonic scheduling would assign process p1 a higher priority  as it has the shorter period the total cpu utilization of the two processes is  25 /50  +  35 j80  = 0.94  and it therefore seems logical that the two processes could be scheduled and still leave the cpu with 6 percent available time figure 19.9 shows the scheduling of processes p1 and p2  initially  p1 runs until it completes its cpu burst at time 25 process p2 then begins running and n.ms until time 50  when it is preempted by p1 at this point  p2 still has 10 milliseconds remaining in its cpu burst process p1 n.ms until time 75 ; consequently  p2 misses the deadline for completion of its cpu burst at time 80 despite being optimal  then  rate-monotonic scheduling has a limitation  cpu utilization is bounded  and it is not always possible to fully maximize cpu resources the worst-case cpu utilization for scheduling n processes is 2  21111  1   with one process in the system  cpu utilization is 100 percent  but it falls to approximately 69 percent as the number of processes approaches infinity with two processes  cpu utilization is bounded at about 83 percent combined cpu utilization for the two processes scheduled in figures 19.7 and 19.8 is 75 percent ; therefore  the rate-monotonic scheduling algorithm is guaranteed to schedule deadlines 0 1 0 20 30 40 50 60 70 80 90 1 00 11 0 120 130 140 150 1 60 figure 19.9 missing deadlines with rate-monotonic scheduling 19.5 771 them so that they can meet their deadlines for the two processes scheduled in figure 19.9  combined cpu utilization is approximately 94 percent ; therefore  rate-monotonic scheduling can not guarantee that they can be scheduled so that they meet their deadlines 19.5.2 earliest-deadline-first scheduling earliest-deadline-first  edf  scheduling dynamically assigns priorities according to deadline the earlier the deadline  the higher the priority ; the later the deadline  the lower the priority under the edf policy  when a process becomes runnable  it must announce its deadline requirements to the system priorities may have to be adjusted to reflect the deadline of the newly rmmable process note how this differs from rate-monotonic scheduling  where priorities are fixed to illustrate edf scheduling  we again schedule the processes shown in figure 19.9  which failed to meet deadline requirements under rate-monotonic scheduling recall that p1 has values of p1 = 50 and t1 = 25 and that p2 has values of p2 = 80 and t2 = 35 the edf scheduling of these processes is shown in figure 19.10 process p1 has the earliest deadline  so its initial priority is higher than that of process p2 process p2 begins rmming at the end of the cpu burst for p1 however  whereas rate-monotonic scheduling allows p1 to preempt p2 at the beginning of its next period at time 50  edf scheduling allows process p2 to continue running p2 now has a higher priority than p1 because its next deadline  at time 80  is earlier than that of p1  at time 100   thus  both p1 and p2 meet their first deadlines process p1 again begins running at time 60 and completes its second cpu burst at time 85  also meeting its second deadline at time 100 p2 begins rum1ing at this point only to be preempted by p1 at the start of its next period at time 100 p2 is preempted because p1 has an earlier deadline  time 150  than p2  time 160   at time 125  p1 completes its cpu burst and p2 resumes execution  finishing at time 145 and meeting its deadline as well the system is idle until time 150  when p1 is scheduled to run once again unlike the rate-monotonic algorithm  edf scheduling does not require that processes be periodic  nor must a process require a constant amount of cpu time per burst the only requirement is that a process a1mom1ce its deadline to the scheduler when it becomes runnable the appeal of edf scheduling is that it is theoretically optimal-theoretically  it can schedule processes so that each process can meet its deadline requirements and cpu utilization will be 100 percent in practice  however  it is impossible to achieve this level of cpu utilization due to the cost of context switching between processes and interrupt handling deadlines 0 1 0 20 30 40 50 60 70 80 90 1 00 11 0 120 130 140 150 160 figure 19.10 earliest-deadline-first scheduling 772 chapter 19 19.5.3 proportional share scheduling proportional share schedulers operate by allocating t shares among all applications an application can receive n shares of timef thus ensuring that the application will have n 1 t of the total processor time as an examplef assume that a total oft = 100 shares is to be divided among three processesf af b f and c a is assigned 50 share sf b is assigned 15 sharesf and c is assigned 20 shares this scheme ensures that a will have 50 percent o total processor time  b will have 15 percent  and c will have 20 percent proportional share schedulers must work in conjunction with an admission control policy to guarantee that an application receives its allocated shares of time an admission control policy will only admit a client requesting a particular number of shares if sufficient shares are available in our current example  we have allocated 50 + 15 + 20 = 85 shares of the total of 100 shares if a new process d requested 30 shares  the admission controller would deny d entry into the system 19.5.4 pthread scheduling the posix standard also provides extensions for real-time computingposix 1b in this section  we cover some of the posix pthread api related to scheduling real-time threads pthreads defines two scheduling classes for real-time threads  sched __fifo sched_rr sched__fifo schedules threads according to a first-come  first-served policy using a fifo queue as outlined in section 5.3.1 however  there is no time slicing among threads of equal priority therefore  the highest-priority real-time thread at the front of the fifo queue will be granted the cpu until it terminates or blocks sched_rr  for round-robin  is sincilar to sched_fifo except that it provides time slicing among threads of equal priority pthreads provides an additional scheduling class-scheddther-but its implementation is undefined and system specific ; it may behave differently on different systems the pthread api specifies the following two functions for getting and setting the scheduling policy  pthread_attr_getsched_policy  pthread_attr_t attr  int policy  pthread_attr_setsched_policy  pthread_attr_t attr  int policy  the first parameter to both functions is a pointer to the set of attributes for the thread the second parameter is either  1  a pointer to an integer that is set to the current scheduling policy  for pthread_attr_getsched_policy    or  2  an integer value  sched_fifo  sched_rr  or sched_other  for the pthread_attr setsched_policy   function both functions return non-zero values if an error occurs 19.5 # include pthread.h # include stdio.h # define num_threads 5 int main  int argc  char argv     int i  policy ; pthread_t tid  num_threads  ; pthread_attr_t attr ; i get the default attributes i pthread_attr_init  &attr  ; i get the current scheduling policy i if  pthread_attr_getschedpolicy  &attr  &policy  ! = 0  fprintf  stderr  unable to get policy \ n  ; else   if  policy = = sched_other  printf  sched_other \ n  ; else if  policy = = sched_rr  printf  sched_rr \ n  ; else if  policy = = sched_fifo  printf  sched_fifo \ n  ; i set the scheduling policy  fifo  rr  or other i 773 if  pthread_attr_setschedpolicy  &attr  sched_other  ! = 0  fprintf  stderr  unable to set policy \ n  ;  i create the threads i for  i = 0 ; i num_threads ; i + +  pthread_create  &tid  i  ,&attr,runner,null  ; i now join on each thread i for  i = 0 ; i num_threads ; i + +  pthread_join  tid  i   null  ; i each thread will begin control in this function i void runner  void param   i do some work  i pthread_exi t  0  ;  figure 19.11 pthread scheduling api 774 chapter 19 19.6 in figure 19.11  we illustrate a pthread program using this api this program first determines the current scheduling policy and then sets the scheduling algorithm to scheddther in this section  we describe vxworks  a popular real-time operating system providing hard real-time support vxworks  commercially developed by wind river systems  is widely used in automobiles  consumer and industrial devices  and networking equipment such as switches and routers vxworks is also used to control the two rovers-spirit and opportunity-that began exploring the planet mars in 2004 the organizationofvxworks is shown in figure 19.12 vxworks is centered on the wind microkernel recall from our discussion in section 2.7.3 that microkernels are designed so that the operating-system kernel provides a bare minimum of features ; additional utilities  such as networking  file systems  and graphics  are provided in libraries outside of the kernel this approach offers many benefits  including minimizing the size of the kernel-a desirable feature for an embedded system requiring a small footprint the wind microkernel supports the following basic features  processes the wind microkernel provides support for individual processes and threads  using the pthread api   however  similar to linux  vxworks does not distinguish between processes and threads  instead referring to both as tasks embedded real-time application figure 19.12 the organization of vxworks 19.6 775 thelinux  operating .systent isbe ~ ng us ~ cl incre ~ singlyinreai-time enviroft ~ ments.yve  hav ~ alreadycovered its softr ~ fil-ti,nteschedv,ling.fe ~ tur ~ s  secti   n 5.6.3  ,whereby real-time tasl sareassignt ; d.thehj.ghe ~ tpriorityi ~ the s  cst ~ m  additionaueatures in the 2,6 release ofthe kernel makelinux even,more suitilb le fot .embedded systems thes ~  features i.nclud ~ ia fullr pree ptive kernel an ~ la more .efficient sched'-lli11g  ~ l ~ orith ! fl   'vhic  t p.ll1sij     1  tinte regardless ofthe number  of tas ~ s  ac.tive in the s ~ .stero  the2.6release also   k ;  r  ei ~    o ~ j ~ ; t ~ ~ ; ; ~ ~  ~ enthardlare arc ~ itecttlresbydi \ tlcljng another strategy jew  integrah1lg linux iftto reanill1e .envirof \ me ~ ts involves combini  \ g th ~  linux operat ~ s,.systt ; it1 vith a mallreal-.ti ! fle ~ ernet tht ; reby  providing  a systell1 th ~ t ~ cts as botl      l gene ~ al 7p ~ rp   s.e   tnd  ~ real-tipte ~ ystem this .is theappro ~ c  h tctken1  jy.t  te ~ ~ lil_1cux ppt ; rat ~ g systerrl in .rtlinux  the standard lirn.tx kernel run ~ as a tas  in ~ sll1ctll re ~ l-timeoperating  syste1n   the real ~ htt1eikerl'lelj  an ~ les alliftterrupts.-,-directing each interrupt to ct handler in the st ~ ndarclkerrtel.or to anintef ~ rupt.randler in the real7tirl'le ketnel fi1jrt,h_ermore/ ~ tlin ~ x  prevents the sh'tndar \ i.linuxkernel  from ever disablb ; j  g intefrl_lpts  t  tus.ens11rirtg th ~ t itc tl1rlotac1c11atencyto.thereal-timesystem.rj  lirtux.ctls0 pr   yi ies difft ; rent schedulingpolicies  includingrate-'mo1lot  i1  i  'schec1ul ~ j1g  sechonj9.5.1  c ; tnd earliest ~ deadline-first scheduling  section19 ; 5.2  scheduling the wind microkernel provides two separate scheduling models  preemptive scheduling and nonpreemptive round-robin scheduling with 256 different priority levels the scheduler also supports the posix api for real-time threads  covered in section 19.5.4 interrupts the wind microkernel also manages interrupts to support hard real-time requirements  interrupt and dispatch latency times are bounded interprocess communication the wind micro kernel provides both shared memory and message passing as mechanisms for communication between separate tasks it also allows tasks to communicate using a technique known as pipes-a mechanism that behaves in the same way as a fifo queue but allows tasks to communicate by writing to a special fik the pipe to protect data shared by separate tasks  vxworks provides semaphores and mutex locks with a priority inheritance protocol to prevent priority mversion outside the microkernet vxworks includes several component libraries that provide support for posix  java  tcp /ip networking  and the like all components are optionat allowing the designer of an embedded system to customize the system according to its specific needs for example  if networking is not required  the tcp /ip library can be excluded from the image of the operating system this strategy allows the operating-system designer to 776 chapter 19 19.7 include only required features  thereby minimizing the size-or footprint-of the operating system vxworks takes an interesting approach to memory management  supporting two levels of virtual memory the first level  which is quite simple  allows for control of the cache on a per-page basis this policy enables an application to specify certain pages as non-cacheable when data are being shared by separate tasks running on a multiprocessor architecture  it is possible that shared data can reside in separate caches local to individual processors unless an architecture supports a cache-coherency policy to ensure that the same data residing in two caches will not be different  such shared data should not be cached and should instead reside only in main memory so that all tasks maintain a consistent view of the data the second level of virtual memory requires the optional virtual memory component vxvmi  figure 19.12   along with processor support for a memorymanagement unit  mmu   when this optional component is loaded on a system with an mmu  vxworks allows a task to mark certain data areas as private a data area marked as private may only be accessed by the task it belongs to furthermore  vxworks allows pages containing kernel code along with the interrupt vector to be declared as read-only this is useful  as vxworks does not distinguish between user and kernel modes ; all applications run in kernel mode  giving an application access to the entire address space of the system a real-time system is a computer system requiring that results arrive within a deadline period ; results arriving after the deadline has passed are useless many real-time systems are embedded in consumer and industrial devices there are two types of real-time systems  soft and hard real-time systems soft real-time systems are the least restrictive  assigning real-time tasks higher scheduling priority than other tasks hard real-time systems must guarantee that real-time tasks are serviced within their deadline periods in addition to strict timing requirements  real-time systems can further be characterized as having only a single purpose and running on small  inexpensive devices to meet timing requirements  real-time operating systems must employ various techniques the scheduler for a real-time operating system must support a priority-based algorithm with preemption furthermore  the operating system must allow tasks running in the kernel to be preempted in favor of higher-priority real-time tasks real-time operating systems also address specific timing issues by minimizing both interrupt and dispatch latency real-time scheduling algorithms include rate-monotonic and earliestdeadline first scheduling rate-monotonic scheduling assignb tasks that require the cpu more often a higher priority than tasks that require the cpu less often earliest-deadline-first scheduling assigns priority according to upcoming deadlines-the earlier the deadline  the higher the priority proportional share scheduling uses the technique of dividing up processor time into shares and assigning each process a number of shares  thus guaranteeing each process its proportional share of cpu time the pthread api provides various features for scheduling real-time threads as well 777 19.1 explain why interrupt and dispatch latency times must be bounded in a hard real-time system 19.2 identify whether hard or soft real-time scheduling is more appropriate in the following environments  a thermostat in a household b control system for a nuclear power plant c fuel economy system in an automobile d landing system in a jet airliner 19.3 consider two processes  p1 and p2  where p1 = 50  t1 = 25  p2 = 75  and t2 = 30 a can these two processes be scheduled using rate-monotonic scheduling illustrate your answer using a gantt chart such as the ones in figures 19.7-19.10 b illustrate the scheduling of these two processes using earliestdeadline first  edf  scheduling 19.4 discuss ways in which the priority inversion problem could be addressed in a real-time system also discuss whether the solutions could be implemented within the context of a proportional share scheduler 19.5 under what circumstances is rate-monotonic scheduling inferior to earliest-deadline-first scheduling in meeting the deadlines associated with processes the scheduling algorithms for hard real-time systems  such as rate monotonic scheduling and earliest-deadline-first scheduling  are presented in liu and in earlier chapters  we generally concerned ourselves with how operating systems handle conventional data  such as text files  programs  binaries  wordprocessing documents  and spreadsheets however  operating systems may have to handle other kinds of data as well a relatively recent trend in technology is the incorporation of multimedia data into computer systems multimedia data consist of continuous-media  audio and video  data as well as conventional files continuous-media data differ from conventional data in that continuous-media data-such as frames of video-must be delivered  streamed  according to certain time restrictions  for example  30 frames per second   in this chapter  we explore the demands of continuous-media data we also discuss in more detail how such data differ from conventional data and how these differences affect the design of operating systems that support the requirements of multimedia systems chapter objectives to identify the characteristics of multimedia data to examine several algorithms used to compress multimedia data to explore the operating  system requirements of multimedia data  including cpu and disk scheduling and network management 20.1 what is multimedia the term multimedia describes a wide range of applications that are in popular use today these include audio and video files such as mp3 audio files  dvd movies  and short video clips of movie previews or news stories downloaded over the internet multimedia applications also include live web casts  broadcast over the world wide web  of speeches or sporting events and even live webcams that allow a viewer in manhattan to observe customers at a cafe in paris multimedia applications need not be either audio or video ; rather  a multimedia application often includes a combination of both for example  a movie may consist of separate audio and video tracks 779 780 chapter 20 nor must multimedia applications be delivered only to desktop personal computers increasingly  they are being directed toward smaller devices  including personal digital assistants  pdas  and cellular telephones for example  a stock trader may have stock quotes delivered in real time to her pda in this section  we explore several characteristics of multimedia systems and examine how multimedia files can be delivered from a server to a client system we also look at common standards for representing multimedia video and audio files 20.1.1 media delivery multimedia data are stored in the file system just like any other data the major difference between a regular file and a multimedia file is that the multimedia file must be accessed at a specific rate  whereas accessing the regular file requires no special timing let 's use video as an example of what we mean by rate video is represented by a series of images  formally known as that are displayed in rapid succession the faster the frames are displayed  the smoother the video appears in general  a rate of 24 to 30 frames per second is necessary for video to appear smooth to human eyes  the eye retains the image of each frame for a short time after it has been presented  a characteristic known as a rate of 24 to 30 frames per second is fast enough to appear continuous  a rate lower than 24 frames per second will result in a choppy-looking presentation the video file must be accessed from the file system at a rate consistent with the rate at which the video is being we refer to data with associated rate requirements as multimedia data may be delivered to a client either from the local file system or from a remote server when the data are delivered from the local file system  we refer to the delivery as examples include watching a dvd on a laptop computer or listening to an mp3 audio file on a handheld mp3 player in these cases  the data comprise a regular file that is stored on the local file system and played back  that is  viewed or listened to  from that system multimedia files may also be stored on a remote server and delivered to a client across a network using a technique known as a client may be a personal computer or a smaller device such as a handheld computer  pda  or cellular telephone data from live continuous media -such as live webcams -are also streamed from a server to clients there are two types of streaming techniques  progressive download and real-time streaming with a download  a media file containing audio or video is downloaded and stored on the client 's local file system as the file is being downloaded  the client is able to play back the media file without having to wait for the file to be downloaded in its entirety because the media file is ultimately stored on the client system  progressive download is most useful for relatively small media files  such as short video clips differs from progressive download in that the media file is streamed to the client but is only played -and not stored -by the client because the media file is not stored on the client system  real-time streaming is preferable to progressive download for media files that might be too large 20.1 781 for storage on the system  such as long videos and internet radio and tv broadcasts both progressive download and real-time streaming may allow a client to move to different points in the stream  just as you can use the fast-forward and rewind operations on a dvd controller to move to different points in the dvd disc for example  we could move to the end of a 5-minute streaming video or replay a certain section of a movie clip the ability to move around within the media stream is known as two types of real-time streaming are available  live streaming and ondemand streaming is used to deliver an event  such as a concert or a lecture  live as it is actually occurring a radio program broadcast over the internet is an example of a live real-time stream in fact  one of the authors of this text regularly listens to a favorite radio station from vermont while at his home in utah as it is streamed live over the internet live real-time streaming is also used for applications such as live webcams and video conferencing due to its live delivery  this type of real-time streaming does not allow clients random access to different poil1.ts in the media stream in addition  live delivery means that a client who wishes to view  or listen to  a particular live stream already in progress will join the session late ; ' thereby missing earlier portions of the stream the same thing happens with a live tv or radio broadcast if you start watching the 7  00p.m news at 7  10p.m  you will have missed the first 10 minutes of the broadcast on-demand streaming is used to deliver media streams such as full-length movies and archived lectures the difference between live and on-demand streaming is that on-demand streaming does not take place as the event is occurring thus  for example  whereas watching a live stream is like watching a news broadcast on tv  watching an on-demand stream is like viewing a movie on a dvd player at some convenient time-there is no notion of arriving late depending on the type of on-demand streaming  a client may or may not have random access to the stream examples of well-known streaming media products include realplayer  apple quicktime  and windows media player these products include both servers that stream the media and client media players that are used for playback 20.1.2 characteristics of multimedia systems the demands of multimedia systems are unlike the demands of traditional applications in general  multimedia systems may have the following characteristics  multimedia files can be quite large for example  a 100-minute mpeg-1 video file requires approximately 1.125gb of storage space ; 100 minutes of high-defuution television  hdtv  requires approximately 15 gb of storage a server storing hundreds or thousands of digital video files may thus require several terabytes of storage continuous media may require very high data rates consider digital video  in which a frame of color video is displayed at a resolution of 800 x 600 if we use 24 bits to represent the color of each pixel  which allows us to have 224  or roughly 16 million  different colors   a single 782 chapter 20 20.2 frame requires 800 x 600 x 24 = 11,520  000 bits of data if the frames are displayed at a rate of 30 frames per second  a bandwidth in excess of 345 mbps is required multimedia applications are sensitive to timing delays during playback once a continuous-media file is delivered to a client  delivery must continue at a certain rate during playback of the media ; otherwise  the listener or viewer will be subjected to pauses during the presentation 20.1.3 operating-system issues for a computer system to deliver continuous-media data  it must guarantee the specific rate and timing requirements-also known as of or qos  requirements-of continuous media providing these qos guarantees affects several components in a computer system and influences such operating-system issues as cpu scheduling  disk scheduling  and network management specific examples include the following  compression and decoding may require significant cpu processing multimedia tasks must be scheduled with certain priorities to ensure meeting the deadline requirements of continuous media similarly  file systems must be efficient to meet the rate requirements of continuous media network protocols must support bandwidth requirements while minimizing delay and jitter  which we discuss further later in the chapter   in later sections  we explore these and several other issues related to qos first  however  we provide an overview of various techniques for compressing multimedia data as suggested above  compression makes significant demands on the cpu because of the size and rate requirements of multimedia systems  multimedia files are often compressed from their original form to a much smaller form once a file has been compressed  it takes up less space for storage and can be delivered to a client more quickly compression is particularly important when the content is beilcg streamed across a network cormection in discussing file compression  we often refer to the which is the ratio of the original file size to the size of the compressed file for example  an 800-kb file that is compressed to 100 kb has a compression ratio of 8  1 once a file has been compressed  encoded   it must be decompressed before it can be accessed a feature of the algorithm used to compress the file affects the later decompression compression algorithms are classified as either or with lossy compression  some of the original data are lost when the file is decoded  whereas lossless compression ensures that 20.2 783 the compressed file can always be restored to its original form in generat lossy techniques provide much higher cone pression ratios obviously  though  only certain types of data can tolerate lossy compression-namely  images  audio  and video lossy compression algorithms often work by eliminating certain data  such as very high or low frequencies that a human ear can not detect some lossy compression algorithms used on video operate by storing only the differences between successive frames lossless algorithms are used for compressing text files  such as computer programs  for example  files   because we want to restore these compressed files to their state a number of different lossy compression schemes for continuous-media data are commercially available in this section  we cover one used by the moving picture experts group  better known as mpeg mpeg refers to a set of file formats and compression standards for digital video because digital video often contains an audio portion as well  each of the standards is divided into three layers layers 3 and 2 apply to the audio and video portions of the media file layer 1  known as the layer  contains timing information to allow the mpeg player to multiplex the audio and video portions so that they are synchronized during playback there are three major mpeg standards  mpeg-1  mpeg-2  and mpeg-4 mpeg-1 is used for digital video and its associated audio stream the resolution of mpeg-1 is 352 x 240 at 30 frames per second with a bitrate of up to 1.5 mbps this provides a quality slightly lower than that of conventional vcr videos mp3 audio files  a popular medium for storing music  use the audio layer  layer 3  of mpeg-1 for video  mpeg-1 can achieve a compression ratio of up to 200  1  although in practice compression ratios are much lower because mpeg-1 does not require high data rates  it is often used to download short video clips over the internet mpeg-2 provides better quality than mpeg-1 and is used for compressing dvd movies and digital television  including high-definition television  or hdtv   mpeg-2 identifies a number of levels and profiles of video compression the refers to the resolution of the video ; the characterizes the video 's quality in general  the higher the level of resolution and the better the quality of the video  the higher the required data rate typical bit rates for mpeg-2 encoded files are 1.5 mbps to 15 mbps because mpeg-2 requires higher rates  it is often lmsuitable for delivery of video across a network and is generally used for local playback mpeg-4 is the most recent of the standards and is used to transmit audio  video  and graphics  including two-dimensional and three-dimensional animation layers animation makes it possible for end users to interact with the file during playback for example  a potential home buyer can download an mpeg-4 file and take a virtual tour through a home she is considering purchasing  moving from room to room as she chooses another appealing feature of mpeg-4 is that it provides a scalable level of quality  allowing delivery over relatively slow network connections such as 56-i bps modems or over high-speed local area networks with rates of several megabits per second furthermore  by providing a scalable level of quality  mpeg-4 audio and video files can be delivered to wireless devices  including handheld computers  pdas  and cell phones all three mpeg standards discussed here perform lossy compression to achieve high compression ratios the fundamental idea behind mpeg 784 chapter 20 20.3 compression is to store the differences between successive frames we do not cover further details of how mpeg performs compression but rather encourage the interested reader to consult the bibliographical notes at the end of this chapter as a result of the characteristics described in section 20.1.2  multimedia applications often require levels of service from the operating system that differ from the requirements of traditional applications  such as word processors  compilers  and spreadsheets tin1.ing and rate requirements are perhaps the issues of foremost concern  as the playback of audio and video data demands that the data be delivered within a certain deadline and at a continuous  fixed rate traditional applications typically do not have such time and rate constraints tasks that request data at constant intervals-or known as ' ' ; ~ ' ' ' for example  an mpeg-1 video might require a rate of 30 frames per second during playback maintaining this rate requires that a frame be delivered approximately every l/301h  or 3.34 hundredths  of a second to put this in the context of deadlines  let 's assume that frame f1 succeeds frame fi in the video playback and that frame fi was displayed at time t0 the deadline for displaying frame f i is 3.34 hundredths of a second after time t0 if the operating system is unable to display the frame by this deadline  the frame will be omitted from the stream as mentioned earlier  rate requirements and deadlines are known as quality of service  qos  requirements there are three qos levels  the system makes a best-effort attempt to satisfy the requirements ; however  no guarantees are made this level treats different types of traffic in different ways  giving certain traffic streams higher priority than other streams however  just as with best-effort service  no guarantees are made the quality-of-service requirements are guaranteed traditional operating systems-the systems we have discussed in this text so far-typically provide only best-effort service and rely on ' ' ' ' that is  they simply assume that the total amount of resources available will tend to be larger than a worst-case workload would demand if demand exceeds resource capacity  manual intervention must take place  and a process  or several processes  must be removed from the system however next-generation multimedia systems can not make such assumptions these systems must provide continuous-media applications with the guarantees made possible by hard qos therefore  in the remainder of this discussion  when we refer to qos  we mean hard qos next  we explore various techniques that enable multimedia systems to provide such service-level guarantees there are a number of parameters defining qos for multimedia applications  including the following  20.3 785 throughput is the total amount of work done during a certain interval for multimedia applications  throughput is the required data rate  delay refers to the elapsed time from when a request is first submitted to when the desired result is produced for example  the time from when a client requests a media stream to when the stream is delivered is the delay jitter is related to delay ; but whereas delay refers to the time a client must wait to receive a stream  jitter refers to delays that occur during playback of the stream certain multimedia applications  such as on-demand real-time streaming  can tolerate this sort of delay jitter is generally considered unacceptable for continuous-media applications  howeve1 ~ because it may mean long pauses-or lost frames-during playback clients can often compensate for jitter by buffering a certain amount of data-say  5 seconds ' worth-before beginning playback reliability refers to how errors are handled during transmission and processing of continuous media errors may occur due to lost packets in the network or processing delays by the cpu in these-and other-scenarios  errors can not be corrected  since packets typically arrive too late to be useful the quality of service may be between the client and the server for example  continuous-media data may be compressed at different levels of quality  the higher the quality  the higher the required data rate a client may negotiate a specific data rate with a server  thus agreeing to a certain level of quality during playback furthermore  many media players allow the client to configure the player according to the speed of the client 's connection to the network this allows a client to receive a streaming service at a data rate specific to a particular connection thus  the client is negotiating quality of service with the content provider to provide qos guarantees  operating systems often use which is simply the practice of admitting a request for service only if the server has sufficient resources to satisfy the request we see admission control quite often in our everyday lives for example  a movie theater only admits as many customers as it has seats in the theater  there are also many situations in everyday life where admission control is not practiced but would be desirable !  if no admission-control policy is used in a multimedia environment  the demands on the system might become so great that the system becomes unable to meet its qos guarantees in chapter 6  we discussed using semaphores as a method of implementing a simple admission-control policy in this scenario  there exist a finite number of nonshareable resources when a resource is requested  we grant the request only if sufficient resources are available ; otherwise  the requesting process must wait until a resource becomes available we can use semaphores to implement an admission-control policy by first initializing a semaphore to the number of resources available every request for a resource is made through a wait   operation on the semaphore ; a resource is released with an invocation of signal   on the semaphore once all resources are in use  subsequent calls to wait   block until there is a corresponding signal    786 chapter 20 20.4 figure 20.1 resources on a file server a common technique for implementing admission control is to use for example  resources on a file server may include the cpu  memory  file system  devices  and network  figure 20.1   note that resources may be either exclusive or shared and that there may be either single or multiple instances of each resource type to use a resource  a client must make a reservation request for the resource il1 advance if the request can not be granted  the reservation is denied an admission-control scheme assigns a to each type of resource requests for resources have associated qos requirements-for example  required data rates when a request for a resource arrives  the resource manager determines if the resource can meet the qos demands of the request if it can not  the request may be rejected  or a lower level of qos may be negotiated between the client and the server if the request is accepted  the resource manager reserves the resources for the requesting client  thus assuring the client that the desired qos requirements will be met in section 20.7.2  we examine the admission-control algorithm used to ensure qos guarantees in the cineblitz multimedia storage server 19  which covers real-time systems  we distinguished between and soft real-time systems simply give scheduling priority to critical processes a soft real-time system ensures that a critical process will be given preference over a noncritical process but provides no guarantee as to when the critical process will be scheduled a typical requirement of continuous media  however  is that data must be delivered to a client by a certain deadline ; data that do not arrive by the deadline are unusable multimedia systems thus require hard real-time scheduling to ensure that a critical task will be serviced withii1 a guaranteed period of time 20.5 20.5 787 another scheduling issue concerns whether a scheduling algorithm uses or distinction first discussed in chapter 5 the difference between the two is that the priority of a process will remain unchanged if the scheduler assigns it a static priority scheduling algorithms that assign dynamic priorities allow priorities to change over time most operating systems use dynamic priorities when scheduling non-real-time tasks with the intention of giving higher priority to interactive processes however  when scheduling real-time tasks  most systems assign static priorities  as the design of the scheduler is less complex several of the real-time scheduling strategies discussed in section 19.5 can be used to meet the rate and deadline qos requirements of continuous-media applications we first discussed disk scheduling in chapter 12 there  we focused primarily on systems that handle conventional data ; for these systems  the scheduling goals are fairness and throughput as a result  most traditional disk schedulers employ some form of the scan  section 12.4.3  or c-scan  section 12.4.4  algorithn l continuous-media files  however  have two constraints that conventional data files generally do not have  timing deadlines and rate requirements these two constraints must be satisfied to preserve qos guarantees  and diskscheduling algorithms must be optimized for the constraints unfortunately  these two constraints are often in conflict continuous-media files typically require very high disk-bandwidth rates to satisfy their data-rate requirements because disks have relatively low transfer rates and relatively high latency rates  disk schedulers must reduce the latency times to ensure high bandwidth however  reducing latency times may result in a scheduling policy that does not prioritize according to deadlines in this section  we explore two diskscheduling algorithms that meet the qos requirements for continuous-media systems 20.5.1 earliest-deadline-first scheduling we first presented the earliest-deadline-first  edf  algorithm in section 19 .5.2 as an example of a cpu-scheduling algorithm that assigns priorities according to deadlines edf can also be used as a disk-scheduling algorithm ; in this context  edf uses a queue to order requests according to the time each request must be completed  its deadline   edf is similar to shortest-seek-time-first  sstf   discussed in section 12.4.2  except that instead of servicing the request closest to the current cylinder  we service requests according to deadline-the request with the closest deadline is serviced first a problem with this approach is that servicing requests strictly according to deadline may result in higher seek tim.es  since the disk heads may move randomly throughout the disk without any regard to their current position for example  suppose a disk head is currently at cylinder 75 and the queue of cylinders  ordered according to deadlines  is 98  183  105 under strict edf scheduling  the disk head will move from 75  to 98  to 183  and then back to 788 chapter 20 105 note that the head passes over cylinder 105 as it travels from 98 to 183 it is possible that the disk scheduler could have serviced the request for cylinder 105 en route to cylinder 183 and still preserved the deadline requirement for cylinder 183 20.5.2 scan-edf scheduling the fundamental problem with strict edf scheduling is that it ignores the position of the read-write heads of the disk ; it is possible that the heads will swil g wildly to and fro across the disk  leading to unacceptable seek times that negatively affect disk throughput recall that this is the same issue faced with fcfs scheduling  section 12.4.1   in the context of cpu scheduling  we can address this issue by adopting scan schedulil g  whereil the disk arm moves in one direction across the disk  servicing requests according to their proximity to the current cylinder once the disk arm reaches the end of the disk  it begins moving in the reverse direction this strategy optimizes seek times scan-edf is a hybrid algorithm that combines edf with scan scheduling scan-edf starts with edf ordering but services requests with the same deadline usil g scan order what if several requests have different deadlines that are relatively close together in this case  scan-edf may batch requests  usil g scan ordering to service requests in the same batch there are many techniques for batching requests with similar deadlines ; the only requirement is that reordering requests within a batch must not prevent a request from being serviced by its deadline if deadlines are equally distributed  batches can be organized in groups of a certain size-say  10 requests per batch another approach is to batch requests whose deadlines fall within a given time threshold-say  100 milliseconds let 's consider an example in which we batch requests in this way assume we have the followil g requests  each with a specified deadline  in milliseconds  and a requested cylinder  ~ ; ' ' ; ! 1  -'  ~  ~ '   ; ~  ;    '  r ' k'j ~ lr         /1  -1 r ~ 'i ! ~  ~ ~ ~ ' k ;  ci ; h  a 150 25 b 201 112 c 399 95 d 94 31 e 295 185 f 78 85 g 165 150 h 125 101 i 300 85 j 210 90 suppose we are at time0  the cylinder currently being serviced is 50  and the disk head is moving toward cylinder 51 according to our batching scheme  requests d and f will be in the first batch ; a g  and h in batch 2 ; b  e  and j il batch 3 ; and c and i il the last batch requests within each batch will 20.6 789 be ordered according to scan order thus  in batch 1  we will first service request f and then request d note that we are moving downward in cylinder nun1.bers  fron 85 to 31 in batch 2  we first service request a ; then the heads begin moving upward in cylinders  servicing requests hand then g batch 3 is serviced in the order e  b  j requests i and care serviced in the final batch perhaps the foremost qos issue with multimedia systems concerns preserving rate requirements for example  if a client wishes to view a video compressed with mpeg-1  the quality of service greatly depends on the system 's ability to deliver the frames at the required rate our coverage of issues such as cpu and disk-scheduling algorithms has focused on how these techniques can be used to better meet the quality-ofservice requirements of multimedia applications however  if the media file is being streamed over a network-perhaps the internet-issues relating to how the network delivers the multimedia data can also significantly affect how qos demands are met in this section  we explore several network issues related to the unique demands of continuous media before we proceed  it is worth noting that computer networks in general -and the internet in particular currently do not provide network protocols that can ensure the delivery of data with timing requirements  there are some proprietary protocols-notably those running on cisco routers-that do allow certain network traffic to be prioritized to meet qos requirements such proprietary protocols are not generalized for use across the internet and therefore do not apply to our discussion  when data are routed across a network  it is likely that the transmission will encounter congestion  delays  and other network traffic issues-issues that are beyond the control of the originator of the data for multimedia data with timing requirements  any timing issues must be synchronized between the end hosts  the server delivering the content and the client playing it back one protocol that addresses timing issues is the  real-thne  rrrf '   rtp is an internet standard for delivering real-time data  including audio and video it can be used for transporting media formats such as mp3 audio files and video files compressed using mpeg rtf does not provide any qos guarantees ; rathe1 ~ it provides features that allow a receiver to remove jitter introduced by delays and congestion in the network in following sections  we consider two other approaches for handling the unique requirements of continuous media 20.6.1 unicasting and multicasting in general  there are three methods for delivering content from a server to a client across a network  the server delivers the content to a single client if the content is being delivered to more than one client  the server must establish a separate unicast for each client 790 chapter 20 '  ' ',''' ' 'nh  the server delivers the content to all clients  regardless of whether they wish to receive the content or not the server delivers the content to a group of receivers that indicate they wish to receive the content ; this method lies somewhere between unicasting and broadcasting an issue with unicast delivery is that the server must establish a separate unicast session for each client this seems especially wasteful for live real-time streaming  where the server must make several copies of the same content  one for each client obviously  broadcasting is not always appropriate  as not all clients may wish to receive the stream  suffice it to say that broadcasting is typically only used across local area networks and is not possible across the public internet  multicasting appears to be a reasonable compromise  since it allows the server to deliver a single copy of the content to all clients indicating that they wish to receive it the difficulty with multicasting from a practical standpoint is that the clients must be physically close to the server or to intermediate routers that relay the content from the originating server if the route from the server to the client must cross intermediate routers  the routers must also support multicasting if these conditions are not met  the delays incurred during routing may result in violation of the timing requirements of the continuous media in the worst case  if a client is connected to an intermediate router that does not support multicasting  the client will be unable to receive the multicast stream at all ! currently  most streaming media are delivered across unicast channels ; however  multicasting is used in various areas where the organization of the server and clients is known in advance for example  a corporation with several sites across a country may be able to ensure that all sites are connected to multicasting routers and are within reasonable physical proximity to the routers the organization will then be able to deliver a presentation from the chief executive officer using multicasting 20.6.2 real-time streaming protocol in section 20.1.1  we described some features of streaming media as we noted there  users may be able to randomly access a media stream  perhaps replaying or pausing  as they would with a dvd controller how is this possible to answer this question  let 's consider how streaming media are delivered to clients one approach is to stream the media from a standard web server using the hypertext transport protocol  or http-the protocol used to deliver documents from a web server quite often  clients use a such as quicktime  realplaye1 ~ or windows media player  to play media streamed from a standard web server typically  the client first requests a  ,e  cajj.u   which contains the location  possibly identified by a uniform resource locatm ~ or url  of the streaming media file this metafile is delivered to the client 's web browser  and the browser then starts the appropriate media player according to the type of media specified by the metafile for example  a real audio stream would require the realplayer  while the windows media player would be used to play back streaming windows media the media player then contacts the web server and requests the streaming media the stream 20.6 791 figure 20.2 streaming media from a conventional web server is delivered from the web server to the media player using standard http requests this process is outlined in figure 20.2 the problem with delivering streaming media from a standard web server is that http is considered a protocol ; thus  a web server does not maintain the state  or status  of its connection with a client as a result  it is difficult for a client to pause during the delivery of streaming media content  since pausing would require the web server to know where in the stream to begin when the client wished to resume playback an alternative strategy is to use a specialized streaming server that is designed specifically for streaming media one protocol designed for communication between streaming servers and media players is known as the or  the significant advantage rtsp provides over http is a connection between the client and the server  which allows the client to pause or seek to random positions in the stream ~ during playback delivery of streaming media using rtsp is similar to delivery using http  figure 20.2  in that the metafile is delivered using a conventional web server however  rather than a web server  the streami.j.1.g media is delivered from a streaming server using the rtsp protocol the operation of rtsp is shown in figure 20.3 rtsp defines several commands as part of its protocol ; these commands are sent from a client to an rtsp streaming server the commands i.j.1.clude the following   the server allocates resources for a client session   the server delivers a stream to a client session established from a setup command the server suspends delivery of a stream but maintains the resources for the session the server breaks down the connection and frees up resources allocated for the session 792 chapter 20 20.7 figure 20.3 real-time streaming protocol  rtsp   the commands can be illustrated with a state machine for the server  as shown in figure 20.4 as you can see in the figure  the rtsp server may be in one of three states  and transitions between these three states are triggered when the server receives one of the rtsp commands from the client using rtsp rather than http for streaming media offers several other advantages  but they are primarily related to networking issues and are therefore beyond the scope of this text we encourage interested readers to consult the bibliographical notes at the end of this chapter for sources of further information the cineblitz multimedia storage server is a high-performance media server that supports both continuous media with rate requirements  such as video and audio  and conventional data with no associated rate requirements  such as text and images   cineblitz refers to clients with rate requirements as whereas have no rate constraints cine blitz guarantees to meet the rate requirements of real-time clients by implementing an admission controller  admitting a client only if there are sufficient resources to allow data retrieval at the required rate in this section  we explore the cineblitz disk-schedulu1.g and admission-control algorithms setup play tear down pause figure 20.4 finite-state machine representing rtsp 20.7 793 20.7.1 disk scheduling the cineblitz disk scheduler services requests in at the beginning of each service cycle  requests are placed inc-scan  section 12.4.4   recall from our earlier discussions of c-scan that the disk heads move from one end of the disk to the other however  rather than reversing direction when they reach the end of the disk  as in pure scan disk scheduling  section 12.4.3   the disk heads move back to the beginr1ing of the disk 20.7.2 admission control the admission-control algorithm in cil eblitz must monitor requests from both real-time and non-real-time clients  ensuring that both classes of clients receive service furthermore  the admission controller must provide the rate guarantees required by real-time clients to ensure fairness  only a fraction p of time is reserved for real-time clients  while the remainder  1 p  is set aside for non-real-time clients here  we explore the admission controller for real-time clients only ; thus  the term client refers to a real-time client the admission controller in cineblitz monitors various system resources  such as disk bandwidth and disk latency  while keeping track of available buffer space the cineblitz admission controller admits a client only if there is enough available disk bandwidth and buffer space to retrieve data for the client at its required rate cineblitz queues requests for continuous media files  where r1  r2  r3    r11 are the requests and r ; is the required data rate for a given request r requests in the queue are served in cyclic order in rounds of time-length t  t being tpically measured in seconds   the scheme is using a technique known as double wherein a buffer is allocated for each request r ; of size 2 x t x r ;  during each cycle i  the server must for each request r j  retrieve the data from disk to buffer  i mod 2   transfer data from the   i + 1  mod 2  buffer to the client this process is illustrated in figure 20.5 for n clients  the total buffer space b required is n l2 x t x r ;     b i = l  20.1  the fundamental idea behil d the admission controller in cineblitz is to bound requests for entry into the queue according to the following criteria  the service time for each request is first estimated a request is admitted only if the sum of the estimated service times for all admitted requests does not exceed the duration of service cycle t lett x r ; bits be retrieved during a cycle for each real-time client r ; with rater ;  if r1  r2    r11 are the clients currently active in the system  then the admission controller must ensure that the total time for retrieving t x r1  t x r2    t x r11 794 chapter 20 0 f---1 double buffer total buffer space  b  figure 20.5 double buffering in cinebiitz bits for the corresponding real-time clients does not exceed t we explore the details of this admission policy in the remainder of this section if b is the size of a disk block  then the maximum number of disk blocks thatcanberetrievedforrequest r1c duringeachcycleis i  t xr ~ c  /bl + 1 the 1 in this formula comes from the fact that if t x yjc is less than b  then it is possible for t x r1c bits to span the last portion of one disk block and the beginning of another  causing two blocks to be retrieved we know that the retrieval of a disk block involves  a  a seek to the track contailling the block and  b  the rotational delay as the data in the desired track arrive under the disk head as described  cineblitz uses a c-scan disk-scheduling algorithm  so disk blocks are retrieved in the sorted order of their positions on the disk if tscek and trot refer to the worst-case seek and rotational delay times  the maximum latency incurred for servicil1g n requests is n  t x r ;  2 x tseek + l 1-b-l + 1 x trot 1 = 1  20.2  in this equation  the 2 x tseek component refers to the maximum disk-seek latency incurred in a cycle the second component reflects the sum of the retrievals of the disk blocks multiplied by the worst-case rotational delay if the transfer rate of the disk is r dislu then the time to h ansfer t x yic bits of data for request r1c is  t x r ~ c  /rdisk as a result  the total time for retrieving t x r1  t x r2    t x r11 bits for requests r1  r2    r11 is the sum of equation 20.2 and n t x r ; t ; raisk  20.3  therefore  the admission controller in cineblitz only admits a new client r if at least 2 x t x r ; bits of free buffer space are available for the client and the following equation is satisfied  n  t x r ;  n t x r ; 2 x tsee/c + l 1-b-l + 1 x trot + l ~ s t i = l i = l diok  20.4  20.8 795 multimedia applications are in common use in modern computer systems multimedia files include video and audio filesf which may be delivered to systems such as desktop computersf personal digital assistantsf and cell phones the primary distinction between multimedia data and conventional data is that multimedia data have specific rate and deadline requiren ents because multimedia files have specific timing requirementsf the data must often be compressed before delivery to a client for playback multimedia data may be delivered either from the local file system or from a multimedia server across a network connection using a technique known as streaming the timing requirements of multimedia data are known as qualityof service requirementsf and conventional operating systems often can not make quality-of-service guarantees to provide quality of servicef multimedia systems must provide a form of admission control whereby a system accepts a request only if it can meet the quality-of-service level specified by the request providing quality-of-service guarantees requires evaluating how an operating system performs cpu schedulingf disk schedulingf and network management both cpu and disk scheduling typically use the deadline requirements of a continuous-media task as a scheduling criterion network management requires the use of protocols that handle delay and jitter caused by the network as well as allowing a client to pause or move to different positions in the stream during playback 20.1 explain why the traditional internet protocols for transmitting data are not sufficient to provide the quality-of-service guarantees required for a multimedia system discuss what changes are required to provide the qos guarantees 20.2 contrast unicastingf multicastingf and broadcasting as techniques for delivering content across a computer network 20.3 assume that we wish to compress a digital video file using mpeg-1 technology the target bit rate is 1.5 mbps if the video is displayed at a resolution of 352 x 240 at 30 frames per second using 24 bits to represent each colorf what is the necessary compression ratio to achieve the desired bit rate 20.4 assume that a digital video file is being displayed at a rate of 30 frames per second ; the resolution of each frame is 640 x 480f and 24 bits are being used to represent each color assuming that no compression is being usedf what is the bandwidth necessary to deliver this file next assuming that the file has been compressed at a ratio of 200  1f what is the bandwidth necessary to deliver the compressed file 20.5 a multimedia application consists of a set containing 100 imagesf 10 minutes of videof and 10 minutes of audio the compressed sizes of the imagesf videof and audio are 500 mbf 550 mbf and 8 mbf respectively the images were compressed at a ratio of 15  1f and the video and 796 chapter 20 audio were compressed at 200  1 and 10  1  respectively what were the sizes of the images  video  and audio before compression 20.6 which of the following types of real-time streaming applications can tolerate delay which can tolerate jitter live real-time streaming on-demand real-time streaming 20.7 distinguish between progressive download and real-time streaming 20.8 the following table contains a number of requests with their associated deadlines and cylinders requests with deadlines occurring within 100 milliseconds of each other will be batched the disk head is currently at cylinder 94 and is moving toward cylinder 95 if scan-edf disk scheduling is used  how are the requests batched together  and what is the order of requests within each batch  o ,j,c ~ ;   ; ;      ;  ~    ' '  !  ;  ;  ~  '..ji  ; ; _ ~  3      ~ tp ~ li ~ ~ ~ ~  ~ r1 57 77 r2 300 95 r3 250 25 r4 88 28 r5 85 100 r6 110 90 r7 299 50 r8 300 77 r9 120 12 r10 212 2 20.9 repeat exercise 20.8  but this time batch requests that have deadlines occurring within 75 milliseconds of each other 20.10 what operating principle is used by the cine blitz system in performing admission control for requests for media files 20.11 consider two processes  p1 and p2  where p1 = 50  t1 = 25  p2 = 75  and t2 = 30 a can these two processes be scheduled using rate-monotonic scheduling illustrate your answer using a gantt chart b illustrate the schedulil1.g of these two processes using earliestdeadline first  edf  scheduling 797 fuhrt  1994  provides a general overview of multimedia systems topics related to the delivery of multimedia through networks can be found in kurose and ross  2005   operating-system support for multimedia is discussed in steinmetz  1995  and leslie et al  1996   resource management for resources such as processing capability and memory buffers is discussed in mercer et al  1994  and druschel and peterson  1993   reddy and wyllie  1994  give a good overview of issues relating to the use of i/0 in a multimedia system discussions regarding the appropriate programming model for developing multimedia applications are presented in regehr et al  2000   an admission control system for a rate-monotonic scheduler is considered in lauzac et al  2003   bolosky et al  1997  present a system for serving video data and discuss the schedule-management issues that arise in such a system the details of part nine we can now integrate the concepts described in this book by describing real operating systems two such systems are covered in great detaiilinux and windows xp we chose linux for several reasons  it is popular  it is freely available  and it represents a full-featured unix system this gives a student of operating systems an opportunity to read-and modifyrea/ operating-system source code we also cover windows xp in great detail this recent operating system from microsoft is gaining popularity  not only in the stand-alonemachine market  but also in the workgroup-server market we chose windows xp because it provides an opportunity for us to study a modern operating system that has a design and implementation drastically different from those of unix in addition  we briefly discuss other highly influential operating systems we have chosen the order of presentation to highlight the similarities and differences among the systems ; it is not strictly chronological and does not reflect the relative importance of the systems finally  we provide on-line coverage of three other systems the freebsd system is another unix system however  whereas linux combines features from several unix systems  freebsd is based on the bsd model of unix freebsd source code  like linux source code  is freely available the mach operating system is a modern operating system that provides compatibility with bsd unix windows is another modern operating system from microsoft for intel pentium and later microprocessors ; it is compatible with ms-dos and microsoft windows applications 21.1 this chapter presents an in-depth examination of the linux operating system by exam.ining a complete  real system  we can see how the concepts we have discussed relate both to one another and to practice linux is a version of unix that has gained popularity in recent years in this chapter  we look at the history and development of linux and cover the user and programmer interfaces that linux presents-interfaces that owe a great deal to the unix tradition we also discuss the internal methods by which linux implements these interfaces linux is a rapidly evolving operating system this chapter describes developments through the linux 2.6 kernel  which was released in late 2003 to explore the history of the unix operating system from which linux is derived and the principles upon which linux is designed to examine the linux process model and illustrate how linux schedules processes and provides interprocess communication to look at memory management in linux to explore how linux implements file systems and manages 110 devices linux looks and feels much like any other unix system ; indeed  unix compatibility has been a major design goal of the linux project however  linux is much younger than most unix systems its development began in 1991  when a finnish student  linus torvalds  wrote and christened linux  a small but self-contained kernel for the 80386 processor  the first true 32-bit processor in intel 's range of pc-compatible cpus early in its development  the linux source code was made available free on the internet as a result  linux 's history has been one of collaboration by many users from all around the world  corresponding almost exclusively over the internet from an initial kernel that partially implemented a small subset of 801 802 chapter 21 the unix system services  the linux system has grown to include much unix functionality in its early days  linux development revolved largely around the central operating-system kernel-the core  privileged executive that n1.anages all system resources and that interacts directly with the computer hardware we need much more than this kernel to produce a full operating systeitl  o course it is useful to make the distinction between the linux kernel and a linux system the is an entirely original piece of software developed from scratch by the linux con1.munity the as we know it today  includes a multitude of components  some written from scratch  others borrowed from other development projects  and still others created in collaboration with other teams the basic linux system is a standard environment for applications and user programming  but it does not enforce any standard means of managing the available functionality as a whole as linux has matured  a need has arisen for another layer of functionality on top of the linux system this need has been met by various linux distributions a includes all the standard components of the linux system  plus a set of administrative tools to simplify the initial installation and subsequent upgrading of linux and to manage installation and removal of other packages on the system a modern distribution also typically includes tools for management of file systems  creation and management of user accounts  administration of networks  web browsers  word processors  and so on 21.1.1 the linux kernel the first linux kernel released to the public was version 0.01  dated may 14  1991 it had no networking  ran only on 80386-compatible intel processors and pc hardware  and had extremely limited device-driver support the virtual memory subsystem was also fairly basic and included no support for memorymapped files ; however  even this early incarnation supported shared pages with copy-on-write the only file system supported was the minix file system -the first linux kernels were cross-developed on a minix platform however  the kernel did implement proper unix processes with protected address spaces the next milestone version  linux 1.0  was released on march 14  1994 this release culminated three years of rapid development of the limlx kernel perhaps the single biggest new feature was networking  1.0 included support for unix 's standard tcp lip networking protocols  as well as a bsd-compatible socket interface for networking programming device-driver support was added for running ip over an ethernet or  using ppp or slip protocols  over serial lines or modems the 1.0 kernel also included a new  much enhanced file system without the limitations of the original minix file system and supported a range of scsi controllers for high-performance disk access the developers extended the virtual memory subsystem to support paging to swap files and memory mapping of arbitrary files  but only read-only memory mapping was implemented in 1.0   a range of extra hardware support was also included in this release although still restricted to the intel pc platform  hardware support had grown to include floppy-disk and cd-rom devices  as well as sound cards  a range of mice  and international keyboards floating-point emulation was provided 21.1 803 in the kernel for 80386 users who had no 80387 math coprocessor ; system v unix-style inclllding shared memory  semaphores  and message queues  was implemented simple support for dynamically loadable and unloadable kernel modules was supplied as well at this point  development started on the 1.1 kernel stream  but numerous bug-fix patches were released subsequently against 1.0 a pattern was adopted as the standard numbering convention for linux kernels kernels with an odd minor-version number  such as 1.1  1.3  and2.1  are evennumbered minor-version numbers are stable updates against the stable kernels are intended only as remedial versions  whereas the development kernels may include newer and relatively untested functionality in march 1995  the 1.2 kernel was released this release did not offer nearly the same improvement in functionality as the 1.0 release  but it did support a much wider variety of hardware  including the new pci hardware bus architecture developers added another pc-specific feature-support for the 80386 cpu 's virtual8086 mode-to allow emulation of the dos operating system for pc computers they also updated the networking stack to provide support for the ipx protocol and made the ip implementation more complete by including accounting and firewalling functionality the 1.2 kernel was the final pc-only linux kernel the source distribution for linux 1.2 included partially implemented support for sparc  alpha  and mips cpus  but full integration of these other architectures did not begin until after the 1.2 stable kernel was released the linux 1.2 release concentrated on wider hardware support and more complete implementations of existing functionality much new functionality was under development at the time  but integration of the new code into the main kernel source code had been deferred until after the stable 1.2 kernel had been released as a result  the 1.3 development stream saw a great deal of new functionality added to the kernel this work was finally released as linux 2.0 in jmce 1996 this release was given a major version-number increment on account of two major new capabilities  support for multiple architectures  including a 64-bit native alpha port  and support for multiprocessor architectures linux distributions based on 2.0 are also available for the motorola 68000-series processors and for sun 's sparc systems a derived version of linux running on top of the mach microkernel also runs on pc and powermac systems the changes in 2.0 did not stop there the memory-management code was substantially improved to provide a unified cache for file-system data independent of the caching of block devices as a result of this change  the kernel offered greatly increased file-system and virtual memory performance for the first time  file-system caching was extended to networked file systems  and writable memory-mapped regions also were supported the 2.0 kernel also included much improved tcp /ip performance  and a number of new networking protocols were added  including apple talk  ax.25 an'lateur radio networking  and isdn support the ability to mount remote netware and smb  microsoft lanmanager  network volumes was added other major improvements in 2.0 were support for internal kernel threads  for handling dependencies between loadable modules  and for automatic loading of modules on demand dynamic configuration of the kernel at run time was much improved through a new  standardized configuration interface 804 chapter 21 additional new features included file-system quotas and posix-compatible real-time process-scheduling classes improvements continued with the release of linux 2.2 in january 1999 a port for ultrasparc systems was added networking was enhanced with more flexible firewalling  better routing and traffic management  and support for tcp large window and selective acks acorn  apple  and nt disks could now be read  and nfs was enhanced and a kernel-mode nfs daemon added signal handling  interrupts  and some i/0 were locked at a finer level than before to improve symmetric multiprocessor  smp  performance advances in the 2.4 and 2.6 releases of the kernel include increased support for smp systems  journaling file systems  and enhancements to the memorymanagement system the process scheduler was modified in version 2.6  providing an efficient 0  1  scheduling algorithm in addition  the limix 2.6 kernel is now preemptive  allowing a process to be preempted while running in kernel mode 21.1.2 the linux system in many ways  the linux kernel forms the core of the linux project  but other components make up the complete linux operating system whereas the linux kernel is composed entirely of code written from scratch specifically for the linux project  much of the supporting software that makes up the linux system is not exclusive to linux but is common to a number of unix-like operating systems in particular  linux uses many tools developed as part of berkeley 's bsd operating system  mit 's x window system  and the free software foundation 's gnu project this sharing of tools has worked in both directions the main system libraries of linux were originated by the gnu project  but the linux community greatly improved the libraries by addressing omissions  li1.efficiencies  and bugs other components  such as the l were already of sufficiently high quality to be used directly in linux the networkingadministration tools under linux were derived from code first developed for 4.3 bsd  but more recent bsd derivatives  such as freebsd  have borrowed code from linux in return examples include the intel floating-point-emulation math library and the pc sound-hardware device drivers the linux system as a whole is maintained by a loose network of developers collaborating over the internet  with small groups or individuals having responsibility for maintaining the integrity of specific components a small number of public internet file-transfer-protocol  ftp  archive sites act as de facto standard repositories for these components the document is also maintained by the linux community as a means of ensuring compatibility across the various system components this standard specifies the overall layout of a standard linux file system ; it determines under which directory names configuration files  libraries  system binaries  and run-time data files should be stored 21.1.3 linux distributions in theory  anybody can install a linux system by fetching the latest revisions of the necessary system components from the ftp sites and compiling them in linux 's early days  this operation was often precisely what a linux user 21.1 805 had to carry out as linux has matured  however  various individuals and groups have attempted to make this job less painful by providing standard  precompiled sets of packages for easy installation these collections  or distributions  include much more than just the basic linux system they typically include extra system-installation and management utilities  as well as precompiled and ready-to-install packages of many of the common unix tools  such as news servers  web browsers  text-processing and editing tools  and even games the first distributions managed these packages by simply providing a means of unpacking all the files into the appropriate places one of the important contributions of modem dish ibutions  however  is advanced package management today ' s linux distributions include a package-tracking database that allows packages to be installed  upgraded  or removed painlessly the sls distribution  dating back to the early days of linux  was the first collection of linux packages that was recognizable as a complete distribution although it could be installed as a single entity  sls lacked the packagemanagement tools now expected of linux distributions the distribution represented a great improvement in overall quality  even though it also had poor package management ; in fact  it is still one of the most widely installed distributions in the linux community since slackware ' s release  many commercial and noncommercial limlx distributions have become available and are particularly popular distributions ; the first comes from a commercial linux support company and the second from the free-software linux community other commercially supported versions of linux include distributions from cz,ldera  and a large linux following in germany has resulted in several dedicated german-language distributions  including versions from and there are too many linux distributions in circulation for us to list all of them here the variety of distributions does not prohibit compatibility across linux distributions  however the rpm package file format is used  or at least understood  by the majority of distributions  and commercial applications distributed in this format can be installed and run on any distribution that can accept rpm files 21.1.4 linux licensing the linux kernel is distributed under the gnu general public license  gpl   the terms of which are set out the free software fmmdation linux is not public-domain software implies that the authors have waived copyright rights in the software  but copyright rights in linux code are still held by the code 's various authors linux is free software  however  in the sense that people can copy it  modify it  use it in any manner they want  and give away their own copies  without any restrictions the main implications of linux 's licensing terms are that nobody using liimx  or creating a derivative of linux  a legitimate exercise   can make the derived product proprietary software released under the gpl camwt be redistributed as a binary-only product if you release software that includes any components covered by the gpl  then  under the gpl  you must make source code available alongside any binary distributions  this resh iction does 806 chapter 21 21.2 not prohibit ntaking-or even selling-binary-only software distributions  as long as anybody who receives binaries is also given the opportunity to get source code for a reasonable distribution charge  in its overall design  linux resembles any other traditional  nonmicrokernel unix implementation it is a multiuser  multitasking system with a full set of unix-compatible tools linux 's file system adheres to traditional unix semantics  and the standard unix networking model is implemented fully the internal details of linux ' s design have been influenced heavily by the history of this operating system 's development although linux runs on a wide variety of platforms  it was developed exclusively on pc architecture a great deal of that early development was carried out by individual enthusiasts  rather than by well-funded development or research facilities  so from the start linux attempted to squeeze as much functionality as possible from limited resources today  linux can run happily on a multiprocessor machine with hundreds of megabytes of main memory and many gigabytes of disk space  but it is still capable of operating usefully in under 4 mb of ram as pcs became more powerful and as memory and hard disks became cheaper  the original  minimalist limix kernels grew to implement more unix functionality speed and efficiency are still important design goals  but much recent and current work on linux has concentrated on a third major design goal  standardization one of the prices paid for the diversity of unix implementations currently available is that source code written for one may not necessarily compile or run correctly on another even when the same system calls are present on two different unix systems  they do not necessarily behave in exactly the same way the posix standards comprise a set of specifications for different aspects of operating-system behavior there are posix documents for common operating-system functionality and for extensions such as process threads and real-time operations linux is designed to be compliant with the relevant posix documents ; at least two linux distributions have achieved official posix certification because it gives standard interfaces to both the programmer and the user  linux presents few surprises to anybody familiar with unix we do not detail these interfaces here the sections on the programmer interface  section a.3  and user interface  section a.4  of bsd apply equally well to linux by default  howeve1 ~ the linux programming interface adheres to svr4 unix semantics  rather than to bsd behavior a separate set of libraries is available to implement bsd semantics in places where the two behaviors differ significantly many other standards exist in the unix world  but full certification of linux with respect to these standards is sometimes slowed because certification is often available only for a fee  and the expense involved in certifying an operating system 's compliance with most standards is substantial however  supporting a wide base of applications is important for any operating system  so implementation of standards is a major goal for linux development  even if the implementation is not formally certified in addition to the basic posix 21.2 807 standard  linux currently supports the posix threading extensions-pthreads -and a subset of the posix extensions for real-time process control 21.2.1 components of a linux system the linux system is composed of three main bodies of code  in line with most traditional unix implementations  kernel the kernel is responsible for maintaining all the important abstractions of the operating system  including such things as virtual memory and processes system libraries the system libraries define a standard set of functions through which applications can interact with the kernel these functions implement much of the operating-system functionality that does not need the full privileges of kernel code system utilities the system utilities are programs that perform individual  specialized management tasks some system utilities may be invoked just once to initialize and configure some aspect of the system ; othersknown as daemons in unix terminology -may run permanently  handling such tasks as responding to incoming network connections  accepting logon requests from terminals  and updating log files figure 21.1 illustrates the various components that make up a full linux system the most important distinction here is between the kernel and everything else all the kernel code executes in the processor 's privileged mode with full access to all the physical resources of the computer linux refers to this privileged mode as kernel under linux  no user-mode code is built into the kernel any operating-system-support code that does not need to run in kernel mode is placed into the system libraries instead although various modern operating systems have adopted a messagepassing architecture for their kernel internals  linux retains un ' lx ' s historical model  the kernel is created as a single  monolithic binary the main reason is to improve performance because all kernel code and data structures are kept iil a single address space  no context switches are necessary when a process calls an operating-system function or when a hardware interrupt is delivered linux kernel loadable kernel modules figure 21.1 components of the linux system 808 chapter 21 this single address space contains not only the core scheduling and virtual memory code but all kernel code  including all device drivers  file systems  and networking code even though all the kernel components share this same m.elting pot  there is still room for modularity in the san1.e way that user applications can load shared libraries at run time to pull in a needed piece of code  so the linux kernel can load  and unload  modules dynamically at run time the kernel does not necessarily need to know in advance which modules may be loaded -they are truly independent loadable components the linux kernel forms the core of the linux operating system it provides all the functionality necessary to run processes  and it provides system services to give arbitrated and protected access to hardware resources the kernel implements all the features required to qualify as an operating system on its own  however  the operating system provided by the linux kernel looks nothing like a unix system it is missing many of the extra features of unix  and the features that it does provide are not necessarily in the format in which a unix application expects them to appear the operating-system interface visible to running applications is not maintained directly by the kernel rather  applications make calls to the system libraries  which in turn call the operatingsystem services as necessary the system libraries provide many types of functionality at the simplest level  they allow applications to make kernel-system service requests making a system call involves transferring control from tmprivileged user mode to privileged kernel mode ; the details of this transfer vary from architecture to architecture the libraries take care of collecting the system-call arguments and  if necessary  arranging those arguments in the special form necessary to make the system call the libraries may also provide more complex versions of the basic system calls for example  the c language 's buffered file-handling functions are all implemented in the system libraries  providing more advanced control of file i/ 0 than the basic kernel system calls the libraries also provide routines that do not correspond to system calls at all  such as sorting algorithms  mathematical functions  and string-manipulation routines all the functions necessary to support the running of unix or posix applications are implemented here in the system libraries the limix system includes a wide variety of user-mode programs-both system utilities and user utilities the system utilities include all the programs necessary to initialize the system  such as those to configure network devices and to load kernel modules continually running server programs also com1.t as system utilities ; such programs handle user login requests  incoming network connections  and the printer queues not all the standard utilities serve key system-administration functions the unix user environment contains a large number of standard utilities to do simple everyday tasks  such as listing directories  moving and deleting files  and displaying the contents of a file more complex utilities can perform text-processing functions  such as sorting textual data and performing pattern searches on input text together  these utilities form a standard tool set that users can expect on any unix system ; although they do not perform any operating-system function  they are an important part of the basic limix system 21.3 21.3 809 the linux kernel has the ability to load and unload arbitrary sections of kernel code on demand these loadable kernel modules run in privileged kernel mode and as a consequence have full access to all the hardware capabilities of the machine on which they run in theory  there is no restriction on what a kernel module is allowed to do ; typically  a module might implement a device driver  a file system  or a networking protocol kernel modules are convenient for several reasons linux ' s source code is free  so anybody wanting to write kernel code is able to compile a modified kernel and to reboot to load that new functionality ; however  recompiling  relinking  and reloading the entire kernel is a cumbersome cycle to undertake when you are developing a new driver if you use kernel modules  you do not have to make a new kernel to test a new driver-the driver can be compiled on its own and loaded into the already-rmming kernel of course  once a new driver is written  it can be distributed as a module so that other users can benefit from it without having to rebuild their kernels this latter point has another implication because it is covered by the gpl license  the linux kernel can not be released with proprietary components added to it  unless those new components are also released under the gpl and the source code for them is made available on demand the kernel 's module interface allows third parties to write and distribute  on their own terms  device drivers or file systems that could not be distributed under the gpl kernel modules allow a linux system to be set up with a standard minimal kernet without any extra device drivers built in any device drivers that the user needs can be either loaded explicitly by the system at startup or loaded automatically by the system on demand and unloaded when not in use for example  a cd-rom driver might be loaded when a cd is mounted and unloaded from memory when the cd is dismounted from the file system the module support under linux has three components  the allows modules to be loaded into memory and to talk to the rest of the kernel the allows modules to tell the rest of the kernel that a new driver has become available a allows different device drivers to reserve hardware resources and to protect those resources from accidental use by another driver 21.3.1 module management loading a module requires more than just loading its binary contents into kernel memory the system must also make sure that any references the module makes to kernel symbols or entry points are updated to point to the correct locations in the kernel 's address space linux deals with this reference updating by splitting the job of module loading into two separate sections  the management of sections of module code in kernel memory and the handling of symbols that modules are allowed to reference 810 chapter 21 linux maintains an internal syncbol table in the kernel this symbol table does not contain the full set of symbols defined in the kernel during the latter 's compilation ; rather  a symbol must be exported explicitly by the kernel the set of exported symbols constitutes a well-defined interface by which a module can interact with the kernel although exporting symbols from a kernel function requires an explicit request by the programmer  no special effort is needed to import those symbols into a module a module writer just uses the standard external linking of the c language  any external symbols referenced by the module but not declared by it are simply marked as unresolved in the final module binary produced by the compiler when a module is to be loaded into the kernel  a system utility first scans the module for these unresolved references all symbols that still need to be resolved are looked up in the kernel 's symbol table  and the correct addresses of those symbols in the currently running kernel are substituted into the module 's code only then is the module passed to the kernel for loading if the system utility can not resolve any references in the module by looking them up in the kernel 's symbol table  then the module is rejected the loading of the module is performed in two stages first  the moduleloader utility asks the kernel to reserve a continuous area of virtual kernel memory for the module the kernel returns the address of the memory allocated  and the loader utility can use this address to relocate the module 's machine code to the correct loading address a second system call then passes the module  plus any symbol table that the new module wants to export  to the kernel the module itself is now copied verbatim into the previously allocated space  and the kernel 's symbol table is updated with the new symbols for possible use by other modules not yet loaded the final module-management component is the module requestor the kernel defines a communication interface to which a module-management program can connect with this connection established  the kernel will inform the management process whenever a process requests a device driver  file system  or network service that is not currently loaded and will give the manager the opportunity to load that service the original service request will complete once the module is loaded the manager process regularly queries the kernel to see whether a dynamically loaded module is still in use and unloads that module when it is no longer actively needed 21.3.2 driver registration once a module is loaded  it remains no more than an isolated region of memory until it lets the rest of the kernel know what new functionality it provides the kernel maintains dynamic tables of all known drivers and provides a set of routines to allow drivers to be added to or removed from these tables at any time the kernel makes sure that it calls a module 's startup routine when that module is loaded and calls the module 's cleanup routine before that module is unloaded  these routines are responsible for registering the module 's functionality a module may register many types of drivers and may register more than one driver if it wishes for example  a device driver might want to register two separate mechanisms for accessing the device registration tables include the following items  21.3 811 device drivers these drivers include character devices  such as printers/ terminals/ and mice  1 block devices  including all disk drives  1 and network interface devices file systems the file system may be anything that implements linux 's virtual-file-system calling routines it might implement a format for storing files on a disk  but it might equally well be a network file system  such as nfs1 or a virtual file system whose contents are generated on demand/ such as linux 's /proc file system network protocols a module may implement an entire networking protocot such as ipx1 or simply a new set of packet-filtering rules for a network firewall binary format this format specifies a way of recognizing/ and loading/ a new type of executable file in addition/ a module can register a new set of entries in the sysctl and/proc tables  to allow that module to be configured dynamically  section 21.7.4   21.3.3 conflict resolution commercial unix implementations are usually sold to run on a vendor/s own hardware one advantage of a single-supplier solution is that the software vendor has a good idea about what hardware configurations are possible pc hardware/ however/ comes in a vast number of configurations  with large numbers of possible drivers for devices such as network cards  scsi controllers/ and video display adapters the problem of managing the hardware configuration becomes more severe when modular device drivers are supported/ since the currently active set of devices becomes dynamically variable linux provides a central conflict-resolution mechanism to help arbitrate access to certain hardware resources its aims are as follows  to prevent modules from clashing over access to hardware resources to prevent aui  oprobes-device-driver probes that auto-detect device configuration-from interfering with existing device drivers to resolve conflicts among multiple drivers trying to access the same hardware-for example  as when both the parallel printer driver and the parallel-line ip  pup  network driver try to talk to the parallel printer port to these ends/ the kernel maintains lists of allocated hardware resources the pc has a limited number of possible i/0 ports  addresses in its hardware i/0 address space   interrupt lines/ and dma channels when any device driver wants to access such a resource  it is expected to reserve the resource with the kernel database first this requirement incidentally allows the system administrator to determine exactly which resources have been allocated by which driver at any given point a module is expected to use this mechanism to reserve in advance any hardware resources that it expects to use if the reservation is rejected because the resource is not present or is already in use  then it is up to the module 812 chapter 21 21.4 to decide how to proceed it may fail its initialization and request that it be unloaded if it can not continue  or it may carry on  using alternative hardware resources a process is the basic context within which all user-requested activity is serviced within the operating system to be compatible with other unix systems  linux must use a process model similar to those of other versions of unix linux operates differently from unix in a few key places  however in this section  we review the traditional unix process model  section a.3.2  and introduce linux 's own threading model 21.4.1 the fork   and exec   process model the basic principle of unix process management is to separate two operations  the creation of a process and the rum1.ing of a new program a new process is created by the fork   system call  and a new program is run after a call to exec    these are two distinctly separate functions a new process may be created with fork   without a new program being run-the new subprocess simply continues to execute exactly the same program that the first  parent  process was running equally  running a new program does not require that a new process be created first  any process may call exec   at any time the currently rumung program is immediately terminated  and the new program starts executing in the context of the existing process this model has the advantage of great simplicity it is not necessary to specify every detail of the environment of a new program in the system call that runs that program ; the new program simply runs in its existing environment if a parent process wishes to modify the environment in which a new program is to be run  it can fork and then  still running the original program in a child process  make any system calls it requires to modify that child process before finally executing the new program under unix  then  a process encompasses all the information that the operating system must maintain to track the context of a single execution of a single program under linux  we can break down this context into a number of specific sections broadly  process properties fall into three groups  the process identity  environment  and context 21.4.1.1 process identity a process identity consists mainly of the following items  process id  pid   each process has a lmique identifier the pid is used to specify the process to the operating system when an application makes a system call to signal  modify  or wait for the process additional identifiers associate the process with a process group  typically  a tree of processes forked by a single user command  and login session credentials each process must have an associated user id and one or more group ids  user groups are discussed in section 10.6.2  that determine the rights of a process to access system resources and files 21.4 813 personality process personalities are not traditionally found on unix systems  but under linux each process has an associated personality identifier that can slightly modify the semantics of certain system calls personalities are primarily used by emulation libraries to request that system calls be compatible with certain varieties of unix most of these identifiers are under the limited control of the process itself the process group and session identifiers can be changed if the process wants to start a new group or session its credentials can be changed  subject to appropriate security checks howeve1 ~ the primary pid of a process is unchangeable and uniquely identifies that process until termination 21.4.1.2 process environment a process 's environment is inherited from its parent and is composed of two null-terminated vectors  the argument vector and the enviromnent vector the argument vector simply lists the command-line arguments used to invoke the running program ; it conventionally starts with the name of the program itself the environment vector is a list of name = value pairs that associates named environment variables with arbitrary textual values the environment is not held in kernel memory but is stored in the process 's own user-mode address space as the first datum at the top of the process 's stack the argument and environment vectors are not altered when a new process is created ; the new child process will inherit the environment that its parent possesses however  a completely new environment is set up when a new program is invoked on calling exec   ,a process must supply the environment for the new program the kernel passes these enviromnent variables to the next program  replacing the process 's current environment the kernel otherwise leaves the environment and command-line vectors alone-their interpretation is left entirely to the user-mode libraries and applications the passing of environment variables from one process to the next and the inheriting of these variables by the children of a process provide flexible ways to pass information to components of the user-mode system software various important environment variables have conventional meanings to related parts of the system software for example  the term variable is set up to name the type of terminal com1.ected to a user 's login session ; many programs use this variable to determine how to perform operations on the user 's display  such as moving the cursor and scrolling a region of text programs with multilingual support use the lang variable to determine in which language to display system messages for programs that include multilingual support the environment-variable mechanism custom-tailors the operating system on a per-process basis  rather than for the system as a whole users can choose their own languages or select their own editors independently of one another 21.4.1.3 process context the process identity and environment properties are usually set up when a process is created and not changed until that process exits a process may choose to change some aspects of its identity if it needs to do so  or it may alter its environment in contrast  process context is the state of the running 814 chapter 21 program at any one time ; it changes constantly process context includes the following parts  scheduling context the most important part of the process context is its scheduling context-the information that the scheduler needs to suspend and restart the process this information includes saved copies of all the process 's registers floating-point registers are stored separately and are restored only when needed  so that processes that do not use floating-point arithmetic do not incur the overhead of saving that state the scheduling context also includes information about scheduling priority and about any outstanding signals waiting to be delivered to the process a key part of the scheduling context is the process 's kernel stack  a separate area of kernel memory reserved for use exclusively by kernel-mode code both system calls and interrupts that occur while the process is executing will use this stack accounting the kernel maintains accounting information about the resources currently being consumed by each process and the total resources consumed by the process in its entire lifetime so far file table the file table is an array of pointers to kernel file structures when making file-i/o system calls  processes refer to files by their index into this table file-system context whereas the file table lists the existing open files  the file-system context applies to requests to open new files the current root and default directories to be used for new file searches are stored here signal-handler table unix systems can deliver asynchronous signals to a process in response to various external events the signal-handler table defines the routine in the process 's address space to be called when a specific signal arrives virtual memory context the virtual memory context describes the full contents of a process 's private address space ; we discuss it in section 21.6 21.4.2 processes and threads linux provides the fork   system call with the traditional functionality of duplicating a process linux also provides the ability to create threads using the clone   system call however  linux does not distinguish between processes and threads in fact  linux generally uses the term task-rather than process or thread-when referring to a flow of control within a program when clone   is invoked  it is passed a set of flags that determine how much sharing is to take place between the parent and child tasks some of these flags are  21.5 21.5 815 thus  if clone   is passed the flags clone_fs  clone_vm  clone_sighand  and clone_files  the parent and child tasks will share the same file-system information  such as the current working directory   the same memory space  the same signal handlers  and the same set of open files using clone   in this fashion is equivalent to creating a thread in other systems  since the parent task shares most of its resources with its child task however  if none of these flags is set when clone   is invoked  no sharing takes place  resulting in functionality similar to the fork   system call the lack of distinction between processes and threads is possible because linux does not hold a process 's entire context within the main process data structure ; rather  it holds the context within independent subcontexts thus  a process 's file-system context  file-descriptor table  signal-handler table  and virtual memory context are held in separate data structures the process data structure simply contains pointers to these other structures  so any number of processes can easily share a subcontext by pointing to the same subcontext the arguments to the clone   system call tell it which subcontexts to copy  and which to share  when it creates a new process the new process always is given a new identity and a new scheduling context according to the arguments passed  however  it may either create new subcontext data struch1res initialized to be copies of the parent 's or set up the new process to use the same subcontext data structures being used by the parent the fork   system call is nothing more than a special case of clone   that copies all subcontexts  sharing none scheduling is the job of allocating cpu time to different tasks within an operating system normally  we think of scheduling as being the running and interrupting of processes  but another aspect of scheduling is also important to linux  the running of the various kernel tasks kernel tasks encompass both tasks that are requested by a running process and tasks that execute internally on behalf of a device driver 21.5.1 process scheduling linux has two separate process-scheduling algorithms one is a time-sharing algorithm for fair  preemptive scheduling among multiple processes ; the other is designed for real-time tasks  where absolute priorities are more important than fairness the scheduling algorithm used for routine time-sharing tasks received a major overhaul with version 2.5 of the kernel earlier versions of the linux kernel ran a variation of the traditional unix scheduling algorithm  which does not provide adequate support for smp systems and does not scale well as the number of tasks on the system grows the overhaul of the linux scheduler with version 2.5 of the kernel provides a scheduling algorithm that runs in constant time-known as 0  1  -regardless of the number of tasks on the system the new scheduler also provides increased support for smp  including processor affin.ity and load balancing  as well as maintaining fairness and support for interactive tasks 816 chapter 21 numeric priority 0 99 100 140 relative priority highest lowest time quantum 200 ms 10 ms figure 21.2 the relationship between priorities and time-slice length the linux scheduler is a preemptive  priority-based algorithm with two separate priority ranges  a real-time range from 0 to 99 and a nice value ranging from 100 to 140 these two ranges map into a global priority scheme whereby numerically lower values indicate higher priorities unlike schedulers for many other systems  the linux scheduler assigns higher-priority tasks longer time quanta and lower-priority tasks shorter time quanta because of the unique nature of the scheduler  this is appropriate for lim.ix  as we shall soon see the relationship between priorities and time-slice length is shown in figure 21.2 a rum able task is considered eligible for execution on the cpu so long as it has time remaining in its time slice when a task has exhausted its time slice  it is considered expired and is not eligible for execution again until all other tasks have also exhausted their time quanta the kernel maintains a list of all runnable tasks in a runqueue data structure because of its support for smp  each processor maintains its own runqueue and schedules itself independently each runqueue contains two priority arrays-active and expired the active array contains all tasks with time remaining in their time slices  and the expired array contains all expired tasks each of these priority arrays includes a list of tasks indexed according to priority  figure 21.3   the scheduler chooses the task with the highest priority from the active array for execution on the cpu on multiprocessor machines  this means that each processor is scheduling the highest-priority task from its own runqueue structure when all tasks have exhausted their time slices  that is  the active array is empty   the two priority arrays are exchanged as the expired array becomes the active array and vice-versa tasks are assigned dynamic priorities that are based on the nice value plus or minus a value up to the value 5 w1 ether a value is added to or subtracted from a task 's nice value depends on the interactivity of the task a task 's interactivity is determined by how long it has been sleeping while waiting for i/0 tasks that are more interactive typically have longer sleep times and therefore are more likely to have adjustments closer to -5  as the scheduler favors such interactive tasks conversely  tasks with shorter sleep times are often cpu-bound and thus will have their priorities lowered a task 's dynamic priority is recalculated when the task has exhausted its time quantum and is to be moved to the expired array thus  when the two active array priority  oj  1   140  task lists o-o o-o-o 0 21.5 expired array priority  0   1   140  task lists o-o-o 0 o-o figure 21.3 list of tasks indexed according to priority 817 arrays are exchanged  all tasks in the new active array have been assigned new priorities and corresponding time slices linux ' s real-time scheduling is simpler still linux implements the two realtime scheduling classes required by posix.lb  first-come  first-served  fcfs  and round-robin  sections 5.3.1 and 5.3.4  respectively   in both cases  each process has a priority irt addition to its scheduling class processes with different priorities can compete with one another to some extent in time-sharing scheduling ; in real-time scheduling  however  the scheduler always runs the process with the highest priority among processes of equal priority  it runs the process that has been waiting longest the only difference between fcfs and round-robin scheduling is that fcfs processes continue to nm until they either exit or block  whereas a round-robill process will be preempted after a while and will be moved to the end of the scheduling queue  so round-robin processes of equal priority will automatically time-share among themselves unlike routine time-sharing tasks  real-time tasks are assigned static priorities linux 's real-time scheduling is soft-rather than hard-real time the scheduler offers strict guarantees about the relative priorities of real-time processes  but the kernel does not offer any guarantees about how quickly a reahim.e process will be scheduled once that process becomes runnable 21.5.2 kernel synchronization the way the kernel schedules its own operations is fundamentally different from the way it schedules processes a request for kernel-mode execution can occur in two ways a running program may request an operating-system service  either explicitly via a system call or implicitly-for example  when a page fault occurs alternatively  a device controller may deliver a hardware interrupt that causes the cpu to start executing a kernel-defined handler for that interrupt the problem posed to the kernel is that all these tasks may try to access the sanl.e internal data structures if one kernel task is in the middle of accessing some data structure when an interrupt service routine executes  then that service routine can not access or modify the same data without risking data corruption this fact relates to the idea of critical sections-portions of code that access shared data and that must not be allowed to execute concurrently as a result  kernel synchronization involves much more than just process scheduling a framework is required that allows kernel tasks to run without violating the integrity of shared data 818 chapter 21 prior to version 2.6  linux was a nonpreernptive kernet meaning that a process running in kernel mode could not be preempted -even if a higherpriority process became available to run with version 2.6  the linux kernel became fully preemptive ; so a task can now be preempted when it is running in the kernel the limix kernel provides spinlocks and semaphores  as well as readerwriter versions of these two locks  for locking in the kernel on smp machines  the fundamental locking mechanism is a spinlock ; the kernel is designed so that the spinlock is held only for short durations on single-processor machines  spinlocks are inappropriate for use and are replaced by enabling and disabling kernel preemption that is  on single-processor machines  rather than holding a spinlock  the task disables kernel preemption when the task would otherwise release the spinlock  it enables kernel preemption this pattern is summarized below  linux uses an interesting approach to disable and enable kernel preemption it provides two simple system calls-preempldisable   and preemplenable   -for disabling and enabling kernel preemption however  in addition  the kernel is not preemptible if a kernel-mode task is holding a lock to enforce this rule  each task in the system has a thread-info structure that includes the field preempt_count  which is a counter indicating the number of locks being held by the task the counter is incremented when a lock is acquired and decremented when a lock is released if the value of preemplcount for the task currently running is greater than zero  it is not safe to preempt the kernet as this task currently holds a lock if the count is zero  the kernel can safely be interrupted  assuming there are no outstanding calls to preempt_disable    spinlocks-along with enabling and disabling of kernel preemption -are used in the kernel only when the lock is held for short durations when a lock must be held for longer periods  semaphores are used the second protection technique used by linux applies to critical sections that occur in interrupt service routines the basic tool is the processor 's interrupt-control hardware by disabling interrupts  or using spinlocks  during a critical section  the kernel guarantees that it can proceed without the risk of concurrent access to shared data structures however  there is a penalty for disabling interrupts on most hardware architectures  interrupt enable and disable instructions are expensive furthermore  as long as interrupts remain disabled  all i/0 is suspended  and any device waiting for servicing will have to wait until interrupts are reenabled ; so performance degrades the linux kernel uses a synchronization architecture that allows long critical sections to run for their entire duration without having interrupts disabled this ability is especially useful in the networking code an interrupt in a network device driver can signal the arrival of an entire network packet  which may result in a great deal of code being executed to disassemble  route  and forward that packet within the interrupt service routine 21.5 819 kernel-system service routines  preemptible  user-mode programs  preemptible  figure 21.4 interrupt protection levels linux implements this architecture by separating interrupt service routines into two sections  the top half and the bottom half the is a normal interrupt service routine that runs with recursive interrupts disabled ; interrupts of a higher priority may interrupt the routine  but interrupts of the same or lower priority are disabled the of a service routine is run  with all interrupts enabled  by a miniature scheduler that ensures that bottom halves never interrupt themselves the bottom-half scheduler is invoked automatically whenever an interrupt service routine exits this separation means that the kernel can complete any complex processing that has to be done in response to an interrupt without worrying about being interrupted itself if another interrupt occurs while a bottom half is executing  then that interrupt can request that the same bottom half execute  but the execution will be deferred until the one currently running completes each execution of the bottom half can be interrupted by a top half but can never be interrupted by a similar bottom half the top-half/bottom-half architecture is completed by a mechanism for disabling selected bottom halves while executing normal  foreground kernel code the kernel can code critical sections easily using this system interrupt handlers can code their critical sections as bottom halves ; and when the foreground kernel wants to enter a critical section  it can disable any relevant bottom halves to prevent any other critical sections from interrupting it at tl1e end of the critical section  the kernel can reenable the bottom halves and run any bottom-half tasks that have been queued by top-half interrupt service routines during the critical section figure 21.4 summarizes the various levels of interrupt protection within the kernel each level may be interrupted by code running at a higher level but will never be interrupted by code running at the same or a lower level ; except for user-mode code  user processes can always be preempted by another process when a time-sharing scheduling interrupt occurs 21.5.3 symmetric multiprocessing the linux 2.0 kernel was the first stable linux kernel to support hardware  allowing separate processes to execute in parallel on separate processors originally  the implementation of smp imposed the restriction that only one processor at a time could be executing kernel-mode code 820 chapter 21 21.6 in version 2.2 of the kernel  a single kernel spinlock  sometimes termed bkl for big kernel lock  was created to allow multiple processes  running on different processors  to be active in the kernel concurrently however  the bkl provided a very coarse level of locking granularity later releases of the kernel made the smp implementation more scalable by splitting this single kernel spinlock into multiple locks  each of which protects only a small subset of the kernel 's data structures such spinlocks are described in section 21.5.2 the 2.6 kernel provided additional smp enhancements  including processor affinity and load-balancing algorithms memory management under linux has two components the first deals with allocating and freeing physical memory-pages  groups of pages  and small blocks of memory the second handles virtual memory  which is memory mapped into the address space of running processes in this section  we describe these two components and then examine the mechanisms by which the loadable components of a new program are brought il lto a process 's virtual memory in response to an exec   system call 21.6.1 management of physical memory due to specific hardware characteristics  linux separates physical memory into three different zones  or regions  zone_dma zone_normal zone_highmem these zones are architecture specific for example  on the intel 80x86 architecture  certain isa  industry standard architecture  devices can only access the lower 16 mb of physical memory using dma on these systems  the first 16 mb of physical memory comprise zonldma zone_normal identifies physical memory that is mapped to the cpu 's address space this zone is used for most routine memory requests for architectures that do not limit what dma can access  zonldma is not present  and zone_normal is used finally  zone_highmem  for high memory  refers to physical memory that is not mapped into the kernel address space for example  on the 32-bit intel architecture  where 232 provides a 4-gb address space   the kernel is mapped into the first 896 mb of the address space ; the remaining memory is referred to as high memory and is allocated from zone_highmem the relationship of zones and physical addresses on the intel80x86 architecture is shown in figure 21.5 the kernel maintains a list of free pages for each zone when a request for physical memory arrives  the kernel satisfies the request using the appropriate zone the priinary physical-memory manager in the lil lux kernel is the page allocator each zone has its own allocator  which is responsible for allocating and freeing all physical pages for the zone and is capable of allocating ranges 21.6 821 figure 21.5 relationship of zones and physical addresses on the lntel80x86 of physically contiguous pages on request the allocator uses a buddy system  section 9.8.1  to keep track of available physical pages in this scheme  adjacent units of allocatable memory are paired together  hence its name   each allocatable memory region has an adjacent partner  or buddy   whenever two allocated partner regions are freed up  they are combined to form a larger region-a buddy heap that larger region also has a partner  with which it can combine to form a still larger free region conversely  if a small memory request can not be satisfied by allocation of an existing small free region  then a larger free region will be subdivided into two partners to satisfy the request separate linked lists are used to record the free memory regions of each allowable size ; under linux  the smallest size allocatable under this mechanism is a single physical page figure 21.6 shows an example of buddy-heap allocation a 4-kb region is being allocated  but the smallest available region is 16 kb the region is broken up recursively until a piece of the desired size is available ultim.ately  all memory allocations in the linux kernel are ncade either statically  by drivers that reserve a contiguous area of memory during system boot time  or dynamically  by the page allocator however  kernel functions do not have to use the basic allocator to reserve memory several specialized memory-management subsystems use the underlying page allocator to manage their own pools of memory the most important are the virtual memory system  described in section 21.6.2 ; the kmalloc   variable-length allocator ; the slab allocator  used for allocating memory for kernel data structures ; and the page cache  used for caching pages belonging to files many components of the linux operating system need to allocate entire pages on request  but often smaller blocks of memory are required the kernel 8kb 8kb 16kb 4kb figure 21.6 splitting of memory in the buddy system 822 chapter 21 provides an additional allocator for arbitrary-sized requests  where the size of a request is not known in advance and may be only a few bytes analogous to the c language 's malloc   function  this kmalloc   service allocates entire pages on demand but then splits them into smaller pieces the kernel maintains lists of pages in use by the kmalloc   service allocating memory involves determining the appropriate list and either taking the first free piece available on the list or allocating a new page and splitting it up memory regions clain'led by the kmalloc   system are allocated permanently until they are freed explicitly ; the kmalloc   system can not reallocate or reclaim these regions in response to memory shortages another strategy adopted by linux for allocating kernel memory is known as slab allocation a slab is used for allocating memory for kernel data structures and is made up of one or more physically contiguous pages a consists of one or more slabs there is a single cache for each unique kernel data structure -a cache for the data structure representing process descriptors  a cache for file objects  a cache for semaphores  and so forth each cache is populated with that are instantiations of the kernel data structure the cache represents for example  the cache representing semaphores stores instances of semaphore objects  and the cache representing process descriptors stores instances of process descriptor objects the relationship among slabs  caches  and objects is shown in figure 21.7 the figure shows two kernel objects 3 kb in size and three objects 7 kb in size these objects are stored in the respective caches for 3-kb and 7-kb objects the slab-allocation algorithm uses caches to store kernel objects when a cache is created  a number of objects are allocated to the cache the number of objects in the cache depends on the size of the associated slab for example  a 12-kb slab  made up of three contiguous 4-kb pages  could store six 2-kb objects initially  all the objects in the cache are marked as free when a new object for a kernel data structure is needed  the allocator can assign any free kernel objects 3-kb i objects l 7-kb objects figure 21.7 slab allocator in linux physically contiguous pages 21.6 823 object from the cache to satisfy the request the object assigned from the cache is marked as used let 's consider a scenario in which the kernel requests memory from the slab allocator for an object representing a process descriptor in linux systems  a process descriptor is of the type struct task_struct  which requires approximately 1.7 kb of memory when the linux kernel creates a new task  it requests the necessary memory for the struct task_struct object from its cache the cache will fulfill the request using a struct task_struct object that has already been allocated in a slab and is marked as free in linux  a slab may be in one of three possible states  1 full all objects in the slab are marked as used empty all objects in the slab are marked as free partial the slab consists of both used and free objects the slab allocator first attempts to satisfy the request with a free object in a partial slab if none exist  a free object is assigned from an empty slab if no empty slabs are available  a new slab is allocated from contiguous physical pages and assigned to a cache ; memory for the object is allocated from this slab two other main subsystems in linux do their own management of physical pages  the page cache and the virtual memory system these systems are closely related to one another the page cache is the kernel 's main cache for block devices  section 21.8.1  and memory-mapped files and is the main mechanism through which i/0 to these devices is performed both the native linux diskbased file systems and the nfs networked file system use the page cache the page cache stores entire pages of file contents and is not limited to block devices ; it can also cache networked data the virtual memory system manages the contents of each process 's virtual address space these two systems interact closely with one another because reading a page of data into the page cache requires mapping pages in the page cache using the virtual memory system in the following section  we look at the virtual memory system in greater detail 21.6.2 virtual memory the limn virtual memory system is responsible for maintaining the address space visible to each process it creates pages of virtual memory on demand and manages loading those pages from disk and swapping them back out to disk as required under linux  the virtual memory manager maintains two separate views of a process 's address space  as a set of separate regions and as a set of pages the first view of an address space is the logical view  describing instructions that the virtual memory system has received concerning the layout of the address space in this view  the address space consists of a set of nonoverlapping regions  each region representing a continuous  page-aligned subset of the address space each region is described internally by a single vm_area_struct structure that defines the properties of the region  including the process 's read  write  and execute permissions in the region as well as information about any files associated with the region the regions for each 824 chapter 21 address space are linked into a balanced binary tree to allow fast lookllp of the region corresponding to any virtual address the kernel also n laintains a second  physical view of each address space this view is stored in the hardware page tables for the process the pagetable entries identify the exact current location of each page of virtual mernory  whether it is on disk or in physical memory the physical view is managed by a set of routines  which are invoked from the kernel 's software-interrupt handlers whenever a process tries to access a page that is not currently present in the page tables each vm_area_struct in the address-space description contains a field that points to a table of functions that implement the key page-management functions for any given virtual memory region all requests to read or write an unavailable page are eventually dispatched to the appropriate handler in the function table for the vm_area_struct  so that the central memorymanagement routines do not have to know the details of managing each possible type of memory region 21.6.2.1 virtual memory regions linux implements several types of virtual memory regions one property that characterizes virtual memory is the backing store for the region  which describes where the pages for the region come from most memory regions are backed either by a file or by nothing a region backed by nothing is the simplest type of virtual memory region such a region represents demand-zero memory  when a process tries to read a page in such a region  it is simply given back a page of memory filled with zeros a region backed by a file acts as a viewport onto a section of that file whenever the process tries to access a page within that region  the page table is filled with the address of a page within the kernel 's page cache corresponding to the appropriate offset in the file the same page of physical memory is used by both the page cache and the process 's page tables  so any changes made to the file by the file system are immediately visible to any processes that have mapped that file into their address space any number of processes can map the same region of the same file  and they will all end up using the same page of physical memory for the purpose a virtual memory region is also defined by its reaction to writes the mapping of a region into the process 's address space can be either private or shared if a process writes to a privately mapped region  then the pager detects that a copy-on-write is necessary to keep the changes local to the process in contrast  writes to a shared region result in updating of the object mapped into that region  so that the change will be visible immediately to any other process that is mapping that object 21.6.2.2 lifetime of a virtual address space the kernel will create a new virtual address space in two situations  when a process runs a new program with the exec   system call and when a new process is created by the fork   system call the first case is easy when a new program is executed  the process is given a new  completely empty virtual address space it is up to the routines for loading the program to populate the address space with virtual memory regions 21.6 825 the second case  creating a new process with fork    involves creating a concplete copy of the existing process 's virtual address space the kernel copies the parent process 's vm_area_struct descriptors  then creates a new set of page tables for the child the parent 's page tables are copied directly into the child 's  and the reference count of each page covered is incremented ; thus  after the fork  the parent and child share the same physical pages of memory in their address spaces a special case occurs when the copying operation reaches a virtual memory region that is mapped privately any pages to which the parent process has written within such a region are private  and subsequent changes to these pages by either the parent or the child must not update the page in the other process 's address space when the page-table entries for such regions are copied  they are set to be read only and are marked for copy-on-write as long as neither process modifies these pages  the two processes share the same page of physical memory however  if either process tries to modify a copy-on-write page  the reference count on the page is checked if the page is still shared  then the process copies the page 's contents to a brand-new page of physical memory and uses its copy instead this mechanism ensures that private data pages are shared between processes whenever possible ; copies are made only when absolutely necessary 21.6.2.3 swapping and paging an important task for a virtual memory system is to relocate pages of memory from physical memory out to disk when that inemory is needed early unix systems performed this relocation by swapping out the contents of entire processes at once  but modern versions of unix rely more on paging-the movement of individual pages of virtual memory between physical memory and disk linux does not implement whole-process swapping ; it uses the newer paging mechanism exclusively the paging system can be divided into two sections first  the decides which to write out to disk and when to write them second  the carries out the transfer and pages data back into physical memory when they are needed again linux 's pageuut policy uses a modified version of the standard clock  or second-chance  algorithm described in section 9.4.5.2 under linux  a multiplepass clock is used  and every page has an age that is adjusted on each pass of the clock the age is more precisely a measure of the page 's youthfulness  or how much activity the page has seen recently frequently accessed pages will attain a higher age value  but the age of infrequently accessed pages will drop toward zero with each pass this age valuing allows the pager to select pages to page out based on a least frequently used  lfu  policy the paging mechanism supports paging both to dedicated swap devices and partitions and to normal files  although swapping to a file is significantly slower due to the extra overhead incurred by the file system blocks are allocated from the swap devices according to a bitmap of used blocks  which is maintained in physical memory at all times the allocator uses a next-fit algorithm to try to write out pages to continuous runs of disk blocks for improved performance the allocator records the fact that a page has been paged out to disk by using a feature of the page tables on modern processors  826 chapter 21 the page-table entry 's page-not-present bit is set  allowing the rest of the pagetable entry to be filled with an index identifying where the page has been written 21.6.2.4 kernel virtual memory linux reserves for its own internal use a constant  architecture-dependent region of the virtual address space of every process the page-table entries that map to these kernel pages are marked as protected  so that the pages are not visible or modifiable when the processor is running in user mode this kernel virtual memory area contains two regions the first is a static area that contains page-table references to every available physical page of memory in the system  so that a simple translation from physical to virtual addresses occurs when kernel code is run the core of the kernel  along with all pages allocated by the normal page allocator  resides in this region the remainder of the kernel 's reserved section of address space is not reserved for any specific purpose page-table entries in this address range can be modified by the kernel to point to any other areas of memory the kernel provides a pair of facilities that allow processes to use this virtual memory the vmalloc   function allocates an arbitrary number of physical pages of memory that may not be physically contiguous into a single region of virtually contiguous kernel memory the vremap   function maps a sequence of virtual addresses to point to an area of memory used by a device driver for memory-mapped i/0 21.6.3 execution and loading of user programs the linux kernel 's execution of user programs is triggered by a call to the exec   system call this exec   call commands the kernel to run a new program within the current process  completely overwriting the current execution context with the initial context of the new program the first job of this system service is to verify that the calling process has permission rights to the file being executed once that matter has been checked  the kernel invokes a loader routine to start running the program the loader does not necessarily load the contents of the program file into physical memory  but it does at least set up the mapping of the program into virtual memory there is no single routine in linux for loadil1.g a new program instead  linux maintains a table of possible loader functions  and it gives each such function the opportunity to try loading the given file when an exec   system call is made the initial reason for this loader table was that  between the releases of the 1.0 and 1.2 kernels  the standard format for linux 's binary files was changed older linux kernels understood the a out format for binary files-a relatively simple format common on older unix systems newer linux systems use the more modern elf format  now supported by most current unix implementations elf has a number of advantages over a out  including flexibility and extensibility new sections can be added to an elf binary  for example  to add extra debugging information  without causing the loader routines to become confused by allowing registration of multiple loader routines  linux can easily support the elf and a out binary formats in a single rmming system 21.6 827 in sections 21.6.3.1 and 21.6.3.2  we concentrate exclusively on the loading and running of elf-format binaries the procedure for loading a out binaries is simpler but is similar in operation 21.6.3.1 mapping of programs into memory under linux  the binary loader does not load a binary file into physical memory rather  the pages of the binary file are mapped into regions of virtual memory only when the program tries to access a given page will a page fault result in the loading of that page into physical memory using demand paging it is the responsibility of the kernel 's binary loader to set up the initial memory mapping an elf-format binary file consists of a header followed by several page-aligned sections the elf loader works by reading the header and mapping the sections of the file into separate regions of virtual memory figure 21.8 shows the typical layout of memory regions set up by the elf loader in a reserved region at one end of the address space sits the kernet in its own privileged region of virtual memory inaccessible to normal user-mode programs the rest of virtual memory is available to applications  which can use the kernel 's memory-mapping functions to create regions that map a portion of a file or that are available for application data the loader 's job is to set up the initial memory mapping to allow the execution of the program to start the regions that need to be initialized il1.clude the stack and the program 's text and data regions the stack is created at the top of the user-mode virtual memory ; it grows downward toward lower-numbered addresses it includes copies of the arguments and environment variables given to the program il1 the exec 0 system call the other regions are created near the bottom end of virtual memory the sections of the binary file that contail1 program text or read-only ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ + + ~ i memory invisible to user-mode code figure 21.8 memory layout for elf programs 828 chapter 21 21.7 data are mapped into memory as a write-protected region writable initialized data are mapped next ; then any uninitialized data are mapped in as a private demand-zero region directly beyond these fixed-sized regions is a variable-sized region that programs can expand as needed to hold data allocated at run time each process has a pointer  brk  that points to the current extent of this data region  and processes can extend or contract their brk region with a single system call -sbrko once these mappings have been set up  the loader initializes the process 's program-counter register with the starting point recorded in the elf header  and the process can be scheduled 21.6.3.2 static and dynamic linking once the program has been loaded and has started running  all the necessary contents of the binary file have been loaded into the process 's virtual address space however  most programs also need to run functions from the system libraries  and these library functions must also be loaded in the simplest case  the necessary library functions are embedded directly in the program 's executable binary file such a program is statically linked to its libraries  and statically linked executables can commence running as soon as they are loaded the main disadvantage of static linking is that every program generated must contain copies of exactly the same common system library functions it is much more efficient  in terms of both physical memory and disk-space usage  to load the system libraries into ltl.emory only once dynamic linking allows this single loading to happen linux implements dynamic linking in user mode through a special linker library every dynamically linked program contains a small  statically linked function that is called when the program starts this static function just maps the link library into memory and runs the code that the function contains the link library determines the dynamic libraries required by the program and the names of the variables and functions needed from those libraries by reading the information contained in sections of the elf binary it then maps the libraries into the middle of virtual memory and resolves the references to the symbols contained in those libraries it does not matter exactly where in memory these shared libraries are mapped  they are compiled into position-independent code  pic   which can run at any address in memory linux retains unix 's standard file-system model in unix  a file does not have to be an object stored on disk or fetched over a network from a remote file server rather  unix files can be anything capable of handling the input or output of a stream of data device drivers can appear as files  and interprocesscommunication channels or network connections also look like files to the user the linux kernel handles all these types of files by hiding the implementation details of any single file type behind a layer of software  the virtual file 21.7 829 systen1  vfs   here  we first cover the virtual file system and then discuss the standard linux file system-ext2fs 21.7 .1 the virtual file system the linux vfs is designed around object-oriented principles it has two components  a set of definitions that specify what file-system objects are allowed to look like and a layer o software to manipulate the objects the vfs defines four main object types  an inode object represents an individual file a file object represents an open file a superblock object represents an entire file system a dentry object represents an il ldividual directory entry for each of these four object types  the vfs defines a set of operations every object of one of these types contains a pointer to a function table the function table lists the addresses of the actual functions that implement the defined operations for that object for example  an abbreviated api for some of the file object 's operations includes  int open       open a file ssize_t read      -read from a file ssize_t write       write to a file int mmap       memory-map a file the complete definition of the file object is specified in the struct file_operations  which is located in the file /usr/include/linux/fs.h an irnplementation of the file object  for a specific file type  is required to implement each function specified il l the definition of the file object the vfs software layer can perform an operation on one of the file-system objects by calling the appropriate function from the object 's function table  without havil g to know in advance exactly what kind of object it is dealing with the vfs does not know  or care  whether an inode represents a networked file  a disk file  a network socket  or a directory file the appropriate function for that file 's read   operation will always be at the same place in its function table  and the vfs software layer will call that function without caring how the data are actually read the inode and file objects are the mechanisms used to access files an inode object is a data structure containing pointers to the disk blocks that contain the actual file contents  and a file object represents a point of access to the data in an open file a process can not access an inode ' s contents without first obtaining a file object pointing to the inode the file object keeps track of where in the file the process is currently reading or writing  to keep track of sequential file l/0 it also remembers whether the process asked for write permissions when the file was opened and tracks the process 's activity if necessary to perform adaptive 830 chapter 21 read-ahead  fetching file data into memory before the process requests the data  to improve performance file objects typically belong to a single process  but inode objects do not even when a file is no longer being used by any processes  its inode object may still be cached by the vfs to improve performance if the file is used again in the near future all cached file data are linked onto a list in the file 's inode object the inode also maintains standard information about each file  such as the owner  size  and time most recently modified directory files are dealt with slightly differently from other files the unix programming interface defines a number of operations on directories  such as creating  deleting  and renaming a file in a directory the system calls for these directory operations do not require that the user open the files concerned  unlike the case for reading or writing data the vfs therefore defines these directory operations in the inode object  rather than in the file object the superblock object represents a connected set of files that form a self-contained file system the operating-system kernel maintains a single superblock object for each disk device mounted as a file system and for each networked file system currently connected the main responsibility of the superblock object is to provide access to inodes the vfs identifies every inode by a unique file-system/inode number pair  and it fil ds the inode corresponding to a particular inode number by asking the superblock object to return the inode with that number finally  a dentry object represents a directory entry that may include the name of a directory in the path name of a file  such as /usr  or the actual file  such as stdio h   for example  the file /usr i include/ stdio h contains the directory entries  1  /   2  usr   3  include  and  4  stdio h each one of these values is represented by a separate dentry object as an example of how dentry objects are used  consider the situation in which a process wishes to open the file with the pathname /usr i include/ stdio h using an editor because linux treats directory names as files  translating this path requires first obtaining the inode for the root/ the operating system must then read through this file to obtain the inode for the file include it must continue this process until it obtains the inode for the file stdio h because path-name translation can be a time-consuming task  linuxmaintains a cache of dentry objects  which is consulted during path-name translation obtaining the inode from the dentry cache is considerably faster than having to read the on-disk file 21.7.2 the linux ext2fs file system the standard on-disk file system used by linux is called ext2fs  for historical reasons linux was originally programmed with a minix-compatible file system  to ease exchanging data with the minix development system  but that file system was severely restricted by 14-character file-name limits and a maximum file-system size of 64 mb the minix file system was superseded by a new file system  which was christened the extended file system  extfs   a later redesign of this file system to improve performance and scalability and to add a few missing features led to the second extended file system  ext2fs   linuxs ext2fs has much in common with the bsd fast file system  ffs   section a.7.7   it uses a similar mechanism for locating the data blocks 21.7 831 belonging to a specific file  storing data-block pointers in indirect blocks throughout the file system with up to three levels of indirection as in ffs  directory files are stored on disk just like normal files  although their contents are interpreted differently each block in a directory file consists of a linked list of entries ; each entry contains the length of the entry  the name of a file  and the inode number of the inode to which that entry refers the main differences between ext2fs and ffs lie in their disk-allocation policies in ffs  the disk is allocated to files in blocks of 8 kb these blocks are subdivided into fragments of 1 kb for storage of small files or partially filled blocks at the ends of files in contrast  ext2fs does not use fragments at all but performs all its allocations in smaller units the default block size on ext2fs is 1 kb  although 2-kb and 4-kb blocks are also supported to maintain high performance  the operating system must try to perform i/0 operations in large chunks whenever possible by clustering physically adjacent l/0 requests clustering reduces the per-request overhead incurred by device drivers  disks  and disk-controller hardware a 1-kb i/0 request size is too small to maintain good performance  so ext2fs uses allocation policies designed to place logically adjacent blocks of a file into physically adjacent blocks on disk  so that it can submit an i/0 request fm several disk blocks as a single operation the ext2fs allocation policy comes in two parts as in ffs  an ext2fs file system is partitioned into multiple block groups ffs uses the similar concept of cylinder groups  where each group corresponds to a single cylinder of a physical disk however  modern disk-drive technology packs sectors onto the disk at different densities  and thus with different cylinder sizes  depending on how far the disk head is from the center of the disk therefore  fixed-sized cylinder groups do not necessarily correspond to the disk 's geometry when allocating a file  ext2fs must first select the block group for that file for data blocks  it attempts to allocate the file to the block group to which the file 's inode has been allocated for inode allocations  it selects the block group in which the file 's parent directory resides  for nondirectory files directory files are not kept together but rather are dispersed throughout the available block groups these policies are designed not only to keep related information within the same block group but also to spread out the disk load among the disk 's block groups to reduce the fragmentation of any one area of the disk within a block group  ext2fs tries to keep allocations physically contiguous if possible  reducing fragmentation if it can it maintains a bitmap of all free blocks in a block group when allocating the first blocks for a new file  it starts searching for a free block from the beginning of the block group ; when extending a file  it continues the search from the block most recently allocated to the file the search is performed in two stages first  ext2fs searches for an entire free byte in the bitmap ; if it fails to find one  it looks for any free bit the search for free bytes aims to allocate disk space in chunks of at least eight blocks where possible once a free block has been identified  the search is extended backward until an allocated block is encountered when a free byte is found in the bitmap  this backward extension prevents ext2fs from leaving a hole between the most recently allocated block in the previous nonzero byte and the zero byte found once the next block to be allocated has been found by either bit or byte search  ext2fs extends the allocation forward for up to eight blocks and preallocates 832 chapter 21 allocating scattered free blocks allocating continuous free blocks block in use d ieee block fj block selected  j by allocator ___ bitmap search bit boundary byte boundary figure 21.9 ext2fs block-allocation policies these extra blocks to the file this preallocation helps to reduce fragmentation during in.terleaved writes to separate files and also reduces the cpu cost of disk allocation by allocating multiple blocks simultaneously the preallocated blocks are returned to the free-space bitmap when the file is closed figure 21.9 illustrates the allocation policies each row represents a sequence of set and unset bits in an allocation bitmap  indicating used and free blocks on disk in the first case  if we can find any free blocks sufficiently near the start of the search  then we allocate them no matter how fragmented they may be the fragmentation is partially compensated for by the fact that the blocks are close together and can probably all be read without any disk seeks  and allocating them all to one file is better in the long run than allocating isolated blocks to separate files once large free areas become scarce on disk in the second case  we have not immediately found a free block close by  so we search forward for an entire free byte in the bitmap if we allocated that byte as a whole  we would end up creating a fragmented area of free space between it and the allocation preceding it so before allocating we back up to make this allocation flush with the allocation preceding it  and then we allocate forward to satisfy the default allocation of eight blocks 21.7.3 journaling one popular feature in a file system is journaling  whereby modifications to the file system are sequentially written to a journal a set of operations that performs a specific task is a transaction once a transaction is written to the journat it is considered to be committed  and the system call modifying the file system  write    can return to the user process  allowing it to continue execution meanwhile  the journal entries relating to the transaction are replayed across the actual file-system structures as the changes are made  a 21.7 833 pointer is updated to indicate which actions have completed and which are still incomplete when an entire committed transaction is concpleted  it is removed from the journal the journal  which is actually a circular buffer  may be in a separate section of the file system  or it may even be on a separate disk spindle it is more efficient  but more complex  to have it under separate read-write heads  thereby decreasing head contention and seek times if the system crashes  some transactions may remain in the journal those transactions were never completed to the file system even though they were committed by the operating system  so they must be completed once the system recovers the transactions can be executed from the pointer until the work is complete  and the file-system structures remain consistent the only problem occurs when a transaction has been aborted -that is  it was not committed before the system crashed any changes from those transactions that were applied to the file system must be undone  again preserving the consistency of the file system this recovery is all that is needed after a crash  eliminating all problems with consistency checking journaling file systems are also typically faster than non-journaling systems  as updates proceed much faster when they are applied to the in-memory journal rather than directly to the on-disk data structures the reason for this improvement is found in the performance advantage of sequential i/0 over random i/0 costly synchronous random writes to the file system are turned into much less costly synchronous sequential writes to the file system 's journal those changes in turn are replayed asynchronously via random writes to the appropriate structures the overall result is a significant gain in performance of file-system metadata-oriented operations  such as file creation and deletion journaling is not provided in ext2fs it is provided  however  in another common file system available for linux systems  ext3  which is based on ext2fs 21.7 .4 the unux process file system the flexibility of the linux vfs enables us to implement a file system that does not store data persistently at all but rather provides an interface to some other functionality the linux process file system  known as the /proc file system  is an example of a file system whose contents are not actually stored anywhere but are computed on demand according to user file i/0 requests a /proc file system is not unique to linux svr4 unix introduced a /proc file system as an efficient interface to the kernel 's process debugging support each subdirectory of the file system corresponded not to a directory on any disk but rather to an active process on the current system a listing of the file system reveals one directory per process  with the directory name being the asch decimal representation of the process 's unique process identifier  pid   linux implements such a /proc file system but extends it greatly by adding a number of extra directories and text files under the file system 's root directory these new entries correspond to various statistics about the kernel and the associated loaded drivers the /proc file system provides a way for programs to access this information as plain text files ; the standard unix user environment provides powerful tools to process such files for example  in the past  the traditional unix ps command for listing the states of all running processes has been implemented as a privileged process that reads the process state directly from the kernel 's virtual memory under lim1x  this command 834 chapter 21 21.8 is implemented as an entirely unprivileged program that simply parses and formats the information from /proc the /proc file system must implement two things  a directory structu.re and the file contents within because a unix file system is defined as a set of file and directory inodes identified by their inode numbers  the /proc file system nmst define a unique and persistent inode number for each directory and the associated files once such a mapping exists  the file system can use this inode number to identify just what operation is required when a user tries to read from a particular file inode or to perform a lookup in a particular directory inode when data are read from one of these files  the /proc file system will collect the appropriate information  format it into textual form  and place it into the requesting process 's read buffer the mapping from inode number to information type splits the inode number into two fields in linux  a pid is 16 bits wide  but an inode number is 32 bits the top 16 bits of the inode number are interpreted as a pid  and the remaining bits define what type of information is being requested about that process a pid of zero is not valid  so a zero pid field in the inode number is taken to mean that this inode contains global-rather than process-specificinformation separate global files exist iil /proc to report information such as the kernel version  free memory  performance statistics  and drivers currently numing not all the inode numbers in this range are reserved the kernel can allocate new /proc inode mappings dynamically  maintaining a bitmap of allocated in ode numbers it also maintains a tree data structure of registered global i pro c file-system entries each entry contains the file 's in ode number  file name  and access permissions  along with the special functions used to generate the file 's contents drivers can register and deregister entries in this tree at any time  and a special section of the tree-appearing under the /proc/sys directoryis reserved for kernel variables files under this tree are managed by a set of common handlers that allow both reading and writing of these variables  so a system administrator can tune the value of kernel parameters simply by writing the new desired values out in ascii decimal to the appropriate file to allow efficient access to these variables from within applications  the /proc/sys subtree is made available through a special system call  sysctl    that reads and writes the same variables in binary  rather than in text  without the overhead of the file system sysctl   is not an extra facility ; it simply reads the /proc dynamic entry tree to identify the variables to which the application is referring to the user  the i/o system in linux looks much like that in any unix system that is  to the extent possible  all device drivers appear as normal files users can open an access channel to a device in the same way they opens any other file-devices can appear as objects within the file system the system administrator can create special files within a file system that contain references to a specific device driver  and a user opening such a file will be able to read from and write to the device referenced by using the normal file-protection device driver driver 21.8 user application character device file character device driver r  1etwork socket i  llne-l   protocol dlsciptlne ' 1 driver .' ' network device driver figure 21.10 device-driver block structure 835 system  which determines who can access which file  the administrator can set access permissions for each device linux splits all devices into three classes  block devices  character devices  and network devices figure 21.10 illustrates the overall structure of the device-driver system include all devices that allow random access to completely independent  fixed-sized blocks of data  including hard disks and floppy disks  cd-roms  and flash memory block devices are typically used to store file systems  but direct access to a block device is also allowed so that programs can create and repair the file system that the device contains applications can also access these block devices directly if they wish ; for example  a database application may prefer to perform its own  fine-tuned laying out of data onto the disk  rather than using the general-purpose file system include most other devices  such as mice and keyboards the fundamental difference between block and character devices is random access-block devices may be accessed randomly  while character devices are only accessed serially for example  seeking to a certain position in a file might be supported for a dvd but makes no sense to a pointil1.g device such as a mouse are dealt with differently from block and character devices users can not directly transfer data to network devices ; instead  they must communicate indirectly by opening a connection to the kernel 's networking subsystem we discuss the interface to network devices separately in section 21.10 21.8.1 block devices block devices provide the main interface to all disk devices in a system performance is particularly important for disks  and the block-device system must provide functionality to ensure that disk access is as fast as possible this functionality is achieved through the scheduling of i/0 operations in the context of block devices  a block represents the unit with which the kernel performs i/0 wl1en a block is read into memory  it is stored in a buffer the request manager is the layer of software that manages the reading and writing of buffer contents to and from a block-device driver a separate list of requests is kept for each block-device driver traditionally  these requests have been scheduled according to a unidirectional-elevator 836 chapter 21  c-scan  algorithm that exploits the order in which requests are inserted in and removed from the lists the request lists are maintained in sorted order of increasing starting-sector number when a request is accepted for processing by a block-device driver  it is not removed fronc the list it is rernoved only after the i/o is complete  at which point the driver continues with the next request in the list  even if new requests have been inserted into the list before the active request as new i/0 requests are made  the request ncanager attempts to merge requests in the lists the scheduling of i/0 operations changed somewhat with version 2.6 of the kernel the fundamental problem with the elevator algorithm is that i/0 operations concentrated in a specific region of the disk can result in starvation of requests that need to occur in other regions of the disk the deadline i/o scheduler used in version 2.6 works similarly to the elevator algorithm except that it also associates a deadline with each request  thus addressing the starvation issue by default  the deadline for read requests is 0.5 second  and that for write requests is 5 seconds the deadline sched ~ uler maintains a sorted queue of pending i/0 operations ordered by sector number however  it also maintains two other queues-a read queue for read operations and a write queue for write operations these two queues are ordered according to deadline every i/0 request is placed in both the sorted queue and either the read or the write queue  as appropriate ordinarily  i/0 operations occur from the sorted queue however  if a deadline expires for a request in either the read or the write queue  i/o operations are scheduled from the queue containing the expired request this policy ensures that an i/0 operation will wait no longer than its expiration time 21.8.2 character devices a character-device driver can be almost any device driver that does not offer random access to fixed blocks of data any character-device drivers registered to the lim.jx kernel must also register a set of functions that implement the file i/0 operations that the driver can handle the kernel performs almost no preprocessing of a file read or write request to a character device ; it simply passes the request to the device in question and lets the device deal with the request the main exception to this rule is the special subset of character-device drivers that implement terminal devices the kernel maintains a standard interface to these drivers by means of a set of tty struct structures each of these structures provides buffering and flow control on the data stream from the terminal device and feeds those data to a line discipline a is an interpreter for the information from the terminal device the most common line discipline is the tty discipline  which glues the terminal 's data stream onto the standard input and output streams of a user 's running processes  allowing those processes to communicate directly with the user 's terminal this job is complicated by the fact that several such processes may be running simultaneously  and the tty line discipline is responsible for attaching and detaching the terminal 's input and output from the various processes connected to it as those processes are suspended or awakened by the user 21.9 21.9 837 other line disciplines also are implemented that have nothing to do with i/0 to a user process the ppp and sup networking protocols are ways of encoding a networking connection over a terminal device such as a serial line these protocols are implemented under linux as drivers that at one end appear to the terminal system as line disciplines and at the other end appear to the networking system as network-device drivers after one of these line disciplines has been enabled on a terminal device  any data appearing on that terminal will be routed directly to the appropriate network-device driver linux provides a rich environment for processes to communicate with each other communication may be just a matter of letting another process know that some event has occurred  or it may involve transferring data from one process to another 21.9.1 synchronization and signals the standard lim.ix mechanism for informing a process that an event has occurred is the signals can be sent from any process to any other process  with restrictions on signals sent to processes owned by another user however  a limited number of signals are available  and they can not carry information only the fact that a signal has occurred is available to a process sigrl.als are not generated only by processes the kernel also generates signals internally ; for example  it can send a signal to a server process when data arrive on a network channel  to a parent process when a child terminates  or to a waiting process when a timer expires internally  the linux kernel does not use signals to communicate with processes ruml.ing in kernel mode if a kernel-mode process is expecting an event to occm ~ it will not normally use signals to receive notification of that event rather  communication about incoming asynchronous events within the kernel takes place through the use of scheduling states and wai t_queue structures these mechanisms allow kernel-mode processes to inform one another about relevant events  and they also allow events to be generated by device drivers or by the networking system whenever a process wants to wait for some event to complete  it places itself on a wait queue associated with that event and tells the scheduler that it is no longer eligible for execution  once the event has completed  it will wake up every process on the wait this procedure allows multiple processes to wait for a single event for if several processes are trying to read a file from a disk  then they will awakened once the data have been read into memory successfully although signals have always been the main mechanism for communicating asynchronous events among processes  linux also implements the semaphore mechanism of system v unix a process can wait on a semaphore as easily as it can wait for a signal  but semaphores have two advantages  large numbers of semaphores can be shared among multiple independent processes  and operations on multiple semaphores can be performed atomically internally  the standard linux wait queue mechanism synchronizes processes that are communicating with semaphores 838 chapter 21 21.10 21.9.2 passing of data among processes linux offers several mechanisms for passing data among processes the standard unix mechanism allows a child process to inherit a communication channel from its parent ; data written to one end of the pipe can be read at the other under lim.jx  pipes appear as just another type of inode to virtual-filesystem software  and each pipe has a pair of wait queues to synchronize the reader and writer unix also defines a set of networking facilities that can send streams of data to both local and remote processes networking is covered in section 21.10 another process communications method  shared memory  offers an extremely fast way to communicate large or small amounts of data any data written by one process to a shared memory region can be read immediately by any other process that has mapped that region into its address space the main disadvantage of shared memory is that  on its own  it offers no synchronization a process can neither ask the operating system whether a piece of shared memory has been written to nor suspend execution until such a write occurs shared memory becomes particularly powerful when used in conjunction with another interprocess-communication mechanism that provides the missing synchronization a shared-memory region in linux is a persistent object that can be created or deleted by processes such an object is treated as though it were a small  independent address space the linux paging algorithms can elect to page out to disk shared-memory pages  just as they can page out a process 's data pages the shared-memory object acts as a backing store for shared-memory regions  just as a file can act as a backing store for a memory-mapped memory region when a file is mapped into a virtual-address-space region  then any page faults that occur cause the appropriate page of the file to be mapped into virtual memory similarly  shared-memory mappings direct page faults to map in pages from a persistent shared-memory object also just as for files  sharedmemory objects remember their contents even if no processes are currently mapping them into virtual memory networking is a key area of functionality for linux not only does linux support the standard internet protocols used for most unix-to-unix communications  but it also implements a number of protocols native to other  non-unix operating systems in particular  since linux was originally implemented primarily on pcs  rather than on large workstations or on server-class systems  it supports many of the protocols typically used on pc networks  such as appletalk and ipx internally  networking in the linux kernel is implemented by three layers of software  the socket interface protocol drivers network-device drivers 21.10 839 user applications perform all networking requests through the socket interface this interface is designed to look like the 4.3 bsd socket layer  so that any programs designed to make use of berkeley sockets will run on linux without any source-code changes this interface is described in section a.9.1 the bsd socket interface is sufficiently general to represent network addresses for a wide range of networking protocols this single interface is used in linux to access not just those protocols implemented on standard bsd systems but all the protocols supported by the system the next layer of software is the protocol stack  which is similar in organization to bsd ' sown framework whenever any networking data arrive at this laye1 ~ either from an application 's socket or from a network-device driver  the data are expected to have been tagged with an identifier specifying which network protocol they contain protocols can communicate with one another if they desire ; for example  within the internet protocol set  separate protocols manage routing  error reporting  and reliable retransmission of lost data the protocol layer may rewrite packets  create new packets  split or reassemble packets into fragments  or simply discard incoming data ultimately  once the protocol layer has finished processing a set of packets  it passes them on  upward to the socket interface if the data are destined for a local connection or downward to a device driver if the data need to be transmitted remotely the protocol layer decides to which socket or device it will send the packet all communication between the layers of the networking stack is performed by passing single skbuff  socket buffer  structures each of these structures contains a set of pointers into a single continuous area of memory  representing a buffer inside which network packets can be constructed the valid data in a skbuff do not need to start at the beginning of the skbuff 's buffer  and they do not need to run to the end the networking code can add data to or trim data from either end of the packet  as long as the result still fits into the skbuff this capacity is especially important on modern microprocessors  where improvements in cpu speed have far outstripped the performance of main memory the skbuff architecture allows flexibility in manipulating packet headers and checksums while avoiding any unnecessary data copying the most important set of protocols in the linux networking system is the tcp /ip protocol suite this suite comprises a number of separate protocols the ip protocol implements routing between different hosts anywhere on the network on top of the routing protocol are built the udp  tcp  and icmp protocols the udp protocol carries arbitrary individual datagrams between hosts the tcp protocol implements reliable connections between hosts with guaranteed in-order delivery of packets and automatic retransmission of lost data the icmp protocol is used to carry various error and status messages between hosts each packet  skbuff  arriving at the networking stack 's protocol software is expected to be already tagged with an internal identifier indicating the protocol to which the packet is relevant different networking-device drivers encode the protocol type in different ways ; thus  the protocol for incoming data must be identified in the device driver the device driver uses a hash table of known networking-protocol identifiers to look up the appropriate protocol 840 chapter 21 21.11 and passes the packet to that protocol new protocols can be added to the hash table as kernel-loadable modules incoming ip packets are delivered to the ip driver the job of this layer is to perform routing after deciding where the packet is to be sent  the ip driver forwards the packet to the appropriate internal protocol driver to be delivered locally or injects it back into a selected network-device-driver queue to be forwarded to another host it performs the routing decision using two tables  the persistent forwarding information base  fib  and a cache of recent routing decisions the fib holds routing-configuration information and can specify routes based either on a specific destination address or on a wildcard representing multiple destinations the fib is organized as a set of hash tables indexed by destination address ; the tables representing the most specific routes are always searched first successful lookups from this table are added to the route-caching table  which caches routes only by specific destination ; no wildcards are stored in the cache  so lookups can be ncade quickly an entry in the route cache expires after a fixed period with no hits at various the ip software passes packets to a separate section of code for management-selective filtering of packets according to arbitrary criteria  usually for purposes the firewall manager maintains a number of separate chain and allows a skbuff to be matched against any chain chains are reserved for separate purposes  one is used for forwarded packets  one for packets being input to this host  and one for data generated at this host each chain is held as an ordered list of rules  where a rule specifies one of a number of possible firewall-decision functions plus some arbitrary data for matching purposes two other functions performed by the ip driver are disassembly and reassembly of large packets if an outgoing packet is too large to be queued to a device  it is simply split up into smaller fragments  which are all queued to the driver at the receiving host  these fragments must be reassembled the ip driver maintains an ipfrag object for each fragment awaiting reassembly and an ipq for each datagram being assembled incoming fragments are matched against each known ipq if a match is found  the fragment is added to it ; otherwise  a new ipq is created once the final fragment has arrived for a ipq  a completely new skbuff is constructed to hold the new packet  and this packet is passed back into the ip driver packets identified by the ip as destined for this host are passed on to one of the other protocol drivers the udp and tcp protocols share a means of associating packets with source and destination sockets  each connected pair of sockets is uniquely identified by its source and destination addresses and by the source and destination port numbers the socket lists are linked to hash tables keyed on these four address-port values for socket lookup on incoming packets the tcp protocol has to deal with unreliable connections  so it maintains ordered lists of unacknowledged outgoing packets to retransmit after a timeout and of incoming out-of-order packets to be presented to the socket when the missing data have arrived linux 's security model is closely related to typical unix security mechanisms the security concerns can be classified in two groups  21.11 841 authentication making sure that nobody can access the system without first proving that she has entry rights access control providing a mechanism for checking whether a user has the right to access a certain object and preventing access to objects as required 21.11.1 authentication authentication in unix has typically been performed through the use of a publicly readable password file a user 's password is combined with a random salt value  and the result is encoded with a one-way transformation function and stored in the password file the use of the one-way function means that the original password cam1ot be deduced from the password file except by trial and error when a user presents a password to the system  the password is recombined with the salt value stored in the password file and passed through the same one-way transformation if the result matches the contents of the password file  then the password is accepted historically  unix implementations of this mechanism have had several problems passwords were often limited to eight characters  and the number of possible salt values was so low that an attacker could easily combine a dictionary of commonly used passwords with every possible salt value and have a good chance of matching one or more passwords in the password file  gaining lmauthorized access to any accounts compromised as a result extensions to the password mechanism have been introduced that keep the encrypted password secret in a file that is not publicly readable  that allow longer passwords  or that use more secure methods of encoding the password other authentication mechanisms have been introduced that limit the times during which a user is permitted to connect to the system also  mechanisms exist to distribute authentication information to all the related systems in a network a new security mechanism has been developed by unix vendors to address authentication problems the system is based on a shared library that can be used by any system component that needs to authenticate users an implementation of this system is available under linux pam allows authentication modules to be loaded on demand as specified in a system-wide configuration file if a new authentication mechanism is added at a later date  it can be added to the configuration file  and all system components will immediately be able to take advantage of it pam modules can specify authentication methods  account restrictions  sessionsetup functions  and password-changing functions  so that  when users change their passwords  all the necessary authentication mechanisms can be updated at once   21.11.2 access control access control under unix systems  including linux  is performed through the use of unique numeric identifiers a user identifier  uid  identifies a single user or a single set of access rights a group identifier  gid  is an extra identifier that can be used to identify rights belonging to more than one user 842 chapter 21 access control is applied to various objects in the system every file available in the system is protected by the standard access-control mechanism in addition  other shared objects  such as shared-memory sections and semaphores  employ the same access system every object in a unix system under user and group access control has a single uid and a single gid associated with it user processes also have a single uid  but they may have more than one gid if a process 's uid matches the uid of an object  then the process has or to that object if the uids do not match but any gid of the process matches the gid  then are conferred ; otherwise  the process has to the object linux performs access control by assigning objects a that specifies which access modes-read  write  or execute-are to be granted to processes with owner  group  or world access thus  the owner of an object might have full read  write  and execute access to a file ; other users in a certain group might be given read access but denied write access ; and everybody else might be given no access at all the only exception is the privileged uid a process with this special uid is granted automatic access to any object in the system  bypassing normal access checks such processes are also granted permission to perform privileged operations  such as reading any physical memory or opening reserved network sockets this mechanism allows the kernel to prevent normal users from accessing these resources  most of the kernel 's key internal resources are implicitly owned by the root uid linux implements the standard unix setuid mechanism described in section a.3.2 this mechanism allows a program to run with privileges different from those of the user running the program for example  the lpr program  which submits a job to a print queue  has access to the system 's print queues even if the user running that program does not the unix implementation of setuid distinguishes between a process 's real and effective uid the real uid is that of the user running the program ; the effective uid is that of the file 's owner under linux  this mechanism is augmented in two ways first  linux implements the posix specification 's saved user-id mechanism  which allows a process to drop and reacquire its effective uid repeatedly for security reasons  a program may want to perform most of its operations in a safe mode  waiving the privileges granted by its setuid status ; but it may wish to perform selected operations with all its privileges standard unix implementations achieve this capacity only by swapping the real and effective uids ; the previous effective uid is remembered  but the program 's real uid does not always correspond to the uid of the user running the program saved uids allow a process to set its effective uid to its real uid and then back to the previous value of its effective uid without having to modify the real uid at any time the second enhancement provided by linux is the addition of a process characteristic that grants just a subset of the rights of the effective uid the and process properties are used when access rights are granted to files the appropriate property is set every time the effective uid or gid is set however  the fsuid and fsgid can be set independently of the effective ids  allowing a process to access files on behalf of another user without taking on the 21.12 21.12 843 identity of that other user in any other way specifically  server processes can use this mechanism to serve files to a certain user without becoming vulnerable to being killed or suspended by that user finally  linux provides a mechanism for flexible passing of rights from one program to another-a mechanism that has become common in modern versions of unix when a local network socket has been set up between any two processes on the system  either of those processes may send to the other process a file descriptor for one of its open files ; the other process receives a duplicate file descriptor for the same file this mechanism allows a client to pass access to a single file selectively to some server process without granting that process any other privileges for example  it is no longer necessary for a print server to be able to read all the files of a user who submits a new print job ; the print client can simply pass the server file descriptors for any files to be printed  denying the server access to any of the user 's other files linux is a modern  free operating system based on unix standards it has been designed to run efficiently and reliably on common pc hardware ; it also runs on a variety of other platforms it provides a programming interface and user interface compatible with standard unix systems and can run a large number of unix applications  including an increasing number of comm.ercially supported applications linux has not evolved in a vacuum a complete linux system includes many components that were developed independently of linux the core linux operating-system kernel is entirely original  but it allows much existing free unix software to run  resulting in an entire unix-compatible operating system free from proprietary code the linux kernel is implemented as a traditional monolithic kernel for performance reasons  but it is modular enough in design to allow most drivers to be dynamically loaded and unloaded at run time linux is a multiuser system  providing protection between processes and running multiple processes according to a time-sharing scheduler newly created processes can share selective parts of their execution environment with their parent processes  allowing multithreaded programming interprocess communication is supported by both system v mechanisms-message queues  semaphores  and shared memory-and bsd 's socket interface multiple networking protocols can be accessed simultaneously through the socket interface the memory-management system uses page sharing and copy-on-write to minimize the duplication of data shared by different processes pages are loaded on demand when they are first referenced and are paged back out to backing store according to an lfu algorithm if physical memory needs to be reclaimed to the user  the file system appears as a hierarchical directory tree that obeys unix semantics internally  linux uses an abstraction layer to manage multiple file systems device-oriented  networked  and virtual file systems are supported device-oriented file systems access disk storage through a page cache that is unified with the virtual memory system 844 chapter 21 21.1 what are the advantages and disadvantages of making only some of the symbols defined inside a kernel accessible to a loadable kernel module 21.2 the linux scheduler implements soft real-time scheduling what features necessary for certain real-time programming tasks are missing how might they be added to the kernel 21.3 in what ways does the linux setuid feature differ from the setuid feature in standard unix 21.4 what socket type should be used to implement an intercomputer file-transfer program what type should be used for a program that periodically tests to see whether another computer is up on the network explain your answer 21.5 what scenarios would cause a page of memory to be mapped into a user program 's address space with the copy-on-write attribute enabled 21.6 what extra costs are incurred in the creation and scheduling of a process  compared with the cost of a cloned thread 21.7 linux runs on a variety of hardware platforms what steps must linux developers take to ensure that the system is portable to different processors and memory-management architectures and to minimize the ammmt of architecture-specific kernel code 21.8 multithreading is a commonly used programming technique describe three different ways to implement threads  and compare these three methods with the linux clone   mechanism when might using each alternative mechanism be better or worse than using clones 21.9 the linux source code is freely and widely available over the internet and from cd-rom vendors what are three implications of this availability for the security of the linux system 21.10 under what circumstances would a user process request an operation that results in the allocation of a demand-zero memory region 21.11 what are the primary goals of the conflict-resolution mechanism used by the linux kernel for loading kernel modules 21.12 what are the advantages and disadvantages of writing an operating system in a high-level language  such as c 21.13 in linux  shared libraries perform many operations central to the operating system what is the advantage of keeping this functionality out of the kernel are there any drawbacks explain your answer 21.14 the linux kernel does not allow paging out of kernel memory what effect does this restriction have on the kernel 's design what are two advantages and two disadvantages of this design decision 845 21.15 the directory structure of a linux operating system could include files corresponding to several different file systems  including the linux /proc file system how might the need to support different file-system types affect the structure of the linux kernel 21.16 would you classify linux threads as user-level threads or as kernel-level threads support your answer with the appropriate arguments 21.17 at one time  unix systems used disk-layout optimizations based on the rotation position of disk data  but modern implementations  including linux  simply optimize for sequential data access why do they do so of what hardware characteristics does sequential access take advantage why is rotational optimization no longer so useful 21.18 discuss how the clone   operation supported by linux is used to support both processes and threads 21.19 in what circumstances is the system-call sequence fork   exec   most appropriate when is vfork   preferable the linux system is a product of the internet ; as a result  much of the available documentation on linux is available in some form on the internet the following key sites reference most of the useful information available  the linux cross-reference pages  http  / /lxr.linux.no/  maintain current listil1.gs of the linux kernel  browsable via the web and fully crossreferenced linux-hq  http  / /www.linuxhq.com/  provides a large amount of information relating to the linux 2.x kernels this site also includes lil1.ks to the home pages of most linux distributions  as well as archives of the major mailing lists the linux documentation project  http  / /sunsite.unc.edu/linux/  lists many books on linux that are available in source format as part of the linux documentation project the project also hosts the linux how-to guides  which contain hints and tips relating to aspects of linux the kernel hackers ' guide is an internet-based guide to kernel internals in general this constantly expandil1.g site is located at http  / /www.redhat.com  8080/hypernews/get/khg.html the kernel newbies website  http  / /www.kernelnewbies.org/  provides a resource for introducing the linux kernel to newcomers many mailing lists devoted to linux are also available the most important are maintained by a mailing-list manager that can be reached at the e-mail address maj ordomo vger rutgers edu send e-mail to this address with the single line help in the mail 's body for information on how to access the list server and to subscribe to any lists 846 chapter 21 finally the linux system itself can be obtained over the internet complete linux distributions can be obtained from the home sites of the companies concerned  and the linux community also maintains archives of current system components at several places on the internet the most important are these  ftp  / /tsx-ll.mit.edu/pub/linux/ ftp  / /sunsite.unc.edu/pub/linux/ ftp  i /linux.kernel.org/pub /linux/ in addition to investigating internet resources  you can read about the internals of the linux kernel in bovet and cesati  2002  and love  2004   22.1 the microsoft windows xp operating system is a 32-i 64-bit preemptive multitasking operating system for amd k6/k7  intel ia32/ia64  and later microprocessors the successor to windows nt and windows 2000  windows xp is also intended to replace the windows 95/98 operating system in this chapter  we discuss the key goals of windows xp  the layered architecture of the system that has made it so easy to use  the file system  the networking features  and the programming interface to explore the principles underlying windows xp 's design and the specific components of the system to explain how windows xp can run programs designed for other operating systems to provide a detailed discussion of the windows xp file system to illustrate the networking protocols supported in windows xp to describe the interface available to system and application programmers in the mid-1980s  microsoft and ibm cooperated to develop the os/2 operating system  which was written in assembly language for single-processor intel 80286 systems in 1988  microsoft decided to make a fresh start and to develop a new technology  or nt  portable operating system that supported both the os/2 and posix application-programming interfaces  apis   in october 1988  dave cutler  the architect of the dec vax/vms operating system  was hired and given the charter of building this new operating system originally  the team planned to use the os/2 api as nts native environment  but during development  nt was changed to use the 32-bit windows api  or win32 api   reflecting the popularity of windows 3.0 the first versions of nt were windows nt 3.1 and windows nt 3.1 advanced server  at that time  847 848 chapter 22 16-bit windows was at version 3.1  windows nt version 4.0 adopted the windows 95 user interface and incorporated internet web-server and webbrowser software in addition  user-interface routines and all graphics code were moved into the kernel to improve performance  with the side effect of decreased system reliability although previous versions of nt had been ported to other microprocessor architectures  the windows 2000 version  released in february 2000  supported only intel  and compatible  processors due to marketplace factors windows 2000 incorporated significant changes it added active directory  an x.soo-based directory service   better networking and laptop support  support for plug-and-play devices  a distributed file system  and support for more processors and more memory in october 2001  windows xp was released as both an update to the windows 2000 desktop operating system and a replacement for windows 95/98 in 2002  the server versions of windows xp became available  called windows .net server   windows xp updated the graphical user interface  gui  with a visual design that took advantage of more recent hardware advances and many new numerous features were added to automatically repair problems in applications and the operating system itself as a result of these changes  windows xp provides better networking and device experience  including zero-configuration wireless  instant messaging  streaming media  and digital photography /video   dramatic performance improvements for both the desktop and large multiprocessors  and better reliability and security than earlier windows operating systems windows xp uses a client-server architecture  like mach  to implement multiple operating-system personalities  such as win32 api and posix  with user-level processes called subsystems the subsystem architecture allows enhancements to be made to one operating-system personality without affecting the application compatibility of any others windows xp is a multiuser operating system  supporting simultaneous access through distributed services or through multiple instances of the graphical user interface via the windows terminal server the server versions of wil,dows xp support simultaneous terminal server sessions from windows desktop systems the desktop versions of terminal server multiplex the keyboard  mouse  and monitor between virtual terminal sessions for each logged-on user this feature  called fast user switching  allows users to preempt each other at the console of a pc without having to log off and onto the system windows xp was the first version of windows to ship a 64-bit version the native nt file system ~ and many of the win32 apis have always used 64-bit integers where appropriate-so the major extension to 64-bit in wil,dows xp was support for large addresses there are two desktop versions of windows xp windows xp professional is the premium desktop system for power users at work and at home windows xp personal provides the reliability and ease of use of windows xp but lacks the more advanced features needed to work seamlessly with active directory or run posix applications the mernbers of the windows .net server family use the same core components as the desktop versions but add a range of features needed for uses such as webserver farms  print/ file servers  clustered systems  and large datacenter machines the large datacenter machines can have up to 64gb of 22.2 22.2 849 memory and 32 processors on ia32 systems and 128 gb and 64 processors on ia64 systems microsoffs design goals for windows xp included security  reliability  windows and posix application compatibility  high performance  extensibility  portability  and international support we discuss each of these goals in the following sections 22.2.1 security windows xp goals required more than just adherence to the design standards that had enabled windows nt 4.0 to receive a c-2 security classification from the u.s government  which signifies a moderate level of protection from defective software and malicious attacks   extensive code review and testing were combined with sophisticated automatic analysis tools to identify and investigate potential defects that might represent security vuh1erabilities 22.2.2 reliability windows 2000 was the most reliable  stable operating system microsoft had ever shipped to that point much of this reliability came from maturity in the source code  extensive stress testing of the and automatic detection of many serious errors in drivers the requirements for windows xp were even more stringent microsoft used extensive manual and automatic code review to identify over 63,000 lines in the source files that might contain issues not detected by testing and then set about reviewing each area to verify that the code was indeed correct windows xp extended driver verification to catch more subtle bugs  improved the facilities for catching programming errors in user-level code  and subjected third-party applications  drivers  and devices to a rigorous certification process furthermme  windows xp added facilities for monitoring the health of the pc  including downloading fixes for problems before they are encountered by users the perceived reliability of windows xp was also improved by making the graphical user interface easier to use through better visual design  simpler menus  and measured improvements in the ease with which users can discover how to perform common tasks 22.2.3 windows and posix application compatibility as mentioned  windows xp was not only an update of windows 2000 but also a replacement for windows 95/98 windows 2000 focused primarily on compatibility for business applications the requirements for windows xp included a much higher with the consumer applications that ran on windows 95/98 is difficult to achieve because each application checks for a particular version of windows  may depend to some extent on the quirks of the implementation of apis  may have latent application bugs that were masked in the previous system  and so forth 850 chapter 22 windows xp introduced a compatibility layer that falls between applications and the win32 apis this layer makes windows xp look  almost  bug-for-bug compatible with previuus versions of windows windows xpf like earlier nt releasesf maintains support for running many 16-bit applications using a thunkingf or conversionf layer that translates 16-bit api calls into equivalent 32-bit calls similarlyf the 64-bit version of windows xp provides a thunking layer that translates 32-bit api calls into native 64-bit calls in additionf posix support in windows xp was much improved by a new posix subsystem called interix most available unix-compatible software compiles and runs under interix without modification 22.2.4 high performance windows xp was designed to provide on desktop systems  which are largely constrained by i/0 performance   server systems  where the cpu is often the bottleneck   and large multithreaded and multiprocessor environments  where locking and cache-line management are keys to scalability   to satisfy performance requirements  nt used a variety of techniques  such as asynchronous i/0  optimized protocols for networks  kernel-based graphics  and sophisticated caching of file-system data the memory-management and synchronization algorithms were designed with an awareness of the performance considerations related to cache lines and multiprocessors windows xp further improved performance by reducing the code-path length in critical functionsf using better algorithms and per-processor data structures  using memory coloring for numa  non-uniform n1emory access  machines  and implementing more scalable locking protocols  such as queued spinlocks the new locking protocols helped reduce system bus cycles and included lock-free lists and queues  atomic read-modify-write operations  like interlocked increment   and other advanced locking techniques the subsystems that constitute windows xp communicate with one another efficiently through a local procedure call  lpc  facility that provides highperformance message passing except while executing in the kernel dispatcherf threads in the subsystems of windows xp can be preempted by higherpriority threads thus  the system responds quickly to external events in addition  windows xp was designed for symmetrical multiprocessing ; on a multiprocessor computer  several threads can run at the same time 22.2.5 extensibility refers to the capacity of an operating system to keep up with advances in computing technology to facilitate change over tim.e  the developers implemented windows xp using a layered architecture the windows xp executive runs in kernel or protected mode and provides the basic system services on top of the executive  several server subsystems operate in user mode among them are that emulate different operating systems thus  programs written for ms-dos  microsoft windowsf and posix all run on windows xp in the appropriate environment  see section 22.4 for more information on environmental subsystems  because of the modular structure  additional environmental subsystems can be added without affecting the executive in addition  windows xp uses loadable drivers in the i/0 system  so new file systems  new kinds of i/0 devices  and new kinds of 22.3 22.3 851 networking can be added while the system is running windows xp uses a client-server model like the mach operating system and supports distributed processing by remote procedure calls  rpcs  as defined by the open software foundation 22.2.6 portability an operating system is if it can be moved from one hardware architecture to another with relatively few changes windows xp was designed to be portable like the unix operating system  windows xp is written primarily in c and c + +  most processor-dependent code is isolated in a dynamic link library  dll  called the  hal   a dll is a file that is mapped into a process 's address space in such a way that any functions in the dll appear to be part of the process the upper layers of the windows xp kernel depend on the hal interfaces rather than on the underlying hardware  bolstering windows xp portability the hal manipulates hardware directly  isolating the rest of windows xp from hardware differences among the platforms on which it runs although for market reasons windows 2000 shipped only on intel ia32 compatible platforms  it was also tested on ia32 and dec alpha platforms until just prior to release to ensure portability windows xp runs on ia32-compatible and ia64 processors microsoft recognizes the importance of multiplatform development and testing  since  as a practical matter  maintaining portability is a matter of use it or lose it 22.2.7 international support windows xp was also designed for provides support for different locales via the the nls api provides specialized routines to format dates  time  and money in accordance with national customs string comparisons are specialized to acc01mt for varying character sets unicode is windows xp ' s native character code windows xp supports ansi characters by converting them to unicode characters before manipulating them  8-bit to 16-bit conversion   system text strings are kept in resource files that can be replaced to localize the system for different languages multiple locales can be used concurrently  which is important to multilingual individuals and businesses the architecture of windows xp is a layered system of modules  as shown in figure 22.1 the main layers are the hal  the kernel  and the executive  all of which run in protected mode  and a collection of subsystems and services that run in user mode the user-mode subsystems fall into two categories  the environmental subsystems  which emulate different operating systems  and the which provide security functions one of the chief advantages of type of architecture is that interactions between modules are kept simple the remainder of this section describes these layers and subsystems 852 chapter 22 auth.entication package t security account manq.ger database figure 22.1 windows xp block diagram 22.3.1 hardware-abstraction layer the hal is the layer of software that hides hardware differences from upper levels of the operating system  to help make windows xp portable the hal exports a virtual machine interface that is used by the kernel dispatcher  the executive  and the device drivers one advantage of this approach is that only a single version of each device driver is required-it runs on all hardware platforms without porting the driver code the hal also provides support for symmetric multiprocessing device drivers map devices and access them directly  but the administrative details of mapping memory  configuring i/0 buses  setting up dma  and coping with motherboard-specific facilities are all provided by the hal interfaces 22.3.2 kernel the kernel of windows xp has four main responsibilities  thread scheduling  interrupt and exception handling  low-level processor synchronization  and recovery after a power failure the kernel is object oriented an object type in windows 2000 is a system-defined data type that has a set of attributes  data values  and a set of methods  for example  functions or operations   an object is an instance of an object type the kernel performs its job by using a set of kernel objects whose attributes store the kernel data and whose methods perform the kernel activities 22.3 853 22.3.2.1 kernel dispatcher the kernel dispatcher provides the foundation for the executive and the subsystems most of the dispatcher is never paged out of ncemmy and its execution is never preempted its main responsibilities are thread sched llling  implementation of synchronization primitives  timer managencent  software interrupts  asynchronous and deferred procedure calls   and exception dispatching 22.3.2.2 threads and scheduling like many other modern operating systems  windows xp uses processes and threads for executable code each process has one or more threads  and each thread has its own scheduling state  including actual priority  processor affinity  and cpu-usage information there are six possible thread states  ready  standby  running  waiting  transition  and terminated indicates that the thread is waiting to run the highest-priority ready thread is moved to the state  which means it is the next thread to run in a multiprocessor system  each process keeps one thread in a standby state a thread is when it is executing on a processor it runs until it is preempted thread  until it terminates  until its allotted execution time ends  or until it blocks on a dispatcher object  such as an event signaling i/0 completion a thread is in the state when it is waiting for a dispatcher object to be signaled a new thread is in the state while it waits for resources necessary for execution a thread enters the state when it finishes execution the dispatcher uses a 32-level priority scheme to determine the order of thread execution priorities are divided into two classes  variable class and real-time class the variable class contains threads having priorities from 0 to 15  and the real-time class contains threads with priorities ranging from 16 to 31 the dispatcher uses a queue for each scheduling priority and traverses the set of queues from highest to lowest until it finds a thread that is ready to run if a thread has a particular processor affinity but that processor is not available  the dispatcher skips past it and continues looking for a ready thread that is willing to run on the available processor if no ready thread is found  the dispatcher executes a special thread called the idle thread when a thread 's time quantum runs out  the clock interrupt queues a quantum-end deferred procedure call  dpc  to the processor in order to reschedule the processor if the preempted thread is in the variable-priority class  its priority is lowered the priority is never lowered below the base priority lowering the thread 's priority tends to limit the cpu consumption of compute-bound threads when a variable-priority thread is released from a wait operation  the dispatcher boosts the priority the amount of the boost depends on the device for which the thread was waiting ; for example  a thread waiting for keyboard i/0 would get a large priority increase  whereas a thread waiting for a disk operation would get a moderate one this strategy tends to give good response times to interactive threads using a mouse and windows it also enables i/o-bound threads to keep the i/o devices busy while permitting compute-bound threads to use spare cpu cycles in the background this strategy is used by several time-sharing operating systems  including unix in addition  the thread associated with the user 's active gui window receives a priority boost to enhance its response time 854 chapter 22 scheduling occurs when a thread enters the ready or wait state  when a thread terminates  or when an application changes a thread 's priority or processor affinity if a higher-priority real-tince thread becomes ready while a lower-priority thread is running  the lower-priority thread is preempted this preemption gives a real-time thread preferential access to the cpu when the thread needs such access windows xp is not a hard real-time operating system  however  because it does not guarantee that a real-time thread will start to execute within a particular time limit 22.3.2.3 implementation of synchronization primitives key operating-system data structures are managed as objects using common facilities for allocation  reference counting  and security control dispatching and synchronization in the system examples of these objects include the following  the is used to record an event occurrence and to synchronize this occurrence with some action notification events signal all waiting threads  and synchronization events signal a single waiting thread the provides kernel-mode or user-mode mutual exclusion with the notion of ownership the available only in kernel mode  provides deadlock-free mutual exclusion the defsemaphore object acts as a counter or gate to control the number of threads that access a resource the is the entity that is scheduled by the kernel dispatcher and is associated with a which encapsulates a virtual address space the is used to keep track of time and to signal timeouts when operations take too long and need to be interrupted or when a periodic activity needs to be scheduled many of the dispatcher objects are accessed from user mode via an open operation that returns a handle the user-mode code polls or waits on handles to synchronize with other threads as well as with the operating system  see section 22.7.1   22.3.2.4 software interrupts  asynchronous and deferred procedure calls the dispatcher implements two types of software interrupts  asynchronous procedure calls and deferred procedure calls an asynchronous procedure call  apc  breaks into an executing thread and calls a procedure apcs are used to begin execution of new threads  terminate processes  and deliver notification that an asynchronous i/0 has completed apcs are queued to specific threads and allow the system to execute both system and user code within a process 's context deferred procedure calls  dpcs  are used to postpone interrupt processing after handling all blocked device-interrupt processes  the interrupt service routine  isr  schedules the remaining processing by queuing a dpc the 22.3 855 dispatcher schedules software interrupts at a lower priority than the device interrupts so that dpcs do not block other lsrs in addition to deferring deviceinterrupt processing  the dispatcher uses dpcs to process timer expirations and to preempt thread execution at the end of the scheduling quantum execution of dpcs prevents threads from being scheduled on the current processor and also keeps apcs from signaling the completion of i/0 this is done so that completion of dpc routines does not take an extended anwunt of time as an alternative  the dispatcher maintains a pool of worker threads isrs and dpcs queue work itencs to the worker threads dpc routines are restricted so that they can not take page faults  be paged out of memory   call system services  or take any other action that might possibly result in an attempt to block execution of a dispatcher object unlike apcs  dpc routines make no assumptions about what process context the processor is executing 22.3.2.5 exceptions and interrupts the kernel dispatcher also provides trap handling for exceptions and interrupts generated by hardware or software windows xp defines several architectureindependent exceptions  including  memory-access violation integer overflow floating-point overflow or underflow integer divide by zero floating-point divide by zero illegal instruction data misalignment privileged instruction page-read error access violation paging file quota exceeded debugger breakpoint debugger single step the trap handlers deal with simple exceptions elaborate exception handling is performed by the kernel 's exception dispatcher the neve d.,.  creates an exception record containing the reason for the exception an exception handler to deal with it when an exception occurs in kernel mode  the exception dispatcher simply calls a routine to locate the exception handler if no handler is found  a fatal system error occurs and the user is left with the infamous blue screen of death that signifies system failure exception handling is more complex for user-mode processes  because an environmental subsystem  such as the posix system  sets up a debugger port and an exception port for every process it creates if a debugger port 856 chapter 22 is registered  the exception handler sends the exception to the port if the debugger port is not found or does not handle that exception  the dispatcher attempts to find an appropriate exception handler if no handler is found  the debugger is called again to catch the error for debugging if no debugger is running  a message is sent to the process 's exception port to give the environmental subsystem a chance to translate the exception for example  the posix environment translates windows xp exception messages into posix signals before sending them to the thread that caused the exception finally  if nothing else works  the kernel simply terminates the process containing the thread that caused the exception the interrupt dispatcher in the kernel handles interrupts by calling either an interrupt service routine  isr  supplied by a device driver or a kernel trap-handler routine the interrupt is represented by an interrupt object that contains all the information needed to handle the interrupt using an interrupt object makes it easy to associate interrupt-service routines with an interrupt without having to access the interrupt hardware directly different processor architectures  such as intel and dec alpha  have different types and numbers of interrupts for portability  the interrupt dispatcher maps the hardware interrupts into a standard set the interrupts are prioritized and are serviced in priority order there are 32 interrupt request levels  irqls  in windows xp eight are reserved for use by the kernel ; the remaining 24 represent hardware interrupts via the hal  although most ia32 systems use only 16   the windows xp interrupts are defined in figure 22.2 the kernel uses an to bind each interrupt level to a service routine in a multiprocessor compute1 ~ windows xp keeps a separate interrupt-dispatch table for each processm ~ and each processor 's irql can be set independently to mask out interrupts all interrupts that occur at a level equal to or less than the irql of a processor are blocked until the irql is lowered by a kernel-level thread or by an isr returning from interrupt processing windows xp takes advantage of this property and uses software interrupts to deliver apcs and dpcs  to perform system functions such as synchronizing threads with i/0 completion  to start thread dispatches  and to handle timers 30 29 28 27 3-26 2 0 machine check or bus error power fail interprocessor notification  request another processor to act ; e.g  dispatch a process or update the tlb  clock  used to keep track of time  traditional pc irq hardware interrupts dispatch and deferred procedure call  dpc   kernel  asynchronous procedure ca ii  a pc  passive figure 22.2 windows xp interrupt request levels 22.3 857 22.3.3 executive the windows xp executive provides a set of services that all enviromnental subsystems use the services are grouped as follows  object manager  virtual memory manager  process manager  local procedure call facility  i/o manager  cache manager  security reference monitor  plug-and-play and power managers  registry  and booting 22.3.3.1 object manager for managing kernel-mode entities  windows xp uses a generic set of interfaces that are manipulated by user-mode programs windows xp calls these entities objects  and the executive component that manipulates them is the each process has an object table containing entries that track the objects used by the process user-mode code accesses these objects using an opaque value called a handle that is returned by many apis object handles can also be created by duplicating an existing handle  either from the same process or from a different process examples of objects are semaphores  mutexes  events  processes  and threads these are all dispatcher objects threads can block in the kernel dispatcher waiting for any of these objects to be signaled the process  thread  and virtual memory apis use process and thread handles to identify the process or thread to be operated on other examples of objects include files  sections  ports  and various internal i/o objects file objects are used to maintain the open state of files and devices sections are used to map files open files are described in terms of file objects local-communication endpoints are implemented as port objects the object manager maintains the windows xp internal name space in contrast to unix  which roots the system name space in the file system  windows xp uses an abstract name space and connects the file systems as devices the object manager provides interfaces for defining both object types and object instances  translating names to objects  maintaining the abstract name space  through internal directories and symbolic links   and managing object creation and deletion objects are typically managed through the use of reference counts in protected-mode code and handles in user-mode code howeve1 ~ some kernel-mode components use the same apis as user-mode code and thus use handles to manipulate objects if a handle needs to exist beyond the lifetime of the current process  it is marked as a kernel handle and stored in the object table for the system process the abstract name space does not persist across reboots but is built up from configuration information stored in the system registry  plug-and-play device discovery  and creation of objects by system components the windows xp executive allows any object to be given a one process ncay create a named object while a second process opens a handle to the object and shares it with the first process processes can also share objects by duplicating handles between processes  in which case the objects need not be named a name can be either permanent or temporary a perrnanent name represents an entity  such as a disk drive  that remains even if no process is accessing it a temporary nance exists only while a process holds a handle to the object 858 chapter 22 object names are structured like file path names in ms-dos and unix name space directories are represented by a that contains the names of all the objects in the directory the object name space is extended by the addition of device objects representing volumes containing file systems objects are manipulated by a set of virtual functions with implementations provided for each object type  create    open    close    delete    query.jlame   ,parse    and security    the latter three functions may need some explanation  query jlame   is called when a thread has a reference to an object but wants to know the object 's name parse   is ~ used by the object manager to search for an object given the object 's name security   is called to make security checks on all object operations  such as when a process opens or closes an object  makes changes to the security descriptor  or duplicates a handle for an object the parse procedure is used to extend the abstract name space to include files the translation of a path name to a file object begins at the root of the abstract name space path-name components are separated by whack characters  \  rather than the slashes  /  used in unix each component is looked up in the current parse directory of the name space internal nodes within the name space are either directories or symbolic links if a leaf object is found and there are no path-name components remaining  the leaf object is returned otherwise  the leaf object 's parse procedure is invoked with the remaining path name parse procedures are only used with a small number of objects belonging to the wilcdows gui  the configuration manager  registry   and -most notably -device objects representing file systems the parse procedure for the device object type allocates a file object and mitiates an open or create i/o operation on the file system if successful  the file object fields are filled in to describe the file in summary  the path name to a file is used to traverse the object-manager namespace  translatilcg the original absolute path name into a  device object  relative path name  pair this pair is then passed to the file system via the i/0 manager  which fills in the file object the file object itself has no name but is referred to by a handle unix file systems have that permit multiple nicknamesor aliases-for the same file the implemented by the windows xp object manager is within the abstract name space  not to provide files with aliases even so  symbolic links are very useful they are used to organize the name space  similar to the organization of the /devices directory in unix they are also used to map standard ms-dos drive letters to drive names drive letters are symbolic links that can be remapped to suit the convenience of the user or administrator drive letters are one place where the abstract name space in windows xp is not global each logged-on user has his or her own set of drive letters so that users can avoid interfering with one another in contrast  terminal server sessions share all processes within a session basenamedobj ects contain the named objects created by most applications 22.3 859 although the name space is not directly visible across a network  the object manager 's parse   method is used to help access a named object on another system when a process attencpts to open an object that resides on a remote computer  the object manager calls the parse method for the device object corresponding to a network redirector this results in an i/0 operation that accesses the file across the network as mentioned  objects are instances of an the object type specifies how instances are to be allocated  the definitions of the data fields  and the implementation of the standard set of virtual functions used for all objects these functions implement operations such as mapping names to objects  closing and deleting  and applying security the object manager keeps track of two counts for each object the pointer count is the number of distinct references made to an object protected-mode code that refers to objects must keep a reference on the objects to ensure that an object is not deleted while in use the handle count is the number of handle table entries referring to an object each handle is also reflected in the reference count when a handle for an object is closed  the object 's close routine is called in the case of file objects  this call causes the i/0 manager to do a cleanup operation at the close of the last handle the cleanup operation tells the file system that the file is no longer accessed by user mode so that sharing restrictions  range locks  and other states specific to the corresponding open routine can be removed each handle close removes a reference from the pointer count  but internal system components may retain additional references when the final reference is removed  the object 's delete procedure is called again using file objects as an example  the delete procedure causes the i/0 manager to send the file system a close operation on the file object this causes the file system to deallocate any internal data structures that were allocated for the file object after the delete procedure for a temporary object completes  the object is deleted from memory the object manager can make an object permanent  at least with respect to the current boot of the system  by taking an extra reference against the object thus  permanent objects are not deleted even when the last reference outside the object manager is removed when a permanent object is made temporary again  the object manager removes the extra reference if this was the last reference  the object is deleted permanent objects are rare  used mostly for devices  drive-letter mappings  and the directory and symbolic link objects the job of the object manager is to supervise the use of all managed objects when a thread wants to use an object  it calls the object manager 's open   method to get a reference to the object if the object is being opened from a user-mode api  the reference is inserted into the process 's object table  and a handle is returned a process gets a handle by creating an object  by opening an existing object  by receiving a duplicated handle from another process  or by inheriting a handle from a similar to the way a unix process gets a file descriptor these handles are all stored in the process 's an entry in the object table contains the access rights and states whether the handle should be inherited by when a process terminates  windows xp automatically closes all the process 's open handles 860 chapter 22 are a standardized interface to all kinds of objects like a file descriptor in unix  an object handle is an identifier unique to a process that confers the ability to access and manipulate a system resource handles can be duplicated within a process or between processes the latter case is used whert child processes are created and when out-of-process execution contexts are implemented since the object manager is the only entity that generates object handles  it is the natural place to check security the object manager checks whether a process has the right to access an object when the process tries to open the object the object manager also enforces quotas  such as the maximum amount of memory a process may use  by charging a process for the memory occupied by all its referenced objects and refusing to allocate more memory when the accumulated charges exceed the process 's quota when the login process authenticates a user  an access token is attached to the user 's process the access token contains information such as the security id  group ids  privileges  primary group  and default access-control list the services and objects a user can access are determined by these attributes the token that controls access is associated with the thread making the access normally  the thread token is missing and defaults to the process token  but services often need to execute code on behalf of their clients windows xp allows threads to impersonate a client temporarily by using a client 's token thus  the thread token is not necessarily the same as the process token in windows xp  each object is protected by an access-control list that contains the security ids and access rights granted when a thread attempts to access an object  the system compares the security id in the thread 's access token with the object 's access-control list to determine whether access should be permitted the check is performed only when an object is opened  so it is not possible to deny access after the open occurs operating-system components executing in kernel mode bypass the access check  since kernel-mode code is assumed to be trusted therefore  kernel-mode code must avoid security vulnerabilities  such as leaving checks disabled while creating a user-modeaccessible handle in an untrusted process generally  the creator of the object determines the access-control list for the object if none is explicitly supplied  one may be set to a default by the object type 's open routine  or a default list may be obtained from the user 's access-token object the access token has a field that controls auditing of object accesses operations that are being audited are logged to the system 's security log with an identification of the user an administrator monitors this log to discover attempts to break into the system or to access protected objects 22.3.3.2 virtual memory manager the executive component that manages the virtual address space  physical memory allocation  and paging is the the design of the vm manager assumes that the underlying hardware supports virtual-to-physical mapping  a paging mechanisnc  and transparent cache coherence on multiprocessor systems  as well as allowing multiple pagetable entries to map to the same physical page frame the vm manager in windows xp uses a page-based management scheme with a page size of 4 kb on 22.3 861 la32-compatible processors and 8 kb on the ia64 pages of data allocated to a process that are not in physical memory are either stored in the on disk or mapped directly to a regular file on a local or remote file system pages can also be marked zero-fill-on-demand  which fills the page with zeros before they are allocated  thus erasing the previous contents on la32 processors  each process has a 4-gb virtual address space the upper 2 gb are mostly identical for all processes and are used by windows xp in kernel mode to access the operating-system code and data structures key areas of the kernel-mode region that are not identical for all processes are the and the hardware references a process 's page using physical page-frame numbers the vm manager maps the page tables into a single 4-mb region in the process 's address space so they are accessed through virtual addresses hyperspace maps the current process 's working-set information into the kernel-mode address space session space is used to share the win32 and other session-specific drivers among all the processes in the same terminal-server session  rather than all the processes in the system the lower 2 gb are specific to each process and are accessible by both user and kernel-mode threads certain configurations of windows xp reserve only 1 gb for operating-system use  allowing a process to use 3gb of address space rum1ing the system in 3-gb mode drastically reduces the amount of data caching in the kernel however  for large applications that manage their own i/0  such as sql databases  the advantage of a larger user-mode address space may be worth the loss of caching the windows xp vm manager uses a two-step process to allocate user memory the first step reserves a portion of the process 's virtual address space the second step commits the allocation by assigning virtual memory space  physical memory or space in the paging files   windows xp limits the amount of virtual memory space a process consumes by enforcing a quota on committed memory a process decommits memory that it is no longer using to free up virtual memory for use by other processes the apis used to reserve virtual addresses and commit virtual memory take a handle on a process object as a parameter this allows one process to control the virtual memory of another environmental subsystems manage the memory of their client processes in this way for performance  the vm manager allows a privileged process to lock selected pages in physical memory  thus ensuring that the pages are not paged out to the paging file processes also allocate raw physical mernory and then map regions into its virtual address space ia32 processors with the physical address extension  pae  feature can have up to 64gb of physical memory on a system this memory can not all be mapped in a process 's address space at once  but windows xp makes it available using the address windowing extension  awe  apis  which allocate physical memory and then map regions of virtual addresses in the process 's address space onto part of the physical memory the awe facility is used primarily by very large applications such as the sql database windows xp implements shared memory by defining a after getting a handle to a section object  a process maps the memory portion it needs into its address space this portion is called a  a process redefines its view of an object to gain access to the entire object  one region at a time 862 chapter 22 a process can control the use of a shared-memory section object in many ways the maximum size of a section can be bounded the section can be backed by disk either in the system-paging file or in a regular file  a a section can be based  meaning the section appears at the same virtual address for all processes attempting to access it finally  the memory protection of pages in the section can be set to read-only  read -write  read-write-execute  execute-only  no access  or copy-on-write let 's look more closely at the last two of these protection settings  a no-access page raises an exception if accessed ; the exception is used  for example  to check whether a faulty program iterates beyond the end of an array both the user-mode memory allocator and the special kernel allocator used by the device verifier can be configured to map each allocation onto the end of a page followed by a no-access page to detect buffer overruns the copy-on-write mechanism enables the vm manager to use physical memory more efficiently when two processes want independent copies of an object  the vm manager places a single shared copy into virtual memory and activates the copy-on-write property for that region of memory if one of the processes tries to modify data in a copy-on-write page  the vm manager makes a private copy of the page for the process the virtual address translation in windows xp uses a multilevel page table for ia32 processors without the physical address extensions enabled  each process has a that contains 1,024  pdes  4 bytes in size each pde points to a that contains 1,024 4 bytes in size each pte points to a 4-kb in physical memory the total size of all page tables for a process is 4mb  so the vm manager pages out individual tables to disk when necessary see figure 22.3 for a diagram of this structure the page directory and page tables are referenced by the hardware via physical addresses to improve performance  the vm manager self-maps the page directory and page tables into a 4-mb region of virtual addresses the self-map allows the vm manager to translate a virtual address into the corresponding pde or pte without additional memory accesses when a process context is changed  a single page-directory entry must be changed to map the new process 's page tables for a variety of reasons  the hardware requires that each page directory or page table occupy a sii gle page thus  the number of pdes or ptes that fit in a page determine how virtual addresses are translated we next consider how virtual addresses are translated into physical addresses on ia32-compatible processors  without pae enabled   a 10-bit value can represent all the values from 0 to 1,023 thus  a 10-bit value can select any entry in the page directory or in a page table this property is used when a virtual address pointer is translated to a byte address in physical memory a 32-bit virtual-memory address is split into three values  as shown in figure 22.4 the first 10 bits of the virtual address are used as an index into the page directory this address selects one page-directory entry  pde   which contains the physical page frame of a page table the memory-management unit  mmu  uses the next 10 bits of the virtual address to select a pte from the page table 22.3 863 figure 22.3 page table layout the pte specifies a page frame in physical memory the remaining 12 bits of the virtual address are the offset of a specific byte in the page frame the mmu creates a pointer to the specific byte in physical memory by concatenating the 20 bits from the pte with the lower 12 bits from the virtual address thus  the 32-bit pte has 12 bits to describe the state of the physical page the ia32 hardware reserves 3 bits for use by the operating system the rest of the bits specify whether the page has been accessed or written  the caching attributes  the access mode  whether the page is globat and whether the pte is valid ia32 processors running with pae enabled use 64-bit pdes and ptes in order to represent the larger 24-bit page-frame number field thus  the secondlevel page directories and the page tables contain only 512 pdes and ptes  respectively to provide 4 gb of virtual address space requires an extra level of page directory containing four pdes translation of a 32-bit virtual address uses 2 bits for the top-level directory index and 9 bits for each of the second-level page directories and the page tables to avoid the overhead of translating every virtual address by looking up the pde and pte  processors use a which contains an associative memory cache for mapping virtual pages to ptes unlike the ia32 architecture  in which the tlb is maintained by the 31 0 figure 22.4 virtual-to-physical address translation on ia32 864 chapter 22 hardware mmu  the ta64 invokes a software-trap routine to supply translations missing from the tlb this gives the vm manager flexibility in choosing the data structures to use in windows xp  a three-level tree structure is chosen for rnapping user-mode virtual addresses on the ia64 on ia64 processors  the page size is 8 kb  but the ptes occupy 64 bits  so a page still contains only 1,024  10 bits ' worth  of pdes or ptes therefore  with 10 bits of top-level pdes  10 bits of second-level pdes  10 bits of page table  and 13 bits of page offset  the user portion of the process 's virtual address space for windows xp on the ia64 is 8 tb  43 bits ' worth   the 8-tb limitation in the current version of windows xp does not take full advantage of the capabilities of the ia64 processor but represents a tradeoff between the number of memory references required to handle tlb misses and the size of the user-mode address space supported a physical page can be in one of six states  valid  free  zeroed  modified  standby  bad  or in transition a valid page is in use by an active process a free page is a page that is not referenced in a pte a zeroed page is a free page that has been zeroed out and is ready for immediate use to satisfy zero-on-demand faults a modified page has been written by a process and must be sent to the disk before it is allocated for another process a standby page is a copy of information already stored on disk standby pages can be pages that were not modified  modified pages that have already been written to the disk  or pages that were prefetched to exploit locality a bad page is unusable because a hardware error has been detected finally  a transition page is on its way in from disk to a page frame allocated in physical memory when the valid bit in a pte is zero  the vm manager defines the format of the other bits invalid pages can have a number of states represented by bits in the pte page-file pages that have never been faulted in are marked zero-ondemand files mapped through section objects encode a pointer to that section object pages that have been written to the page file contain enough information to find the page on disk  and so forth the actual structure of the page-file pte is shown in figure 22.5 the pte contains 5 bits for page protection  20 bits for page-file offset  4 bits to select the paging file  and 3 bits that describe the page state a page-file pte is marked to be an invalid virtual address to the mmu since executable code and memorymapped files already have a copy on disk  they do not need space in a paging file i one of these pages is not in physical memory  the pte structure is as follows  the most significant bit is used to specify the page protection  the next 28 bits are used to index into a system data structure that indicates a file and offset within the file for the page  and the lower 3 bits specify the page state invalid virtual addresses can also be in a number of temporary states that are part of the paging algorithms when a page is removed from a processs 22.3 865 31 0 figure 22.5 page-file page-table entry the valid bit is zero working set  it is moved either to the modified list  to be written to disk  or directly to the standby list if written to the standby list  the page is reclaimed without being read from disk if it is needed again before it is moved to the free list when possible  the vm manager uses idle cpu cycles to zero pages on the free list and move them to the zeroed list transition pages have been allocated a physical page and are awaiting the completion of the paging i/0 before the pte is marked as valid windows xp uses section objects to describe pages that are sharable between processes each process has its own set of virtual page tables  but the section object also includes a set of page tables containing the master  or prototype  pies when a pte in a process page table is marked valid  it points to the physical page frame containing the page  as it must on ia32 processors  where the hardware mmu reads the page tables directly from memory but when a shared page is made invalid  the pte is edited to point to the prototype pte associated with the section object the page tables associated with a section object are virtual insofar as they are created and trimmed as needed the only prototype pies needed are those that describe pages for which there is a currently mapped view this greatly improves performance and allows more efficient use of kernel virtual addresses the prototype pte contains the page-frame address and the protection and state bits thus  the first access by a process to a shared page generates a page fault after the first access  further accesses are performed in the normal manner if a process writes to a copy-on-write page marked read-only in the pte  the vm manager makes a copy of the page and marks the pte writable  and the process effectively does not have a shared page any longer shared pages never appear in the page file but are instead found in the file system the vm manager keeps track of all pages of physical m.emory in a there is one entry for every page of physical memory in system the entry points to the pte  which in turn points to the page frame  so the vm n1.anager can maintain the state of the page page frames not referenced by a valid pte are linked to lists according to page type  such as zeroed  modified  or free if a shared physical page is marked as valid for any process  the page can not be removed from memory the vm manager keeps a count of valid pies for each page in the page-frame database when the count goes to zero  the physical page can be reused once its contents have been written back to disk  if it was marked dirty   when a page fault occurs  the vm manager finds a physical page to hold the data for zero-on-demand pages  the first choice is to find a page that has 866 chapter 22 already been zeroed if none is available  a page from the free list or standby list is chosen  and the page is zeroed if the faulted page has been marked as in transition  it is either already being read in fron1 disk or has been unmapped or trimmed and is still available on the standby or modified list the thread either waits for the l/0 to complete or  in the latter cases  reclaims the page from the appropriate list otherwise  an i/0 must be issued to read the page in from the paging file or file systein the vm manager tries to allocate an available page from either the free list or the standby list pages in the modified list can not be used until they have been written back to disk and transferred to the standby list if no pages are available  the thread blocks until the working-set manager trims pages from memory or until a page in physical memory is unmapped by a process windows xp uses a per-process first-in  first-out  fifo  replacement policy to take pages from processes as appropriate when a process is started  it is assigned a default minimum working-set size of 50 pages windows xp monitors the page faulting of each process that is at its minimum working-set size and adjusts the working-set size accordingly the vm manager replaces and trims pages in the working set of a process according to their age the age of a page is determin.ed by how many trimming cycles have occurred without the pte trimmed pages are moved to the standby or modified list  depending on whether the modified bit is set in the page 's pte the vm manager does not fault in only the page immediately needed research shows that the memory referencing of a thread tends to have a property ; when a page is used  it is likely that adjacent pages will be referenced in the near future  think of iterating over an array or fetching sequential instructions that form the executable code for a thread  because of locality  when the vm manager faults in a page  it also faults in a few adjacent pages this prefetching tends to reduce the total number of page faults writes are also clustered to reduce the number of independent t/0 operations in addition to managing committed memory  the vm manager manages each process 's reserved memory  or virtual address space each process has an associated splay tree that describes the ranges of virtual addresses in use and what the uses are this allows the vm manager to fault in page tables as needed if the pte for a faulting address does not exist  the vm manager searches for the address in the process 's tree of and uses this information to fill in the missing pte and retrieve the page in some cases  a page-table page itself may not exist ; such a page must be transparently allocated and initialized by the vm manager 22.3.3.3 process manager the windows xp process manager provides services for creating  deleting  and using processes  threads  and jobs it has no kn.owledge about parent-child relationships or process hierarchies ; those refinements are left to the particular environmental subsystem that owns the process the process manager is also not involved in the scheduling of processes  other than setting the priorities and affinities in processes and threads when they are created thread scheduling takes place in the kernel dispatcher each process contains one or more threads processes themselves can be collected into large units called the use of job objects allows limits to 22.3 867 be placed on cpu usage  working-set size  and processor affinities that control multiple processes at once job objects are used to manage large data-center machines an example of process creation in the win32 api environncent is as follows  a win32 api application calls createprocess    a message is sent to the win32 api subsystem to notify it that the process is being created createprocess   in the original process then calls an api in the process manager of the nt executive to actually create the process the process manager calls the object manager to create a process object and returns the object handle to win32 api win32 api calls the process manager again to create a thread for the process and returns handles to the new process and thread the windows xp apis for manipulating virtual memory and threads and for duplicating handles take a process handle  so subsystems can perform operations on behalf of a new process without having to execute directly in the new process 's context once a new process is created  the initial thread is created  and an asynchronous procedure call is delivered to the thread to prompt the start of execution at the user-mode image loader the loader is an ntdll.dll  which is a link library automatically mapped into every newly created process windows xp also supports a unix fork   style of process creation in order to support the posix environmental subsystem although the win32 api environment calls the process manager from the client process  posix uses the cross-process nature of the windows xp apis to create the new process from within the subsystem process the process manager also implements the queuing and delivery of asynchronous procedure calls  apcs  to threads apcs are used by the system to initiate thread execution  complete i/0  terminate threads and processes  and attach debuggers user-mode code can also queue an apc to a thread for delivery of signal-like notifications to support posix  the process manager provides apis that send alerts to threads to unblock them from system calls the debugger support in the process manager includes the capability to suspend and resume threads and to create threads that begin in a suspended mode there are also process-manager apis that get and set a thread 's register context and access another process 's virtual memory threads can be created in the current process ; they can also be injected into another process within the executive  existing threads can temporarily attach to another process this method is used by worker threads that need to execute in the context of the process originating a work request the process manager also supports impersonation a thread running in a process with a security token belonging to one user can set a thread-specific token belonging to another user this facility is fundamental to the clientserver computing model  where services need to act on behalf of a variety of clients with different security ids 868 chapter 22 22.3.3.4 local procedure call facility the implementation of windows xp ~ uses a client-server nlodel the environn ental subsystems are servers that implement particular operating-system personalities the client-server model is used for implementing a variety of operating-system services besides the environmental subsystems security management  printer spooling  web services  network file systems  plug-andplay  and many other features are implemented using this model to reduce the memory footprint  multiple services are often collected into a few processes  which then rely on the user-mode thread-pool facilities to share threads and wait for messages  see section 22.3.3.3   the operating system uses the local procedure call  lpc  facility to pass requests and results between client and server processes within a single machine in particular  lpc is used to request services from the various windows xp subsystems lpc is similar in many respects to the rpc mechanisms used by many operating systems for distributed processing across networks  but lpc is optimized for use within a single system the windows xp implementation of open software foundation  osf  rpc often uses lpc as a transport on the local machine lpc is a message-passing mechanism the server process publishes a globally visible connection-port object when a client wants services from a subsystem  it opens a handle to the subsystem 's corn1.ection-port object and sends a connection request to the port the server creates a channel and returns a handle to the client the charn1.el consists of a pair of private communication ports  one for client-to-server messages and the other for server-to-client messages communication channels support a callback mechanism  so the client and server can accept requests when they would normally be expecting a reply when an lpc channel is created  one of three message-passing tecl111.iques must be specified the first technique is suitable for small messages  up to a few hundred bytes   in this case  the port 's message queue is used as intermediate storage  and the messages are copied from one process to the other the second technique is for larger messages in this case  a sharedmemory section object is created for the channel messages sent through the port 's message queue contain a pointer and size information referring to the section object this avoids the need to copy large messages the sender places data into the shared section  and the receiver views them directly the third teclu ique uses apis that read and write directly into a process 's address space the lpc provides functions and synchronization so that a server can access the data in a client the win32 api window manager uses its own form of message passing  which is independent of the executive lpc facilities when a client asks for a connection that uses window-manager messaging  the server sets up three objects   1  a dedicated server thread to handle requests   2  a 64-kb section object  and  3  an event-pair object an event-pair object is a synchronization 22.3 869 object used by the win32 api subsysten to provide notification when the client thread has copied a message to the win32 api server  or vice versa the section object passes the messages  and the event-pair object performs synchronization window-manager messaging has several advantages  the section object eliminates message copying  since it represents a region of shared memory the event-pair object eliminates the overhead of using the port object to pass messages containing pointers and lengths the dedicated server thread eliminates the overhead of determining which client thread is calling the server  since there is one server thread per client thread the kernel gives scheduling preference to these dedicated server threads to improve performance 22.3.3.5 i/0 manager the is responsible for file systems  device drivers  and network drivers it keeps track of which device drivers  filter drivers  and file systems are loaded  and it also manages buffers for i/o requests it works with the vm manager to provide memory-mapped file i/0 and controls the windows xp cache manager  which handles caching for the entire i/0 system the i/0 manager is fundamentally asynchronous  providing synchronous i/0 by explicitly waiting for an i/0 operation to complete the i/o manager provides several models of asynchronous i/0 completion  including setting of events  delivery of apcs to initiating threads  and use of i/0 completion ports  which allow a single thread to process i/0 completions from many other threads device drivers are arranged in a list for each device  called a driver or i/0 stack   the i/0 manager converts the requests it receives into a standard form called an  it then forwards the irp to the first driver in the stack for processing after a driver processes the irp  it calls the i/0 manager either to forward the irp to the next driver in the stack or  if all processing is finished  to complete the operation on the irp the i/0 request may be completed in a context different from the one in which it was made for example  if a driver is performing its part of an i/0 operation and is forced to block for an extended time  it may queue the irp to a worker thread to continue processing in the system context in the original thread  the driver returns a status indicating that the i/0 request is pending so that the thread can continue executing in parallel with the i/0 operation an irp may also be processed in interrupt-service routines and completed in an arbitrary context because some final processing may need to take place in the context that initiated the i/0  the i/0 manager uses an apc to do final i/o-completion processing in the context of the originating thread the stack model is very flexible as a driver stack is built  various drivers have the opportunity to insert themselves into the stack as filter drivers can examine and potentially modify each l/0 operation mount management  partition management  and disk striping and mirroring are all examples of functionality implemented using filter drivers that execute beneath the file system in the stack file-system filter drivers execute above the file 870 chapter 22 system and have been used to implement functionalities such as hierarchical storage management single instancing of files for remote boot  and dynamic forncat conversion third parties also use file-system filter drivers to implement virus detection device drivers for windows xp are written to the windows driver model  wdm  specification this model lays out all the requirements for device drivers  including how to layer filter drivers  share common code for handling power and plug-and-play requests  build correct cancellation logic  and so forth because of the richness of the wdm  writing a full wdm device driver for each new hardware device can involve an excessive amount of work fortunately  the port/miniport model makes it unnecessary to do this within a class of similar devices  such as audio drivers  scsi devices  or ethernet controllers  each instance of a device shares a common driver for that class  called a the port driver implements the standard operations for the class and then calls device-specific routines in the device 's to implement device-specific functionality 22.3.3.6 cache manager in many operating systems  caching is done by the file system instead  windows xp provides a centralized caching facility the works closely with the vm manager to provide cache services for all components under the control of the i/0 manager caching in windows xp is based on files rather than raw blocks the size of the cache changes dynamically according to how much free memory is available in the system recall that the upper 2gb of a process 's address space comprise the system area ; it is available in the context of all processes the vm manager allocates up to one-half of this space to the system cache the cache manager maps files into this address space and uses the capabilities of the vm manager to handle file i/0 the cache is divided into blocks of 256 kb each cache block can hold a view  that is  a memory-mapped region  of a file each cache block is described by a control block  vacb  that stores the virtual address and file offset for the view  as well as the number of processes using the view the vacbs reside in a single array maintained by the cache manager for each open file  the cache manager maintains a separate vacb index array that describes the caching for the entire file this array has an entry for each 256-kb chunk of the file ; so  for instance  a 2-mb file would have an s-entry vacb index array an entry in the vacb index array points to the vacb if that portion of the file is in the cache ; it is null otherwise when the i/0 manager receives a file 's user-level read request  the i/0 manager sends an irp to the device-driver stack on which the file resides the file system attempts to look up the requested data in the cache manager  unless the request specifically asks for a noncached read   the cache manager calculates which entry of that file 's vacb index array corresponds to the byte offset of the request the entry either points to the view in the cache or is invalid if it is invalid  the cache manager allocates a cache block  and the corresponding entry in the vacb array  and maps the view into the cache block the cache manager then attempts to copy data from the mapped file to the caller 's buffer if the copy succeeds  the operation is completed 22.3 871 cached 1/0 page fault figure 22.6 file 1/0 if the copy fails  it does so because of a page fault  which causes the vm manager to send a non cached read request to the i/ 0 manager the i/ 0 manager sends another request down the driver stack  this time requesting a paging operation  which bypasses the cache manager and reads the data from the file directly into the page allocated for the cache manager upon completion  the vacb is set to point at the page the data  now in the cache  are copied to the caller 's buffer  and the original i/0 request is completed figure 22.6 shows an overview of these operations when possible  for synchronous operations on cached files  i/0 is handled by the mechanism this mechanism parallels the normal irp-based i/0 but calls into the driver stack directly rather than passing down an irp because no irp is involved  the operation should not block for an extended period of time and can not be queued to a worker thread therefore  when the operation reaches the file system and calls the cache manager  the operation fails if the information is not already in cache the i/0 manager then attempts the operation using the normal irp path a kernel-level read operation is similar  except that the data can be accessed directly from the cache  rather than being copied to a buffer in user space to use file-system metadata  data structures that describe the file system   the kernel uses the cache manager 's mapping interface to read the metadata to the metadata  the file system uses the cache manager 's pinning interface a page locks the page into a physical-memory page frame so that the vm manager can not move or page out the page after updating the metadata  the file system asks the cache manager to unpin the page a modified page is marked dirty  and so the vm manager flushes the page to disk the metadata are stored in a regular file to improve performance  the cache manager keeps a small history of read requests and from this history attempts to predict future requests if the cache manager finds a pattern in the previous three requests  such as sequential access forward or backward  it prefetches data into the cache before the next request is submitted by the application in this way  the application finds its data already 872 chapter 22 cached and does not need to wait for disk i/0 the win32 apt openfile   and createfile   functions can be passed the file_flag_sequentialscan flag  which is a hint to the cache manager to try to prefetch 192 kb ahead of the thread 's requests typically  windows xp performs i/0 operations in chunks of 64 kg  or 16 pages ; thus  this read-ahead is three times the normal amount the cache manager is also responsible for telling the vm manager to flush the contents of the cache the cache manager 's default behavior is write-back caching  it accumulates writes for 4 to 5 seconds and then wakes up the cachewriter thread when write-through caching is needed  a process can set a flag when opening the file  or the process can call an explicit cache-flush function a fast-writing process could potentially fill all the free cache pages before the cache-writer thread had a chance to wake up and flush the pages to disk the cache writer prevents a process from flooding the system in the following way w11en the amount of free cache memory becomes low  the cache manager temporarily blocks processes attempting to write data and wakes the cachewriter thread to flush pages to disk if the fast-writing process is actually a network redirector for a network file system  blocking it for too long could cause network transfers to time out and be retransmitted this retransmission would waste network bandwidth to prevent such waste  network redirectors can instruct the cache manager to limit the backlog of writes in the cache because a network file system needs to move data between a disk and the network interface  the cache manager also provides a dma interface to move the data directly moving data directly avoids the need to copy data through an intermediate buffer 22.3.3.7 security reference monitor centralizing management of system entities in the object manager enables windows xp to use a uniform mechanism to perform run-time access validation and audit checks for every user-accessible in the system whenever a process opens a handle to an object  the checks the process 's security token and the object 's access-control list to see whether the process has the necessary rights the srm is also responsible for manipulating the privileges in security tokens special privileges are required for users to perform backup or restore operations on file systems  debug processes  and so forth tokens can also be marked as being restricted in their privileges so that they can not access objects that are available to most users restricted tokens are primarily used to restrict the damage that can be done by execution of untrusted code another responsibility of the srm is logging security audit events a c-2 security rating requires that the system have the ability to detect and log all attempts to access system resources so that it is easier to trace attempts at unauthorized access because the srm is responsible for making access checks  it generates most of the audit records in the security-event log 22.3.3.8 plug-and-play and power managers the operating system uses the plug--and-play to recognize and adapt to changes in the hardware configuration for pnp to work  both the device and the driver must support the pnp standard the pnp manager automatically recognizes installed devices and detects changes in devices as the 22.3 873 system operates the manager also keeps track of resources used by a device  as well as potential resources that could be used  and takes care of loading the appropriate drivers this management of hardware resources-primarily interrupts and i/o memory ranges-has the goal of determining a hardware configuration in which all devices are able to operate for example  if device b can use interrupt 5 and device a can use 5 or 7  then the pnp manager will assign 5 to band 7 to a in previous versions  the user might have had to remove device a and reconfigure it to use interrupt 7 before installing device b the user thus had to study system resources before installing new hardware and had to determine which devices were using which hardware resources the proliferation of pcmcia cards  laptop docks  and usb  ieee 1394  infiniband  and other hot-pluggable devices also dictates the support of dynamically configurable resources the pnp manager handles dynamic reconfiguration as follows first  it gets a list of devices from each bus driver  for example  pci  usb   it loads the installed driver  or installs one  if necessary  and sends an add-device request to the appropriate driver for each device the pnp manager figures out the optimal resource assignments and sends a start-device request to each driver  along with the resource assignment for the device if a device needs to be reconfigured  the pnp manager sends a query-stop request  which asks the driver whether the device can be temporarily disabled if the driver can disable the device  then all pending operations are completed  and new operations are prevented from starting next  the pnp manager sends a stop request ; it can then reconfigure the device with another start-device request the pnp manager also supports other requests  such as query-remove this request which is used when the user is getting ready to eject a pccard device  operates in a fashion similar to query-stop the surprise-remove request is used when a device fails 01 ~ more likely  when a user removes a pccard device without stopping it first the remove request tells the driver to stop using the device and release all resources allocated to it windows xp supports sophisticated power management although this feature is useful for home systems to reduce power consumption  it is primarily useful in promoting ease of use  quicker access  and extending the battery life of laptops the system and individual devices can be moved to low-power mode  called standby or sleep mode  when not in use  so the battery is primarily directed at physical memory  ram  data retention the system can turn itself back on when packets are received from the network  a phone line to a modem rings  or a user opens a laptop or pushes a soft power button windows xp can also hibernate a system by storing physical memory contents to disk  completely shutting down the machine  and then restoring the system at a later point before execution continues further strategies for reducing power consumption are supported as well rather than allowing it to spin in a processor loop when the cpu is idle  windows xp moves the system to a state requiring lower power consumption if the cpu is underutilized  windows xp reduces the cpu clock speed  which can save significant power 22.3.3.9 registry windows xp keeps much of its configuration information in an internal database called the a registry database is called a there are 874 chapter 22 22.4 separate hives for system information  default user preferences  software installation  and security because the information in the is required to boot the system  the registry manager is implemented as a component of the executive every time the system successfully boots  it saves the system hive as last known good if the user installs software  such as a device driver  that produces a system-hive configuration that will not boot  the user can usually boot using the last-known-good configuration damage to the system hive from installing third-party applications and drivers is so common that windows xp has a component called that periodically saves the hives  as well as other software states like driver executables and configuration files  so that the system can be restored to a previously working state in cases where the system boots but no longer operates as expected 22.3.3.10 booting the booting of a windows xp pc begins when the hardware powers on and the bios begins executing from rom the bios identifies the to be booted and loads and executes the bootstrap loader from the front of the disk this loader knows enough about the file-system format to load the ntldr program from the root directory of the system device ntldr is used to determine which contains the operating system next  the ntldr loads in the hal library  the kernel  and the system hive from the boot device from the system hive  it determines what device drivers are needed to boot the system  the boot drivers  and loads them finally  ntldr begins kernel execution the kernel initializes the system and creates two processes the contains all the internal worker threads and never executes in user mode the first user-mode process created is smss  which is similar to the init  initialization  process in unix smss does further initialization of the system  including establishing the paging files and loading device drivers  and creates the winlogon and csrss processes csrss is the win32 api subsystem winlogon brings up the rest of the system  including the lsass security subsystem and the remaining services needed to run the system the system optimizes the boot process by pre-loading files from disk based on previous boots of the system disk access patterns at boot are also used to lay out system files on disk to reduce the number of i/0 operations required the processes required to start the system are reduced by grouping services into one process all of these approaches contribute to a dramatic reduction in system boot time of course  system boot time is less important than it once was because of the sleep and hibernation capabilities of windows xp  which allow users to power down their computers and then quickly resume where they left off environmental subsystems are user-mode processes layered over the native windows xp executive services to enable windows xp to run programs 22.4 875 developed for other operating systems  including 16-bit windows  ms-dos  and posix each environmental subsystem provides a single application en vironm.en t windows xp uses the win32 api subsysten1 as the main operating environment  and thus this subsystem starts all processes when an application is executed  the win32 api subsystem calls the vm manager to load the application 's executable code the memory manager returns a status to win32 indicating the type of executable if it is not a native win32 api executable  the win32 api environment checks whether the appropriate environmental subsystem is running ; if the subsystem is not running  it is started as a user-mode process the subsystem then takes control over the application startup the environmental subsystems use the lpc facility to provide operatingsystem services to client processes the windows xp subsystem architecture keeps applications from mixing api routines from different environments for instance  a win32 api application cam1.ot make a posix system call  because only one environmental subsystem can be associated with each process since each subsystem is run as a separate user-mode process  a crash in one has no effect on other processes the exception is win32 api  which provides all keyboard  mouse  and graphical display capabilities if it fails  the system is effectively disabled and requires a reboot the win32 api environment categorizes applications as either graphical or character based  where a character-based application is one that thinks interactive output goes to a character-based  command  window win32 api transforms the output of a character-based application to a graphical representation in the command window this transformation is easy  whenever an output routine is called  the environmental subsystem calls a win32 routine to display the text since the win32 api environment performs this function for all characterbased windows  it can transfer screen text between windows via the clipboard this transformation works for ms-dos applications  as well as for posix command-line applications 22.4.1 ms-dos environment the ms-dos environment does not have the complexity of the other windows xp environmental subsystems it is provided by a win32 api application called the since the vdm is a user-mode process  it is and dispatched like any other windows xp application the vdm has to execute or emulate intel 486 instructions vdm also provides routines to emulate the ms-dos rom bios and int 21 software-interrupt services and has virtual device drivers for the screen  keyboard  and communication ports the vdm is based on ms-dos 5.0 source code ; it allocates at least 620 kb of memory to the application the windows xp command shell is a program that creates a window that looks like an ms-dos environment it can run both 16-bit and 32-bit executables when an ms-dos application is run  the command shell starts a vdm process to execute the program if windows xp is running on a ia32-compatible processor  ms-dos graphical applications run in full-screen mode  and character applications can run full screen or in a window not all ms-dos applications run under the vdm for example  some ms-dos applications access the disk hardware directly  so they 876 chapter 22 fail to run on windows xp because disk access is restricted to protect the file system in general  ms-dos applications that directly access hardware will fail to operate under windows xp since ms-dos is not a multitasking environment  some applications have been written in such a way as to hog the cpu for instance  the use of busy loops can cause time delays or pauses in execution the scheduler in the kernel dispatcher detects such delays and automatically throttles the cpu usage  but this may cause the offending application to operate incorrectly 22.4.2 16-bit windows environment the win16 execution environment is provided by a vdm that incorporates additional software called windows on windows  wow32 for 16-bit applications  ; this software provides the windows 3.1 kernel routines and stub routines for window-manager and graphical-device-interface  gdi  functions the call the appropriate win32 api subroutines-converting  or thunlcing  16-bit addresses into 32-bit addresses applications that rely on the internal structure of the 16-bit window manager or gdi may not work  because the underlying win32 api implementation is  of course  different from true 16-bit windows wow32 can multi task with other processes on windows xp  but it resembles windows 3.1 in many ways only one win16 application can run at a time  all applications are single tlueaded and reside in the same address space  and all share the same input queue these features imply that an application that stops receivil1.g input will block all the other win16 applications  just as ill wi11.dows 3.x  and one win16 application can crash other win16 applications by corrupting the address space the user can enable multiple win16 environments to coexist  however  by usil1.g the command start /separate win16application from the command line there are relatively few 16-bit applications that users need to continue to run on wmdows xp  but some of them mclude common installation  setup  programs thus  the wow32 environment conti11.ues to exist primarily because a number of 32-bit applications can not be il1.stalled on windows xp without it 22.4.3 32-bit windows environment on ia64 the native environment for windows on ia64 uses 64-bit addresses and the native ia64 il1.struction set to execute ia32 programs in this environment requires a thunking layer to translate 32-bit win32 api calls into the corresponding 64-bit calls-just as 16-bit applications require translation on ia32 systems thus  64-bit windows supports the wow64 environment the implementations of 32-bit and 64-bit windows are essentially identical  and the ia64 processor provides direct execution of ia32 instructions  so wow64 achieves a higher level of compatibility than wow32 22.4.4 win32 environment as mentioned earlier  the main subsystem in windows xp is the win32 api  which runs win32 api applications and manages all keyboard  mouse  and screen i/0 since it is the controlling environment  it is designed to be extremely robust several features of the win32 api contribute to this robushl.ess unlike 22.4 877 processes in the win16 environment  each win32 process has its own input queue the window manager dispatches all input on the system to the appropriate process 's input queue  so a failed process does not block input to other processes the windows xp kernel also provides preemptive multitasking  which enables the user to terminate applications that have failed or are no longer needed the win32 api also validates all objects before using them  to prevent crashes that could otherwise occur if an application tried to use an invalid or wrong handle the win32 api subsystem verifies the type of the object to which a handle points before using the object the reference counts kept by the object manager prevent objects from being deleted while they are still being used and prevent their use after they have been deleted to achieve a high level of compatibility with windows 95/98 systems  windows xp allows users to specify that individual applications be run using a which modifies the win32 api to better approximate the behavior expected by old applications for example  some applications expect to see a particular version of the system and fail on new versions frequently  applications have latent bugs that become exposed due to changes in the implementation running an application with the windows 95/98 shims enabled causes the system to provide behavior much closer to windows 95/98 -though with reduced performance and limited interoperability with other applications 22.4.5 posix subsystem the posix subsysten1 is designed to nm posix applications written to follow the posix standard  which is based on the unix model posix applications can be started by the win32 api subsystem or by another posix application posix applications use t ~ e posix subsystem server psxss exe  the posix dynamic link library psxdll dll  and the posix console session manager po six exe although tl1e posix standard does not specify printing  posix applications can use printers transparently via the windows xp redirection mechanism posix applications have access to any file system on the windows xp system ; the posix environment enforces unix-like permissions on directory trees due to scheduling issues  the posix system in windows xp does not ship with the system but is available separately for professional desktop systems and servers it provides a much higher level of compatibility with unix applications than previous versions of nt of the commonly available unix applications  most compile and run without change with the latest version of interix 22.4.6 logon and security subsystems before a user can access objects on windows xp  that user must be authenticated by the logon service  winlogon winlogon is responsible for responding to the secure attention sequence  control-alt-delete   the secure attention sequence is a required n1echanism for keeping an application from acting as a trojan horse only winlogon can intercept this sequence in order to put up a logon screen  change passwords  and lock the workstation to be authenticated  a user must have an account and provide the password 878 chapter 22 22.5 that account alternatively  a user logs on by using a smart card and personal identification number  subject to the security policies in effect for the domain the local security authority subsystem  lsass  is the process that generates access tokens to represent users on the system it calls an to perform authentication using information from the logon subsystem or network server typically  the authentication package simply looks up the account information in a local database and checks to see that the password is correct the security subsystem then generates the access token for the user id containing the appropriate privileges  quota limits  and group ids whenever the user attempts to access an object in the system  such as by opening a handle to the object  the access token is passed to the security reference monitor  which checks privileges and quotas the default authentication package for windows xp domains is kerberos lsass also has the responsibility for implementing security policy  such as strong passwords ; for authenticating users ; and for performing encryption of data and keys historically  ms-dos systems have used the file-allocation table  fat  file system the 16-bit fat file system has several shortcomings  including internal fragmentation  a size limitation of 2gb  and a lack of access protection for files the 32-bit fat file system has solved the size and fragmentation problems  but its performance and features are still weak by comparison with modern file systems the ntfs file system is much better it was designed to include many features  including data recovery  security  fault tolerance  large files and file systems  multiple data streams  unicode names  sparse files  encryption  journaling  volume shadow copies  and file compression windows xp uses ntfs as its basic file system  and we focus on it here windows xp continues to use fat16  however  to read floppies and other removable media and despite the advantages of ntfs  fat32 continues to be important for interoperability of media with windows 95/98 systems windows xp supports additional file-system types for the common formats used for cd and dvd media 22.5.1 ntfs internal layout the fundamental entity in ntfs is a volume a volume is created by the windows xp logical disk management utility and is based on a logical disk partition a volume may occupy a portion of a disk  may occupy an entire disk  or may span several disks ntfs does not deal with individual sectors of a disk but instead uses clusters as the units of disk allocation a is a number of disk sectors that is a power of 2 the cluster size is configured when an ntfs file system is formatted the default cluster size is the sector size for volumes up to 512mb  1 kb for volumes up to 1 gb  2 kb for volumes up to 2 gb  and 4 kb for larger volumes this cluster size is much smaller than that for the 16-bit fat file system  and the small size reduces the amount of internal fragmentation as an example  consider a 1.6-gb disk with 16,000 files if you use a fat -16 file system  400 mb 22.5 879 may be lost to internal fragmentation because the cluster size is 32 kb under ntfs  only 17mb wo11ld be lost when storing the same files ntfs uses as disk addresses it assigns them by numbering from the beginning of the disk to the end using this scheme  the system can calculate a physical disk offset  in bytes  by multiplying the lcn by the cluster size a file in ntfs is not a simple byte stream as it is in ms-dos or unix ; rather  it is a structured object consisting of typed each attribute of a file is an independent byte stream that can be created  deleted  read  and written some attribute types are standard for all files  including the file name  or names  if the file has aliases  such as an ms-dos shortname   the creation time  and the security descriptor that specifies access control user data is stored in data attributes most traditional data files have an unnamed data attribute that contains all the file 's data however  additional data streams can be created with explicit names for instance  in macintosh files stored on a windows xp server  the resource fork is a named data stream the iprop interfaces of the component object model  com  use a named data stream to store properties on ordinary files  including thumbnails of images in general  attributes may be added as necessary and are accessed using a file-name  attribute syntax ntfs returns the size of the unnamed attribute only in response to file-query operations  such as when rmming the dir command every file in ntfs is described by one or more records in an array stored in a special file called the master file table  mft   the size of a record is determined when the file system is created ; it ranges from 1 to 4 kb small attributes are stored in the mft record itself and are called large attributes  such as the unnamed bulk data  are called and are stored in one or more contiguous on the disk ; a pointer to each extent is stored in the mft record for a small file  even the data attribute may fit inside the mft record if a file has many attributes-or if it is highly fragmented  so that many pointers are needed to point to all the fragments -one record in the mft might not be large enough in this case  the file is described by a record called the which contains pointers to overflow records that hold the additional pointers and attributes each file in an ntfs volume has a unique id called a  the file reference is a 64-bit quantity that consists of a 48-bit file number and a 16-bit sequence number the file number is the record number  that is  the array slot  in the mft that describes the file the sequence number is incremented every time an mft entry is reused the sequence number enables ntfs to perform internal consistency checks  such as catching a stale reference to a deleted file after the mft entry has been reused for a new file 22.5.1.1 ntfs b + tree as in ms-dos and unix  the ntfs namespace is organized as a hierarchy of directories each directory uses a data structure called a to store an index of the file names in that directory in a b + tree  the length of every path from the root of the tree to a leaf is the same  and the cost of reorganizing the tree is eliminated the of a directory contains the top level of the b + tree for a large directory  this top level contains pointers to disk extents 880 chapter 22 that hold the remainder of the tree each entry in the directory contains the name and file reference of the file  as well as a copy of the update timestamp and file size taken from the file 's resident attributes in the mft copies of this information are stored in the directory  so a directory listing can be efficiently generated because all the file names  sizes  and update times are available from the directory itself  there is no need to gather these attributes from the mft entries for each of the files 22.5.1.2 ntfs metadata the ntfs volume 's metadata are all stored in files the first file is the mft the second file  which is used during recovery if the mft is damaged  contains a copy of the first 16 entries of the mft the next few files are also special in purpose they include the files described below the records all metadata updates to the file system the contains the name of the volume  the version of ntfs that formatted the volume  and a bit that tells whether the volume may have been corrupted and needs to be checked for consistency the indicates which attribute types are used in the volume and what operations can be performed on each of them the is the top-level directory in the file-system hierarchy indicates which clusters on a volume are allocated to files and which are free the contains the startup code for windows xp and must be located at a particular disk address so that it can be found easily by a simple rom bootstrap loader the boot file also contains the physical address of the mft the keeps track of any bad areas on the volume ; ntfs uses this record for error recovery 22.5.2 recovery in many simple file systems  a power failure at the wrong time can damage the file-system data structures so severely that the entire volume is scrambled many versions of unix store redundant metadata on the disk  and they recover from crashes by using the f sck program to check all the file-system data structures and restore them forcibly to a consistent state restoring them often involves deleting damaged files and freeing data clusters that had been written with user data but not properly recorded in the file system 's metadata structures this checking can be a slow process and can cause the loss of significant amounts of data ntfs takes a different approach to file-system robustness in ntfs  all filesystem data-structure updates are performed inside transactions before a data structure is altered  the transaction writes a log record that contains redo and undo information ; after the data structure has been changed  the transaction writes a commit record to the log to signify that the transaction succeeded 22.5 881 after a crash  the system can restore the file-system data structures to a consistent state by processing the log records  first redoing the operations for committed transactions and then undoing the operations for transactions that did not commit successfully before the crash periodically  usually every 5 seconds   a checkpoint record is written to the log the system does not need log records prior to the checkpoint to recover from a crash they can be discarded  so the log file does not grow without bounds the first time after system_ startup that an ntfs volume is accessed  ntfs automatically performs file-system recovery this scheme does not guarantee that all the user-file contents are correct after a crash ; it ensures only that the file-system data structures  the metadata files  are undamaged and reflect some consistent state that existed prior to the crash it would be possible to extend the transaction scheme to cover user files  and microsoft may do so in the future the log is stored in the third metadata file at the beginning of the volume it is created with a fixed maximum size when the file system is formatted it has two sections  the which is a circular queue of log records  and the which holds context information  such as the position in the logging area where ntfs should start reading during a recovery in fact  the restart area holds two copies of its information  so recovery is still possible if one copy is damaged during the crash the logging functionality is provided by the windows xp ' ' ' '---l ' ' in addition to writing the log records and performing recovery actions  the log-file service keeps track of the free space in the log file if the free space gets too low  the log-file service queues pending transactions  and ntfs halts all new i/o operations after the in-progress operations complete  ntfs calls the cache manager to flush all data and then resets the log file and performs the queued transactions 22.5.3 security the security of an ntfs volume is derived from the windows xp object model each ntfs file references a security descriptor  which contains the access token of the owner of the file  and an access-control list  which states the access privileges granted to each user having access to the file in normal operation  ntfs does not enforce permissions on traversal of directories in file path names however  for compatibility with posix  these checks can be enabled traversal checks are inherently more expensive  since modem parsing of file path names uses prefix matching rather than component-by-component opening of directory names 22.5.4 volume management and fault tolerance ftdisk is the fault-tolerant disk driver for windows xp when installed  it provides several ways to combine multiple disk drives into one logical volume so as to improve performance  capacity  or reliability 22.5.4.1 volume set one way to combine multiple disks is to concatenate them logically to form a large logical volume  as shown in figure 22.7 in windows xp  this logical 882 chapter 22 disk 1  2.5 gb  disk 2  2.5 gb  d disk c   fat  2gb ilcns 128001-7833611 d logical drive d   ntfs  3 gb figure 22.7 volume set on two drives volume  called a can consist of up to 32 physical partitions a volume set that contains an ntfs volume can be extended without disturbance of the data already stored in the file system the bitmap metadata on the ntfs volume are simply extended to cover the newly added space ntfs continues to use the same lcn mechanism that it uses for a single physical disk  and the ftdisk driver supplies the mapping from a logical-volume offset to the offset on one particular disk 22.5.4.2 stripe set another way to combine multiple physical partitions is to interleave their blocks in round-robin fashion to form a as shown in figure 22.8 this scheme is also called raid level 0  or ftdisk uses a stripe size of 64 kb the first 64 kb of the logical volume are stored in the first disk 1  2gb  disk 2  2gb  lcns 0-15 lcns 16-31 lcns 32-47 lcns 48-63 lcns 64-79 lcns 80-95     d logical drive c  4 gb figure 22.8 stripe set on two drives 22.5 883 disk 1  2 gb  disk 2  2gb  disk 3  2gb  d logical drive c  4 gb figure 22.9 stripe set with parity on three drives physical partition  the second 64 kb in the second physical partition  and so on  until each partition has contributed 64 kb of space then  the allocation wraps ar01md to the first disk  allocating the second 64-kb block a stripe set forms one large logical volume  but the physical layout can improve the i/0 bandwidth  because  for a large i/0  all the disks can transfer data in parallel 22.5.4.3 stripe set with parity a variation of this idea is the  which is shown in figure 22.9 this scheme is also called raid levels suppose that a stripe set has eight disks seven of the disks will store data stripes  with one data stripe on each disk  and the eighth disk will store a parity stripe for each data stripe the parity stripe contains the byte-wise exclusive or of the data stripes if any one of the eight stripes is destroyed  the system can reconstruct the data by calculating the exclusive or of the remaining seven this ability to reconstruct data makes the disk array much less likely to lose data in case of a disk failure notice that an update to one data stripe also requires recalculation of the parity stripe seven concurrent writes to seven different data stripes thus require updates to seven parity stripes if the parity stripes were all on the same disk  that disk could have seven times the i/0 load of the data disks to avoid creating this bottleneck  we spread the parity stripes over all the disks by assigning them in round-robin style to build a stripe set with parity  we need a minimum of three equal-sized partitions located on three separate disks 22.5.4.4 disk mirroring an even more robust scheme is called or raid level 1 ; it is depicted in figure 22.10 a comprises two equal-sized partitions on two disks when an application writes data to a mirror set  ftdisk writes the data to both partitions  so that the data contents of the two partitions are identical if one partition fails  ftdisk has another copy safely stored on the mirror mirror sets can also improve performance  because read requests can 884 chapter 22 disk 1  2gb  disk 2  2gb  drive c  2gb copy of drive c  2gb figure 22.10 mirror set on two drives be split between the two mirrors  giving each mirror half of the workload to protect against the failure of a disk controller  we can attach the two disks of a mirror set to two separate disk controllers this arrangement is called a 22.5.4.5 sedor sparing and cluster remapping to deal with disk sectors that go bad  ftdisk uses a hardware technique called sector sparing  and ntfs uses a software technique called cluster remapping is a hardware capability provided by many disk drives when is formatted  it creates a map from logical block numbers to good sectors on the disk it also leaves extra sectors unmapped  as spares if a sector fails  ftdisk instructs the disk drive to substitute a spare is a software technique performed by the file system if a disk goes bad  ntfs substitutes a different  unallocated block by changing any affected pointers in the mft ntfs also makes a note that the bad block should never be allocated to any file when a disk block goes bad  the usual outcome is a data loss but sector sparing or cluster remapping can be combined with fault-tolerant volumes to mask the failure of a disk block if a read fails  the system reconstructs the missing data by reading the mirror or by calculating the exclusive or parity in a stripe set with parity the reconstructed data are stored in a new location that is obtained by sector sparing or cluster remapping 22.5.5 compression and encryption ntfs can perform data compression on individual tlles or on all data files in a directory to compress a file  ntfs divides the file 's data into which are blocks of 16 contiguous clusters when a compression unit is written  a data-compression algoritln11 is applied if the result fits into 22.5 885 fewer than 16 clusters  the compressed version is stored when reading  ntfs can determine whether data have been compressed  if they have been  the length of the stored compression unit is less than 16 clusters to improve performance when reading contiguous compression units  ntfs prefetches and decompresses ahead of the application requests for sparse files or files that contain mostly zeros  ntfs uses another technique to save space clusters that contain only zeros because they have never been written are not actually allocated or stored on disk instead  gaps are left in the sequence of virtual-duster numbers stored in the mft entry for the file when reading a file  if ntfs finds a gap in the virtual-duster numbers  it just zero-fills that portion of the caller 's buffer this technique is also used by unix ntfs supports encryption of files individual files or entire directories can be specified for encryption the security system manages the keys used  and a key-recovery service is available to retrieve lost keys 22.5.6 mount points mount points are a form of symbolic link specific to directories on ntfs they provide a mechanism for organizing disk volumes that is more flexible than the use of global names  like drive letters   a mount point is implemented as a symbolic link with associated data that contain the true volume name ultimately  mount points will supplant drive letters completely  but there will be a long transition due to the dependence of many applications on the drive-letter scheme 22.5.7 change journal ntfs keeps a journal describing all changes that have been made to the file system user-mode services can receive notifications of changes to the journal and then identify what files have changed the content-indexing service uses the change journal to identify files that need to be re-indexed the filereplication service uses it to identify files that need to be replicated across the network 22.5.8 volume shadow copies windows xp implements the capability of bringing a volume to a known state and then creating a shadow copy that can be used to back up a consistent view of the volume making a shadow copy of a volume is a form of copy-on-write  where blocks modified after the shadow copy is created are stored in their original form in the copy to achieve a consistent state for the volume requires the cooperation of applications  since the system can not know when the data used by the application are in a stable state from which the application could be safely restarted the server version of windows xp uses shadow copies to efficiently maintain old versions of files stored on file servers this allows users to see documents stored on file servers as they existed at earlier points in time the user can use this feature to recover files that were accidentally deleted or simply to look at a previous version of the file  all without pulling out a backup tape 886 chapter 22 22.6 windows xp supports both peer-to-peer and client-server networking it also has facilities for network management the networking components in windows xp provide data transport  i11terprocess communication  file sharing across a network  and the ability to send print jobs to remote printers 22.6.1 network interfaces to describe networking in windows xp  we must first mention two of the internal networking interfaces  the and the developed in 1989 by microsoft and 3com to separate network adapters from transport protocols so that either could be changed without affecting the other ndis resides at the interface between the data-link and network layers in the iso model and enables many protocols to operate over many different network adapters in terms of the iso model  the tdi is the interface between the transport layer  layer 4  and the session layer  layer 5   this interface enables any session-layer component to use any available transport mechanism  similar reasoning led to the streams mechanism in unix  the tdi supports both connection-based and connectionless transport and has functions to send any type of data 22.6.2 protocols windows xp implements transport protocols as drivers these drivers can be loaded and unloaded from the system dynamically  although in practice the system typically has to be rebooted after a change windows xp comes with several networking protocols next  we discuss a number of the protocols supported in windows xp to provide a variety of network functionality 22.6.2.1 server-message block the protocol was first introduced in ms-dos 3.1 the system uses the protocol to send ii 0 requests over the network the smb protocol has four message types session control messages are commands that start and end a redirector corldection to a shared resource at the server a redirector uses file messages to access files at the server printer messages are used to send data to a remote print queue and to receive status information from the queue  and message messages are used to communicate with another workstation the smb protocol was published as the  cifs  and is supported on a number of operating systems 22.6.2.2 network basic input/output system the is a hardware-abstraction interface for networks  analogous to the bios hardware-abstraction interface devised for pcs running ms-dos netbios  developed in the early 1980s  has becmne a standard network-programming interface netbios is used to establish logical names on the network ; to establish logical connections  or between two logical names on the network ; and to support reliable data transfer for a session via either netbios or smb requests 22.6 887 22.6.2.3 netbios extended user interface the was introduced by ibm in 1985 as a simple  efficient networking protocol for up to 254 machines it is the default protocol for windows 95 peer networking and for windows for workgroups windows xp uses netbeui when it wants to share resources with these networks among the limitations of netbeui are that it uses the actual name of a computer as the address and that it does not support routing 22.6.2.4 transmission control protocol/internet protocol the transmission control protocol/internet protocol  tcp lip  suite that is used on the internet has become the de facto standard networking infrastructure windows xp uses tcp lip to connect to a wide variety of operating systems and hardware platforms the windows xp tcp lip package includes the simple network-management protocol  snm   dynamic host-configuration protocol  dhcp   windows internet name service  wins   and netbios support 22.6.2.5 point-to-point tunneling protocol is a protocol provided by xp to communicate remote-access server modules running on windows xp server machines and other client systems that are connected over the internet the remote-access servers can encrypt data sent over the connection  and they support multi-protocol  vpns  over the internet 22.6.2.6 novell netware protocols then ovell n etware protocols  ipx datagram service on the spx transport layer  are widely used for pc lans the windows xp nwlink protocol connects the netbios to netware networks in combination with a redirector  such as microsoft 's client service for netware or novell 's netware client for windows   this protocol enables a windows xp client to connect to a netware server 22.6.2.7 web-distributed authoring and versioning protocol web-distributed authoring and versioning  webdav  is an http-based protocol for collaborative authoring across a network windows xp builds a webdav redirector into the file system building this support directly into the file system enables webdav to work with other features  such as encryption personal files can now be stored securely in a public place 22.6.2.8 appletalk protocol the p ;  oi  ocoi was designed as a low-cost connection by apple to allow macintosh computers to share files windows xp systems can share files and printers with macintosh computers via apple talk if a windows xp server on the network is running the windows services for macintosh package 888 chapter 22 22.6.3 distributed-processing mechanisms although windows xp is not a distributed operating system  it does support distributed applications mechanisms that support distributed processing on windows xp include netbios  named pipes and mailslots  windows sockets  rpcs  the microsoft interface definition language  and com 22.6.3.1 netbios in windows xp  netbios applications can communicate over the network using netbeui  nwlink  or tcp /if 22.6.3.2 named pipes are a connection-oriented messaging mechanism named pipes were originally developed as a high-level interface to netbios connections over the network a process can also use named pipes to communicate with other processes on the same machine since na1ned pipes are accessed through the file-system interface  the security mechanisms used for file objects also apply to named pipes the format of pipe names follows the a unc name looks like a typical remote file name format is \ \ server_name \ share_name \ x \ y \ z  where server_name identifies a server on the network ; share_name identifies any resource that is made available to network users  such as directories  files  named pipes  and printers ; and \ x \ y \ z is a normal file path name 22.6.3.3 mailslots are a connectionless messaging mechanism they are unreliable when accessed across the network  in that a message sent to a mailslot may be lost before the intended recipient receives it mailslots are used for broadcast applications  such as finding components on the network ; they are also used by the windows computer browser service 22.6.3.4 winsock is the windows xp sockets api winsock is a session-layer interface that is largely compatible with unix sockets but has some added windows xp extensions it provides a standardized interface to many transport protocols that may have different addressing schemes  so that any winsock application can run on any winsock-compliant protocol stack 22.6.3.5 remote procedure calls a remote procedure call  rpc  is a client-server mechanism that enables an application on one machine to n ake a procedure call to code on another machine the client calls a local procedure-a stub routine-which packs its arguments into a message and sends them across the network to a particular server process the client-side stub routine then blocks meanwhile  the server unpacks the message  calls the procedure  packs the return results into a message  and sends them back to the client stub the client stub unblocks  22.6 889 receives the message  unpacks the results of the rpc and returns them to the caller this packing of argun'lents is sometimes called the windows xp rpc mechanism follows the widely computing-environment standard for rpc messages  so programs written to use windows xp rpcs are highly portable the rpc standard is detailed it hides many of the architectural differences among computers  such as the sizes of binary numbers and the order of bytes and bits in computer words  by specifying standard data formats for rpc messages windows xp can send rpc messages using netbios  or winsock on tcp /ip networks  or named pipes on lan manager networks the lpc facility  discussed earlie1 ~ is similar to rpc  except that in the case of lpc the messages are passed between two processes running on the same computer 22.6.3.6 microsoft interface definition language it is tedious and error-prone to write the code to marshal and transmit arguments in the standard format  to unmarshal and execute the remote procedure  to marshal and send the return results  and to unmarshal and return them to the caller fortunately  howeve1 ~ much of this code can be generated automatically from a simple description of the and return results windows xp provides the to describe the remote procedure names  arguments  and results the compiler for this language generates header files that declare the stubs for the remote procedures  as well as the data types for the arguments and return-value messages it also generates source code for the stub routines used at the client side and for an unmarshaller and dispatcher at the server side when the application is linked  the stub routines are included when the application executes the rpc stub  the generated code handles the rest 22.6.3.7 component object model the jbjecj is a mechanism for interprocess communication was developed for windows com objects provide a well-defined interface to manipulate the data in the object for instance  com is the infrastructure used by microsoft 's technology for inserting spreadsheets into microsoft documents xp has a distributed extension called that can be used over a network utilizing rpc to provide a transparent method of developing distributed applications 22.6.4 redirectors and servers in windows xp  an application can use the windows xp i/o api to access files from a remote computer as though they were locat provided that the remote computer is running a cifs server  such as is provided by windows xp and earlier windows systems a is the client-side object that forwards i/0 requests to remote files  where they are satisfied by a server for performance and security  the redirectors and servers run in kernel mode in more detait access to a remote file occurs as follows  the application calls the i/o manager to request that a file be opened with a file name in the standard unc format 890 chapter 22 the i/0 manager builds an i/0 request packet  as described in section 22.3.3.5 the i/0 manager recognizes that the access is for a remote file and calls a driver called a the mup sends the i/o request packet asynchronously to all registered redirectors a redirector that can satisfy the request responds to the mup to avoid asking all the redirectors the same question in the future  the mup uses a cache to remember which redirector can handle this file the redirector sends the network request to the remote system the remote-system network drivers receive the request and pass it to the server driver the server driver hands the request to the proper local file-system driver the proper device driver is called to access the data the results are returned to the server driver  which sends the data back to the requesting redirector the redirector then returns the data to the calling application via the i/o manager a similar process occurs for applications that use the win32 api network api  rather than the unc services  except that a module called a multi-provider router is used instead of a mup for portability  redirectors and servers use the tdi api for network transport the requests themselves are expressed in a higher-level protocol  which by default is the smb protocol mentioned in section 22.6.2 the list of redirectors is maintained in the system registry database 22.6.4.1 distributed file system unc names are not always convenient  because multiple file servers may be available to serve the same content  and unc names explicitly include the name of the server windows xp supports a protocol that allows a network administrator to serve up files multiple servers using a single distributed name space 22.6.4.2 folder redirection and client-side caching to improve the pc experience for business users who frequently switch among computers  windows xp allows administrators to give users which keep users ' preferences and other settings on servers is then used to auton'latically store a user 's documents and other files on a server this works well until one of the computers is no longer attached to the network  as when a user takes a laptop onto an airplane to give users off-line access to their redirected files  windows xp uses esc is used when the computer is online to keep copies of the server files on the local machine for better performance the files are pushed up to the server as they are changed if the computer becomes disconnected  the files are 22.6 891 still available  and the update of the server is deferred until the next tin'le the computer is online 22.6.5 domains many networked environments have natural groups of users  such as students in a computer laboratory at school or employees in one department in a business frequently  we want all the members of the group to be able to access shared resources on their various computers in the group to manage the global access rights within such groups  windows xp uses the concept of a domain previously  these domains had no relationship whatsoever to the domain-name system  dns  that maps internet host names to ip addresses now  however  they are closely related specifically  a windows xp domain is a group of windows xp workstations and servers that share a common security policy and user database since windows xp now uses the kerberos protocol for trust and authentication  a windows xp domain is the same thing as a kerberos realm previous versions of nt used the idea of primary and backup domain controllers ; now all servers in a domain are domain controllers in addition  previous versions required the setup of one-way trusts between domains windows xp uses a hierarchical approach based on dns and allows transitive trusts that can flow up and down the hierarchy this approach reduces the number of trusts required for n domains from n  n  1  to o  n   the workstations in the domain trust the domain controller to give correct information about the access rights of each user  via the user 's access token   all users retain the ability to restrict access to their own workstations  however  no matter what any domain controller may say 22.6.5.1 domain trees and forests because a business may have many departments and a school may have many classes  it is often necessary to manage multiple domains within a single organization a is a contiguous dns naming hierarchy for managing multiple domains for example  bell-labs.com might be the root of the tree  with research.bell-labs.com and pez.bell-labs.com as children-domains research and pez a is a set of noncontiguous names an example would be the trees bell-labs.conz andlucent.com a forest may be made up of only one domain tree  however 22.6.5.2 trust relationships trust relationships may be set up between domains in three ways  one-way  transitive  and cross-link versions of nt through 4.0 allowed only one-way trusts a is exactly what its name implies  domain a is told it can trust domain b however  b will not trust a unless another relationship is configured under a if a trusts band b trusts c  then a b  and call trust one another  since transitive trusts are two-way by default transitive trusts are enabled by default for new domains in a tree and can be configured only among domains within a forest the third type  a is useful to cut down on authentication traffic suppose that domains a and b are leaf nodes and that users in a often use resources in b if a standard transitive trust 892 chapter 22 22.7 is used  authentication requests must traverse up to the common ancestor of the two leaf nodes ; but if a and b have a cross-linking trust the authentications are sent directly to the other node 22.6.6 active directory ,.,.,cu i.n ' is the windows xp implementation of services active directory stores the topology information about the domain  keeps the doncain-based user and group accounts and passwords  and provides a domain-based store for technologies like and administrators use group policies to establish uniform standards for desktop preferences and software for many corporate information-technology groups  uniformity drastically reduces the cost of computing intellimirror is used in conjunction with group policies to specify what software should be available to each class of user  even automatically installing it on demand from a corporate server 22.6.7 name resolution in tcp/ip networks on an ip network  is the process of converting a computer name to an ip address  such as resolving www.bell-labs.com to 135.104.1.14 windows xp provides several methods of name resolution  including windows internet name service  wins   broadcast-name resolution  domain-name system  dns   a hosts file  and an lmhosts file most of these methods are used by many operating systems  so we describe only wins here under wins  two or more wins servers maintain a dynamic database of name-to-if address bindings  along with client software to query the servers at least two servers are used  so that the wins service can survive a server failure and so that the name-resolution workload can be spread over multiple machines wins uses the dynamic host-configuration protocol  dhcp   dhcp updates address configurations automatically in the wins database  without user or administrator intervention  as follows when a dhcp client starts up  it broadcasts a discover message each dhcp server that receives the message replies with an offer message that contains an ip address and configuration information for the client the client chooses one of the configurations and sends a request message to the selected dhcp server the dhcp server responds with the ip address and configuration information it gave previously and with a for that address the lease gives the client the right to use the ip address for a specified period of time when the lease time is half expired  the client attempts to renew the lease for the address if the lease is not renewed  the client must obtain a new one the win32 api is the fundamental interface to the capabilities of windows xp this section describes five main aspects of the win32 api  access to kernel objects  sharing of objects between processes  process management  interprocess communication  and memory management 22.7 security_attributes sa ; sa.nlength = sizeof  sa  ; sa.lpsecuritydescriptor = null ; sa.binherithandle = true ; handle a_semaphore = createsemaphore  &sa  1  1  null  ; char comand_line  132  ; ostrstream ostring  command_line  sizeof  command_line   ; ostring a_semaphore ends ; createprocess  another _process exe  command_line  null  null  true    ; figure 22.11 code enabling a child to share an object by inheriting a handle 22.7.1 access to kernel objects 893 the windows xp kernel provides many services that application programs can use application programs obtain these services by manipulating kernel objects a process gains access to a kernel object named xxx by calling the createxxx function to open a handle to xxx this handle is unique to the process depending on which object is being opened  if the create   function fails  it may return 0  or it may return a special constant named invalid.j1andllvalue a process can close any handle by calling the closehandle   function  and the system may delete the object if the count of processes using the object drops to 0 22.7.2 sharing objects between processes windows xp provides three ways to share objects between processes the first way is for a child process to inherit a handle to the object when the parent calls the createxxx function  the parent supplies a securities__attributes structure with the blnheri thandle field set to true this field creates an inheritable handle next  the child process is created  passing a value of true to the createprocess   function 's binheri thandle argument figure 22.11 shows a code sample that creates a semaphore handle inherited by a child process assuming the child process knows which handles are shared  the parent and child can achieve interprocess communication through the shared objects in the example in figure 22.11  the child process gets the value of the handle from the first command-line argument and then shares the semaphore with the parent process the second way to share objects is for one process to give the object a name when the object is created and for the second process to open the name this method has two drawbacks  windows xp does not provide a way to check whether an object with the chosen name already exists  and the object name space is global  without regard to the object type for instance  two applications 1nay create an object named pipe when two distinct-and possibly differentobjects are desired named objects have the advantage that unrelated processes can readily share them the first process calls one of the createxxx functions and supplies a name in the lpszname parameter the second process gets a handle to share 894 chapter 22 ii process a handle a_semaphore createsemaphore  null  1  1  mysem1  ; ii process b handle b_semaphore opensemaphore  semaphore_all_access  false  mysem1  ; figure 22.12 code for sharing an object by name lookup the object by calling openxxx    or createxxx  with the same name  as shown in the example of figure 22.12 the third way to share objects is via the duplicatehandle   function this method requires some other method of interprocess communication to pass the duplicated handle given a handle to a process and the value of a handle within that process  a second process can get a handle to the same object and thus share it an example of this method is shown in figure 22.13 22.7.3 process management in windows xp  a is an executing instance of an application  and a is a unit of code that can be scheduled by the operating system thus  a process contains one or more threads a process is started when some other process calls the createprocess   routine this routine loads any dynamic link libraries used by the process and creates a additional threads can be created by the createthread   function each ii process a wants to give process b access to a semaphore ii process a handle a_semaphore = createsemaphore  null  1  1  null  ; ii send the value of the semaphore to process b ii using a message or shared memory object ii process b handle process_a = openprocess  process_all_access  false  process_id_of....a  ; handle b_semaphore ; duplicatehandle  process_a  a_semaphore  getcurrentprocess    &b_semaphore  0  false  duplicate_same_access  ; ii use b_semaphore to access the semaphore figure 22.13 code for sharing an object by passing a handle 22.7 895 thread is created with its own stack  which defaults to 1 mb unless specified otherwise in an argument to create thread    because some c run-time functions maintain state in static variables  such as errno  a multithread application needs to guard against unsynchronized access the wrapper function beginthreadex   provides appropriate synchronization 22.7.3.1 instance handles every dynamic link library or executable file loaded into the address space of a process is identified by an ; the value of the instance handle is actually the virtual address where the file is loaded an application can get the handle to a module in its address space by passing the name of the module to getmodulehandle    if null is passed as the name  the base address of the process is returned the lowest 64 kb of the address space are not used  so a faulty program that tries to de-reference a null pointer gets an access violation priorities in the win32 api environment are based on the windows xp scheduling model  but not all priority values may be chosen win32 api uses four priority classes  idle_priority _class  priority level4  normal_priority _class  priority level 8  high_priority _class  priority levell3  real time_priority _class  priority level24  processes are typically members of the normalpriority _class unless the parent of the process was of the idllpridrity _class or another class was specified when createprocess was called the priority class of a process can be changed with the setpriori tyclass   function or by passing of an argument to the start command for example  the command start /realtime cbserver exe would run the cbserver program in the realtime_ pridrity _class only users with the increase scheduling priority privilege can move a process into the realtimlpriority_class administrators and power users have this privilege by default 22.7.3.2 scheduling rule when a user is running an interactive program  the system needs to provide especially good performance for the process for this reason  windows xp has a special scheduling rule for processes in the normalpriority _class windows xp distinguishes between the foreground process that is currently selected on the screen and the background processes that are not currently selected when a process moves into the foreground  windows xp increases the scheduling quantum by some factor-typically by 3  this factor can be changed via the performance option in the system section of the control panel  this increase gives the foreground process three times longer to run before a time-sharing preemption occurs 896 chapter 22 22.7.3.3 thread priorities a thread starts with an initial priority determined by its class the priority can be altered by the setthreadpriori ty   function this function takes an argument that specifies a priority relative to the base priority of its class  thread_priority _lowest  base 2 thread_priority _below_normal  base  1 thread_priority _normal  base + 0 thread_priority _above_normal  base + 1 thread_priority _highest  base + 2 two other designations are also used to adjust the priority recall from section 22.3.2.1 that the kernel has two priority classes  16-31 for the realtime class and 0-15 for the variable-priority class thread_priority _idle sets the priority to 16 for real-time threads and to 1 for variable-priority threads thread_pridritltimlcritical sets the priority to 31 for real-time threads and to 15 for variable-priority threads as we discussed in section 22.3.2.1  the kernel adjusts the priority of a thread dynamically depending on whether the thread is i/0 bound or cpu bound the win32 api provides a method to disable this adjustment via setprocesspriori tyboost   and setthreadpriori tyboost   functions 22.7.3.4 thread synchronization a thread can be created in a the thread does not execute until another thread makes it eligible via the resumethread   function the suspend thread   function does the opposite these functions set a counter so that if a thread is suspended twice  it must be resumed twice before it can nm to synchronize the concurrent access to shared objects by threads  the kernel provides synchronization objects  such as semaphores and mutexes in addition  synchronization of threads can be achieved by use of the waitforsingleobject   and waitformultipleobjects   functions another method of synchronization in the win32 api is the critical section a critical section is a synchronized region of code that can be executed by only one thread at a time a thread establishes a critical section by calling ini tializecri ticalsection    the application must call entercri ticalsection   before entering the critical section and leavecri ticalsection   after exiting these two routines guarantee that  if multiple threads attempt to enter the critical section concurrently  only one thread at a time will be permitted to proceed ; the others will wait in the entercri ticalsection   routine the critical-section mechanism is faster than using kernel-synchronization objects because it does not allocate kernel objects until it first encounters contention for the critical section 22.7.3.5 fibers a is user-mode code that is scheduled according to a user-defined scheduling algorithm a process may have multiple fibers in it  just as it may 22.7 897 have multiple threads a major difference between threads and fibers is that whereas threads can execute concurrently  only one fiber at a tin1e is permitted to execute  even on multiprocessor hardware this mechanism is included in windows xp to facilitate the porting of those legacy unix applications that were written for a fiber-execution model the system creates a fiber by calling either convertthreadtofi ber   or createfiber    the primary difference between these functions is that createfiber   does not begin executing the fiber that was created to begin execution  the application must call swi tchtofiber    the application can terminate a fiber by calling deletefiber    22.7.3.6 thread pool repeated creation and deletion of threads can be expensive for applications and services that perform small amounts of work in each instantiation the thread pool provides user-mode programs with three services  a queue to which work requests may be submitted  via the queueuserworkitern   api   an api that can be used to bind callbacks to waitable handles  registerwai tforsingleobj ect    ,and apis to bind callbacks to timeouts  createtirnerqueue   and createtirnerqueuetirner     the thread pool 's goal is to increase performance threads are relatively expensive  and a processor can only be executing one thing at a time no matter how many threads are used the thread pool attempts to reduce the number of outstanding threads by slightly delaying work requests  reusing each thread for many requests  while providing enough threads to effectively utilize the machine 's cpus the wait and timer-callback apis allow the thread pool to further reduce the number of threads in a process  using far fewer threads than would be necessary if a process were to devote one thread to servicing each waitable handle or timeout 22.7.4 lnterprocess communication win32 api applications handle interprocess communication in several ways one way is by sharing kernel objects another way is by passing messages  an approach that is particularly popular for windows gui applications one thread can send a message to another thread or to a window by calling postmessage    postthreadmessage    sendmessage    sendthreadmessage   ,or sendmessagecallback   .posting a message and sending a message differ in this way  the post routines are asynchronous ; they return immediately  and the calling thread does not know when the message is actually delivered the send routines are synchronous ; they block the caller until the message has been delivered and processed in addition to sending a message  a thread can send data with the message since processes have separate address spaces  the data must be copied the system copies data by calling sendmessage   to send a message of type wm_copydata with a copydatastruct data structure that contains the length and address of the data to be transferred when the message is sent  windows xp copies the data to a new block of memory and gives the virtual address of the new block to the receiving process unlike threads in the 16-bit windows environment  every win32 api thread has its own input queue from which it receives messages  all input is received 898 chapter 22 ii allocate 16mb at the top of our address space void buf = virtualalloc  o  ox1000000  mem_reserve i mem_top_down  p age_readwrite  ; ii commit the upper 8mb of the allocated space virtualalloc  buf + ox800000  ox800000  mem_commit  page_readwrite  ; ii do something with the memory ii now decommit the memory virtualfree  buf + ox800000  ox800000  mem_decommit  ; ii release all of the allocated address space virtualfree  buf  0  mem_release  ; figure 22.14 code fragments for allocating virtual memory via messages  this structure is more reliable than the shared input queue of 16-bit windows  because  with separate queues  it is no longer possible for one stuck application to block input to the other applications if a win32 api application does not call getmessage   to handle events on its input queue  the queue fills up ; and after about five seconds  the system marks the application as not responding  22.7.5 memory management the win32 api provides several ways for an application to use memory  virtual memory  memory-mapped files  heaps  and thread-local storage 22.7.5.1 virtual memory an application calls virtualalloc   to reserve or commit virtual memory and virtualfree   to decommit or release the memory these functions enable the application to specify the virtual address at which the memory is allocated they operate on multiples of the memory page size  and the starting address of an allocated region must be greater than ox10000 examples of these functions appear in figure 22.14 a process may lock some of its committed pages into physical memory by calling virtuallock    the maximum number of pages a process can lock is 30  unless the process first calls setprocessworkingsetsize   to increase the maximum working-set size 22.7.5.2 memory-mapping files another way for an application to use memory is by memory-mapping a file into its address space memory mapping is also a convenient way for two processes to share memory  both processes map the same file into their virtual memory memory mapping is a multistage process  as you can see in the example in figure 22.15 if a process wants to map some address space just to share a memory region with another process  no file is needed the process calls createfilemapping   with a file handle of oxffffffff and a particular size the resulting file-mapping object can be shared by inheritance  by name lookup  or by duplication 22.7 899 ii open the file or create it if it does not exist handle hfile = createfile  somefile  generic_read i generic_write  file_share_read i file_share_write  null  open_always  file_attribute_normal  null  ; ii create the file mapping 8mb in size handle hmap = createfilemapping  hfile  page_readwrite  sec_commit  0  ox800000  shm_l  ; ii now get a view of the space mapped void buf = mapviewoffile  hmap  file_map_all_access  0  0  0  ox800000  ; ii do something with the mapped file ii now unmap the file unmapviewoffile  buf  ; closehandle  hmap  ; closehandle  hfile  ; figure 22.15 code fragments for memory mapping of a file 22.7.5.3 heaps heaps provide a third way for applications to use memory a heap in the win32 environment is a region of reserved address when a win32 api process is initialized  it is created with a 1-mb since many win32 api functions use the default heap  access to the heap is synchronized to protect the heap 's space-allocation data structures from being damaged by concurrent updates by multiple threads win32 api provides several heap-management functions so that a process can allocate and manage a private heap these functions are heapcreate    heapalloc    heaprealloc    heapsize    heapfree    and heapdestroy    the win32 api also provides the heaplock   and heapunlock   functions to enable a thread to gain exclusive access to a heap unlike virtuallock    these functions perform only synchronization ; they do not lock pages into physical memory 22.7.5.4 thread-local storage the fourth way for applications to use memory is through a thread-local storage mechanism functions that rely on global or static data typically fail to work properly in a multithreaded environment for instance  the c runtime function strtok   uses a static variable to keep track of its current position while parsing a string for two concurrent threads to execute strt ok   correctly  they need separate current position variables the thread-local storage mechanism allocates global storage on a per-thread basis it provides both dynamic and static methods of creating thread-local storage the dynamic method is illustrated in figure 22.16 to use a thread-local static variable  the application declares the variable as follows to ensure that every thread has its own private copy  __ declspec  thread  dword cur_pos = 0 ; 900 chapter 22 22.8 ii reserve a slot for a variable dword var_index = t1salloc   ; ii set it to the value 10 t1ssetvalue  var_index  10  ; ii get the value int var t1sgetvalue  var_index  ; ii release the index t1sfree  var_index  ; figure 22.16 code for dynamic thread-local storage microsoft design.ed windows xp to be an extensible  portable operating system -one able to take advantage of new techniques and hardware windows xp supports multiple operating environments and symmetric multiprocessing  including both 32-bit and 64-bit processors and numa computers the use of kernel objects to provide basic services  along with support for client-server computing  enables windows xp to support a wide variety of application environments for instance  windows xp can run programs compiled for ms-dos  winl6  windows 95  windows xp  and posix it provides virtual memory  integrated caching  and preemptive scheduling windows xp supports a security model stronger than those of previous microsoft operating systems and includes internationalization features windows xp nms on a wide variety of computers  so users can choose cu1.d upgrade hardware to match their budgets and performance requirements without needing to alter the applications they run 22.1 describe the booting process for a windows xp system 22.2 what is a process  and how is it managed in windows xp 22.3 describe some of the ways in which an application can use memory via the windows32 api 22.4 describe a useful application of the no-access page facility provided in windows xp 22.5 under what circumstances would one use the deferred procedure calls facility in windows xp 22.6 describe the three main architectural layers of windows xp 22.7 what is the purpose of the win16 execution environment what limitations are imposed on the programs executing inside this environment what are the protection guarantees provided between different applications executing inside the win16 environment what are the protection guarantees provided between an application executing inside the win16 environment and a 32-bit application 901 22.8 what is the fiber abstraction provided by windows xp how does it differ from the threads abstraction 22.9 describe two user-mode processes that enable windows xp to run programs developed for other operating systems 22.10 describe the management scheme of the virtual memory manager how does the vm manager improve performance 22.11 describe the three techniques used for communicating data in a local procedure call what settings are most conducive to the application of the different message-passing teclll'liques 22.12 how does ntfs handle data structures how does ntfs recover from a system crash what is guaranteed after a recovery takes place 22.13 describe two user-mode processes that windows xp provides to enable it to run programs developed for other operating systems 22.14 what is the job of the object manager 22.15 the ia64 processors contain registers that can be used to address a 64-bit address space however  windows xp limits the address space of user programs to 8 tb  which corresponds to 43 bits ' worth why was this decision made 22.16 how does the ntfs directory structure differ from the directory structure used in unix operating systems 22.17 what are the responsibilities of the i/o manager 22.18 what manages caching in windows xp how is caching managed 22.19 what is a handle  and how does a process obtain a handle solomon and russinovich  2000  give an overview of windows xp and considerable technical detail about system internals and components tate  2000  is a good reference on using windows xp the microsoft windows xp server resource kit  microsoft  2000b   is a six-volume set helpful for using and deploying windows xp the microsoft developer network library  microsoft  2000a    issued quarterly  supplies a wealth of information on windows xp and other microsoft products iseminger  2000  provides a good reference on the windows xp active directory richter  1997  gives a detailed discussion on writing programs that use the win32 api silberschatz et al  2001  contains a good discussion of b + trees 23.1 now that you understand the fundamental concepts of operating systems  cpu scheduling  memory management  processes  and so on   we are in a position to examine how these concepts have been applied in several older and highly influential operating systems some of them  such as the xds-940 and the the system  were one-of-a-kind systems ; others  such as os/360  are widely used the order of presentation highlights the similarities and differences of the systems ; it is not strictly chronological or ordered by importance the serious student of operating systems should be familiar with all these systems as we describe early systems  we include references to further reading the papers  written by the designers of the systems  are important both for their technical content and for their style and flavor to explain how operating-system features migrate over time from large computer systems to smaller ones to discuss the features of several historically important operating systems one reason to study early architectures and operating systems is that a feature that once ran only on huge systems may eventually have made its way i11.to very small systems indeed  an examination of operating systems for mainframes and microcomputers shows that many features once available only on mainframes have been adopted for microcomputers the same operating-system concepts are thus appropriate for various classes of computers  mainframes  minicomputers  microcomputers  and handhelds to understand modern operating systems  then  you need to recognize the theme of feature migration and the long history of many operating-system features  as shown in figure 23.1 a good example of feature migration started with the multiplexed information and computing services  multics  operating system multics was developed from 1965 to 1970 at the massachusetts institute of technology   as 903 904 chapter 23 23.2 figure 23.1 migration of operating-system concepts and features a computing  it ran on a large  complex mainframe computer  the ge 645   many of the ideas that were developed for multics were subsequently used at bell laboratories  one of the original partners in the development of multics  in the design of unix the unix operating system was designed around 1970 for a pdp-11 minicomputer around 1980  the features of unix became the basis for unix-like operating systems on microcomputers ; and these features are included in several more recent operatirlg systems for microcomputers  such as microsoft windows  windows xp  and the mac osx operating system linux includes some of these same features  and they can now be found on pdas we turn our attention now to a historical overview of early computer systems we should note that the history of computing starts far before computers with looms and calculators  as described in frah  2001  and shown graphically in frauenfelder  2005   we begin our discussion  however  with the computers of the twentieth century before the 1940s  computing devices were designed and implemented to perform specific  fixed tasks modifying one of those tasks required a great deal of effort and manual labor all that changed in the 1940s when alan turing and john von neumann  and colleagues   both separately and together  worked on the idea of a more general-purpose computer such a machine 23.2 905 has both a program store and a data store  where the program store provides instructions about what to do to the data this fundamental computer concept quickly generated a number of general-purpose computers  but much of the history of these machines is blurred by time and the secrecy of their development during world war ii it is likely that the first working stored-program general-purpose computer was the manchester mark 1  which ran successfully in 1949 the first commercial computer was its offspring  the ferranti mark 1  which went on sale in 1951 these early computing efforts are described by rojas and hashagen  2000  and ceruzzi  1998   early computers were physically enormous machines run from consoles the programmer  who was also the operator of the computer system  would write a program and then would operate the program directly from the operator 's console first  the program would be loaded manually into memory from the front panel switches  one instruction at a time   from paper tape  or from punched cards then the appropriate buttons would be pushed to set the starting address and to start the execution of the program as the program ran  the programmer i operator could monitor its execution by the display lights on the console if errors were discovered  the programmer could halt the program  examine the contents of memory and registers  and debug the program directly from the console output was printed or was punched onto paper tape or cards for later printing 23.2.1 dedicated computer systems as time went on  additional software and hardware were developed card readers  lil1.e printers  and magnetic tape became commonplace assemblers  loaders  and linkers were designed to ease the programming task libraries of common functions were created common functions could then be copied into a new program without having to be written again  providing software reusability the routines that performed i/o were especially important each new i/o device had its own characteristics  requiring careful programming a special subroutine-called a device driver-was written for each ii 0 device a device driver knows how the buffers  flags  registers  control bits  and status bits for a particular device should be used each type of device has its own driver a simple task  such as reading a character from a paper-tape reader  might involve complex sequences of device-specific operations rather than writil1.g the necessary code every time  the device driver was simply used from the library later  compilers for fortran  cobol  and other languages appeared  making the programming task much easier but the operation of the computer more complex to prepare a fortran program for execution  for example  the programmer would first need to load the fortran compiler into the computer the compiler was normally kept on magnetic tape  so the proper tape would need to be mounted on a tape drive the program would be read through the card reader and written onto another tape the fortran compiler produced assembly-language output  which then had to be assembled this procedure required mounting another tape with the assembler the output of the assembler would need to be linked to supporting library routines finally  906 chapter 23 the binary object form of the program would be ready to execute it could be loaded into memory and debugged from the console  as before a significant amount of could be involved in the running of a job each job consisted of many separate steps  loading the fortran compiler tape running the compiler unloading the compiler tape loading the assembler tape running the assembler unloading the assembler tape loading the object program running the object program if an error occurred during any step  the programmer/operator might have to start over at the beginning each job step might involve the loading and unloading of magnetic tapes  paper tapes  and punch cards the job set-up time was a real problem while tapes were being mounted or the programmer was operating the console  the cpu sat idle remember that  in the early days  few computers were available  and they were expensive a computer might have cost millions of dollars  not including the operational costs of power  cooling  programmers  and so on thus  computer time was extremely valuable  and owners wanted their computers to be used as much as possible they needed high to get as much as they could from their investments 23.2.2 shared computer systems the solution was twofold first  a professional computer operator was hired the programmer no longer operated the machine as soon as one job was finished  the operator could start the next since the operator had more experience with mounting tapes than a programmer  set-up time was reduced the programmer provided whatever cards or tapes were needed  as well as a short description of how the job was to be run of course  the operator could not debug an incorrect program at the console  since the operator would not understand the program therefore  in the case of program error  a dump of memory and registers was taken  and the programmer had to debug from the dump dumping the memory and registers allowed the operator to continue immediately with the next job but left the programmer with the more difficult debugging problem second  jobs with similar needs were batched together and run through the computer as a group to reduce set-up time for instance  suppose the operator received one fortran job  one cobol job  and another fortran job if she ran them in that order  she would have to set up for fortran  load the compiler tapes and so on   then set up for cobol  and then set up for fortran again if she ran the two fortran programs as a batch  however  she could set up only once for fortran  saving operator time 23.2 907 monitor figure 23.2 memory layout for a resident monitor but there were still problems for example  when a job stopped  the operator would have to notice that it had stopped  by observing the console   determine why it stopped  normal or abnormal termination   dump memory and register  if necessary   load the appropriate device with the next job  and restart the computer during this transition from one job to the next  the cpu sat idle to overcome this idle time  people developed with this technique  the first rudimentary operating systems were a small program  called a moni loj '  was created to transfer control automatically from one job to the next  figure 23.2   the resident monitor is always in memory  or resident   when the computer was turned on  the resident monitor was invoked  and it would transfer control to a program when the program terminated  it would return control to the resident monitor  which would then go on to the next program thus  the resident monitor would automatically sequence from one program to another and from one job to another but how would the resident monitor know which program to execute previously  the operator had been given a short description of what programs were to be run on what data were introduced to provide this information directly to the monitor the idea is simple in addition to the program or data for a job  the programmer included the control cards  which contained directives to the resident monitor indicating what program to run for example  a normal user program might require one of three programs to run  the fortran compiler  ftn   the assembler  asm   or the user 's program  run   we could use a separate control card for each of these  $ ftn-execute the fortran compiler $ asm-execute the assembler $ run-execute the user program these cards tell the resident monitor which program to run 908 chapter 23 we can use tvw additional control cards to define the boundaries of each job  $ job-first card of a job $ end-final card of a job these two cards might be useful in accounting for the machine resources used by the programmer parameters can be used to defirte the job name  account number to be charged  and so on other control cards can be defined for other ftmctions  such as asking the operator to load or unload a tape one problem with conh ol cards is how to distinguish them from data or program cards the usual solution is to identify them by a special character or pattern on the card several systems used the dollar-sign character  $  in the first colun  m to identify a control card others used a different code ibm 's job control language  jcl  used slash marks  ! /  in the first two columns figure 23.3 shows a sample card-deck setup for a simple batch system a resident monitor thus has several identifiable parts  the is responsible for reading and carrying out the insh uctions on the cards at the point of execution the is invoked by the control-card interpreter to load system programs an.d application programs into memory at intervals the are used by both the control-card interpreter and the loader for the system 's i/0 devices often  the system and application programs are linked to these same device drivers  providing continuity in their operation/ as well as saving memory space and programming time these batch systems work fairly well the resident monitor provides automatic job sequencing as indicated by the control cards when a control card indicates that a program is to be run  the monitor loads the program into memory and transfers control to it when the program completes  it figure 23.3 card deck for a simple batch system 23.2 909 transfers control back to the monitor ; which reads the next control card  loads the appropriate program  and so on this cycle is repeated until all control cards are interpreted for the job then the monitor automatically continues with the next job the switch to batch systems with automatic job sequencing was made to improve performance the problem  quite simply  is that humans are considerably slower than the computer consequently  it is desirable to replace human operation with operating-system software automatic job sequencing elim.inates the need for human set-up time and job sequencing even with this arrangement  however  the cpu is often idle the problem is the speed of the mechanical i/0 devices  which are intrinsically slower than electronic devices even a slow cpu works in the n licrosecond range  with thousands of instructions executed per second a fast card reader  in contrast  might read 1,200 cards per minute  or 20 cards per second   thus  the difference in speed between the cpu and its i/0 devices may be three orders of magnitude or more over time  of course  improvements in technology resulted in faster i/0 devices unfortunately  cpu speeds increased even faster  so that the problem was not only unresolved but also exacerbated 23.2.3 overlapped 1/0 one common solution to the i/0 problem was to replace slow card readers  input devices  and line printers  output devices  with magnetic-tape units most computer systems in the late 1950s and early 1960s were batch systems reading from card readers and writing to line printers or card punches the cpu did not read directly from cards/ however ; instead  the cards were first copied onto a magnetic tape via a separate device when the tape was sufficiently full  it was taken down and carried over to the computer when a card was needed for input to a program  the equivalent record was read from the tape similarly  output was written to the tape  and the contents of the tape were pri  n.ted later the card readers and lilce printers were operated off-line  rather than the main computer  figure 23.4   an obvious advantage of off-line operation was that the mam computer was no longer constrained by the speed of the card readers and line printers but was limited only by the speed of the much faster magnetic tape units card reader line printer  a  card reader tape drives tape drives iine printer figure 23.4 operation of 1/0 devices  a  online and off-line 910 chapter 23 the technique of using magnetic tape for alll/0 could be applied with any similar equipment  such as card readers  card punches  plotters  paper tape  and printers   the real gain in off-line operation comes from the possibility of using multiple reader-to-tape and tape-to-printer systems for one cpu if the cpu can process input twice as fast as the reader can read cards  then two readers working simultaneously can produce enough tape to keep the cpu busy there is a disadvantage  too  however-a longer delay in getting a particular job run the job must first be read onto tape then it must wait until enough additional jobs are read onto the tape to fill it the tape must then be rewound  unloaded  hand-carried to the cpu  and mounted on a free tape drive this process is not unreasonable for batch systems  of course many similar jobs can be batched onto a tape before it is taken to the computer although off-line preparation of jobs continued for some time  it was quickly replaced in most systems disk systems became widely available and greatly improved on off-line operation the problem with tape systems was that the card reader could not write onto one end of the tape while the cpu read from the other the entire tape had to be written before it was rewound and read  because tapes are by nature devices disk systems eliminated this problem by being because the head is moved from one area of the disk to another  it can switch rapidly from the area on the disk being used by the card reader to store new cards to the position needed by the cpu to read the next card in a disk system  cards are read directly from the card reader onto the disk the location of card images is recorded in a table kept by the operating system when a job is executed  the operating system satisfies its requests for card-reader input by reading from the disk similarly  when the job requests the printer to output a line  that line is copied into a system buffer and is written to the disk when the job is cmttpleted  the output is actually printed this form of processing is called spooling  figure 23.5  ; the name is an acronym for simultaneous peripheral operation on-line spooling  in essence  uses the disk as a huge buffer for reading as far ahead as possible on input devices and for storing output files until the output devices are able to accept them disk line printer figure 23.5 spooling 23.3 23.3 911 spooling is also used for processing data at remote sites the cpu sends the data via communication paths to a remote printer  or accepts an entire input job from a remote card reader   the remote processing is done at its own speed  with no cpu intervention the cpu just needs to be notified when the processing is completed  so that it can spool the next batch of data spooling overlaps the i/o of one job with the computation of other jobs even in a simple systen l  the spooler may be reading the input of one job while printing the output of a different job during this time  still another job  or other jobs  may be executed  reading its cards from disk and printing its output lines onto the disk spooling has a direct beneficial effect on the performance of the system  for the cost of some disk space and a few tables  the computation of one job and the i/0 of other jobs can take place at the same time thus  spooling can keep both the cpu and the i/0 devices working at much higher rates spooling leads naturally to multiprogramming  which is the foundation of all modern operating systems the atlas operating system  kilburn et al  1961   howarth et al  1961   was designed at the university of manchester in england in the late 1950s and early 1960s many of its basic features that were novel at the time have become standard parts of modern operating systems device drivers were a major part of the system in addition  system calls were added by a set of special instructions called extra codes atlas was a batch operating system with spooling spooling allowed the system to schedule jobs according to the availability of peripheral devices  such as magnetic tape units  paper tape readers  paper tape punches  fuce printers  card readers  and card punches the most remarkable feature of atlas  however  was its memory management core was new and expensive at the time many computers  like the ibm 650  used a drum for primary memory the atlas system used a drum for its main memory  but it had a small amount of core memory that was used as a cache for the drum demand paging was used to transfer information between core memory and the drum automatically the atlas system used a british computer with 48-bit words addresses were 24 bits but were encoded in decimal  which allowed only 1 million words to be addressed at that time  this was an extremely large address space the physical memory for atlas was a 98-kb-word drum and 16-kb words of core memory was divided into 512-word pages  providing 32 frames in physical memory an associative memory of 32 registers implemented the mapping from a virtual address to a physical address if a page fault occurred  a page-replacement algorithm was invoked one memory frame was always kept empty  so that a drum transfer could start immediately the page-replacement algorithm attempted to predict future memory-accessing behavior based on past behavior a reference bit for each frame was set whenever the frame was accessed the reference bits were read into memory every 1,024 instructions  and the last 32 values of these bits were 912 chapter 23 23.4 retained this history was used to define the time since the most recent reference  t1  and the interval between the last two references  t2   pages were chosen for replacement in the following order  any page with t1 t2 + 1 ; such a page is considered to be no longer in use iff   =  t2 for all pages  then replace the page with the largest t2 t1  the page-replacement algorithm assumes that programs access memory in loops if the time between the last two references is t2  then another reference is expected t2 time units later if a reference does not occur  t1 t2   it is assumed that the page is no longer being used  and the page is replaced if all pages are still in use  then the page that will not be needed for the longest time is replaced the time to the next reference is expected to be t2 t1  the xds-940 operating system  lichtenberger and pirtle  1965   was designed at the university of california at berkeley like the atlas system  it used paging for memory management unlike the atlas system  it was a time-shared system the paging was used only for relocation ; it was not used for demand paging the virtual memory of any user process was made up of 16-kb words  whereas the physical memory was made up of 64-kb words each page was made up of 2-kb words the page table was kept in registers since physical memory was larger than virtual memory  several user processes could be in memory at the same time the number of users could be increased by sharing of pages when the pages contained read-only reentrant code processes were kept on a drum and were swapped in and out of memory as necessary the xds-940 system was constructed from a modified xds-930 the modifications were typical of the changes made to a basic computer to allow an operating system to be written properly a user-monitor ltlode was added certain instructions  such as i/0 and halt  were defined to be privileged an attempt to execute a privileged instruction in user mode would trap to the operating system a system-call instruction was added to the user-mode instruction set this instruction was used to create new resources  such as files  allowing the operating system ~ to manage the physical resources files  for example  were allocated in 256-word blocks on the drum a bit map was used to manage free drum blocks each file had an index block with pointers to the actual data blocks index blocks were chained together the xds-940 system also provided system calls to allow processes to create  start  suspend  and destroy subprocesses a programmer could construct a system of processes separate processes could share memory for communication and synchronization process creation defined a tree structure  where a process is the root and its subprocesses are nodes below it in the tree each of the subprocesses could  in turn  create more subprocesses 23.5 23.6 23.6 913 the the operating system  dijkstra  1968   mckeag and wilson  1976   was designed at the technische hogeschool at eindhoven in the netherlands it was a batch system running on a dutch computer  the el xs  with 32 kb of 27-bit words the system was mainly noted for its clean design  particularly its layer structure  and its use of a set of concurrent processes employing semaphores for synchronization unlike those in the xds-940 system  however  the set of processes in the the system was static the operating system  itself was designed as a set of cooperating processes in addition  five user processes were created that served as the active agents to compile  execute  and print user programs when one job was finished  the process would return to the input queue to select another job a priority cpu-scheduling algorithm was used the priorities were recomputed every 2 seconds and were inversely proportional to the amount of cpu time used recently  in the last 8 to 10 seconds   this scheme gave higher priority to i/o-bound processes and to new processes memory management was limited by the lack of hardware support however  since the system was limited and user programs could be written only in algol  a software paging scheme was used the algol compiler automatically generated calls to system routines  which made sure the requested information was in memory  swapping if necessary the backing store was a 512-kb-word drum a 512-word page was used  with an lru page-replacement strategy another major concern of the the system was deadlock control the banker 's algorithm was used to provide deadlock avoidance closely related to the the system is the venus system  liskov  1972    the venus system was also a layer-structured design  using semaphores to synchronize processes the lower levels of the design were implemented in microcode  howeve1 ~ providing a much faster system the memory management was changed to paged-segmented memory the system was also designed as a time-sharing system  rather than a batch system the rc 4000 system  like the the system  was notable primarily for its design concepts it was designed for the danish 4000 computer by regnecentralen  particularly by brinch-hansen  brinch-hansen  1970   brinch-hansen  1973    the objective was not to design a batch system  or a time-sharing system  or any other specific system rathel ~ the goal was to create an operating-system nucleus  or kernel  on which a complete operating system could be built thus  the system structure was layered  and only the lower levels-comprising the kernel-were provided the kernel supported a collection of concurrent processes a round-robin cpu scheduler was used although processes could share memory  the primary communication and synchronization itl.echanism was the provided by the kernel processes could communicate with each other by exchanging fixed-sized rnessages of eight words in length all messages were 914 chapter 23 23.7 stored in buffers from a common buffer pool when a ncessage buffer was no longer required  it was returned to the common pool a message queue was associated with each process it contained all the messages that had been sent to that process but had not yet been received messages were removed from the queue in fifo order the system supported four primitive operations  which were executed atomically  send-message  in receiver  in message  out buffer  wait-message  out sender  out message  out buffer  send-answer  out resul  in message  in buffer  wait-answer  out result  out message  in buffer  the last two operations allowed processes to exchange several messages at a time these primitives required that a process service its message queue in fifo order and that it block itself while other processes were handling its messages to remove these restrictions  the developers provided two additional communication primitives that allowed a process to wait for the arrival of the next message or to answer and service its queue in any order  wait-event  in previous-buffer  out next-buffer  out result  get-event  out buffer  i/o devices were also treated as processes the device drivers were code that converted the device interrupts and registers into messages thus  a process would write to a terminal by sending that terminal a message the device driver would receive the message and output the character to the terminal an input character would interrupt the system and transfer to a device driver the device driver would create a message from the input character and send it to a waiting process the compatible time-sharing system  ctss   corbato et al  1962   was designed at mit as an experimental time-sharing system it was implemented on an ibm 7090 and eventually supported up to 32 interactive users the users were provided with a set of interactive commands that allowed them to manipulate files and to compile and run programs through a terminal the 7090 had a 32-kb memory made up of 36-bit words the monitor used 5-kb words  leaving 27 kb for the users user memory images were swapped between memory and a fast drum cpu scheduling employed a multilevelfeedback queue algorithm the time quantum for level i was 2 i time units if a program did not finish its cpu burst in one time quantum  it was moved down to the next level of the queue  giving it twice as much time the program at the highest level  with the shortest quantum  was run first the initial level of a program was determined by its size  so that the time quantum was at least as long as the swap time 23.8 23.9 23.9 ibm os/360 915 ctss was extremely successful and was in use as late as 1972 although it was limited  it succeeded in demonstrating that time sharing was a convenient and practical mode of computing one result of ctss was increased development of time-sharing systems another result was the development of multics the multics operating system  corbato and vyssotsky  1965   organick  1972   was designed at mit as a natural extension of ctss ctss and other early tin e-sharing systems were so successful that they created an immediate desire to proceed quickly to bigger and better systems as larger computers became available  the designers of ctss set out to create a time-sharing utility computing service would be provided like electrical power large computer systems would be connected by telephone wires to terminals in offices and homes throughout a city the operating system would be a time-shared system running continuously with a vast file system of shared programs and data multics was designed by a team from mit  ge  which later sold its computer department to honeywell   and bell laboratories  which dropped out of the project in 1969   the basic ge 635 computer was modified to a new computer system called the ge 645  mainly by the addition of pagedsegmentation memory hardware in multics  a virtual address was composed of an 18-bit segment number and a 16-bit word offset the segments were then paged in 1-kb-word pages the second-chance page-replacement algorithm was used the segmented virtual address space was merged into the file system ; each segment was a file segments were addressed by the name of the file the file system itself was a multilevel tree structure  allowing users to create their own subdirectory structures like ctss  multics used a multilevel feedback queue for cpu scheduling protection was accomplished through an access list associated with each file and a set of protection rings for executing processes the system  which was written almost entirely in pl/l  comprised about 300,000 lines of code it was extended to a multiprocessor system  allowing a cpu to be taken out of service for maintenance while the system continued running the longest line of operating-system development is undoubtedly that of ibm computers the early ibm computers  such as the ibm 7090 and the ibm 7094  are prime examples of the development of common l/0 subroutines  followed by development of a resident monitor  privileged instructions  memory protection  and simple batch processing these systems were developed separately  often at independent sites as a result  ibm was faced with many different computers  with different languages and different system software the ibm/360 was designed to alter this situation the ibm/360 was designed as a family of computers spanning the complete range from small business machines to large scientific machines only one set of software would be 916 chapter 23 needed for these systems  which all used the same operating system  os/360  mealy et al  1966    this arrangement was intended to reduce maintenance problems for ibm and to allow users to move programs and applications freely from ~ one ibm system to another unfortunately  os/360 tried to be all things for all people as a result  it did none of its tasks especially well the file system included a type field that defined the type of each file  and different file types were defined for fixed-length and variable-length records and for blocked and unblocked files contiguous allocation was used  so the user had to guess the size of each output file the job control language  jcl  added parameters for every possible option  making it incomprehensible to the average user the memory-management routines were hampered by the architecture although a base-register addressing mode was used  the program could access and modify the base register  so that absolute addresses were generated by the cpu this arrangement prevented dynamic relocation ; the program was bound to physical memory at load time two separate versions of the operating system were produced  os/mft used fixed regions and os/mvt used variable regions the system was written in assembly language by thousands of programmers  resulting in millions of lines of code the operating system itself required large amounts of memory for its code and tables operating-system overhead often consumed one-half of the total cpu cycles over the years  new versions were released to add new features and to fix errors however  fixing one error often caused another in some remote part of the system  so that the number of known errors in the system remained fairly constant virtual memory was added to os/360 with the change to the ibm 370 architecture the underlying hardware provided a segmented-paged virtual memory new versions of os used this hardware in different ways os/vsl created one large virtual address space and ran os/mft lie that virtual memory thus  the operating system itself was paged  as well as user programs os/vs2 release 1 ran os/mvt in virtual memory finally  os/vs2 release 2  which is now called mvs  provided each user with his own virtual memory mvs is still basically a batch operating system the ctss system was run on an ibm 7094  but the developers at mit decided that the address space of the 360  ibm 's successor to the 7094  was too small for multics  so they switched vendors ibm then decided to create its own time-sharing system  tss/360  lett and konigsford  1968    like multics  tss/360 was supposed to be a large  time-shared utility the basic 360 architecture was modified in the model 67 to provide virtual memory several sites purchased the 360 i 67 in anticipation of tss/360 tss/360 was delayed  however  so other tiine-sharing systems were developed as temporary systems until tss/360 was available a time-sharing option  tso  was added to os/360 ibm 's cambridge scientific center developed cms as a single-user system and cp /67 to provide a virtual machine to run it on  meyer and seawright  1970   parmelee et al  1972    when tss/360 was eventually delivered  it was a failure it was too large and too slow as a result  no site would switch from its temporary system to tss/360 today  time sharing on ibm systems is largely provided either by tso under mvs or by cms under cp i 67  renamed vm   neither tss/360 nor multics achieved commercial success what went wrong part of the problem was that these advanced systems were too large 23.13 919 those systems and more useful for many purposes minicomputers died out  replaced by general and special-purpose servers although personal computers continue to increase in capacity and performance  servers tend to stay ahead of them in amount of memory  disk space  and number and speed of available cpus today  servers typically run in data centers or machine rooms  while personal computers sit on or next to desks and talk to each other and servers across a network the desktop rivalry between apple and microsoft continues today  with new versions of windows and mac os trying to outdo each other in features  usability and application functionality other operating systems  such as amigaos and os/2  have appeared over time but have not been long-term competitors to the two leading desktop operating systems meanwhile  linux in its many forms continues to gain in popularity among more technical users -and even with nontechnical users on systems like the one per children 's connected computer network  http  i /laptop org/   for more information on these operating systems and their history  see freiberger and swaine  2000   23.13 the mach operating system traces its ancestry to the accent operating system developedatcarnegiemellon university  cmu   rashid and robertson  1981    mach 's communication system and philosophy are derived from accent but many other significant portions of the system  for example  the virtual memory system and task and thread management  were developed from scratch  rashid  1986   tevanian et al  1989l and accetta et al  1986    the mach scheduler was described in detail by tevanian et al  1987a  and black  1990   an early version of the mach shared-memory and memory-mapping system was presented by tevanian et al  1987b   the mach operating system was designed with the following three critical goals in mind  1 emulate 4.3bsd unix so that the executable files from a unix system can run correctly under mach be a modern operating system that supports many memory models  as well as parallel and distributed computing 3 have a kernel that is simpler and easier to modify than is 4.3bsd mach 's development followed an evolutionary path from bsd unix systems mach code was initially developed inside the 4.2bsd kernet with bsd kernel components replaced by mach components as the mach components were completed the bsd components were updated to 4.3bsd when that became available by 1986  the virtual memory and communication subsystems were running on the dec vax computer family  including multiprocessor versions of the vax versions for the ibm rt /pc and for sun 3 workstations followed shortly then  1987 saw the completion of the encore multimax and sequent balance multiprocessor versions  including task and thread support  as well as the first official releases of the system  release 0 and release 1 920 chapter 23 23.14 through release 2  mach provided compatibility with the corresponding bsd systems by including much o bsd ' s code in the kernel the new features and capabilities of mach made the kernels in these releases larger than the corresponding bsd kernels mach 3 moved the bsd code outside the kernet leaving a much smaller microkernel this system implements only basic mach features in the kernel ; all unix-specific code has been evicted to run in user-mode servers excluding unix-specific code rom the kernel allows the replacement of bsd with another operating system or the simultaneous execution of multiple operating-system interfaces on top of the microkernel in addition to bsd  user-mode implementations have been developed for dos  the macintosh operating system  and osf/1 this approach has similarities to the virtual machine concept but here the virtual machine is defined by software  the mach kernel interface   rather than by hardware with release 3.0  mach became available on a wide variety of systems  including single processor sun  intet ibm  and dec machines and multiprocessor dec  sequent  and encore systems mach was propelled to the forefront of industry attention when the open software foundation  osf  announced in 1989 that it would use mach 2.5 as the basis for its new operating system  osf/1  mach 2.5 was also the basis for the operating system on the next workstation  the brainchild of steve jobs  of apple computer fame  the initial release of osf /1 occurred a year later  and this system competed with unix system v  release 4  the operati.j.l.g system of choice at that time among unix international  ui  members osf members i.j.l.cluded key technological companies such as ibm  dec and hp osf has since changed its direction  and only dec l.jn ix is based on the mach kernel unlike unix  which was developed without regard for multiprocessing  mach incorporates multiprocessing support throughout this support is also exceedingly flexible  ranging rom shared-memory systems to systems with no memory shared between processors mach uses lightweight processes  in the form of multiple threads of execution within one task  or address space   to support multiprocessing and parallel computation its extensive use of messages as the only communication method ensures that protection mechanisms are complete and efficient by integrating messages with the virtual memory system  mach also ensures that messages can be handled efficiently finally  by having the virtual memory system use messages to communicate with the daemons managing the backing store  mach provides great 1exibility in the design and implementation of these memory-objectmanaging tasks by providing low-level  or primitive  system calls from which more complex functions can be buik mach reduces the size of the kernel while permitting operating-system emulation at the user levet much like ibm 's virtual-machine systems some previous editions of operating system concepts included an entire chapter on mach this chapter  as it appeared in the fourth edition  is available on the web  http  i lwww os-book com   there are  of course  other operating systems  and most of them have interesting properties the mcp operating system for the burroughs computer 921 family  mckeag and wilson  1976   was the first to be written in a system.programming language it supported segmentation and multiple cpus the scope operating system for the cdc 6600  mckeag and wilson  1976   was also a multi-cpu system the coordination and synchronization of the multiple processes were surprisingly well designed history is littered with operating systems that suited a purpose for a time  be it a long or a short time  and then  when faded  were replaced by operating systems that had more features  supported newer hardware  were easier to use  or were better marketed we are sure this trend will continue in the future 23.1 consider the page replacement algorithm used by atlas in what ways is it different from the clock algorithm discussed in section 9.4.5.2 23.2 consider the multilevel feedback queue used by ctss and multics suppose a program consistently uses seven time units every time it is scheduled before it performs an l/0 operation and blocks how many time units are allocated to this program when it is scheduled for execution at different points in time 23.3 what optimizations were used to minimize the discrepancy between cpu and i/o speeds on early computer systems 23.4 what conclusions can be drawn about the evolution of operating systems what causes some operating systems to gain in popularity and others to fade  accetta et al 1986  m accetta  r baron  w bolosky  d b golub  r rashid  a tevanian  and m young  mach  a new kernel foundation for unix development  proceedings of the summer usenix conference  1986   pages 93-112  adl-tabatabai et al 2007  a.-r adl-tabatabai  c kozyrakis  and b saha  unlocking concurrency  queue  volume 4  number 10  2007   pages 24-33  agrawal and abbadi 1991  d p agrawal and a e abbadi  an efficient and fault-tolerant solution of distributed mutual exclusion  acm transactions on computer systems  volume 9  number 1  1991   pages 1-20  agre 2003  p e agre  p2p and the promise of internet equality  communications of the acm  voh.une 46  number 2  2003   pages 39-42  ahituv et al 1987  n ahituv  y lapid  and s neumann  processing encrypted data  communications of the acm  volume 30  number 9  1987   pages 777-780  ahmed 2000  i ahmed  cluster computing  a glance at recent events  ieee concurrency  volmne 8  number 1  2000    ald 1983  s g akl  digital signatures  a tutorial survey  computer  volume 16  number 2  1983   pages 15-24  akyurek and salem 1993  s akyurek and k salem  adaptive block rearrangement  proceedings of the international conference on data engineering  1993   pages 182-189  alt 1993  h alt  removable media in solaris  proceedings of the winter usenix conference  1993   pages 281-287  anderson 1990  t e anderson  the performance of spin lock alternatives for shared-money multiprocessors  ieee trans parallel distrib sysl  volume 1  number 1  1990   pages 6-16  anderson et al 1989  t e anderson  e d lazowska  and h m levy  the performance implications of thread management alternatives for shared-memory multiprocessors  ieee transactions on computers  volume 38  number 12  1989   pages 1631-1644  anderson et al 1991  t e anderson  b n bershad  e d lazowska  and h m levy  scheduler activations  effective kernel support for the user-level management of parallelism  proceedings of the acm symposium on operati11g systems principles  1991   pages 95-109  anderson et al 1995  t e anderson  m d dahlin  f m neefe  d a patterson  d s roselli  and r y wang  serverless network file systems  proceedings of the acm symposium on operating systems principles  1995   pages 109-126  ande1 son et al 2000  d anderson  j chase  and a vahdat  interposed request routing for scalable network storage  proceedings of the fourth symposium on operating systems design and implementalion  2000    apple 1987  apple technical introduction lo the macintosh family addison-wesley  1987    as thana and finkelstein 1995  p asthana and b finkelstein  superdense optical storage  ieee spectrum  volume 32  number 8  1995   pages 25-31  audsley et al 1991  n c audsley  a burns  m f richardson  and a j wellings  hard real time scheduling  the deadline monotonic approach  proceedings of the ieee workshop on real-time operating systerns and software  1991   923 924  axelsson 1999  s axelsson  the base-rate fa.llacy and its implications for intrusion detection' '  procwiings of the acm conference on computer and commuuications  1999   pages 1-7  babaoglu and marzullo 1993  0 babaoglu and k marzullo ' consistent global states of distributed systems  fundamental concepts and mechanisms  pages 55-96 addison-wesley  1993    bach 1987  m j bach  the design of the unix operating system  prentice hah  1987    back et al 2000  g back  p tullman  l stoller  w c hsieh  and j lepreau  techniques for the design of java operating systems  2000 us en ix annual tee/uri cal confermce  2000    baker et al.1991  m.g baker  j h hartman  m.d k11pfer  k w shirriff  and j k uusterhout  measurements of a distribl1ted file system  proceedings of the acm s tjnzposiwn on operating systems principles  1991   pages 198-212  balakrishnan et al 2003  h balakrishnan  lvi f kaashoek  d karge1 ~ r morris  and i stoica  looking up data in p2p systems  communications of the acm  volume 46  number 2  2003   pages 43-48  baldwin 2002  j baldwin  'locking in the multithreaded freebsd kernel' '  usenix bsd  2002    barnes 1993  g barnes  a method for implementing lock-free shared data structures  proceedings of the acm symposium on pam/lei algorithms and architectures  1993   pages 261-270  barrera 1991  j s barrera  a fast mach network ipc implementation  of the us en ix mach symposium  1991   pages 1-12  basu et al 1995  a basu  v buch  w vogels  and t von eicken  u-net  a user-level network interface for parallel and distributed computing  proceedings of tlze acm sy1nposium on operating systems principles  1995    bays 1977  c bays  'a comparison of next-fit  first-fit and best-fit  communications of the acm  volume 20  number 3  1977   pages 191-192  belady 1966  l.a belady  a study of replacement algorithms for a virtual-storage computer  ibm systems journal  volume 5  number 2  1966   pages 78-101  belady et al 1969  l a be lady  r a nelson  and g s shedler  ' an anomaly in space-time characteristics of certain programs running in a paging machine  communications of tlze acm  volume 12  number 6  1969   pages 349-353  bellovin 1989  s m bellovin  security problems in the tcp lip protocol suite  computer comnzunication.s reuiew  volume 19  2   1989   pages 32-48  ben-ari 1990  m ben-ari  principles of concurrent and distributed programming  prentice hall  1990    benjamin 1990  c d benjamin  the role of optical storage technology for nasa' '  proceedings  storage and retrieval systems and applimtions  1990   pages 10-17  bernstein and goodman 1980  p a bernstein and n goodman  time-stamp-based algorithms for concurrency control in distributed database systems  proceedings of the international conference on very large databases  1980   pages 285-300  bernstein et al 1987  a bernstein  v hadzilacos  and n goodman  concurrency control and recovery in database systems  addison-wesley  1987    bershad 1993  b bershad  practical considerations for non-blocking concurrent objects  ieee international conference on distributed computing systems  1993   pages 264-273  bershad and pinkerton 1988  b n bershad and c b pinkerton  watchdogs  extending the unix file system  of tlze winter usenix conference  1988    bershad et al 1990  b n bershad  t e anderson  e d lazowska  and h m levy  lightweight remote procedure call  acm transactions on computer systems  volume 8  number 1  1990   pages 37-55  bershad et al 1995  b n bershad  s savage  p pard yak  e g sirer  m fiuczynski  d becke1 ~ s eggers  and c chambers  extensibility  safety and performance in the spln operating system  ' ' ' ' ' of tile acm symposium on operating stjstems principles  1995   pages 267-284  beveridge and wiener 1997  j beveridge and r wiener  mutlithreading in win32  addison-wesley  1997    binding 1985  c binding  cheap concurrency inc  sigplan notices  volume 20  number 9  1985   pages 21-27  birrell1989  a d birrell  an introduction to programming with threads  technical report  dec-src  1989    birrell and nelson 1984  a d birrell and b j nelson  ''implementing remote procedure calls  acm transactions on computer systems  volume 2  number 1  198,1   pages 39-59  blaauw and brooks 1997  g blaauw and f brooks  computer architecture  concepts and evolution  addison-wesley  1997    black 1990  d l black  'scheduling support for concurrency and parallelism in the mach operating system  ieee computer  volume 23  number 5  1990   pages 35-43 925  bobrow et al.l972  d g bobrow  j.d burchfiel  d l murphy  and r s tomlinson  'tenex  a paged time sharing system for the pdp-10  co111munications of the acm  volume 15  number 3  1972    bolosky eta !  1997  w j bolosky  r  j fitzgerald  and j r douceur  ' distributed schedule management in the ttger vtdeo .fileserver  proceedings of the acm symposiwn on operating systems principles  1997   pages 212-223  bonwick 1994  j bonwick  ' the slab allocator  an object-caching kernel memory allocator '  usenix summer  1994   pages 87-98  bonwick and adams 2001  j bonwick and j adams  magazines and vmem  extending the slab allocator to many cpus and arbitrary resources  proceedings of the 2001 use nix annual technical conference  2001    bovet and cesati 2002  d p bovet and m cesati  understanding the linux kemel  second edition  o'reilly & associates  2002    bovet and cesati 2006  d bovet and m cesati  understanding the limtx kernel  third edition  o'reilly & associates  2006    brain 1996  m brain  win32 system services  second edition  prentice hall  1996    b1 ent 1989  r brent  efficient implementation of tbe .first-fit strategy for dynamic storage allocation  acm transactions on programming languages and systems  volume 11  number 3  1989   pages 388-403  brereton 1986  0 p brereton  'management of replicated files in a unix environment '  software -practice and experience  volume 16   1986   pages 771-780  brinch-hansen 1970  p brinch-hansen  the nucleus of a multiprogramming system '  com7 ! 1ltnications of the acm  volume 13  number 4  1970   pages 238-241 and 250  brinch-hansen 1972  p brinch-hansen  ''structured multiprograrm11ing '  communications of tlze acm  volume 15  number 7  1972   pages 574-578  brinch-hansen 1973  p bri.nch-hansen  operating system principles  prentice hall  1973    brookshear 2003  j g brookshear  computer science  an overview  seventh edition  addison-wesley  2003    brownbridge eta !  1982  d r brownbridge  l f marshall  and b randell  'the newcastle connection or unixes of the world unite !  software practice and experience  volume 12  number 12  1982   pages 1147-1162  burns 1978  j e burns  'mutual exclusion with linear waiting using binary shared variables '  sigact news  volume 10  number 2  1978   pages 42-47  butenhof 1997  d butenhof  programming with posjx threads  addison-wesley  1997    buyya 1999  r buyya  high pe7formance cluster computing  architectures and systems  prentice hall  1999    callaghan 2000  b callaghan  nfs illustrated  addison-wesley  2000    can trill eta !  2004  b m cantril !  m w shapiro  and a h leventhal  'dynamic instrumation of production systems '  2004 use nix annual technical conference  2004    carr and hennessy 1981  w r carr and j l hennessy  wsc ! ock-a simple and effective algoritlun for virtual memory management '  proceedings of the acm symposium on operating systems principles  1981   pages 87-95  carvalbo and roucairol1983  0 s carvallio and g roucairol  'on mutual exclusion in computer networks '  communications of the acm  volume 26  number 2  1983   pages 146-147  ceruzzi 1998  p e ceruzzi  a history of modem computing  mit press  1998    chandy and lamport 1985  k m chandy and l lamport  ' distributed snapshots  determining global states of distributed systems  acm transactions on computer systems  volume 3  number 1  1985   pages 63-75  chang 1980  e chang  n-philosophers  an exercise in distributed control  computer networlcs  volume 4  number 2  1980   pages 71-76  chang and mergen 1988  a chang and m f mergen  801 storage  architecture and programming' '  acm transactions on computer systems  volume 6  number 1  1988   pages 28-50  chase eta !  1994  j s chase  h m levy  m j feeley  and e d lazowska  sharing and protection in a single-address-space operating system' '  acm transactions on computer systems  volume 12  number 4  1994   pages 271-307  chen eta !  1994  p.m chen  e k lee  g a gibson  r h katz  and d a patterson  'raid  high-performance  reliable secondary storage  acm computing surveys  volume 26  number 2  1994   pages 145-185  cheriton 1988  d cheriton  ' the v distributed system  communications of the acm  volume 31  number 3  1988   pages 314-333 926  cheriton et al 1979  d r cheriton  m.a malcolm  l s me len  and g r sager  'thoth  a portable real-time operating system  communications of the acm  volume 22  number 2  1979   pages 105-115  cheswick et al 2003  w cheswick  s bellovin  and a rubin  firewalls and internet security  repelling the wily hacker  second edition  addison-wesley  2003    cheung and loong 1995  w h cheung and a h s loong  exploring issues of operating systems structuring  from microkemel to extensible systems '  operating systems review  volume 29  number 4  1995   pages 4-16  chi 1982  c s chi  advances in computer mass storage technology  computer  volume 15  number 5  1982   pages 60-74  coffman et al 1971  e g coffman  m j elphick  and a shoshani  system deadlocks '  computing surveys  volume 3  number 2  1971   pages 67-78  cohen and jefferson 1975  e s cohen and d jefferson  protection in the hydra operating system  proceedings of the acm symposium on operating systems principles  1975   pages 141-160  cohen and woodring 1997  a cohen and m woodring  win32 multithreaded programming  o'reilly & associates  1997    comer 1999  d comer  internetworlcing with tcp/ip  volume ii  third edition  prentice hall  1999    comer 2000  d comer  intemetworlcing with tcp/ip  volume i  fourth edition  prentice hall  2000    corbato and vyssotsky 1965  f j corbato and v a vyssotsky  introduction and overview of the multics system  proceedings of the afips fall joint computer conference  1965   pages 185-196  corbato et al 1962  f j corbato  m merwin-daggett  and r c daley  an experimental time sharing system  proceedings of the aflps fall joint computer conference  1962   pages 335-344  carmen et al 2001  t h cormen  c e leiserson  r l rivest  and c stein  introduction to algorithms  second edition  mit press  2001    coulouris et al 2001  g coulouris  j dollimore  and t kind berg  distributed systems concepts and designs  third edition  addison wesley  2001    courtois et al 1971  p j courtois  f heymans  and d l parnas  concurrent control with 'readers ' and 'writers '  communications of the acm  volume 14  number 10  1971   pages 667-668  culler et al 1998  d e culler  j p singh  and a gupta  parallel computer architecture  a hardware/software approach  morgan kaufmann publishers inc  1998    custer 1994  h custer  inside the windows nt file system  microsoft press  1994    dabek et al 2001  f dabek  m f kaashoek  d karger  r morris  and i stoica  wide-area cooperative storage with cfs  proceedings of the acm symposium on operating systems principles  2001   pages 202-215  daley and dennis 1967  in multics  proceedings 121-128 r c daley and j b dermis  virtual memory  processes  and sharing of the acm symposium on operating systems principles  1967   pages  davcev and burkhard 1985  d davcev and w a burkhard  consistency and recovery control for replicated files  proceedings of the acm symposium on operating systems principles  1985   pages 87-96  davies 1983  d w davies  applying the rsa digital signature to electronic mail  computer  volume 16  number 2  1983   pages 55-62  debruijn 1967  n g debruijn  additional comments on a problem in concurrent programming and control  communications of the acm  volume 10  number 3  1967   pages 137-138  deitel1990  h m deitel  an introduction to operating systems  second edition  addison-wesley  1990    denning 1968  p j denning  the working set model for program behavior  communications of the acm  volume 11  number 5  1968   pages 323-333  denning 1980  p j denning  working sets past and present  ieee transactions on software engineering  volume se-6  number 1  1980   pages 64--84  denning 1982  d e derming  cryptography and data security  addison-wesley  1982    denning 1983  d e denning  protecting public keys and signature keys  computer  volume 16  number 2  1983   pages 27-35  denning 1984  d e denning  digital signatures with rsa and other public-key cryptosystems  conzmunications of the acm  volume 27  number 4  1984   pages 388-392  denning and denning 1979  d e denning and p j denning  data security  acm computing surveys  volume 11  number 3  1979   pages 227-249  dennis 1965  j b dennis  segmentation and the design of multiprogrammed computer systems  communications of the acm  volume 8  number 4  1965   pages 589-602 927  dennis and horn 1966  j b dennis and e c v horn  programming semantics for mu.ltiprogrammed computations' '  communications of the acm  volume 9  number 3  1966   pages h3-155  di pietro and mancini 2003  r di pietro and l v mancini  security and privacy issues o handheld and wearable wireless devices  communicalions of the acm  volume 46  number 9  2003   pages 74-79  diffie and hellman1976  w diffie and m e hellman  new directions in cryptography  ieee transactions on information theory  volume 22  number 6  1976   pages 644-654  diffie and hellman 1979  w diffle and m e hellman  privacy and authentication  proceedings of the ieee  1979   pages 397-427  dijkstra 1965a  e w dijkstra  cooperating sequential processes  teclmical report  teclmological university  eindhoven  the netherlands  1965    dijkstra 1965b  e w dijkstra  solution of a problem in concurrent programming control  communications of the acm  volume 8  number 9  1965   page 569  dijkstra 1968  e w dijkstra  the sh ucture of the the multiprogramming system  communications of the acm  volume 11  number 5  1968   pages 341-346  dijkstra 1971  e w dijkstra  hierarchical ordering of sequential processes  acta informatica  volume 1  number 2  1971   pages 115-138  dod 1985  trusted computer system evaluation criteria department of defense  1985    dougan et al 1999  c dougan  p mackerras  and v yodaiken  optimizing the idle task and other mmu tricks '  proceedings of the symposium on operating system design and implementation  1999    douglis and ousterhout 1991  f doughs and j k ousterhout  transparent process migration  design alternatives and the sprite implementation  software practice and experience  volume 21  number 8  1991   pages 757-785  douglis et al 1994  f doughs  f kaashoek  k li  r caceres  b marsh  and j a taube1 ~ storage alternatives for mobile computers  proceedings of the symposium on operating systems design and implementation  1994   pages 25-37  douglis et al 1995  f doughs  p krishnan  and b bershad  adaptive disk spin-down policies for mobile computers  proceedings of the usenix symposium on mobile and location independent computing  1995   pages 121-137  druschel and peterson 1993  p druschel and l l peterson  fbufs  a high-bandwidth cross domain transfer facility  proceedings of the acm symposium on operating systems principles  1993   pages 189-202  eastlake 1999  d eastlake  domain name system security extensions  network working group  request for comm.ents  2535  1999    eisenberg and mcguire 1972  m a eisenberg and m r mcguire  further comments on dijksh a 's concurrent programming control problem  communications of the acm  volume 15  number 11  1972   page 999  ekanadham and bernstein 1979  k ekanadham and a j bernstein  conditional capabilities  ieee transactions on software engineering  volume se-5  number 5  1979   pages 458-464  engelschall2000  r engelschall  portable multithreading  the signal stack trick for user-space thread creation '  proceedings of the 2000 usenix annual technical conference  2000    eswaran et al 1976  k p eswaran  j n gray  r a lorie  and i l traiger  the notions of consistency and predicate locks in a database system  communications of the acm  volume 19  number 11  1976   pages 624-633  fang et al 2001  z fang  l zhang  j b carter  w c hsieh  and s a mckee  reevaluating online superpage promotion witl1 hardware support  proceedings of the international symposium on high-pejformance computer architecture  volume 50  number 5  2001    farrow 1986a  r farrow  security for superusers  or how to break the unix system  unix world  may 1986   pages 65-70  farrow 1986b  r farrow  security issues and strategies for users  unix world  april 1986   pages 65-71  fidge 1991  c fidge  logical time in distributed computing systems  computer  volume 24  number 8  1991   pages 28-33  filipski and hanko 1986  a filipski and j hanko  making unix secure  byte  april 1986   pages 113-128  fisher 1981  j a fisher  trace scheduling  a technique for global microcode compaction  ieee transactions on computers  volume 30  number 7  1981   pages 478-490 928  folk and zoellick 1987  m j folk and b zoellick  file slructures  addison-wesley  1987    forrest eta !  1996  s forrest  s a hofmey1 ~ and t a longstaff  'a sense of sel for u ntx processes '  proceedings of lite ieee sy1uposiwn on security and  1996   pages 120-128  fortier 1989  p j fortier  handbook of lan technology  mccraw-h.ill  1989    frah 2001  c frah  the universal history ofcol ! lpuling  john wiley and sons  2001    frauenfelder 2005  m frauenfelder  the computer-an illustrated histon ;  carlton books  2005    freedman 1983  d h freedman  searching for denser disks '  jnfosyslcms  1983   page 56  freiberger and swaine 2000  p freiberger and m swaine  fire in the valley-the of the personal computer  mcgraw-hill  2000    fuhrt 1994  b fuhrt  multimedia systems  an overview '  ieee multimedia  volume 1  number 1  199'1   pages 47-59  fujitani 1984  l fujitani  laser optical disk  the coming revolution in on-line storage  communications of the acm  volume 27  number 6  1984   pages 546-554  gait 1988  j gait  the optical file cabinet  a random-access file system for write-on optical disks  computer  volume 21  number 6  1988    ganapathy and schimmel1998  n canapathy and c schimmel  general purpose operating system support for multiple page sizes '  proceedings of the usenix technical conference  1998    ganger et al 2002  c r gange1 ~ d r engle1 ~ m f kaashoek  h m briceno  r hunt  and t pinckney  'fast and flexible application-level networking on exokernel systems '  acm transactions on computer systems  volume 20  number 1  2002   pages 49-83  garcia-molina 1982  h garcia-molina  elections in distributed computing systems '  ieee transactions 011 computers  volum.e c-31  number 1  1982    garfinkel et al 2003  s garfinkel  g spafford  and a schwartz  practical unix & lntemel o'reilly & associates  2003    chemawat et al 2003  s chemawat  h gobiof  and s.-t leung  the coogle file system  proceedings of tlze acm st ; 1nposiwn on operating systems principles  2003    gibson et al 1997a  c gibson  d nagle  k amiri  f chang  h cobiof  e riedel  d rochberg  and j zelenka  'filesystem s for network-attached secure disks  teclmical report  carnegie-mellon university  1997    gibson et al.1997b  g a gibson  d nagle  k amiri  f w chang  e m feinberg  h gobio  c lee  b ozceri  e riedel  d rochberg  andj zelenka  file server scaling with network-attached secure disks  measurement and modeling of computer systems  1997   pages 272-284  gifford 1982  d k gifford  cryptographic sealing for infonnation secrecy and authentication '  comnw.nications of the acm  volume 25  number 4  1982   pages 274-286  goetz et al 2006  b goetz  t peirls  j bloch  j bow beer  d holmes  and d lea  java concurrency in practice  addison-wesley  2006    goldberg et al 1996  i goldberg  d wagner  r thomas  and e a brewe1 ~ a secure environment for untrusted helper applications  proceedings of the 6th usenix security syrnposiwn  1996    colden and pechura 1986  d golden and m pechura  'the structure o microcomputer file systems  communications of the acm  volume 29  number 3  1986   pages 222-230  golding et al 1995  r a golding  p b ii  c staelin  t sullivan  and j wilkes  idleness is not sloth  usenix winter  1995   pages 201-212  cohn et al 2002  m golm  m felser  c wawersich  and j kleinoder  ' the jx operati11g system  2002 usenix annual technical conference  2002    gong 2002  l gong  peer-to-peer networks in action  ieee internet computini   volume 6  nwnber 1  2002    gong et al 1997  l gong  m muelle1 ~ h prafullchandra  and r schemers  going beyond the sandbox  an overview of the new security architecture in the java development kit 1.2  proceedings of the usenix synzposium on internet technologies and systems  1997    goodman eta !  1989  j r goodman  m k vernon  and p j woest  efficient synchronization primitives for large-scale cache-coherent multiprocessors  proceedings of the international conference on architectural support for programming languages and operaling st ; sterrjs  1989   pages 64-75  gosling et al 1996   1996   j gosling  b joy  and c steele  tlzejava language specification  addison-wesley  covindan and anderson 1991  r govindan and d p anderson  scheduling and ipc mechanisms for continuous media  proceedings of the acm sy1nposium on operating systems  1991   pages 68-80  crampp and morris 1984  f t crampp and r h morris  unix operating-system security '  at&t bell laboratories technical journal  volume 63  number 8  1984   pages 1649-1672 929  gray 1978  j n gray  'notes on data base operating systems  in  bayer et al 1978   1978   pages 393-'181  gray 1981  j n g1 ay  ''the transaction concept  virtues and limitations' '  proceedings of the international conference on very large databases  1981   pages 144-154  gray 1997    gray  lnlerprocess communications in unix  prentice hall  1997    gray et al 1981    n gray  p.r mc  ones  and m blasgen  the recovery manager of the system r database manager  acm computing surveys  volume 13  number 2  1981   pages 223-2-12  greenawalt 1994  p greenawalt  modeling power management for hard disks  proceedings of the symposium on modeling and simulation of computer telecomnnmimtion systems  1994   pages 62-66  grosshans 1986  d grosshans  file systems design and implementation  prentice hall  1986    habermann 1969  a n habermann  prevention of system deadlocks  communications of tlze acm  volume 12  number 7  1969   pages 373-377,385  hallet al 1996  l hall  d shmoys  and j wein  scheduling to minimize average completion time  off-line and on-line algorithms  soda  acm-s ! am symposium on discrete algorithms  1996    hamacher et al 2002  c hamacher  z vranesic  and s zaky  computer organization  fifth edition  mcgraw-hill  2002    han and ghosh 1998  k han and s ghosh  a comparative analysis of virtual versus physical process-migration strategies for distributed modeling and simulation of mob.ile computing networks' '  wireless networks  volume 4  number 5  1998   pages 365-378  harchol-balter and downey 1997  m harchol-balter and a b downey  'exploiting process lifetime distributions for dynamic load balancing  acm transactions 011 computer systems  volume 15  number 3  1997   pages 253-285  harish and owens 1999  v c harish and b owens  dynamic load balancing dns  limtx journal  volume 1999  number 64  1999    harker et al 1981  j m harker  d w brede  r e pattison  g r santana  and l g taft  a quarter century of disk file imwvation' '  ibm journal of research and development  volume 25  number 5  1981   pages 677-689  harold 2005  e r harold  java network programming  third edition  o'reilly & associates  2005    harrison et al 1976  m a harrison  w l ruzzo  and j d ullman  protection in operating systems' '  communicatio11s of the acm  volume 19  number 8  1976   pages 461-471  hart 2005  j m hart  windows system programming  tlzird edition  addison-wesley  2005    hartman and ousterhout 1995  j h hartman and j k ousterhout  the zebra striped network file system' '  acm ti ansactions 011 computer systems  volume 13  number 3  1995   pages 274-310  havender 1968  j w havender  avoiding deadlock in multitasking systems' '  ibm systems joumal  volume 7  number 2  1968   pages 74-84  hecht et al 1988  m.s hecht  a johri  r aditham  and t j wei  ' experience adding c2 security features to unix  proceedings of the summer use nix conference  1988   pages 133-146  hennessy and patterson 2002  j l he1messy and d a patterson  computer architecture  a quantitative approach  third edition  morgan kaufmann publishers  2002    hennessy and patterson 2007  j hennessy and d patterson  computer architecture  a quantitative approach  fourth edition  morgan kaufmann  2007    henry 1984  g henry  the fair share scheduler' '  at&t bell labomtories technical joumal  1984    herlihy 1993  m herlihy  a methodology for implementing highly concurrent data objects  acm transactions on programming and systems  volume 15  number 5  1993   pages 745-770  herlihy and moss 1993  m herlihy and j e b moss  transactional memory  architectural support for lock-free data structures  proceedings of ! he twentiefh annual intemational symposium on computer architeclure  1993    hitz et al 1995  d hitz  j lau  and m malcolm  file system design for an nfs file server appliance  technical report  netapp  http  //www.netapp.com/techjibrary /3002.html   1995    hoagland 1985  a s hoagland  infonnation storage tedmology-a look at the future  cornputer  volume 18  number 7  1985   pages 60-68  hoare 1972  c a r hoare  towards a theory of parallel programming  in  hoare and perrott 1972   1972   pages 61-71  hoare 1974  c a r hoare  monitors  an operating system structuring concepf '  comllwnications ofl-lie acm  volume 17  number 10  1974   pages 549-557  holt 1971  r c holt  ''comments on prevention of system deadlocks  conmiunicalions of the acm  volume 14  number 1  1971   pages 36-38  holt 1972  r c holt  some deadlock properties of computer systems  computing surve ! fs  volume 4  number 3  1972   pages 179-196 930  holub 2000  a holub  yarning java threads  apress  2000    howard eta !  1988  j h howard  m l kazat ~ s g menees  d a nichols  m satyanarayanan  and r n sidebotham  scale and performance in a distributed file system  acm transactions on computer systems  volume 6  number i  1988   pages 55-81  howarth et al 1961  d j howarth  r b payne  and f h sunmer  the manchester university atlas operating system  part ii  user 's description  computer journal  volume 4  number 3  1961   pages 226-229  hsiao eta !  1979  d k hsiao  d s kerr  and s e maclnick  computer security  academic press  1979    hu and perrig 2004  y.-c hu and a pe1-rig  spv  a secure path vector routing scheme for securing bgp  proceedings of acm sigcomm conference on data communication  2004    hu eta !  2002  y.-c hu  a perrig  and d johnson  ariadne  a secure on-demand routing protocol for ad hoc networks  proceedings oft he annual international conference on mobile computing and networking  2002    hyman 1985  d hyman  the columbus chicken statute and more bonehead legislation  s greene press  1985    iacobucci 1988  e iacobucci  os/2 programmer 's guide  osborne mcgraw-hill  1988    ibm 1983  technical reference ibm corporation  1983    iliffe and jodeit 1962  j k iliffe andj g jodeit  a dynamic storage allocation system  computer journal  volume 5  number 3  1962   pages 200-209  intel1985a  iapx 286 programmer 's reference manual intel corporation  1985    intel1985b  iapx 86/88  186/188 user 's manual programmer 's reference intel corporation  1985    intel1986  iapx 386 programmer 's reference manual intel corporation  1986    intel1990  i486 microprocessor programmer 's reference manual intel corporation  1990    lntel1993  pentium processor user 's manual  volume 3  architecture and programming manual intel corporation  1993    iseminger 2000  d iseminger  active directory services for microsoft windows 2000 technical reference  microsoft press  2000    jacob and mudge 1997  b jacob and t mudge  software-mm1aged address translation  proceedings of the international symposium on high performance computer architecture and implementation  1997    jacob and mudge 1998a  b jacob and t mudge  virtual memory in contemporary microprocessors  ieee micro magazine  volume 18   1998   pages 60-75  jacob and mudge 1998b  b jacob and t mudge  virtual memory  issues of implementation  ieee computer magazine  volume 31  number 6  1998   pages 33-43  jacob and mudge 2001  b jacob and t mudge  uniprocessor virtual memory without tlbs  ieee transactions on computers  volume 50  number 5  2001    jacobson and wilkes 1991  d m jacobson and j wilkes  disk scheduling algorithms based on rotational position  teclmical report  hewlett-packard laboratories  1991    jensen et al 1985  e d jensen  c d locke  and h tokuda  a time-driven scheduling model for real-time operating systems  proceedings of the ieee real-time systems symposium  1985   pages 112-122  johnstone and wilson 1998  m s jolmstone and p r wilson  the memory fragmentation problem  solved  proceedings of the first international symposium on memory management  1998   pages 26-36  jones and liskov 1978  a k jones and b h liskov  a language extension for expressing constraints on data access  communications of the acm  volume 21  nmnber 5  1978   pages 358-367  jul et al 1988  e jul  h levy  n hutchinson  and a black  fine-grained mobility in the emerald system  acm transactions on computer systerns  volume 6  number 1  1988   pages 109-133  kaashoek eta !  1997  m f kaashoek  d r engler  g r ganger  h m briceno  r hunt  d mazieres  t pinckney  r grimm  j jannotti  and k mackenzie  application performance and flexibility on exokernel systems  proceedings of the acm symposium on operating systems principles  1997   pages 52-65  katz et al 1989  r h katz  g a gibson  and d a patterson  disk system architectures for high performance computing '  proceedings of the ieee  1989    kay and lauder 1988  j kay and p lauder  a fair share scheduler  communications of the acm  volume 31  number 1  1988   pages 44-55  kenah et al 1988  l j kenah  r e goldenberg  and s f bate  vax/vms internals and data structures  digital press  1988   931  kent el al 2000  s kent  c lynn  and k seo  secure border gateway protocol  secure-bgp   ieee joumal on selected areas in comrnunications  volume 18  number 4  2000   pages 582-592  kerrville 1982  r f kenv  lle  optical disk data storage  computer  volume 15  nltmber 7  1982   pages 21-26  kessels 1977  j l w kessels  ''an alternative to event queues for synchronization in monitors  communications of the acm  volume 20  number 7  1977   pages 500-503  kieburtz and silberschatz 1978  r b kieburtz and a silberschatz  ' capability managers  ieee transactions on software engineering  volume se-4  number 6  1978   pages 467-477  kieburtz and silberschatz 1983  r b kieburtz and a silberschatz  access right expressions  acm transactions on programming languages and syslems  volume 5  number 1  1983   pages 78-96  kilburn et al 1961  t kilburn  d j howarth  r b payne  and e h sumner  the manchester university atlas operating system  part i  internal organization  computer journal  volume 4  number 3  1961   pages 222-225  kim and spafford 1993  g i-l kim and e h spafford  the design and implementation of tripwire  a file system integrity checker  technical report  purdue university  1993    king 1990  r p king  disk arm movement in anticipation of future requests' '  acm il ansactions on computer systems  volw11e 8  number 3  1990   pages 214-229  kistler and satyanarayanan 1992  j kistler and m satyanarayanan  disconnected operation in the coda file system  acm transactions on computer systems  volume 10  number 1  1992   pages 3-25  kleinrock 1975  l kleinrock  queueing systems  volume ii  computer applications  wiley interscience  1975    knapp 1987  e knapp  ' deadlock detection in distributed databases  computing surveys  volume 19  number 4  1987   pages 303-328  knowlton 1965  k c knowlton  'a fast storage allocator  communications of the acm  volume 8  number 10  1965   pages 623-624  knuth 1966  d e knuth  additional comments on a problem in concurrent programming control  communications of the acm  volume 9  number 5  1966   pages 321-322  knuth 1973  d e knuth  the art of computer programming  volume 1  fundamental algorithms  second edition  addison-wesley  1973    knuth 1998  d e knuth  the art of computer programming  volume 3  sorting and searching  second edition  addison-wesley  1998    koch 1987  p d l koch  disk file allocation based on the buddy system  acm ti ansactions on computer systems  volume 5  number 4  1987   pages 352-370  kongetira et al 2005  p kongetira  k aingaran  and k olukotun  niagara  a 32-way multithreaded sparc processor  ieee micro magazine  volume 25  number 2  2005   pages 21-29  kopetz and reisinger 1993  h kopetz and j reisinge1 ~ the non-blocking write protocol nbw  a solution to a real-time synchronisation problem  ieee real-time systems symposium  1993   pages 131-137  kosaraju 1973  s kosaraju  'limitations of dijkstra 's semaphore primitives and petri nets' '  operating systems review  volume 7  number 4  1973   pages 122-126  kozierok 2005  c kozierok  the tcp/ip guide  no starch press  2005    kramer 1988  s m kramer  'retaining suid programs in a secure unix  proceedings of the summer usenjx conference  1988   pages 107-118  kubiatowicz et al 2000  j kubiatowicz  d bindel  y chen  s czerwinski  p eaton  d geels  r gummadi  s rhea  h weatherspoon  w weimer  c wells  and b zhao  'oceanstore  an architecture for global-scale persistent storage  proc of architectural support for programming languages and operating systems  2000    kurose and ross 2005  j kurose and k ross  computer networking-a top-down approach featuring the intemet  third edition  addison-wesley  2005    lamport 1974  l lamport  ' a new solution of dijkstra 's concurrent progra1mning problem  communications of the acm  volume 17  number 8  1974   pages 453-455  lamport 1976  l lamport  'synchronization of independent processes' '  acta informatica  volume 7  number 1  1976   pages 15-34  lamport 1977  l lamport  'concurrent reading and writing  communications of the acm  volume 20  number 11  1977   pages 806-811  lamport 1978a  l lamport  the implementation of reliable distributed multiprocess systems  computer networks  volume 2  number 2  1978   pages 95-114  lamport 1978bj l lamport  time  clocks and the ordering of events in a distributed system  communications of the acm  volume 21  number 7  1978   pages 558-565 932  lamport 1981  l lamport  password authentication with lnsecure communications  cornmunicalion s of the acm  volume 24  number 11  1981   pages 770-772  lamport 1986  l lamport  the mutual exclusion problem  comnumimlions of the acm  volume 33  number 2  1986   pages 313-348  lamport 1987  l lamport  a fast mutual excll ! sion algorithm  acm transactions on computer systems  volume 5  number 1  1987   pages 1-11  lamport 1991  l lamport  the mutual exclusion problem has been solved  connmmicalions of the acm  volume 34  number 1  1991   page 110  lamport et al 1982  l lamport  r shostak  and m pease  the byzantine generals problem  acm ij ansactions on progrmnming languages and systems  volume '1  number 3  1982   pages 382-401  lampson 1969  b w lampson  dynamic protection stmctures  proceedings of tlze afips fall joint computer  1969   pages 27-38  lampson 1971  b w lampson  protection  of the fifth annual princeton conference on information systems science  1971   pages 437-443  lampson 1973  b w lampson  a note on the confinement problem  conzmunicntiohs of the acm  volume 10  number 16  1973   pages 613-615  lampson and redell1979  b w lampson and d d redell  experience with processes and monitors in mesa  proceedings of the 7th acm symposium on operating systems principles  sosp   1979   pages 43-44  lampson and sturgis 1976  b lampson and h sturgis  crash recovery in a distributed data storage system  technical report  xerox research center  1976    landwehr 1981  c e landweht  formal models of computer security  computing surveys  volume 13  number 3  1981   pages 247-278  lann 1977  c l lann  distributed systems-toward a formal approach  proceedings of the ifjp  1977   pages 155-160  larson and kajla 1984  p larson and a kajla  file organization  implementation of a method guaranteeing retrieval in one access  communications of the acm  volume 27  number 7  1984   pages 670-677  lauzac et al 2003  s lauzac  r melhem  and d mosse  an improved rate-monotonic admission control and its applications  ieee ti-ansnctions on computers  volume 52  number 3  2003    lee 2003  j lee  an end-user perspective on file-sharing systems  communications of the acm  volume 46  number 2  2003   pages 49-53  lee and thekkath 1996  e k lee and c a thekkath  petal  distributed virtual disks '  proceedings of the seventh international conference on architectural support for programming and operating systems  1996   pages 84-92  leffler et al 1989  s j leffler  m k mckusick  m j karels  and j s quarterman  tlze design and implementation of the 4.3bsd unix operating system  addison-wesley  1989    lehmann 1987  f lehmann  computer break-ins' '  communications of the acm  volume 30  number 7  1987   pages 584-585  lehoczky et al 1989  j lehoczky  l sha  andy ding  the rate 'yionotonic scheduling algorithm  exact characterization and average case behaviour  proceedings of 10th ieee real-time sijstenzs stjmposiwn  1989    lempel1979  a lempel  cryptology in transition  computing surveys  volume 11  number 4  1979   pages 286-303  leslie et al 1996  i m leslie  d mcauley  r black  t roscoe  p t barham  d evers  r fairbairns  and e hyden  the design and implementation of an operating system to support distributed multimedia applications  ieee journal of selected areas in communications  volume 14  number 7  1996   pages 1280-1297  lett and konigsford 1968  a l lett and w l konigsford  tss/360  a time-shared operating system  proceedings of the afips fall joint computer conference  1968   pages 15-28  levin et al 1975  r levin  e s cohen  w m corwin  f j pollack  and w a wulf  'policy i mechanism separation in hydra  proceedings of the acm symposium on operating systems principles  1975   pages 132-140  levine 2003  c levine  defining deadlock  operating systems review  volume 37  number 1  2.003    levy 1994  s levy  backers  penguin books  1994    lewis and berg 1998  b lewis and d berg  multithreaded progrmmning with threads  sun microsystems press  1998    lewis and berg 2000  b lewis and d berg  multithreaded progrmmning with jaw technology  sun microsystems press  2000   933  lichtenberger and pirtle 1965  w w lichtenberger and l'vl w pirtle  'a facility for experimentation in man-machine interaction  proceedings of the aflps fall joirll computer conference  1965   pages 589-598  lindholm and yellin 1999  t lindholm and p yellin  the java virtuallvlachine specificalioll  second edition  addison-wesley  1999    ling et al 20001 y ling  t mullen  and x lin  analysis of optimal thread pool size  opera/ ing st ; slcl/1 review  volume 3'1  nltmbet 2  2000    lipner 1975  s lipne1 ~ 'a comment on the confinement problem  operating st ; stcm review  volume 9  number 5  1975   pages 192-196  lipton 1974  r lipton  on synchronization primitive systems phd thesis  carnegie-mellon university  1974    liskov1972  b h liskov  ''the design of the venus operating system  conunur1.ications of the acm  volume 15  number 3  1972   pages 144-149  liu and layland 1973  c l liu andj w layland  ' schedulingalgorithmsformultiprogramming in a hard real-time environment' '  conmnmicalions of the acm  volume 20  number 1  1973   pages 46-61  lobel1986  j lobel  foiling the system breakers  computer security and access control  mcgraw-hill  1986    loo 2003  a w loo  the future of peer-to-peer computing  comnnmicalions of the acm  volume 46  number 9  2003   pages 56-61  love 2004  r love  linux kernel development  developer 's library  2004    love 2005  r love  linux kemel development  second edition  developer 's library  2005    lowney et al 1993  p g lowney  s m freudenberger  t j i arzes  w d lichtenstein  r p nix  j s o'donnell  and j c ruttenberg  the multiflow trace scheduling compiler' '  journal of supercomputing  volume 7  number 1-2  1993   pages 51-142  ludwig 1998  m ludwig  the giant black book of computer viruses  second edition  american eagle publications  1998    ludwig 2002  m ludwig  the little black book of email viruses  american eagle publications  2002    lumb et al 2000  c lumb  j schindler  g r ganger  d f nagle  and e riedel  towards higher disk head utilization  extracting free bandwidth from busy disk drives  symposium 011 operating systems design and implementation  2000    maekawa 1985  m maekawa  a square root algorithm for mutual exclusion in decentralized systems' '  acm ti ansnctions on computer systems  volume 3  number 2  1985   pages 145-159  mahet et al 1994  c mahet ~ j s goldick  c kerby  and b zumach  the integration of distributed file systems and  viass storage systems '  proceedings of the ieee symposium 011 mass storage systems  1994   pages 27-31  marsh et al 1991  b d marsh  m l scott  t j leblanc  and e p markatos  first-class user-level threads' ' of the 13th acm symposium on operating systems principle  1991   pages 110-121  mattern 1988  f mattern  virtual time and global states of distributed systems  workshop on para/leland distributed algorithms  1988    mattson et al 1970  r l mattson  j gecsei  d r slutz  and i l traiger  evaluation techniques for storage hierarchies  ibm systems journal  volume 9  number 2  1970   pages 78-117  mauro and mcdougall2007  j mauro and r mcdougall  solar is internals  core kernel arcl1itecture  prentice   ! all  2007    mccanne and jacobson 1993  s mccanne and v jacobson  'the bsd packet filter  a new architecture for user-level packet capture  ltsenix winter  1993   pages 259-270  mcdougall and laudon 2006  r mcdougall and j laudon  multi-core processors are here' '  usenjx ; login  the usenjx magazine  volume 31  number 5  2006   pages 32-39  mcdougall and mauro 2007  r mcdougall and   mauro  solaris jntemals  second edition  prentice hall  2007    mcgraw and andrews 1979  j r mcgraw and g r andrews  access control in parallel programs' '  ieee transactions on software volume se-5  number 1  1979   pages 1-9  mckeag and wilson 1976  r m mckeag and r wilson  studies in operating systems  academic press  1976    mckeon 1985  b mckeon  an algorithm for disk caching with limited memory  byte  volume 10  number 9  1985   pages 129-138  mckusick and neville-neil 2005  m k mckusick and g v neville-neil  the design and imple 111cl1talion of l11e frcebsd unix operating system  addison wesley  2005    mckusick et al 1984  l \ 11 k mckusick  w n joy  s   leffler  and r s fabry  a fast file system for unlx  acm transaclions 011 computer st ; slcms  volume 2  number 3  1984   pages 181-197 934  mckusick et al 1996  m k mckusick  k bostic  and m j karels  the design and implementation of the 4.4 bsd unix system  john wiley and sons  1996    mcnairy and bhatia 2005  c mcnairy and r bhatia  montecito  a dual-core  dual-threaded ltanium processor '  ieee micro magazine  volume 25  number 2  2005   pages 10-20  mcvoy and kleiman 1991  l w mcvoy and s r kleiman  extent-like performance from a unix file system  of the winter usenix conference  1991   pages 33-44  mealy et al 1966  g fl mealy  b i witt  and w a clark  the functional structure of os/360  ibm journal  volume 5  number 1  1966    mellor-crummey and scott 1991  j m mellor-crummey and m l scott  algorithms for scalable synchronization on shared-memory multiprocessors  acm transactions on computer systems  volume 9  number i  1991   pages 21-65  menasce and muntz 1979  d menasce and r r muntz  locking and deadlock detection in distributed data bases  ieee transactions on software volume se-5  number 3  1979   pages 195-202  mercer et al 1994  c w merce1 ~ s savage  and h tokuda  processor capacity reserves  operating system support for multimedia applications  international conference on mu ! tim.edia computing and systems  1994   pages 90-99  meyer and seawright 1970  r a meyer and l h seawright  a virtual machine time-sharing system  ibm systems journal  volume 9  number 3  1970   pages 199-218  microsoft 1986  microsoft ms-dos user 's reference and microsoft ms-dos programmer 's reference microsoft press  1986    microsoft 1996  microsoft windows nt workstation resource kit microsoft press  1996    microsoft 2000a  microsoft developer network development library microsoft press  2000    microsoft 2000b  microsoft windows 2000 server resource kit microsoft press  2000    milenkovic 1987  m milenkovic  operating systems  concepts and design  mcgraw-hill  1987    miller and katz 1993  e l miller and r h katz  'an analysis of file migration in a unix supercomputing environment  proceedings of the winter usenix conference  1993   pages 421-434  milojicic et al 2000  d s milojicic  f douglis  y paindaveine  r wheele1 ~ and s zhou  process migration  acm computing survcljs  volume 32  number 3  2000   pages 241-299  mockapetris 1987  p mockapetris  domain names-concepts and facilities  network working group  request for comments  1034  1987    mohan and lindsay 1983  c mohan and b lindsay  efficient c01 = 1it protocols for the tree of processes model of distributed transactions  proceedings of the acm symposium on principles of database systems  1983    mok 1983  a k mok  fundamental design problems of distributed systems for the hard real-time environment phd thesis  massachusetts institute of teclmology  ma  1983    morris 1973  j h morris  protection in programming languages  communications of the acm  volume 16  number 1  1973   pages 15-21  morris and thompson 1979  r morris and k thompson  password security  a case history  communications of the acm  volm11e 22  number 11  1979   pages 594-597  morris et al 1986  j h morris  m satyanarayanan  m h co1mer  j h howard  d s h rosenthal  and f d smith  andrew  a distributed personal computing environment  communications of the acm  volume 29  number 3  1986   pages 184-201  morshedian 1986  d morshedian  how to fight password pirates  computer  volume 19  number 1  1986    motorola 1993  power pc 601 risc microprocessor user 's manual motorola inc  1993    mullender 1993  s mullendej ~ distributed systems  third edition  addison-wesley  1993    myers and beigl2003  b myers and m beigl  handheld cornputing  computer  volume 36  number 9  2003   pages 27-29  navarro et al 2002  j navarro  s lyer  p druschel  and a cox  practical  transparent operating system support for superpages  proceedings of the usenix on operating systems design and implementation  2002    needham and walker 1977  r m needham and r d h walker  the cambridge cap computer and its protection system  proceedings of the sixth symposium on operating system principles  1977   pages 1-10  nelson et al 1988  m nelson  b welch  and j k ousterhout  caching in the sprite network file system  acm transactions on computer systems  volume 6  number 1  1988   pages 134-154  norton and wilton 1988  p norton and r wilton  the new peter norton programmer 's guide to the ibm pc & ps/2  microsoft press  1988   935  nutl2004  g nutt  operating systems  a modem perspective  third edition  addison-wesley  2004    oaks and wong 1999  s oaks and h wong  java threads  second edition  o'reilly & associates  1999    obermarck 1982  r obermarck  ' distributed deadlock detection algorithm  acm transactions on database systerns  volume 7  number 2  1982   pages  87-208  o'leary and kitts 1985  b t o'leary and d l kitts  optical device for a mass storage system  computer  volume 18  nurnber 7  1985    olsen and kenley 1989  r p olsen and g kenley  virtual optical disks solve the on-line storage crunch  computer design  volmtte 28  number 1  1989   pages 93-96  organick 1972  e i organick  the multics system  an examination of its structure  mit press  1972    ortiz 2001  s ortiz  embedded oss gain the inside track  computer  volume 34  number 11  2001    ousterhout 1991  f ousterhout 'the role of distributed state  in cmu computer science  a 25th anniversary commemorative  1991   r f rashid  ed  addison-wesley  1991    ousterhout et al 1985  f k ousterhout  h d costa  d harrison  j a kunze  m kupfe1 ~ and j g thompson  a trace-driven analysis of the unix 4.2 bsd file system  proceedings of the acm symposium on operating systems principles  1985   pages 15-24  ousterhout et al 1988  j k ousterhout  a r cherenson  f doughs  m n nelson  and b b welch  the sprite network-operating system  computer  volume 21  number 2  1988   pages 23-36  parameswaran et al 2001  m parameswaran  a susarla  and a b whinston  p2p networking  an information-sharing alternative  computer  volume 34  number 7  2001    parmelee et al 1972  r p parmelee  t i peterson  c c tillman  and d hatfield  virtual storage and virtual machine concepts '  ibm systems journal  volume 11  number 2  1972   pages 99-130  parnas 1975  d l parnas  on a solution to the cigarette smokers ' problem without conditional statements  communications of the acm  volume 18  number 3  1975   pages 181-183  patil1971  s patil  limitations and capabilities of dijkstra ' s semaphore primitives for coordination among processes  technical report  massachusetts institute of technology  1971    patterson et al 1988  d a patterson  g gibson  and r h katz  a case for redundant arrays of inexpensive disks  raid   proceedings of the acm sigmod international conference on tlze management of data  1988    pease et al 1980  m pease  r shostak  and l lamport  reaching agreement in the presence of faults '  communications of the acm  volume 27  number 2  1980   pages 228-234  pechura and schoeffler 1983  m.a pechura and j d schoeffler  estimating file access time of floppy disks '  communications of the acm  volume 26  number 10  1983   pages 754-763  perlman 1988  r perlman  network layer protocols with byzantine robustness phd thesis  massachusetts institute of technology  1988    peterson 1981  g l peterson  myths aboutthe mutual exclusion problem  information processing letters  volume 12  number 3  1981    peterson and norman 1977  j l peterson and t a norman  buddy systems  communications of the acm  volume 20  number 6  1977   pages 421-431  pfleeger and pfleeger 2003  c pfleeger and s pfleeger  security in computing  third edition  prentice hall  2003    philbin et al 1996  j philbin  j edler  0 j anshus  c c douglas  and k li  ' thread scheduling for cache locality  archit  ectura/ support for programrning languages and operating systems  1996   pages 60-71  pinilla and gill2003  r pinilla and m gill  jvm  platform independent vs performance dependent  operating system review  2003    popek 1974  g j popek  protection structures '  computer  volume 7  number 6  1974   pages 22-33  popek and walker 1985  g popek and b walker  editors  the locus distributed system arclzilecturc  mit press  1985    prieve and fabry 1976  b g prieve and r s fabry  ' vmin-an optimal variable space page-replacement algoritlun  communications of the acm  volume 19  number 5  1976   pages 295-297  psaltis and mok 1995  d psaltis and f mok  holographic memories  scientific american  volume 273  number 5  1995   pages 70-76  purdin et al 1987  t d m purdin  r d schlichting  and g r andrews  a file replication facility for berkeley unix  software-practice and experience  volume 17   1987   pages 923-940 936  purdom  jr and stigler 1970  p w purdorn,jr ands m stigler  statistical propertieso.f the buddy system    acm  volume 17  number 4  1970   pages 683-697  quinlan 1991  s quinlan  a cached worm '  and experience  volume 21  number 12  1991   pages 1289-1299  rago 1993  s raga  unix system v network prograrmning  addison-wesley  1993    rashid 1986  r f rashid  from rig to accent to mach  the evolution of a network operating system  of the acm/ ! eee computer society  fall joint computer conference  1986    rashid and robertson 1981  r rashid and g robertson  accent  a conm1unication-oriented network operating system kernel  proceedings of the acm symposium on operating system  1981    raymond 1999  e s raymond  the cathedral & the bazaar  o'reilly & associates  1999    raynal1986  m raynal  algorithms for mutual exclusion  mit press  1986    raynal1991  m raynal  a simple taxonomy for distributed mutual exclusion algorithms  operating systems review  volume 25  number 1  1991   pages 47-50  raynal and singhal1996  m raynal and m singhal  logical time  capturing causality in distributed systems  computer  volume 29  number 2  1996   pages 49-56  reddy and wyllie 1994  a l n reddy and j c wyllie  1/0 issltes in a multimedia system  computer  volume 27  number 3  1994   pages 69-74  redell and fabry 1974  d d redell and r s fabry  ' selective revocation of capabilities  proceedings of the iria tntemotional workshop on protection in operating systems  1974   pages 197-210  redell eta !  1980  d d redell  y k dalal  t r horsley  h c lauer  w c lynch  p r mcjones  h g murray  and s p purcell  pilot  an operating system for a personal computer '  communimtions of the a.cm  volume 23  number 2  1980   pages 81-92  reed 1983  d p reed  implementing atomic actions on decentralized data  acm transactions on computer volume 1  number 1  1983   pages 3-23  reed and kanodia 1979  d p reed and r k kanodia  synchronization with eventcounts and sequences  communications of !  he acm  volume 22  number 2  1979   pages 115-123  regehr et al 2000  j rcgeh.1 ~ m b jones  and j a stankovic  ' operating system support for multimedia  the programming model matters  teclmical report  microsoft research  2000    reid 1987  b reid  'reflections on some recent widespread computer break-ins  communications of the acm  volume 30  number 2  1987   pages 103-105  ricart and agrawala 1981  g ricart and a k agrawala  'an optimal algoritlun for mutual exclusion in computer networks  com1nunications of the acm  volume 24  number 1  1981   pages 9-17  richards 1990  a e richards  a file system approach for integrating removable media devices and jukeboxes '  optical information systems  volume 10  number 5  1990   pages 270-274  richter 1997  j richter  advanced windows  microsoft press  1997    riedel et al 1998  e riedel  g a gibson  and c faloutsos  active storage for large-scale data mining and multimedia  proceedings of 24th lnternntional conference on very large data bases  1998   pages 62-73  ripeanu et al 2002  m ripeanu  a immnitchi  and 1 foster  ' mapping the gnutella nenqork  ieee internet volume 6  number 1  2002    rivest et al 1978  r l rivest  a shamir  and l adleman  ''on digital signatures and public key cryptosystems  communications of the acm  volume 21  number 2  1978   pages 120-126  robbins and robbins 2003  k robbins and s robbins  unix s1 ; stems programming  conmtunicntion  concurrenct ; and threads  second edition  prentice hall  2003    roberson 2003  j roberson  ' ule  a modern scheduler for freebsd  of tlze usenix bsdcon conference  2003    rodeheffer and schroeder 1991  t l rodeheffer and m d schroedet ~ automatic .reconfiguration in autonet  of ! he acm symposium on opeml ing systenzs principles  1991   pages 183-97  rojas and hashagen 2000  r rojas and u hashagen  the first and architectures  mit press  2000    rosenblum and ousterhout 1991  m rosenblum and j k ousterhout  the design and implementation of a log-structured file system '  proceedings of the acm symposiunt oil opera ling systems principles  1991   pages 1-15  rosenkrantz et al 1978  d j rosenkrantz  r e stearns  and p.m lewis  system level concurrency control for distributed database systems' '  acm transactions on database systems  volume 3  number 2  1978   pages 178-198 937  ruemmler and wilkes 1991  c ruemmler and j wilkes  disk shuffling  technical report  1991    ruemmler and wilkes 1993  c ruemmler a11d j wilkes  unix disk access patterns  proceedin8s oft he wiuler usenjx confereuce  1993   pages 405-120  ruemmler and wilkes 1994  c ruemmler and j wilkes  ''an introduction to disk drive modeling  computer  volttme 27  nttmber 3  1994   pages 17-29 ! rush by 1981  j m rush by  design and verification of secure systems '  proceedings of the acm sr ; mposiwn on opemti11g srjstcujs principles  1981   pages 12-21  rushby and randell1983    rushby and b randell  ' a distributed secure system  computer  volume 16  number 7  1983   pages 55-67  russell and gangemi 1991  d russell and g t gangemi  computer security basics  o'reilly & associates  1991    russinovich and solomon 2005  m e russinovich and d a solomon  microsofl windows internals  fourth edition  microsoft press  2005    saltzer and schroeder 1975  j h saltzer and m d schroeder  the protection of information in computer systems  proceedings of tile ieee  1975   pages 1278-1308  sandberg 1987  r sandberg  the sun network file system  design  implementation and experience  sun microsystems  1987    sandberg et al 1985  r sandberg  d goldberg  s kleiman  d walsh  and b lyon  design and implementation of the sun nelwork filesystem' '  proceedings of the summer usenix conference  1985   pages 119-130  sargent and shoemaker 1995  m sargent and r shoemaker  the personal computer from the inside out  tlzird edition  addison-wesley  1995    sarisky 1983  l sarisky  will removable hard disks replace the floppy  byte  1983   pages 110-117  satyanarayanan 1990  m satyanarayanan  scalable  secure and highly available distributed file access '  computer  volume 23  number 5  1990   pages 9-21  savage et al 2000  s savage  d wetherall  a r karlin  and t anderson  practical network support for ip traceback '  of acm sigcomm conference on data communication  2000   pages 295-306  schell1983  r r schell  a security kernel for a multiprocessor microcomputer  computer  1983   pages 47-53  schindler and gregory 1999  j schindler and g gregory  automated disk drive characterization  technical report  1999    schlichting and schneider 1982  r d schlichting and f b schneider  understanding and using asynchronous message passing primitives  proceedings of the symposium on principles of distributed computing  1982   pages 141-147  schneider 1982  f b schneide1 ~ synchwnization in distributed programs  acm transactions on programming languages and systems  volume 4  number 2  1982   pages 125-148  schneier 1996  b sclmeier  applied cryptography  second edition  john wiley and sons  1996    schrage 1967  l e schrage  the queue m/g/l with feedback to lower priority queues  management science  volume 13   1967   pages 466-474  schwarz and mattern 1994  r schwarz and f mattern  detecting causal relationships i11 distributed computations  in search of the holy grail' '  distributed computing  volume 7  number 3  1994   pages 149-174  seely 1989  d seely  'password cracking  a game of wits  conummicalious of the acm  volume 32  number 6  1989   pages 700-704  seltzer et al 1990  m seltzer  p cl1en  and j ousterhout  disk scheduling revisited  of tlic winter use nix conference  1990   pages 313-323  seltzer et al 1993  m i seltze1 ~ k bostic  m k mckusick  and c staelin  'an implementation of a log-structured file system for unix '  usenfx winter  1993   pages 307-326  seltzer et al 1995  m !  seltzer  i  a smith  h balakrishnan  j chang  s mcmains  and y n padmanabhan  file system logging versus clustering  a performance comparison  usen1x winter  1995   pages 249-264  shrivastava and panzieri 1982  s k shrivastava and f panzieri  the design of a reliable remote procedure call mechanism  ieee transactions on computers  volume c-31  number 7  1982   pages 692-697  siddha et al 2007  s siddha  v pallipadi  and a mallick  process scheduling challenges in the era of multi-core processors  fntd technology journal  volume 11   2007    silberschatz et al 2001  a silberschatz  h f korth  and s sudarshan  database systen1 concepts  fourth edition  mcgraw-hill  2001   938  silverman 1983  j m silverman  reflections on the verification of the security of an operating system kernel  proceedings of the acm symposium on operating systems principles  1983   pages 143-154  silvers 2000  c silvers  ubc  an efficient unified l/0 and memory caching subsystem for netbsd  usenix annual technical conference-freenjx tracie  2000    simmons 1979  g j sim.mons  symmetric and asymmetric encryption  computing surveys  volume 11  number 4  1979   pages 304-330  sincerbox 1994  g t sincerbox  editor  selected papers on holographic storage  optical engineering press  1994    singh 2007  a singh  mac os x internals  a systerns approach  addison-wesley  2007    singhal1989  m singhal  deadlock detection in distributed systems  computer  volume 22  number 11  1989   pages 37-48  sirer et al 1999  e g sirer  r grimm  a j gregory  and b n bershad  design and implementation of a distributed virtual machine for networked computers  symposium on operating systems principles  1999   pages 202-216  smith 1982  a j smith  cache memories  acm computing surveys  volume 14  number 3  1982   pages 473-530  smith 1985  a j smith  disk cache-miss ratio analysis and design considerations  acm transactions on computer systems  volume 3  number 3  1985   pages 161-203  sobti et al 2004  s sobti  n garg  f zheng  j lai  y shao  c zhang  e ziskind  a krishnamurthy  and r wang  segank  a distributed mobile storage system  proceedings of the third usenix conference on file and storage technologies  2004    solomon 1998  d a solomon  inside windows nt  second edition  microsoft press  1998    solomon and russinovich 2000  d a solomon and m e russinovich  inside microsoft windows 2000  third edition  microsoft press  2000    spafford 1989  e h spafford  the internet worm  crisis and aftermath  communications of the acm  volume 32  number 6  1989   pages 678-687  spector and schwarz 1983  a z spector and p m schwarz  transactions  a construct for reliable distributed computing  acm sigops operating systems review  volume 17  number 2  1983   pages 18-35  stallings 2000a  w stallings  local and metropolitan area networks  prentice hall  2000    stallings 2000b  w stallings  operating systems  fourth edition  prentice hall  2000    stallings 2003  w stallings  cryptography and network security  principles and practice  third edition  prentice hall  2003    stankovic 1982  j s stankovic  'softvvare communication mechanisms  procedure calls versus messages  computer  volume 15  number 4  1982    stankovic 1996  j a stankovic  strategic directions in real-time and embedded systems  acm computing surveys  volume 28  number 4  1996   pages 751-763  staunstrup 1982  j staunsh up  message passing communication versus procedure call communication  software-practice and experience  volume 12  nmnber 3  1982   pages 223-234  steinmetz 1995  r steinmetz  analyzing the multimedia operating system  ieee multimedia  volume 2  number 1  1995   pages 68-84  stephenson 1983  c j stephenson  fast fits  a new method for dynamic storage allocation  proceedings of the nintlz symposium on operating systems principles  1983   pages 30-32  stevens 1992  r stevens  advanced programming in the unix environment  addison-wesley  1992    stevens 1994  r stevens  tcp/ip illustrated volume i  the protocols  addison-wesley  1994    stevens 1995  r stevens  tcp/ip illustrated  volume 2  the implementation  addison-wesley  1995    stevens 1997  w r stevens  l  nix network programming-volume i  prentice hall  1997    stevens 1998  w r stevens  unix network programming-volume ii  prentice hall  1998    stevens 1999  w r stevens  unix network programrning inter process communications volume 2  prentice hall  1999    stoica et al 1996  i stoica  h abdei-wahab  k jeffay  s baruah  j gehrke  and g plaxton  a proportional share resource allocation algorithm for real-time  time-shared systems  ieee real-tim.e systems symposium  1996    stokes 2007  j stokes  inside the machine  no starch press  2007    su 1982  z su  a distributed system for internet name service  network working group  request for comments  830  1982    sugerman et al 2001  j sugerman  g venkitachalam  and b lim  virtualizing i/0 devices on vmware workstation 's hosted virtual machine monitor  2001 usenix annual technical conference  2001   939  sun 1990  network programming guide sun microsystems  1990    svobodova 1984  l svobodova  file servers for network-based distributed systems  acm computing surveys  volume 1 6  number 4  1984   pages 353-398  tall uri et al 1995  m tall uri  m.d hill  andy a khalidi  a new page table for 64-bit address spaces  proceedings of the acm symposium on operating systems principles  1995    tamches and miller 1999  a tamches and b p mille ! ~ fine-grained dynamic instrumentation of commodity operating system kernels  usenjx symposium on operating systems design and tmplementation  1999    tanenbaum 1990  a s tanenbaum  structured computer organization  third edition  prentice hall  1990    tanenbaum 2001  a s tanenbaum  modem operating systerns  prentice hah  2001    tanenbaum 2003  a s tanenbaum  computer networks  fourth edition  prentice hall  2003    tanenbaum and van renesse 1985  a s tanenbaum and r van renesse  distributed operating systems  acm computing surveys  volume 17  number 4  1985   pages 419-470  tanenbaum and van steen2002  a tanenbaum and m van steen  distributed systems  principles and paradigms  prentice hall  2002    tanenbaum and woodhull1997  a s tanenbaum and a s woodhull  operating system design and implementation  second edition  prentice hall  1997    tate 2000  s tate  windows 2000 essential reference  new riders  2000    tay and ananda 1990  b h tay and a l ananda  a survey of remote procedure calls  operating systems review  volume 24  number 3  1990   pages 68-79  teorey and pinkerton 1972  t f teorey and t b pinkerton  a comparative analysis of disk scheduling policies  communications of the acm  volume 15  number 3  1972   pages 177-184  tevanian et al 1987a  a tevanian  jr  r f rashid  d b golub  d l black  e cooper  and m w young  mach threads and the unix kernel  the battle for control  proceedings of the summer usenix conference  1987    tevanian et al 1987b  a tevanian  jr  r f rashid  m w yolmg  d b golub  m r thompson  w bolosky  and r sanzi  a unix interface for shared memory and memory mapped files under mach '  teclmical report  carnegie-mellon university  1987    tevanian et al 1989  a teva.nian  jr  and b smith  mach  the model for future unix  byte  1989    thekkath et al 1997  c a thekkath  t maim  ailed e k lee  frangipani  a scalable distributed file system '  symposium on operating systems principles  1997   pages 224-237  thompson 1984  i  thompson  reflections on trusting trust  communications of acm  volume 27  nmnber 8  1984   pages 761-763  thorn 1997  t thorn  'programming languages for mobile code  acm computing surveys  volume 29  number 3  1997   pages 213-239  toigo 2000  j toigo  avoiding a data crunch '  scientific american  volume 282  number 5  2000   pages 58-74  traiger et al 1982  i l traiger  j n gray  c a galtieri  and b g lindsay  transactions and consistency in distributed database management systems '  acm transactions on database systems  volume 7  number 3  1982   pages 323-342  tudor 1995  p n tudor mpeg-2 video compression tutorial  ieee coloquium on mpeg-2 what it is and what it is n't  1995    vahalia 1996  u vahalia  unix internals  the new frontiers  prentice hall  1996    vee and hsu 2000  v vee and w hsu  locality-preserving load-balancing mechanisms for synchronous simulations on shared-memory multiprocessors '  proceedings of the fourteenth workshop 011 parallel and distributed simulation  2000   pages 131-138  venners 1998  b venners  inside the java virtual machine  mcgraw-hill  1998    wah 1984  b w wah  file placement on distributed computer systems  computer  volume 17  number 1  1984   pages 23-32  wahbe et al 1993a  r wahbe  s lucca  t e anderson  and s l graham  ''efficient software based fault isolation  acm sigops operating systems review  volume 27  number 5  1993   pages 203-216  wahbe et al 1993b  r wahbe  s lucco  t e anderson  and s l graham  efficient software based fault lsolation' '  acm sigops operating systems review  volume 27  number 5  1993   pages 203-216  wallach et al 1997  d s wallach  d balfanz  d dean  and e w felten  extensible security architectures for java  proceedings of the acm symposium 011 operating systems principles  1997   940  wilkes et al 1996  j wilkes  r golding  c staelin  and t sullivan  the hp autorald hierarchical storage system  acm transacl ions on computer volume 14  number 1  1996   pages 108-136  williams 2001  wesley  2001   r williams  computer systems architec/.ure-a networ ! cing approach  addison  williams 2002  n williams  an implementation o schedlller activations on the netbsd operating system  2002 usenix annual technical conference  freen ! x track  2002    wilson et al 1995  p r wilson  m s johnstone  m neely  and d boles  dynamic storage allocation  a survey and critical review  proceedings of the jntcnwtional workshop on memory mmwgcment  1995   pages 1-116  wolf 2003  w wolf  a decade of hardware/software codesign  computer  volume 36  number 4  2003   pages 38-43  wood and kochan 1985  p wood and s kochan  unix system security  hayden  1985    woodside 1986  c woodside  controllability of computer performance tradeoffs obtained using controlled-share queue schedulers  ieee transactions on software engineering  volume se-12  number 10  1986   pages 1041-1048  worthington et al 1994  b l worthington  g r ganger  andy n patt  scheduling algoritlm1s for modern disk drives  of the acm conference on measurernent and modeling of computer systems  1994   pages 241-251  worthington et al 1995  b l worthington  g r ganger  y n patt  and j wilkes  on-line extraction of scsi disk drive parameters  proceedings of the acm sigmetrics on measurement and modeling of computer systems  1995   pages 146-156  wulf 1969  w a wul  ' performance monitors for multiprogramming systems '  of the acm symposium on operating systems  1969   pages 175-181  wulf et al 1981  w a wulf  r levin  and s p harbison  an experimental computer systern  mcgraw-hill  1981    yeong et al 1995  w yeong  t howes  and s kille  lightweight directory access protocol  network working group  request for cormnents  1777  1995    young et al 1987  m young  a tevanian  r rashid  d golub  and j eppinge1 ~ 'the duality of memory and communication in the implementation of a multiprocessor operating system  proceedings of the acm symposium on operating principles  1987   pages 63-76  yu et al 2000  x yu  b gum  y chen  r y wang  k li  a krishnamurthy and t e anderson  ' trading capacity for performance in a disk array  proceedings of the 2000 symposium on operating systems and  2000   pages 243-258  zabatta and young 1998  f zabatta and k young  a thread performance comparison  windows nt and solaris on a symmetric multiprocessor  of the 2nd usenix windows nt synzposium  1998    zapata and asokan 2002  m zapata and n asokan  'securing ad hoc routing protocols  proc 2002 acm worlcshop on wireless security  2002   figure 1.11  from hermesy and patterson  computer architecture  a quantitative approach  third edition  2002  morgan kaufmam1 publishers  figure 5.3  p 394 reprinted with permission of the publisher figure 5.13 adapted with permission from sun microsystems  inc figure 9.18  from ibm systems journal  vol 10  no.3  1971  international business machines corporation reprinted by permission of ibm corporation figure 11.9  from leffler/mckusick/karels/quarterman  the design and implementation of the 4.3bsd unix operating system  1989 by addison-wesley publishing co  inc  reading  massachusetts figure 7.6  p 196 reprinted with permission of the publisher figure 13.4  from pentiwn processor user 's manual  architecture and programming manual  volume 3  copyright 1993 reprinted by permission of intel corporation figures 16.6  16.7  and 16.9  from halsall  data communications  computer networks  and open systems  third edition  1992  addison-wesley publishing co  inc  reading  massachusetts figure 1.9  p 14  figure 1.10  p 15  and figure 1.11  p 18 reprinted with permission of the publisher figure 19.5  from i hanna/sebree/zolnowsky  realtime scheduling in sunos 5.0  proceedings of winter usenix  january 1992  san francisco  california derived with permission of the authors figure 23.6  is due to dan murphy  http  / /tenex.opost.com/kapix.html   sections of chapter 6 and chapter 18  from silberschatz/korth  database systern concepts  third edition  copyright 1997  mcgraw-hill  inc  new york  new york section 13.5  p 451-454,14.1.1  p 471-742,14.1.3  p 476-479  14.2  p 482-485  15.2.1  p 512-513  15.4  p 517-518  15.4.3  p 523-524  18.7  p 613-617  18.8  p 617-622 reprinted with permission of the publisher 941 2pc protocol  see two-phase commit protocol lobaset ethernet  681 16-bit windows environment  876 32-bit windows environment  876-877 50-percent rule  327 loobaset ethernet  681 aborted transactions  259 absolute code  318 absolute path names  439 abstract data type  423 access  anonymous  447 controlled  452 file  see file access access control  in linux  841-843 access-control list  acl   452 access latency  540 access lists  nfs v4   720 access matrix  598-602 and access control  605-606 defined  598 implementation of  602-605 and revocation of access rights  606-607 access rights  594  606-607 accounting  operating system service   51 accreditation  664 acl  access-control list   452 active array  limlx   816 active directory  windows xp   892 active list  7 49 acyclic graph  441 acyclic-graph directories  440-443 adaptive mutex  254 additional-reference-bits algorithm  378 additional sense code  575 additional sense-code qualifier  575 address  es   defined  56  internet  686 linear  346 logical  319 physical  319 virtual  319 address binding  318-319 address resolution protocol  arp   699 address space  logical vs physical  319-320 virtual  359  824-825 address-space identifiers  asids   333 administrative complexity  709 admission control  785  793-794 admission-control algorithms  769 advanced encryption standard  aes   641 advanced technology attachment  ata  buses  507 advisory file-locking mechanisms  427 aes  advanced encryption standard   641 affinity  process01 ~ 202-203 aging  193  700 allocation  buddy-system  397 of disk space  471-479 contiguous allocation  471-473 indexed allocation  476-477 linked allocation  473-476 and performance  477-479 equal  383 as problem  432 proportional  383 slab  398-399 analytic evaluation  213 andrew file system  afs   718-723 file operations in  721-722 implementation of  722-723 shared name space in  720-721 anomaly detection  657 anonymous access  447 anonymous memory  521 anonymous pipes  136-137 apcs  see asynchronous procedure calls api  see application program interface apple computers  53 apple macintosh computer  918 appletalk protocol  887 application domain  83 application interface  i/0 systems   565-571 block and character devices  567-568 blocking and nonblocking i/0  570-571 clocks and timers  569-570 network devices  568-569 application layer  692 943 944 application programs  4  6  67 disinfection of  658-660 multistep processing of  318  319 processes vs  24 system utilities  66-67 with virtual machines  78 application program interface  api   56-57 application proxy firewalls  662 arbitrated loop  fc-al   509 architecture  s   12-18 clustered systems  16-18 multiprocessor systems  13-16 single-processor systems  12-13 of windows xp  851 archived to tape  535 areal density  551 argument vector  813 armored viruses  633 arp  address resolution protocol   699 arrays  358 asids  see address-space identifiers assignment edge  287 asymmetric clustering  17 asymmetric encryption  642-643 asymmetric multiprocessing  14  15  202 asynchronous devices  566  567 asynchronous  nonblocking  message passing  122 asynchronous procedure calls  apcs   168  854-855 asynchronous thread cancellation  166 asynchronous writes  485 ata buses  507 atlas operating system  911-912 atomicity  733-736 atomic transactions  232  257-267 and checkpoints  261-262 concurrent  262-267 and locking protocols  264-265 and serializability  262-264 and timestamp-based protocols  265-267 system model for  257-260 write-ahead logging of  260-261 attacks  622-623 see also denial-of-service attacks man-in-the-middle  623 replay  622 zero-day  657 attributes  879 authentication  breaching of  622 and communication protocols  693  694 and encryption  643-645 in linux  841 two-factor  653 in windows  878 automatic job sequencing  907 automatic variables  628 automatic work-set trimming  windows xplt 4-6 automount feature  709 autoprobes  811 auxiliary rights  hydra   607-608 back door  567 background processes  196 backing store  322 backups  489 bad blocks  519-520 bandwidth  disk  511 effective  539-540 sustained  539 banker 's algorithm  298 base file record  879 base register  316  317 basic file systems  462 batch files  427 batch interface  50 bayes ' theorem  658 belady 's anomaly  374 beowulf clusters  17 best-fit strategy  327 biased protocol  738 binary semaphore  234 binding  318 biometrics  653-654 bit  s   defined  6 mode  21 nl.odify  dirty   371 reference  378 valid-invalid  335-336 bit-interleaved parity organization  526 bit-level striping  524 bit vector  bit maplt 479 black-box transformations  641 blade servers  16 block  s   58  326  430 bad  519-520 boot  90  517-518 boot control  464 defined  835 direct  477 file-control  463 index  476 index to  432-433 indirect  477 logical  508 volume control  464 block ciphers  641 block devices  566-568  835 block groups  831 blocking  indefinite  193 blocking i/0  570-571 blocking  synchronous  message passing  122 block-interleaved distributed parity  527 block-interleaved parity organization  526-527 block-level striping  524 block number  relative  432 boot block  90  464  517-518 boot control block  464 boot disk  system diskt 90  518 booting  89-90  874 boot partition  518 boot sector  518 bootstrap programs  bootstrap loaders   8  89-90  517-518  634 boot viruses  631 bottlenecks  84 bottom half interrupt service routines  819 bounded-buffer problem  240 bounded capacity  of queue   122-123 bourne shell command interpreter  53 breach of availability  622 breach of confidentiality  622 breach of integrity  622 broadcasting  699  790 bsd unix  39-40 b1 tree  ntfs   879-880 buddy heap  linux   821 buddy system  linuxlt 821 buddy-system allocation  397 buffer  835 circular  488 defined  572 buffer cache  484 buffering  122-123  572-574  793 buffer-overflow attacks  627-630 bugs  84 bully algorithm  748-749 bus  507 defined  556 expansion  556 pci  556 bus architecture  12 bus-mastering i/0 boards  563 busy waiting  235  559 byte  6 bytecode  82 byzantine generals problem  750 cache  483-484 buffer  484 defined  574 in linux  822 as memory buffer  317 nonvolatile ram  524 page  484 and performance improvement  483-484 and remote file access  and consistency  713-714 location of cache  711-712 update policy  712  713 slabs in  398 unified buffer  484-485 in windows xp  870-872 cache coherency  29 cache-consistency problem  711 cachefs file system  712 cache management 27 caching  27-29  574 client-side  890 double  484 remote service vs  713-714 write-back  712 callbacks  721 cambridge cap system  609-610 cancellation  thread  166-167 cancellation points  167 capability  -ies   603  609 945 capability-based protection systems  607-610 cambridge cap system  609-610 hydra  607-609 capability lists  603 carrier sense with multiple access  csma   690 cascading termination  116 cav  constant angular velocity   508 cd  see collision detection central processing unit  see under cpu certificate authorities  646 certification  664 challenging  passwords   652 change journal  windows xp   885 character devices  linux   836-837 character-stream devices  566-568 checkpoints  261-262 checksums  531  700 child processes  859 children  110 cifs  common internet file system   449 cineblitz  792-794 cipher-block chaining  641 circuit switching  689 circular buffer  488 circular scan  c-scan  scheduling algorithm  514 circular-wait condition  deadlocks   292-294 claim edge  296-297 classes  java   613 946 class loader  82 cli  dtrace command-line interface   50 c library  62 client  s   defined  706 diskless  708 in ssl  648 client interface  706 client-server model  447-448 client-side caching  csc   890 client systems  35-36 clocks  569-570  729 clock algorithm  see second-chance page-replacement algorithm clones  497 c-look scheduling algorithm  515 closed-source operating systems  37 close   operation  424 clusters  17  517  696-697  878 clustered page tables  340 clustered systems  16-18 clustering  696-697 asymmetric  17 in windows xp  405 cluster remapping  884 cluster server  719 clv  constant linear velocity   508 coarse-grained multithreading  205 code  absolute  318 reentrant  336 code books  653 collisions  of file names   470 collision detection  cd   690 com  see component object model combined scheme index block  477 command interprete1 ~ 52-53 commit protocol  733 committed transactions  259 common internet file system  cifs   449 communication  s   direct  120 i11 distributed operating systems  675 indirect  121 interprocess  see interprocess communication systems programs for  66-67 unreliable  751 communications  operating system service   51 communication links  120 communication processors  681 communications sessions  688 communication system calls  65-66 compaction  327-328  472-473 compiler-based enforcement  610-613 compile time  318 complexity  administrative  709 component object model  com   889 component units  706 compression  in multimedia systems  782-784 in windows xp  884-885 compression ratio  782 compression units  884 computation migration  678-679 computation speedup  674 compute clusters  697 computer environments  34-37 client-server computing  35-36 peer-to-peer computing  36 traditional  34-35 web-based computing  37 computer programs  see application programs computer system  s   architecture of  clustered systems  16-18 multiprocessor systems  13-16 single-processor systems  12-13 distributed systems  30-31 file-system management in  25-26 i/0 structure in  11-13 memory management in  24-25 operating system viewed by  5 operation of  7-9 process management in  23-24 protection in  29-30 secure  622 security in  29-30 special-purpose systems  32-34 handheld systems  33-34 multimedia systems  33 real-tim.e embedded systems  32-33 storage in  9-11 storage management in  25-29 caching  27-29 i/0 systems  29 mass-storage management  26-27 threats to  633-638 computing  safe  660 concurrency control  736-740 with locking protocols  736-739 with timestamping  739-740 concurrency-control algorithms  262 conditional-wait construct  251 confidentiality  breach of  622 confinement problem  602 conflicting operations  263 conflict phase  of dispatch latency   767 conflict resolution module  linux   811-812 connectionless messages  688 connectionless  udp  sockets  129 connection-oriented  tcp  sockets  129 conservative timestamp-ordering scheme  740 consistency  713-714 consistency checker  487 consistency checking  486-487 consistency semantics  450 consolidation  system  78 constant angular velocity  cay   508 constant linear velocity  clv   508 consumers  dtrace   86 container objects  windows xp   665 containers  solaris 10   79 contention  689-690 contention scope  199-200 context  of process   110 context switches  110  582-583 contiguous disk space allocation  471-473 contiguous memory allocation  325 continuous-media data  780 control cards  59  907 control-card interpreter  908 controlled access  452 controller  s   507  556-557 defined  556 direct-memory-access  563 disk  507 host  507 control programs  5 control register  558 convenience  3 convoy effect  189 cooperating processes  116 cooperative scheduling  186 copylefting  38 copy-on-write technique  367-369 copy protection  38 copy semantics  573 core dump  84 core memory  911 cores  15-16 counting  481 counting-based page replacement algorithm  380 counting semaphore  234 covert channels  626 cp/m  917 cpu  central processing unit   4  315-317 cpu-bound processes  109 cpu burst  184 cpu clock  316 cpu-i/o burst cycle  184-185 cpu schedule1 ~ see short-term scheduler cpu scheduling  20 a bout  183-184 algorithms fm ~ 187-199 criteria  187-188 evaluation of  213-217 947 first-come  first-served scheduling of  188-189 implementation of  217 multilevel feedback-queue scheduling of  198-199 multilevel queue scheduling of  196-197 priority scheduling of  192-193 round-robin scheduling of  194-196 shortest-job-first scheduling of  189-192 dispatcher  role of  187 and i/o-cpu burst cycle  184-185 models for  213-217 deterministic modeling  213-215 and implementation  217 queueing-network analysis  215 simulations  216 in multimedia systems  786-787 multiprocessor scheduling  200-206 approaches to  202 and load balancing  204 and processor affinity  202-203 preemptive scheduling  185-186 in real-time systems  768-774 earliest-deadline-first schedufu1g  771 proportional share scheduling  772 pthread scheduling  772-774 rate-monotonic scheduling  769-771 short-term schedule1 ~ role of  185 crackers  622 crashes  84 crash dumps  84 creation  of files  423 process  110-115 critical sections  227 critical-section problem  227-229 peterson 's solution to  229-230 and semaphores  234-239 deadlocks  238 implementation  235-238 priority inversion  238-239 starvation  238 usage  234-235 and synchronization hardware  231-234 cross-link trust  891 cryptography  638-639 and encryption  639-646 implementation of  646-647 ssl example of  647-649 esc  client-side caching   890 c-scan scheduling algorithm  514 csma  see carrier sense with multiple access ctss operating system  914-915 current directory  439 948 current-file-position pointer  423 cycles  in cineblitz  793 cpu-i/o burst  184-185 cycle stealing  564 cylinder groups  831 d  page offset   329 daemon process  596 daisy chain  556 data  multimedia  33 recovery of  486-490 tluead-specific  170 database systems  257 data capability  609 data-encryption standard  des   641 data files  422 data fork  430 datagrams  688 data-in register  558 data-link layer  691 data loss  mean time to  523 data migration  677-678 data-out register  558 data section  of process   102 data striping  524 dcom  889 ddos attacks  622 deadline i/0 scheduler  836 deadlock  s   238  740-747 avoidance of  290  294-300 with banker 's algorithm  298 with resource-allocation-graph algorithm  296-297 with safe-state algorithm  295-296 defined  283 detection of  301-304  742-747 algorithm usage  303-304 several instances of a resource type  301-303 single instance of each resource type  301 methods for handling  290-291 with mutex locks  285-286 necessary conditions 01 ~ 285-287 prevention/avoidance of  740-742 prevention of  290-294 and circular-wait condition  292-294 and hold-and-wait condition  291-292 and mutual-exclusion condition  291 and no-preemption condition  292 recovery from  304-306 by process termination  304-305 by resource preemption  305-306 system model for  283-285 system resource-allocation graphs for describing  287-289 deadlock-detection coordinator  743 debuggers  59  84 debugging  65  84-88 defined  84 failure analysis  84 and performance tuning  84-85 using dtrace for  85-88 dedicated devices  566  567 default signal handlers  167 deferred procedure calls  dpcs   854-855 deferred thread cancellation  167 degree of multiprogramming  108 delay  785 delay-write policy  712 delegation  nfs v4   717 deletion  file  423 demand paging  361-367 basic mechanism  362-364 defined  361 with inverted page tables  402 and i/0 interlock  404-405 and page size  400-401 and performance  365-367 and prepagil g  399-400 and program structure  402-403 pure  364 and restartil1g instructions  364-365 and tlb reach  401-402 demand-zero memory  824 demilitarized zone  dmz   661 denial-of-service  dos  attacks  622  638 density  areal  551 dentry objects  469  829 des  data-encryption standard   641 design of operating systems  distributed operating systems  697-699 goals  68 lilmx  805-808 mechanisms and policies  68-69 windows xp  849-851 desktop  53 deterministic modeling  213-215 development kernels  linux   803 device controllers  7  579 see also i/0 system  s  device directory  434 see also directories device drivers  12  462  556  579  908 device-management system calls  64 device queues  107-108 device reservation  574-575 dfs  see distributed file system digital certificates  645-646 digital rights management  drm   38 digital signatures  644 digital-signature algorithm  644 dining-philosophers problem  242-244  248-249 direct access  files   431-432 direct blocks  477 direct communication  120 direct i/0  568 direct memory access  dma   12  563-564 direct-memory-access  dma  controlle1 ~ 563 directories  434-444 acyclic-graph  440-443 general graph  443-444 implementation of  470-471 recovery of  486-490 single-level  436-437 tree-sh uctured  438-440 two-level  437-438 directory objects  windows xp   858 direct virtual memory access  dvma   564 dirty bits  modify bits   371 disinfection  program  658-660 disk  s   505-507 see also mass-storage structure allocation of space on  471-479 contiguous allocation  471-473 indexed allocation  476-477 linked allocation  473-476 and performance  477-479 bad blocks  519-520 boot  90  518 boot block  517-518 efficient use of  482-483 electronic  11 floppy  506-507 formatting  516-517 free-space management for  479-481 host-attached  509 low-level formatted  508 magnetic  10 magneto-optic  534 mini  434 network-attached  509-510 performance improvement for  483-486 phase-change  535 raw  381  433  467 read-only  535 read-write  535 removable  534-535 scheduling algorithms  510-516 c-scan  514 fcfs  511-512 look  515 scan  513-514 selecting  515-516 sstf  512-513 solid-state  28 storage-area network  510 structure of  508 system  518 worm  535 disk ann  506 disk controller  507 diskless clients  708 disk mirroring  883-884 disk scheduling  cineblitz  793 in multimedia systems  787-789 disk striping  882 dispatched process  107 dispatcher  187 dispatcher objects  255 windows xp  854 in windows xp  857 dispatch latency  187  767 distributed coordination  and atomicity  733-736 and concurrency control  736-740 and deadlocks  740-747 detection  742-747 prevention/ avoidance  740-7 42 election algorithms for  747-750 and event ordering  727-730 and mutual exclusion  730-732 reaching algorithncs for  750-752 949 distributed denial-of-service  ddos  attacks  622 distributed file systems  dfss   447  705-706 afs example of  718-723 file operations  721-722 implementation  722-723 shared name space  720-721 defined  705 naming in  707-710 remote file access in  710-715 basic scheme for  711 and cache location  711-712 distributed file systems  dfss   contd  and cache-update policy  712  713 and caching vs remote service  714-715 and consistency  713-714 replication of files in  716-718 stateful vs stateless service in  715-716 stateless  449-450 windows xp  890 distributed information systems  distributed naming services   448 distributed lock manager  dlm   17 950 distributed naming services  see distributed information systems distributed operating systems  677-679 distributed-processing mechanisms  888-889 distributed systems  30-31 benefits of  673-675 defined  673 distributed operating systems as  677-679 network operating systems as  675-677 distributions  gnu/linux   38 dlls  see dynamic link libraries dlm  distributed lock manager   17 dma  see direct memory access dma controller  see direct-memory-access controller dmca  u.s digital millennium copyright act   38 dmz  demilitarized zone   661 domains  449  891-892 domain-name system  dns   448  685 domain switching  595 domain trees  891 dos attacks  see denial-of-service attacks double buffering  573  793-794 double caching  484 double indirect blocks  477 downsizing  675 down time  473 dpcs  deferred procedure calls   854-855 dram  dynamic random-access memory   9 driver end  stream   580-581 driver registration module  linux   810-811 drm  digital rights management   38 dtrace  85-88 dtrace command-line interface  cli   50 dual-booted systems  467 dual-core design  16 dumpster diving  624 duplex set  884 dvma  direct virtual memory access   564 dynamic linking  828 dynamic link libraries  dlls   321-322  851 dynamic loading  320-321 dynamic priority  787 dynamic protection  594 dynamic random-access memory  dram   9 dynamic routing  687 dynamic storage-allocation problem  326  472 earliest-deadline-first  edf  scheduling  771  787-788 ease of use  4 ease of use features  848 ecbs  enabling control blocks   87 ecc  see error-correcting code edf scheduling  see earliest-deadline-first scheduling effective access time  365 effective bandwidth  539-540 effective memory-access lime  334 effective uid  30 efficiency  3  482-483 eide buses  507 election  690 election algorithms  747-750 electronic disk  11 elevator algorithm  see scan scheduling algorithm embedded systems  760 emulation  78-79 emulators  69 enabling control blocks  ecbs   87 encapsulation  java   615 encoded files  782 encrypted passwords  651-652 encrypted viruses  632 encryption  639-646 asymmetric  642-643 authentication  643-645 key distribution  645-646 symmetric  641-642 windows xp  884-885 enhanced integrated drive electronics  eide  buses  507 entry section  227 entry set  253 environmental subsystems  850-851 environment vector  813 eprom  erasable programmable read-only memory   90 equal allocation  383 erasable programmable read-only mem01y  eprom   90 error  s   575 hard  520 soft  517 error conditions  358 error-correcting code  ecc   515-516  525-526 error detection  51 escalate privileges  30 escape  operating systems   567 events  255 event latency  765-766 event objects  windows xp   854 event ordering  727-730 exceptions  with interrupts   561 exclusive lock mode  736 exclusive locks  426 exec   system call  165-166 executable files  102  422 execution of user programs  826-827 execution time  318 exit section  227 expansion bus  556 expired array  linux   816 expired tasks  linux   816 exponential average  191 export list  493-494 ext2fs  see second extended file system extended file system  463  830 extent  contiguous space   473 extents  879 external data representation  xdr   132 external fragmentation  327-328  472 failure  detection of  694-695 mean time to  523 recovery from  696 during writing of block  533-534 failure analysis  84 failure handling  2pc protocol   734-736 failure modes  directories   449-450 false negatives  657 false positives  657 fast i/0 mechanism  871 fat  file-allocation table   475-476 fault tolerance  14  696-697  881-884 fault-tolerant systems  696-697 fc  fiber channel   509 fc-al  arbitrated loop   509 fcb  file-control block   463 fc buses  507 fcfs scheduling algorithm  see first-come  first-served scheduling algorithm feature migration  903-904 fibers  896-897 fiber channel  fc   509 fiber channel  fc  buses  507 fids  nfs v4   720 fifos  139 fifo page replacement algorithm  373-375 50-percent rule  327 file  s   25-26  421-422 see also directories accessing information on  430-433 direct access  431-432 sequential access  431 attributes of  422-423 batch  427 defined  422 executable  102 internal structure of  429-430 locking open  425-427 operations on  l123-427 protecting  451-456 via file access  451-456 via passwords/permissions  455-456 recovery of  486-490 storage structure for  434-435 file access  425  451-456 file-allocation table  fat   475-476 file-control block  fcb   463 file descriptor  466 file extensions  427-428 file handle  466 filelock  java   425 file management  67 file-management system calls  64 file mapping  393 file migration  707 file modification  67 file objects  469  829 file-organization module  463 file pointers  425 file reference  879 file replication  distributed file systems   716-718 file session  450 file sharing  446-451 and consistency semantics  450-451 with multiple users  446-447 with networks  448-450 and client-server model  447-448 951 and distributed information systems  448-449 and failure modes  449-450 file systems  421  461-463 basic  462 creation of  436 design problems with  462 distributed  see distributed file systems extended  462 imple1nentation of  464-470 mounting  467-468 partitions  467-468 virh1al systems  468-470 levels of  462 linux  828-834 log-based transaction-oriented  487-488 logical  462 mounting of  444-446 network  490-496 remote  447 wafl  496-498 file system hierarchy standard document  804 file-system management  25-26 952 file-system manipulation  operating system service   50 file transfe1 ~ 676-677 file transfer protocol  ftp   447  676-677 file viruses  631 filter drivers  869-870 fine-grained multithreading  205 firewalls  35  661-662 firewall chains  840 firewall management  840 firewire  508 firmware  8  90 first-come  first-served  fcfs  scheduling algorithm  188-189  511-512 first-fit strategy  326 fixed-partition scheme  324 fixed routing  687 floppy disks  506-507 flow control  581 flushing  333 folders  53 footprint  761 foreground processes  196 forests  891 fork   and exec   process model  linux   812-814 fork   system call  165-166 formatting  516-517 forwarding  519 forward-mapped page tables  338 fragments  packet  840 fragmentation  327-328 external  327-328  472 internal  327  430 frame  s   329  688  780 stack  628-629 victim  371 frame allocation  382-385 equal allocation  383 global vs local  384-385 proportional allocation  383-384 frame-allocation algorithm  372 frame pointers  628-629 free-behind technique  485-486 free objects  398  822 free software foundation  fsf   38 free-space list  479 free-space management  disks   479-482 bit vectm ~ 479-480 counting  481 grouping  480-481 linked list  480 and space maps  481-482 front-end processors  584 fsf  free software foundation   38 ftp  see file transfer protocol full backup  489 fully distributed deadlock-detection algorithm  745-747 fuse file-system  463 gantt chart  189 garbage collection  82  444 gateways  688 gb  gigabyte   6 gee  gnu c compiler   804 gdt  global descriptor table   346 general graph directories  443-444 gigabyte  gb   6 global descriptor table  gdt   346 global ordering  729 global replacement  384 gnu c compiler  gee   804 gnu general public license  gpl   38 gnu/linux  38-39 gnu portable threads  158 gpl  gnu general public license   38 graceful degradation  14 graphs  acyclic  441 graphical user interfaces  guis   52-54 grappling hook  634 green threads  158 group identifiers  30 grouping  480-481 group policies  892 group rights  linux   842 guest  76 guest operating systems  81 guis  see graphical user interfaces hal  see hardware-abstraction layer handheld computers  5 handheld systems  33-34 handles  857  860 handshaking  559  578 hands-on computer systems  see interactive computer systems happened-before relation  728-729 hard affinity  202 hard-coding techniques  120-121 hard errors  520 hard links  443 hard real-time systems  760  786 hardware  4 l/0 systems  556-565 direct memory access  563-sm interrupts  560-563 polling  559 for storing page tables  332-334 synchronization  231-234 for virtualization  80 hardware-abstraction layer  hal   851  852 hardware objects  593 hashed page tables  340 hash functions  644 hash tables  470-471 hash value  message digest   644 heaps  102  899 heavyweight processes  153 hierarchical paging  337-340 hierarchical storage management  hsm   539 high availability  16 high-availability clusters  697 high performance  850 high-performance computing  17 hijacking  session  623 hit ratio  334  401 hive  873-874 hold-and-wait condition  deadlocks   291-292 holes  326-327 holographic storage  536 homogeneity  202 host  76 host adapter  557 host-attached storage  509 host controller  507 host-id  685 hot spare disks  529 hot-standby mode  17 hsm  hierarchical storage management   539 human security  624 hydra  607-609 hyperspace  861 ibm os/360  915-917 identifiers  file  422 group  30 use1 ~ 30 idle threads  209 idss  see intrusion-detection systems ike protocol  647 ilm  information life-cycle management   539 immutable shared files  451 implementation  of cpu scheduling algorithms  217 of operating systems  69-70 of real-time operating systems  764-768 and minimizing latency  765-768 and preemptive kernels  765 and priority-based scheduling  764-765 of transparent naming techniques  709-710 of virtual machines  80 incremental backup  489 indefinite blocking  starvation   193  238 independence  location  707 independent disks  523 independent processes  116 index  432 index block  476 indexed disk space allocation  476-477 index root  879-880 indirect blocks  477 indirect communication  121 information life-cycle management  ilm   539 information-maintenance system calls  65 inode  463 inode objects  469  829 input/output  see under i/0 input queue  318 inserv storage array  530 instance handles  895 instruction-execution cycle  9-10  315-316 instruction-execution unit  875 instruction register  9 integrity  breach of  622 intellimirrm ~ 892 intel pentium processor  345-348 interactive  hands-on  computer systems  19 interface  s   batch  50 client  706 defined,565 intermachine  706 windows xp networking  886 interlock  i/0  404-405 intermachine interface  706 internal fragmentation  327  430 international use  851 internet address  686 internet protocol  ip   646-647 interprocess communication  ipc   116-123 in client-server systems  128-140 remote procedure calls  131-134 sockets  128-130 in linux  803  837-838 953 954 interprocess communication  ipc   contd  mach example of  124-126 in message-passing systems  119-120 posix shared-memory example of  123-124 in shared-memory systems  117-119 windows xp example of  127-128 interrupt  s   8-9  560-563 defined  560 in linux  818-819 interrupt chaining  561 interrupt-controller hardware  561 interrupt-dispatch table  windows xp   856 interrupt-driven data transfe1 ~ 396 interrupt-driven operating systems  20-23 interrupt-handler routine  560 interrupt latency  766-767 interrupt priority levels  561 interrupt-request line  560 interrupt vector  8-9  324  561 intruders  622 intrusion detection  656-658 intrusion-detection systems  idss   656-658 intrusion-prevention systems  ipss   656 inverted page tables  340-342  402 i/0  input/output   4  11-13 memory-mapped  395-396 overlapped  909-911 programmed  396 i/o-bound processes  109 i/0 burst  184 i/0 channel  584 i/0 interlock  404-405 1/0 manager  869 i/0 operations  operating system service   50-51 i/0 ports  396 i/0 request packet  irp   869 i/0 subsystem  s   29 kernels in  571-578 procedures supervised by  577-578 i/0 system  s   555-556 application interface  565-571 block and character devices  567-568 blocking and nonblocking i/0  570-571 clocks and timers  569-570 network devices  568-569 hardware  556-565 direct memory access  563-564 interrupts  560-563 polling  559 kernels  571-578 buffering  572-574 caching  574 data structures  576-577 error handling  575 i/0 scheduling  571-572 and l/0 subsystems  577-578 protection  575-576 spooling and device reservation  574-575 linux  834-837 block devices  835-836 character devices  836-837 streams mechanism  580-582 and system performance  582-585 transformation of requests to hardware operations  578-580 ip  see internet protocol ipc  see interprocess communication ipsec  647 ipss  intrusion-prevention systems   656 irp  i/0 request packet   869 iscsi  510 iso protocol stack  692-693 iso reference model  647 java  file locking in  425--426 language-based protection in  613-615 monitors in  253 java threads  162  164-165 java virtual machine  jvm   82  84 jit compiler  82  84 jitter  785 jobs  processes vs  102-103 job objects  866-867 job pool  20 job queues  105 job scheduler  108 job scheduling  20 journaling  832-833 journaling file systems  see log-based transaction-oriented file systems just-in-time  jit  compiler  82  84 jvm  java virtual machine   82  84 kb  kilobyte   6 kerberos  878 kernel  s   6  571-578 buffering  572-574 caching  574 data structures  576-577 error handling  575 i/0 scheduling  571-572 and i/0 subsystems  577-578 linux  807  808 multimedia systems  784-786 nonpreemptive  228-229 preemptive  228-229  765 protection  575-576 real-time  762-764 spooling and device reservation  574-575 task synchronization  in linux   817-819 windows xp  852-856  893 kernel extensions  76 kernel memory allocation  396-399 kernel mode  21  807 kernel modules  809-812 conflict resolution  811-812 driver registration  810-811 management of  809-810 kernel threads  157 kernighan 's law  85 kerr effect  534 keys  369  604  607 private  642 public  642 key distribution  645-646 key ring  645 keystreams  642 keystroke logger  633 kilobyte  kb   6 language-based protection systems  610-615 compiler-based enforcement  610-613 java  613-615 lans  see local-area networks latency  in real-time systems  765-768 layers  of network protocols   646 layered approach  operating system structure   71-73 lazy swapper  361 lcns  logical cluster numbers   879 ldap  see lightweight directory-access protocol ldt  local descriptor table   346 least-frequently used  lfu  page-replacement algorithm  380 least privilege  principle of  592-593 least-recently-used  lru  page-replacement algorithm  376-378 levels  783 lfu page-replacement algorithm  380 lgroups  385 libraries  linux system  807  808 shared  322  360 licenses  software  272 lightweight directory-access protocol  ldap   449  892 limit register  316  317 linear addresses  346 linear lists  files   470 line discipline  836 link  s   communication  120 defined  441 hard  443 resolving  441 symbolic  858 linked disk space allocation  473-476 linked lists  480 linked scheme index block  477 linking  dynamic vs static  321-322  828 linux  38-40  801-843 adding system call to linux kernel  project   95-97 design principles for  805-808 file systems  828-834 ext2fs  830-832 journaling  832-833 process  833 virtual  829-830 history of  801-806 distributions  804-805 first kernel  802-804 licensing  805-806 system description  804 interprocess communication  837-838 i/0 system  834-837 block devices  835-836 character devices  836-837 kernel modules  809-812 memory management  820-828 955 execution and loading of user programs  826-827 physical memory  820-823 virtual memory  823-826 network structure  838-840 on pentium systems  348-349 process management  812-815 fork   and exec   process model  812-814 processes and threads  814-815 process representation in  106 real-time  775 scheduling  815-820 kernel synchronization  817-819 process  815-817 symmetric multiprocessing  819-820 scheduling example  211-213 security model  840-843 access control  841-843 authentication  841 swap-space rnanagernent in  522 synchronization in  256-257 threads example  173-174 linux distributions  802  804-805 956 linux kernel  802-804 limlx system  components of  802  807-808 lists  358 little 's formula  215 livecd  38-39 livedvd  39 live streaming  781 load balancers  37 load balancing  203-204 loade1 ~ 908 loading  dynamic  320-321 in linux  826-828 load sharing  200  674 load time  318 local-area networks  lans   16  31  679-681 local descriptor table  ldt   346 locality model  387 locality of reference  364 local name space  719 local playback  780 local procedure calls  lpcs   850  868-869 local replacement  384 local replacement algorithm  priority replacement algorithm   386 location  file  422 location independence  707 location-independent file identifiers  710 location transparency  707 lock  s   231  604 acquire  61-62 advisory  427 exclusive  426 in java api  425-426 mandatory  427 mutex  235 reader-writer  241-242 release  61-62 shared  426 locking protocols  264-265  736-739 lock-key scheme  604 lock   operation  425 log-based transaction-oriented file systems  487-488 log files  84 log-file service  881 logging  write-ahead  260-261 logging area  881 logical address  319 logical address space  319-320 logical blocks  508 logical clock  729 logical cluster numbers  lcns   879 logical file system  463 logical formatting  517 logical memory  20  359 see also virtual memory logical records  431 logical units  509 login  network  449 long-term scheduler  job scheduler   108 look scheduling algorithm  515 loopback  130 lossless compression  782-783 lossy compression  782-783 low-level formatted disks  508 low-level formatting  disks   516 lpcs  see local procedure calls lru-approximation page replacement algorithm  378-380 mac  message-authentication code   644 mac  medium access control  address  699 mach operating system  73-74  124-126  919-920 macintosh operating system  429-430  918-919 macro viruses  631 magic number  files   429 magnetic disk  s   10  505-507 see also disk  s  magnetic tapes  507  535-536 magneto-optic disks  534 mailboxes  121 mailbox sets  126 mailslots  888 mainframes  5 main memory  9-10 and address binding  318-319 contiguous allocation of  324-325 and fragmentation  327-328 mapping  325 methods  325-327 protection  325 and dynamic linking  321-322 and dynamic loading  320-321 and hardware  316-318 intel pentium example  with linux  348-349 paging  346-348 segmentation  345-348 and logical vs physical address space  319-320 paging for nwnagement of  328-341 basic method  329-332 hardware  332-334 hashed page tables  340 hierarchical paging  337-340 intel pentium example  346-348 inverted page tables  340-342 protection  335-336 and shared pages  336-337 segmentation for management of  342-345 basic method  3ll2-345 hardware  344-345 intel pentium example  345-348 and swapping  322-324 majority protocol  737-738 mans  metropolitan-area networks   31 mandatory file-locking mechanisms  427 man-in-the-middle attack  623 many-to-many multithreading model  158-159 many-to-one multithreading model  157-158 marshalling  889 mars pathfinder  239 maskable interrupts  561 masquerading  622 mass-storage management  26-27 mass-storage structure  505-508 disk attachment  host-attached  509 network-attached  509-510 storage-area network  510 disk management  bad blocks  519-520 boot block  517-518 formatting of disks  516-517 disk scheduling algorithms  510-516 c-scan  514 fcfs  511-512 look  515 scan  513-514 selecting  515-516 sstf  512-513 disk structure  508 extensions  530 magnetic disks  505-507 magnetic tapes  507 raid structure  522-532 performance improvement  524 problems with  531-532 raid levels  524-529 reliability improvement  522-524 stable-storage implementation  533-534 swap-space management  520-522 tertiary-storage  534-543 future tedmology for  536-537 magnetic tapes  535-536 and operating system support  536-538 performance issues with  539-543 removable disks  534-535 master book record  mbr   518 master file directory  mfd   437 master file table  464 master key  607 master secret  ssl   648 matchmakers  133 matrix product  178 mb  megabyte   6 mbr  master book record   518 mcp operating system  920-921 mean time to data loss  523 mean time to failure  523 mean time to repair  523 mechanisms  68-69 media players  790 medium access control  mac  address  699 medium-term schedule1 ~ 109 megabyte  mb   6 memory  anonymous  521 core  911 direct memory access  12 direct virtual memory access  564 logical  20  359 ncain  see main memory over-allocation of  369 physical  20 secondary  364 semiconductor  10 shared  116  360 unified virtual memory  484 virtual  see virtual memory memory-address register  319 memory allocation  325-327 memory management  24-25 in linux  820-828 execution and loading of user programs  826-828 physical memory  820-823 virtual memory  823-826 in windows xp  898-899 heaps  899 memory-mapping files  898 thread-local storage  899 virtual memory  898 957 memory-management unit  mmu   319-320  862-863 memory-mapped files  862 memory-mapped i/0  395-396  557 memory mapping  325  390-396 basic mechanism  391-393 defined  390 i/0  memory-mapped  395-396 in linux  827-828 in win32 api  393-395 memory-mapping files  898 memory protection  325 958 memory-resident pages  362 memory stall  204 memory-style error-correcting organization  525-526 memory transactions  258 mems  micro-electronic mechanical systems   536 messages  connectionless  688 in distributed operating systencs  675 message-authentication code  mac   644 message digest  hash value   644 message modification  622 message passing  116  148-152 message-passing model  65  119-120 message queue  914 message switching  689 metadata  449  880 metafiles  790 metaslabs  482 methods  java   613 metropolitan-area networks  mans   31 mfd  master file directory   437 mfu page-replacement algorithm  380 micro-electronic mechanical systems  mems   536 microkernels  73-75 microsoft interface definition language  889 microsoft windows  324  918-919 see also under windows migration  computation  678-679 data  677-678 file  707 process  679 minicomputers  5 minidisks  434 miniport driver  870 mirroring  523 mirror set  883 mmu  see memory-management unit mobility  user  492 mode bit  21 modify bits  dirty bits   371 modules  74-76  580-581 monitors  244-252 dining-philosophers solution using  248-249 implementation of  using semaphores  250 resumption of processes within  250-252 usage of  245-247 monitor calls  see system calls monoculture  633 monotonic  729 morris  robert  634-636 most-frequently used  mfu  page-replacement algorithm  380 mounting  467-468 mount points  444  885 mount protocol  492-493 mount table  467  578 mpeg files  783-784 ms-dos  875-876  917 multicasting  790 multicore processors  204-205 multicore programming  156-157 multics operating system  596-598  903  904  915 multilevel feedback-queue scheduling algorithm  198-199 multilevel index  477 multilevel queue scheduling algorithm  196-197 multimedia  779-780 operating system issues with  782 as term  779-780 multimedia data  33  780-781 multimedia systems  33  779 characteristics of  781-782 cineblitz example  792-794 compression in  782-784 cpu scheduling in  786-787 disk scheduling in  787-789 kernels in  784-786 network management in  789-792 multinational use  851 multipartite viruses  633 multiple-coordinator approach  concurrency control   737 multiple-partition method  326 multiple universal-naming-convention provider  mup   890 multiprocessing  asymmetric  14  15  202 memory access model for  15 symmetric  14-15  202  819-820 multiprocessor scheduling  200-206 approaches to  202 examples of  linux  211-213 solaris  206-208 windows xp  208-211 and load balancing  203-204 and ntulticore processors  204-205 and processor affinity  202-203 and virtualization  205-206 multiprocessor systems  parallel systems  tightly coupled systems   13-16 multiprogramming  18-20  108 multitasking  see time sharing multithreading  benefits of  155 cancellation  thread  166-167 coarse-grained  205 and exec   system call  165-166 fine-grained  205 and fork   system call  165-166 n10dels of  157-159 pools  thread  168-170 and scheduler activations  170-171 and signal handling  167-168 and thread-specific data  170 mup  multiple universal-naming-convention provider   890 mutex  adaptive  254 in windows xp  854 mutex locks  235  285-286 mutual exclusion  285-286  730-732 centralized approach to  730 fully-distributed approach to  730-732 token-passing approach to  730 mutual-exclusion condition  deadlocks   291 names  resolution of  685  892 in windows xp  857-858 named pipes  888 naming  120-122  448-449 defined  707 domain name system  448 of files  422 lightweight directory-access protocol  449 and network communication  685-686 national-language-support  nls  api  851 ndis  network device interface specification   886 near-line storage  536 negotiation  785 netbeui  netbiosextended user interface   887 netbios  network basic input/output system   886  888 netbiosextended user interface  netbeui   887 .net framework  83 network  s   see also local-area networks  lans  ; wide-area networks  wans  communication protocols in  690-694 communication structure of  684-690 and connection strategies  688-689 and contention  689-690 and naming/name resolution  685-686 and packet strategies  688 and routing strategies  687 defined  31 design issues with  697-699 example  699-701 in linux  838-840 metropolitan-area  mans   31 robustness of  694-697 security in  624 small-area  31 threats to  633-638 topology of  683-684 types of  679-680 in windows xp  886-892 active directory  892 959 distributed-processing mechanisms  888-889 domains  891-892 interfaces  886 name resolution  892 protocols  886 redirectors and servers  889-891 wireless  35 network-attached storage  509-510 network basic input/output system  see netbios network computers  34 network devices  568-569  835 network device interface specification  ndis   886 network file systems  nfs   490-496 mount protocol  492-493 nfs protocol  493-494 path-name translation  494-495 remote operations  495 network information service  nis   448 network layer  691 network-layer protocol  646 network login  449 network management  in multimedia systems  789-792 network operating systems  31  675-677 network virtual memory  711 new state  103 nfs  see network file systems nfs protocol  493-494 nfs v4  717 nice value  linux   211  816 nis  network information service   448 nls  national-language-support  api  851 nonblocking i/0  570-571 nonblocking  asynchronous  message passing  122 noncontainer objects  windows xp   665 nonmaskable interrupt  561 nonpreemptive kernels  228-229 960 nonpreemptive scheduling  186 non-real-time clients  792 nonrepudialion  645 nonresident attributes  879 nonserial schedule  263 nonsignaled state  255 non-uniform memory access  numa   15  385 nonvolatile ram  nvram   11 nonvolatile ram  nvram  cache  524 nonvolatile storage  10-11  260 no-preemption condition  deadlocks   292 novell netware protocols  887 ntfs  878-880 numa  see non-uniform memory access nvram  nonvolatile ram   11 nvram  nonvolatile ram  cache  524 objects  access lists for  602-603 in cache  398 free  398 hardware vs software  593 in linux  822 used  398 in windows xp  857-860 object files  422 object linking and embedding  ole   889 object table  859 object types  469  859 off-line compaction of space  473 ole  see object linking and embedding olpc  one laptop per child   919 on-demand streaming  781 one laptop per child  olpc   919 one-time pad  653 one-time passwords  653 one-to-one multithreading model  158 one-way trust  891 on-line compaction of space  473 open-file table  424 open   operation  424 open-source operating systems  7  37-40 open virtual machine format  78 operating system  s   dosed-source  37 defined  3  5-6 design goals for  68 early  904-911 dedicated computer systems  905-906 overlapped i/0  909-911 shared computer systems  906-909 feature migration with  903-904 features of  3 functioning of  3-6 guest  81 i1nplementation of  69-70 interrupt-driven  20-23 mechanisms for  68-69 network  31 open-source  37-40 operations of  modes  21-23 and time1 ~ 23 policies for  68-69 real-time  32-33 as resource allocatm ~ 5 security in  624 services provided by  49-52 structure of  18-20  70-75 layered approach  71-73 microkernels  73-75 modules  74-76 simple structure  70-71 study of  7 system 's view of  5 user interface with  4-5  52-55 optimal page replacement algorithm  374-376 ordering  event  see event ordering orphan detection and elimination  716 os/2 operating system  847 out-of-band key delivery  645 over allocation  of memory   369 overlapped i/0  909-911 overprovisioning  784 owner rights  linux   842 p  page number   329 packets  688  840 packet switching  689 packing  430 pages  defined  329 shared  336-337 page allocator  linux   820 page-buffering algorithms  380-381 page cache  484  823 page directory  862 page-directory entries  pdes   862 page .fault  363 page-fault-frequency  pff   390-391 page-fault rate  367 page frames  862 page-frame database  865 page number  p   329 page offset  d   329 pageout  solaris   406 pageout policy  linux   825 pager  term   361 page replacement  369-381 see also frame allocation and application performance  38  basic mechanism  370-373 counting-based page replacement  380 fifo page replacement  373-375 global vs local  384 lru-approximation page replacement  378-380 lru page replacement  376-378 optimal page replacement  374-376 and page-buffering algorithms  380-381 page replacement algorithm  372 page size  400-401 page slots  522 page table  s   329-332  364  862 clustered  340 forward-mapped  338 hardware for storing  332-334 hashed  340 inverted  340-342  402 page-table base register  ptbr   333 page-table length register  ptlr   336 page-table self-map  861 paging  328-341 basic method of  329-332 hardware support for  332-334 hashed page tables  340 hierarchical  337-340 intel pentium example  346-348 inverted  340-342 in linux  825-826 and memory protection  335-336 priority  407 and shared pages  336-337 swapping vs  520 paging files  windows xp   861 paging mechanism  limlx   825 paired passwords  652 pam  pluggable authentication modules   841 parallelization  17 parallel systems  see multiprocessor systems para-virtualization  79 parent process  110  859 partially connected networks  683-684 partition  s   325-326  433  434  467-468 boot  518 raw  521 root  467 partition boot sector  464 partitioning  disk  517 passwords  649-653 encrypted  651-652 one-time  652-653 vulnerabilities of  650-651 path name  438 path names  absolute  439 relative  439-440 path-name translation  494-495 pcbs  see process control blocks pci bus  556 pcs  process-contention scopet 199-200 pc systems  3 pdas  see personal digital assistants pdes  page-directory entries   862 peer-to-peer computing  36 penetration test  654 performance  and allocation of disk space  477-479 and i/0 system  582-585 with tertiary-storage  539-543 cost  541-543 reliability  541 speed  539-541 of windows xp  850 performance improvement  483-486  524 performance tuning  84-85 periods  784 periodic processes  784 permissions  455 per-process open-file table  465 persistence of vision  780 personal computer  pc  systems  3 personal digital assistants  pdas   11  33 personal firewalls  662 personal identification number  pin   653 peterson 's solution  229-230 pff  see page-fault-frequency phase-change disks  535 phishing  624 physical address  319 physical address space  319-320 physical formatting  516 physical layer  691 physical memory  20  357-358  820-823 physical security  623 pic  position-independent code   828 pid  process identifier   110-111 pin  personal identification number   653 pinning  871 pio  see programmed 1/0 pipes  134-140 anonymous  136-137 named  137-139 ordinary  134-137 use of  140 pipe mechanism  838 platter  disks   505-506 961 plug-and-play and  pnp  managers  872-873 962 pluggable authentication modules  pam   841 pnp managers  sec plug-and-play and managers point-to-point tunneling protocol  pptp   887 policy  ies   68-69 group  892 security  654 policy algorithm  linux   825 polling  559 polymorphic viruses  632 pools  of free pages  369 of storage  532 thread  168-170 pop-up browser windows  626 ports  396  556 portability  851 portals  34 port driver  870 port scanning  637 position-independent code  pic   828 positioning time  disks   506 posix  847  850 interprocess communication example  123-124 message passing in  148-152 in windows xp  877 possession  of capability   603 power-of-2 allocator  397 pptp  point-to-point tunneling protocol   887 p 1 q redundancy scheme  527 preemption points  765 preemptive kernels  228-229  765 preemptive scheduling  185-186 premaster secret  ssl   648 prepaging  399-400 presentation layer  692 primary thread  894 principle of least privilege  592-593 priority-based scheduling  764-765 priority-inheritance protocol  239  254-255  768 priority inversion  238-239  254  768 priority number  251 priority paging  407 priority replacement algorithm  386 priority scheduling algorithm  192-193 private keys  642 privileged instructions  22 privileged mode  see kernel mode probes  dtrace   86 process  es   20 background  196 communication between  see interprocess communication components of  102-103 context of  110  813-814 and context switches  110 cooperating  116 defined  101 environment of  813 faulty  751-752 foreground  196 heavyweight  153 independent  116 i/o-bound vs cpu-bound  109 job vs  102 in linux  814-815 multithreaded  see multithreading operations on  110-115 creation  110-115 termination  115-116 programs vs  24  102-103 scheduling of  105-110 single-threaded  153 state of  103 as term  101-102 threads performed by  104-105 in windows xp  894 process-contention scope  pcs   199-200 process control blocks  pcbs  task control blocks   103-104 process-control system calls  60-64 process file systems  linux   833-834 process identifier  pid   110-111 process identity  linux   812-813 process management  23-24 in linux  812-815 fork   and exec   process model  812-814 processes and threads  814-815 process manager  windows xp   866-867 process migration  79 process mix  109 process objects  windows xp   854 processor affinity  202-203 processor sets  202 processor sharing  195 process representation  linux   106 process scheduler  105 process scheduling  in linux  815-817 thread scheduling vs  183 process synchronization  about  225-227 and atomic transactions  257-267 checkpoints  261-262 concurrent transactions  262-267 log-based recovery  260-261 system model  257-260 bounded-buffer problem  240 critical-section problem  227-229 hardware solution to  231-234 peterson 's solution to  229-230 dining-philosophers problem  242-24l1  248-249 examples of  java  253 linux  256-257 pthreads  257 solaris  253-255 windows xp  255-256 monitors for  244-252 dining-philosophers solution  248-249 resumption of processes within  250-252 semaphores  implementation using  250 usage  245-247 readers-writers problem  241-242 semaphores for  234-239 process termination  deadlock recovery by  304-305 production kernels  linux   803 profiles  783 profiling  dtrace   85-86 programs  processes vs  102-103 see also application programs program counters  24  102 program execution  operating system service   50 program files  422 program loading and execution  67 programmable interval timer  569 programmed i/0  pio   396  563 programming-language support  67 program threats  625-633 logic bombs  627 stack or buffer overflow attacks  627-630 trap doors  626 trojan horses  625-626 viruses  630-633 progressive download  780 projects  208 proportional allocation  383 proportional share scheduling  772 protection  66  591 access control for  451-456 access mah ix as model of  598-602 control  access  605-606 implementation  602-605 capability-based systems  607-610 cambridge cap system  609-610 hydra  607-609 in computer systen1s  29-30 domain of  593-598 multics example  596-598 structure  594-595 unix example  595-596 error handling  575 file  422 of file systems  451-456 goals of  591-592 l/0  575-576 language-based systems  610-615 compiler-based enforcement  610-613 java  613-615 as operating system service  51-52 in paged environment  335-336 permissions  455 and principle of least privilege  592-593 retrofitted  456 and revocation of access rights  606-607 security vs  621 static vs dynamic  594 from viruses  658-660 protection domain  594 protection mask  linux   842 963 protection subsystems  windows xp   851 protocols  windows xp networking  886-887 providers  dtrace   86 ptbr  page-table base register   333 pthreads  160-161 scheduling  200-201 synchronization in  257 pthread scheduling  772-774 ptlr  page-table length register   336 public domain  805 public keys  642 pull migration  203-204 pure code  336 pure demand paging  364 push migration  203-204  708 quantum  853 queue  s   105-108 capacity of  122-123 input  318 message  914 ready  105-107  323 queueing diagram  107 queueing-network analysis  215 race condition  227 raid  redundant arrays of inexpensive disks   522-532 levels of  524-529 performance improvement  524 problems with  531-532 reliability improvement  522-524 structuring  523 raid array  523 raid levels  524-529 ram  random-access memory   9 random access  781 964 random-access devices  566  567  910 random-access memory  ram   9 random-access time  disks   506 rate-monotonic scheduling algorithm  769-771 raw disk  381  433  467 raw i/0  568 raw partitions  521 rbac  role-based access control   605 rc 4000 operating system  913-914 reaching algorithms  750-752 read-ahead technique  486 read-end  of pipe   134 readers  241 reader-writer locks  241-242 readers-writers problem  241-242 reading files  423 read-modify-write cycle  527 read only devices  566  567 read-only disks  535 read-only memory  rom   90  518 read queue  836 read-write devices  566  567 read-write disks  535 ready queue  105-107  323 ready state  103 ready thread state  windows xp   853 real-addressing mode  763 real-time class  209 real-time clients  792 real-time operating systems  32-33 real-time range  limlx schedulers   816 real-time streaming  780  790-792 real-time systems  32-33  759-760 address translation in  763-764 characteristics of  760-762 cpu scheduling in  768-774 defined  759 features not needed in  762-763 footprint of  7 61 hard  760  786 implementation of  764-768 and minimizing latency  765-768 and preemptive kernels  765 and priority-based scheduling  764-765 soft  760  786 vxworks example  774-776 real-time transport protocol  rtp   789 real-time value  linux   211 reconfiguration  694-695 records  logical  431 master boot  518 recovery  backup and restore  489-490 and consistency checking  486-487 from deadlock  304-306 by process termination  304-305 by resource preemption  305-306 from failure  696 of files and directories  486-490 windows xp  880-881 redirectors  889 redundancy  523 see also raid redundant arrays of inexpensive disks  see raid reed-solomon codes  527 reentrant code  pure code   336 reference bits  378 reference model  iso  647 reference string  372 register  s   58 base  316  317 limit  316  317 memory-address  319 page-table base  333 page-table length  336 for page tables  332-333 relocation  320 registry  67  873-874 relative block number  432 relative path names  439-440 relative speed  228 release   operation  425 reliability  688 of distributed operating systems  674-675 in multimedia systems  785 of windows xp  849 relocation register  320 remainder section  227 remote file access  distributed file systems   710-715 basic scheme for  711 and cache location  711-712 and cache-update policy  712  713 and caching vs remote service  714-715 and consistency  713-714 remote file systems  447 remote file transfer  676-677 remote login  676 remote operations  495 remote procedure calls  rpcs   888-889 remote-service mechanism  710 removable storage media  537-538 application interface with  537-538 disks  534-535 and file naming  538 and hierarchical storage management  539 magnetic disks  505-507 magnetic tapes  507  535-536 rendezvous  122 repair  mean time to  523 replay attacks  622 replication  497  498  529 repositioning  in files   423 request edge  287 request manager  835 resident attributes  879 resident monitor  907 resolution  name  685 and page size  401 resolving links  441 resource allocation  operating system service   51 resource-allocation graph algorithm  296-297 resource allocato1 ~ operating system as  5 resource fork  429-430 resource manager  786 resource preemption  deadlock recovery by  305-306 resource-request algorithm  299 resource reservations  786 resource sharing  155  674 resource utilization  4 response time  19  187-188 restart area  881 restore  data  489-490 state  110 retrofitted protection mechanisms  456 reverse engineering  37 revocation of access rights  606-607 rich text format  rtf   660 rights amplification  hydra   608 ring algorithm  749-750 ring structure  732 risk assessment  654-656 roaming profiles  890 robotic jukebox  539 robustness  694-697 roles  605 role-based access control  rbac   605 rolled-back transactions  259 roll out  roll in  323 rom  see read-only memory root partitions  467 root uid  linuxt 842 rotational latency  diskst 506  511 round-robin  rr  scheduling algorithm  194-196 routing  and network communication  687-688 in partially cmmected networks  683-684 routing protocols  688 routing table  687 rpcs  remote procedure calls   888-889 rr scheduling algorithm  see round-robin scheduling algorithm rtf  rich text format   660 r-timestamp  266 rtp  real-time transport protocol   789 running state  103 running system  90 running thread state  windows xp   853 runqueue data structure  212  816 rw  read-write  format  26 safe computing  660 safe sequence  295 safety algorithm  298-299 safety-critical systems  760 sandbox  tripwire file system   658 sans  see storage-area networks sata buses  507 save  state  110 scalability  155  697-698 965 scan  elevator  scheduling algorithm  513-514  788-798 schedules  262 scheduler  s   108-109 long-term  108 medium-term  109 short-term  108 scheduler activation  170-171 scheduling  cooperative  186 cpu  see cpu scheduling disk scheduling algorithms  510-516 c-scan  514 fcfs  511-512 look  515 scan  513-514 selecting  515-516 sstf  512-513 earliest-deadline-first  771 i/0  571-572 job  20 in linux  815-820 kernel synchronization  817-819 process  815-817 symmetric multiprocessing  819-820 multiprocessor  see multiprocessor scheduling nonpreemptive  186 preemptive  185-186 priority-based  764-765 proportional share  772 pthread  772-774 rate-monotonic  769-771 966 scheduling   contd  thread  199-201 in windows xp  853-854  895-897 scheduling rules  895 scope operating system  921 script kiddies  630 scs  system-contention scope   199 scsi  small computer-systems interface   12 scsi buses  507 scsi initiator  509 scsi targets  509 search path  438 secondary memory  364 secondary storage  10  461 see also disk  s  second-chance page-replacement algorithm  clock algorithm   378-379 second extended file system  ext2fs   830-832 section objects  127 sectors  disk  506 sector slipping  519 sector sparing  519  884 secure single sign-on  449 secure systems  622 security see also file access ; program threats ; protection ; user authentication classifications of  662-664 in computer systems  29-30 and firewalling  661-662 implementation of  654-661 and accounting  660-661 and auditing  660-661 and intrusion detection  656-658 and logging  660-661 and security policy  654 and virus protection  658-660 and vulnerability assessment  654-656 levels of  623-624 in linux  840-843 access control  841-843 authentication  841 as operating system service  51 as problem  621-625 protection vs  621 and system/network threats  633-638 denial of service  638 port scanning  637 worms  634-637 use of cryptography for  638-649 and encryption  639-646 implementation  646-647 ssl example  647-649 via user authentication  649-654 biometrics  653-654 passwords  649-653 windows xp  881 in windows xp  664-665  849 security access tokens  windows xp   664 security context  windows xp   664-665 secu.rity descriptor  windows xp   665 security domains  661 security policy  654 security reference monitor  srm   872 security-through-obscurity approach  656 seeds  652-653 seek  file  423 seek time  disks   506  511 segmentation  342-345 basic method  342-345 defined  343 hardware  344-345 intel pentium example  345-348 segment base  344 segment limit  344 segment tables  344 semantics  consistency  450-451 copy 573 immutable-shared-files  451 session  451 semaphore  s   234-239 binary  234 counting  234 and deadlocks  238 defined  234 implementation  235-238 implementation of monitors using  250 and priority inversion  238-239 and starvation  238 usage of  234-235 windows xp  854 semiconductor memory  10 sense key  575 sequential access  files   431 sequential-access devices  910 sequential devices  566  567 serial ata  sata  buses  507 serializability  262-264 serial schedule  263 server  s   5 cluste1 ~ 719 defined  706 in ssl  648 server-message-block  smb   886 server subject  windows xp   664 services  operating system  49-52 session hijacking  623 session layer  691 session object  861 session semantics  451 session space  861 sharable devices  566  567 shares  208 shared files  immutable  451 shared libraries  322  360 shared lock  426 shared lock mode  736 shared memory  116  360 shared-memory model  66  117-119 shared name space  719 sharing  load  200  674 and paging  336-337 resource  674 time  19 shells  52 shell script  429 shortest-job-first  sjf  scheduling algorithm  189-192 shortest-remaining-time-first scheduling  192 shortest-seek-time  sstf  scheduling algorithm  512-513 short-term scheduler  cpu scheduler   108  185 shoulder surfing  650 signals  linux  837 unix  167-168 signaled state  255 signal handlers  167-168 signatures  656-657 signature-based detection  656-657 simple operating system structure  70-71 simple subject  windows xp   664 simulation  s   78-79  216 single indirect blocks  477 single-level directories  436-437 single-processor systems  12-13  183 single-threaded processes  153 sjf scheduling algorithm  see shortest-job-first scheduling algorithm slab allocation  398-399  821-822 sleeping-barber problem  274-280 slices  434 small-area networks  31 small computer-systems interface  see under scsi smb  see server-message-block smp  sec symmetric multiprocessing snapshots  488 sniffing  650 social engineering  624 sockets  128-130 socket interface  568 soc strategy  see system-on-chip strategy soft affinity  202 soft error  517 soft real-time systems  760  786 software capability  609 software interrupts  traps   561 software objects  593 solaris  40 and processor affinity  202 scheduling example  206-208 swap-space management in  521-522 synchronization in  253-255 virtual memory in  406-407 solid-state disks  ssds   28  536 sorted queue  836 source-code viruses  631 source files  422 space maps  481-482 sparseness  340  360 special-purpose computer systems  32-34 handheld systems  33-34 multimedia systems  33 real-time embedded systems  32-33 speed  of operations  i/0 devices   566  567 relative  228 spinlock  236 spoofed client identification  447-448 spoofing  662 spool  574 spooling  574-575  910-911 spyware  626 srm  see security reference monitor ssds  see solid-state disks ssl 3.0  647-649 sstf scheduling algorithm  967 sec shortest-seek-time scheduling algorithm stable storage  260  533-534 stack  58  102 stack algorithms  377 stack frame  628-629 stack inspection  614 stack-overflow attacks  627-630 stage  magnetic tape   535 stalling  316 standby thread state  windows xp   853 starvation  see indefinite blocking state  of process   103 stateful file service  715 stateless dfs  450 stateless file service  715 stateless protocols  791 state restore  110 state save  110 968 static linking  321-322  828 static priority  787 static protection  594 status information  67 status register  558 stealth viruses  632 storage  9-11 see also mass-storage structure definitions and notations  6 holographic  536 nonvolatile  10-11  260 secondary  10  461 stable  260 tertiary  27 utility  530 volatile  10  259 storage-area networks  sans   18  509  510 storage array  523 storage management  25-29 caching  27-29 i/0 systems  29 mass-storage management  26-27 stored program computers  904-905 stream ciphers  641-642 stream head  580 streaming  780-781 stream modules  580-581 streams mechanism  580-582 string  reference  372 stripe set  882-883 stubs  321 stub routines  888 sunos  40 superblock  464 superblock objects  469  829 supervisor mode  see kernel mode suspended state  896 sustained bandwidth  539 swap map  522 swapper  term   361 swapping  20  109  322-324  361 in linux  825-826 paging vs  520 swap space  364 swap-space management  520-522 switch architecture  12 switching  circuit  689 domain  595 message  689 packet  689 symbolic links  858 symbolic-link objects  858 symmetric encryption  641-642 symmetric mode  17 symmetric multiprocessing  smp   14-15  202  819-820 synchronization  122 see also process synchronization synchronous devices  566  567 synchronous message passing  122 synchronous writes  485 sysgen  sec system generation system boot  89-90 system calls  monitor calls   8  55-58 and api  56-57 for communication  65-66 for device management  64 for file management  64 functioning of  55-56 for information maintenance  65 for process control  60-64 system-call firewalls  662 system-call interface  57 system-contention scope  scs   199 system device  874 system disk  see boot disk system files  438 system generation  sysgen   88-89 system hive  874 systems layer  783 system libraries  linux   807  808 system mode  see kernel mode system-on-chip  soc  strategy  761  762 system process  windows xp   874 system programs  66-67 systems programs  6 system resource-allocation graph  287-289 system restore  874 system utilities  66-67  807  808 system-wide open-file table  464 table  s   358 file-allocation  475-476 hash  470-471 m.aster file  464 mount  467  578 object  859 open-file  424 page  364,862 per-process open-file  465 routing  687 segment  344 system-wide open-file  464 tags  603 tapes  magnetic  507  535-536 target thread  166 tasks  linux  814-815 vxworks  774 task control blocks  see process control blocks tcb  trusted computer base   663 tcp/ip  sec transmission control protocol/internet protocol tcp sockets  129 tdi  transport driver interface   886 telnet  676 terminal concentrators  584 terminated state  103 terminated thread state  windows xp   853 termination  cascading  116 process  110-116  304-305 tertiary-storage  534-543 future technology fm ~ 536 and operating system support  536-538 performance issues with  539-543 removable disks  534-535 tapes  535-536 tertiary storage devices  27 text files  422 text section  of process   102 theft of service  622 the operating system  913 thrashing  386-387 cause of  386-387 defined  386 and page-fault-frequency sh ategy  390-391 and working-set model  387-389 threads see also multithreading cancellation  thread  166-167 components of  153 functions of  153-156 idle  209 kernel  157 in linux  173-174  814-815 and multicore programming  156-157 pools  thread  168-170 and process model  104-105 scheduling of  199-201 target  166 use1 ~ 157 in windows xp  171-173  853-854  894-897 thread libraries  159-165 about  159-160 java tlueads  162  164-165 pthreads  160-161 win32 threads  162-163 thread pool  897 thread scheduling  183 thread-specific data  170 threats  622 see also program threats throughput  187  785 thunking  876 tightly coupled systems  see multiprocessor systems time  compile  318 effective access  365 effective memory-access  334 execution  318 of file creation/ use  423 load  318 response  19  187-188 turnaround  187 waiting  187 time-out schemes  695  751 time profiles  65 time quantum  194 timer  programmable interval  569 variable  23 timers  569-570 timer objects  854 time sharing  multitasking   19 timestamp-based protocols  265-267 timestamping  739-740 timestamps  729 tlb  see translation look-aside buffer tlb miss  333 tlb reach  401-402 tokens  690  732 token passing  690  732 top half interrupt service routines  819 topology  network  683-684 tops-20  917 torvalds  linus  801 trace tapes  216 tracks  disk  506 traditional computing  34-35 969 transactions  257-258 see also atomic transactions defined  832 in linux  832-833 in log-structured file systems  487-488 transactional memory  258-259 transarc dfs  718 transfer rate  disks   506  507 transition thread state  windows xp   853 transitive trust  891 translation coordinator  733 translation look-aside buffer  tlb   333  863-864 transmission control protocol  tcp   693 transmission control protocol/internet protocol  tcpiip   887 transparency  697  706  707 970 transport driver interface  tdi   886 transport layer  691 transport-layer protocol  tcp   646 traps  20-21  363  562 trap doors  626 tree-structured directories  438-440 triple des  641 triple indirect blocks  477 tripwire file system  658-659 trojan horses  625-626 trusted computer base  tcb   663 trust relationships  891-892 tunneling viruses  632 turnaround time  187 turnstiles  254 two-factor authentication  653 twofish algorithm  641 two-level directories  437-438 two-phase commit  2pc  protocol  733-734 two-phase locking protocol  265 two tuple  343 type safety  java   615 udp  user datagram protocol   693 udp sockets  129 ufd  user file directory   437 ufs  unix file system   463 ui  see user interface uma  uniform memory access   15 unbounded capacity  of queue   123 unc  uniform naming convention   888 unicasting  789 unicode  851 unified buffer cache  484  485 unified virtual memory  484 uniform memory access  uma   15 uniform naming convention  unc   888 universal serial buses  usbs   507 unix file system  ufs   463 unix operating system  consistency semantics fm ~ 450--451 domain switching in  595-596 feature migration with  903  904 and linux  801 permissions in  455 signals in  167-168 swapping in  324 unreliability  688 unreliable communications  751 upcalls  171 npcall handler  171 u.s digital millennium copyright act  dmca   38 usbs  see universal serial buses used objects  398  823 users  4-5  446-447 user accounts  664 user authentication  649-654 with biometrics  653-654 with passwords  649-653 user datagram protocol  udp   693 user-defined signal handlers  167 user file directory  ufd   437 user identifiers  user ids   30 effective  30 for files  423 user interface  ui   50-55 user mobility  492 user mode  21 user programs  user tasks   101-102  826-827 user rights  linux   842 user threads  157 utilities  904 utility storage  530 utilization  906 vacb  see virtual address control block vads  virtual address descriptors   866 valid-invalid bit  335 variable class  208-209 variables  automatic  628 variable timer  23 vdm  see virtual dos machine vector programs  634 vforko  virtual memory fork   369 vfs  sec virtual file system victim frames  371 views  861 virtual address  319 virtual address control block  vacb   870  871 virtual address descriptors  vads   866 virtual address space  359  824-825 virtual dos machine  vdm   875-876 virtual file system  vfs   468-470  829-830 virtualization  hardware fm ~ 80 and multiprocessor scheduling  205-206 virtual machines  76-84 basic idea of  76 benefits of  77-78 history of  76-77 implementation of  80 java virtual machine as example of  82 vmware as example of  81-82 virtual memory  20  357-360 and copy-on-write technique  367-369 demand paging for conserving  361-367 972 windows xp  contd  history of  84,7-849 interprocess communication example  127-128 networking  886-892 active directory  892 distributed-processing lttechanisms  888-889 domains  891-892 interfaces  886 name resolution  892 protocols  886-887 redirectors and servers  889-891 performance of  850 portability of  851 programmer interface  892-899 interprocess communication  897-898 kernel object access  893 memory management  898-899 process management  894-897 sharing objects between processes  893-894 reliability of  849 scheduling example  208-211 security  877-878 security in  849 synchronization in  255-256 system components for  851-874 executive  see windows xp executive hardware-abstraction layer  852 kernel  852-856 tlu eads example  171-173 virtual memory in  405---406 windows xp executive  857-874 booting  874 cache manager  870-872 i/0 manager  869-870 local procedure call facility  868-869 object manager  857-860 plug-and-play and power managers  872-873 process managet ~ 866-867 registry  873-874 security reference monitor  872 virtual memory manager  860-866 winsock  888 wireless  wifi  networks  35  680-681 wirte-on-close policy  713 witness  293 word  6 working sets  387  391 working-set maximum  windows xp   405 working-set minimum  windows xp   405 working-set model  387-389 workstations  5 world rights  linux   842 world wide web  447 worms  634-637 worm disks  see write-once  read-many-times disks worm  write-once  read-many  format  26 worst-fit strategy  327 wound-wait scheme  741-742 write-ahead logging  260-261 write-back caching  712 write-end  of pipe   134 write-once  read-many-times  worm  disks  535 write only devices  566  567 write queue  836 writers  241 write-through policy  712 writing files  423 w-timestamp  266 xdr  external data representation   132 xds-940 operating system  912 xerox  53 xml firewalls  662 zero capacity  of queue   122 zero-day attacks  657 zero-fill-on-demand technique  369 zfs file system  481-482  488 zipped files  783 zombie systems  637 zones  linux   820 zones  solaris 10   79 basic mechanism  362-364 with inverted page tables  402 and l/0 interlock  404-405 and page size  400-401 and performance  365-367 and prepaging  399-400 and program structure  402-403 pure demand paging  364 and restarting instructions  364-365 and tlb reach  401-402 direct virtual memory access  564 and france allocation  382-385 equal allocation  383 global vs local allocation  384-385 proportional allocation  383-384 kernel  826 and kernel memory allocation  396-399 in linux  823-826 and memory mapping  390-396 basic mechanism  391-393 i/0  memory-mapped  395-396 in win32 api  393-395 network  711 page replacement for conserving  369-381 and application performance  381 basic mechanism  370-373 counting-based page replacement  380 fifo page replacement  373-375 lru-approximation page replacement  378-380 lru page replacement  376-378 optimal page replacement  374-376 and page-buffering algorithms  380-381 separation of logical memory from physical memory by  359 size of  358 in solaris  406-407 and tlu ashing  386-387 cause  386-387 page-fault-frequency strategy  390-391 worki1cg-set model  387-389 unified  484 in windows xp  405-406 virtual memory fork  369 virtual memory  vm  manager  860-866 virtual memory regions  824 virtual private networks  vpns   647  887 virtual routing  687 viruses  630-633  658-660 virus droppe1 ~ 631 vm manager  sec virtual memory manager vmware  81-82 vmware workstation  80-82 vnode  468 vnode number  nfs v4   720 volatile storage  10  259 971 volumes  434  720 volume control block  464 volume-location database  nfs v4   720 volume management  windows xp   881-884 volume set  881-882 volume shadow copies  885 volume table of contents  434 von neumann architecture  9 vpns  see virtual private networks vulnerability scans  654-656 vxworks  774-776 wafl file system  488  496-498 wait-die scheme  741-742 waiting state  103 waiting thread state  windows xp   853 waiting time  187 wait queue  837 wans  see wide-area networks web-based computing  37 web clipping  34 web distributed authoring and versioning  webdav   887 wide-area networks  wans   17  31  681-683 wifi networks  see wireless networks win32 api  393-395  847-848  877 win32 thread library  162-163 windows 2000  849  851 windows nt  847-848 windows xp  847-900 application compatibility of  849-850 design principles for  849-851 desktop versions of  848 environmental subsystems for  874-878 16-bit windows  876 32-bit windows  876-877 logon  877-878 ms-dos  875-876 posix  877 security  877-878 wi.n32  876-877 extensibility of  850-851 file systems  878-885 change journal  885 compression and encryption  884-885 mount points  885 ntfs b1 tree  879-880 ntfs internal layout  878-879 ntfs metadata  880 recovery  880-881 security  881 volume management and fault tolerance  881-884 volume shadow copies  885 