in chapter 5  we showed how the cpu can be shared by a set of processes as a result of cpu scheduling  we can improve both the utilization of the cpu and the speed of the computer 's response to its users to realize this increase in performance  however  we must keep several processes in memory ; that is  we must share memory in this chapter  we discuss various ways to manage memory the memorymanagement algorithms vary from a primitive bare-machine approach to paging and segmentation strategies each approach has its own advantages and disadvantages selection of a memory-management method for a specific system depends on many factors  especially on the hardware design of the system as we shall see  many algorithms require hardware support  although recent designs have closely integrated the hardware and operating system to provide a detailed description of various ways of organizing memory hardware to discuss various memory-management techniques  including paging and segmentation to provide a detailed description of the intel pentium  which supports both pure segmentation and segmentation with paging as we saw in chapter 1  memory is central to the operation of a modern computer system memory consists of a large array of words or bytes  each with its own address the cpu fetches instructions from memory according to the value of the program counter these instructions may cause additional loading from and storing to specific memory addresses a typical instruction-execution cycle  for example  first fetches an instruction from memory the instruction is then decoded and may cause operands to be fetched from memory after the instruction has been executed on the 315 316 chapter 8 operands  results may be stored back in memory the mernory unit sees only a stream of memory addresses ; it does not know how they are generated  by the instruction counter  indexing  indirection  literal addresses  and so on  or what they are for  instructions or data   accordingly  we can ignore hozu a program generates a memory address we are interested only in the sequence of memory addresses generated by the running program we begin our discussion by covering several issues that are pertinent to the various techniques for managing memory this coverage includes an overview of basic hardware issues  the binding of symbolic memory addresses to actual physical addresses  and the distinction between logical and physical addresses we conclude the section with a discussion of dynamically loading and linking code and shared libraries 8.1.1 basic hardware main memory and the registers built into the processor itself are the only storage that the cpu can access directly there are machine instructions that take memory addresses as arguments  but none that take disk addresses therefore  any instructions in execution  and any data being used by the instructions  must be in one of these direct-access storage devices if the data are not in memory  they must be moved there before the cpu can operate on them registers that are built into the cpu are generally accessible within one cycle of the cpu clock most cpus can decode instructions and perform simple operations on register contents at the rate of one or more operations per clock tick the same can not be said of main memory  which is accessed via a transaction on the memory bus completing a memory access may take many cycles of the cpu clock in such cases  the processor normally needs to stall  since it does not have the data required to complete the instruction that it is executing this situation is intolerable because of the frequency of memory accesses the remedy is to add fast memory between the cpu and 0 operating system 256000 process 300040 i soa  lj.o i process base 420940 i 120 ! 1go i i  limit process 880000 1024000 figure 8.1 a base and a limit register define a logical address space 8.1 317 main memory a memory buffer used to accommodate a speed differential  called a is described in section 1.8.3 not only are we concerned with the relative speed of accessing physical memory  but we also must ensure correct operation to protect the operating system from access by user processes and  in addition  to protect user processes from one another this protection must be provided by the hardware it can be implemented in several ways  as we shall see throughout the chapter in this section  we outline one possible implementation we first need to make sure that each process has a separate memory space to do this  we need the ability to determine the range of legal addresses that the process may access and to ensure that the process can access only these legal addresses we can provide this protection by using two registers  usually a base and a limit  as illustrated in figure 8.1 the base holds the smallest legal physical memory address ; the specifies the size of the range for example  if the base register holds 300040 and the limit register is 120900  then the program can legally access all addresses from 300040 through 420939  inclusive   protection of memory space is accomplished by having the cpu hardware compare every address generated in user mode with the registers any attempt by a program executing in user mode to access operating-system memory or other users ' memory results in a trap to the operating system  which treats the attempt as a fatal error  figure 8.2   this scheme prevents a user program from  accidentally or deliberately  modifying the code or data structures of either the operating system or other users the base and limit registers can be loaded only by the operating system  which uses a special privileged instruction since privileged instructions can be executed only in kernel mode  and since only the operating system executes in kernel mode  only the operating system can load the base and limit registers this scheme allows the operating system to change the value of the registers but prevents user programs from changing the registers ' contents the operating system  executing in kernel mode  is given unrestricted access to both operating system memory and users ' memory this provision allows the operating system to load users ' programs into users ' memory  to yes no trap to operating system monitor-addressing error memory figure 8.2 hardware address protection with base and limit registers 318 chapter 8 dump out those programs in case of errors  to access and modify parameters of system calls  and so on 8.1.2 address binding usually  a program resides on a disk as a binary executable file to be executed  the program must be brought into memory and placed within a process depending on the memory management in use  the process may be moved between disk and memory during its execution the processes on the disk that are waiting to be brought into memory for execution form the the normal procedure is to select one of the processes in the input queue and to load that process into memory as the process is executed  it accesses instructions and data from memory eventually  the process terminates  and its memory space is declared available most systems allow a user process to reside in any part of the physical memory thus  although the address space of the computer starts at 00000  the first address of the user process need not be 00000 this approach affects the addresses that the user program can use in most cases  a user program will go through several steps-some of which may be optional-before bein.g executed  figure 8.3   addresses may be represented in different ways during these steps addresses in the source program are generally symbolic  such as count   a compiler will typically bind these symbolic addresses to relocatable addresses  such as 14 bytes from the beginning of this module   the lin.kage editor or loader will in turn bind the relocatable addresses to absolute addresses  such as 74014   each binding is a mapping from one address space to another classically  the binding of instructions and data to memory addresses can be done at any step along the way  compile time if you know at compile time where the process will reside in memory  then can be generated for example  if you krww that a user process will reside starting at location r  then the generated compiler code will start at that location and extend up from there if  at some later time  the starting location changes  then it will be necessary to recompile this code the ms-dos .com-format programs are bound at compile time load time if it is not known at compile time where the process will reside in memory  then the compiler must generate in this case  final binding is delayed until load time if the starting address changes  we need only reload the user code to incorporate this changed value execution time if the process can be moved during its execution from one memory segment to another  then binding must be delayed until run time special hardware must be available for this scheme to work  as will be discussed in section 8.1.3 most general-purpose operating systems 11se this method a major portion of this chapter is devoted to showing how these various bindings can be implemented effectively in a computer system and to discussing appropriate hardware support 8.1 compile time load time  execution time  run time  figure 8.3 multistep processing of a user program 8.1.3 logical versus physical address space an address generated by the cpu is commonly referred to as a 319 whereas an address seen by the memory unit-that is  the one loaded into the of the memory-is commonly referred to as a the compile-time and load-time address-binding methods generate identical logical and physical addresses however  the execution-time addressbinding scheme results in differing logical and addresses in this case  we usually refer to the logical address as a we use logical address and virtual address interchangeably in this text the set of all logical addresses generated by a program is a logical the set of all physical addresses corresponding to these logical addresses is a physical thus  in_ the execution-time address-binding scheme  the logical and physical address spaces differ the run-time mapping from virtual to physical addresses is done by a hardware device called the we can choose from many different methods to accomplish such mapping  as we discuss in 320 chapter 8 figure 8.4 dynamic relocation using a relocation register sections 8.3 through 8.7 for the time being  we illustrate this mapping with a simple mmu scheme that is a generalization of the base-register scheme described in section 8.1.1 the base register is now called a the value in the relocation register is added to every address generated by a user process at the time the address is sent to memory  see figure 8.4   for example  if the base is at 14000  then an attempt by the user to address location 0 is dynamically relocated to location 14000 ; an access to location 346 is mapped to location 14346 the ms-dos operating system running on the intel 80x86 family of processors used four relocation registers when loading and running processes the user program never sees the real physical addresses the program can create a pointer to location 346  store it in memory  manipulate it  and compare it with other addresses-all as the number 346 only when it is used as a memory address  in an indirect load or store  perhaps  is it relocated relative to the base register the user program deals with logical addresses the memory-mapping hardware converts logical addresses into physical addresses this form of execution-time binding was discussed in section 8.1.2 the final location of a referenced memory address is not determined until the reference is made we now have two different types of addresses  logical addresses  in the range 0 to max  and physical addresses  in the ranger + 0 tor + max for a base valuer   the user generates only logical addresses and thinks that the process runs in locations 0 to max the user program generates only logical addresses and thinks that the process runs in locations 0 to max however  these logical addresses must be mapped to physical addresses before they are used the concept of a logical address space that is bound to a separate physical address space is central to proper memory management 8.1.4 dynamic loading in our discussion so far  it has been necessary for the entire program and all data of a process to be in physical memory for the process to execute the size of a process has thus been limited to the size of physical memory to obtain better memory-space utilization  we can use dynamic with dynancic 8.1 321 loading  a routine is not loaded until it is called all routines are kept on disk in a relocatable load format the main program is loaded into memory and is executed when a routine needs to call another routine  the calling routine first checks to see whether the other routine has been loaded if it has not  the relocatable linking loader is called to load the desired routine into menwry and to update the program 's address tables to reflect this change then control is passed to the newly loaded routine the advantage of dynamic loading is that an unused routine is never loaded this method is particularly useful when large amounts of code are needed to handle infrequently occurring cases  such as error routines in this case  although the total program size may be large  the portion that is used  and hence loaded  may be much smaller dynamic loading does not require special support from the operating system it is the responsibility of the users to design their programs to take advantage of such a method operating systems may help the programmer  however  by providing library routines to implement dynamic loading 8.1.5 dynamic linking and shared libraries figure 8.3 also shows some operating systems support only linking  in system language libraries are treated like any other object module and are combined by the loader into the binary program image dynamic linking  in contrast  is similar to dynamic loading here  though  linking  rather than loading  is postponed until execution time this feature is usually used with system libraries  such as language subroutine libraries without this facility  each program on a system must include a copy of its language library  or at least the routines referenced by the program  in the executable image this requirement wastes both disk space and main memory with dynamic linking  a stub is included in the image for each libraryroutine reference the stub is a small piece of code that indicates how to locate the appropriate memory-resident library routine or how to load the library if the routine is not already present when the stub is executed  it checks to see whether the needed routine is already in memory if it is not  the program loads the routine into memory either way  the stub replaces itself with the address of the routine and executes the routine thus  the next time that particular code segment is reached  the library routine is executed directly  incurring no cost for dynamic linking under this scheme  all processes that use a language library execute only one copy of the library code this feature can be extended to library updates  such as bug fixes   a library may be replaced by a new version  and all programs that reference the library will automatically use the new version without dynamic linking  all such programs would need to be relinked to gain access to the new library so that programs will not accidentally execute new  incompatible versions of libraries  version information is included in both the program and the library more than one version of a library may be loaded into memory  and each program uses its version information to decide which copy of the library to use versions with minor changes retain the same version number  whereas versions with major changes increment the number thus  only programs that are compiled with the new library version are affected by any incompatible changes incorporated 322 chapter 8 8.2 in it other programs linked before the new library was installed will continue using the older library this system is also known as 'h  = ' unlike dynamic loading  dynamic linking generally requires help from the operating system if the processes in memory are protected from one another  then the operating system is the only entity that can check to see whether the needed routine is in another process 's memory space or that can allow multiple processes to access the same memory addresses we elaborate on this concept when we discuss paging in section 8.4.4 a process must be in memory to be executed a process  however  can be temporarily out of memory to a and then brought into memory for continued execution for example  assume a multiprogramming environment with a round-robin cpu-scheduling algorithm when a quantum expires  the memory manager will start to swap out the process that just finished and to swap another process into the memory space that has been freed  figure 8.5   in the meantime  the cpu scheduler will allocate a time slice to some other process in memory when each process finishes its quantum  it will be swapped with another process ideally  the memory manager can swap processes fast enough that some processes will be in memory  ready to execute  when the cpu scheduler wants to reschedule the cpu in addition  the quantum must be large enough to allow reasonable amounts of computing to be done between swaps a variant of this swapping policy is used for priority-based scheduling algorithms if a higher-priority process arrives and wants service  the memory manager can swap out the lower-priority process and then load and execute the higher-priority process when the higher-priority process finishes  the @ swap out @ swap in backing store main memory figure 8.5 swapping of two processes using a disk as a backing store 8.2 323 lower-priority process can be swapped back in and continued this variant of swapping is sometimes called roll normally  a process that is swapped out will be swapped back into the same memory space it occupied previously this restriction is dictated by the method of address binding if binding is done at assembly or load time  then the process can not be easily moved to a different location if execution-time binding is being used  however  then a process can be swapped into a different memory space  because the physical addresses are computed during execution time swapping requires a backing store the backing store is commonly a fast disk it must be large enough to accommodate copies of all memory images for all users  and it must provide direct access to these memory images the system maintains a consisting of all processes whose memory images are on the backing store or in memory and are ready to run whenever the cpu scheduler decides to execute a process  it calls the dispatcher the dispatcher checks to see whether the next process in the queue is in memory if it is not  and if there is no free memory region  the dispatcher swaps out a process currently in memory and swaps in the desired process it then reloads registers and transfers control to the selected process the context-switch time in such a swapping system is fairly high to get an idea of the context-switch time  let us assume that the user process is 100 mb in size and the backing store is a standard hard disk with a transfer rate of 50mb per second the actual transfer of the 100-mb process to or from main memory takes 100mb/50mb per second = 2 seconds assuming an average latency of 8 milliseconds  the swap time is 2008 milliseconds since we must both swap out and swap in  the total swap time is about 4016 milliseconds notice that the major part of the swap time is transfer time the total transfer time is directly proportional to the amount of memory swapped if we have a computer system with 4 gb of main memory and a resident operating system taking 1 gb  the maximum size of the user process is 3gb however  many user processes may be much smaller than this-say  100 mb a 100-mb process could be swapped out in 2 seconds  compared with the 60 seconds required for swapping 3 gb clearly  it would be useful to know exactly how much memory a user process is using  not simply how much it might be using then we would need to swap only what is actually used  reducing swap time for this method to be effective  the user must keep the system informed of any changes in memory requirements thus  a process with dynamic memory requirements will need to issue system calls  request memory and release memory  to inform the operating system of its changing memory needs swapping is constrained by other factors as well if we want to swap a process  we must be sure that it is completely idle of particular concern is any pending i/0 a process may be waiting for an i/0 operation when we want to swap that process to free up memory however  if the i/0 is asynchronously accessing the user memory for i/0 buffers  then the process can not be swapped assume that the i/0 operation is queued because the device is busy if we were to swap out process p1 and swap in process p2  the 324 chapter 8 8.3 i/0 operation might then attempt to use memory that now belongs to process p2  there are two main solutions to this problem  never swap a process with pending i/0  or execute i/0 operations only into operating-system buffers transfers between operating-system buffers and process memory then occur only when the process is swapped in the assumption  mentioned earlier  that swapping requires few  if any  head seeks needs further explanation we postpone discussing this issue until chapter 12  where secondary-storage structure is covered generally  swap space is allocated as a chunk of disk  separate from the file system  so that its use is as fast as possible currently  standard swapping is used in few systems it requires too much swapping time and provides too little execution time to be a reasonable memory-management solution modified versions of swapping  however  are found on many systems a modification of swapping is used in many versions of unix swapping is normally disabled but will start if many processes are running and are using a threshold amount of memory swapping is again halted when the load on the system is reduced memory management in unix is described fully in sections 21.7 and a.6 early pcs-which lacked the sophistication to implement more advanced memory-management methods-ran multiple large processes by using a modified version of swapping a prime example is the microsoft windows 3.1 operating system  which supports concurrent execution of processes in memory if a new process is loaded and there is insufficient main memory  an old process is swapped to disk this operating system does not provide full swapping  however  because the user  rather than the scheduler  decides when it is time to preempt one process for another any swapped-out process remains swapped out  and not executing  until the user selects that process to run subsequent versions of microsoft operating systems take advantage of the advanced mmu features now found in pcs we explore such features in section 8.4 and in chapter 9  where we cover virtual memory the main memory must accommodate both the operating system and the various user processes we therefore need to allocate main menlory in the most efficient way possible this section explains one common method  contiguous memory allocation the memory is usually divided into two partitions  one for the resident operating system and one for the user processes we can place the operating system in either low memory or high memory the major factor affecting this decision is the location of the interrupt vector since the interrupt vector is often in low memory  programmers usually place the operating system in low memory as well thus  in this text  we discuss only the situation in which the operating system resides in low memory the development of the other situation is similar we usually want several user processes to reside in memory at the same time we therefore need to consider how to allocate available memory to the processes that are in the input queue waiting to be brought into memory 8.3 325 in contiguous memory allocation  each process is contained in a single contiguous section of memory 8.3.1 memory mapping and protection before discussing memory allocation further  we must discuss the issue of memory mapping and protection we can provide these features by using a relocation register  as discussed in section 8.1.3  together with a limit register  as discussed in section 8.1.1 the relocation register contaitls the value of the smallest physical address ; the limit register contains the range of logical addresses  for example  relocation = 100040 and limit = 74600   with relocation and limit registers  each logical address must be less than the limit register ; the mmu maps the logical address dynamically by adding the value in the relocation register this mapped address is sent to memory  figure 8.6   when the cpu scheduler selects a process for execution  the dispatcher loads the relocation and limit registers with the correct values as part of the context switch because every address generated by a cpu is checked against these registers  we can protect both the operating system and the other users ' programs and data from being modified by this running process the relocation-register scheme provides an effective way to allow the operating system 's size to change dynamically this flexibility is desirable in many situations for example  the operating system contains code and buffer space for device drivers if a device driver  or other operating-system service  is not commonly used  we do not want to keep the code and data in memory  as we might be able to use that space for other purposes such code is sometimes called transient operating-system code ; it comes and goes as needed thus  using this code changes the size of the operating system during program execution 8.3.2 memory allocation now we are ready to turn to memory allocation one of the simplest methods for allocating memory is to divide memory into several fixed-sized each partition may contain exactly one process thus  the degree no trap  addressing error figure 8.6 hardware supportfor relocation and limit registers 326 chapter 8 of multiprogramming is bound by the number of partitions in this when a partition is free  a process is selected from the input queue and is loaded into the free partition when the process terminates  the partition becomes available for another process this method was originally used by the ibm os/360 operating system  called mft  ; it is no longer in use the method described next is a generalization of the fixed-partition scheme  called mvt  ; it is used primarily in batch environments many of the ideas presented here are also applicable to a time-sharing environment in which pure segmentation is used for memory management  section 8.6   in the scheme  the operating system keeps a table indicating which parts of memory are available and which are occupied initially  all memory is available for user processes and is considered one large block of available memory a eventually as you will see  memory contains a set of holes of various sizes as processes enter the system  they are put into an input queue the operating system takes into account the memory requirements of each process and the amount of available memory space in determining which processes are allocated memory when a process is allocated space  it is loaded into memory  and it can then compete for cpu time when a process terminates  it releases its memory which the operating system may then fill with another process from the input queue at any given time  then  we have a list of available block sizes and an input queue the operating system can order the input queue according to a scheduling algorithm memory is allocated to processes untit finally  the memory requirements of the next process can not be satisfied -that is  no available block of memory  or hole  is large enough to hold that process the operating system can then wait until a large enough block is available  or it can skip down the input queue to see whether the smaller memory requirements of some other process can be met in generat as mentioned  the memory blocks available comprise a set of holes of various sizes scattered throughout memory when a process arrives and needs memory  the system searches the set for a hole that is large enough for this process if the hole is too large  it is split into two parts one part is allocated to the arriving process ; the other is returned to the set of holes when a process terminates  it releases its block of memory  which is then placed back in the set of holes if the new hole is adjacent to other holes  these adjacent holes are merged to form one larger hole at this point  the system may need to check whether there are processes waiting for memory and whether this newly freed and recombined memory could satisfy the demands of any of these waiting processes this procedure is a particular instance of the general which concerns how to satisfy a request of size n from a there are many solutions to this problem the and strategies are the ones most commonly used to select a free hole from the set of available holes first fit allocate the first hole that is big enough searching can start either at the beginning of the set of holes or at the location where the previous first-fit search ended we can stop searching as soon as we find a free hole that is large enough 8.3 327 best fit allocate the smallest hole that is big enough we must search the entire list  unless the list is ordered by size this strategy produces the smallest leftover hole worst fit allocate the largest hole again  we must search the entire list  unless it is sorted by size this strategy produces the largest leftover hole  which may be more useful than the smaller leftover hole from a best-fit approach simulations have shown that both first fit and best fit are better than worst fit in terms of decreasing time and storage utilization neither first fit nor best fit is clearly better than the other in terms of storage utilization  but first fit is generally faster 8.3.3 fragmentation both the first-fit and best-fit strategies for memory allocation suffer from external as processes are loaded and removed from memory  the free memory space is broken into little pieces external fragmentation exists when there is enough total memory space to satisfy a request but the available spaces are not contiguous ; storage is fragmented into a large number of small holes this fragmentation problem can be severe in the worst case  we could have a block of free  or wasted  memory between every two processes if all these small pieces of memory were in one big free block instead  we might be able to run several more processes whether we are using the first-fit or best-fit strategy can affect the amount of fragmentation  first fit is better for some systems  whereas best fit is better for others  another factor is which end of a free block is allocated  which is the leftover piece-the one on the top or the one on the bottom  no matter which algorithm is used  however  external fragmentation will be a problem depending on the total amount of memory storage and the average process size  external fragmentation may be a minor or a major problem statistical analysis of first fit  for instance  reveals that  even with some optimization  given n allocated blocks  another 0.5 n blocks will be lost to fragmentation that is  one-third of memory may be unusable ! this property is known as the memory fragmentation can be internal as well as external consider a multiple-partition allocation scheme with a hole of 18,464 bytes suppose that the next process requests 18,462 bytes if we allocate exactly the requested block  we are left with a hole of 2 bytes the overhead to keep track of this hole will be substantially larger than the hole itself the general approach to avoiding this problem is to break the physical memory into fixed-sized blocks and allocate memory in units based on block size with this approach  the memory allocated to a process may be slightly larger than the requested memory the difference between these two numbers is internal memory that is internal to a partition one solution to the problem of external fragmentation is the goal is to shuffle the memory contents so as to place all free n'lemory together in one large block compaction is not always possible  however if relocation is static and is done at assembly or load time  compaction can not be done ; compaction is possible only if relocation is dynamic and is done at execution 328 chapter 8 8.4 time if addresses are relocated dynamically  relocation requires only moving the program and data and then changing the base register to reflect the new base address when compaction is possible  we must determine its cost the simplest compaction algorithm is to move all processes toward one end of memory ; all holes move in the other direction  producing one large hole of available memory this scheme can be expensive another possible solution to the external-fragmentation problem is to permit the logical address space of the processes to be noncontiguous  thus allowing a process to be allocated physical memory wherever such memory is available two complementary techniques achieve this solution  paging  section 8.4  and segmentation  section 8.6   these techniques can also be combined  section 8.7   is a memory-management scheme that permits the physical address space a process to be noncontiguous paging avoids external fragmentation and the need for compaction it also solves the considerable problem of fitting memory chunks of varying sizes onto the backin.g store ; most memorymanagement schemes used before the introduction of paging suffered from this problem the problem arises because  when some code fragments or data residing in main memory need to be swapped out  space must be fmmd on the backing store the backing store has the same fragmentation problems discussed in connection with main memory  but access is much slower  so compaction is impossible because of its advantages over earlier methods  paging in its various forms is used in most operating systems physical address foooo  0000 f1111  1111 page table figure 8.7 paging hardware 1---------1 physical memory 8.4 329 traditionally  support for paging has been handled by hardware however  recent designs have implemented paging by closely integrating the hardware and operating system  especially on 64-bit microprocessors 8.4.1 basic method the basic method for implementing paging involves breaking physical memory into fixed-sized blocks called harnes and breaking logical memory into blocks of the same size called when a process is to be executed  its pages are loaded into any available memory frames from their source  a file system or the backing store   the backing store is divided into fixed-sized blocks that are of the san1.e size as the memory frames the hardware support for paging is illustrated in figure 8.7 every address generated the cpu is divided into two parts  a  p  and a  the page number is used as an index into a the page table contains the base address of each page in physical memory this base address is combined with the page offset to define the physical memory address that is sent to the memory unit the paging model of memory is shown in figure 8.8 the page size  like the frame size  is defined by the hardware the size of a page is typically a power of 2  varying between 512 bytes and 16 mb per page  depending on the computer architecture the selection of a power of 2 as a page size makes the translation of a logical address into a page number and page offset particularly easy if the size of the logical address space is 2m  and a page size is 271 addressing units  bytes or wordst then the high-order m n bits of a logical address designate the page number  and the n low-order bits designate the page offset thus  the logical address is as follows  logical memory ~ w page table frame number physical memory figure 8.8 paging model of logical and physical memory 330 chapter 8 page number page offset d m -n n where p is an index into the page table and d is the displacement within the page as a concrete  although minuscule  example  consider the memory in figure 8.9 here  in the logical address  n = 2 and m = 4 using a page size of 4 bytes and a physical memory of 32 bytes  8 pages   we show how the user 's view of memory can be mapped into physical memory logical address 0 is page 0  offset 0 indexing into the page table  we find that page 0 is in frame 5 thus  logical address 0 maps to physical address 20  =  5 x 4  + 0   logical address 3  page 0  offset 3  maps to physical address 23  =  5 x 4  + 3   logical address 4 is page 1  offset 0 ; according to the page table  page 1 is mapped to frame 6 thus  logical address 4 maps to physical address 24  =  6 x 4  + o   logical address 13 maps to physical address 9 you may have noticed that paging itself is a form of dynamic relocation every logical address is bound by the paging hardware to some physical address using paging is similar to using a table of base  or relocation  registers  one for each frame of memory ~ m6 2 1 3 2 page table logical memory physical memory figure 8.9 paging example for a 32-byte memory with 4-byte pages 8.4 331 when we use a paging scheme  we have no external fragmentation  any free frame can be allocated to a process that needs it however  we may have some internal fragmentation notice that frames are allocated as units if the memory requirements of a process do not happen to coincide with page boundaries  the last frame allocated may not be completely full for example  if page size is 2,048 bytes  a process of 72,766 bytes will need 35 pages plus 1,086 bytes it will be allocated 36 frames  resulting in internal fragmentation of 2,048  1,086 = 962 bytes in the worst case  a process would need 11 pages plus 1 byte it would be allocated 11 + 1 frames  resulting in internal fragmentation of almost an entire frame if process size is independent of page size  we expect internal fragmentation to average one-half page per process this consideration suggests that small page sizes are desirable however  overhead is involved in each page-table entry  and this overhead is reduced as the size of the pages increases also  disk i/0 is more efficient when the amount data being transferred is larger  chapter 12   generally  page sizes have grown over time as processes  data sets  and main memory have become larger today  pages typically are between 4 kb and 8 kb in size  and some systems support even larger page sizes some cpus and kernels even support multiple page sizes for instance  solaris uses page sizes of 8 kb and 4 mb  depending on the data stored by the pages researchers are now developing support for variable on-the-fly page size usually  each page-table entry is 4 bytes long  but that size can vary as well a 32-bit entry can point to one of 232 physical page frames if frame size is 4 kb  then a system with 4-byte entries can address 244 bytes  or 16 tb  of physical memory when a process arrives in the system to be executed  its size  expressed in pages  is examined each page of the process needs one frame thus  if the process requires 11 pages  at least 11 frames must be available in memory if n frames are available  they are allocated to this arriving process the first page of the process is loaded injo one of the allocated frames  and the frame number is put in the page table for this process the next page is loaded into another frame  its frame number is put into the page table  and so on  figure 8.10   an important aspect of paging is the clear separation between the user 's view of memory and the actual physical memory the user program views memory as one single space  containing only this one program in fact  the user program is scattered throughout physical memory  which also holds other programs the difference between the user 's view of memory and the actual physical memory is reconciled by the address-translation hardware the logical addresses are translated into physical addresses this mapping is hidden from the user and is controlled by the operating system notice that the user process by definition is unable to access memory it does not own it has no way of addressing memory outside of its page table  and the table includes only those pages that the process owns since the operating system is managing physical memory  it must be aware of the allocation details of physical memory-which frames are allocated  which frames are available  how many total frames there are  and so on this information is generally kept in a data structure called a frame the frame table has one entry for each physical page frame  indicating whether the latter is free or allocated and  if it is allocated  to which page of which process or processes 332 chapter 8 free-frame list free-frame list 14 13 15 13 13 18 20 14 14 15 15 15 16 16 17 17 18 18 19 01 19 1 13 20 2 18 20 3.20 21 new-process page table 21  a   b  figure 8.10 free frames  a  before allocation and  b  after allocation in addition  the operating system must be aware that user processes operate in user space  and all logical addresses must be mapped to produce physical addresses if a user makes a system call  to do i/0  for example  and provides an address as a parameter  a buffe1 ~ for instance   that address must be mapped to produce the correct physical address the operating system maintains a copy of the page table for each process  just as it maintains a copy of the instruction counter and register contents this copy is used to translate logical addresses to physical addresses whenever the operating system must map a logical address to a physical address manually it is also used by the cpu dispatcher to define the hardware page table when a process is to be allocated the cpu paging therefore increases the context-switch time 8.4.2 hardware support each operating system has its own methods for storing page tables most allocate a page table for each process a pointer to the page table is stored with the other register values  like the instruction counter  in the process control block when the dispatcher is told to start a process  it must reload the user registers and define the correct hardware page-table values from the stored user page table the hardware implementation of the page table can be done in several in the simplest case  the page table is implemented as a set of dedicated these registers should be built with very high-speed logic to make the paging-address translation efficient every access to memory nlust go through the paging map  so efficiency is a major consideration the cpu dispatcher reloads these registers  just as it reloads the other registers instructions to load or modify the page-table registers are  of course  privileged  so that only the operating system can change the memory map the dec pdp-11 is an example of such an architecture the address consists of 16 bits  and the page size is 8 kb the page table thus consists of eight entries that are kept in fast registers 8.4 333 the use of registers for the page table is satisfactory if the page table is reasonably sncall  for example  256 entries   most contemporary computers  however  allow the page table to be very large  for example  1 million entries   for these machines  the use of fast registers to implement the page table is not feasible rather  the page table is kept in main memory  and a points to the page table changing page tables requires changing only this one register  substantially reducing context-switch time the problem with this approach is the time required to access a user memory location if we want to access location i  we must first index into the page table  using the value in the ptbr offset by the page number fori this task requires a memory access it provides us with the frame number  which is combined with the page offset to produce the actual address we can then access the desired place in memory with this scheme  two memory accesses are needed to access a byte  one for the page-table entry  one for the byte   thus  memory access is slowed by a factor of 2 this delay would be intolerable under most circumstances we might as well resort to swapping ! the standard solution to this problem is to use a special  small  fastlookup hardware cache  called a bc.1her the tlb is associative  high-speed memory each entry in the tlb consists of two parts  a key  or tag  and a value when the associative memory is presented with an item  the item is compared with all keys simultaneously if the item is found  the corresponding value field is returned the search is fast ; the hardware  however  is expensive typically  the number of entries in a tlb is small  often numbering between 64 and 1,024 the tlb is used with page tables in the following way the tlb contains only a few of the page-table entries when a logical address is generated by the cpu  its page number is presented to the tlb if the page number is found  its frame number is immediately available and is used to access memory the whole task may take less than 10 percent longer than it would if an unmapped memory reference were used if the page number is not in the tlb  known as a a memory reference to the page table must be made when the frame number is obtained  we can use it to access memory  figure 8.11   in addition  we add the page number and frame number to the tlb  so that they will be found quickly on the next reference if the tlb is already full of entries  the operating system must select one for replacement replacement policies range from least recently used  lru  to random furthermore  some tlbs allow certain entries to be meaning that they can not be removed from the tlb typically  tlb entries for kernel code are wired down some tlbs store in each tlb entry an asid uniquely identifies each process and is used to provide address-space protection for that process when the tlb attempts to resolve virtual page numbers  it ensures that the asid for the currently running process matches the asid associated with the virtual page if the asids do not match  the attempt is treated as a tlb miss in addition to providing address-space protection  an asid allows the tlb to contain entries for several different processes simultaneously if the tlb does not support separate asids  then every time a new table is selected  for instance  with each context switch   the tlb must  or erased  to ensure that the next executing process does not use the wrong translation information otherwise  the tlb could include old entries that 334 chapter 8 tlb hit tlb p tlb miss page table figure 8.11 paging hardware with tlb physical memory contain valid virtual addresses but have incorrect or invalid physical addresses left over from the previous process the percentage of times that a particular page number is found in the tlb is called the an 80-percent hit ratio  for example  means that we find the desired page number in the tlb 80 percent of the time if it takes 20 nanoseconds to search the tlb and 100 nanoseconds to access memory  then a mapped-memory access takes 120 nanoseconds when the page number is in the tlb if we fail to find the page number in the tlb  20 nanoseconds   then we must first access memory for the page table and frame number  100 nanoseconds  and then access the desired byte in memory  100 nanoseconds   for a total of 220 nanoseconds to find the effective we weight the case by its probability  effective access time = 0.80 x 120 + 0.20 x 220 = 140 nanoseconds in this example  we suffer a 40-percent slowdown in memory-access time  from 100 to 140 nanoseconds   for a 98-percent hit ratio  we have effective access time = 0.98 x 120 + 0.02 x 220 = 122 nanoseconds this increased hit rate produces only a 22 percent slowdown in access time we will further explore the impact of the hit ratio on the tlb in chapter 9 8.4 335 8.4.3 protection memory protection in a paged environment is accomplished by protection bits associated with each frame normally  these bits are kept in the page table one bit can define a page to be read-write or read-only every reference to memory goes through the page table to find the correct frame nuncber at the same time that the physical address is being computed  the protection bits can be checked to verify that no writes are being made to a read-only page an attempt to write to a read-only page causes a hardware trap to the operating system  or memory-protection violation   we can easily expand this approach to provide a finer level of protection we can create hardware to provide read-only  read-write  or execute-only protection ; or  by providing separate protection bits for each kind of access  we can allow any combination of these accesses illegal attempts will be trapped to the operating system one additional bit is generally attached to each entry in the page table  a bit when this bit is set to valid  the associated page is in the process 's logical address space and is thus a legal  or valid  page when the bit is set to invalid  the page is not in the process 's logical address space illegal addresses are trapped by use of the valid -invalid bit the operating system sets this bit for each page to allow or disallow access to the page suppose  for example  that in a system with a 14-bit address space  0 to 16383   we have a program that should use only addresses 0 to 10468 given a page size of 2 kb  we have the situation shown in figure 8.12 addresses in 0 frame number j valid-invalid bit 0 10,468 1 2,287 '-----'--'--' ' page n figure 8 i 2 valid  v  or invalid  i  bit in a page table 336 chapter 8 pages 0  1  2  3  4  and 5 are mapped normally through the page table any attempt to generate an address in pages 6 or 7  however  will find that the valid -invalid bit is set to invalid  and the computer will trap to flee operating system  invalid page reference   notice that this scheme has created a problem because the program extends only to address 10468  any reference beyond that address is illegal howeve1 ~ references to page 5 are classified as valid  so accesses to addresses up to 12287 are valid only the addresses from 12288 to 16383 are invalid this problem is a result of the 2-kb page size and reflects the internal fragmentation of paging rarely does a process use all its address range in fact many processes use only a small fraction of the address space available to them it would be wasteful in these cases to create a page table with entries for every page in the address range most of this table would be unused but would take up valuable memory space some systems provide hardware  in the form of a length to indicate the size of the page table value is checked against every logical address to verify that the address is in the valid range for the process failure of this test causes an error trap to the operating system 8.4.4 shared pages an advantage of paging is the possibility of sharing common code this consideration is particularly important in a time-sharing environment consider a system that supports 40 users  each of whom executes a text editor if the text editor consists of 150 kb of code and 50 kb of data space  we need 8,000 kb to support the 40 users if the code is  or pure however  it can be shared  as shown in figure 8.13 here we see a three-page editor-each page 50 kb in size  the large page size is used to simplify the figure  -being shared among three processes each process has its own data page reentrant code is non-self-modifying code  it never changes during execution thus  two or more processes can execute the same code at the same time each process has its own copy of registers and data storage to hold the data for the process 's execution the data for two different processes wilt of course  be different only one copy of the editor need be kept in physical memory each user 's page table maps onto the same physical copy of the editor  but data pages are mapped onto different frames thus  to support 40 users  we need only one copy of the editor  150 kb   plus 40 copies of the 50 kb of data space per user the total space required is now 2  50 kb instead of 8,000 kb-a significant savings other heavily used programs can also be shared -compilers  window systems  run-time libraries  database systems  and so on to be sharable  the code must be reentrant the read-only nature of shared code should not be left to the correctness of the code ; the operating system should enforce this property the sharing of memory among processes on a system is similar to the sharing of the address space of a task by threads  described in chapter 4 furthermore  recall that in chapter 3 we described shared memory as a method 8.5 ed 1  ed 2 ed 3 data .1 process p1 process p3 page table for p1 page table for p3 8.5 ed 1 ed 2 ed 3 data 2 process p2 0 data 1 2 data 3 3 ed 1 ed 2 ed 3  4 5 6 data 2 page table for p2 7 8 9 10 11 figure 8.13 sharing of code in a paging environment 337 of interprocess corrununication some operating systems implement shared memory using shared pages organizing memory according to pages provides numerous benefits in addition to allowing several processes to share the same physical pages we cover several other benefits in chapter 9 in this section  we explore some of the most common techniques for structuring the page table 8.5.1 hierarchical paging most modern computer systems support a large logical address space  232 to 264   in such an environment  the page table itself becomes excessively large for example  consider a system with a 32-bit logical address space if the page size in such a system is 4 kb  212   then a page table may consist of up to 1 million entries  232 /212   assuming that each entry consists of 4 bytes  each process may need up to 4mb of physical address space for the page table alone clearly  we would not want to allocate the page table contiguously in main memory one simple solution to this problem is to divide the page table into smaller pieces we can accomplish this division in several ways one way is to use a two-level paging algorithm  in which the page table itself is also paged  figure 8.14   for example  consider again the system with 338 chapter 8 0 page table memory figure 8.14 a two-level page-table scheme a 32-bit logical address space and a page size of 4 kb a logical address is divided into a page number consisting of 20 bits and a page offset consisting of 12 bits because we page the page table  the page number is further divided into a 10-bit page number and a 10-bit page offset thus  a logical address is as follows  page number page offset d 10 10 12 where p1 is an index into the outer page table and p2 is the displacement within the page of the outer page table the address-translation method for this architecture is shown in figure 8.15 because address translation works from the outer page table inward  this scheme is also known as a the vax architecture supports a variation of two-level paging the vax is a 32-bit machine with a page size of 512 bytes the logical address space of a process is divided into four equal sections  each of which consists of 230 bytes each section represents a different part of the logical address space of a process the first 2 high-order bits of the logical address designate the appropriate section the next 21 bits represent the logical page number of that section  and the final 9 bits represent an offset in the desired page by partitioning the page outer page table 8.5 figure 8 15 address translation for a two-level 32-bit paging architecture 339 table in this manner  the operating system can leave partitions unused until a process needs them an address on the vax architecture is as follows  section page offset s p d 2 21 9 where s designates the section number  p is an index into the page table  and d is the displacement within the page even when this scheme is used  the size of a one-level page table for a vax process using one section is 221 bits 4 bytes per entry = 8mb to further reduce main-memory use  the vax pages the user-process page tables for a system with a 64-bit logical address space  a two-level paging scheme is no longer appropriate to illustrate this point  let us suppose that the page size in such a system is 4 kb  212   in this case  the page table consists of up to 252 entries if we use a two-level paging scheme  then the iml.er page tables can conveniently be one page long  or contain 210 4-byte entries the addresses look like this  outer page inner page offset i  pl  i p2  i d 42 10 12 the outer page table consists of 242 entries  or 244 bytes the obvious way to avoid such a large table is to divide the outer page table into smaller pieces  this approach is also used on some 32-bit processors for added flexibility and efficiency  we can divide the outer page table in various ways we can page the outer page table  giving us a three-level paging scheme suppose that the outer page table is made up of standard-size pages  210 entries  or 212 bytes   in this case  a 64-bit address space is still daunting  2nd outer page outer page inner page offset i pr   p2 i p3 i d 32 10 10 12 the outer page table is sti11234 bytes in size 340 chapter 8 the next step would be a four-level paging scheme  where the second-level outer page table itself is also paged  and so forth the 64-bit ultrasparc would require seven levels of paging-a prohibitive number of memory accessesto translate each logical address you can see from this example why  for 64-bit architectures  hierarchical page tables are generally considered inappropriate 8.5.2 hashed page tables a common approach for handling address spaces larger than 32 bits is to use a with the hash value being the virtual page number each entry in the hash table contains a linked list of elements that hash to the same location  to handle collisions   each element consists of three fields   1  the virtual page number   2  the value of the mapped page frame  and  3  a pointer to the next element in the linked list the algorithm works as follows  the virtual page number in the virtual address is hashed into the hash table the virtual page number is compared with field 1 in the first element in the linked list if there is a match  the corresponding page frame  field 2  is used to form the desired physical address if there is no match  subsequent entries in the linked list are searched for a matching virtual page number this scheme is shown in figure 8.16 a variation of this scheme that is favorable for 64-bit address spaces has been proposed this variation uses which are similar to hashed page tables except that each entry in the hash table refers to several pages  such as 16  rather than a single page therefore  a single page-table entry can store the mappings for multiple physical-page frames clustered page tables are particularly useful for address spaces  where memory references are noncontiguous and scattered throughout the address space 8.5.3 inverted page tables usually  each process has an associated page table the page table has one entry for each page that the process is using  or one slot for each virtual hash table figure 8.16 hashed page table physical address physical memory 8.5 341 address  regardless of the latter 's validity   this table representation is a natural one  since processes reference pages through the pages ' virtual addresses the operating system must then translate this reference into a physical memory address since the table is sorted by virtual address  the operating system is able to calculate where in the table the associated physical address entry is located and to use that value directly one of the drawbacks of this method is that each page table may consist of millions of entries these tables may consume large amounts of physical memory just to keep track of how other physical memory is being used to solve this problem  we can use an page an inverted page table has one entry for each real page  or frame  of memory each entry consists of the virtual address of the page stored in that real memory location  with information about the process that owns the page thus  only one page table is in the system  and it has only one entry for each page of physical memory figure 8.17 shows the operation of an inverted page table compare it with figure 8.7  which depicts a standard page table in operation inverted page tables often require that an address-space identifier  section 8.4.2  be stored in each entry of the page table  since the table usually contains several different address spaces mapping physical memory storing the address-space identifier ensures that a logical page for a particular process is mapped to the corresponding physical page frame examples of systems using inverted page tables include the 64-bit ultrasparc and powerpc to illustrate this method  we describe a simplified version of the i11verted page table used in the ibm rt each virtual address in the system consists of a triple  process-id  page-number  offset  each inverted page-table entry is a pair process-id  page-number where the process-id assumes the role of the address-space identifier when a memory page table physical address figure 8.17 inverted page table physical memory 342 chapter 8 8.6 reference occurs  part of the virtual address  consisting of process-id  pagenumber  is presented to the memory subsystem the inverted page table is then searched for a match if a match is found-say  at entry i-then the physical address i  offset is generated if no match is found  then an illegal address access has been attempted although this scheme decreases the amount of memory needed to store each page table  it increases the amount of time needed to search the table when a page reference occurs because the inverted page table is sorted by physical address  but lookups occur on virtual addresses  the whole table might need to be searched for a match this search would take far too long to alleviate this problem  we use a hash table  as described in section 8.5.2  to limit the search to one-or at most a few-page-table entries of course  each access to the hash table adds a memory reference to the procedure  so one virtual memory reference requires at least two real memory reads-one for the hash-table entry and one for the page table  recall that the tlb is searched first  before the hash table is consulted  offering some performance improvement  systems that use inverted page tables have difficulty implementing shared memory shared memory is usually implemented as multiple virtual addresses  one for each process sharing the memory  that are mapped to one physical address this standard method can not be used with inverted page tables ; because there is only one virtual page entry for every physical page  one physical page can not have two  or more  shared virtual addresses a simple technique for addressing this issue is to allow the page table to contain only one mapping of a virtual address to the shared physical address this means that references to virtual addresses that are not mapped result in page faults an important aspect of memory management that became unavoidable with paging is the separation of the user 's view of memory from the actual physical memory as we have already seen  the user 's view of memory is not the same as the actual physical memory the user 's view is mapped onto physical memory this mapping allows differentiation between logical memory and physical memory 8.6.1 basic method do users think of memory as a linear array of bytes  some containing instructions and others containing data most people would say no rather  users prefer to view memory as a collection of variable-sized segments  with no necessary ordering among segments  figure 8.18   consider how you think of a program when you are writing it you think of it as a main program with a set of methods  procedures  or functions it may also include various data structures  objects  arrays  stacks  variables  and so on each of these modules or data elements is referred to by name you talk about the stack  the math library  the n1.ain program  without caring what addresses in memory these elements occupy you are not concerned with whether the stack is stored before or after the sqrt   function each of these segments is of variable length ; the length is intrinsically defined by subroutine symbol table  main program logical address 8.6 figure 8.18 user 's view of a program 343 the purpose of the segment in the program elements within a segment are identified by their offset from the begim1.ing of the segment  the first statement of the program  the seventh stack frame entry in the stack  the fifth instruction of the sqrt    and so on is a memory-management scheme that supports this user view of memory a logical address space is a collection of segments each segment has a name and a length the addresses specify both the segment name and the offset within the segment the user therefore specifies each address by two quantities  a segment name and an offset  contrast this scheme with the paging scheme  in which the user specifies only a single address  which is partitioned by the hardware into a page number and an offset  all invisible to the programmer  for simplicity of implementation  segments are numbered and are referred to by a segn lent number  rather than by a segment name thus  a logical address consists of a two tuple  segment-number  offset  normally  the user program is compiled  and the compiler automatically constructs segments reflecting the input program a c compiler might create separate segments for the following  the code global variables the heap  from which memory is allocated the stacks used by each thread the standard c library 344 chapter 8 no segment table yes trap  addressing error + figure 8.19 segmentation hardware physical memory libraries that are linked in during compile time might be assign.ed separate segments the loader would take all these segments and assign them segment numbers 8.6.2 hardware although the user can now refer to objects in the program by a two-dimensional address  the actual physical memory is still  of course  a one-dimensional sequence of bytes thus  we must define an implementation to map twodimensional user-defined addresses into one-dimensional physical addresses this mapping is effected by a each entry in the segment table has a segment base and a segment limit the segment base contains the startilcg physical address where the segment resides in memory  and the segment limit specifies the length of the segment the use of a segment table is illustrated in figure 8.19 a logical address consists of two parts  a segment number  s  and an offset into that segment  d the segment number is used as an index to the segment table the offset d of the logical address must be between 0 and the segment limit if it is not  we trap to the operating system  logical addressing attempt beyond end of segment   when an offset is legal  it is added to the segment base to produce the address in physical memory of the desired byte the segment table is thus essentially an array of base-limit register pairs as an example  consider the situation shown in figure 8.20 we have five segments numbered from 0 through 4 the segments are stored in physical memory as shown the segment table has a separate entry for each segment  giving the beginning address of the segment in physical memory  or base  and the length of that segment  or limit   for example  segment 2 is 400 bytes long and begins at location 4300 thus  a reference to byte 53 of segment 2 is mapped 8.7 subroutine segment o segment1 symbol table  segment 4 main program segment 2 logical address space 8.7 0 2 3 4 limit base 1000 1400 400 6300 400 4300 1100 3200 1000 4700 segment table figure 8.20 example of segmentation 14001---1 segment o 2400 3200 1-----1 segment 3 4300 1 ~ --1 4700 segment 2 segment 4 5700 f--------1 6300   s ~ gt \ 1e ! it 1 6700 physical memory 345 onto location 4300 + 53 = 4353 a reference to segment 3  byte 852  is mapped to 3200  the base of segment 3  + 852 = 4052 a reference to byte 1222 of segment 0 would result in a trap to the operating system  as this segment is only tooo bytes long both paging and segmentation have advantages and disadvantages in fact some architectures provide both in this section  we discuss the intel pentium architecture  which supports both pure segmentation and segmentation with paging we do not give a complete description of the memory-management structure of the pentium in this text rather  we present the major ideas on which it is based we conclude our discussion with an overview of linux address translation on pentium systems in pentium systems  the cpu generates logical addresses  which are given to the segmentation unit the segmentation unit produces a linear address for each logical address the linear address is then given to the paging unit  which in turn generates the physical address in main memory thus  the segmentation and paging units form the equivalent of the memory-management unit  mmu   this scheme is shown in figure 8.21 8.7.1 pentium segmentation the pentium architecture allows a segment to be as large as 4 gb  and the maximum number of segments per process is 16 k the logical-address space 346 chapter 8 i cpu i figure 8.21 logical to physical address translation in the pentium of a process is divided into two partitions the first partition consists of up to 8 k segments that are private to that process the second partition consists of up to 8 k segments that are shared all the processes information about the first partition is kept in the information about the second partition is kept in the each entry in the ldt and gdt consists of an 8-byte segment descriptor with detailed information about a particular segment  including the base location and limit of that segment the logical address is a pair  selector  offset   where the selector is a 16-bit number  g p 13 2 in which s designates the segment number  g indicates whether the segment is in the gdt or ldt  and p deals with protection the offset is a 32-bit number specifying the location of the byte  or word  within the segment in question the machine has six segment registers  allowing six segments to be addressed at any one time by a process it also has six 8-byte microprogram registers to hold the corresponding descriptors from either the ldt or gdt this cache lets the pentium avoid having to read the descriptor from memory for every memory reference the linear address on the pentium is 32 bits long and is formed as follows the segment register points to the appropriate entry in the ldt or gdt the base and limit information about the segment in question is used to generate a first  the limit is used to check for address validity if the address is not valid  a memory fault is generated  resulting in a trap to the operating system if it is valid  then the value of the offset is added to the value of the base  resulting in a 32-bit linear address this is shown in figure 8.22 in the following section  we discuss how the paging unit turns this linear address into a physical address 8.7.2 pentium paging the pentium architecture allows a page size of either 4 kb or 4 mb for 4-kb pages  the pentium uses a two-level paging schence in which the division of the 32-bit linear address is as follows  page number page offset d 10 10 12 the address-translation scheme for this architecture is similar to the scheme shown in figure 8.15 the intel pentium address translation is shown in more 8.7 347 logical address offset + 32-bit linear address figure 8.22 intel pentium segmentation detail in figure 8.23 the 10 high-order bits reference an entry in the outern'lost page table  which the pentium terms the page directory  the cr3 register points to the page directory for the current process  the page directory entry points to an inner page table that is indexed by the contents of the innermost 10 bits in the linear address finally  the low-order bits 0-11 refer to the offset in the 4-kb page pointed to in the page table one entry in the page directory is the page size flag  which-if setindicates that the size of the page frame is 4 mb and not the standard 4 kb if this flag is set  the page directory points directly to the 4-mb page frame  bypassing the inner page table ; and the 22 low-order bits in the linear address refer to the offset in the 4-mb page frame 31 cr3 registe r page directory page directory page directory  logical address  page table 22 21 l 1211 page table  i offset 31 22 21 offset j 4-kb page 4-mb page figure 8.23 paging in the pentium architecture 0 0 3l ! 8 chapter 8 to improve the efficiency of physical memory use  intel pentium page tables can be swapped to disk in this case  an invalid bit is used in the page directory entry to indicate whether the table to which the entry is pointing is in memory or on disk if the table is on disk  the operating system can use the other 31 bits to specify the disk location of the table ; the table then can be brought into memory on demand 8.7.3 linux on pentium systems as an illustration  consider the linux operating system running on the intel pentium architecture because linux is designed to run on a variety of processors many of which may provide only limited support for segmentationlinux does not rely on segmentation and uses it minimally on the pentium  linux uses only six segments  a segment for kernel code a segment for kernel data a segment for user code a segment for user data a task-state segment  tss  1i a default ldt segment the segments for user code and user data are shared by all processes running in user mode this is possible because all processes use the same logical address space and all segment descriptors are stored in the global descriptor table  gdt   furthermore  each process has its own task-state segment  tss   and the descriptor for this segment is stored in the gdt the tss is used to store the hardware context of each process during context switches the default ldt segment is normally shared by all processes and is usually not used however  if a process requires its own ldt  it can create one and use that instead of the default ldt as noted  each segment selector includes a 2-bit field for protection thus  the pentium allows four levels of protection of these four levels  limlx only recognizes two  user mode and kernel mode although the pentium uses a two-level paging model  linux is designed to run on a variety of hardware platforms  many of which are 64-bit platforms where two-level paging is not plausible therefore  linux has adopted a threelevel paging strategy that works well for both 32-bit and 64-bit architectures the linear address in linux is broken into the following four parts  global directory middle directory page table figure 8.24 highlights the three-level paging model in linux the number of bits in each part of the linear address varies according to architecture however  as described earlier in this section  the pentium architecture only uses a two-level paging model how  then  does linux apply 8.8 lglobal directory global directory cr3 __,.c__ ___ __l register 8.8  linear address  middle directory figure 8.24 three-level paging in linux offset page frame 349 its three-level model on the pentium in this situation  the size of the middle directory is zero bits  effectively bypassing the middle directory each task in linux has its own set of page tables and -just as in figure 8.23 -the cr3 register points to the global directory for the task currently executing during a context switch  the value of the cr3 register is saved and restored in the tss segments of the tasks involved in the context switch memory-management algorithms for multiprogrammed operating systems range from the simple single-user system approach to paged segmentation the most important determinant of the method used in a particular system is the hardware provided every memory address generated by the cpu must be checked for legality and possibly mapped to a physical address the checking can not be implemented  efficiently  in software hence  we are constrained by the hardware available the various memory-management algorithms  contiguous allocation  paging  segmentation  and combinations of paging and segmentation  differ in many aspects in comparing different memory-management strategies  we use the following considerations  hardware support a simple base register or a base-limit register pair is sufficient for the single and multiple-partition schemes  whereas paging and segmentation need mapping tables to define the address map performance as the memory-management algorithm becomes more complex  the time required to map a logical address to a physical address increases for the simple systems  we need only compare or add to the logical address-operations that are fast paging and segmentation can be as fast if the mapping table is implemented in fast registers if the table is 350 chapter 8 in memory  however  user memory accesses can be degraded substantially a tlb can reduce the performance degradation to an acceptable level fragmentation a multiprogrammed system will generally perform more efficiently if it has a higher level of multiprogramming for a given set of processes  we can increase the multiprogramming level only by packing more processes into memory to accomplish this task  we must reduce memory waste  or fragmentation systems with fixed-sized allocation units  such as the single-partition scheme and paging  suffer from internal fragmentation systems with variable-sized allocation units  such as the multiple-partition scheme and segmentation  suffer from external fragmentation relocation one solution to the external-fragmentation problem is compaction compaction involves shifting a program in memory in such a way that the program does not notice the change this consideration requires that logical addresses be relocated dynamically  at execution time if addresses are relocated only at load time  we can not compact storage swapping swapping can be added to any algorithm at intervals determined by the operating system  usually dictated by cpu-scheduling policies  processes are copied from main memory to a backing store and later are copied back to main memory this scheme allows more processes to be run than can be fit into memory at one time sharing another means of increasing the multiprogramming level is to share code and data among different users sharing generally requires that either paging or segmentation be used to provide small packets of information  pages or segments  that can be shared sharing is a means of running many processes with a limited amount of memory  but shared programs and data must be designed carefully protection if paging or segmentation is provided  different sections of a user program can be declared execute-only  read -only  or read-write this restriction is necessary with shared code or data and is generally useful in any case to provide simple run-time checks for common programming errors 8.1 explain the difference between internal and external fragmentation 8.2 compare the memory organization schemes of contiguous memory allocation  pure segmentation  and pure paging with respect to the following issues  a external fragmentation b internal fragmentation c ability to share code across processes 351 8.3 why are segmentation and paging sometimes combined into one scheme 8.4 most systems allow a program to allocate more memory to its address space during execution allocation of data in the heap segments of programs is an example of such allocated memory what is required to support dynamic memory allocation in the following schemes a contiguous memory allocation b pure segmentation c pure paging 8.5 consider the intel address-translation scheme shown in figure 8.22 a describe all the steps taken by the intel pentium in translatil g a logical address into a physical address b what are the advantages to the operating system of hardware that provides such complicated memory translation c are there any disadvantages to this address-translation system if so  what are they if not  why is this scheme not used by every manufacturer 8.6 what is the purpose of paging the page tables 8.7 explain why sharil g a reentrant module is easier when segmentation is used than when pure paging is used 8.8 on a system with paging  a process can not access memory that it does not own why how could the operating system allow access to other memory why should it or should it not 8.9 compare the segmented pagil g scheme with the hashed page table scheme for handling large address spaces under what circumstances is one scheme preferable to the other 8.10 consider a paging system with the page table stored in memory a if a memory reference takes 200 nanoseconds  how long does a paged memory reference take b if we add tlbs  and 75 percent of all page-table references are found in the tlbs  what is the effective memory reference time  assume that finding a page-table entry in the tlbs takes zero time  if the entry is there  352 chapter 8 8.11 compare paging with segmentation with respect to the amount of memory required by the address translation structures in order to convert virtual addresses to physical addresses 8.12 consider a system in which a program can be separated into two parts  code and data the cpu knows whether it wants an instruction  instruction fetch  or data  data fetch or store   therefore  two baselimit register pairs are provided  one for instructions and one for data the instruction base-limit register pair is automatically read-only  so programs can be shared among different users discuss the advantages and disadvantages of this scheme 8.13 consider the following process for generating binaries a compiler is used to generate the object code for individual modules  and a linkage editor is used to combine multiple object modules into a single program bilcary how does the linkage editor change the bindmg of instructions and data to memory addresses what information needs to be passed from the compiler to the linkage editor to facilitate the memory-binding tasks of the linkage editor 8.14 consider a logical address space of 64 pages of 1,024 words each  mapped onto a physical memory of 32 frames a how many bits are there in the logical address b how many bits are there in the physical address 8.15 consider the hierarchical paging scheme used by the vax architecture how many memory operations are performed when a user program executes a memory-load operation 8.16 given five memory partitions of 100 kb  500 kb  200 kb  300 kb  and 600 kb  ill order   how would the first-fit  best-fit  and worst-fit algorithms place processes of 212 kb  417 kb  112 kb  and 426 kb  in order  which algorithm makes the most efficient use of memory 8.17 describe a mechanism by which one segment could belong to the address space of two different processes 8.18 consider a computer system with a 32-bit logical address and 4-kb page size the system supports up to 512mb of physical memory how many entries are there in each of the following a a conventional single-level page table b an inverted page table 353 8.19 assuming a 1-kb page size  what are the page numbers and offsets for the following address references  provided as decimal numbers   a 2375 b 19366 c 30000 d 256 e 16385 8.20 program binaries in many systems are typically structured as follows code is stored starting with a small  fixed virtual address  such as 0 the code segment is followed by the data segment that is used for storing the program variables when the program starts executing  the stack is allocated at the other end of the virtual address space and is allowed to grow toward lower virtual addresses what is the significance of this structure for the following schemes a contiguous memory allocation b pure segmentation c pure paging 8.21 consider the following segment table  segment base length 0 219 600 1 2300 14 2 90 100 3 1327 580 4 1952 96 what are the physical addresses for the following logical addresses a 0,430 b 1,10 c 2,500 d 3,400 e 4,112 8.22 consider a logical address space of 32 pages with 1,024 words per page  mapped onto a physical memory of 16 frames a how many bits are required in the logical address b how many bits are required in the physical address 354 chapter 8 8.23 sharing segments among processes without requiring that they have the same segment number is possible in a dynamically linked segmentation system a define a system that allows static linking and sharing of segments without requiring that the segment numbers be the same b describe a paging scheme that allows pages to be shared without requiring that the page numbers be the same 8.24 assume that a system has a 32-bit virtual address with a 4-kb page size write a c program that is passed a virtual address  in decincal  on the command line and have it output the page number and offset for the given address as an example  your program would run as follows  ./a.out 19986 your program would output  the address 19986 contains  page number = 4 offset = 3602 writing this program will require using the appropriate data type to store 32 bits we encourage you to use unsigned data types as well dynamic storage allocation was discussed by knuth  1973   section 2.5   who found through simulation results that first fit is generally superior to best fit knuth  1973  also discussed the 50-percent rule the concept of paging can be credited to the designers of the atlas system  which has been described by kilburn et al  1961  and by howarth et al  1961   the concept of segmentation was first discussed by dennis  1965   paged segmentation was first supported in the ge 645  on which multics was originally implemented  organick  1972  and daley and dennis  1967    inverted page tables are discussed in an article about the ibm rt storage manager by chang and mergen  1988   address translation in software is covered in jacob and mudge  1997   hennessy and patterson  2002  explains the hardware aspects of tlbs  caches  and mmus talluri et al  1995  discusses page tables for 64-bit address spaces alternative approaches to enforcing memory protection are proposed and studied in wahbe et al  1993a   chase et al  1994   bershad et al  1995   and thorn  1997   dougan et al  1999  and jacob and mudge  2001  discuss 355 tedmiques for managing the tlb fang et al  2001  evaluate support for large pages tanenbaum  2001  discusses intel80386 paging memory management for several architectures-such as the pentiunl ii  powerpc  and ultrasparcare described by jacob and mudge  1998a   segmentation on lim1x systems is presented in bovet and cesati  2002   9.1 c er in chapter 8  we discussed various memory-management strategies used in computer systems all these strategies have the same goal  to keep many processes in memory simultaneously to allow multiprogramming however  they tend to require that an entire process be in memory before it can execute virtual memory is a tecrucique that allows the execution of processes that are not completely in memory one major advantage of this scheme is that programs can be larger than physical memory further  virtual memory abstracts main memory into an extremely large  uniform array of storage  separating logical memory as viewed by the user from physical memory this technique frees programmers from the concerns of memory-storage limitations virtual memory also allows processes to share files easily and to implement shared memory in addition  it provides an efficient mechanism for process creation virtual memory is not easy to implement  however  and may substantially decrease performance if it is used carelessly in this chapter  we discuss virtual memory in the form of demand paging and examine its complexity and cost to describe the benefits of a virtual memory system to explain the concepts of demand paging  page-replacement algorithms  and allocation of page frames to discuss the principles of the working-set model the memory-management algorithms outlined in chapter 8 are necessary because of one basic requirement  the instructions being executed must be in physical memory the first approach to meeting this requirement is to place the entire logical address space in physical memory dynamic loading can help to ease this restriction  but it generally requires special precautions and extra work by the programmer 357 358 chapter 9 the requirement that instructions m.ust be in physical memory to be executed seems both necessary and reasonable ; but it is also unfortunate  since it limits the size of a program to the size of physical memory in fact  an examination of real programs shows us that  in many cases  the entire program is not needed for instance  consider the following  programs often have code to handle unusual error conditions since these errors seldom  if ever  occur in practice  this code is almost never executed arrays,lists  and tables are often allocated more memory than they actually need an array may be declared 100 by 100 elements  even though it is seldom larger than 10 by 10 elements an assembler symbol table may have room for 3,000 symbols  although the average program has less than 200 symbols certain options and features of a program may be used rarely for instance  the routines on u.s government computers that balance the budget have not been used in many years even in those cases where the entire program is needed  it may not all be needed at the same time the ability to execute a program that is only partially in memory would confer many benefits  a program would no longer be constrained by the amount of physical memory that is available users would be able to write programs for an extremely large virtual address space  simplifying the programming task page 0 page 1 page 2 page v virtual memory memory map physical memory figure 9.1 diagram showing virtual memory that is larger than physical memory 9.1 359 because each user program could take less physical memory  more programs could be run at the sance time  with a corresponding increase in cpu utilization and throughput but with no increase in response time or turnaround time less i/o would be needed to load or swap user programs into memory  so each user program would run faster thus  running a program that is not entirely in memory would benefit both the system and the user involves the separation of logical memory as perceived by users from physical memory this separation allows an extremely large virtual memory to be provided for programmers when only a smaller physical memory is available  figure 9.1   virtual memory makes the task of programming much easier  because the programmer no longer needs to worry about the amount of physical memory available ; she can concentrate instead on the problem to be programmed the address space of a process refers to the logical  or virtual  view of how a process is stored in memory typically  this view is that a process begins at a certain logical address-say  address 0-and exists in contiguous memory  as shown in figure 9.2 recall from chapter 8  though  that in fact physical memory may be organized in page frames and that the physical page frames assigned to a process may not be contiguous it is up to the memorymanagement unit  mmu  to map logical pages to physical page frames in memory note in figure 9.2 that we allow for the heap to grow upward in memory as it is used for dynamic memory allocation similarly  we allow for the stack to grow downward in memory through successive function calls the large blank space  or hole  between the heap and the stack is part of the virtual address figure 9.2 virtual address space 360 chapter 9 space but will require actual physical pages only if the heap or stack grows virtual address spaces that include holes are known as sparse address spaces using a sparse address space is beneficial because the holes can be filled as the stack or heap segments grow or if we wish to dynam.ically link libraries  or possibly other shared objects  during program execution in addition to separating logical memory from physical memory  virtual memory allows files and memory to be shared by two or more processes through page sharing  section 8.4.4   this leads to the following benefits  system libraries can be shared by several processes through mapping of the shared object into a virtual address space although each process considers the shared libraries to be part of its virtual address space  the actual pages where the libraries reside in physical memory are shared by all the processes  figure 9.3   typically  a library is mapped read-only into the space of each process that is linked with it similarly  virtual memory enables processes to share memory recall from chapter 3 that two or more processes can communicate through the use of shared memory virtual memory allows one process to create a region of memory that it can share with another process processes sharing this region consider it part of their virtual address space  yet the actual physical pages of memory are shared  much as is illustrated in figure 9.3 virtual memory can allow pages to be shared during process creation with the fork   system calt thus speeding up process creation we further explore these-and other-benefits of virtual memory later in this chapter first though  we discuss implementing virtual memory through demand paging shared library shared pages shared library figure 9.3 shared library using virtual memory 9.2 9.2 361 consider how an executable program might be loaded from disk into n'lemory one option is to load the entire program in physical memory at program execution time however  a problent with this approach is that we may not initially need the entire program in memory suppose a program starts with a list of available options from which the user is to select loading the entire program into memory results in loading the executable code for all options  regardless of whether an option is ultimately selected by the user or not an alternative strategy is to load pages only as they are needed this technique is known as paging and is commonly used in virtual memory systems with demand-paged virtual memory  pages are only loaded when they are demanded during program execution ; pages that are never accessed are thus never loaded into physical memory a demand-paging system is similar to a paging system with swapping  figure 9.4  where processes reside in secondary memory  usually a disk   when we want to execute a process  we swap it into memory rather than swapping the entire process into memory  however  we use a a lazy swapper never swaps a page into memory unless that page will be needed since we are now viewing a process as a sequence of pages  rather than as one large contiguous address space  use of the term swapper is technically incorrect a swapper manipulates entire processes  whereas a is concerned with the individual pages of a process we thus use pager  rather than swapper  in connection with demand paging program a program b main memory swap out so 90100110 120130140150 swap in 16017 figure 9.4 transfer of a paged memory to contiguous disk space 362 chapter 9 9.2.1 basic concepts when a process is to be swapped in  the pager guesses which pages will be used before the process is swapped out again instead of swapping in a whole process  the pager brings only those pages into memory thus  it avoids reading into memory pages that will not be used anyway  decreasing the swap time and the amount of physical memory needed with this scheme  we need some form of hardware support to distinguish between the pages that are in memory and the pages that are on the disk the valid -invalid bit scheme described in section 8.4.3 can be used for this purpose this time  however  when this bit is set to valid/ ' the associated page is both legal and in n1.emory if the bit is set to invalid/ ' the page either is not valid  that is  not in the logical address space of the process  or is valid but is currently on the disk the page-table entry for a page that is brought into memory is set as usuat but the page-table entry for a page that is not currently in memory is either simply marked invalid or contains the address of the page on disk this situation is depicted in figure 9.5 notice that marking a page invalid will have no effect if the process never attempts to access that page hence  if we guess right and page in all and only those pages that are actually needed  the process will run exactly as though we had brought in all pages while the process executes and accesses pages that are execution proceeds normally 0 2 3 4 5 6 7 valid-invalid frame bit ' \  i 0 4 v logical memory physical memory dod d  1j   @ jtb  odd figure 9.5 page table when some pages are not in main memory operating system reference   ; \  page is on \   v backing store trap restart instruction page table reset page table physical memory 9.2 0 bring in missing page figure 9.6 steps in handling a page fault 363 but what happens if the process tries to access a page that was not brought into memory access to a page marked invalid causes a the paging hardware  in translating the address through the page table  will notice that the invalid bit is set  causing a trap to the operating system this trap is the result of the operating system 's failure to bring the desired page into memory the procedure for handling this page fault is straightforward  figure 9.6   we check an internal table  usually kept with the process control block  for this process to determine whether the reference was a valid or an invalid memory access if the reference was invalid  we terminate the process if it was valid  but we have not yet brought in that page  we now page it in we find a free frame  by taking one from the free-frame list  for example   we schedule a disk operation to read the desired page into the newly allocated frame when the disk read is complete  we modify the internal table kept with the process and the page table to indicate that the page is now in memory we restart the instruction that was interrupted by the trap the process can now access the page as though it had always been in memory in the extreme case  we can start executing a process with no pages in memory when the operating system sets the instruction pointer to the first 364 chapter 9 instruction of the process  which is on a non-memory-resident page  the process immediately faults for the page after this page is brought into memory  the process continues to execute  faulting as necessary until every page that it needs is in memory at that it can execute with no more faults this scheme is never bring a page into memory until it is required theoretically  some programs could access several new pages of memory with each instruction execution  one page for the instruction and many for data   possibly causing multiple page faults per instruction this situation would result in unacceptable system performance fortunately  analysis of running processes shows that this behavior is exceedingly unlikely programs tend to have described in section 9.6.1  which results in reasonable performance from demand paging the hardware to support demand paging is the same as the hardware for paging and swapping  page table this table has the ability to mark an entry invalid through a valid -invalid bit or a special value of protection bits secondary memory this memory holds those pages that are not present in main memory the secondary memory is usually a high-speed disk it is known as the swap device  and the section of disk used for this purpose is known as swap-space allocation is discussed in chapter 12 a crucial requirement for demand paging is the ability to restart any instruction after a page fault because we save the state  registers  condition code  instruction counter  of the interrupted process when the page fault occurs  we must be able to restart the process in exactly the same place and state  except that the desired page is now in memory and is accessible in most cases  this requirement is easy to meet a page fault may occur at any memory reference if the page fault occurs on the instruction fetch  we can restart by fetching the instruction again if a page fault occurs while we are fetching an operand  we must fetch and decode the instruction again and then fetch the operand as a worst-case example  consider a three-address instruction such as add the content of a to b  placing the result in c these are the steps to execute this instruction  fetch and decode the instruction  add   fetch a fetch b add a and b store the sum in c if we fault when we try to store inc  because c is in a page not currently in memory   we will have to get the desired page  bring it in  correct the page table  and restart the instruction the restart will require fetching the instruction again  decoding it again  fetching the two operands again  and 9.2 365 then adding again however  there is not much repeated work  less than one complete instruction   and the repetition is necessary only when a page fault occurs the major difficulty arises when one instruction may modify several different locations for example  consider the ibm system 360/370 mvc  move character  instruction  which can ncove up to 256 bytes from one location to another  possibly overlapping  location if either block  source or destination  straddles a page boundary  a page fault might occur after the move is partially done in addition  if the source and destination blocks overlap  the source block may have been modified  in which case we can not simply restart the instruction this problem can be solved in two different ways in one solution  the microcode computes and attempts to access both ends of both blocks if a page fault is going to occm ~ it will happen at this step  before anything is modified the move can then take place ; we know that no page fault can occur  since all the relevant pages are in memory the other solution uses temporary registers to hold the values of overwritten locations if there is a page fault  all the old values are written back into memory before the trap occurs this action restores memory to its state before the instruction was started  so that the instruction can be repeated this is by no means the only architectural problem resulting from adding paging to an existing architecture to allow demand paging  but it illustrates some of the difficulties involved paging is added between the cpu and the memory in a computer system it should be entirely transparent to the user process thus  people often assume that paging can be added to any system although this assumption is true for a non-demand-paging environment  where a page fault represents a fatal errm ~ it is not true where a page fault means only that an additional page must be brought into memory and the process restarted 9.2.2 performance of demand paging demand paging can significantly affect the performance of a computer system to see why  let 's compute the effective access time for a demand-paged memory for most computer systems  the memory-access time  denoted ma  ranges from 10 to 200 nanoseconds as long as we have no page faults  the effective access time is equal to the memory access time if  howeve1 ~ a page fault occurs  we must first read the relevant page from disk and then access the desired word let p be the probability of a page fault  0    ; p    ; 1   we would expect p to be close to zero-that is  we would expect to have only a few page faults the t'tp r ' ! nrr access is then effective access time =  1  p  x ma + p x page fault time to compute the effective access time  we must know how much time is needed to service a page fault a page fault causes the following sequence to occur  trap to the operating system save the user registers and process state 366 chapter 9 deterncine that the interrupt was a page fault check that the page reference was legal and determine the location of the page on the disk issue a read from the disk to a free frame  a wait in a queue for this device until the read request is serviced b wait for the device seek and/ or latency time c begin the transfer of the page to a free frame while waiting  allocate the cpu to some other user  cpu scheduling  optional   receive an interrupt from the disk i/0 subsystem  i/0 completed   save the registers and process state for the other user  if step 6 is executed   determine that the interrupt was from the disk correct the page table and other tables to show that the desired page is now in memory wait for the cpu to be allocated to this process again restore the user registers  process state  and new page table  and then resume the interrupted instruction not all of these steps are necessary in every case for example  we are assuming that  in step 6  the cpu is allocated to another process while the i/o occurs this arrangement allows multiprogramming to maintain cpu utilization but requires additional time to resume the page-fault service routine when the i/0 transfer is complete in any case  we are faced with tlu ee major components of the page-fault service time  service the page-fault interrupt read in the page restart the process the first and third tasks can be reduced  with careful coding  to several hundred instructions these tasks may take from 1 to 100 microseconds each the page-switch time  however  will probably be close to 8 milliseconds  a typical hard disk has an average latency of 3 milliseconds  a seek of 5 milliseconds  and a transfer time of 0.05 milliseconds thus  the total paging time is about 8 milliseconds  including hardware and software time  remember also that we are looking at only the device-service time if a queue of processes is waiting for the device  we have to add device-queueing time as we wait for the paging device to be free to service our request  increasing even more the time to swap with an average page-fault service time of 8 milliseconds and a memoryaccess time of 200 nanoseconds  the effective access time in nanoseconds is 3 9.3 effective access time =  1  p  x  200  + p  8 milliseconds  =  1 p  x 200 + p x 8,000,000 = 200 + 7,999,800 x p 367 we see  then  that the effective access time is directly proportional to the if one access out of 1,000 causes a page fault  the effective access time is 8.2 microseconds the computer will be slowed down by a factor of 40 because of demand paging ! if we want performance degradation to be less than 10 percent  we need 220 200 + 7,999,800 x p  20 7,999,800 x p  p 0.0000025 that is  to keep the slowdown due to paging at a reasonable level  we can allow fewer than one memory access out of 399,990 to page-fault in sum  it is important to keep the page-fault rate low in a demand-paging system otherwise  the effective access time increases  slowing process execution dramatically an additional aspect of demand paging is the handling and overall use of swap space disk i/0 to swap space is generally faster than that to the file system it is faster because swap space is allocated in much larger blocks  and file lookups and indirect allocation methods are not used  chapter 12   the system can therefore gain better paging throughput by copying an entire file image into the swap space at process startup and then performing demand paging from the swap space another option is to demand pages from the file system initially but to write the pages to swap space as they are replaced this approach will ensure that only needed pages are read from the file system but that all subsequent paging is done from swap space some systems attempt to limit the amount of swap space used through demand paging of binary files demand pages for such files are brought directly from the file system however  when page replacement is called for  these frames can simply be overwritten  because they are never modified   and the pages can be read in from the file system again if needed using this approach  the file system itself serves as the backing store howeve1 ~ swap space must still be used for pages not associated with a file ; these pages include the stack and heap for a process this method appears to be a good compromise and is used in several systems  including solaris and bsd unix in section 9 .2  we illustrated how a process can start quickly by merely demandpaging in the page containing the first instruction however  process creation using the fork   system call may initially bypass the need for demand paging by using a technique similar to page sharing  covered in section 8.4.4   this technique provides for rapid process creation and minimizes the number of new pages that must be allocated to the newly created process 368 chapter 9 physical figure 9.7 before process i modifies page c recall thatthe fork   system call creates a child process that is a duplicate of its parent traditionally  fork   worked by creating a copy of the parent 's address space for the child  duplicating the pages belonging to the parent however  considering that many child processes invoke the exec   system call immediately after creation  the copying of the parent 's address space may be unnecessary instead  we can use a technique known as which works by allowing the parent and child processes initially to share the same pages these shared pages are marked as copy-on-write pages  meaning that if either process writes to a shared page  a copy of the shared page is created copy-on-write is illustrated in figures 9.7 and figure 9.8  which show the contents of the physical memory before and after process 1 modifies page c for example  assume that the child process attempts to modify a page containing portions of the stack  with the pages set to be copy-on-write the operating system will create a copy of this page  nl.apping it to the address space of the child process the child process will then modify its copied page and not the page belonging to the parent process obviously  when the copy-on-write technique is used  only the pages that are modified by either process are copied ; all unmodified pages can be shared by the parent and child processes note  too  process1 physical memory figure 9.8 after process 1 modifies page c process2 9.4 9.4 369 that only pages that can be nwdified need be m ~ arked as copy-on-write pages that can not be modified  pages containing executable code  can be shared by the parent and child copy-on-write is a common technique used by several operating systems  including windows xp  linux  and solaris when it is determined that a page is going to be duplicated using copyon write  it is important to note the location from which the free page will be allocated many operating systems provide a of free pages for such requests these free pages are typically allocated when the stack or heap for a process must expand or when there are copy-on-write pages to be managed operating systems typically allocate these pages using a technique known as zem-fhl-on-den  1and zero-fill-on-demand pages have been zeroed-out before being allocated  thus erasing the previous contents several versions of unix  including solaris and linux  provide a variation ofthe fork   system call-vfork    for fori    that operates differently from fork   with copy-on-write with vfork    the parent process is suspended  and the child process uses the address space of the parent because vfork   does not use copy-on-write  if the child process changes any pages of the parent 's address space  the altered pages will be visible to the parent once it resumes therefore  vf ork   must be used with caution to ensure that the child process does not modify the address space of the parent vf or k   is intended to be used when the child process calls exec   immediately after creation because no copying of pages takes place  vf ork   is an extremely efficient method of process creation and is sometimes used to implement unix command-line shell interfaces in our earlier discussion of the page-fault rate  we assumed that each page faults at most once  when it is first referenced this representation is not strictly accurate  however if a process of ten pages actually uses only half of them  then demand paging saves the i/0 necessary to load the five pages that are never used we could also increase our degree of multiprogramming by running twice as many processes thus  if we had forty frames  we could run eight processes  rather than the four that could run if each required ten frames  five of which were never used   if we increase our degree of multiprogramming  we are memory if we run six processes  each of which is ten pages in size but uses only five pages  we have higher cpu utilization and throughput  ten frames to spare it is possible  however  that each of these processes  for a particular data set  may suddenly try to use all ten of its pages  resulting in a need for sixty frames when only forty are available further  consider that system memory is not used only for holding program pages buffers for i/ 0 also consume a considerable amount of memory this use can increase the strain on memory-placement algorithms deciding how much memory to allocate to i/0 and how much to program pages is a significant challenge some systems allocate a fixed percentage of memory for i/0 buffers  whereas others allow both user processes and the i/0 subsystem to compete for all system memory 370 chapter 9 valid-invalid pc   -_ = ' ~ ~ = =  ! came f il logical memory for user 1 page table for user 1 valid-invalid 0 frame ~ bi ~ r ~ v v ~ -------' ' 2 3 logical memory for user 2 page table for user 2 0 monitor 2 3 4 5 j 6 a 7 e physical memory figure 9.9 need for page replacement over-allocation of memory manifests itself as follows while a user process is executing  a page fault occurs the operating system determines where the desired page is residing on the disk but then finds that there are no free frames on the free-frame list ; all memory is in use  figure 9.9   the operating system has several options at this point it could terminate the user process however  demand paging is the operating system 's attempt to improve the computer system 's utilization and throughput users should not be aware that their processes are running on a paged system-paging should be logically transparent to the user so this option is not the best choice the operating system could instead swap out a process  freeing all its frames and reducing the level of multiprogramming this option is a good one in certain circumstances  and we consider it further in section 9.6 here  we discuss the most common solution  9.4.1 basic page replacement page replacement takes the following approach if no frame is free  we find one that is not currently being used and free it we can free a frame by writing its contents to swap space and changing the page table  and all other tables  to indicate that the page is no longer in memory  figure 9.10   we can now use the freed frame to hold the page for which the process faulted we modify the page-fault service routine to include page replacement  find the location of the desired page on the disk find a free frame  a if there is a free frame  use it 9.4 371 b if there is no free frame  use a page-replacement algorithnc to select a c write the victim frame to the disk ; change the page and frame tables accordingly read the desired page into the newly freed frame ; change the page and frame tables restart the user process notice that  if no frames are free  two page transfers  one out and one in  are required this situation effectively doubles the page-fault service time and increases the effective access time accordingly we can reduce this overhead by using a  or when this scheme is used  each page or frame has a modify bit associated with it in the hardware the modify bit for a page is set by the hardware whenever any word or byte in the page is written into  indicating that the page has been modified when we select a page for replacement  we examine its modify bit if the bit is set  we know that the page has been modified since it was read in from the disk in this case  we must write the page to the disk if the modify bit is not set  however  the page has not been modified since it was read into memory in this case  we need not write the memory page to the disk  it is already there this technique also applies to read-only pages  for example  pages of binary code   such pages can not be modified ; thus  they may be discarded when desired this scheme can significantly reduce the time required to service a page fault  since it reduces i/o time by one-half if the page has not been modified frame valid-invalid bit ' \  / physical memory figure 9.10 page replacement 372 chapter 9 page replacement is basic to demand paging it completes the separation between logical memory and physical memory with this mechanism  an enormous virtual memory can be provided for programn'lers on a smaller physical memory with no demand paging  user addresses are mapped into physical addresses  so the two sets of addresses can be different all the pages of a process still must be in physical memory  however with demand paging  the size of the logical address space is no longer constrained by physical memory if we have a user process of twenty pages  we can execute it in ten frames simply by using demand paging and using a replacement algorithm to find a free frame whenever necessary if a page that has been modified is to be replaced  its contents are copied to the disk a later reference to that page will cause a page fault at that time  the page will be brought back into memory  perhaps replacing some other page in the process we must solve two major problems to implement demand develop a algorithm and a ' ' '  ' ' l tceme ~ lu ~ ~ f ' ' ~ ~ ''h ' that is  if we have multiple processes in memory  we must decide how many frames to allocate to each process ; and when page replacement is required  we must select the frames that are to be replaced designing appropriate algorithms to solve these problems is an important task  because disk i/0 is so expensive even slight improvements in demand-paging methods yield large gains in system performance there are many different page-replacement algorithms every operating system probably has its own replacement scheme how do we select a particular replacement algorithm in general  we want the one with the lowest page-fault rate we evaluate an algorithm by running it on a particular string of memory references and computing the number of page faults the string of memory references is called a reference we can generate reference strings artificially  by using a random-number generator  for example   or we can trace a given system and record the address of each memory reference the latter choice produces a large number of data  on the order of 1 million addresses per second   to reduce the number of data  we use two facts first  for a given page size  and the page size is generally fixed by the hardware or system   we need to consider only the page number  rather than the entire address second  if we have a reference to a page p  then any references to page p that immediately follow will never cause a page fault page p will be in memory after the first reference  so the immediately following references will not fault for example  if we trace a particular process  we might record the following address sequence  0100,0432,0101,0612,0102,0103,0104,0101,0611,0102,0103  0104,0101,0610,0102,0103,0104,0101,0609,0102,0105 at 100 bytes per page  this sequence is reduced to the following reference string  1  4  1  6  1  6  1  6  1  6  1 9.4 373 16 g  14    j  ; 2 12 q  ol co 10 0 0 8 ' q  ..0 6 e    j c 4 2 2 3 4 5 6 number of frames figure 9.1 i graph of page faults versus number of frames to determine the number of page faults for a particular reference string and page-replacement algorithm  we also need to know the number of page frames available obviously  as the number of frames available increases  the number of page faults decreases for the reference stril'lg considered previously  for example  if we had three or more frames  we would have only three faultsone fault for the first reference to each page in contrast  with only one frame available  we would have a replacement with every reference  resulting in eleven faults in general  we expect a curve such as that in figure 9.11 as the number of frames increases  the number of page faults drops to some minimal level of course  adding physical memory increases the number of frames we next illustrate several page-replacement algorithms in doing so  we use the reference string for a memory with three frames 9.4.2 fifo page replacement the simplest page-replacement algorithm is a first-in  first-out  fifo  algorithm a fifo replacement algorithm associates with each page the time when that page was brought into memory when a page must be replaced  the oldest page is chosen notice that it is not strictly necessary to record the time when a page is brought in we can create a fifo queue to hold all pages in memory we replace the page at the head of the queue when a page is brought into memory  we insert it at the tail of the queue for our example reference string  our three frames are initially empty the first three references  7  0  1  cause page faults and are brought into these empty frames the next reference  2  replaces page 7  because page 7 was brought in first since 0 is the next reference and 0 is already in memory  we have no fault for this reference the first reference to 3 results in replacement of page 0  since 374 chapter 9 reference string 7 0 2 0 3 0 4 2 3 0 3 2 2 0 7 0 page frames figure 9.12 fifo page-replacement algorithm it is now first in line because of this replacement  the next reference  to 0  will fault page 1 is then replaced by page 0 this process continues as shown in figure 9.12 every time a fault occurs  we show which pages are in our three frames there are fifteen faults altogether the fifo page-replacement algorithm is easy to lmderstand and program however  its performance is not always good on the one hand  the page replaced may be an initialization module that was used a long time ago and is no longer needed on the other hand  it could contain a heavily used variable that was initialized early and is in constant use notice that  even if we select for replacement a page that is in active use  everything still works correctly after we replace an active page with a new one  a fault occurs almost immediately to retrieve the active page some other page must be replaced to bring the active page back into memory thus  a bad replacement choice increases the page-fault rate and slows process execution it does not  however  cause incorrect execution to illustrate the problems that are possible with a fifo page-replacement algorithm  we consider the following reference string  1  2  3  4  1  2  5  1  2  3  4  5 figure 9.13 shows the curve of page faults for this reference string versus the number of available frames notice that the number of faults for four frames  ten  is greater than the number of faults for three frames  nine  ! this most unexpected result is known as  for some page-replacement algorithms  the page-fault rate may increase as the number of allocated frames increases we would expect that giving more memory to a process would improve its performance in some early research  investigators noticed that this assumption was not always true belady 's anomaly was discovered as a result 9.4.3 optimal page replacement of belady 's anomaly was the search for an which has the lowest page-fault rate of all algorithms and will never suffer from belady 's anomaly such an algorithm does exist and has been called opt or min it is simply this  replace the page that will not be used for the longest period of time 9.4 375 16 ~    5 2 12 cj  moj 10 0 0 8 cj _o e 6    5 c 4 2 number of frames figure 9.13 page-fault curve for fifo replacement on a reference string use of this page-replacement algorithm guarantees the lowest possible pagefault rate for a fixed number of frames for example  on our sample reference string  the optimal page-replacement algorithm would yield nine page faults  as shown in figure 9.14 the first three references cause faults that fill the three empty frames the reference to page 2 replaces page 7  because page 7 will not be used until reference 18  whereas page 0 will be used at 5  and page 1 at 14 the reference to page 3 replaces page 1  as page 1 will be the last of the three pages in memory to be referenced again with only nine page faults  optimal replacement is much better than a fifo algorithm  which results in fifteen faults  if we ignore the first three  which all algorithms must suffer  then optimal replacement is twice as good as fifo replacement  irt fact  no replacement algorithm can process this reference string in three frames with fewer than nine faults unfortunately  the optimal page-replacement algorithm is difficult to implement  because it requires future knowledge of the reference string  we encountered a similar situation with the sjf cpu-schedulin.g algorithm in section 5.3.2  as a result  the optimal algorithm is used mainly for comparison studies for instance  it may be useful to know that  although a new algorithm reference string 7 0 2 0 3 0 4 2 3 0 3 2 2 0 7 0 page frames figure 9.14 optimal page-replacement algorithm 376 chapter 9 is not optimat it is within 12.3 percent of optimal at worst and within 4.7 percent on average 9.4.4 lru page replacement lf the optimal algorithm is not feasible  perhaps an approximation of the optimal algorithm is possible the key distinction between the fifo and opt algorithms  other than looking backward versus forward in time  is that the fifo algorithm uses the time when a page was brought into memory  whereas the opt algorithm uses the time when a page is to be used if we use the recent past as an approximation of the near future  then we can replace the that has not been used for the longest period of time this approach is the lru replacement associates with each page the time of that page 's last use when a page must be replaced  lru chooses the page that has not been used for the longest period of time we can think of this strategy as the optimal page-replacement algorithm looking backward in time  rather than forward  strangely  if we let sr be the reverse of a reference strings  then the page-fault rate for the opt algorithm on sis the same as the page-fault rate for the opt algorithm on sr similarly  the page-fault rate for the lru algorithm on sis the same as the page-fault rate for the lru algorithm on sr  the result of applying lru replacement to our example reference string is shown in figure 9.15 the lru algorithm produces twelve faults notice that the first five faults are the same as those for optimal replacement when the reference to page 4 occurs  however  lru replacement sees that  of the three frames in memory  page 2 was used least recently thus  the lru algorithm replaces page 2  not knowing that page 2 is about to be used when it then faults for page 2  the lru algorithm replaces page 3  since it is now the least recently used of the three pages in memory despite these problems  lru replacement with twelve faults is much better than fifo replacement with fifteen the lru policy is often used as a page-replacement algorithm and is considered to be good the major problem is how to implement lru replacement an lru page-replacement algorithm may require substantial hardware assistance the problem is to determine an order for the frames defined by the time of last use two implementations are feasible  counters in the simplest case  we associate with each page-table entry a time-of-use field and add to the cpu a logical clock or counter the clock is reference string 7 0 2 0 3 0 4 2 3 0 3 2 2 0 7 0 page frames figure 9.15 lru page-replacement algorithm 9.4 377 incremented for every memory reference whenever a reference to a page is made  the contents of the clock register are copied to the ti1ne-of-use field in the page-table entry for that page in this way  we always have the time of the last reference to each page we replace the page with the smallest time value this scheme requires a search of the page table to find the lru page and a write to memory  to the time-of-use field in the page table  for each memory access the times must also be m ~ aintained when page tables are changed  due to cpu scheduling   overflow of the clock must be considered stack another approach to implementing lru replacement is to keep a stack of page numbers whenever a page is referenced  it is removed from the stack and put on the top in this way  the most recently used page is always at the top of the stack and the least recently used page is always at the bottom  figure 9.16   because entries must be removed from the middle of the stack  it is best to implement this approach by using a doubly linked list with a head pointer and a tail pointer removing a page and putting it on the top of the stack then requires changing six pointers at worst each update is a little more expensive  but there is no search for a replacement ; the tail pointer points to the bottom of the stack  which is the lru page this approach is particularly appropriate for software or microcode implementations of lru replacement like optimal replacement  lru replacement does not suffer from belady 's both belong to a class of page-replacement algorithms  called si  ack that can never exhibit belady 's anomaly a stack algorithm is an algorithm for which it can be shown that the set of pages in memory for n frames is always a subset of the set of pages that would be in memory with n + 1 frames for lru replacement  the set of pages in memory would be the n most recently referenced pages if the number of frames is increased  these n pages will still be the most recently referenced and so will still be in memory note that neither implementation of lru would be conceivable without hardware assistance beyond the standard tlb registers the updating of the reference string 4 7 0 7 stack before a 0 2 stack after b 2 7 2 i l a b figure 9.16 use of a stack to record the most recent page references 378 chapter 9 clock fields or stack must be done for every memory reference if we were to use an interrupt for every reference to allow software to update such data structures  it would slow every memory reference by a factor of at least ten  hence slowing every user process by a factor of ten few systems could tolerate that level of overhead for memory management 9.4.5 lru-approximation page replacement few computer systems provide sufficient hardware support for true lru page replacement some systems provide no hardware support  and other pagereplacement algorithms  such as a fifo algorithm  must be used many systems provide some help  however  in the form of a the reference bit for a page is set by the hardware whenever that page is referenced  either a read or a write to any byte in the page   reference bits are associated with each entry in the page table initially  all bits are cleared  to 0  by the operating system as a user process executes  the bit associated with each page referenced is set  to 1  by the hardware after some time  we can determine which pages have been used and which have not been used by examining the reference bits  although we do not know the order of use this information is the basis for many page-replacement algorithms that approximate lru replacement 9.4.5.1 additional-reference-bits algorithm we can gain additional ordering information by recording the reference bits at regular intervals we can keep an 8-bit byte for each page in a table in memory at regular intervals  say  every 100 milliseconds   a timer interrupt transfers control to the operating system the operating system shifts the reference bit for each page into the high-order bit of its 8-bit byte  shifting the other bits right by 1 bit and discarding the low-order bit these 8-bit shift registers contain the history of page use for the last eight time periods if the shift register contains 00000000  for example  then the page has not been used for eight time periods ; a page that is used at least once in each period has a shift register value of 11111111 a page with a history register value of 11000100 has been used more recently than one with a value of 01110111 if we interpret these 8-bit bytes as unsigned integers  the page with the lowest number is the lru page  and it can be replaced notice that the numbers are not guaranteed to be unique  however we can either replace  swap out  all pages with the smallest value or use the fifo method to choose among them the number of bits of history included in the shift register can be varied  of course  and is selected  depending on the hardware available  to make the updating as fast as possible in the extreme case  the number can be reduced to zero  leaving only the reference bit itself this algorithm is called the 9.4.5.2 second-chance algorithm the basic algorithm of second-chance replacement is a fifo replacement algorithm when a page has been selected  however  we inspect its reference bit if the value is 0  we proceed to replace this page ; but if the reference bit is set to 1  we give the page a second chance and move on to select the next next victim 9.4 reference pages reference pages bits bits circular queue of pages circular queue of pages  a   b  figure 9.17 second-chance  clock  page-replacement algorithm 379 fifo page when a page gets a second chance  its reference bit is cleared  and its arrival time is reset to the current time thus  a page that is given a second chance will not be replaced until all other pages have been replaced  or given second chances   in addition  if a page is used often enough to keep its reference bit set  it will never be replaced one way to implement the second-chance algorithm  sometimes referred to as the clock algorithm  is as a circular queue a poi11ter  that is  a hand on the clock  indicates which page is to be replaced next when a frame is needed  the pointer advances until it finds a page with a 0 reference bit as it advances  it clears the reference bits  figure 9.17   once a victim page is found  the page is replaced  and the new page is inserted in the circular queue in that position notice that  in the worst case  when all bits are set  the pointer cycles through the whole queue  giving each page a second chance it clears all the reference bits before selecting the next page for replacement second-chance replacement degenerates to fifo replacement if all bits are set 9.4.5.3 enhanced second-chance algorithm we can enhance the second-chance algorithm by considering the reference bit and the modify bit  described in section 9.4.1  as an ordered pair with these two bits  we have the following four possible classes   0  0  neither recently used nor modified -best page to replace 380 chapter 9  0  1  not recently used hut modified-not quite as good  because the page will need to be written out before replacement  1  0  recently used but clean-probably will be used again soon  1  1  recently used and modified -probably will be used again soon  and the page will be need to be written out to disk before it can be replaced each page is in one of these four classes when page replacement is called for  we use the same scheme as in the clock algorithm ; but instead of examining whether the page to which we are pointing has the reference bit set to 1  we examine the class to which that page belongs we replace the first page encountered in the lowest nonempty class notice that we may have to scan the circular queue several times before we find a page to be replaced the major difference between this algorithm and the simpler clock algorithm is that here we give preference to those pages that have been modified to reduce the number of i/os required 9.4.6 counting-based page replacement there are many other algorithms that can be used for page replacement for example  we can keep a counter of the number of references that have been made to each page and develop the following two schemes the least frequently used  lfu  page-replacement algorithm requires that the page with the smallest count be replaced the reason for this selection is that an actively used page should have a large reference count a problem arises  however  when a page is used heavily during the initial phase of a process but then is never used again since it was used heavily  it has a large count and remains in memory even though it is no longer needed one solution is to shift the counts right by 1 bit at regular intervals  forming an exponentially decaying average usage count the most frequently used  mfu  page-replacement algorithm is based on the argument that the page with the smallest count was probably just brought in and has yet to be used as you might expect  neither mfu nor lfu replacement is common the implementation of these algorithms is expensive  and they do not approxin'late opt replacement well 9.4.7 page-buffering algorithms other procedures are often used in addition to a specific page-replacement algorithm for example  systems commonly keep a pool of free frames when a page fault occurs  a victim frame is chosen as before however  the desired page is read into a free frame from the pool before the victim is written out this procedure allows the process to restart as soon as possible  without waiting for the victim page to be written out when the victim is later written out  its frame is added to the free-frame pool 9.4 381 an expansion of this idea is to maintain a list of modified pages whenever the paging device is idle  a modified page is selected and is written to the disk its modify bit is then reset this scheme increases the probability that a page will be clean when it is selected for replacement and will not need to be written out another modification is to keep a pool of free frames but to remember which page was in each frame since the frame contents are not modified when a frame is written to the disk  the old page can be reused directly fronc the free-frame pool if it is needed before that frame is reused no i/o is needed in this case when a page fault occurs  we first check whether the desired page is in the free-frame pool if it is not  we must select a free frame and read into it this technique is used in the vax/vms system along with a fifo replacement algorithm when the fifo replacement algorithm mistakenly replaces a page that is still in active use  that page is quickly retrieved from the free-frame pool  and no i/o is necessary the free-frame buffer provides protection against the relatively poor  but sirnple  fifo replacement algorithm this method is necessary because the early versions of vax did not implement the reference bit correctly some versions of the unix system use this method in conjunction with the second-chance algorithm it can be a useful augmentation to any pagereplacement algorithm  to reduce the penalty incurred if the wrong victim page is selected 9.4.8 applications and page replacement in certain cases  applications accessing data through the operating system 's virtual memory perform worse than if the operating system provided no buffering at all a typical example is a database  which provides its own memory management and i/0 buffering applications like this understand their memory use and disk use better than does an operating system that is implementing algorithms for general-purpose use if the operating system is buffering i/0  and the application is doing so as well  then twice the memory is being used for a set of i/0 in another example  data warehouses frequently perform massive sequential disk reads  followed by computations and writes the lru algorithm would be removing old pages and preserving new ones  while the application would more likely be reading older pages than newer ones  as it starts its sequential reads again   here  mfu would actually be more efficient than lru because of such problems  some operating systems give special programs the ability to use a disk partition as a large sequential array of logical blocks  without any file-system data structures this array is sometimes called the raw disk  and i/o to this array is termed raw i/0 raw i/0 bypasses all the filesystem services  such as file i/0 demand paging  file locking  prefetching  space allocation  file names  and directories note that although certain applications are more efficient when implementing their own special-purpose storage services on a raw partition  most applications perform better when they use the regular file-system services 382 chapter 9 9.5 we turn next to the issue of allocation how do we allocate the fixed amount of free memory among the various processes if we have 93 free frames and two processes  how many frames does each process get the simplest case is the single-user system consider a single-user system with 128 kb of memory composed of pages 1 kb in size this system has 128 frames the operating system may take 35 kb  leaving 93 frames for the user process under pure demand paging  all 93 frames would initially be put on the free-frame list when a user process started execution  it would generate a sequence of page faults the first 93 page faults would all get free frames from the free-frame list when the free-frame list was exhausted  a page-replacement algorithm would be used to select one of the 93 in-memory pages to be replaced with the 94th  and so on when the process terminated  the 93 frames would once again be placed on the free-frame list there are many variations on this simple strategy we can require that the operating system allocate all its buffer and table space from the free-frame list when this space is not in use by the operating system  it can be used to support user paging we can try to keep three free frames reserved on the free-frame list at all times thus  when a page fault occurs  there is a free frame available to page into while the page swap is taking place  a replacement can be selected  which is then written to the disk as the user process continues to execute other variants are also possible  but the basic strategy is clear  the user process is allocated any free frame 9.5.1 minimum number of frames our strategies for the allocation of frames are constrained in various ways we can not  for example  allocate more than the total number of available frames  unless there is page sharing   we must also allocate at least a minimum number of frames here  we look more closely at the latter requirement one reason for allocating at least a minimum number of frames involves performance obviously  as the number of frames allocated to each process decreases  the page-fault rate increases  slowing process execution in addition  remember that when a page fault occurs before an executing ilcstruction is complete  the instruction must be restarted consequently we must have enough frames to hold all the different pages that any single ilcstruction can reference for example  consider a machine in which all memory-reference instructions may reference only one memory address in this case  we need at least one frame for the instruction and one frame for the mernory reference in addition  if one-level indirect addressing is allowed  for example  a load instruction on page 16 can refer to an address on page 0  which is an indirect reference to page 23   then paging requires at least three frames per process think about what might happen if a process had only two frames the minimum number of frames is defined by the computer architecture for example  the move instruction for the pdp-11 includes more than one word for some addressing modes  and thus the ilcstruction itself may straddle two pages in addition  each of its two operands may be indirect references  for a total of six frames another example is the ibm 370 mvc instruction since the 9.5 383 instruction is from storage location to storage location  it takes 6 bytes and can straddle two pages the block of characters to move and the area to which it is to be m.oved can each also straddle two pages this situation would require six frames the worst case occurs when the mvc instruction is the operand of an execute instruction that straddles a page boundary ; in this case  we need eight frames the worst-case scenario occurs in computer architectures that allow multiple levels of indirection  for example  each 16-bit word could contain a 15-bit address plus a 1-bit indirect indicator   theoretically  a simple load instruction could reference an indirect address that could reference an indirect address  on another page  that could also reference an indirect address  on yet another page   and so on  until every page in virtual memory had been touched thus  in the worst case  the entire virtual memory must be in physical memory to overcome this difficulty  we must place a limit on the levels of indirection  for example  limit an instruction to at most 16levels of indirection   when the first indirection occurs  a counter is set to 16 ; the counter is then decremented for each successive irtdirection for this instruction if the counter is decremented to 0  a trap occurs  excessive indirection   this limitation reduces the maximum number of memory references per instruction to 17  requiring the same number of frames whereas the minimum number of frames per process is defined by the architecture  the maximum number is defined by the amount of available physical memory in between  we are still left with significant choice in frame allocation 9.5.2 allocation algorithms the easiest way to split m frames among n processes is to give everyone an equal share  m/n frames for instance  if there are 93 frames and five processes  each process will get 18 frames the three leftover frames can be used as a free-frame buffer pool this scheme is called an alternative is to recognize that various processes will need differing amounts of memory consider a system with a 1-kb frame size if a small student process of 10 kb and an interactive database of 127 kb are the only two processes running in a system with 62 free frames  it does not make much sense to give each process 31 frames the student process does not need more than 10 frames  so the other 21 are  strictly speaking  wasted to solve this problem  we can use in which we allocate available memory to each process according to its size let the size of the virtual memory for process p ; be s ;  and define s = ls ;  then  if the total number of available frames is m  we allocate a ; frames to process p ;  where a ; is approximately a ; = s ; /s x m 384 chapter 9 of course  we must adjust each ai to be an integer that is greater than the ncinimum number of frames required by tl1e instruction set  with a sum not exceeding m with proportional allocation  we would split 62 frames between two processes  one of 10 pages and one of 127 pages  by allocating 4 frames and 57 frames  respectively  since 10/137 x 62 ~ 4  and 127/137 x 62 ~ 57 in this way  both processes share the available frames according to their needs  rather than equally in both equal and proportional allocation  of course  the allocation may vary according to the multiprogramming level if the multiprogramming level is increased  each process will lose some frames to provide the memory needed for the new process conversely  if the multiprogramming level decreases  the frames that were allocated to the departed process can be spread over the remaining processes notice that  with either equal or proportional allocation  a high-priority process is treated the same as a low-priority process by its definition  however  we may want to give the high-priority process more memory to speed its execution  to the detriment of low-priority processes one solution is to use a proportional allocation scheme wherein the ratio of frames depends not on the relative sizes of processes but rather on the priorities of processes or on a combination of size and priority 9.5.3 global versus local allocation another important factor in the way frames are allocated to the various processes is page replacement with multiple processes competing for frames  we can classify page-replacement algorithms into two broad categories  ; .no ' '-c'u ~ ' ' and local global replacement allows a process to a replacement frame from the set of all frames  even if that frame is currently allocated to some other process ; that is  one process can take a frame from another local replacement requires that each process select from only its own set of allocated frames for example  consider an allocation scheme wherein we allow high-priority processes to select frames from low-priority processes for replacement a process can select a replacement from among its own frames or the frames of any lower-priority process this approach allows a high-priority process to increase its frame allocation at the expense of a low-priority process with a local replacement strategy  the number of frames allocated to a process does not change with global replacement  a process may happen to select only frames allocated to other processes  thus increasing the number of frames allocated to it  assuming that other processes do not choose its frames for replacement   one problem with a global replacement algorithm is that a process can not control its own page-fault rate the set of pages in memory for a process depends not only on the paging behavior of that process but also on the paging behavior of other processes therefore  the same process may perform quite 9.5 385 differently  for example  taking 0.5 seconds for one execution and 10.3 seconds for the next execution  because of totally external circuntstances such is not the case with a local replacement algorithm under local replacement  the set of pages in memory for a process is affected by the paging behavior of only that process local replacement might hinder a process  however  by not making available to it other  less used pages of memory thus  global replacement generally results in greater system throughput and is therefore the more common method 9.5.4 non-uniform memory access thus far in our coverage of virtual memory  we have assumed that all main memory is created equal-or at least that it is accessed equally on many computer systems  that is not the case often  in systems with multiple cpus  section 1.3.2   a given cpu can access some sections of main memory faster than it can access others these performance differences are caused by how cpus and memory are interconnected in the system frequently  such a system is made up of several system boards  each containing multiple cpus and some memory the system boards are interconnected in various ways  ranging from system busses to high-speed network connections like infiniband as you might expect  the cpus on a particular board can access the memory on that board with less delay than they can access memory on other boards in the system systems in which memory access times vary significantly are known collectively as systems  and without exception  they are slower than systems in which memory and cpus are located on the same motherboard managing which page frames are stored at which locations can significantly affect performance in numa systems if we treat memory as uniform in such a system  cpus may wait significantly longer for memory access than if we modify memory allocation algorithms to take numa into account similar changes must be rnade to the scheduling system the goal of these changes is to have memory frames allocated as close as possible to the cpu on which the process is running the definition of close is with minimum latency  which typically means on the same system board as the cpu the algorithmic changes consist of having the scheduler track the last cpu on which each process ran if the scheduler tries to schedule each process onto its previous cpu  and the memory-management system tries to allocate frames for the process close to the cpu on which it is being scheduled  then improved cache hits and decreased memory access times will result the picture is more complicated once threads are added for example  a process with many running threads may end up with those threads scheduled on many different system boards how is the memory to be allocated in this case solaris solves the problem by creating an entity in the kernel each lgroup gathers together close cpus and memory in fact  there is a hierarchy of lgroups based on the amount of latency between the groups solaris tries to schedule all threads of a process and allocate all memory of a process within an lgroup if that is not possible  it picks nearby lgroups for the rest of the resources needed in this manner  overall memory latency is minimized  and cpu cache hit rates are maximized 386 chapter 9 9.6 if the number of frames allocated to a low-priority process falls below the minimum number required by the computer architecture  we must suspend that process 's execution we should then page out its remaining pages  freeing all its allocated frames this provision introduces a swap-in  swap-out level of intermediate cpu scheduling in fact  look at any process that does not have enough frames if the process does not have the num.ber of frames it needs to support pages in active use  it will quickly page-fault at this point  it must replace some page however  since all its pages are in active use  it must replace a page that will be needed again right away consequently  it quickly faults again  and again  and again  replacing pages that it must back in immediately this high paging activity is called a process is thrashing if it is spending more time paging than executing 9.6.1 cause of thrashing thrashing results in severe performance problems consider the following scenario  which is based on the actual behavior of early paging systems the operating system monitors cpu utilization if cpu utilization is too low  we increase the degree of multiprogramming by introducing a new process to the system a global page-replacement algorithm is used ; it replaces pages without regard to the process to which they belong now suppose that a process enters a new phase in its execution and needs more frames it starts faulting and taking frames away from other processes these processes need those pages  however  and so they also fault  taking frames from other processes these faulting processes must use the pagin.g device to swap pages in and out as they queue up for the paging device  the ready queue empties as processes wait for the paging device  cpu utilization decreases the cpu scheduler sees the decreasing cpu utilization and increases the degree of multiprogramming as a result the new process tries to get started by taking frames from running processes  causing more page faults and a longer queue for the paging device as a result  cpu utilization drops even further  and the cpu scheduler tries to increase the degree of multiprogramming even more thrashing has occurred  and system throughput plunges the pagefault rate increases tremendously as a result  the effective m.emory-access time increases no work is getting done  because the processes are spending all their time paging this phenomenon is illustrated in figure 9.18  in which cpu utilization is plotted against the degree of multiprogramming as the degree of multiprogramming increases  cpu utilization also ilccreases  although more slowly  until a maximum is reached if the degree of multiprogramming is increased even further  thrashing sets in  and cpu utilization drops sharply at this point  to increase cpu utilization and stop thrashing  we must decrease the degree of multiprogramming we can limit the effects of thrashing by using a  or with local replacement  if one process starts thrashing  it can not frames from another process and cause the latter to thrash as well however  the problem is not entirely solved if processes are 9.6 387 degree of multiprogramming figure 9.18 thrashing thrashing  they will be in the queue for the paging device most of the time the average service time for a page fault will increase because of the longer average queue for the paging device thus  the effective access time will increase even for a process that is not thrashing to prevent thtashing  we must provide a process with as many frames as it needs but how do we know how many frames it needs there are several teclmiques the working-set strategy  section 9.6.2  starts by looking at how frames a process is actually using this approach defines the locality of process execution the locality model states that  as a process executes  it moves from locality to locality a locality is a set of pages that are actively used together  figure 9.19   a program is generally composed of several different localities  which may overlap for example  when a function is called  it defines a new locality in this locality  memory references are made to the instructions of the function call  its local variables  and a subset of the global variables when we exit the function  the process leaves this locality  since the local variables and instructions of the function are no longer in active use we may return to this locality later thus  we see that localities are defined by the program structure and its data structures the locality model states that all programs will exhibit this basic memory reference structure note that the locality model is the unstated principle behind the caching discussions so far in this book if accesses to any types of data were random rather than patterned  caching would be useless suppose we allocate enough frames to a process to accommodate its current locality it will fault for the pages in its locality until all these pages are in memory ; then  it will not fault again until it changes localities if we do not allocate enough frames to accommodate the size of the current locality  the process will thrash  since it can not keep in memory all the pages that it is actively using 9.6.2 working-set model as mentioned  the is based on the assumption of locality this model uses a paramete1 ~ / '    to define the vrindovv the idea 388 chapter 9 32 ~ ~  ~ ~ = = ~ ~ ~ ~ ~ wl ~ ~ #  ~ ~  ~ ~ ~  \ jjl  jlli111 28  j   j   !  0 0  lj 26 i ' c 0 i e i  lj e execution time  figure 9.19 locality in a memory-reference pattern is to examine the most recent 6 references the set of pages in the most recent 6 page references is the  figure 9.20   if a page is in active use  it will be in the working set if it is no longer being used  it will drop from the working set 6 time units after its last reference thus  the working set is an approximation of the program 's locality for example  given the sequence of memory references shown in figure 9.20  if 6 = 10 memory references  then the working set at time t1 is  1  2  5  6  7   by time t2  the working set has changed to  3  4   the accuracy of the working set depends on the selection of 6 if 6 is too small  it will not encompass the entire locality ; if 6 is too large  it may overlap 9.6 page reference table    2 6 1 5 7 7 7 7 5 1 6 2 3 4 1 2 3 4 4 4 3 4 3 4 4 4 1 3 2 3 4 4 4 3 4 4 4  ~ ~ r ~ r t1 ws  t1  =  1 ,2,5,6,7  figure 9.20 working-set model 389 several localities in the extrem.e  if l is infinite  the working set is the set of pages touched during the process execution the most important property of the working set  then  is its size if we compute the working-set size  wss ;  for each process in the system  we can then consider that where dis the total demand for frames each process is actively using the pages in its working set thus  process i needs wss ; frames if the total demand is greater than the total number of available frames  d m   thrashing will occur  because some processes will not have enough frames once l has been selected  use of the working-set model is simple the operating system monitors the working set of each process and allocates to that working set enough frames to provide it with its working-set size if there are enough extra frames  another process can be initiated if the sum of the working-set sizes increases  exceeding the total number of available frames  the operating system selects a process to suspend the process 's pages are written out  swapped   and its frames are reallocated to other processes the suspended process can be restarted later this working-set strategy prevents thrashing while keeping the degree of multiprogramming as high as possible thus  it optimizes cpu utilization the difficulty with the working-set model is keeping track of the working set the working-set window is a moving window at each memory reference  a new reference appears at one end and the oldest reference drops off the other end a page is in the working set if it is referenced anywhere in the working-set window we can approximate the working-set model with a fixed-interval timer interrupt and a reference bit for example  assum.e that l equals 10,000 references and that we can cause a timer interrupt every 5,000 references when we get a timer interrupt  we copy and clear the reference-bit values for each page thus  if a page fault occurs  we can examine the current reference bit and two in-memory bits to determine whether a page was used within the last 10,000 to 15,000 references if it was used  at least one of these bits will be on if it has not been used  these bits will be off those pages with at least one bit on will be considered to be in the working set note that this arrangement is not entirely accurate  because we can not tell where  within an interval of 5,000  a reference occurred we can reduce the uncertainty by increasing the number of history bits and the frequency of interrupts  for example  10 bits and interrupts every 1,000 references   however  the cost to service these more frequent interrupts will be correspondingly higher 390 chapter 9 9.7 number of frames figure 9.21 page-fault frequency 9.6.3 page-fault frequency the working-set model is successful  and knowledge of the working set can be useful for prepaging  section 9.9.1   but it seems a clumsy way to control thrashilcg a strategy that uses the takes a more direct approach the specific problem is how to prevent thrashilcg thrashing has a high page-fault rate thus  we want to control the page-fault rate when it is too high  we know that the process needs more frames conversely  if the page-fault rate is too low  then the process may have too many frames we can establish upper and lower bounds on the desired page-fault rate  figure 9.21   if the actual page-fault rate exceeds the upper limit  we allocate the process another frame ; if the page-fault rate falls below the lower limit  we remove a frame from the process thus  we can directly measure and control the page-fault rate to prevent thrashing as with the working-set strategy  we may have to suspend a process if the page-fault rate ilccreases and no free frames are available  we must select some process and suspend it the freed frames are then distributed to processes with high page-fault rates consider a sequential read of a file on disk using the standard system calls open   ,read    and write    each file access requires a system call and disk access alternatively  we can use the virtual memory techniques discussed so far to treat file i/0 as routine memory accesses this approach  known as a file  allows a part of the virtual address space to be logically associated with the file as we shall see  this can lead to significant performance increases when performing i/0 9.7 391 working sets and page faultrates there is a directrelationship between the working set of a process and its page-fault rate typically as shown in figure 9.20  the working set ofa process changes pver time as references to data and code sections move from one locality to another assuming there is sufficient memory to store the working set of .a process  that is  the processis 11.ot thrashing   tbe page-fault rate of the process will transition between peaks and valleys over time this general behavior is shown in figure 9.22 page fault rate working set time figure 9.22 page fault rate over time a peak in the page-fault rate occurs when we begin demand-paging a new locality however  once the working set of this new locality is in memory  the page-fault rate falls when the process moves to a new working set  the page  fault rate rises toward a peak once again  returning to a lower rate once the new working set is loaded into memory the span oftime between the start of one peak and the start of thenext peak represents the transition from one working set to another 9.7.1 basic mechanism memory mapping a file is accomplished by mapping a disk block to a page  or pages  in memory initial access to the file proceeds through ordinary demand paging  resulting in a page fault however  a page-sized portion of the file is read from the file system into a physical page  some systems may opt to read in more than a page-sized chunk of memory at a time   subsequent reads and writes to the file are handled as routine memory accesses  thereby simplifying file access and usage by allowing the system to manipulate files through memory rather than incurring the overhead of using the read   and write   system calls similarly  as file l/0 is done in memory as opposed to using system calls that involve disk i/0  file access is much faster as well note that writes to the file mapped in memory are not necessarily imm.ediate  synchronous  writes to the file on disk some systems may choose to update the physical file when the operating system periodically checks 392 chapter 9 whether the page in memory has been modified when the file is closed  all the memory-mapped data are written back to disk and ren loved from the virtual memory of the process some operating systems provide memory mapping only through a specific system call and use the standard system calls to perform all other file i/0 however  some systems choose to memory-map a file regardless of whether the file was specified as memory-mapped let 's take solaris as an example if a file is specified as memory-mapped  using the mmap   system call   solaris maps the file into the address space of the process if a file is opened and accessed using ordinary system calls  such as open    read    and write    solaris still memory-maps the file ; however  the file is mapped to the kernel address space regardless of how the file is opened  then  solaris treats all file i/0 as memory-mapped  allowing file access to take place via the efficient memory subsystem multiple processes may be allowed to map the same file concurrently  to allow sharing of data writes by any of the processes modify the data in virtual memory and can be seen by all others that map the same section of the file given our earlier discussions of virtual memory  it should be clear how the sharing of memory-mapped sections of memory is implemented  the virtual memory map of each sharing process points to the same page of physical memory-the page that holds a copy of the disk block this memory sharing is illustrated in figure 9.23 the memory-mapping system calls can also support copy-on-write functionality  allowing processes to share a file in read-only mode but to have their own copies of any data they modify so that r i i i 1  r   ; i i 1 1 i -1 ii i i i i i j---r ' -rl..-r i i i i -r ' i i i i 1 -1 i i 1 _ i i i i i f +  =   .....c.c ~ ..-'---r ~   i i  .l i j i i i 1 i i i i i i i i i i l_ ~ i process a 1 1 1 virtual memory  ~ 1  disk file figure 9.23 memory-mapped files process b virtual memory 9.7 memory-mapped file figure 9.24 shared memory in windows using memory-mapped 1/0 393 access to the shared data is coordinated  the processes involved might use one of the mechanisms for achieving mutual exclusion described in chapter 6 in many ways  the sharing of memory-mapped files is similar to shared memory as described in section 3.4.1 not all systems use the same mechanism for both ; on unix and linux systems  for example  memory mapping is accomplished with the mmap   system call  whereas shared memory is achieved with the posix-compliant shmget   and shmat   systems calls  section 3.5.1   on windows nt  2000  and xp systems  howeve1 ~ shared memory is accomplished by memory mapping files on these systems  processes can communicate using shared memory by having the communicating processes memory-map the same file into their virtual address spaces the memorymapped file serves as the region of shared memory between the communicating processes  figure 9.24   in the following section  we illustrate support in the win32 api for shared memory using memory-mapped files 9.7.2 shared memory in the win32 api the general outline for creating a region of shared memory using memorymapped files in the win32 api involves first creating a file mapping for the file to be mapped and then establishing a view of the mapped file in a process 's virtual address space a second process can then open and create a view of the mapped file in its virtual address space the mapped file represents the shared-menwry object that will enable communication to take place between the processes we next illustrate these steps in more detail in this example  a producer process first creates a shared-memory object using the memory-mapping features available in the win32 api the producer then writes a message to shared m.emory after that  a consumer process opens a mapping to the shared-memory object and reads the message written by the consum.er to establish a memory-mapped file  a process first opens the file to be mapped with the createfile   function  which returns a handle to the opened file the process then creates a mapping of this file handle using the createfilemapping   function once the file mapping is established  the process then establishes a view of the mapped file in its virtual address space with the mapviewdffile   function the view of the mapped file represents the portion of the file being mapped in the virtual address space of the process 394 chapter 9 # include windows.h # include stdio.h int main  int argc  char argv      handle hfile  hmapfile ; lpvoid lpmapaddress ; hfile = createfile  temp.txt  //file name genericjread i generic_write  // read/write access 0  ii no sharing of the file null  //default security open_always  //open new or existing file file_attribute_normal  //routine file attributes null  ; //no file template hmapfile = createfilemapping  hfile  //file handle null  //default security pagejreadwrite  //read/write access to mapped pages 0  ii map entire file 0  text  sharedobject   ; //named shared memory object lpmapaddress = mapviewdffile  hmapfile  //mapped object handle filejmap_all_access  //read/write access 0  ii mapped view of entire file 0  0  ; ii write to shared memory sprintf  lpmapaddress  shared memory message  ; unmapviewoffile  lpmapaddress  ; closehandle  hfile  ; closehandle  hmapfile  ; figure 9.25 producer writing to shared memory using the win32 api -the entire file or only a portion of it may be mapped we illustrate this sequence in the program shown in figure 9 .25  we eliminate much of the error checking for code brevity  the call to createfilemapping   creates a named shared-memory object called sharedobj ect the consumer process will communicate using this shared-memory segment by creating a mapping to the same named object the producer then creates a view of the memory-mapped file in its virtual address space by passing the last three parameters the value 0  it indicates that the mapped view is the entire file it could instead have passed values specifying an offset and size  thus creating a view containing only a subsection of the file  it is important to note that the entire mapping may not be loaded # include windows.h # include stdio.h int main  int argc  char argv     handle hmapfile ; lpvoid lpmapaddress ; 9.7 395 hmapfile = openfilemapping  file_map_all_access  // r/w access false  //no inheritance  text  sharedobject   ; //name of mapped file object lpmapaddress = mapviewoffile  hmapfile  //mapped object handle filejmap_all_access  //read/write access 0  ii mapped view of entire file 0  0  ; ii read from shared memory printf  read message % s  lpmapaddress  ; unmapviewoffile  lpmapaddress  ; closehandle  hmapfile  ; figure 9.26 consumer reading from shared memory using the win32 api into memory when the mapping is established rather  the mapped file may be demand-paged  thus bringing pages into memory only as they are accessed  the mapviewoffile   fm1ction returns a pointer to the shared-memory object ; any accesses to this memory location are thus accesses to the memory-mapped file in this ii1stance  the producer process writes the message shared memory message to shared memory a program illustrating how the consumer process establishes a view of the named shared-memory object is shown in figure 9.26 this program is somewhat simpler than the one shown in figure 9.25  as all that is necessary is for the process to create a mapping to the existii1g named shared-memory object the consumer process must also create a view of the mapped file  just as the producer process did ii1 the program in figure 9.25 the consumer then reads from shared memory the message shared memory message thatwas written by the producer process finally  both processes remove the view of the mapped file with a call to unmapviewoffile    we provide a programming exercise at the end of this chapter using shared memory with memory mapping in the win32 api 9.7.3 memory-mapped i/0 in the case of i/0  as mentioned in section 1.2.1  each i/0 controller includes registers to hold commands and the data being transferred usually  special i/0 instructions allow data transfers between these registers and system memory 396 chapter 9 9.8 to allow more convenient access to i/0 devices1 many computer architectures provide in this case/ ranges of memory addresses are set aside and are mapped to the device registers reads and writes to these memory addresses cause the data to be transferred to and from the device registers this method is appropriate for devices that have fast response times/ such as video controllers in the ibm pc each location on the screen is mapped to a n1.emory location displaying text on the screen is almost as easy as writing the text into the appropriate memory-mapped locations memory-mapped i/o is also convenient for other devices/ such as the serial and parallel ports used to connect modems and printers to a computer the cpu transfers data through these kinds of devices by reading and writing a few device registers/ called an i/0 to send out a long string of bytes through a memory-mapped serial port1 the cpu writes one data byte to the data register and sets a bit in the control register to signal that the byte is available the device takes the data byte and then clears the bit in the control register to signal that it is ready for the next byte then the cpu can transfer the next byte if the cpu uses polling to watch the control bit/ constantly looping to see whether the device is ready/ this method of operation is called if the cpu does not poll the control bit/ but instead receives an interrupt when the device is ready for the next byte/ the data transfer is said to be when a process running in user rnode requests additional memory/ pages are allocated from the list of free page frames maintained by the kernel this list is typically populated using a page-replacement algorithm such as those discussed in section 9.4 and most likely contains free pages scattered throughout physical memory/ as explained earlier remember/ too/ that if a user process requests a single byte of memory/ internal fragmentation will result/ as the process will be granted an entire page frame kernel memory/ however1 is often allocated from a free-memory pool different from the list used to satisfy ordinary user-mode processes there are two primary reasons for this  the kernel requests memory for data structures of varying sizes  some of which are less than a page in size as a result1 the kernel must use memory conservatively and attempt to minimize waste due to fragmentation this is especially important because many operating systems do not subject kernel code or data to the paging system 2 pages allocated to user-mode processes do not necessarily have to be in contiguous physical memory however/ certain hardware devices interact directly with physical memory-without the benefit of a virtual memory interface-and consequently may require memory residing in physically contiguous pages in the following sections/ we examine two strategies for managing free memory that is assigned to kernel processes  the buddy system and slab allocation 9.8 397 9.8.1 buddy system tbe buddy system allocates memory from a fixed-size segment consisting of physically contiguous pages memory is allocated from this segment using a power-of-2 allocator  which satisfies requests in units sized as a power of 2  4 kb  8 kb  16 kb  and so forth   a request in units not appropriately sized is rounded up to the next highest power of 2 for example  if a request for 11 kb is made  it is satisfied with a 16-kb segment let 's consider a simple example assume the size of a memory segment is initially 256 kb and the kernel requests 21 kb of memory the segment is initially divided into two buddies-which we will call al and ar -each 128 kb in size one of these buddies is further divided into two 64-kb buddiesbland br however  the next-highest power of 2 from 21 kb is 32 kb so either bt or br is again divided into two 32-kb buddies  cl and cr one of these buddies is used to satisfy the 21-kb request this scheme is illustrated in figure 9.27  where cl is the segment allocated to the 21 kb request an advantage of the buddy system is how quickly adjacent buddies can be combined to form larger segments using a teclmique known as coalescing in figure 9.27  for example  when the kernel releases the cl unit it was allocated  the system can coalesce c l and c r into a 64-kb segment this segment  b l  can in turn be coalesced with its buddy b r to form a 128-kb segment ultimately  we can end up with the original256-kb segment the obvious drawback to the buddy system is that rounding up to the next highest power of 2 is very likely to cause fragmentation within allocated segments for example  a 33-kb request can only be satisfied with a 64 kb segment in fact  we can not guarantee that less than 50 percent of the allocated unit will be wasted due to internal fragmentation in the following section  we explore a memory allocation scheme where no space is lost due to fragmentation physically contiguous pages 256 kb figure 9.27 buddy system allocation 398 chapter 9 9.8.2 slab allocation a second strategy for allocating kernel memory is known as a is made up of one or nwre physically contiguous pages a consists of one or more slabs there is a single cache for each unique kernel data structure -for example  a separate cache for the data structure representing process descriptors  a separate cache for file objects  a separate cache for semaphores  and so forth each cache is populated with that are instantiations of the kernel data structure the cache represents for example  the cache representing semaphores stores instances of semaphore objects  the cache representing process descriptors stores instances of process descriptor objects  and so forth the relationship between slabs  caches  and objects is shown in figure 9.28 the figure shows two kernel objects 3 kb in size and three objects 7 kb in size these objects are stored in their respective caches the slab-allocation algorithm uses caches to store kernel objects when a cache is created  a number of objects-which are initially marked as free-are allocated to the cache the number of objects in the cache depends on the size of the associated slab for example  a 12-kb slab  made up of three continguous 4-kb pages  could store six 2-kb objects initially  all objects in the cache are marked as free when a new object for a kernel data structure is needed  the allocator can assign any free object from the cache to satisfy the request the object assigned from the cache is marked as used let 's consider a scenario in which the kernel requests memory from the slab allocator for an object representing a process descriptor in linux systems  a process descriptor is of the type struct task_struct  which requires approximately 1.7 kb of memory when the linux kernel creates a new task  it requests the necessary memory for the struct task_struct object from its cache the cache will fulfill the request using a struct task_struct object that has already been allocated in a slab and is marked as free in linux  a slab may be in one of three possible states  kernel objects slabs 3-kb objects 7-kb objects figure 9.28 slab allocation physically contiguous pages 9.9 9.9 full all objects in the slab are marked as used empty all objects in the slab are marked as free partial the slab consists of both used and free objects 399 the slab allocator first attempts to satisfy the request with a free object in a partial slab if none exist  a free object is assigned from an empty slab if no empty slabs are available  a new slab is allocated from contiguous physical pages and assigned to a cache ; memory for the object is allocated from this slab the slab allocator provides two main benefits  no memory is wasted due to fragmentation fragn entation is not an issue because each unique kernel data structure has an associated cache  and each cache is made up of one or more slabs that are divided into chunks the size of the objects being represented thus  when the kernel requests memory for an object  the slab allocator returns the exact amount of memory required to represent the object memory requests can be satisfied quickly the slab allocation scheme is thus particularly effective for mm aging memory when objects are frequently allocated and deallocated  as is often the case with requests from the kernel the act of allocating-and releasing-memory can be a time-consuming process however  objects are created in advance and thus can be quickly allocated from the cache furthermore  when the kernel has finished with an object and releases it  it is marked as free and returned to its cache  thus making it immediately available for subsequent requests fi om the kernel the slab allocator first appeared in the solaris 2.4 kernel because of its general-purpose nature  this allocator is now also used for certain user-mode memory requests in solaris linux originally used the buddy system ; however  beginning with version 2.2  the linux kernel adopted the slab allocator the major decisions that we make for a paging system are the selections of a replacement algorithm and an allocation policy  which we discussed earlier in this chapter there are many other considerations as well  and we discuss several of them here 9.9.1 prepaging an obvious property of pure demand paging is the large number of page faults that occur when a process is started this situation results from trying to get the initial locality into memory the same situation may arise at other times for instance  when a swapped-out process is restarted  all its are on the disk  and each must be brought in by its own page fault is an attempt to prevent this high level of initial paging the strategy is to bring into memory at 400 chapter 9 one tin1.e all the pages that will be needed some operating systerns-notably solaris-prepage the page frames for small files in a system using the working-set model  for example  we keep with each process a list of the pages in its working set if we must suspend a process  due to an i/0 wait or a lack of free frames   we remember the working set for that process when the process is to be resumed  because i/0 has finished or enough free frames have become available   we automatically bring back into memory its entire working set before restarting the process prepaging may offer an advantage in some cases the question is simply whether the cost of using prepaging is less than the cost of servicing the corresponding page faults it may well be the case that many of the pages brought back into memory by prepaging will not be used assume that s pages are prepaged and a fraction a of these s pages is actually used  0  '    a  '    1   the question is whether the cost of the s .a saved page faults is greater or less than the cost of prepaging s   1  a  unnecessary pages if a is close to 0  prepaging loses ; if a is close to 1  prepaging wins 9.9.2 page size the designers of an operating system for an existing machine seldom have a choice concerning the page size however  when new machines are being designed  a decision regarding the best page size must be made as you might expect  there is no single best page size rather  there is a set of factors that support various sizes page sizes are invariably powers of 2  generally ranging from 4,096  212  to 4,194,304  222  bytes how do we select a page size one concern is the size of the page table for a given virtual memory space  decreasing the page size increases the number of pages and hence the size of the page table for a virtual memory of 4 mb  222   for example  there would be 4,096 pages of 1,024 bytes but only 512 pages of 8,192 bytes because each active process must have its own copy of the page table  a large page size is desirable memory is better utilized with smaller pages  however if a process is allocated memory starting at location 00000 and continuing until it has as much as it needs  it probably will not end exactly on a page boundary thus  a part of the final page must be allocated  because pages are the units of allocation  but will be unused  creating internal fragmentation   assuming independence of process size and page size  we can expect that  on the average  half of the final page of each process will be wasted this loss is only 256 bytes for a page of 512 bytes but is 4,096 bytes for a page of 8,192 bytes to minimize internal fragmentation  then  we need a small page size another problem is the time required to read or write a page i/0 time is composed of seek  latency  and transfer times transfer time is proportional to the amount transferred  that is  the page size  -a fact that would seem to argue for a small page size howeve1 ~ as we shall see in section 12.1.1  latency and seek time normally dwarf transfer time at a transfer rate of 2 mb per second  it takes only 0.2 milliseconds to transfer 512 bytes latency time  though  is perhaps 8 milliseconds and seek time 20 milliseconds of the total i/0 time  28.2 milliseconds   therefore  only 1 percent is attributable to the actual transfer doubling the page size increases i/0 time to only 28.4 milliseconds it takes 28.4 milliseconds to read a single page of 1,024 bytes but 9.9 401 56.4 milliseconds to read the sam.e amount as two pages of 512 bytes each thus  a desire to minimize 1/0 time argues for a larger page size with a smaller page size  though  to tall /0 should be reduced  since locality will be improved a smaller page size allows each page to match program locality more accurately for example  consider a process 200 kb in size  of which only half  100 kb  is actually used in an execution if we have only one large page  we must bring in the entire page  a total of 200 kb transferred and allocated if instead we had pages of only 1 byte  then we could bring in only the 100 kb that are actually used  resulting in only 100 kb transferred and allocated with a smaller page size  we have better allowing us to isolate only the memory that is actually needed with a larger page size  we must allocate and transfer not only what is needed but also anything else that happens to be in the page  whether it is needed or not thus  a smaller page size should result in less i/0 and less total allocated memory but did you notice that with a page size of 1 byte  we would have a page fault for each byte a process of 200 kb that used only half of that memory would generate only one page fault with a page size of 200 kb but 102,400 page faults with a page size of 1 byte each page fault generates the large amount of overhead needed for processing the interrupt  saving registers  replacing a page  queueing for the paging device  and updating tables to minimize the number of page faults  we need to have a large page size other factors must be considered as well  such as the relationship between page size and sector size on the paging device   the problem has no best answer as we have seen  some factors  internal fragmentation  locality  argue for a small page size  whereas others  table size  i/0 time  argue for a large page size however  the historical trend is toward larger page sizes indeed  the first edition of operating system concepts  1983  used 4,096 bytes as the upper bound on page sizes  and this value was the most common page size in 1990 modern systems may now use much larger page sizes  as we will see in the following section 9.9.3 tlb reach in chapter 8  we introduced the of the tlb recall that the hit ratio for the tlb refers to the percentage of virtual address translations that are resolved in the tlb rather than the page table clearly  the hit ratio is related to the number of entries in the tlb  and the way to increase the hit ratio is by increasing the number of entries in the tlb this  however  does not come cheaply  as the associative memory used to construct the tlb is both expensive and power hungry related to the hit ratio is a similar metric  the the tlb reach refers to the amount of memory accessible from the tlb and is simply the number of entries multiplied by the page size ideally  the working set for a process is stored in the tlb if it is not  the process will spend a considerable amount of time resolving memory references in the page table rather than the tlb if we double the number of entries in the tlb  we double the tlb reach however  for some memory-intensive applications  this may still prove insufficient for storing the working set another approacl1 for increasing the tlb reach is to either increase the size of the page or provide multiple page sizes if we increase the page size-say  402 chapter 9 from 8 kb to 32 kb-we quadruple the tlb reach however  this may lead to an increase in fragmentation for some applications that do not require such a large page size as 32 kb alternatively  an operating system may provide several different page sizes for example  the ultrasparc supports page sizes of 8 kb  64 kb  512 kb  and 4mb of these available pages sizes  solaris uses both 8-kb and 4-mb page sizes and with a 64-entry tlb  the tlb reach for solaris ranges from 512 kb with 8-kb pages to 256mb with 4-mb pages for the majority of applications  the 8-kb page size is sufficient  although solaris maps the first 4 mb of kernel code and data with two 4-mb pages solaris also allows applications-such as databases-to take advantage of the large 4-mb page size providing support for multiple page sizes requires the operating system -not hardware-to manage the tlb for example  one of the fields in a tlb entry must indicate the size of the page frame corresponding to the tlb entry managing the tlb in software and not hardware comes at a cost in performance howeve1 ~ the increased hit ratio and tlb reach offset the performance costs indeed  recent trends indicate a move toward softwaremanaged tlbs and operating-system support for multiple page sizes the ultrasparc  mips  and alpha architectures employ software-managed tlbs the powerpc and pentium manage the tlb in hardware 9.9.4 inverted page tables section 8.5.3 introduced the concept of the inverted page table the purpose of this form of page management is to reduce the amount of physical memory needed to track virtual-to-physical address translations we accomplish this savings by creating a table that has one entry per page of physical memory  indexed by the pair process-id  page-number  because they keep information about which virtual memory page is stored in each physical frame  inverted page tables reduce the amount of physical memory needed to store this information however  the inverted page table no longer contains complete information about the logical address space of a process  and that information is required if a referenced page is not currently in memory demand paging requires this information to process page faults for the information to be available  an external page table  one per process  must be kept each such table looks like the traditional per-process page table and contains information on where each virtual page is located but do external page tables negate the utility of inverted page tables since these tables are referenced only when a page fault occurs  they do not need to be available quickly instead  they are themselves paged in and out of memory as necessary unfortunately  a page fault may now cause the virtual memory n1.anager to generate another page fault as it pages in the external page table it needs to locate the virtual page on the backing store this special case requires careful handling in the kernel and a delay in the page-lookup processing 9.9.5 program structure demand paging is designed to be transparent to the user program in many cases  the user is completely unaware of the paged nature of memory in other cases  however  system performance can be improved if the user  or compiler  has an awareness of the underlying demand paging 9.9 403 let 's look at a contrived but informative example assume that pages are 128 words in size consider a c program whose function is to initialize to 0 each element of a 128-by-128 array the following code is typical  inti  j ; int  128j  128j data ; for  j = 0 ; j 128 ; j + +  for  i = 0 ; i 128 ; i + +  data  ij  jj = 0 ; notice that the array is stored row major ; that is  the array is stored data  oj  oj  data  oj  1j   data  oj  127j  data  1j  oj  data  1j  1j   data  127j  127j for pages of 128 words  each row takes one page thus  the preceding code zeros one word in each page  then another word in each page  and so on if the operating system allocates fewer than 128 frames to the entire program  then its execution will result in 128 x 128 = 16,384 page faults in contrast  suppose we change the code to inti  j ; int  128j  128j data ; for  i = 0 ; i 128 ; i + +  for  j = 0 ; j 128 ; j + +  data  ij  jj = 0 ; this code zeros all the words on one page before starting the next page  reducing the number of page faults to 128 careful selection of data structures and programming structures can increase locality and hence lower the page-fault rate and the number of pages in the working set for example  a stack has good locality  since access is always made to the top a hash table  in contrast  is designed to scatter references  producing bad locality of course  locality of reference is just one measure of the efficiency of the use of a data structure other heavily weighted factors include search speed  total number of memory references  and total number of pages touched at a later stage  the compiler and loader can have a sigicificant effect on paging separating code and data and generating reentrant code means that code pages can be read-only and hence will never be modified clean pages do not have to be paged out to be replaced the loader can avoid placing routines across page boundaries  keeping each routine completely in one page routines that call each other many times can be packed into the same page this packaging is a variant of the bin-packing problem of operations research  try to pack the variable-sized load segments into the fixed-sized pages so that interpage references are minimized such an approach is particularly useful for large page sizes the choice of programming language can affect paging as well for example  c and c + + use pointers frequently  and pointers tend to randomize access to memory  thereby potentially diminishing a process 's locality some studies have shown that object-oriented programs also tend to have a poor locality of reference 404 chapter 9 9.9.6 1/0 interlock when demand paging is used  we sometimes need to allow some of the pages to be in n emory one such situation occurs when i/0 is done to or from user  virtual  memory l/0 is often implemented by a separate i/0 processor for example  a controller for a usb storage device is generally given the number of bytes to transfer and a memory address for the buffer  figure 9.29   when the transfer is complete  the cpu is interrupted we must be sure the following sequence of events does not occur  a process issues an i/0 request and is put in a queue for that i/o device meanwhile  the cpu is given to other processes these processes cause page faults ; and one of them  using a global replacement algorithm  replaces the page containing the memory buffer for the waiting process the pages are paged out some time later  when the i/o request advances to the head of the device queue  the i/o occurs to the specified address however  this frame is now being used for a different page belonging to another process there are two common solutions to this problem one solution is never to execute i/0 to user memory instead  data are always copied between system memory and user memory i/0 takes place only between system memory and the i/0 device to write a block on tape  we first copy the block to system memory and then write it to tape this extra copying may result in unacceptably high overhead another solution is to allow pages to be locked into memory here  a lock bit is associated with every frame if the frame is locked  it can not be selected for replacement under this approach  to write a block on tape  we lock into memory the pages containing the block the system can then continue as usual locked pages can not be replaced when the i/o is complete  the pages are unlocked figure 9.29 the reason why frames used for 1/0 must be in memory 9.10 9.10 405 lock bits are used in various situations frequently  some or all of the operating-system kernel is locked into memory  as many operating systems can not tolerate a page fault caused by the kernel another use for a lock bit involves normal page replacement consider the following sequence of events  a low-priority process faults selecting a replacement frame  the paging system reads the necessary page into memory ready to continue  the low-priority process enters the ready queue and waits for the cpu since it is a low-priority process  it may not be selected by the cpu scheduler for a time while the low-priority process waits  a high-priority process faults looking for a replacement  the paging system sees a page that is in memory but has not been referenced or modified  it is the page that the low-priority process just brought in this page looks like a perfect replacement  it is clean and will not need to be written out  and it apparently has not been used for a long time whether the high-priority process should be able to replace the low-priority process is a policy decision after all  we are simply delaying the low-priority process for the benefit of the high-priority process however  we are wasting the effort spent to bring in the page for the low-priority process if we decide to prevent replacement of a newly brought-in page until it can be used at least once  then we can use the lock bit to implement this mechanism when a page is selected for replacement  its lock bit is turned on ; it remains on until the faulting process is again dispatched using a lock bit can be dangerous  the lock bit may get turned on but never turned off should this situation occur  because of a bug in the operating system  for example   the locked frame becomes unusable on a single-user system  the overuse of locking would hurt only the user doing the locking multiuser systems must be less trusting of users for instance  solaris allows locking hints  but it is free to disregard these hints if the free-frame pool becomes too small or if an individual process requests that too many pages be locked in memory in this section  we describe how windows xp and solaris implement virtual memory 9.10.1 windows xp windows xp implements virtual memory using demand paging with clustering handles page faults by bringing in not only the faultil1.g page also several pages following the faulting page when a process is first created  it is assigned a working-set minimum and maximum the is the minimum number of pages the process is guaranteed to in memory if sufficient memory is available  a process may be assigned as many pages as its for most applications  the value of working-set minimum and working-set maximum is 50 and 345 pages  respectively  in some circumstances  a process may be allowed to exceed its working-set maximum  the virtual memory manager maintains a list of free page frames associated with this list is a threshold value that is used to 406 chapter 9 indicate whether sufficient free memory is available if a page fault occurs for a process that is below its working-set maximum  the virtual memory manager allocates a page from this list of free pages if a process that is at its working-set rnaximum incurs a page fault  it must select a page for replacement using a local page-replacement policy when the amount of free memory falls below the threshold  the virtual memory manager uses a tactic known as to restore the value above the threshold automatic working-set trimming works by evaluating the number of pages allocated to processes if a process has been allocated more pages than its working-set minimum  the virtual memory manager removes pages until the process reaches its working-set minimum a process that is at its working-set minimum may be allocated pages from the free-page-frame list once sufficient free memory is available the algorithm used to determine which page to remove from a working set depends on the type of processor on single-processor 80x86 systems  windows xp uses a variation of the clock algorithm discussed in section 9.4.5.2 on alpha and multiprocessor x86 systems  clearing the reference bit may require invalidatil g the entry in the translation look-aside buffer on other processors rather than incurring this overhead  windows xp uses a variation on the fifo algorithm discussed in section 9.4.2 9.10.2 solaris in solaris  when a thread incurs a page fault  the kernel assigns a page to the faulting thread from the list of free pages it maintains therefore  it is imperative that the kernel keep a sufficient amount of free memory available associated with this list of free pages is a parameter-zotsfree-that represents a threshold to begin paging the lotsfree parameter is typically set to 1/64 the size of the physical memory four times per second  the kernel checks whether the amount of free memory is less than lotsfree if the number of free pages falls below lotsfree  a process known as a pageout starts up the pageout process is similar to the second-chance algorithm described in section 9.4.5.2  except that it uses two hands while scanning pages  rather than one the pageout process works as follows  the front hand of the clock scans all pages in memory  setting the reference bit to 0 later  the back hand of the clock examines the reference bit for the pages in memory  appending each page whose reference bit is still set to 0 to the free list and writing to disk its contents if modified solaris maintains a cache list of pages that have been freed but have not yet been overwritten the free list contains frames that have invalid contents pages can be reclaimed from the cache list if they are accessed before being moved to the free list the pageout algorithm uses several parameters to control the rate at which pages are scam ed  known as the scanrate   the scanrate is expressed in pages per second and ranges from slowscan to fastscan when free memory falls below lotsfree  scanning occurs at slowscan pages per second and progresses to fastscan  depending on the amount of free memory available the default value of slowscan is 100 pages per second ; fasts can is typically set to the value  total physical pages  /2 pages per second  with a maximum of 8,192 pages per second this is shown in figure 9.30  withfastscan set to the maximum   the distance  in pages  between the hands of the clock is determil ed by a system parameter  handspread the amount of time between the front hand 's 9.11 8192 fastscan cll 7 c  1j u en 100 slowscan minfree desfree amount of free memory figure 9.30 solaris page scanner 9.11 407 lotsfree clearing a bit and the back hand 's investigating its value depends on the scanrate and the handspread if scam-ate is 100 pages per second and handspread is 1,024 pages  10 seconds can pass between the time a bit is set by the front hand and the time it is checked by the back hand however  because of the demands placed on the memory system  a scanrate of several thousand is not uncommon this means that the amount of time between clearing and investigating a bit is often a few seconds as mentioned above  the pageout process checks memory four times per second however  if free memory falls below desfree  figure 9.30   pageout will nm 100 times per second with the intention of keeping at least desfree free memory available if the pageout process is unable to keep the amount of free memory at desfree for a 30-second average  the kernel begins swapping processes  thereby freeing all pages allocated to swapped processes in general  the kernel looks for processes that have been idle for long periods of time if the system is unable to maintain the amount of free memory at minfree  the pageout process is called for every request for a new page recent releases of the solaris kernel have provided enhancements of the paging algorithm one such enhancement involves recognizing pages from shared libraries pages belonging to libraries that are being shared by several processes-even if they are eligible to be claimed by the scannerare skipped during the page-scanning process another enhancement concerns distinguishing pages that have been allocated to processes from pages allocated to regularfiles this is known as and is covered in section 11.6.2 it is desirable to be able to execute a process whose logical address space is larger than the available physical address space virtual memory is a technique 408 chapter 9 that enables us to map a large logical address space onto a smaller physical menlory virtual memory allows us to run extremely large processes and to raise the degree of multiprogramming  increasing cpu utilization further  it frees application programmers from worrying about memory availability in addition  with virtual memory  several processes can share system libraries and memory virtual memory also enables us to use an efficient type of process creation known as copy-on-write  wherein parent and child processes share actual pages of memory virtual memory is commonly implemented by demand paging pure demand paging never brings in a page until that page is referenced the first reference causes a page fault to the operating system the operating-system kernel consults an internal table to determine where the page is located on the backing store it then finds a free frame and reads the page in from the backing store the page table is updated to reflect this change  and the instruction that caused the page fault is restarted this approach allows a process to run even though its entire memory image is not in main memory at once as long as the page-fault rate is reasonably low  performance is acceptable we can use demand paging to reduce the number of frames allocated to a process this arrangement can increase the degree of multiprogramming  allowing more processes to be available for execution at one time  and-in theory  at least-the cpu utilization of the system it also allows processes to be run even though their memory requirements exceed the total available physical memory such processes run in virtual memory if total memory requirements exceed the capacity of physical memory  then it may be necessary to replace pages from memory to free frames for new pages various page-replacement algorithms are used fifo page replacement is easy to program but suffers from belady 's anomaly optimal page replacement requires future knowledge lru replacement is an approximation of optimal page replacement  but even it may be difficult to implement most page-replacement algorithms  such as the second-chance algorithm  are approximations of lru replacement in addition to a page-replacement algorithm  a frame-allocation policy is needed allocation can be fixed  suggesting local page replacement  or dynamic  suggesting global replacement the working-set model assumes that processes execute in localities the working set is the set of pages in the current locality accordingly  each process should be allocated enough frames for its current working set if a process does not have enough memory for its working set  it will thrash providing enough frames to each process to avoid thrashing may require process swapping and schedulil g most operating systems provide features for memory mappil1g files  thus allowing file i/0 to be treated as routine memory access the win32 api implements shared memory through memory mappil1g files kernel processes typically req1.1ire memory to be allocated using pages that are physically contiguous the buddy system allocates memory to kernel processes in units sized according to a power of 2  which often results in fragmentation slab allocators assign kernel data structures to caches associated with slabs  which are made up of one or more physically contiguous pages with slab allocation  no memory is wasted due to fragmentation  and memory requests can be satisfied quickly 409 in addition to reqmnng that we solve the major problems of page replacement and frame allocation  the proper design of a paging systern requires that we consider prep aging  page size  tlb reach  inverted page tables  program structure  i/0 interlock  and other issues 9.1 assume there is a 1,024-kb segment where memory is allocated using the buddy system using figure 9.27 as a guide  draw a tree illustrating how the following memory requests are allocated  request 240 bytes request 120 bytes request 60 bytes request 130 bytes next modify the tree for the followilcg releases of memory perform coalescing whenever possible  release 240 bytes release 60 bytes release 120 bytes 9.2 consider the page table for a system with 12-bit virtual and physical addresses with 256-byte pages the list of free page frames is d  e  f  that is  dis at the head of the list e is second  and f is last   410 chapter 9 convert the following virtual addresses to their equivalent physical addresses in hexadecimal all numbers are given in hexadecimal  a dash for a page frame indicates that the page is not in memory  9ef 111 700 off 9.3 a page-replacement algorithm should minimize the number of page faults we can achieve this minimization by distributing heavily used pages evenly over all of memory  rather than having them compete for a small number of page frames we can associate with each page frame a counter of the number of pages associated with that frame then  to replace a page  we can search for the page frame with the smallest counter a define a page-replacement algorithm using this basic idea specifically address these problems  i what is the initial value of the counters ii when are counters increased iii when are counters decreased 1v how is the page to be replaced selected b how many page faults occur for your algorithm for the following reference string with four page frames 1  2  3  4  5  3  4  1  6  7  8  7  8  9  7  8  9  5  4  5  4  2 c what is the minimum number of page faults for an optimal pagereplacement strategy for the reference string in part b with four page frames 9.4 consider a demand-paging system with the following time-measured utilizations  cpu utilization paging disk other i/0 devices 20 % 97.7 % 5 % for each of the following  say whether it will  or is likely to  improve cpu utilization explain your answers a install a faster cpu b install a bigger paging disk c increase the degree of multiprogramming d decrease the degree of multiprogramming 411 e install more main n1.enl0ry f install a faster hard disk or multiple controllers with multiple hard disks g add prepaging to the page-fetch algorithms h increase the page size 9.5 consider a demand-paged computer system where the degree of multiprogramming is currently fixed at four the system was recently measured to determine utilization of the cpu and the paging disk the results are one of the following alternatives for each case  what is happening can the degree of multiprogramming be increased to increase the cpu utilization is the paging helping a cpu utilization 13 percent ; disk utilization 97 percent b cpu utilization 87 percent ; disk utilization 3 percent c cpu utilization 13 percent ; disk utilization 3 percent 9.6 consider a demand-paging system with a paging disk that has an average access and transfer time of 20 milliseconds addresses are translated through a page table in main memory  with an access time of 1 microsecond per memory access thus  each memory reference through the page table takes two accesses to improve this time  we have added an associative memory that reduces access time to one memory reference if the page-table entry is in the associative memory assume that 80 percent of the accesses are in the associative memory and that  of those remaining  10 percent  or 2 percent of the total  cause page faults what is the effective memory access time 9.7 a simplified view of thread states is ready  running  and blocked  where a thread is either ready and waiting to be scheduled  is running on the processor  or is blocked  i.e is waiting for i/0  this is illustrated in figure 9.31 assuming a thread is in the running state  answer the following questions   be sure to explain your answer  a will the thread change state if it incurs a page fault if so  to what new state figure 9.31 thread state diagram for exercise 9.7 412 chapter 9 b will the thread change state if it generates a tlb miss that is resolved in the page table if so  to what new state c will the thread change state if an address reference is resolved in the page table if so  to what new state 9.8 discuss the hardware support required to support demand paging 9.9 consider the following page reference string  1  2  3  4  2  1  5  6  2  1  2  3  7  6  3  2  1  2  3  6 how many page faults would occur for the following replacement algorithms  assuming one  two  three  four  five  six  and seven frames remember that all frames are initially empty  so your first unique pages will cost one fault each lru replacement fifo replacement optimal replacement 9.10 consider a system that allocates pages of different sizes to its processes what are the advantages of such a paging scheme what modifications to the virtual memory system provide this functionality 9.11 discuss situations in which the most frequently used page-replacement algorithm generates fewer page faults than the least recently used page-replacement algorithm also discuss under what circumstances the opposite holds 9.12 under what circumstances do page faults occur describe the actions taken by the operating system when a page fault occurs 9.13 suppose that a machine provides instructions that can access memory locations using the one-level indirect addressing scheme what sequence of page faults is ilccurred when all of the pages of a program are currently nonresident and the first instruction of the program is an indirect memory-load operation what happens when the operating system is using a per-process frame allocation technique and only two pages are allocated to this process 9.14 consider a system that provides support for user-level and kernellevel threads the mapping in this system is one to one  there is a corresponding kernel thread for each user thread   does a multithreaded process consist of  a  a working set for the entire process or  b  a working set for each thread explain 413 9.15 what is the copy-on-write feature  and under what circumstances is it beneficial to use this feature what hardware support is required to implement this feature 9.16 consider the two-dimensional array a  int a     = new int  100   100  ; where a  oj  oj is at location 200 in a paged memory system with pages of size 200 a small process that manipulates the matrix resides in page 0  locations 0 to 199   thus  every instruction fetch will be from page 0 for three page frames  how many page faults are generated by the following array-initialization loops  using lru replacement and assuming that page frame 1 contains the process and the other two are initially empty a for  int j = 0 ; j 100 ; j + +  for  int i = 0 ; i 100 ; i + +  a  i   j  = 0 ; b for  int i = 0 ; i 100 ; i + +  for  int j = 0 ; j 100 ; j + +  a  i   j  = 0 ; 9.17 discuss situations in which the least frequently used page-replacement algorithm generates fewer page faults than the least recently used page-replacement algorithm also discuss under what circumstances the opposite holds 9.18 what is the cause of thrashing how does the system detect thrashing once it detects thrashing  what can the system do to eliminate this problem 9.19 assume that you are monitoring the rate at which the pointer in the clock algorithm  which indicates the candidate page for replacement  moves what can you say about the system if you notice the following behavior  a pointer is moving fast b pointer is moving slow 9.20 the vax/vms system uses a fifo replacement algorithm for resident pages and a free-frame pool of recently used pages assume that the free-frame pool is managed using the least recently used replacement policy answer the following questions  a if a page fault occurs and if the page does not exist in the free-frame pool  how is free space generated for the newly requested page 414 chapter 9 b if a page fault occurs and if the page exists in the free-frame pool  how is the resident page set and the free-france pool managed to make space for the requested page c what does the system degenerate to if the number of resident pages is set to one d what does the system degenerate to if the number of pages in the free-frame pool is zero 9.21 the slab-allocation algorithm uses a separate cache for each different object type assuming there is one cache per object type  explain why this scheme does n't scale well with multiple cpus what could be done to address this scalability issue 9.22 assume that we have a demand-paged memory the page table is held in registers it takes 8 milliseconds to service a page fault if an empty frame is available or if the replaced page is not modified and 20 milliseconds if the replaced page is modified memory-access time is 100 nanoseconds assume that the page to be replaced is modified 70 percent of the time what is the maximum acceptable page-fault rate for an effective access time of no more than 200 nanoseconds 9.23 segmentation is similar to paging but uses variable-sized pages define two segment-replacement algorithms based on fifo and lru pagereplacement schemes remember that since segments are not the same size  the segment that is chosen to be replaced may not be big enough to leave enough consecutive locations for the needed segment consider strategies for systems where segments cam ot be relocated and strategies for systems where they can 9.24 which of the following programming techniques and structures are good for a demand-paged environment which are not good explain your answers a stack b hashed symbol table c sequential search d binary search e pure code f vector operations a indirection b 9.25 when a page fault occurs  the process requesting the page must block while waiting for the page to be brought from disk into physical memory assume that there exists a process with five user-level threads and that the mapping of user threads to kernel threads is many to one if one user thread incurs a page fault while accessing its stack  would the other user user threads belonging to the same process also be affected by the page fault-that is  would they also have to wait for the faulting page to be brought into memory explain 415 9.26 consider a system that uses pure demand paging a when a process first starts execution  how would you characterize the page fault rate b once the working set for a process is loaded into memory  how would you characterize the page fault rate c assume that a process changes its locality and the size of the new working set is too large to be stored in available free memory identify some options system designers could choose from to handle this situation 9.27 assume that a program has just referenced an address in virtual memory describe a scenario in which each of the following can occur  if no such scenario can occur  explain why  tlb miss with no page fault tlb miss and page fault tlb hit and no page fault tlb hit and page fault 9.28 a certain computer provides its users with a virtual memory space of 232 bytes the computer has 218 bytes of physical memory the virtual memory is implemented by paging  and the page size is 4,096 bytes a user process generates the virtual address 11123456 explain how the system establishes the corresponding physical location distinguish between software and hardware operations 9.29 when virtual memory is implemented in a computing system  there are certain costs associated with the technique and certain benefits list the costs and the benefits is it possible for the costs to exceed the benefits if it is  what measures can be taken to ensure that this does not happen 9.30 give an example that illustrates the problem with restarting the move character instruction  mvc  on the ibm 360/370 when the source and destination regions are overlapping 9.31 consider the parameter 6 used to define the working-set window in the working-set model what is the effect of setting 6 to a small value on the page-fault frequency and the number of active  nonsuspended  processes currently executing in the system what is the effect when 6 is set to a very high value 9.32 is it possible for a process to have two working sets  one representing data and another representing code explain 9.33 suppose that your replacement policy  in a paged system  is to examine each page regularly and to discard that page if it has not been used since the last examination what would you gain and what would you lose by using this policy rather than lru or second-chance replacement 416 chapter 9 9.34 write a program that implements the fifo and lru page-replacement algorithms presented in this chapter first  generate a random pagereference string where page numbers range from 0 to 9 apply the random page-reference string to each algorithm  and record the number of page faults incurred by each algorithm implement the replacement algorithms so that the number of page frames can vary from 1 to 7 assume that demand paging is used 9.35 the catalan numbers are an integer sequence c11 that appear in treeenumeration problems the first catalan numbers for n = 1  2  3   are 1  2  5  14  42  132   a formula generating c11 is 1  2n   2n  ! ell =  n + 1   ; ; =  n + 1  ! n ! design two programs that communicate with shared memory using the win32 api as outlined in section 9.7.2 the producer process will generate the catalan sequence and write it to a shared memory object the consumer process will then read and output the sequence from shared memory in this instance  the producer process will be passed an integer parameter on the command line specifying how many catalan numbers to produce  for example  providing 5 on the command line means the producer process will generate the first five catalan numbers   demand paging was first used iil the atlas system  implemented on the manchester university muse computer around 1960  kilburn et al  1961    another early demand-paging system was multics  implemented on the ge 645 system  organick  1972    belady et al  1969  were the first researchers to observe that the fifo replacement strategy may produce the anomaly that bears belady 's name mattson et al  1970  demonstrated that stack algorithms are not subject to belady 's anomaly the optimal replacement algorithm was presented by belady  1966  and was proved to be optimal by mattson et al  1970   belady ' s optimal algorithm is for a fixed allocation ; prieve and fabry  1976  presented an optimal algorithm for situations in which the allocation can vary the enl lanced clock algorithm was discussed by carr and hennessy  1981   the working-set model was developed by denning  1968   discussions concerning the working-set model were presented by denning  1980   the scheme for monitoring the page-fault rate was developed by wulf  1969   who successfully applied this technique to the burroughs bssoo computer system wilson et al  1995  presented several algoritluns for dynamic memory allocation jolmstone and wilson  1998  described various memory-fragmentation 417 issues buddy system memory allocators were described in knowlton  1965l peterson and norman  1977   and purdom  jr and stigler  1970   bonwick  1994  discussed the slab allocator  and bonwick and adams  2001  extended the discussion to multiple processors other memory-fitting algorithms can be found in stephenson  1983   bays  1977   and brent  1989   a survey of memory-allocation strategies can be found in wilson et al  1995   solomon and russinovich  2000  and russinovich and solomon  2005  described how windows implements virtual memory mcdougall and mauro  2007  discussed virtual memory in solaris virtual memory techniques in linux and bsd were described by bovet and cesati  2002  and mckusick et al  1996   respectively ganapathy and schimmel  1998  and navarro et al  2002  discussed operating system support for multiple page sizes ortiz  2001  described virtual memory used in a real-time embedded operating system jacob and mudge  1998b  compared implementations of virtual memory in the mips  powerpc  and pentium architectures a companion article  jacob and mudge  1998a   described the hardware support necessary for implementation of virtual memory in six different architectures  including the ultrasparc part five since main memory is usually too small to accommodate all the data and programs permanently  the computer system must provide secondary storage to back up main memory modern computer systems use disks as the primary on-line storage medium for information  both programs and data   the file system provides the mechanism for on-line storage of and access to both data and programs residing on the disks a file is a collection of related information defined by its creator the files are mapped by the operating system onto physical devices files are normally organized into directories for ease of use the devices that attach to a computer vary in many aspects some devices transfer a character or a block of characters at a time some can be accessed only sequentially  others randomly some transfer data synchronously  others asynchronously some are dedicated  some shared they can be read-only or read-write they vary greatly in speed in many ways  they are also the slowest major component of the computer because of all this device variation  the operating system needs to provide a wide range of functionality to applications  to allow them to control all aspects of the devices one key goal of an operating system 's 1/0 subsystem is to provide the simplest interface possible to the rest of the system because devices are a performance bottleneck  another key is to optimize 1/0 for maximum concurrency 