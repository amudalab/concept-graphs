522 chapter 12 12.7 swap partition or swap file swap map 1---------swap area--------1 page i slot -1 l ~  ~ ---_l __ _l _ ~ figure 12.10 the data structures for swapping on linux systems  this scheme gives better performance on modern computers  which have more physical memory than older systems and tend to page less  linux is similar to solaris in that swap space is only used for anonymous memory or for regions of memory shared by several processes linux allows one or more swap areas to be established a swap area may be in either a swap file on a regular file system or a raw-swap-space partition each swap area consists of a series of 4-kb which are used to hold swapped pages  associated with each swap area is a .u1.2.p-an array of integer counters  each corresponding to a page slot in the swap area if the value of a counter is 0  the corresponding page slot is available values greater than 0 indicate that the page slot is occupied by a swapped page the value of the counter ij.l.dicates the number of mappings to the swapped page ; for example  a value of 3 indicates that the swapped page is mapped to three different processes  which can occur if the swapped page is storing a region of memory shared by three processes   the data structures for swapping on linux systems are shown in figure 12.10  disk drives have continued to get smaller and cheaper  so it is now economically feasible to attach many disks to a computer system having a large number of disks in a system presents opportunities for improving the rate at which data can be read or written  if the disks are operated in parallel furthermore  this setup offers the potential for improving the reliability of data storage  because redundant information can be stored on multiple disks thus  failure of one disk does not lead to loss of data a of disk-organization techniques  collectively called disks  raids   are commonly used to address the performance and reliability issues  in the past  raids composed of small  cheap disks were viewed as a cost-effective alternative to large  expensive disks ; today  raids are used for their higher reliability and higher data-transfer rate  rather than for economic reasons hence  the i in raid  which once stood for inexpensive/ ' now stands for ij.l.dependent  12.7.1 improvement of reliability via redundancy let us first consider the reliability of raids the chance that some disk out of a set of n disks will fail is much higher than the chance that a specific single 12.7 523 structuring raid raid storage can be structured in a variety of ways for example  a system can have disks directly attached to its buses in this case  the operating system or system software can implement raid flmctionality alternatively  an intelligent host controller can control multiple attached disks and can implement raid on those disks in hardware finally  a  or can be used a raid array is a standalone unit with its own controller  cache  usually   and disks it is attached to the host via one or more standard ata scsi or fc controllers this common setup allows any operating system and software without raid functionality to have raid-protected disks it is even used on systems that do have raid software layers because of its simplicity and flexibility  disk will fail suppose that the of a single disk is 100,000 hours then the mean time to failure of some disk in an array of 100 disks will be 100,000/100 = 1,000 hours  or 41.66 days  which is not long at all ! if we store only one copy of the data  then each disk failure will result in loss of a significant amount of data -and such a high rate of data loss is unacceptable  the solution to the problem of reliability is to introduce  we store extra information that is not normally needed but that can be used in the event of failure of a disk to rebuild the lost information thus  even if a disk fails  data are not lost  the simplest  but most expensive  approach to introducing redundancy is to duplicate every disk this technique is called with mirroring  a logical disk consists of two physical disks  and every write is carried out on both disks the result is called a mirrored volume if one of the disks in the volume fails  the data can be read from the other data will be lost only if the second disk fails before the first failed disk is replaced  the mean time to failure of a mirrored volume-where failure is the loss of data depends on two factors one is the mean time to failure of the individual disks the other is the which is the time it takes  on average  to replace a failed disk and to restore the data on it suppose that the failures of the two disks are that is  the failure of one disk is not connected to the failure of the other then  if the mean time to failure of a single disk is 100,000 hours and the mean time to repair is 10 hours  the of a mirrored disk system is 100  0002 /  2 10  = 500 106 hours  or 57,000 years ! you should be aware that the assumption of independence of disk failures is not valid power failures and natural disasters  such as earthquakes  fires  and floods  may result in damage to both disks at the same time also  manufacturing defects in a batch of disks can cause correlated failures as disks age  the probability of failure grows  increasing the chance that a second disk will fail while the first is being repaired in spite of all these considerations  however  n1.irrored-disk systems offer much higher reliability than do singledisk systems  power failures are a particular source of concern  since they occur far more frequently than do natural disasters even with mirroring of disks  if writes are 524 chapter 12 in progress to the same block in both disks  and power fails before both blocks are fully written  the two blocks can be in an inconsistent state one solution to this is to write one copy first then the next another is to add a cache to the raid array this write-back cache is protected from data loss during power failures  so the write can be considered complete at that point  assuming the nvram has some kind of error protection and correction  such as ecc or mirroring  12.7.2 improvement in performance via parallelism now let 's consider how parallel access to multiple disks improves performance  with disk mirroring  the rate at which read requests can be handled is doubled  since read requests can be sent to either disk  as long as both disks in a pair are functionat as is almost always the case   the transfer rate of each read is the same as in a single-disk system  but the number of reads per unit time has doubled  with multiple disks  we can improve the transfer rate as well  or instead  by striping data across the disks in its simplest form  consists of the bits of each byte across multiple disks ; such striping is called for example  if we have an array of eight disks  we write bit i of each byte to disk i the array of eight disks can be treated as a single disk with sectors that are eight times the normal size and  more important that have eight times the access rate in such an organization  every disk participates in every access  read or write  ; so the number of accesses that can be processed per second is about the same as on a single disk  but each access can read eight times as many data in the same time as on a single disk  bit-level striping can be generalized to include a number of disks that either is a multiple of 8 or divides 8 for example  if we use an array of four disks  bits i and 4 + i of each go to disk i further  striping need not occur at the bit level in for instance  blocks of a file are striped across multiple disks ; with n disks  block i of a file goes to disk  i mod n  + 1  other levels of striping  such as bytes of a sector or sectors of a block  also are possible block-level striping is the most common  parallelism in a disk system  as achieved through striping  has two main goals  increase the throughput of multiple small accesses  that is  page accesses  by load balancing  reduce the response time of large accesses  12.7.3 raid levels mirroring provides high reliability  but it is expensive striping provides high data-transfer rates  but it does not improve reliability numerous schemes to provide redundancy at lower cost by using disk striping combined with parity bits  which we describe next  l1.ave been proposed these schemes have different cost-performance trade-offs and are classified according to levels called we describe the various levels here ; figure 12.11 shows them pictorially  in the figure  p indicates error-correcting bits  and c 12.7 raid structure 525  a  raid 0  non-redundant striping   b  raid 1  mirrored disks   c  raid 2  memory-style error-correcting codes   d  raid 3  bit-interleaved parity   e  raid 4  block-interleaved parity   f  raid 5  block-interleaved distributed parity   g  raid 6  p + q redundancy  figure 12.11 raid levels  indicates a second copy of the data   in all cases depicted in the figure  four disks ' worth of data are stored  and the extra disks are used to store redundant information for failure recovery  raid level 0 raid level 0 refers to disk arrays with striping at the level of blocks but without any redundancy  such as mirroring or parity bits   as shown in figure 12.1l  a   raid ievell raid level1 refers to disk mirroring figure 12.1l  b  shows a mirrored organization  ' raid level2 raid level2 is also known as memory-style error-correctingcode  ecc  organization memory systems have long detected certain errors by using parity bits each byte in a memory system may have a parity bit associated with it that records whether the number of bits in the byte set to 1 is even  parity = 0  or odd  parity = 1   if one of the bits in the 526 chapter 12 byte is damaged  either a 1 becomes a 0  or a 0 becomes an the parity of the byte changes and thus does not match the stored parity similarly  if the stored parity bit is damaged  it does not match the computed parity thus  all single-bit errors are detected by the menwry system  error-correcting schemes store two or more extra bits and can reconstruct the data if a single bit is damaged the idea of ecc can be used directly in disk arrays via striping of bytes across disks for example  the first bit of each byte can be stored in disk 1  the second bit in disk 2  and so on until the eighth bit is stored in disk 8 ; the error-correction bits are stored in further disks this scheme is shown pictorially in figure 12.1l  c   where the disks labeled p store the error-correction bits if one of the disks fails  the remaining bits of the byte and the associated error-correction bits can be read from other disks and used to reconstruct the damaged data note that raid level 2 requires only three disks ' overhead for four disks of data  unlike raid level 1  which requires four disks ' overhead  raid level 3 raid level 3  or improves on level 2 by taking into account the fact that  unlike memory systems  disk controllers can detect whether a sector has been read correctly  so a single parity bit can be used for error correction as well as for detection the idea is as follows  if one of the sectors is damaged  we know exactly which sector it is  and we can figure out whether any bit in the sector is a 1 or a 0 by computing the parity of the corresponding bits from sectors in the other disks if the parity of the remaining bits is equal to the stored parity  the missing bit is 0 ; otherwise  it is 1 raid level3 is as good as level 2 but is less expensive in the number of extra disks required  it has only a one-disk overhead   so level 2 is not used in practice this scheme is shown pictorially in figure 12.1l  d   raid level 3 has two advantages over level 1 first  the storage overhead is reduced because only one parity disk is needed for several regular disks  whereas one mirror disk is needed for every disk in level1 second  since reads and writes of a byte are spread out over multiple disks with n-way striping of data  the transfer rate for reading or writing a single block is n times as fast as with raid level 1 on the negative side  raid level3 supports fewer l/os per second  since every disk has to participate in every i/0 request  a further performance problem with raid 3-and with all paritybased raid levels-is the expense of computing and writing the parity  this overhead results in significantly slower writes than with non-parity raid arrays to moderate this performance penalty  many raid storage arrays include a hardware controller with dedicated parity hardware this controller offloads the parity computation from the cpu to the array the array has an nvram cache as well  to store the blocks while the parity is computed and to buffer the writes from the controller to the spindles this combination can make parity raid almost as fast as non-parity in fact  a caching array doing parity raid can outperform a non-caching non-parity raid  raid level 4 raid level4  or uses block-level striping  as in raid 0  and in addition keeps a parity block on a separate disk for corresponding blocks from n other disks this scheme is 12.7 527 diagramed in figure 12.1l  e   if one of the disks fails  the parity block can be used with the corresponding blocks from the other disks to restore the blocks of the failed disk  a block read accesses only one disk  allowing other requests to be processed by the other disks thus  the data-transfer rate for each access is slowe1 ~ but multiple read accesses can proceed in parallel  leading to a higher overall i/0 rate the transfer rates for large reads are high  since all the disks can be read in parallel ; large writes also have high transfer rates  since the data and parity can be written in parallel  small independent writes can not be performed in parallel an operatingsystem write of data smaller than a block requires that the block be read  modified with the new data  and written back the parity block has to be updated as well this is known as the syti  .e  thus  a single write requires four disk accesses  two to read the two old blocks and two to write the two new blocks  wafl  chapter 11  uses raid level4 because this raid level allows disks to be added to a raid set seamlessly if the added disks are initialized with blocks containing all zeros  then the parity value does not change  and the raid set is still correct  raid levels raid levels  or  differs from level 4 by spreading data and parity among all n + 1 disks  rather than storing data in n disks and parity in one disk for each block  one of the disks stores the parity  and the others store data for example  with an array of five disks  the parity for the nth block is stored in disk  n mod 5  + 1 ; the nth blocks of the other four disks store actual data for that block this setup is shown in figure 12.11  f   where the ps are distributed across all the disks a parity block can not store parity for blocks in the same disk  because a disk failure would result in loss of data as well as of parity  and hence the loss would not be recoverable by spreading the parity across all the disks in the set  raid 5 avoids potential overuse of a single parity disk  which can occur with raid 4 raid 5 is the most common parity raid system  raid level 6 raid level 6  also called the is much like raid level 5 but stores extra redundant information to guard against disk failures instead of parity  error-correcting codes such as the are used in the scheme shown in figure 12.11  g   2 bits of redundant data are stored for every 4 bits of datacompared with 1 parity bit in level 5-and the system can tolerate two disk failures  raid levels 0 + 1 and 1 + 0 raid level 0 + 1 refers to a combination of raid levels 0 and 1 raid 0 provides the performance  while raid 1 provides the reliability generally  this level provides better performance than raid 5 it is common in enviromnents where both performance and reliability are important unfortunately  like raid 1  it doubles the number of disks needed for storage  so it is also relatively expensive in raid 0 + 1  a set of disks are striped  and then the stripe is mirrored to another  equivalent stripe  528 chapter 12 stripe a  raid 0 + 1 with a single disk failure  ua mirror b  raid 1 + 0 with a single disk failure  figure 12.12 raid 0 + 1 and 1 + 0  another raid option that is becoming available commercially is raid level 1 + 0  in which disks are mirrored in pairs and then the resulti.j.l.g mirrored pairs are striped this scheme has some theoretical advantages over raid 0 + 1 for example  if a single disk fails in raid 0 + 1  an entire stripe is inaccessible  leaving only the other stripe available with a failure in raid 1 + 0  a single disk is unavailable  but the disk that mirrors it is still available  as are all the rest of the disks  figure 12.12   numerous variations have been proposed to the basic raid schemes described here as a result  some confusion may exist about the exact definitions of the different raid levels  the implementation of raid is another area of variation consider the following layers at which raid can be implemented  volume-management software can implement raid within the kernel or at the system software layer in this case  the storage hardware can provide a minimum of features and still be part of a full raid solution parity raid is fairly slow when implemented in software  so typically raid 0  1  or 0 + 1 is used  raid can be implemented in the host bus-adapter  hba  hardware only the disks directly connected to the hba can be part of a given raid set  this solution is low in cost but not very flexible  12.7 529 raid can be implemented in the hardware of the storage array the storage array can create raid sets of various levels and can even slice these sets into smaller volumes  which are then presented to the operating system  the operating system need only implement the file system on each of the volumes arrays can have multiple connections available or can be part of a san  allowing multiple hosts to take advantage of the array 's features  raid can be implemented in the san interconnect layer by disk virtualization devices in this case  a device sits between the hosts and the storage  it accepts commands from the servers and manages access to the storage  it could provide mirroring  for example  by writing each block to two separate storage devices  other features  such as and replication  can be implemented at each of these levels as well involves the automatic duplication of writes between separate sites for redundancy and disaster recovery replication can be synchronous or asynchronous in synchronous replication  each block must be written locally and remotely before the write is considered complete  whereas in asynchronous replication  the writes are grouped together and written periodically asynchronous replication can result in data loss if the primary site fails  but it is faster and has no distance limitations  the implementation of these features differs depending on the layer at which raid is implemented for example  if raid is implemented in software  then each host may need to carry out and manage its own replication if replication is implemented in the storage array or in the san intercom1ect  however  then whatever the host operating system or its features  the host 's data can be replicated  one other aspect of most raid implementations is a hot spare disk or disks  a is not used for data but is configured to be used as a replacement in case disk failure for instance  a hot spare can be used to rebuild a mirrored pair should one of the disks in the pair fail in this way  the raid level can be reestablished automatically  without waiting for the failed disk to be replaced  allocating more than one hot spare allows more than one failure to be repaired without human intervention  12.7.4 selecting a raid level given the many choices they have  how do system designers choose a raid level one consideration is rebuild performance if a disk fails  the time needed to rebuild its data can be significant this may be an important factor if a continuous supply of data is required  as it is in high-performance or interactive database systems furthermore  rebuild performance influences the mean time to failure  rebuild performance varies with the raid level used rebuilding is easiest or raid level1  since data can be copied from another disk ; for the other levels  we need to access all the other disks in the array to rebuild data in a failed disk  rebuild times can be hours for raid 5 rebuilds of large disk sets  raid level 0 is used in high-performance applications where data loss is not critical raid level1 is popular for applications that require high reliability with fast recovery raid 0 + 1 and 1 + 0 are used where both performance and reliability are important-for example  for small databases due to raid 1 's 530 chapter 12 the inserv storage array im1ovation  in an effort to provide better  faster  and less expensive solutions  frequently blurs the lines that separated previous technologies consider the inserv storage array from 3par unlike most other storage arrays  inserv does not require that a set of disks be configured at a specific raid level  rather  each disk is broken into 256-mb chunklets ram is then applied at the chunklet level a disk can thus participate in multiple and various raid levels as its chunklets are used for multiple volumes  inserv also provides snapshots similar to those created by the wafl file system the format of inserv snapshots can be read-write as well as readonly  allowing multiple hosts to mount copies of a given file system without needing their own copies of the entire file system any changes a host makes in its own copy are copy-on-write and so are not reflected in the other copies  a further innovation is  some file systems do not expand or shrink on these systems  the original size is the only size  and any change requires copying data an administrator can configure inserv to provide a host with a large amount of logical storage that initially occupies only a small amount of physical storage as the host starts using the storage  unused disks are allocated to the host  up to the original logical level the host thus can believe that it has a large fixed storage space  create its file systems there  and so on disks can be added or removed from the file system by inserv without the file systems noticing the change this feature can reduce the number of drives needed by hosts  or at least delay the purchase of disks until they are really needed  high space overhead  raid levels is often preferred for storing large volumes of data level6 is not supported currently by many raid implementations  but it should offer better reliability than levels  raid system designers and administrators of storage have to make several other decisions as well for example  how many disks should be in a given raid set how many bits should be protected by each parity bit if more disks are in an array  data-transfer rates are higher  but the system is more expensive  if more bits are protected by a parity bit  the space overhead due to parity bits is lower  but the chance that a second disk will fail before the first failed disk is repaired is greater  and that will result in data loss  12.7.5 extensions the concepts of raid have been generalized to other storage devices  including arrays of tapes  and even to the broadcast of data over wireless systems when applied to arrays of tapes  raid structures are able to recover data even if one of the tapes in an array is damaged when applied to broadcast of data  a block of data is split into short units and is broadcast along with a parity unit ; if one of the units is not received for any reason  it can be reconstructed from the other units comrnonly  tape-drive robots containing multiple tape drives will stripe data across all the drives to increase throughput and decrease backup time  12.7 531 12.7.6 problems with raid unfortunately  raid does not always assure that data are available for the operating system and its users a pointer to a file could be wrong  for example  or pointers within the file structure could be wrong incomplete writes  if not properly recovered  could result in corrupt data some other process could accidentally write over a file system 's structures  too raid protects against physical media errors  but not other hardware and software errors as large as is the landscape of software and hardware bugs  that is how numerous are the potential perils for data on a system  the solaris zfs file system takes an innovative approach to solving these problems through the use of  a technique which is used to verify the integrity of data zfs maintains internal checksums of all blocks  including data and metadata these checksums are not kept with the block that is being checksummed rathel ~ they are stored with the pointer to that block  see figure 12.13  consider an inode with pointers to its data within the inode is the checksum of each block of data if there is a problem with the data  the checksum will be incorrect and the file system will know about it if the data are mirrored  and there is a block with a correct checksum and one with an incorrect checksum  zfs will automatically update the bad block with the good one similarly  the directory entry that points to the inode has a checksum for the inode any problem in the inode is detected when the directory is accessed  this checksumming takes places throughout all zfs structures  providing a much higher level of consistency  error detection  and error correction than is found in raid disk sets or standard file systems the extra overhead that is created by the checksum calculation and extra block read-modify-write cycles is not noticeable because the overall performance of zfs is very fast  another issue with most raid implementations is lack of flexibility  consider a storage array with twenty disks divided into four sets of five disks  each set of five disks is a raid level 5 set as a result  there are four separate data 1 figure 12.13 zfs checksums all metadata and data  532 chapter 12 volumes  each holding a file system but what if one file system is too large to fit on a five-disk raid level 5 set and what if another file system needs very little space if such factors are known ahead of time  then the disks and volumes can be properly allocated very frequently  however  disk use and requirements change over time  even if the storage array allowed the entire set of twenty disks to be created as one large raid set other issues could arise several volumes of various sizes could be built on the set but some volume managers do not allow us to change a volume 's size in that case  we would be left with the same issue described above-mismatched file-system sizes some volume n lanagers allow size changes  but some file systems do not allow for file-system growth or shrinkage the volumes could change sizes  but the file systems would need to be recreated to take advantage of those changes  zfs combines file-system management and volume management into a unit providing greater functionality than the traditional separation of those functions allows disks  or partitions of disks  are gathered together via raid sets into of storage a pool can hold one or more zfs file systems the entire pool 's free space is available to all file systems within that pool zfs uses the memory model of malloc and free to allocate and release storage for each file system as blocks are used and freed within the file system as a result there are no artificial limits on storage use and no need to relocate file systems between volumes or resize volumes zfs provides quotas to limit the size of a file system and reservations to assure that a file system can grow by a specified amount  but those variables may be changed by the file system owner at any time figure 12.14  a  depicts traditional volumes and file systems  and figure 12.14  b  shows the zfs model  i fs i ~  a  traditional volumes and file systems   b  zfs and pooled storage  figure 12.14  a  traditional volumes and file systems  b  a zfs pool and file systems  12.8 12.8 533 in chapter 6  we introduced the write-ahead log  which requires the availability of stable storage by definition  information residing in stable storage is never lost to implement such storage  we need to replicate the required information on multiple storage devices  usually disks  with independent failure modes  we also need to coordinate the writing of updates in a way that guarantees that a failure during an update will not leave all the copies in a damaged state and that  when we are recovering from a failure  we can force all copies to a consistent and correct value  even if another failure occurs during the recovery  in this section  we discuss how to meet these needs  a disk write results in one of three outcomes  successful completion the data were written correctly on disk  partial failure a failure occurred in the midst of transfer  so only some of the sectors were written with the new data  and the sector being written during the failure may have been corrupted  total failure the failure occurred before the disk write started  so the previous data values on the disk remain intact  whenever a failure occurs during writing of a block  the system needs to detect it and invoke a recovery procedure to restore the block to a consistent state to do that  the system must maintain two physical blocks for each logical block an output operation is executed as follows  write the information onto the first physical block  when the first write completes successfully  write the same injormation onto the second physical block  declare the operation complete only after the second write completes successfully  during recovery from a failure  each pair of physical blocks is examined  if both are the same and no detectable error exists  then no further action is necessary if one block contains a detectable error  then we replace its contents with the value of the other block if neither block contains a detectable error  but the blocks differ in content  then we replace the content of the first block with that of the second this recovery procedure ensures that a write to stable storage either succeeds completely or results in no change  we can extend this procedure easily to allow the use of an arbitrarily large number of copies of each block of stable storage although having a large number of copies further reduces the probability of a failure  it is usually reasonable to simulate stable storage with only two copies the data in stable storage are guaranteed to be safe unless a failure destroys all the copies  because waiting for disk writes to complete  synchronous i/o  is time consuming  many storage arrays add nvram as a cache since the memory is nonvolatile  it usually has battery power to back up the unit 's power   it can be trusted to store the data en route to the disks it is thus considered part of 534 chapter 12 12.9 the stable storage writes to it are much faster than to disk  so performance is greatly improved  would you buy a dvd or cd player that had one disk sealed inside of course not you expect to use a dvd or cd player with many relatively inexpensive disks on a computer as well  using many inexpensive cartridges with one drive lowers the overall cost low cost is the defining characteristic of tertiary storage  which we discuss in this section  12.9.1 tertiary-storage devices because cost is so important  in practice  tertiary storage is built with the most common examples are floppy disks  tapes  and read-only  write-once  and rewritable cds and dvds many any other kinds of tertiarystorage devices are available as well  including removable devices that store data in flash memory and interact with the computer system via a usb interface  12.9.1.1 removable disks removable disks are one kind of tertiary storage floppy disks are an example of removable magnetic disks they are made from a thin  flexible disk coated with magnetic material and enclosed in a protective plastic case although common floppy disks can hold only about 1 mb  similar technology is used for removable magnetic disks that hold more than 1 gb removable magnetic disks can be nearly as fast as hard disks  although the recording stuface is at greater risk of from scratches  a is another kind of removable disk it records data on a rigid platter coated with magnetic material  but the recording technology is quite different from that for a magnetic disk the magneto-optic head flies much farther from the disk surface than a magnetic disk head does  and the magnetic material is covered with a thick protective layer of plastic or glass  this arrangement makes the disk much more resistant to head crashes  the magneto-optic disk drive has a coil that produces a magnetic field ; at room temperature  the field is too large and too weak to magnetize a bit on the disk to write a bit  the disk head flashes a laser beam at the disk surface the laser is aimed at a tiny spot where a bit is to be written the laser heats this spot  which makes the spot susceptible to the magnetic field now the large  weak magnetic field can record a tiny bit  the magneto-optic head is too far from the disk surface to read the data by detecting the tiny magnetic fields in the way that the head of a hard disk does  instead  the drive reads a bit using a property of laser light called the when a laser beam is bounced off of a magnetic spot  the polarization of the laser beam is rotated clockwise or counterclockwise  dependin ~ g on the orientation of the magnetic field this rotation is what the head detects to read a bit  another category of removable disk is the optical disks do not use magnetism at all instead  they use special materials that can be altered by laser light to have relatively dark or bright spots one exarnple of optical-disk 12.9 535 technology is the which is coated with a material that can freeze into either a crystalline or an amorphous state the crystalline state is more transparent  and hence a laser beam is brighter when it passes through the ltlaterial and bounces off the reflective layer the phase-change drive uses laser light at three different powers  low power to read data  medium power to erase the disk by melting and refreezing the recording medium into the crystalline state  and high power to melt the medium into the amorphous state to write to the disk the most common examples of this technology are the re-recordable cd-rw and dvd-rw  the kinds of disks just described can be used over and over they are called in contrast  can be written only once an old way to make a worm disk is to manufacture a thin aluminum film sandwiched between two glass or plastic platters to write a bit  the drive uses a laser light to burn a small hole through the aluminum this burning can not be reversed although it is possible to destroy the information on a worm disk by burning holes everywhere  it is virtually impossible to alter data on the disk  because holes can only be added  and the ecc code associated with each sector is likely to detect such additions worm disks are considered durable and reliable because the metal layer is safely encapsulated between the protective glass or plastic platters and magnetic fields can not damage the recording a newer write-once technology records on an organic polymer dye instead of an aluminum layer ; the dye absorbs laser light to form marks this technology is used in the recordable cd-r and dvd-r  read-oniv such as cd-rom and dvd-rom  come from the factory with the data prerecorded they use technology similar to that of worm disks  although the bits are pressed  not burned   and they are very durable  most removable disks are slower than their nonremovable counterparts  the writing process is slower  as are rotation and sometimes seek time  12.9.1.2 tapes magnetic tape is another type of removable medium as a general rule  a tape holds more data than an optical or magnetic disk cartridge tape drives and disk drives have similar transfer rates but random access to tape is much slower than a disk seek  because it requires a fast-forward or rewind operation that takes tens of seconds or even minutes  although a typical tape drive is more expensive than a typical disk drive  the price of a tape cartridge is lower than the price of the equivalent capacity of magnetic disks so tape is an economical medium for purposes that do not require fast random access tapes are commonly used to hold backup copies of disk data they are also used in large supercomputer centers to hold the enornwus volumes of data used in scientific research and by large commercial enterprises  large tape installations typically use robotic tape changers that move tapes between tape drives and storage slots in a tape library these mechanisms give the computer automated access to many tape cartridges  a robotic tape library can lower the overall cost of data storage a diskresident file that will not be needed for a while can be to tape  where the cost per gigabyte is lower ; if the file is needed in the future  the computer can it back into disk storage for active use a robotic tape library is 536 chapter 12 sometimes called storage  since it is between the high performance of on-line magnetic disks and the low cost of off-line tapes sitting on shelves in a storage room  12.9.1.3 future technology in the future  other storage technologies may become important sometimes old technologies are used in new ways  as economics change or the technologies evolve for example  solid-state disks  or are growing in importance and becoming more common simply described  an ssd is a disk that is used like a hard drive depending on the memory technology used  it can be volatile or nonvolatile the memory technology also affects performance nonvolatile ssds have the same characteristics as traditional hard disks but can be more reliable because they have no moving parts and faster because they have no seek time or latency in addition  they use less energy however  they are more expensive per megabyte than traditional hard disks  have lower capacity than the larger hard disks  and may have shorter life-spans than hard disks ; so their uses are limited in one example  ssds are being used in storage arrays to hold metadata which requires high-performance such as the journal of a journaling file system ssds are also being added to notebook computers to make them smaller  faster  and more energy efficient  another promising storage technology  bologt ; ;  phk uses laser light to record holographic photographs on special media we can think of a hologram as a three-dimensional array of pixels each pixel represents one bit  0 for black or 1 for white and all the pixels in a hologram are transferred in one flash of laser light  so the data transfer rate is extremely high with continued development  holographic storage may become commercially viable  another technology under active research is based on  iv ! e \ 1s   the idea is to apply the fabrication technologies that produce electronic chips to the manufacture of small datastorage machines one proposal calls for the fabrication of an array of 10,000 tiny disk heads  with a square centimeter of magnetic storage material suspended above the array when the storage material is moved lengthwise over the heads  each head accesses its own linear track of data on the material the storage material can be shifted sideways slightly to enable all the heads to access their next track although it remains to be seen whether this technology can be successful  it may provide a nonvolatile data-storage technology that is faster than magnetic disk and cheaper than semiconductor dram  whether the storage medium is a removable magnetic disk  a dvd  or a magnetic tape  the operating system needs to provide several capabilities to use removable media for data storage these capabilities are discussed in section 12.9.2  12.9.2 operating-system support two major jobs of an operating system are to manage physical devices and to present a virtual machine abstraction to applications in this chapter  we have seen that  for hard disks  the operating system provides two abstractions  one is the raw device  which is just an array of data blocks the other is a file system for a file system on a magnetic disk  the operating system queues and 12.9 537 schedules the interleaved requests from several applications now  we shall see how the operating system does its job when the storage media are removable  12.9.2.1 application interface most operating systems can handle removable disks almost exactly as they do fixed disks when a blank cartridge is inserted into the drive  or mounted   the cartridge must be formatted  and then an empty file system is generated on the disk this file system is used just like a file system on a hard disk tapes are often handled differently the operating system usually presents a tape as a raw storage medium an application does not open a file on the tape ; it opens the whole tape drive as a raw device usually  the tape drive is then reserved for the exclusive use of that application until the application exits or closes the tape device this exclusivity makes sense  because random access on a tape can take tens of seconds  or even a few minutes  so interleaving random accesses to tapes from more than one application would be likely to cause thrashing  when the tape drive is presented as a raw device  the operating system does not provide file-system services the application must decide how to use the array of blocks for instance  a program that backs up a hard disk to tape might store a list of file names and sizes at the beginning of the tape and then copy the data of the files to the tape in that order  it is easy to see the problems that can arise from this way of using tape  since every application makes up its own rules for how to organize a tape  a tape full of data can generally be used only by the program that created it  for instance  even if we know that a backup tape contains a list of file names and file sizes followed by the file data  we will still find it difficult to use the tape how exactly are the file names stored are the file sizes in binary or ascii form are the files written one per block  or are they all concatenated in one tremendously long string of bytes we do not even know the block size on the tape  because this variable is generally one that can be chosen separately for each block written  for a disk drive  the basic operations are read    write    and seek    tape drives have a different set of basic operations instead of seek    a tape drive uses the locate   operation the tape locate   operation is more precise than the disk seek   operation  because it positions the tape to a specific logical block  rather than an entire track locating to block 0 is the same as rewinding the tape  for most kinds of tape drives  it is possible to locate to any block that has been written on a tape in a partly filled tape  however  it is not possible to locate into the empty space beyond the written area  because most tape drives do not manage their physical space in the same way disk drives do for a disk drive  the sectors have a fixed size  and the formatting process must be used to place empty sectors in their final positions before any data can be written most tape drives have a variable block size  and the size of each block is detern ined on the fly  when that block is written if an area of defective tape is encountered during writing  the bad area is skipped and the block is written again this operation explains why it is not possible to locate into the empty space beyond the written area -the positions and numbers of the logical blocks have not yet been detennined  538 chapter 12 most tape drives have a read_position   operation that returns the logical block number where the tape head is currently located many tape drives also support a space   operation for relative motion so  for example  the operation space  -2  would locate backward over two logical blocks  for most kinds of tape drives  writing a block has the side effect of logically erasing everything beyond the position of the write in practice  this side effect means that most tape drives are append-only devices  because updating a block in the middle of the tape also effectively erases everything beyond that block the tape drive implements this appending by placing an end-of-tape  eot  mark after a block that is written the drive refuses to locate past the eot mark  but it is possible to locate to the eot and then start writing doing so overwrites the old eot mark and places a new one at the end of the new blocks just written  in principle  a file system can be implemented on a tape but many of the file-system data structures and algorithms would be different from those used for disks  because of the append-only property of tape  12.9.2.2 file naming another question that the operating system needs to handle is how to name files on removable media for a fixed disk  naming is not difficult on a pc  the file name consists of a drive letter followed by a path name in unix  the file name does not contain a drive letter  but the molmt table enables the operating system to discover on what drive the file is located if the disk is removable  however  knowing what drive contained the cartridge at some time in the past does not mean knowing how to find the file if every removable cartridge in the world had a different serial number  the name of a file on a removable device could be prefixed with the serial number  but to ensure that no two serial numbers are the same would require each one to be about 12 digits in length who could remember the names of her files if she had to memorize a 12-digit serial number for each one the problem becomes even more difficult when we want to write data on a removable cartridge on one computer and then use the cartridge in another computer if both machines are of the same type and have the same kind of removable drive  the only difficulty is knowing the contents and data layout on the cartridge but if the machines or drives are different  many additional problems can arise even if the drives are compatible  different computers may store bytes in different orders and may use different encodings for binary numbers and even for letters  such as ascii on pcs versus ebcdic on mainframes   today 's operating systems generally leave the name-space problem unsolved for removable media and depend on applications and users to figure out how to access and interpret the data fortunately  a few kinds of removable media are so well standardized that all computers use them the same way one example is the cd music cds use a universal format that is understood by any cd drive data cds are available in only a few different formats  so it is usual for a cd drive and the operating-system device driver to be programmed to handle all the comn1on formats dvd fonnats are also well standardized  12.9 539 12.9.2.3 hierarchical storage management a ju enables the computer to change the removable cartridge in a tape or disk drive without human assistance two major uses of this technology are for backups and hierarchical storage systems the use of a jukebox for backups is simple  when one cartridge becomes full  the computer instructs the jukebox to switch to the next cartridge some jukeboxes hold tens of drives and thousands of cartridges  with robotic arms managing the movement of tapes to the drives  a hierarchical storage system extends the storage hierarchy beyond primary memory and secondary storage  that is  magnetic disk  to incorporate tertiary storage tertiary storage is usually implemented as a jukebox of tapes or removable disks this level of the storage hierarchy is larger  cheaper  and slower  although the virtual memory system can be extended in a straightforward manner to tertiary storage  this extension is rarely carried out in practice the reason is that a retrieval from a jukebox can take tens of seconds or even minutes  and such a long delay is intolerable for demand paging and for other forms of virtual memory use  the usual way to incorporate tertiary storage is to extend the file system  small and frequently used files remain on magnetic disk  while large and old files that are not actively used are archived to the jukebox in some file-archiving systems  the directory entry for the file continues to exist  but the contents of the file no longer occupy space in secondary storage if an application tries to open the file  the open   system call is suspended until the file contents can be staged in from tertiary storage when the contents are again available from magnetic disk  the open   operation returns control to the application  which proceeds to use the disk-resident copy of the data  today  is usually found in installations that have large volumes of data that are used seldom  sporadically  current work in hsm includes extending it to provide full here  data move from disk to tape and back to disk  as needed  but are deleted on a schedule or according to policy for example  some sites save e-mail for seven years but want to be sure that at the end of seven years it is destroyed at that point  the data might be on disk  hsm tape  and backup tape ilm centralizes knowledge of where the data are so that policies can be applied across all these locations  12.9.3 performance issues as with any component of the operating system  the three most important aspects of tertiary-storage performance are speed  reliability  and cost  12.9.3.1 speed the speed of tertiary storage has two aspects  bandwidth and latency we measure the bandwidth in bytes per second the ~ 'l ' is the average data rate during a transfer-that is  the number of bytes divided by the transfer time the calculates the average over the entire l/0 time  including the time for seek   or locate   and any 540 chapter 12 cartridge-switching time in a jukebox in essence  the sustained bandwidth is the rate at which the data stream actually flows  and the effective bandwidth is the overall data rate provided by the drive the bandwidth of a drive is generally understood to mean the sustained bandwidth  for removable disks  tlce bandwidth ranges from a few megabytes per second for the slowest to over 40 mb per second for the fastest tapes have a similar range of bandwidths  from a few megabytes per second to over 30mb per second  the second aspect of speed is the  by this performance measure  disks are much faster than tapes disk storage is essentially twodimensional all the bits are out in the open a disk access simply moves the ann to the selected cylinder and waits for the rotational latency  which may take less than 5 milliseconds by contrast  tape storage is three-dimensional  at any time  a small portion of the tape is accessible to the head  whereas most of the bits are buried below hundreds or thousands of layers of tape wound on the reel a random access on tape requires winding the tape reels until the selected block reaches the tape head  which can take tens or hundreds of seconds so we can generally say that random access within a tape cartridge is more than a thousand times slower than random access on disk  if a jukebox is involved  the access latency can be significantly higher for a removable disk to be changed  the drive must stop spinning  then the robotic arm must switch the disk cartridges  and then the drive must spin up the new cartridge this operation takes several seconds-about a hundred times longer than the random-access time within one disk so switching disks in a jukebox incurs a relatively high performance penalty  for tapes  the robotic-ann time is about the same as for disks but for tapes to be switched  the old tape generally must rewind before it can be ejected  and that operation can take as long as 4 minutes and  after a new tape is loaded into the drive  many seconds can be required for the drive to calibrate itself to the tape and to prepare for i/0 although a slow tape jukebox can have a tape-switch time of 1 or 2 minutes  this time is not enormously greater than the random-access time within one tape  to generalize  we can say that random access in a disk jukebox has a latency of tens of seconds  whereas random access in a tape jukebox has a latency of hundreds of seconds ; switching tapes is expensive  but switching disks is not we must be careful not to overgeneralize  though some expensive tape jukeboxes can rewind  eject  load a new tape  and fast-forward to a random item of data all in less than 30 seconds  if we pay attention to only the performance of the drives in a jukebox  the bandwidth and latency seem reasonable but if we focus our attention on the cartridges instead  we find a terrible bottleneck consider first the bandwidth the bandwidth-to-storage-capacity ratio of a robotic library is much less favorable than that of a fixed disk to read all the data stored on a large hard disk could take about an hour to read all the data stored in a large tape library could take years the situation with respect to access latency is nearly as bad to illustrate  if 100 requests are queued for a disk drive  the average waiting time will be about a second if 100 requests are queued for a tape library  the average waiting time could be over an hour the low cost of tertiary storage results from having many cheap cartridges share a few expensive drives but a removable library is best devoted to the storage of 12.9 541 infrequently used data  because the library can satisfy only a relatively small number of i/0 requests per hour  12.9.3.2 reliability although we often think good pe1jormance means high speed  another important aspect of performance is reliability if we try to read some data and are unable to do so because of a drive or media failure  for all practical purposes the access time is infinitely long and the bandwidth is infinitely small so it is important to understand the reliability of removable media  removable n1.ag  netic disks are somewhat less reliable than are fixed hard disks  because they are more likely to be exposed to harmful environmental conditions such as dust  large changes in temperature and humidity  and mechanical forces such as shock and bending optical disks are considered very reliable  because the layer that stores the bits is protected by a transparent plastic or glass layer the reliability of magnetic tape varies widely  depending on the kind of drive some inexpensive drives wear out tapes after a few dozen uses ; other drives are gentle enough to allow millions of reuses by comparison with a magnetic-disk head  the head in a magnetic-tape drive is a weak spot  a disk head flies above the media  but a tape head is in close contact with the tape the scrubbing action of the tape can wear out the head after a few thousands or tens of thousands of hours  in summary  we can say that a fixed-disk drive is likely to be more reliable than a removable-disk or tape drive  and an optical disk is likely to be more reliable than a magnetic disk or tape but a fixed magnetic disk has one weakness a head crash in a hard disk generally destroys the data  whereas the failure of a tape drive or optical-disk drive often leaves the data cartridge unharmed  12.9.3.3 cost storage cost is another important factor here is a concrete example of how removable media may lower the overall storage cost suppose that a hard disk that holds x gb has a price of $ 200 ; of this amom1.t  $ 190 is for the housing  motor  and controller  and $ 10 is for the magnetic platters the storage cost for this disk is $ 200/ x per gigabyte now  suppose that we can manufacture the platters in a removable cartridge for one drive and 10 cartridges  the total price is $ 190 + $ 100  and the capacity is lox gb  so the storage cost is $ 291 x per gigabyte even if it is a little more expensive to make a removable cartridge  the cost per gigabyte of removable storage may well be lower than the cost per gigabyte of a hard disk  because the expense of one drive is averaged with the low price of many removable cartridges  figures 12.15  12.16  and 12.17 show cost trends per megabyte for dram memory  magnetic hard disks  and tape drives the prices in the graphs are the lowest prices found in advertisements in various computer magazines and on the world wide web at the end of each year these prices reflect the smallcomputer marketplace of the readership of these magazines  where prices are low by comparison with the mainframe and minicomputer markets in the case of tape  the price is for a drive with one tape the overall cost of tape storage becomes much lower as more tapes are purchased for use with the drive  542 chapter 12 co   ; ; ; &  5 160 80 40 20 10 1.2 0.8 0.4 64 kb 32 128mb 512mb 2gb 0 02 --'-c-19 = '  ,8.,.-2 -1 98-cc4  -19 = '  c8 = -6 -1 ~ 9 '     88---,19 = '  c9 = -o -1c  '91 '     92  1 c  '  99-4 -c  c19l96---,19c  '  9 = -8 -2.,-,o '     oo  2c  '  oo = -2 -2      0l.04     2c  '  oo = -6   '2oos year figure 12 15 price per megabyte of dram  from 1981 to 2008  because the price of a tape is a small fraction of the price of the drive however  in a huge tape library containing thousands of cartridges  the storage cost is dominated by the cost of the tape cartridges as of 2004  the cost per gb of tape cartridges was around $ .40  as figure 12.15 shows  the cost of dram fluctuates widely in the period from 1981 to 2004  we can see three price crashes  around 1981  1989  and 1996  as excess production caused a glut in the marketplace we can also see two periods  around 1987 and 1993  where shortages in the marketplace caused sigrtificant price increases in the case of hard disks  figure 12.16   the price decline has been steadier tape-drive prices also fell steadily up to 1997  figure 12.17   since 1997  the price per gigabyte of inexpensive tape drives has ceased its dramatic fall  although the price of mid-range tape technology  such as dat /dds  has continued to fall and is now approaching that of the co   ; ; ; ' 100 50 20 5 2 0.5 0.2 0.05 0.02 0.004 0.001 0.0005 0.0002 10 20 1982 1984 1986 120 1.2 2 1988 1990 1992 19 gb gb gb 1994 1996 1998 2000 2002 2004 2006 2008 year figure 12.16 price per megabyte of magnetic hard disk  from 1981 to 2008  12.10 oj ~ 12.10 40 20 8 60 120 1.2 0.5 0.1 72gb 0.025 320gb 0.01 320gb 0.0051 ~ 9c-c84-1  l98  -6 -19  '  -88c---cc19 ~ 90c  c19 ~ 92  --c-c19l94-c-c19l96,---.,-c19'cc98-2,-jooc-c0--c2,-j.00,-,-2  ~   .,j = ~ 2008 year figure 12.17 price per megabyte of a tape drive  from 1984 to 2008  543 in.expensive drives tape-drive prices are not shown for years prior to 1984  because  as mentioned  the magazines used in tracking prices are targeted to the small-computer marketplace  and tape drives were not widely used with small computers prior to 1984  we can see from these graphs that the cost of storage has fallen dramatically  by comparing the graphs  we can also see that the price of disk storage has plummeted relative to the price of dram and tape  the price per megabyte of magnetic disk storage improved by more than four orders of magnitude from 1981 to 2004  whereas the corresponding improvement for main memory was only three orders of magnitude main memory today is more expensive than disk storage by a factor of 100  the price per megabyte dropped much more rapidly for disk drives than for tape drives as well in fact  the price per megabyte of a magnetic disk drive is approaching that of a tape cartridge without the tape drive consequently  small and medium-sized tape libraries have a higher storage cost than disk systems with equivalent capacity  the dramatic fall in disk prices has largely rendered tertiary storage obsolete we no longer have any tertiary storage technology that is orders of magnitude less expensive than magnetic disk it appears that the revival of tertiary storage must await a revolutionary technology breakthrough  meanwhile  tape storage will find its use mostly limited to purposes such as backups of disk drives and archival storage in enormous tape libraries that greatly exceed the practical storage capacity of large disk farms  disk drives are the major secondary-storage i/0 devices on most computers  most secondary storage devices are either magnetic disks or n1.agnetic tapes  modern disk drives are structured as large one-dimensional arrays of logical disk blocks generally  these logical blocks are 512 bytes in size disks may be attached to a computer system in one of two ways   1  through the local i/0 ports on the host computer or  2  through a network cmmection  544 chapter 12 requests for disk i/0 are generated by the file system and by the virtual memory system each request specifies the address on the disk to be referenced  in the form of a logical block number disk-schedliling algorithms can improve the effective bandwidth  the average response time  and the variance in response time algorithms such as sstf  scan  c-scan  look  and c-look are designed to make such improvements through strategies for disk-queue ordering  performance can be harmed by external fragmentation some systems have utilities that scan the file system to identify fragmented files ; they then move blocks around to decrease the fragmentation defragmenting a badly fragmented file system can significantly improve performance  but the systenc may have reduced performance while the defragmentation is in progress  sophisticated file systems  such as the unix fast file system  incorporate many strategies to control fragmentation during space allocation so that disk reorganization is not needed  the operating system manages the disk blocks first  a disk must be lowlevel formatted to create the sectors on the raw hardware-new disks usually come preformatted then  the disk is partitioned  file systems are created  and boot blocks are allocated to store the system 's bootstrap program finally  when a block is corrupted  the system must have a way to lock out that block or to replace it logically with a spare  because an efficient swap space is a key to good performance  systems usually bypass the file system and use raw disk access for paging i/0 some systems dedicate a raw disk partition to swap space  and others use a file within the file system instead still other systems allow the user or system administrator to make the decision by providing both options  because of the amount of storage required on large systems  disks are frequently made redundant via raid algorithms these algorithms allow more than one disk to be used for a given operation and allow continued operation and even automatic recovery in the face of a disk failure raid algorithms are organized into different levels ; each level provides some combination of reliability and high transfer rates  the write-ahead log scheme requires the availability of stable storage  to implement such storage  we need to replicate the needed information on multiple nonvolatile storage devices  usually disks  with independent failure modes we also need to update the information in a controlled manner to ensure that we can recover the stable data after any failure during data transfer or recovery  tertiary storage is built from disk and tape drives that use removable media many different technologies are available  including magnetic tape  removable magnetic and magneto-optic disks  and optical disks  for removable disks  the operating system generally provides the full services of a file-system interface  including space management and requestqueue scheduling for many operating systems  the name of a file on a removable cartridge is a combination of a drive name and a file name within that drive this convention is simpler but potentially more confusing than is using a name that identifies a specific cartridge  for tapes  the operating system generally provides only a raw interface  many operating systems have no built-in support for jukeboxes jukebox 545 support can be provided by a device driver or by a privileged application designed for backups or for hsm  three important aspects of performance are bandwidth  latency  and reliability many bandwidths are available for both disks and tapes  but the random-access latency for a tape is generally much greater than that for a disk  switching cartridges in a jukebox is also relatively slow because a jukebox has a low ratio of drives to cartridges  reading a large fraction of the data in a jukebox can take a long time optical media  which protect the sensitive layer with a transparent coating  are generally more robust than magnetic media  which are more likely to expose the magnetic material to physical damage  lastly  the cost of storage has decreased greatly in the past two decades  most notably for disk storage  12.1 what would be the effects on cost and performance if tape storage had the same areal density as disk storage  areal density is the number of gigabits per square inch  12.2 it is sometimes said that tape is a sequential-access medium  whereas a magnetic disk is a random-access medium in fact the suitability of a storage device for random access depends on the transfer size  the term streaming transfer rate denotes the rate for a data transfer that is underway  excluding the effect of access latency by contrast  the effective transfer rate is the ratio of total bytes per total seconds  including overhead time such as access latency  suppose that  in a computer  the level-2 cache has an access latency of 8 nanoseconds and a streaming transfer rate of 800 megabytes per second  the main memory has an access latency of 60 nanoseconds and a streaming transfer rate of 80 megabytes per second  the magnetic disk has an access latency of 15 milliseconds and a streaming transfer rate of 5 megabytes per second  and a tape drive has an access latency of 60 seconds and a streaming transfer rate of 2 megabytes per seconds  a random access causes the effective transfer rate of a device to decrease  because no data are transferred during the access time  for the disk described  what is the effective transfer rate if an average access is followed by a streaming transfer of  1  512 bytes   2  8 kilobytes   3  1 megabyte  and  4  16 megabytes b the utilization of a device is the ratio of effective transfer rate to streaming transfer rate calculate the utilization of the disk drive for each of the four transfer sizes given in part a  c suppose that a utilization of 25 percent  or higher  is considered acceptable using the performance figures given  compute the smallest transfer size for disk that gives acceptable utilization  546 chapter 12 d complete the following sentence  a disk is a random-access device for transfers larger than ______ bytes and is a sequentialaccess device for s1naller transfers  e compute the minimum transfer sizes that give acceptable utilization for cache  memory  and tape  f when is a tape a random-access device  and when is it a sequential-access device 12.3 the reliability of a hard-disk drive is typically described in terms of a quantity called mean time between failures  mtbf   although this quantity is called a time  the mtbf actually is measured in drive-hours per failure  a if a system contains 1,000 disk drives  each of which has a 750,000 hour mtbf  which of the following best describes how often a drive failure will occur in that disk farm  once per thousand years  once per century  once per decade  once per year  once per month  once per week  once per day  once per hour  once per minute  or once per second b mortality statistics indicate that  on the average  a u.s resident has about 1 chance in 1,000 of dying between the ages of 20 and 21  deduce the mtbf hours for 20-year-olds convert this figure from hours to years what does this mtbf tell you about the expected lifetime of a 20-year-old c the manufacturer guarantees a 1-million-hour mtbf for a certain model of disk drive what can you conclude about the number of years for which one of these drives is under warranty 12.4 discuss how an operating system could maintain a free-space list for a tape-resident file system assume that the tape technology is append-only and that it uses eot marks and locate  space  and read position commands as described in section 12.9.2.1  12.5 imagine that a holographic storage drive has been invented the drive costs $ 10,000 and has an average access time of 40 milliseconds it uses a $ 100 cartridge the size of a cd this cartridge holds 40,000 images  and each image is a square black-and-white picture with a resolution of 6  000 x 6  000 pixels  each pixel stores 1 bit   the drive can read or write one picture in 1 millisecond answer the following questions  a what would be some good uses for this device b how would this device affect the l/0 performance of a computing system c what kinds of storage devices  if any  would become obsolete as a result of the invention of this device 547 12.6 the term fast wide scsi-ii denotes a scsi bus that operates at a data rate of 20 megabytes per second when it moves a packet of bytes between the host and a device suppose that a fast wide scsi-ii disk drive spins at 7,200 rpm  has a sector size of 512 bytes  and holds 160 sectors per track a estimate the sustained transfer rate of this drive in megabytes per second  b suppose that the drive has 7,000 cylinders  20 tracks per cylinde1 ~ a head-switch time  from one platter to another  of 0.5 millisecond  and an adjacent-cylinder seek time of 2 milliseconds use this additional information to give an accurate estimate of the sustained transfer rate for a huge transfer  c suppose that the average seek time for the drive is 8 milliseconds  estimate the i/0 operations per second and the effective transfer rate for a random-access workload that reads individual sectors that are scattered across the disk d calculate the random-access i/0 operations per second and transfer rate for i/0 sizes of 4 kilobytes  8 kilobytes  and 64 kilobytes  e if multiple requests are in the queue  a scheduling algorithm such as scan should be able to reduce the average seek distance suppose that a random-access workload is reading 8-kilobyte pages  the average queue length is 10  and the scheduling algorithm reduces the average seek time to 3 milliseconds now calculate the i/0 operations per second and the effective transfer rate of the drive  12.7 compare the performance of write operations achieved by a raid level 5 organization with that achieved by a raid level1 organization  12.8 suppose that a disk drive has 5,000 cylinders  numbered 0 to 4999 the drive is currently serving a request at cylinder 143  and the previous request was at cylinder 125 the queue of pending requests  in fifo order  is  86,1470,913,1774,948,1509,1022,1750,130 starting from the current head position  what is the total distance  in cylinders  that the disk arm moves to satisfy all the pending requests for each of the following disk-scheduling algorithms a fcfs b sstf 548 chapter 12 c scan d look e c-scan f c-look 12.9 elementary physics states that when an object is subjected to a constant acceleration a  the relationship between distance d and time t is given by d = ~ at2  suppose that  during a seek  the disk in exercise 12.8 accelerates the disk arm at a constant rate for the first half of the seek  then decelerates the disk arm at the same rate for the second half of the seek assume that the disk can perform a seek to an adjacent cylinder in 1 n lillisecond and a full-stroke seek over all 5,000 cylinders in 18 milliseconds  a the distance of a seek is the number of cylinders that the head moves explain why the seek time is proportional to the square root of the seek distance  b write an equation for the seek time as a function of the seek distance this equation should be of the form t = x + y ~  where t is the time in milliseconds and l is the seek distance in cylinders  c calculate the total seek time for each of the schedules in exercise 12.8 determine which schedule is the fastest  has the smallest total seek time   d the percentage speedup is the time saved divided by the original time what is the percentage speedup of the fastest schedule over fcfs 12.10 the accelerating seek described in exercise 12.9 is typical of hard-disk drives by contrast  floppy disks  and many hard disks manufactured before the mid-1980s  typically seek at a fixed rate suppose that the disk in exercise 12.9 has a constant-rate seek rather than a constantacceleration seek  so the seek time is of the form t = x + yl  where t is the time in milliseconds and l is the seek distance suppose that the time to seek to an adjacent cylinder is 1 millisecond  as before  and the time to seek to each additional cylinder is 0.5 milliseconds  a write an equation for this seek time as a function of the seek distance  b using this seek-time function  calculate the total seek time or each of the schedules in exercise 12.8 is your answer the same as the one or exercise 12.9  c  549 c what is the percentage speedup of the fastest scb.edule over fcfs in this case 12.11 suppose that the disk in exercise 12.9 rotates at 7,200 rpm  a what is the average rotational latency of this disk drive b what seek distance can be covered in the tim.e that you found or part a 12.12 suppose that a one-sided 5.25-inch optical-disk cartridge has an areal density of 1 gigabit per square inch further suppose that a magnetic tape has an areal density of 20 megabits per square inch and is 1/2 inch wide and 1,800 feet long calculate an estimate of the storage capacities of these two kinds of storage media suppose that an optical tape exists that has the same physical size as the magnetic tape but the same storage density as the optical disk what volume of data could the optical tape hold what would be a marketable price for the optical tape if the magnetic tape cost $ 25 12.13 write a program that simulates the disk-scheduling algorithms discussed in section 12.4  12.14 why is rotational latency usually not considered in disk scheduling how would you modify sstf  scan  and c-scan to include latency optimization 12.15 remapping bad blocks by sector sparing or sector slipping can influence perfonnance suppose that the drive in exercise 12.6 has a total of 100 bad sectors at random locations and that each bad sector is mapped to a spare that is located on a different track within the same cylinder estimate the number of i/0 operations per second and the effective transfer rate for a random-access workload consisting of 8 kilobyte reads  assuming a queue length of 1  that is  the choice of scheduling algorithm is not a factor   what is the effect of a bad sector on performance 12.16 discuss the relative advantages and disadvantages of sector sparing and sector slipping  12.17 compare the performance of c-scan and scan scheduling  assuming a uniform distribution of requests consider the average response time  the time between the arrival of a request and the completion of that request 's service   the variation in response time  and the effective 550 chapter 12 bandwidth how does performance depend on the relative sizes of seek time and rotational latency 12.18 none of the disk-scheduling disciplines  except fcfs  is truly fair  starvation may occur   a explain why this assertion is true  b describe a way to modify algorithms such as scan to ensure fairness  c explain why fairness is an important goal in a time-sharing system  d give three or more examples of circumstances in which it is important that the operating system be unfair in serving i/o requests  12.19 consider a raid level 5 organization comprising five disks  with the parity for sets of four blocks on four disks stored on the fifth disk how many blocks are accessed in order to perform the following a a write of one block of data b a write of seven continuous blocks of data 12.20 the operating system generally treats removable disks as shared file systems but assigns a tape drive to only one application at a time give three reasons that could explain this difference in treatment of disks and tapes describe the additional features that an operating system would need to support shared file-system access to a tape jukebox would the applications sharing the tape jukebox need any special properties  or could they use the files as though the files were disk-resident explain your answer  12.21 how would use of a ram disk affect your selection of a disk-scheduling algorithm what factors would you need to consider do the same considerations apply to hard-disk scheduling  given that the file system stores recently used blocks in a buffer cache in main memory 12.22 you can use simple estimates to compare the cost and performance of a terabyte storage system made entirely from disks with one that incorporates tertiary storage suppose that each magnetic disk holds 10gb  costs $ 1,000  transfers 5mb per second  and has an average access latency of 15 milliseconds also suppose that a tape library costs $ 10 per gigabyte  transfers 10 mb per second  and has an average access latency of 20 seconds compute the total cost  the maximum total data rate  and the average waiting time for a pure disk system if you make 551 any assumptions about the workload  describe and justify them now  suppose that 5 percent of the data are frequently used  so they must reside on disk  but the other 95 percent are archived in the tape library  further suppose that the disk system handles 95 percent of the requests and the library handles the other 5 percent what are the total cost  the maximum total data rate  and the average waiting time for this hierarchical storage system 12.23 assume that you have a mixed configuration comprising disks organized as raid levell and raid levels disks assume that the system has flexibility in deciding which disk organization to use for storing a particular file which files should be stored in the raid level 1 disks and which in the raid levels disks in order to optimize performance 12.24 what are the tradeoffs involved in rereading code pages from the file system versus using swap space to store them 12.25 requests are not usually uniformly distributed for example  we can expect a cylinder containing the file-system fat or inodes to be accessed more frequently than a cylinder containing only files suppose you know that 50 percent of the requests are for a small  fixed number of cylinders  a would any of the scheduling algorithms discussed in this chapter be particularly good for this case explain your answer  b propose a disk-scheduling algorithm that gives even better performance by taking advantage of this hot spot on the disk  c file systems typically fil1.d data blocks via an indirection table  such as a fat in dos or inodes in unix describe one or more ways to take advantage of this indirection to improve disk performance  12.26 discuss the reasons why the operating system might require accurate information on how blocks are stored on a disk how could the operating system improve file system performance with this knowledge 12.27 in a disk jukebox  what would be the effect of having more open files than the number of drives in the jukebox 12.28 compare the throughput achieved by a raid levels organization with that achieved by a raid levell organization for the following  a read operations on single blocks b read operations on multiple contiguous blocks 552 chapter 12 12.29 could a raid level 1 organization achieve better performance for read requests than a raid level 0 organization  with nonredundant striping of data  if so  how discussions of redundant arrays of independent disks  raids  are presented by patterson et al  1988  and in the detailed survey of chen et al  1994   disk-system architectures for high-performance computing are discussed by katz et al  1989   enhancements to raid systems are discussed in wilkes et al  1996  and yu et al  2000   teorey and pinkerton  1972  present an early comparative analysis of disk-scheduling algorithms they use simulations that model a disk for which seek time is linear in the number of cylinders crossed  for this disk look is a good choice for queue lengths below 140  and c-look is good for queue lengths above 100 king  1990  describes ways to improve the seek time by moving the disk ann when the disk is otherwise idle seltzer et al   1990  and jacobson and wilkes  1991  describe disk-scheduling algorithms that consider rotational latency in addition to seek time scheduling optimizations that exploit disk idle times are discussed in lumb et al  2000   worthington et al  1994  discuss disk performance and show the negligible performance impact of defect management the placement of hot data to improve seek times has been considered by ruemmler and wilkes  1991  and akyurek and salen'l  1993   ruemmler and wilkes  1994  describe an accurate performance model for a modern disk drive worthington et al  1995  tell how to determine low-level disk properties such as the zone structure  and this work is further advanced by schindler and gregory  1999   disk power management issues are discussed in douglis et al  1994l douglis et al  1995l greenawalt  1994l and golding et al  1995   the i/0 size and randomness of the workload has a considerable influence on disk performance ousterhout et al  1985  and ruemmler and wilkes  1993  report numerous interesting workload characteristics  including that most files are smalt most newly created files are deleted soon thereafter  most files that are opened for reading are read sequentially in their entirety  and most seeks are short mckusick et al  1984  describe the berkeley fast file system  ffs   which uses many sophisticated techniques to obtain good performance for a wide variety of workloads mcvoy and kleiman  1991  discuss further improvements to the basic ffs quinlan  1991  describes how to implement a file system on worm storage with a magnetic disk cache ; richards  1990  discusses a file-system approach to tertiary storage maher et al  1994  give an overview of the integration of distributed file systems and tertiary storage  the concept of a storage hierarchy has been studied for more than thirty years for instance  a 1970 paper by mattson et al  1970  describes a mathematical approach to predicting the performance of a storage hierarchy  alt  1993  describes the accommodation of removable storage in a commercial operating system  and miller and katz  1993  describe the characteristics of tertiary-storage access in a supercomputing environment benjamin  1990  gives an overview of the massive storage requirements for the eosdis project at nasa management and use of network-attached disks and programmable 553 disks are discussed in gibson et al  1997b t gibson et al  1997at riedel et al   1998t and lee and thekkath  1996   holographic storage technology is the subject of an article by psaltis and mok  1995  ; a collection of papers on this topic dating from 1963 has been assembled by sincerbox  1994   asthana and finkelstein  1995  describe several emerging storage technologies  including holographic storage  optical tape  and electron trapping toigo  2000  gives an in-depth description of modern disk technology and several potential future storage technologies  13.1 r the two main jobs of a computer are i/0 and processing in many cases  the main job is i/0  and the processing is merely incidental for instance  when we browse a web page or edit a file  our immediate interest is to read or enter some information  not to compute an answer  the role of the operating system in computer i/0 is to manage and control i/0 operations and i/0 devices although related topics appear in other chapters  here we bring together the pieces to paint a complete picture of i/0 first  we describe the basics of i/o hardware  because the nature of the hardware interface places constraints on the internal facilities of the operating system next  we discuss the i/0 services provided by the operating system and the embodiment of these services in the application i/0 interface then  we explain how the operating system bridges the gap between the hardware interface and the application interface we also discuss the unix system v streams mechanism  which enables an application to assemble pipelines of driver code dynamically finally  we discuss the performance aspects of i/o and the principles of operating-system design that improve i/0 performance  to explore the structure of an operating system 's 1/0 subsystem  to discuss the principles and complexities of 110 hardware  to explain the performance aspects of 110 hardware and software  the control of devices connected to the computer is a major concern of operating-system designers because i/o devices vary so widely in their function and speed  consider a mouse  a hard disk  and a cd-rom jukebox   varied methods are needed to control them these methods form the i/0 subsystem of the kernet which separates the rest of the kernel from the complexities of managing i/0 devices  555 556 chapter 13 13.2 i/o-device technology exhibits two conflicting trends on the one hand  we see increasing standardization of software and hardware interfaces this trend helps 11s to incorporate improved device generations into existing computers and operating systems on the other hand  we see an increasingly broad variety of 1/0 devices some new devices are so unlike previous devices that it is a challenge to incorporate them into our computers and operating systems this challenge is met by a combination of hardware and software techniques the basic i/0 hardware elements  such as ports  buses  and device controllers  accommodate a wide variety of i/0 devices to encapsulate the details and oddities of different devices  the kernel of an operating system is structured to use device-driver modules the present a uniform deviceaccess interface to the i/0 subsystem  much as system calls provide a standard interface between the application and the operating system  computers operate a great many kinds of devices most fit into the general categories of storage devices  disks  tapes   transmission devices  network cards  modems   and human-interface devices  screen  keyboard  mouse   other devices are more specialized  s11ch as those involved in the steering of a military fighter jet or a space shuttle in these aircraft  a human gives input to the flight computer via a joystick and foot pedals  and the computer sends output commands that cause motors to move rudders  flaps  and thrusters  despite the incredible variety of i/0 devices  though  we need only a few concepts to understand how the devices are attached and how the software can control the hardware  a device communicates with a computer system by sending signals over a cable or even through the air the device communicates with the machine via a connection point  or example  a serial port if devices use a common set of wires  the connection is called a bus a is a set of wires and a rigidly defined protocol that specifies a set of messages that can be sent on the wires  in terms of the electronics  the messages are conveyed by patterns of electrical voltages applied to the wires with defined timings when device a has a cable that plugs into device b  and device b has a cable that plugs into device c  and device c plugs into a port on the computer  this arrangement is called a a daisy chain usually operates as a bus  buses are used widely in computer architecture and vary in their signaling methods  speed  throughput  and connection methods a typical pc bus structure appears in figure 13.1 this figure shows a  the common pc system bus  that connects the processor-memory subsystem to the fast devices and an that connects relatively slow devices  such as the keyboard and serial and usb ports in the upper-right portion of the figure  four disks are c01mected together on a scsi bus plugged into a scsi controller  other common buses used to interconnect main parts of a computer include with up to 4.3 gb ;  pcie   with throughput up with throughput up to 20 gb  is a collection of electronics that can operate a port  a bus  or a device a serial-port controller is a simple device controller it is a single chip  or portion of a chip  in the computer that controls the signals on the 13.2 557 figure 13.1 a typical pc bus structure  wires of a serial port by contrast  a scsi bus controller is not simple because the scsi protocol is complex  the scsi bus controller is often implemented as a separate circuit board  or a that plugs into the computer it typically contains a processor  microcode  and some private memory to enable it to process the scsi protocol messages some devices have their own built-in controllers if you look at a disk drive  you will see a circuit board attached to one side this board is the disk controller it implements the disk side of the protocol for some kind of com1ection-scsi or ata  for instance it has microcode and a processor to do many tasks  such as bad-sector mapping  prefetching  buffering  and caching  how can the processor give commands and data to a controller to accomplish an i/0 transfer the short answer is that the controller has one or more registers for data and control signals the processor communicates with the controller by reading and writing bit patterns in these registers one way in which this communication can occur is through the use of special i/0 instructions that specify the transfer of a byte or word to an i/0 port address the i/0 instruction triggers bus lines to select the proper device and to move bits into or out of a device register alternatively  the device controller can support in this case  the device-control registers are mapped into the address space of the processor the cpu executes i/0 requests using the standard data-transfer instructions to read and write the device-control registers  some systems use both techniques for instance  pcs use i/0 instructions to control some devices and memory-mapped i/0 to control others figure 13.2 shows the usual i/o port addresses for pcs the graphics controller has i/o ports for basic control operations  but the controller has a large memory558 chapter 13 000-00f dma controller 020-021 interrupt controller 040-043 timer 200-20f game controller 2f8-2ff serial port  secondary  320-32f hard-disk controller 378-37f parallel port 3d0-3df graphics controller 3f0-3f7 diskette-drive controller 3f8-3ff serial port  primary  figure 13.2 device 1/0 port locations on pcs  partial   mapped region to hold screen contents the process sends output to the screen by writing data into the memory-mapped region the controller generates the screen image based on the contents of this memory this technique is simple to use moreover  writing millions of bytes to the graphics memory is faster than issuing millions of i/0 instructions but the ease of writing to a memory-mapped i/0 controller is offset by a disadvantage because a common type of software fault is a write through an incorrect pointer to an unintended region of memory  a memory-mapped device register is vulnerable to accidental modification of course  protected memory helps to reduce this risk  an i/0 port typically consists of four registers  called the  1  status   2  control   3  data-in  and  4  data-out registers  the the is read by the host to get input  is written by the host to send output  the contains bits that can be read by the host these bits indicate states  such as whether the current command has completed  whether a byte is available to be read from the data-in register  and whether a device error has occurred  the can be written by the host to start a command or to change the nlode of a device for instance  a certain bit in the control register of a serial port chooses between full-duplex and half-duplex communication  another bit enables parity checking  a third bit sets the word length to 7 or 8 bits  and other bits select one of the speeds supported by the serial port  the data registers are typically 1 to 4 bytes in size some controllers have fifo chips that can hold several bytes of input or output data to expand the capacity of the controller beyond the size of the data register a fifo chip can hold a small burst of data until the device or host is able to receive those data  13.2 559 13.2.1 polling the complete protocol for interaction between the host and a controller can be intricate  but the basic handshaking notion is simple we explain handshaking with an example assume that 2 bits are used to coordinate the producer-consumer relationship between the controller and the host the controller indicates its state through the busy bit in the status register  recall that to set a bit means to write a 1 into the bit and to clear a bit means to write a 0 into it  the controller sets the busy bit when it is busy working and clears the busy bit when it is ready to accept the next comm.and the host signals its wishes via the command-ready bit in the command register the host sets the command-ready bit when a command is available for the controller to execute  for this example  the host writes output through a port  coordinating with the controller by handshaking as follows  the host repeatedly reads the busy bit until that bit becomes clear  the host sets the write bit in the command register and writes a byte into the data-out register  the host sets the command-ready bit  when the controller notices that the command-ready bit is set  it sets the busy bit  the controller reads the command register and sees the write command  it reads the data-out register to get the byte and does the i/o to the device  the controller clears the command-ready bit  clears the error bit in the status register to indicate that the device i/o succeeded  and clears the busy bit to indicate that it is finished  this loop is repeated for each byte  in step 1  the host is or it is in a loop  reading the status register over and over until the busy bit becomes clear if the controller and device are fast  this method is a reasonable one but if the wait may be long  the host should probably switch to another task how  then  does the host know when the controller has become idle for some devices  the host must service the device quickly  or data will be lost for instance  when data are streaming in on a serial port or from a keyboard  the small buffer on the controller will overflow and data will be lost if the host waits too long before returning to read the bytes  in many computer architectures  three cpu-instruction cycles are sufficient to poll a device  read a device register  logical-and to extract a status bit  and branch if not zero clearly  the basic polling operation is efficient but polling becomes inefficient when it is attempted repeatedly yet rarely finds a device to be ready for service  while other useful cpu processing remains undone in such instances  it may be more efficient to arrange for the hardware controller to notify the cpu when the device becomes ready for service  rather than to require the cpu to poll repeatedly for an i/0 completion the hardware mechanism that enables a device to notify the cpu is called an 560 chapter 13 7 cpu device driver initiates 1/0 cpu executing checks for interrupts between instructions cpu resumes processing of interrupted task 1/0 controller 4 figure 13.3 interrupt-driven 1/0 cycle  13.2.2 interrupts the basic interrupt mechanism works as follows the cpu hardware has a wire called the that the cpu senses after executing every instruction when the cpu detects that a controller has asserted a signal on the line  the cpu performs a state save and jumps to the at a fixed address in memory the interrupt handler determines the cause of the interrupt  performs the necessary processing  performs a state restore  and executes a return from interrupt instruction to return the cpu to the execution state prior to the interrupt we say that the device controller raises an interrupt by asserting a signal on the interrupt request line  the cpu catches the interrupt and dispatches it to the interrupt handler  and the handler clears the interrupt by servicing the device figure 13.3 summarizes the interrupt-driven i/0 cycle  this basic interrupt mechanism enables the cpu to respond to an asynchronous event  as when a device controller becomes ready for service in a modern operating system  however  we need nlore sophisticated interrupthandling features  we need the ability to defer interrupt handling during critical processing  13.2 561 we need an efficient way to dispatch to the proper interrupt handler for a device without first polling all the devices to see which one raised the interrupt  we need multilevel interrupts  so that the operating system can distinguish between high and low-priority interrupts and can respond with the appropriate degree of urgency  in modern computer hardware  these three features are provided by the cpu and by the most cpus have two interrupt request lines one is the ' ' ' ' which is reserved for events such as unrecoverable memory errors  the second interrupt line is it can be turned off by the cpu before the execution of critical instruction sequences that must not be interrupted  the maskable interrupt is used by device controllers to request service  the interrupt mechanism accepts an number that selects a specific interrupt-handling routine from a small set in most architectures  this address is an offset in a table called the  this vector contains the memory addresses of specialized interrupt handlers the purpose of a vectored interrupt mechanism is to reduce the need for a single interrupt handler to search all possible sources of interrupts to determine which one needs service in practice  however  computers have more devices  and  hence  interrupt handlers  than they have address elements in the interrupt vector  a common way to solve this problem is to use the technique of interrupt chaining  in which each element in the interrupt vector points to the head of a list of interrupt handlers when an il1.terrupt is raised  the handlers on the corresponding list are called one by one  until one is found that can service the request this structure is a compromise between the overhead of a huge interrupt table and the inefficiency of dispatching to a single interrupt handler  figure 13.4 illustrates the design of theinterruptvector for the intel pentium processor the events from 0 to 31  which are nonmaskable  are used to signal various error conditions the events from 32 to 255  which are maskable  are used for purposes such as device-generated interrupts  the interrupt mechanism also implements a system of this mechanism enables the cpu to defer the handling of low-priority interrupts without maskii1.g off all interrupts and makes it possible for a high-priority interrupt to preempt the execution of a low-priority interrupt  a modern operating system interacts with the interrupt mechanism in several ways at boot time  the operating system probes the hardware buses to determine what devices are present and installs the corresponding interrupt handlers into the interrupt vector during i/0  the various device controllers raise interrupts when they are ready for service these interrupts signify that output has cornpleted  or that input data are available  or that a failure has been detected the interrupt mechanism is also used to handle a wide variety of such as dividing by zero  accessing a protected or nonexistent memory address  or attempting to execute a privileged instruction from user mode the events that trigger interrupts have a common property  they are occurrences that induce the cpu to execute an urgent self-contained routine  an operating system has other good uses for an efficient hardware and software mechanism that saves a small amount of processor state and then 562 chapter 13 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 i 18 19-31 32-255 breakpoint into-detected overflow bound range exception invalid opcode device not available double fault coprocessor segment overrun  reserved  invalid task state segment segment not present stack fault general protection page fault  intel reserved  do not use  floating-point error alignment check machine check  intel reserved  do not use  maskable interrupts figure i3.4 intel pentium processor event-vector table  calls a privileged routine in the kernel for example  many operating systems use the interrupt mechanism for virtual memory paging a page fault is an exception that raises an interrupt the interrupt suspends the current process and jumps to the page-fault handler in the kernel this handler saves the state of the process  moves the process to the wait queue  performs page-cache management  schedules an i/0 operation to fetch the page  schedules another process to resume execution  and then returns from the interrupt  another example is found in the implementation of system calls usually  a program uses library calls to issue system calls the library routines check the arguments given by the application  build a data structure to convey the arguments to the kernel  and then execute a special instruction called a or  this instruction has an operand that identifies the desired kernel service when a process executes the trap instruction  the interrupt hardware saves the state of the user code  switches to supervisor mode  and dispatches to the kernel routine that implements the requested service the trap is given a relatively low interrupt priority compared with those assigned to device interrupts-executilcg a system call on behalf of an application is less urgent than servicing a device controller before its fifo queue overflows and loses data  interrupts can also be used to manage the flow of control within the kernel  for example  consider the processing required to complete a disk read one step is to copy data from kernel space to the user buffer this copying is time consuming but not urgent-it should not block other high-priority interrupt 13.2 563 handling another step is to start the next pending l/0 for that disk drive this step has higher priority if the disks are to be used efficiently  we need to start the next i/o as soon as the previous one completes consequently  a pair of interrupt handlers implen ents the kernel code that completes a disk read the high-priority handler records the l/0 status  clears the device interrupt  starts the next pending i/0  and raises a low-priority interrupt to complete the work  later  when the cpu is not occupied with high-priority work  the low-priority interrupt will be dispatched the corresponding handler completes the userlevel i/0 by copying data from kernel buffers to the application space and then calling the scheduler to place the application on the ready queue  a threaded kernel architecture is well suited to implement multiple interrupt priorities and to enforce the precedence of interrupt handling over background processing in kernel and application routines we illustrate this point with the solaris kernel in solaris  interrupt handlers are executed as kernel threads a range of high priorities is reserved for these threads  these priorities give interrupt handlers precedence over application code and kernel housekeeping and implement the priority relationships among interrupt handlers the priorities cause the solaris thread scheduler to preempt lowpriority interrupt handlers in favor of higher-priority ones  and the threaded implementation enables multiprocessor hardware to run several interrupt handlers concurrently we describe the interrupt architecture of windows xp and unix in chapter 22 and appendix a  respectively  in summa  r  y  interrupts are used throughout modern operating systems to handle asynchronous events and to trap to supervisor-mode routines in the kernel to enable the most urgent work to be done first  modern computers use a system of interrupt priorities device controllers  hardware faults  and system calls all raise interrupts to trigger kernel routines because interrupts are used so heavily for time-sensitive processing  efficient interrupt handling is required for good system performance  13.2.3 direct memory access for a device that does large transfers  such as a disk drive  it seems wasteful to use an expensive general-purpose processor to watch status bits and to feed data into a controller register one byte at a time-a process termed many computers avoid burdening the main cpu with pio by offloading some of this work to a special-purpose processor called a to initiate a dma transfer  the host writes a dma command block into memory this block contains a pointer to the source of a transfer  a pointer to the destination of the transfer  and a count of the number of bytes to be transferred the cpu writes the address of this command block to the dma controller  then goes on with other work the dma controller proceeds to operate the memory bus directly  placing addresses on the bus to perform transfers without the help of the main cpu a simple dma controller is a standard component in pcs  and for the pc usually contain their own high-speed dma hardware  handshaking between the dma controller and the device controller is performed via a pair of wires called dma-request and dma-acknowledge  the device controller places a signal on the dma-request wire when a word of data is available for transfer this signal causes the dma controller to seize 564 chapter 13 the memory bus  place the desired address on the memory-address wires  and place a signal on the dl \ iia -acknowledge wire when the device controller receives the dma-acknowledge signat it transfers the word of data to memory and removes the dma-request signal  when the entire transfer is finished  the dma controller interrupts the cpu  this process is depicted in figure 13.5 when the dma controller seizes the memory bus  the cpu is momentarily prevented from accessing main memory although it can still access data items in its primary and secondary caches  although this can slow down the cpu computation  offloading the data-transfer work to a dma controller generally improves the total system performance some computer architectures use physical memory addresses for dma  but others perform mercwry using virtual addresses that undergo translation to physical addresses dvma can perform a transfer between two memory-mapped devices without the intervention of the cpu or the use of main memory  on protected-mode kernels  the operating system generally prevents processes from issuing device commands directly this discipline protects data from access-control violations and also protects the system from erroneous use of device controllers that could cause a system crash instead  the operating system exports functions that a sufficiently privileged process can use to access low-level operations on the underlying hardware on kernels without memory protection  processes can access device controllers directly this direct access can be used to achieve high performance/ since it can avoid kernel communication  context switches  and layers of kernelsoftware unfortunately  it interferes with system security and stability the trend in general-purpose operating systems is to protect memory and devices so that the system can try to guard against erroneous or malicious applications  5 dma controller transfers bytes to buffer x  increasing memory address and decreasing c until c = 0 1 device driver is told to transfer disk data to buffer at address x 2 device driver tells l  ' '  ~ i ' ' ~ --' ' disk controller to transfer c bytes from disk to buffer at address x 6 when c = 0  dma interrupts cpu to signal transfer completion 1  2.'.c ~ li.ip  2i  .2-j rc-c ~ .,.,---j' = ,._ ~ 3 disk controller initiates dma transfer c'g ! ,or  tt_ront ; r ' i 4 disk controller sends each byte to dma controller figure 13.5 steps in a dma transfer  13.3 13.3 565 13.2.4 1/0 hardware summary although the hardware aspects of i/0 are complex when considered at the level of detail of electronics-hardware design  the concepts that we have just described are sufficient to enable us to understand many i/0 features of operating systen s let 's review the main concepts  a bus a controller an i/0 port and its registers the handshaking relationship between the host and a device controller the execution of this handshaking in a polling loop or via interrupts the offloading of this work to a dma controller for large transfers we gave a basic example of the handshaking that takes place between a device controller and the host earlier in this section in reality  the wide variety of available devices poses a problem for operating-system implementers each kind of device has its own set of capabilities  control-bit definitions  and protocols for interacting with the host-and they are all different how can the operating system be designed so that we can attach new devices to the computer without rewriting the operating system and when the devices vary so widely  how can the operating system give a convenient  uniform i/0 interface to applications we address those questions next  in this section  we discuss structuring techniques and interfaces for the operating system that enable i/0 devices to be treated in a standard  uniform way we explain  for instance  how an application can open a file on a disk without knowing what kind of disk it is and how new disks and other devices can be added to a cmnputer without disruption of the operating system  like other complex software-engineering problems  the approach here involves abstraction  encapsulation  and software layering specifically  we can abstract away the detailed differences in i/0 devices by identifying a few general kinds each kind is accessed through a standardized set of functions-an the differences are encapsulated in kernel modules called device drivers that internally are custom-tailored to specific devices but that export one of the standard interfaces figure 13.6 illustrates how the i/o-related portions of the kernel are structured in software layers  the purpose of the device-driver layer is to hide the differences among device controllers from the i/o subsystem of the kernel  much as the i/0 system calls encapsulate the behavior of devices in a few generic classes that hide hardware differences from applications making the i/0 subsystem independent of the hardware simplifies the job of the operating-system developer it also benefits the hardware manufacturers they either design new devices to be compatible with an existing host controller interface  such as scsi-2   or they write device drivers to interface the new hardware to popular 566 chapter 13 figure 13.6 a kernel i/o structure  operating systems thus  we can attach new peripherals to a computer without waiting for the operating-system vendor to develop support code  unfortm1ately for device-hardware manufacturers  each type of operating system has its own standards for the device-driver interface a given device may ship with multiple device drivers-for instance  drivers for ms-dos  windows 95/98  windows nt/2000  and solaris devices vary on many dimensions  as illustrated in figure 13.7  character-stream or block a character-stream device transfers bytes one by one  whereas a block device transfers a block of bytes as a unit  sequential or random access a sequential device transfers data in a fixed order determined by the device  whereas the user of a random-access device can instruct the device to seek to any of the available data storage locations  synchronous or asynchronous a synchronous device performs data transfers with predictable response times an asynchronous device exhibits irregular or unpredictable response times  sharable or dedicated a sharable device can be used concurrently by several processes or threads ; a dedicated device can not  speed of operation device speeds range from a few bytes per second to a few gigabytes per second  read -write  read only  or write only some devices perform both input and output  but others support only one data transfer direction  access method transfer schedule i/o direction 13.3 synchronous asynchronous dedicated sharable latency seek time transfer rate delay between operations read only write only read-write tape keyboard tape keyboard cd-rom figure i 3 7 characteristics of 1/0 devices  567 for the purpose of application access  many of these differences are hidden by the operating system  and the devices are grouped into a few conventional types the resulting styles of device access have been found to be useful and broadly applicable although the exact system calls may differ across operating systems  the device categories are fairly standard the major access conventions include block i/0  character-stream i/0  memory-mapped file access  and network sockets operating systems also provide special system calls to access a few additional devices  such as a time-of-day clock and a timer  some operating systems provide a set of system calls for graphical display  video  and audio devices  most operating systems also have an  or that transparently passes arbitrary conunands from an application to a device driver in unix  this system call is ioctl    for i/0 control   the ioctl   system call enables an application to access any functionality that can be implernented by any device driver  without the need to invent a new system call the ioctl   system call has three arguments the first is a file descriptor that connects the application to the driver by referring to a hardware device managed by that driver the second is an integer that selects one of the commands implemented in the driver the third is a pointer to an arbitrary data structure in memory that enables the application and driver to communicate any necessary control information or data  13.3.1 block and character devices the captures all the aspects necessary for accessing disk drives and other block-oriented devices the device is expected to understand commands such as read   and write   ; if it is a random-access device  it is also expected to have a seek   command to specify which block to transfer next  568 chapter 13 applications normally access such a device through a file-system interface  we can see that read    write    and seek 0 capture the essen.tial behaviors of block-storage devices  so that applications are insulated from the low-level differences among those devices  the operating system itself  as well as special applications such as databasemanagement systems  may prefer to access a block device as a simple linear array of blocks this mode of access is sometimes called if the application performs its own buffering  then using a file systen1 would cause extra  unneeded buffering likewise  if an application provides its own locking of file blocks or regions  then any operating-system locking services would be redundant at the least and contradictory at the worst to avoid these conflicts  raw-device access passes control of the device directly to the application  letting the operating system step out of the way unfortunately  no operating-system services are then performed on this device a compromise that is becoming common is for the operating system to allow a mode of operation on a file that disables buffering and locking in the unix world  this is called memory-mapped file access can be layered on top of block-device drivers  rather than offering read and write operations  a memory-mapped interface provides access to disk storage via an array of bytes in main memory the system call that maps a file into memory returns the virtual memory address that contains a copy of the file the actual data transfers are performed only when needed to satisfy access to the memory image because the transfers are handled by the same mechanism as that used for demand-paged virtual memory access  memory-mapped i/o is efficient memory mapping is also convenient for programmers-access to a memory-mapped file is as simple as reading from and writing to memory operating systems that offer virtual memory commonly use the mapping interface for kernel services for instance  to execute a program  the operating system maps the executable into memory and then transfers control to the entry address of the executable the mapping interface is also commonly used for kernel access to swap space on disk  a keyboard is an example of a device that is accessed through a the basic system calls in this interface enable an application to get   or put   one character on top of this interface  libraries can be built that offer line-at-a-time access  with buffering and editing services  for example  when a user types a backspace  the preceding character is removed from the input stream   this style of access is convenient for input devices such as keyboards  mice  and modems that produce data for input spontaneously -that is  at times that cam1.ot necessarily be predicted by the application this access style is also good for output devices such as printers and audio boards  which naturally fit the concept of a linear stream of bytes  13.3.2 network devices because the performance and addressing characteristics of network i/0 differ significantly from those of disk i/0  most operating systems provide a network i/o interface that is different from the read   -write   -seek   interface used for disks one interface available in many operating systerns  including unix and windows nt  is the network interface  think of a wall socket for electricity  any electrical appliance can be plugged in by analogy  the system calls in the socket interface enable an application 13.3 569 to create a socket  to connect a local socket to a remote address  which plugs this application into a socket created by another application   to listen for any remote application to plug into the local socket  and to send and receive packets over the connection to support the implementation of servers  the socket interface also provides a function called select   that manages a set of sockets a call to select   returns information about which sockets have a packet waiting to be received and which sockets have room to accept a packet to be sent the use of select   eliminates the polling and busy waiting that would otherwise be necessary for network i/0 these functions encapsulate the essential behaviors of networks  greatly facilitating the creation of distributed applications that can use any underlying network hardware and protocol stack many other approaches to interprocess communication and network communication have been implemented for instance  windows nt provides one interface to the network interface card and a second interface to the network protocols  appendix c.6   in unix  which has a long history as a proving ground for network technology  we find half-duplex pipes  full-duplex fifos  full-duplex streams  message queues  and sockets information on unix networking is given in appendix a.9  13.3.3 clocks and timers most computers have hardware clocks and timers that provide three basic functions  give the current time  give the elapsed time  set a timer to trigger operation x at time t  these functions are used heavily by the operating system  as well as by timesensitive applications unfortunately  the system calls that implement these functions are not standardized across operating systems  the hardware to measure elapsed time and to trigger operations is called a  it can be set to wait a certain amount of time generate an interrupt  and it can be set to do this once or to repeat the process to generate periodic interrupts the scheduler uses this mechanism to generate an interrupt that will preempt a process at the end of its time slice  the disk i/o subsystem uses it to invoke the periodic flushing of dirty cache buffers to disk  and the network subsystem uses it to cancel operations that are proceeding too slowly because of network congestion or failures the operating system may also provide an interface for user processes to use timers the operating system can support more timer requests than the number of timer hardware chan11els by simulating virtual clocks to do so  the kernel  or the timer device driver  maintains a list of interrupts wanted by its own routines and by user requests  sorted in earliest-time-first order it sets the timer for the earliest tince when the timer interrupts  the kernel signals the requester and reloads the timer with the next earliest time  on many computers  the interrupt rate generated by the hardware clock is between 18 and 60 ticks per second this resolution is coarse  since a modern computer can execute hundreds of millions of instructions per second the 570 chapter 13 precision of triggers is limited by the coarse resolution of the timer  together with the overhead of maintaining virtual clocks furthermore  if the timer ticks are used to maintain the system time-of-day clock  the system clock can drift in most computers  the hardware clock is constructed from a highfrequency counter in some computers  the value of this counter can be read from a device register  in which case the counter can be considered a highresolution clock although this clock does not generate interrupts  it offers accurate measurements of time intervals  13.3.4 blocking and nonblocking 1/0 another aspect of the system-call interface relates to the choice between blocking i/0 and nonblocking i/0 when an application issues a system call  the execution of the application is suspended the application is moved from the operating system 's run queue to a wait queue after the system call completes  the application is moved back to the run queue  where it is eligible to resume execution when it resumes execution  it will receive the values returned by the system call the physical actions performed by i/0 devices are generally asynchronous-they take a varying or unpredictable amount of time nevertheless  most operating systems use blocking system calls for the application interface  because blocking application code is easier to understand than nonblocking application code  some user-level processes need i/0 one example is a user interface that receives keyboard and mouse input while processing and displaying data on the screen another example is a video application that reads frames from a file on disk while simultaneously decompressing and displaying the output on the display  one way an application writer can overlap execution with i/0 is to write a multithreaded application some threads can perform blocking system calls  while others continue executing the solaris developers used this technique to implement a user-level library for asynchronous i/0  freeing the application writer from that task some operating systems provide nonblocking i/0 system calls a nonblocking call does not halt the execution of the application for an extended time h1.stead  it returns quickly  with a return value that indicates how many bytes were transferred  an alternative to a nonblocking system call is an asynchronous system call an asynchronous call returns immediately  without waiting for the i/0 to complete the application continues to execute its code the completion of the i/0 at some future time is communicated to the application  either through the setting of some variable in the address space of the application or through the triggering of a signal or software interrupt or a call-back routine that is executed outside the linear control flow of the application the difference between nonblocking and asynchronous system calls is that a nonblocking read   returns immediately with whatever data are available-the full number of bytes requested  fewer  or none at all an asynchronous read   call requests a transfer that will be performed in its entirety but will complete at some future time these two i/0 methods are shown in figure 13.8  a good example of nonblocking behavior is the select   system call for network sockets this system call takes an argument that specifies a maximum waiting time by setting it to 0  an application can poll for network activity 13.4 13.4 571 kernel user user kernel  a   b  figure 13.8 two 1/0 methods   a  synchronous and  b  asynchronous  without blocking but using select   introduces extra overhead  because the select   call only checks whether i/0 is possible for a data transfer  select   must be followed by some kind of read   or write   command  a variation on this approach  fotmd in mach  is a blocking multiple-read call  it specifies desired reads for several devices in one system call and returns as soon as any one of them completes  kernels provide many services related to i/0 several services-scheduling  buffering  caching  spooling  device reservation  and error handlil1.g-are provided by the kernel 's i/0 subsystem and build on the hardware and devicedriver infrastructure the i/o subsystem is also responsible for protectil1.g itself from errant processes and malicious users  13.4.1 1/0 scheduling to schedule a set of i/o requests means to determine a good order in which to execute them the order in which applications issue system calls rarely is the best choice scheduling can improve overall system performance  can share device access fairly among processes  and can reduce the average waiting time for i/0 to complete here is a simple example to illustrate suppose that a disk arm is near the begilming of a disk and that three applications issue blocking read calls to that disk application 1 requests a block near the end of the disk  application 2 requests one near the beginning  and application 3 requests one in the middle of the disk the operating system can reduce the distance that the disk ann travels by serving the applications in the order 2  3  1 rearrangil1.g the order of service in this way is the essence of i/0 scheduling  operating-system developers implement scheduling by maintaining a wait queue of requests for each device when an application issues a blocking i/0 system call  the request is placed on the queue for that device the i/0 scheduler rearranges the order of the queue to improve the overall system efficiency and the average response time experienced by applications the operating 572 chapter 13 figure 13.9 device-status table  system may also try to be fair  so that no one application receives especially poor service  or it may give priority service for delay-sensitive requests for instance  requests from the virtual memory subsystem may take priority over application requests several scheduling algorithms for disk i/0 are detailed in section 12.4  when a kernel supports asynchronous i/0  it must be able to keep track of many i/0 requests at the same time for this purpose  the operating system might attach the wait queue to a  able the kernel manages this table  which contains an entry for each i/0 device  as shown in figure 13.9  each table entry indicates the device 's type  address  and state  not functioning  idle  or busy   if the device is busy with a request  the type of request and other parameters will be stored in the table entry for that device  one way in which the i/0 subsystem improves the efficiency of the computer is by scheduling i/0 operations another way is by using storage space in main memory or on disk via teclul.iques called buffering  caching  and spooling  13.4.2 buffering a is a memory area that stores data being transferred between two devices or between a device and an application buffering is done for three reasons one reason is to cope with a speed mismatch between the producer and consumer of a data stream suppose  for example  that a file is being received via modem for storage on the hard disk the modem is about a thousand times slower than the hard disk so a buffer is created in main mernory to accumulate the bytes received from the modem when an entire buffer of data has arrived  the buffer can be written to disk in a single operation since the disk write is not instantaneous and the modem still needs a place to store additional incoming data  two buffers are used after the modem fills the first buffer  the disk write is requested the modem then starts to fill the second buffer while the first buffer is written to disk by the time the modem has filled 13.4 573 the second buffer  the disk write from the first one should have completed  so the modem can switch back to the first buffer while the disk writes the second one this decouples the producer of data from the consun1.er  thus relaxing timing requirements between them the need for this decoupling is illustrated in figure 13.10  which lists the enormous differences in device speeds for typical computer hardware  a second use of buffering is to provide adaptations for devices that have different data-transfer sizes such disparities are especially common in computer networking  where buffers are used widely for fragmentation and reassembly of messages at the sending side  a large message is fragmented into small network packets the packets are sent over the network  and the receiving side places them in a reassembly buffer to form an image of the source data  a third use of buffering is to support copy semantics for application i/0  an example will clarify the meaning of copy semantics suppose that an application has a buffer of data that it wishes to write to disk it calls the write   systemcalt providing a pointer to the buffer and an integer specifying the number of bytes to write after the system call returns  what happens if the application changes the contents of the buffer with the version of the data written to disk is guaranteed to be version at the time of the application system calt independent of any subsequent changes in the application 's buffer a simple way in which the operating system can guarantee copy semantics is for the write   system call to copy the application i system bus hype  ~ ransport  32,pair  ~ ~ ~ iii ~ ~ ~ ~ ~ ~ ~ ~ i pci ~ xpress 2.0  32  i lnfi ! l.i band  qdr ; .1 2x  0.00001 0.001 0.1 10 1000 100000 1 efigure 13.10 sun enterprise 6000 device-transfer rates  logarithmic   574 chapter 13 data into a kernel buffer before returning control to the application the disk write is performed from the kernel buffer  so that subsequent changes to the application buffer have no effect copying of data between kernel buffers and application data space is common in operating systems  despite the overhead that this operation introduces  because of the clean semantics the same effect can be obtained more efficiently by clever use of virtual memory mapping and copy-on-write page protection  13.4.3 caching a is a region of fast memory that holds copies of data access to the cached copy is more efficient than access to the original for instance  the instructions of the currently running process are stored on disk  cached ilc physical memory  and copied again ill the cpu 's secondary and primary caches the difference between a buffer and a cache is that a buffer may hold the only existing copy of a data item  whereas a cache  by definition  holds a copy on faster storage of an item that resides elsewhere  caching and buffering are distinct functions  but sometinces a region of memory can be used for both purposes for illstance  to preserve copy semantics and to enable efficient scheduling of disk i/0  the operating system uses buffers in maill memory to hold disk data these buffers are also used as a cache  to improve the i/o efficiency for files that are shared by applications or that are being written and reread rapidly when the kernel receives a file i/0 request  the kernel first accesses the buffer cache to see whether that region of the file is already available in main memory if it is  a physical disk i/o can be avoided or deferred also  disk writes are accumulated ill the buffer cache for several seconds  so that large transfers are gathered to allow efficient write schedules this strategy of delayilcg writes to improve i/o efficiency is discussed  in the context of remote file access  ill section 17.3  13.4.4 spooling and device reservation a is a buffer that holds output for a device  such as a printer  that can not accept ilcterleaved data streams although a prillter can serve only one job at a time  several applications may wish to print their output concurrently  without having their output mixed together the operating system solves this problem by intercepting all output to the printer each application 's output is spooled to a separate disk file when an application finishes printing  the spooling system queues the correspondilcg spool file for output to the printer  the spooling system copies the queued spool files to the printer one at a time in some operating systems  spooling is managed by a system daemon process in others  it is handled by an in-kernel thread in either case  the operating system provides a control interface that enables users and system administrators to display the queue  remove unwanted jobs before those jobs print  suspend printing while the printer is serviced  and so on  some devices  such as tape drives and printers  can not usefully multiplex the i/0 requests of multiple concurrent applications spooling is one way operating systems can coordinate concurrent output another way to deal with concurrent device access is to provide explicit facilities for coordination some operating systems  including vms  provide support for exclusive device access by enabling a process to allocate an idle device and to deallocate that device 13.4 575 when it is no longer needed other operating systems enforce a limit of one open file handle to such a device many operating systems provide functions that enable processes to coordinate exclusive access among then'lselves for instance  windows nt provides system calls to wait until a device object becomes available it also has a parameter to the open   system call that declares the types of access to be permitted to other concurrent threads on these systems  it is up to the applications to avoid deadlock  13.4.5 error handling an operating system that uses protected memory can guard against many kinds of hardware and application errors  so that a complete system failure is not the usual result of each minor mechanical glitch devices and i/0 transfers can fail in many ways  either for transient reasons  as when a network becomes overloaded  or for permanent reasons  as when a disk controller becomes defective operating systems can often compensate effectively for transient failures for instance  a disk read   failure results in a read   retry  and a network send   error results in a res end    if the protocol so specifies  unfortunately  if an important component experiences a permanent failure  the operating system is unlikely to recover  as a general rule  an i/0 system call will return one bit of information about the status of the call  signifying either success or failure in the unix operating system  an additional integer variable named errno is used to return an error code-one of about a hundred values-indicating the general nature of the failure  for example  argument out of range  bad pointer  or file not open   by contrast  some hardware can provide highly detailed error information  although many current operating systems are not designed to convey this information to the application for instance  a failure of a scsi device is reported by the scsi protocol in three levels of detail  a key that identifies the general nature of the failure  such as a hardware error or an illegal request ; an that states the category of failure  such as a bad command parameter or a self-test failure ; and an 'x ' l '.l that gives even more detail  such as which command parameter was in error or which hardware subsystem failed its self-test further  many scsi devices maintain internal pages of error-log information that can be requested by the host-but seldom are  13.4.6 1/0 protection errors are closely related to the issue of protection a user process may accidentally or purposely attempt to disrupt the normal operation of a systern by attempting to issue illegal i/0 instructions we can use various mechanisms to ensure that such disruptions cam'lot take place in the system  to prevent users from performing illegal i/0  we define all i/0 instructions to be privileged instructions thus  users can not issue i/o instructions directly ; they must do it through the operating system to do i/0  a user program executes a system call to request that the operating system perform i/0 on its behalf  figure 13.11   the operating system  executing in monitor mode  checks that the request is valid and  if it is  does the i/0 requested the operating system then returns to the user  576 chapter 13 cd trap to monitor kernel perform 1/0 return to user user program figure 13.1 1 use of a system call to perform 1/0  in addition  any memory-mapped and i/o port memory locations must be protected from user access by the memory-protection system note that a kernel can not simply deny all user access most graphics games and video editing and playback software need direct access to memory-mapped graphics controller memory to speed the performance of the graphics  for example the kernel might in this case provide a locking mechanism to allow a section of graphics memory  representing a window on screen  to be allocated to one process at a time  13.4.7 kernel data structures the kernel needs to keep state information about the use of i/0 components  it does so through a variety of in-kernel data structures  such as the open-file table structure from section 11.1 the kernel uses many similar structures to track network connections  character-device communications  and other i/0 activities  unix provides file-system access to a variety of entities  such as user files  raw devices  and the address spaces of processes although each of these entities supports a read   operation  the semantics differ for instance  to read a user file  the kernel needs to probe the buffer cache before deciding whether to perform a disk i/0 to read a raw disk  the kernel needs to ensure that the request size is a multiple of the disk sector size and is aligned on a sector boundary to read a process image  it is merely necessary to copy data from memory unix encapsulates these differences within a uniform structure by using an object-oriented teclucique the open-file record  shown in 13.4 577 system-wide open-file table 1 ;  ;       ;  ' ; t ; . ' file-system record 1  ~  s  1 ~ if ~    inode pointer +   ;       ' pointer to read and write functions i      i  ; .l  pointer to select function ; ;  ;    ;  ~ ti ~ ~ ~ ~ ~ ple pointer to ioctl function file descriptor .,_   ;  \ ' '  ' ' pointer to close function n   .r              r ; i ~ ~ ~  ~ ~ ~ kl  f user-process memory networking  socket  record i  ' ~ 1 ~ t ~ 6 ~ ! v'.'  pointer to network info + f   ;    pointer to read and write.functions     ~   pointer to select function pointer to ioctl function pointer to close f..un ction  kernel memory figure 13 12 unix 1/0 kernel structure  figure 13.12  contains a dispatch table that holds pointers to the appropriate routines  depending on the type of file  some operating systems use object-oriented methods even more extensively  for instance  windows nt uses a message-passing implementation for i/0 an i/0 request is converted into a message that is sent through the kernel to the ii 0 manager and then to the device driver  each of which may change the message contents for output  the message contains the data to be written for input  the message contains a buffer to receive the data the message-passing approach can add overhead  by comparison with procedural techniques that use shared data structures  but it simplifies the structure and design of the i/0 system and adds flexibility  13.4.8 kernel i/o subsystem summary in summary  the i/0 subsystem coordinates an extensive collection of services that are available to applications and to other parts of the kernel the i/0 subsystenc supervises these procedures  management of the name space for files and devices access control to files and devices operation control  for example  a modem can not seek    file-system space allocation device allocation 578 chapter 13 13.5 buffering  caching  and spooling i/0 scheduling device-status monitoring  error handling  and failure recovery device-driver configuration and initialization the upper levels of the i/o subsystem access devices via the uniform interface provided by the device drivers  earlier  we described the handshaking between a device driver and a device controller  but we did not explain how the operating system connects an application request to a set of network wires or to a specific disk sector  consider  for example  reading a file from disk the application refers to the data by a file name within a disk  the file system maps from the file name through the file-system directories to obtain the space allocation of the file for instance  in ms-dos  the name maps to a number that indicates an entry in the file-access table  and that table entry tells which disk blocks are allocated to the file in unix  the name maps to an inode number  and the corresponding inode contains the space-allocation information but how is the connection made from the file name to the disk controller  the hardware port address or the memory-mapped controller registers  one method is that used by ms-dos  a relatively simple operating system  the first part of an ms-dos file name  preceding the colon  is a string that identifies a specific hardware device for example  c  is the first part of every file name on the primary hard disk the fact that c  represents the primary hard disk is built into the operating system ; c  is mapped to a specific port address through a device table because of the colon separator  the device name space is separate from the file-system name space this separation makes it easy for the operating system to associate extra functionality with each device for instance  it is easy to invoke spooling on any files written to the printer  if  instead  the device name space is incorporated in the regular file-system name space  as it is in unix  the normal file-system name services are provided automatically if the file system provides ownership and access control to all file names  then devices have owners and access control since files are stored on devices  such an interface provides access to the i/o system at two levels  names can be used to access the devices themselves or to access the files stored on the devices  unix represents device names in the regular file-system name space unlike an ms-dos file name  which has a colon separator  a unix path name has no clear separation of the device portion in fact  no part of the path name is the name of a device unix has a that associates prefixes of path names with specific device names to resolve a path name  unix looks up the name in the mount table to find the longest ncatchilcg prefix ; the corresponding entry in the mount table gives the device name this device name also has the form of a name in the file-system name space when unix looks up this name in the file-system directory structures  it finds not an inode number but a major  13.5 579 minor device number the m.ajor device number identifies a device driver that should be called to handle l/0 to this device the minor device number is passed to the device driver to index into a device table the corresponding device-table entry gives the port address or the memory-mapped address of the device controller  modern operating systems obtain significant flexibility from the multiple stages of lookup tables in the path between a request and a physical device controller the mechanisms that pass requests between applications and drivers are general thus  we can introduce new devices and drivers into a computer without recompiling the kernel in fact  some operating systems have the ability to load device drivers on demand at boot time  the system first probes the hardware buses to determine what devices are present ; it then loads in the necessary drivers  either immediately or when first required by an i/0 request  we next describe the typical life cycle of a blocking read request  as depicted in figure 13.13 the figure suggests that an i/0 operation requires a great many steps that together consume a tremendous number of cpu cycles  a process issues a blocking read   system call to a file descriptor of a file that has been opened previously  the system-call code in the kernel checks the parameters for correctness  in the case of input  if the data are already available irl the buffer cache  the data are returned to the process  and the i/o request is completed  otherwise  a physical i/0 must be performed the process is removed from the run queue and is placed on the wait queue for the device  and the i/0 request is scheduled eventually  the i/0 subsystem sends the request to the device driver depending on the operating system  the request is sent via a subroutine call or an in-kernel message  the device driver allocates kernel buffer space to receive the data and schedules the i/0 eventually  the driver sends commands to the device controller by writing into the device-control registers  the device controller operates the device hardware to perform the data transfer  the driver may poll for status and data  or it may have set up a dma transfer into kernel memory we assume that the transfer is managed by a dma controller  which generates an interrupt when the transfer completes  the correct interrupt handler receives the interrupt via the interruptvector table  stores any necessary data  signals the device driver  and returns from the interrupt  the device driver receives the signal  determines which i/0 request has completed  determines the request 's status  and signals the kernel i/0 subsystem that the request has been completed  the kernel transfers data or return codes to the address space of the requesting process and moves the process from the wait queue back to the ready queue  580 chapter 13 13.6 system call device-controller commands user process kernel 1/0 subsystem kernel 1/0 subsystem device driver interrupt handler device controller return from system call interrupt ~ -------tim_e ~  ~   figure 13.13 the life cycle of an 1/0 request  moving the process to the ready queue unblocks the process when the scheduler assigns the process to the cpu  the process resumes execution at the completion of the system call  unix system v has an interesting mechanism  called that enables an application to assemble pipelines of driver code dynamically a stream is a full-duplex connection between a device driver and a user-level process it consists of a that interfaces with the user process  a  id that controls the device  and zero or more between the stream user process 13.6 i streams modules _j figure 13.14 the streams structure  581 head and the driver end each of these components contains a pair of queues -a read queue and a write queue message passing is used to transfer data between queues the streams structure is shown in figure 13.14  modules provide the functionality of streams processing ; they are pushed onto a stream by use of the ioctl   system call for examplef a process can open a serial-port device via a stream and can push on a module to handle input editing because messages are exchanged between queues in adjacent modules  a queue in one module may overflow an adjacent queue to prevent this from occurring  a queue may support without flow control  a queue accepts all messages and immediately sends them on to the queue in the adjacent module without buffering them a queue supporting flow control buffers messages and does not accept messages without sufficient buffer space ; this process involves exchanges of control messages between queues in adjacent modules  a user process writes data to a device using either the write   orputmsg   system call the write   system call writes raw data to the stream  whereas putmsg   allows the user process to specify a message regardless of the system call used by the user process  the stream head copies the data into a message and delivers it to the queue for the next module in line this copying of messages continues until the message is copied to the driver end and hence the device similarly  the user process reads data from the stream head using either the read   or getmsg   system call if read   is used  the stream head gets a message from its adjacent queue and returns ordinary data  an unstructured byte stream  to the process if getmsg   is used  a message is returned to the process  582 chapter 13 13.7 streams i/0 is asynchronous  or nonblocking  except when the user process communicates with the stream ~ head when writing to the stream  the user process will block  assuming the next queue uses flow controt until there is room to copy the message likewise  the user process will block when reading from the stream ~ until data are available  as mentioned  the driver end-like the stream head and modules-has a read and write queue however  the driver end must respond to interrupts  such as one triggered when a frame is ready to be read from a network unlike the stream head  which may block if it is unable to copy a message to the next queue in line  the driver end must handle all incoming data drivers must support flow control as well however  if a device 's buffer is fult the device typically resorts to dropping incoming messages consider a network card whose input buffer is full the network card must simply drop further messages until there is ample buffer space to store incoming messages  the benefit of using streams is that it provides a framework for a modular and incremental approach to writing device drivers and network protocols modules may be used by different streams and hence by different devices for example  a networking module may be used by both an ethernet network card and a 802.11 wireless network card furthermore  rather than treating character-device i/o as an unstructured byte stream  streams allows support for message boundaries and control information when communicating between modules most unix variants support streams  and it is the preferred method for writing protocols and device drivers for example  system v unix and solaris implement the socket mechanism using streams  i/ 0 is a major factor in system performance it places heavy demands on the cpu to execute device-driver code and to schedule processes fairly and efficiently as they block and unblock the resulting context switches stress the cpu and its hardware caches i/o also exposes any inefficiencies in the interrupt-handling mechanisms in the kernel in addition  i/o loads down the memory bus during data copies between controllers and physical memory and again durilcg copies between kernel buffers and application data space coping gracefully with all these demands is one of the major concerns of a computer architect  although modern computers can handle many thousands of interrupts per second  interrupt handling is a relatively expensive task each interrupt causes the system to perform a state change  to execute the interrupt handler  and then to restore state programmed i/0 can be more efficient than internjpt-driven i/0  if the number of cycles spent in busy waiting is not excessive an i/0 completion typically unblocks a process  leading to the full overhead of a context switch  network traffic can also cause a high context-switch rate consider  for instance  a remote login from one machine to another each character typed on the local machine must be transported to the remote machine on the local machine  the character is typed ; a keyboard interrupt is generated ; and the character is passed through the interrupt handler to the device driver  to the kernet and then to the user process the user process issues a network i/o system call to send the character to the remote machine the character then 13.7 583 flows into the local kernel  through the network layers that construct a network packet  and into the network device driver the network device driver transfers the packet to the network controller  which sends the character and generates an interrupt the interrupt is passed back up through the kernel to cause the network l/0 system call to complete  now  the remote system 's network hardware receives the packet  and an interrupt is generated the character is unpacked from the network protocols and is given to the appropriate network daemon the network daemon identifies which remote login session is involved and passes the packet to the appropriate subdaemon for that session throughout this flow  there are context switches and state switches  figure 13.15   usually  the receiver echoes the character back to the sender ; that approach doubles the work  to eliminate the context switches involved in moving each character between daemons and the kernel  the solaris developers reimplemented the daemon using in-kernel threads sun estimates that this improvement sending system receiving system figure 13.15 lntercomputer communications  584 chapter 13 increased the maximum number of network logins from a few hundred to a few thousand on a large server  other systems use separate for terminal i/0 to reduce the interrupt burden on the main cpu for instance  a can multiplex the traffic from hundreds of remote terminals into one port on a large computer an is a dedicated  special-purpose cpu found in mainframes and in other high-end systems the job o a channel is to offload i/0 work from the main cpu the idea is that the cham1.els keep the data flowing smoothly  while the main cpu remains free to process the data like the device controllers and dma controllers found in smaller computers  a channel can process more general and sophisticated programs  so channels can be tuned for particular workloads  we can employ several principles to improve the efficiency of i/0  reduce the number of context switches  reduce the number of times that data must be copied in memory while passing between device and application  reduce the frequency of interrupts by using large transfers  smart controllers  and polling  if busy waiting can be minimized   increase concurrency by using dma-knowledgeable controllers or channels to offload simple data copying from the cpu  move processing primitives into hardware  to allow their operation in device controllers to be concurrent with cpu and bus operation  balance cpu  memory subsystem  bus  and i/o performance  because an overload in any one area will cause idleness in others  i/0 devices vary greatly in complexity for instance  a mouse is simple the mouse movements and button clicks are converted into numeric values that are passed from hardware  through the mouse device driver  to the application by contrast  the functionality provided by the windows nt disk device driver is complex it not only manages individual disks but also implements raid arrays  section 12.7   to do so  it converts an application 's read or write request into a coordinated set of disk i/0 operations moreover  it implements sophisticated error-handling and data-recovery algorithms and takes many steps to optimize disk performance  where should the i/0 functionality be implemented -in the device hardware  in the device driver  or in application software sometimes we observe the progression depicted in figure 13.16  initially  we implement experimental i/0 algorithms at the application level  because application code is f1exible and application bugs are unlikely to cause system crashes furthermore  by developing code at the application level  we avoid the need to reboot or reload device drivers after every change to the code an application-level implementation can be inefficient  however  because of the overhead o context switches and because the application can not take advantage of internal kernel data structures and 13.8 13.8 585 device code  hardware  figure 13.16 device functionality progression  kernel functionality  such as efficient in-kernel messaging  threading  and locking   when an application-level algorithm has demonstrated its worth  we may reimplement it in the kernel this can improve performance  but the development effort is more challenging  because an operating-system kernel is a large  complex software system moreover  an in-kernel implementation must be thoroughly debugged to avoid data corruption and system crashes  the highest performance may be obtained through a specialized implementation in hardware  either in the device or in the controller the disadvantages of a hardware implementation include the difficulty and expense of making further improvements or of fixing bugs  the increased development time  months rather than days   and the decreased flexibility  for instance  a hardware raid controller may not provide any means for the kernel to influence the order or location of individual block reads and writes  even if the kernel has special information about the workload that would enable it to improve the i/0 performance  the basic hardware elements involved in i/0 are buses  device controllers  and the devices themselves the work of moving data between devices and main memory is perform.ed by the cpu as programmed i/0 or is offloaded to a dma controller the kernel module that controls a device is a device driver the system-call interface provided to applications is designed to handle several basic categories of hardware  including block devices  character devices  memory-mapped files  network sockets  and programmed interval timers the system calls usually block the processes that issue them  but nonblocking and 586 chapter 13 asynchronous calls are used by the kernel itself and by applications that must not sleep while waiting for an i/0 operation to complete  the kernel 's i/o subsystem provides num.erous services among these are i/0 scheduling  buffering  caching  spooling  device reservation  and error handling another service  name translation  makes the connections between hardware devices and the symbolic file names used by applications it involves several levels of mapping that translate from character-string names  to specific device drivers and device addresses  and then to physical addresses of ii 0 ports or bus controllers this mapping may occur within the file-system name space  as it does in unix  or in a separate device name space  as it does in ms-dos  streams is an implementation and methodology that provides a framework for a modular and incremental approach to writing device drivers and network protocols through streams  drivers can be stacked  with data passing through them sequentially and bidirectionally for processing  i/o system calls are costly in terms of cpu consumption because of the many layers of software between a physical device and an application these layers imply overhead from several sources  context switching to cross the kernel 's protection boundary  signal and interrupt handling to service the i/0 devices  and the load on the cpu and memory system to copy data between kernel buffers and application space  13.1 write  in pseudocode  an implementation of virtual clocks  including the queueing and management of timer requests for the kernel and applications assume that the hardware provides three timer channels  13.2 what are the advantages and disadvantages of supporting memorymapped i/0 to device control registers 13.3 typically  at the completion of a device i/0  a single interrupt is raised and appropriately handled by the host processor in certain settings  however  the code that is to be executed at the completion of the i/0 can be broken into two separate pieces the first piece executes immediately after the i/0 completes and schedules a second interrupt for the remaining piece of code to be executed at a later time what is the purpose of using this strategy in the design of interrupt handlers 13.4 why might a system use interrupt-driven i/0 to manage a single serial port and polling i/0 to manage a front-end processor  such as a termii1.al concentrator 13.5 what are the various kinds of performance overhead associated with servicing an interrupt 13.6 unix coordinates the activities of the kernel i/0 components by manipulating shared in-kernel data structures  whereas windows nt uses object-oriented message passing between kernel i/o components  discuss three pros and three cons of each approach  587 13.7 in most multiprogrammed systems  user programs access memory through virtual addresses  while the operating system uses raw physical addresses to access men10ry what are the implications of this design for the initiation of i/0 operations by the user program and their execution by the operating system 13.8 polling for an i/0 completion can waste a large number of cpu cycles if the processor iterates a busy-waiting loop many times before the i/0 completes but if the i/0 device is ready for service  polling can be much more efficient than is catching and dispatching an interrupt describe a hybrid strategy that combines polling  sleeping  and interrupts for i/0 device service for each of these three strategies  pure polling  pure interrupts  hybrid   describe a computing environment in which that strategy is more efficient than is either of the others  13.9 consider the following i/0 scenarios on a single-user pc  a a mouse used with a graphical user interface b a tape drive on a multitasking operating system  with no device preallocation available  c a disk drive containing user files d a graphics card with direct bus connection  accessible through memory-mapped i/0 for each of these scenarios  would you design the operating system to use buffering  spooling  caching  or a combin_ation would you use polled i/o or interrupt-driven i/0 give reasons for your choices  13.10 the example of handshaking in section 13.2 used 2 bits  a busy bit and a command-ready bit is it possible to implement this handshaking with only 1 bit if it is  describe the protocol if it is not  explain why 1 bit is insufficient  13.11 discuss the advantages and disadvantages of guaranteeing reliable transfer of data between modules in the streams abstraction  13.12 some dma controllers support direct virtual memory access  where the targets of i/0 operations are specified as virtual addresses and a translation from virtual to physical address is performed during the dma how does this design complicate the design of the dma controller what are the advantages of providing such functionality 13.13 why is it important to scale up system-bus and device speeds as cpu speed increases 13.14 when multiple interrupts from different devices appear at about the same time  a priority scheme could be used to determine the order in which the interrupts would be serviced discuss what issues need to be considered in assigning priorities to different interrupts  588 chapter 13 13.15 describe three circumstances under which blocking ii 0 should be used  describe three circumstances under which nonblocking i/0 should be used why not just implement nonblocking i/0 and have processes busy-wait until their devices are ready vahalia  1996  provides a good overview of i/o and networking in unix  leffler et al  1989  detail the i/o structures and methods employed in bsd unix milenkovic  1987  discusses the complexity of i/0 methods and implementation the use and programming of the various interprocesscommunication and network protocols in unix are explored in stevens  1992   brain  1996  documents the windows nt application interface the i/o implementation in the sample minix operating system is described in tanenbaum and woodhull  1997   custer  1994  includes detailed information on the nt message-passing implementation of i/0  for details of hardware-level ii 0 handling and memory-mapping functionality  processor reference manuals  motorola  1993  and intel  1993   are among the best sources hennessy and patterson  2002  describe multiprocessor systems and cache-consistency issues tanenbaum  1990  describes hardware i/0 design at a low level  and sargent and shoemaker  1995  provide a programmer 's guide to low-level pc hardware and software the ibm pc device i/o address map is given in ibm  1983   the march 1994 issue of ieee computer is devoted to i/0 hardware and software raga  1993  provides a good discussion of streams  part six protection mechanisms control access to a system by limiting the types of file access permitted to users in addition  protection must ensure that only processes that have gained proper authorization from the operating system can operate on memory segments  the cpu  and other resources  protection is provided by a mechanism that controls the access of programs  processes  or users to the resources defined by a computer system this mechanism must provide a means for specifying the controls to be imposed  together with a means of enforcing them  security ensures the authentication of system users to protect the integrity of the information stored in the system  both data and code   as well as the physical resources of the computer system the security system prevents unauthorized access  malicious destruction 01 alteration of data  and accidental introduction of inconsistency  14.1 chapter the processes in an operating system must be protected from one another 's activities to provide such protection  we can use various mechanisms to ensure that only processes that have gained proper authorization from the operating system can operate on the files  memory segments  cpu  and other resources of a system  protection refers to a mechanism for controlling the access of programs  processes  or users to the resources defined by a computer system this mechanism must provide a means for specifying the controls to be imposed  together with a means of enforcement we distinguish between protection and security  which is a measure of confidence that the integrity of a system and its data will be preserved in this chapter  we focus on protection security assurance is a much broader topic  and we address it in chapter 15  to discuss the goals and principles of protection in a modern computer system  to explain how protection domains  combined with an access matrix  are used to specify the resources a process may access  to examine capability and language-based protection systems  as computer systems have become more sophisticated and pervasive in their applications  the need to protect their integrity has also grown protection was originally conceived as an adjunct to multiprogramming operating systems  so that untrustworthy users might safely share a common logical name space  such as a directory of files  or share a common physical name space  such as memory modern protection concepts have evolved to increase the reliability of any complex system that makes use of shared resources  we need to provide protection for several reasons the most obvious is the need to prevent the mischievous  intentional violation of an access restriction 591 592 chapter 14 14.2 by a user of more general importance  however  is the need to ensure that each program component active in a system uses system resources only in ways consistent with stated policies this requirement is an absolute one for a reliable system  protection can improve reliability by detecting latent errors at the interfaces between component subsystems early detection of interface errors can often prevent contamination of a healthy subsystem by a malfunctioning subsystem  also  an unprotected resource can not defend against use  or misuse  by an unauthorized or incompetent user a protection-oriented system provides means to distinguish between authorized and unauthorized usage  the role of protection in a computer system is to provide a mechanism for the enforcement of the policies governing resource use these policies can be established in a variety of ways some are fixed in the design of the system  while others are formulated by the management of a system still others are defined by the individual users to protect their own files and programs a protection system must have the flexibility to enforce a variety of policies  policies for resource use may vary by application  and they may change over time for these reasons  protection is no longer the concern solely of the designer of an operating system the application programmer needs to use protection mechanisms as well  to guard resources created and supported by an application subsystem against misuse in this chapter  we describe the protection mechanisms the operating system should provide  but application designers can use them as well in designing their own protection software  note that mechanisms are distinct from policies mechanisms determine how something will be done ; policies decide what will be done the separation of policy and mechanism is important for flexibility policies are likely to change from place to place or time to time in the worst case  every change in policy would require a change in the underlying mechanism using general mechanisms enables us to avoid such a situation  frequently  a guiding principle can be used throughout a project  such as the design of an operating system following this principle simplifies design decisions and keeps the system consistent and easy to understand a key  time-tested guiding principle for protection is the it dictates that programs  users  and even systems be given just enough privileges to perform their tasks  consider the analogy of a security guard with a passkey if this key allows the guard into just the public areas that she guards  then misuse of the key will result in minimal damage if  however  the passkey allows access to all areas  then damage from its being lost  stolen  misused  copied  or otherwise compromised will be much greater  an operating system following the principle of least privilege implements its features  programs  system calls  and data structures so that failure or compromise of a component does the minimum damage and allows the n1inimum damage to be done the overflow of a buffer in a system daemon might cause the daemon process to fail  for example  but should not allow the execution of code from the daemon process 's stack that would enable a remote 14.3 14.3 593 user to gain maximum privileges and access to the entire system  as happens too often today   such an operating system also provides system calls and services that allow applications to be written with fine-grained access controls it provides mechanisms to enable privileges when they are needed and to disable them when they are not needed also beneficial is the creation of audit trails for all privileged function access the audit trail allows the prograrnmer  systems administrator  or law-enforcement officer to trace all protection and security activities on the system  managing users with the principle of least privilege entails creating a separate account for each user  with just the privileges that the user needs an operator who needs to mount tapes and back up files on the system has access to just those commands and files needed to accomplish the job some systems implement role-based access control  rbac  to provide this functionality  computers implemented in a computing facility under the principle of least privilege can be limited to running specific services  accessing specific remote hosts via specific services  and doing so during specific times typically  these restrictions are implemented through enabling or disabling each service and through using access control lists  as described in sections 10.6.2 and 14.6  the principle of least privilege can help produce a more secure computing environment unfortunately  it frequently does not for example  windows 2000 has a complex protection scheme at its core and yet has many security holes by comparison  solaris is considered relatively secure  even though it is a variant of unix  which historically was designed with little protection in mind one reason for the difference may be that windows 2000 has more lines of code and more services than solaris and thus has more to secure and protect another reason could be that the protection scheme in windows 2000 is irtcomplete or protects the wrong aspects of the operating system  leaving other areas vulnerable  a computer system is a collection of processes and objects by objects  we mean both  such as the cpu  memory segments  printers  disks  and tape drives  and  such as files  programs  and semaphores   each object has a unique name that differentiates it from all other objects in the system  and each can be accessed only through well-defined and meaningful operations objects are essentially abstract data types  the operations that are possible may depend on the object for example  on a cpu  we can only execute memory segments can be read and written  whereas a cd-rom or dvd-rom can only be read tape drives can be read  written  and rewound data files can be created  opened  read  written  closed  and deleted ; program files can be read  written  executed  and deleted  a process should be allowed to access only those resources for which it has authorization furthermore  at any time  a process should be able to access only those reso1jrces that it currently reqllires to complete its task this second requirement  conunonly referred to as the need-to-know principle  is useful in limiting the amount of damage a faulty process can cause in the system for example  when process p invokes procedure a    the procedure should be 594 chapter14 allowed to access only its own variables and the formal parameters passed to it ; it should not be able to access all the variables of process p similarly  consider the case in which process p invokes a compiler to compile a particular file the compiler should not be able to access files arbitrarily but should have access only to a well-defined subset of files  such as the source file  listing file  and so on  related to the file to be compiled conversely  the compiler may have private files used for accounting or optimization purposes that process p should not be able to access the need-to-know principle is similar to the principle of least privilege discussed in section 14.2 in that the goals of protection are to minimize the risks of possible security violations  14.3.1 domain structure to facilitate the scheme just described  a process operates within a which specifies the resources that the process may access each domain defines a set of objects and the types of operations that may be invoked on each object the ability to execute an operation on an object is an a domain is a collection of access rights  each of which is an ordered pair object-name  rights-set  for example  if domain d has the access right file f   read  write   then a process executing in domain d can both read and write file f ; it can not  however  perform any other operation on that object  domains do not need to be disjoint ; they may share access rights for example  in figure 14.1  we have three domains  d1  d2  and d3  the access right 0 4   print  is shared by d2 and d3  implying that a process executing in either of these two domains can print object 0 4  note that a process must be executing in domain d1 to read and write object 0 1  while only processes in domain d3 may execute object 0 1  the association between a process and a domain may be either if the set of resources available to the process is fixed throughout the process 's lifetime  or as might be expected  establishing dynamic protection domains is more complicated than establishing static protection domains  if the association between processes and domains is fixed  and we want to adhere to the need-to-know principle  then a mechanism must be available to change the content of a domain the reason stems from the fact that a process may execute in two different phases and may  for example  need read access in one phase and write access in another if a domain is static  we must define the domain to include both read and write access however  this arrangement provides more rights than are needed in each of the two phases  since we have read access in the phase where we need only write access  and vice versa  thus  the need-to-know principle is violated we must allow the contents of 0 3   read  write  0 1  read  write  0 2   execute  0 2   write  01   execute  0 3   read  figure 14.1 system with three protection domains  14.3 595 a domain to be modified so that the domain always reflects the n1inimum necessary access rights  if the association is dynamic  a mechanism is available to allow enabling the process to switch from one domain to another we may also want to allow the content of a domain to be changed if we can not change the content of a domain  we can provide the same effect by creating a new domain with the changed content and switching to that new domain when we want to change the domain content  a domain can be realized in a variety of ways  each user may be a domain in this case  the set of objects that can be accessed depends on the identity of the user domain switching occurs when the user is changed -generally when one user logs out and another user logs in  each process may be a domain in this case  the set of objects that can be accessed depends on the identity of the process domain switching occurs when one process sends a message to another process and then waits for a response  each procedure may be a domain in this case  the set of objects that can be accessed corresponds to the local variables defined within the procedure  domain switching occurs when a procedure call is made  we discuss domain switching in greater detail in section 14.4  consider the standard dual-mode  monitor-user mode  model of operating-system execution when a process executes in monitor mode  it can execute privileged instructions and thus gain complete control of the computer system in contrast  when a process executes in user mode  it can invoke only nonprivileged instructions consequently  it can execute only within its predefined memory space these two modes protect the operating system  executing in monitor domain  from the user processes  executing in user domain   in a multiprogrammed operating system  two protection domains are insufficient  since users also want to be protected from one another therefore  a more elaborate scheme is needed we illustrate such a scheme by examining two influential operating systems-unix and multics -to see how they implement these concepts  14.3.2 an example  unix in the unix operating system  a domain is associated with the user switching the domain corresponds to changing the user identification temporarily  this change is accomplished tbough the file system as follows an owner identification and a domain bit  known as the setuid bit  are associated with each file when the setuid bit is on  and a user executes that file  the user id is set to that of the owner of the file ; when the bit is off  however  the user id does not change for example  when a user a  that is  a user with userid = a  starts executing a file owned by b  whose associated domain bit is off  the userid of the process is set to a when the setuid bit is on  the userid is set to that of the owner of the file  b when the process exits  this temporary userid change ends  596 chapter 14 other methods are used to change domains in operating systems in which user ids are used for domain definition  because almost all systems need to provide such a mechanism this mechanism is used when an otherwise privileged facility needs to be made available to the general user population  for instance  it might be desirable to allow users to access a network without letting them write their own networking programs in such a case  on a unix system  the setuid bit on a networking program would be set  causing the user ld to change when the program was run the user ld would change to that of a user with network access privilege  such as root  the most powerful user id   one problem with this method is that if a user manages to create a file with user id root and with its setuid bit on  that user can become root and do anything and everything on the system the setuid mechanism is discussed further in appendix a  an alternative to this method used in other operating systems is to place privileged programs in a special directory the operating system would be designed to change the user ld of any program run from this directory  either to the equivalent of root or to the user ld of the owner of the directory this eliminates one security problem with setuid programs in which crackers create and hide such programs for later use  using obscure file or directory names   this method is less flexible than that used in unix  however  even more restrictive  and thus more protective  are systems that simply do not allow a change of user id in these instances  special techniques must be used to allow users access to privileged facilities for instance  a may be started at boot time and run as a special user id users then run a separate program  which sends requests to this process whenever they need to use the facility this method is used by the tops-20 operating system  in any of these systems  great care must be taken in writing privileged programs any oversight can result in a total lack of protection on the system  generally  these programs are the first to be attacked by people trying to break into a system ; unfortunately  the attackers are frequently successful  for example  security has been breached on many unix systems because of the setuid feature we discuss security in chapter 15  14.3.3 an example  mul tics in the multics system  the protection domains are organized hierarchically into a ring structure each ring corresponds to a single domain  figure 14.2   the rings are numbered from 0 to 7 let d ; and dj be any two domain rings  if j i  then d ; is a subset of dj that is  a process executing in domain dj has more privileges than does a process executing in domain d ;  a process executing in domain do has the most privileges if only two rings exist  this scheme is equivalent to the monitor-user n1ode of execution  where monitor mode corresponds to do and user mode corresponds to d1  multics has a segmented address space ; each segment is a file  and each segment is associated with one of the rings a segm.ent description includes an entry that identifies the ring number in addition  it includes three access bits to control reading  writing  and execution the association between segments and rings is a policy decision with which we are not concerned here  a current-ring-number counter is associated with each process  identifying the ring in which the process is executing currently when a process is executing 14.3 597 figure 14.2 multics ring structure  in ring i  it cmmot access a segment associated with ring j  j i   it can access a segment associated with ring k  k    i   the type of access  however  is restricted according to the access bits associated with that segment  domain switching in multics occurs when a process crosses from one ring to another by calling a procedure in a different ring obviously  this switch must be done in a controlled mmmer ; otherwise  a process could start executing in ring 0  and no protection would be provided to allow controlled domain switching  we modify the ring field of the segment descriptor to include the following  access bracket a pair of integers  bl and b2  such that bl   =  b2  limit an integer b3 such that b3 b2  list of gates identifies the entry points  or may be called  at which the segments if a process executing in ring i calls a procedure  or segncent  with access bracket  bl,b2   then the call is allowed if bl   =  i   =  b2  and the current ring number of the process remains i otherwise  a trap to the operating system occurs  and the situation is handled as follows  if i bl  then the call is allowed to occur  because we have a transfer to a ring  or domain  with fewer privileges however  if parameters are passed that refer to segments in a lower ring  that is  segments not accessible to the called procedure   then these segments must be copied into an area that can be accessed by the called procedure  if i b2  then the call is allowed to occur only if b3 is greater than or equal to i and the call has been directed to one of the designated entry points in the list of gates this scheme allows processes with limited access rights to call procedures in lower rings that have more access rights  but only in a carefully controlled mmmer  598 chapter 14 14.4 the main disadvantage of the ring  or hierarchical  structure is that it does not allow us to enforce the need-to-know principle in particular  if an object must be accessible in domain 0 j but not accessible in domain oi  then we must have j i but this requirement means that every segment accessible in oi is also accessible in 0 1  the multics protection system is generally more complex and less efficient than are those used in current operating systems if protection interferes with the ease of use of the system or significantly decreases system performance  then its use must be weighed carefully against the purpose of the system for instance  we would want to have a complex protection system on a computer used by a university to process students ' grades and also used by students for classwork a similar protection system would not be suited to a computer being used for number crunching  in which performance is of utmost importance we would prefer to separate the mechanism from the protection policy  allowing the same system to have complex or simple protection depending on the needs of its users to separate mechanism from policy  we require a more general model of protection  our model of protection can be viewed abstractly as a matrix  called an the rows of the access matrix represent domains  and the columns represent objects each entry in the matrix consists of a set of access rights  because the column defines objects explicitly  we can omit the object name from the access right the entry access  i,j  defines the set of operations that a process executing in domain oi can invoke on object oj  to illustrate these concepts  we consider the access matrix shown in figure 14.3 there are four domains and four objects-three files  f1  f2  f3  and one laser printer a process executing in domain 0 1 can read files f1 and f3  a process executing in domain 0 4 has the same privileges as one executing in domain 0 1 ; but in addition  it can also write onto files f1 and f3  note that the laser printer can be accessed only by a process executing in domain 0 2  the access-matrix scheme provides us with the mechanism for specifying a variety of policies the mechanism consists of implementing the access 01 read read 02 print 03 read execute 04 read read write write figure 14.3 access matrix  14.4 599 matrix and ensuring that the semantic properties we have outlined hold  more specifically  we must ensure that a process executing in domain n can access only those objects specified in row  and then only as allowed by the access-matrix entries  the access matrix can implement policy decisions concerning protection  the policy decisions involve which rights should be included in the  i,j  th entry we must also decide the domain in which each process executes this last policy is usually decided by the operating system  the users normally decide the contents of the access-matrix entries when a user creates a new object oi  the column oi is added to the access matrix with the appropriate initialization entries  as dictated by the creator the user may decide to enter some rights in some entries in cohum1 j and other rights in other entries  as needed  the access matrix provides an appropriate mechanism for defining and implementing strict control for both the static and dynamic association between processes and domains when we switch a process from one domain to another  we are executing an operation  switch  on an object  the domain   we can control domain switching by including domains among the objects of the access matrix similarly  when we change the content of the access matrix  we are performing an operation on an object  the access matrix again  we can control these changes by including the access matrix itself as an object  actually  since each entry in the access matrix may be modified individually  we must consider each entry in the access matrix as an object to be protected  now  we need to consider only the operations possible on these new objects  domains and the access matrix  and decide how we want processes to be able to execute these operations  processes should be able to switch from one domain to another switching from domain d ; to domain di is allowed if and only if the access right switch e access  i,j   thus  in figure 14.4  a process executing in domain d2 can switch to domain d3 or to domain d4  a process in domain d4 can switch to d1  and one in domain d1 can switch to d2 allowing controlled change in the contents of the access-matrix entries requires three additional operations  copy  owner  and control we examine these operations next  01 read read switch 02 print switch switch 03 read execute 04 read read switch write write figure 14.4 access matrix of figure 14.3 with domains as objects  600 chapter 14  a  execute read execute execute read  b  figure 14.5 access matrix with copy rights  the ability to copy an access right from one domain  or row  of the access matrix to another is denoted by an asterisk   appended to the access right  the copy right allows the access right to be copied only within the colurrm   that is  for the object  for which the right is defined for example  in figure 14.5  a   a process executing in domain d2 can copy the read operation into any entry associated with file f2  hence  the access matrix of figure 14.5  a  can be modified to the access matrix shown in figure 14.5  b   this scheme has two variants  a right is copied from access  i  j  to access  jc  j  ; it is then removed from access  i  j   this action is a transfer of a right  rather than a copy  propagation of the copy right may be limited that is  when the right r is copied from access  i,j  to access  lc,j   only the right r  not r  is created a process executing in domain d ~ r can not further copy the right r  a system may select only one of these three copy rights  or it may provide all three by identifying them as separate rights  copy  transfer  and limited copy  we also need a mechanism to allow addition of new rights and removal of some rights the owner right controls these operations if access  i  j  includes the owner right  then a process executing in domain di can add and remove any right in any entry in column j for example  in figure 14.6  a   domain d1 is the owner of f1 and thus can add and delete any valid right in column f1  similarly  domain d2 is the owner of f2 and f3 and thus can add and remove any valid right within these two columns thus  the access matrix of figure 14.6  a  can be modified to the access matrix shown in figure 14.6  b   14.4 601 01 owner write execute read read 02 owner owner write 03 execute  a  01 owner write execute owner read 02 read owner write write 03 write write  b  figure 14.6 access matrix with owner rights  the copy and owner rights allow a process to change the entries in a column  a mechanism is also needed to change the entries in a row the control right is applicable only to domain objects if access  i  j  includes the control right  then a process executing in domain di can remove any access right from row j for example  suppose that  in figure 14.4  we include the control right in access  d2  d4   then  a process executil1.g in domain d2 could modify domai11 d4  as shown in figure 14.7  read read switch print switch switch control read execute write write switch figure 14.7 modified access matrix of figure 14.4  602 chapter 14 14.5 the copy and owner rights provide us with a mechanism to limit the propagation of access rights however  they do not give us the appropriate tools for preventing the propagation  or disclosure  of information the problem of guaranteeing that no information initially held in an object can migrate outside of its execution environment is called the  this problem is in general unsolvable  see the bibliographical notes at the end of the chapter   these operations on the domains and the access matrix are not in themselves important  but they illustrate the ability of the access-matrix model to allow the implementation and control of dynamic protection requirements  new objects and new domains can be created dynamically and included in the access-matrix model however  we have shown only that the basic mechanism exists ; system designers and users must make the policy decisions concerning which domains are to have access to which objects in which ways  how can the access matrix be implemented effectively in general  the matrix will be sparse ; that is  most of the entries will be empty although datastructure techniques are available for representing sparse matrices  they are not particularly useful for this application  because of the way in which the protection facility is used here  we first describe several methods of implementing the access matrix and then compare the methods  14.5.1 global table the simplest implementation of the access matrix is a global table consisting of a set of ordered triples domain  object  rights-set  whenever an operation m is executed on an object oj within domain d ;  the global table is searched for a triple d ;  0 1  r ~ c  with me r ~ c if this triple is found  the operation is allowed to continue ; otherwise  an exception  or error  condition is raised  this implementation suffers from several drawbacks the table is usually large and thus can not be kept in main memory  so additional i/0 is needed  virtual memory techniques are often used for managing this table in addition  it is difficult to take advantage of special groupings of objects or domains  for example  if everyone can read a particular object  this object must have a separate entry in every domain  14.5.2 access lists for objects each column in the access matrix can be implemented as an access list for one object  as described in section 10.6.2 obviously  the empty entries can be discarded the resulting list for each object consists of ordered pairs domain  rights-set  which define all domains with a nonempty set of access rights for that object  this approach can be extended easily to define a list plus a default set of access rights when an operation m on an object oi is attempted in domain d ;  we search the access list for object 0 i  looking for an entry d ;  r1c with me rjc if the entry is found  we allow the operation ; if it is not  we check the default set if m is in the default set  we allow the access otherwise  access is 14.5 603 denied  and an exception condition occurs for efficiency  we may check the default set first and then search the access list  14.5.3 capability lists for domains rather than associating the columns of the access matrix with the objects as access lists  we can associate each row with its domain a ltst for a domain is a list of objects together with the operations allowed on tbose objects an object is often represented by its physical name or address  called a to execute operation m on object 0 1  the process executes the operation m  specifying the capability  or pointer  for object 0 j as a parameter  simple of the capability means that access is allowed  the capability list is associated with a domain  but it is never directly accessible to a process executing in that domain rather  the capability list is itself a protected object  maintained by the operating system and accessed by the user only indirectly capability-based protection relies on the fact that the capabilities are never allowed to migrate into any address space directly accessible by a user process  where they could be modified   if all capabilities are secure  the object they protect is also secure against unauthorized access  capabilities were originally proposed as a kind of secure pointer  to meet the need for resource protection that was foreseen as multiprogrammed computer systems came of age the idea of an inherently protected pointer provides a fom1dation for protection that can be extended up to the applications level  to provide inherent protection  we must distinguish capabilities from other kinds of objects  and they must be interpreted by an abstract machine on which higher-level programs run capabilities are usually distinguished from other data in one of two ways  each object has a to denote whether it is a capability or accessible data the tags themselves must not be directly accessible by an application program hardware or firmware support may be used to enforce this restriction although only one bit is necessary to distinguish between capabilities and other objects  more bits are often used this extension allows all objects to be tagged with their types by the hardware thus  the hardware can distinguish integers  floating-point numbers  pointers  booleans  characters  instructions  capabilities  and uninitialized values by their tags  alternatively  the address space associated with a program can be split into two parts one part is accessible to the program and contains the program 's normal data and instructions the other part  containing the capability list  is accessible only by the operating system a segmented memory space  section 8.6  is useful to support this approach  several capability-based protection systems have been developed ; we describe them briefly in section 14.8 the mach operating system also uses a version of capability-based protection ; it is described in appendix b  604 chapter 14 14.5.4 a lock-key mechanism the t ' is a compromise between access lists and capability lists each object has a list of unique bit patterns  called similarly  each domain has a list of unique bit patterns  called a process executing in a domain can access an object only if that domain has a key that matches one of the locks of the object  as with capability lists  the list of keys for a domain must be managed by the operating system on behalf of the domain users are not allowed to examine or modify the list of keys  or locks  directly  14.5.5 comparison as you might expect choosing a technique for implementing an access matrix involves various trade-offs using a global table is simple ; however  the table can be quite large and often can not take advantage of special groupings of objects or domains access lists correspond directly to the needs of users  when a user creates an object he can specify which domains can access the object as well as what operations are allowed however  because access-rights information for a particular domain is not localized  determining the set of access rights for each domain is difficult in addition  every access to the object must be checked  requiring a search of the access list in a large system with long access lists  this search can be time consuming  capability lists do not correspond directly to the needs of users ; they are usefut however  for localizing information for a given process the process attempting access must present a capability for that access then  the protection system needs only to verify that the capability is valid revocation of capabilities  however  may be inefficient  section 14.7   the lock-key mechanism  as mentioned  is a compromise between access lists and capability lists the mechanism can be both effective and flexible  depending on the length of the keys the keys can be passed freely from domain to domain in addition  access privileges can be effectively revoked by the simple technique of changing some of the locks associated with the object  section 14.7   most systems use a combination of access lists and capabilities when a process first tries to access an object  the access list is searched if access is denied  an exception condition occurs otherwise  a capability is created and attached to the process additional references use the capability to demonstrate swiftly that access is allowed after the last access  the capability is destroyed  this strategy is used in the multics system and in the cal system  as an example of how such a strategy works  consider a file system in which each file has an associated access list when a process opens a file  the directory structure is searched to find the file  access permission is checked  and buffers are allocated all this information is recorded in a new entry in a file table associated with the process the operation returns an index into this table for the newly opened file all operations on the file are made by specification of the index into the file table the entry in the file table then points to the file and its buffers when the file is closed  the file-table entry is deleted since the file table is maintained by the operating system  the user carmot accidentally corrupt it thus  the user can access only those files that have been opened  14.6 14.6 605 since access is checked when the file is opened  protection is ensured this strategy is used in the unix system  the right to access must still be checked or1 each access  and the file-table entry has a capability only for the allowed operations if a file is opened for reading  then a capability for read access is placed in the file-table entry if an attempt is made to write onto the file  the system identifies this protection violation by com.paring the requested operation with the capability in the file-table entry  in section 10.6.2  we described how access controls can be used on files within a file system each file and directory are assigned an owner  a group  or possibly a list of users  and for each of those entities  access-control information is assigned a similar function can be added to other aspects of a computer system a good example of this is found in solaris 10  solaris 10 advances the protection available in the sun microsystems operating system by explicitly adding the principle of least privilege via this facility revolves around privileges  a privilege is the right to execute a system call or to use an option within that system call  such as opening a file with write access   privileges can be assigned to processes,limiting them to exactly the access they need to perform their work privileges and programs can also be assigned to users are assigned roles or can take roles based on passwords to the roles in this way a user can take a role that enables a privilege  allowing the user to run a program to accomplish a specific task  as depicted in figure 14.8 this implementation of privileges decreases the security risk associated with superusers and setuid programs  user1 executes with role 1 privileges ~ figure 14.8 role-based access control in solaris 10  606 chapter 14 14.7 notice that this facility is similar to the access matrix described in section 14.4 this relationship is further explored in the exercises at the end of the chapter  in a dynamic protection system  we may sometimes need to revoke access rights to objects shared by different users various questions about revocation may arise  immediate versus delayed does revocation occur immediately  or is it delayed if revocation is delayed  can we find out when it will take place selective versus general when an access right to an object is revoked  does it affect all the users who have an access right to that object  or can we specify a select group of users whose access rights should be revoked partial versus total can a subset of the rights associated with an object be revoked  or must we revoke all access rights for this object temporary versus permanent can access be revoked permanently  that is  the revoked access right will never again be available   or can access be revoked and later be obtained again with an access-list scheme  revocation is easy the access list is searched for any access rights to be revoked  and they are deleted from the list revocation is immediate and can be general or selective  total or partial  and permanent or temporary  capabilities  howeve1 ~ present a much more difficult revocation problem  as mentioned earlier since the capabilities are distributed throughout the system  we must find them before we can revoke them schemes that implement revocation for capabilities include the following  reacquisition periodically  capabilities are deleted from each domain if a process wants to use a capability  it may find that that capability has been deleted the process may then try to reacquire the capability if access has been revoked  the process will not be able to reacquire the capability  back-pointers a list of pointers is maintained with each object  pointing to all capabilities associated with that object when revocation is required  we can follow these pointers  changing the capabilities as necessary this scheme was adopted in the multics system it is quite general  but its implementation is costly  indirection the capabilities point indirectly  not directly  to the objects  each capability points to a unique entry in a global table  which in turn points to the object we implement revocation by searching the global table for the desired entry and deleting it then  when an access is attempted  the capability is found to point to an illegal table entry table entries can be reused for other capabilities without difficulty  since both the capability and the table entry contain the unique name of the object the object for a 14.8 14.8 607 capability and its table entry must match this scheme was adopted in the cal system it does not allow selective revocation  keys a key is a unique bit pattern that can be associated with a capability  this key is defined when the capability is created  and it can be neither modified nor inspected by the process that owns the capability a is associated with each object ; it can be defined or replaced with the set-key operation when a capability is created  the current value of the master key is associated with the capability when the capability is exercised  its key is compared with the master key if the keys match  the operation is allowed to continue ; otherwise  an exception condition is raised revocation replaces the master key with a new value via the set-key operation  invalidating all previous capabilities for this object  this scheme does not allow selective revocation  since only one master key is associated with each object if we associate a list of keys with each object  then selective revocation can be implemented finally  we can group all keys into one global table of keys a capability is valid only if its key matches some key in the global table we implement revocation by removing the matching key from the table with this scheme  a key can be associated with several objects  and several keys can be associated with each object  providing maximum flexibility  in key-based schemes  the operations of defining keys  inserting them into lists  and deleting them from lists should not be available to all users  in particular  it would be reasonable to allow only the owner of an object to set the keys for that object this choice  however  is a policy decision that the protection system can implement but should not define  in this section  we survey two capability-based protection systems these systems differ in their complexity and in the types of policies that can be implemented on them neither system is widely used  but both provide interesting proving grounds for protection theories  14.8.1 an example  hydra hydra is a capability-based protection system that provides considerable flexibility the system implements a fixed set of possible access rights  including such basic forms of access as the right to read  write  or execute a memory segment in addition  a user  of the protection system  can declare other rights  the interpretation of user-defined rights is performed solely by the user 's program  but the system provides access protection for the use of these rights  as well as for the use of system-defined rights these facilities constitute a significant development in protection technology  operations on objects are defined procedurally the procedures that implement such operations are themselves a form of object  and they are accessed indirectly by capabilities the names of user-defined procedures must be identified to the protection system if it is to deal with objects of the userdefined type when the definition of an object is made krtown to hydra  the names of operations on the type become auxiliary rights 608 chapter 14 can be described in a capability for an instance of the type for a process to perform an operation on a typed object  the capability it holds for that object must contain the name of the operation being invoked among its auxiliary rights this restriction enables discrin lination of access rights to be made on an instance-by-instance and process-by-process basis  hydra also provides   ' ! fnrwk ;  j1o  l this scheme allows a procedure to be certified as to act on a formal parameter of a specified type on behalf of any process that holds a right to execute the procedure the rights held by a trustworthy procedure are independent oc and may exceed  the rights held by the calling process however  such a procedure must not be regarded as universally trustworthy  the procedure is not allowed to act on other types  for instance   and the trustworthiness must not be extended to any other procedures or program segments that might be executed by a process  amplification allows implementation procedures access to the representation variables of an abstract data type if a process holds a capability to a typed object a  for instance  this capability may include an auxiliary right to invoke some operation p but does not include any of the so-called kernel rights  such as read  write  or execute  on the segment that represents a such a capability gives a process a means of indirect access  through the operation p  to the representation of a  but only for specific purposes  when a process invokes the operation p on an object a  howeve1 ~ the capability for access to a may be amplified as control passes to the code body of p this amplification may be necessary to allow p the right to access the storage segment representing a so as to implement the operation that p defines on the abstract data type the code body of p may be allowed to read or to write to the segment of a directly  even though the calling process cmmot  on return from p the capability for a is restored to its originat unamplified state this case is a typical one in which the rights held by a process for access to a protected segment must change dynamically  depending on the task to be performed the dynamic adjustment of rights is performed to guarantee consistency of a programmer-defined abstraction amplification of rights can be stated explicitly in the declaration of an abstract type to the hydra operating system  when a user passes an object as an argument to a procedure  we may need to ensure that the procedure can not modify the object we can implement this restriction readily by passing an access right that does not have the modification  write  right howeve1 ~ if amplification may occur  the right to modify may be reinstated thus  the user-protection requirement can be circumvented  in generat of course  a user may trust that a procedure performs its task correctly this assumption is not always correct however  because of hardware or software errors hydra solves this problem by restricting amplifications  the procedure-call mechanism of hydra was designed as a direct solution to the problem of mutually suspicious subsystems this problem is defined as follows suppose that a program is provided that can be invoked as a service by a number of different users  for example  a sort routine  a compile1 ~ a game   when users invoke this service program  they take the risk that the program will malfunction and will either damage the given data or retain some access right to the data to be used  without authority  later similarly  the service program may have som.e private files  for accounting purposes  14.8 609 for example  that should not be accessed directly by the calling user program  hydra provides mechanisms for directly dealing with this problem  a hydra subsystem is built on top of its protection kernel and may require protection of its own components a subsystem interacts with the kernel through calls on a set of kernel-defined primitives that define access rights to resources defined by the subsystenl the subsystem designer can define policies for use of these resources by user processes  but the policies are enforceable by use of the standard access protection afforded by the capability system  programmers can make direct use of the protection system after acquainting themselves with its features in the appropriate reference rnanual hydra provides a large library of system-defined procedures that can be called by user programs programmers can explicitly incorporate calls on these system procedures into their program code or can use a program translator that has been interfaced to hydra  14.8.2 an example  cambridge cap system a different approach to capability-based protection has been taken in the design of the cambridge cap system cap 's capability system is simpler and superficially less powerful than that of hydra however  closer examination shows that it  too  can be used to provide secure protection of user-defined objects cap has two kinds of capabilities the ordinary kind is called a  it can be used to provide access to objects  but the only rights provided are the standard read  write  and execute of the individual storage segments associated with the object data capabilities are interpreted by microcode in the cap machine  the second kind of capability is the so-called which is protected  but not interpreted  by the cap microcode it is interpreted by a protected  that is  privileged  procedure  which may be written by an application programmer as part of a subsystem a particular kind of rights amplification is associated with a protected procedure when executing the code body of such a procedure  a process temporarily acquires the right to read or write the contents of a software capability itself this specific kind of rights amplification corresponds to an implementation of the seal and unseal primitives on capabilities of course  this privilege is still subject to type verification to ensure that only software capabilities for a specified abstract type are passed to any such procedure universal trust is not placed in any code other than the cap machine 's microcode  see bibliographical notes for references  the interpretation of a software capability is left completely to the subsystem  through the protected procedures it contains this scheme allows a variety of protection policies to be implemented although programmers can define their own protected procedures  any of which might be incorrect   the security of the overall system can not be compromised the basic protection system will not allow an unverified  user-defined  protected procedure access to any storage segments  or capabilities  that do not belong to the protection environment in which it resides the most serious consequence of an insecure protected procedure is a protection breakdown of the subsystem for which that procedure has responsibility  610 chapter 14 14.9 the designers of the cap system have noted that the use of software capabilities allowed them to realize considerable economies in formulating and implementing protection policies commensurate with the requirements of abstract resources however  subsystem designers who want to make use of this facility can not simply study a reference manual  as is the case with hydra  instead  they must learn the principles and techniques of protection  since the system provides them with no library of procedures  to the degree that protection is provided in existing computer systems  it is usually achieved through an operating-system kernel  which acts as a security agent to inspect and validate each attempt to access a protected resource since comprehensive access validation may be a source of considerable overhead  either we must give it hardware support to reduce the cost of each validation or we must allow the system designer to compromise the goals of protection  satisfying all these goals is difficult if the flexibility to implement protection policies is restricted by the support mechanisms provided or if protection environments are made larger than necessary to secure greater operational efficiency  as operating systems have become more complex  and particularly as they have attempted to provide higher-level user interfaces  the goals of protection have become much more refined the designers of protection systems have drawn heavily on ideas that originated in programming languages and especially on the concepts of abstract data types and objects protection systems are now concerned not only with the identity of a resource to which access is attempted but also with the functional nature of that access in the newest protection systems  concern for the function to be invoked extends beyond a set of system-defined functions  such as standard file-access methods  to include functions that may be user-defined as well  policies for resource use may also vary  depending on the application  and they may be subject to change over time for these reasons  protection can no longer be considered a matter of concern only to the designer of an operating system it should also be available as a tool for use by the application designe1 ~ so that resources of an applications subsystem can be guarded against tampering or the influence of an error  14.9.1 compiler-based enforcement at this point  programming languages enter the picture specifying the desired control of access to a shared resource in a system is making a declarative statement about the resource this kind of statement can be integrated into a language by an extension of its typing facility when protection is declared along with data typing  the designer of each subsystem can specify its requirements for protection  as well as its need for use of other resources in a system such a specification should be given directly as a program is composed  and in the language in which the program itself is stated this approach has several significant advantages  14.9 611 protection needs are simply declared  rather than programmed as a sequence of calls on procedures of an operating system  protection requirements can be stated independently of the facilities provided by a particular operating system  the means for enforcement need not be provided by the designer of a subsystem  a declarative notation is natural because access privileges are closely related to the linguistic concept of data type  a variety of techniques can be provided by a programming-language implementation to enforce protection  but any of these must depend on some degree of support from an underlying machine and its operating system for example  suppose a language is used to generate code to run on the cambridge cap system on this system  every storage reference made on the underlying hardware occurs indirectly through a capability this restriction prevents any process from accessing a resource outside of its protection environment at any time however  a program may impose arbitrary restrictions on how a resource can be used during execution of a particular code segment  we can implement such restrictions most readily by usin.g the software capabilities provided by cap a language implementation might provide standard protected procedures to interpret software capabilities that would realize the protection policies that could be specified in the language this scheme puts policy specification at the disposal of the programmers  while freeing them from implementing its enforcement  even if a system does not provide a protection kernel as powerful as those of hydra or cap  mechanisms are still available for implementing protection specifications given in a programming language the principal distinction is that the security of this protection will not be as great as that supported by a protection kernel  because the mechanism must rely on more assumptions about the operational state of the system a compiler can separate references for which it can certify that no protection violation could occur from those for which a violation might be possible  and it can treat them differently the security provided by this form of protection rests on the assumption that the code generated by the compiler will not be modified prior to or during its execution  what  then  are the relative merits of enforcement based solely on a kernel  as opposed to enforcement provided largely by a compiler security enforcement by a kernel provides a greater degree of security of the protection system itself than does the generation of protectionchecking code by a compiler in a compiler-supported scheme  security rests on correctness of the translator  on some underlying mechanism of storage management that protects the segments from which compiled code is executed  and  ultimately  on the security of files from which a program is loaded some of these considerations also apply to a softwaresupported protection kernel  but to a lesser degree  since the kernel may reside in fixed physical storage segments and may be loaded only from a designated file with a tagged-capability system  in which all address 612 chapter 14 computation is performed either by hardware or by a fixed microprogram  even greater security is possible hardware-supported protection is also relatively immune to protection violations that might occur as a result of either hardware or system software malfunction  flexibility there are limits to the flexibility of a protection kernel in implementing a user-defined policy  although it may supply adequate facilities for the system to provide enforcement of its own policies  with a programming language  protection policy can be declared and enforcem.ent provided as needed by an implementation if a language does not provide sufficient flexibility  it can be extended or replaced with less disturbance of a system in service than would be caused by the modification of an operating-system kernel efficiency the greatest efficiency is obtained when enforcement of protection is supported directly by hardware  or microcode   insofar as software support is required  language-based enforcement has the advantage that static access enforcement can be verified off-line at compile time also  since an intelligent compiler can tailor the enforcement mechanism to meet the specified need  the fixed overhead of kernel calls can often be avoided  in summary  the specification of protection in a programming language allows the high-level description of policies for the allocation and use of resources a language implementation can provide software for protection enforcement when automatic hardware-supported checking is unavailable in addition  it can interpret protection specifications to generate calls on whatever protection system is provided by the hardware and the operating system  one way of making protection available to the application program is through the use of a software capability that could be used as an object of computation inherent in this concept is the idea that certain program components might have the privilege of creating or examining these software capabilities a capability-creating program would be able to execute a primitive operation that would seal a data structure  rendering the latter 's contents inaccessible to any program components that did not hold either the seal or the unseal privilege such components might copy the data structure or pass its address to other program components  but they could not gain access to its contents the reason for introducing such software capabilities is to bring a protection mechanism into the programming language the only problem with the concept as proposed is that the use of the seal and unseal operations takes a procedural approach to specifying protection a nonprocedural or declarative notation seems a preferable way to make protection available to the application programmer  what is needed is a safe  dynamic access-control mechanism for distributing capabilities to system resources among user processes to contribute to the overall reliability of a system  the access-control mechanism should be safe to use to be useful in practice  it should also be reasonably efficient this requirement has led to the development of a number of language constructs that allow the programmer to declare various restrictions on the use of a specific managed resource  see the bibliographical notes for appropriate references  these constructs provide mechanisms for three functions  14.9 613 distributing capabilities safely and efficiently among customer processes  in particular  mechanisms ensure that a user process will use the managed resource only if it was granted a capability to that resource  specifying the type of operations that a particular process may invoke on an allocated resource  for example  a reader of a file should be allowed only to read the file  whereas a writer should be able both to read and to write   it should not be necessary to grant the same set of rights to every user process  and it should be impossible for a process to enlarge its set of access rights  except with the authorization of the access-control mechanism  specifying the order in which a particular process may invoke the various operations of a resource  for example  a file must be opened before it can be read   it should be possible to give two processes different restrictions on the order in which they can invoke the operations of the allocated resource  the incorporation of protection concepts into programming languages  as a practical tool for system design  is in its infancy protection will likely become a matter of greater concern to the designers of new systems with distributed architectures and increasingly stringent requirements on data security then the importance of suitable language notations in which to express protection requirements will be recognized more widely  14.9.2 protection in java because java was designed to run in a distributed environment  the java virtual machine-or jvm-has many built-in protection mechanisms java programs are composed of each of which is a collection of data fields and functions  called that operate on those fields the jvm loads a class in response to a request to create instances  or objects  of that class one of the most novel and useful features ofj ava is its support for dynamically loading untrusted classes over a network and for executing mutually distrusting classes within the same jvm  because of these capabilities of java  protection is a paramount concern  classes running in the same jvm may be from different sources and may not be equally trusted as a result  enforcing protection at the granularity of the jvm process is insufficient intuitively  whether a request to open a file should be allowed will generally depend on which class has requested the open the operating system lacks this knowledge  thus  such protection decisions are handled within the jvm when the jvm loads a class  it assigns the class to a protection domain that gives the permissions of that class the protection domain to which the class is assigned depends on the url from which the class was loaded and any digital signatures on the class file  digital signatures are covered in section 15.4.1.3  a configurable policy file determines the permissions granted to the domain  and its classes   for example  classes loaded from a trusted server might be placed in a protection domain that allows them to access files in the user 's home directory  whereas classes loaded from an untrusted server might have no file access permissions at all  614 chapter 14 it can be complicated for the jvm to determine what class is responsible for a request to access a protected resource accesses are often performed indirectly  through system libraries or other classes for example  consider a class that is not allowed to open network connections it could call a system library to request the load of the contents of a url the jvm must decide whether or not to open a network connection for this request but which class should be used to determine if the connection should be allowed  the application or the system library the philosophy adopted in java is to require the library class to explicitly permit a network corucection more generally  in order to access a protected resource  some method in the calling sequence that resulted in the request must explicitly assert the privilege to access the resource by doing so  this method takes responsibility for the request ; presumably  it will also perform whatever checks are necessary to ensure the safety of the request of course  not every method is allowed to assert a privilege ; a method can assert a privilege only if its class is in a protection domain that is itself allowed to exercise the privilege  this implementation approach is called every thread in the jvm has an associated stack of its ongoing invocations when a caller may not be trusted  a method executes an access request within a dopri vileged block to perform the access to a protected resource directly or indirectly dopri vileged   is a static method in the accesscontroller class that is passed a class with a run   method to invoke when the dopri vileged block is entered  the stack frame for this method is annotated to indicate this fact then  the contents of the block are executed when an access to a protected resource is subsequently requested  either by this method or a method it calls  a call to checkpermissions   is used to invoke stack inspection to determine if the request should be allowed the inspection examines stack frames on the calling thread 's stack  starting from the most recently added frame and working toward the oldest if a stack frame is first found that has the dopri vileged   annotation  then checkpermissions   returns immediately and silently  allowing the access if a stack frame is first found for which access is disallowed based on the protection domain of the method 's class  then checkpermissions   throws an accesscontrolexception if the stack inspection exhausts the stack without finding either type of frame  then whether access is allowed depends on the implementation  for example  some implementations of the jvm may allow access  while other implementations may disallow it   stack inspection is illustrated in figure 14.9 here  the gui   method of a class in the untrusted applet protection domain performs two operations  first a get   and then an open    the former is an invocation of the get   method of a class in the url loader protection domain  which is permitted to open   sessions to sites in the lucent com domain  in particular a proxy server proxy .lucent com for retrieving urls for this reason  the untrusted applet 's get   invocation will succeed  the checkpermissions   call in the networking library encounters the stack frame of the get   method  which performed its open   in a dopri vileged block however  the untrusted applet 's open   invocation will result in an exception  because the checkpermissions   call finds no dopri vileged annotation before encountering the stack frame of the gui   method  14.10 protection domain  socket permission  class  none gui  get  uri  ; open  addr  ; 14.10 .lucent.com  80  connect get  url u   doprivileged  open  'proxy.lucent.com  80 '  ;  request u from proxy figure 14.9 stack inspection  615 any open  addr a   checkpermission  a  connect  ; connect  a  ; of course  for stack inspection to work  a program must be unable to modify the annotations on its own stack frame or to do other manipulations of stack inspection this is one of the most important differences between java and many other languages  including c + +   a java program can not directly access memory ; it can manipulate only an object for which it has a reference references can not be forged  and the manipulations are made only through well-defined interfaces compliance is enforced through a sophisticated collection of load-time and run-time checks as a result  an object can not manipulate its run-time stack  because it camlot get a reference to the stack or other components of the protection system  more generally  java 's load-time and run-time checks enforce of java classes type safety ensures that classes can not treat integers as pointers  write past the end of an array  or otherwise access memory in arbitrary ways  rather  a program can access an object only via the methods defined on that object by its class this is the f01mdation of java protection  since it enables a class to effectively  and protect its data and methods from other classes loaded in the same jvm for example  a variable can be defined as private so that only the class that contains it can access it or protected so that it can be accessed only by the class that contains it  subclasses of that class  or classes in the same package type safety ensures that these restrictions can be enforced  computer systems contain many objects  and they need to be protected from misuse objects may be hardware  such as memory  cpu time  and i/0 devices  or software  such as files  programs  and semaphores   an access right is permission to perform an operation on an object a domain is a set of access rights processes execute in domains and may use any of the access rights in the domain to access and manipulate objects during its lifetime  a process may be either bound to a protection domain or allowed to switch from one domain to another  616 chapter 14 the access matrix is a general model of protection that provides a mechanisnc for protection without imposing a particular protection policy on the system or its users the separation of policy and mechanism is an important design property  the access matrix is sparse it is normally implemented either as access lists associated with each object or as capability lists associated with each domain  we can include dynamic protection in the access-matrix model by considering domains and the access matrix itself as objects revocation of access rights in a dynamic protection model is typically easier to implement with an access-list scheme than with a capability list  real systems are much more limited than the general model and tend to provide protection only for files unix is representative  providing read  write  and execution protection separately for the owner  group  and general public for each file multics uses a ring structure in addition to file access hydra  the cambridge cap system  and mach are capability systems that extend protection to user-defined software objects solaris 10 implements the principle of least privilege via role-based access controt a form of the access matrix  language-based protection provides finer-grained arbitration of requests and privileges than the operating system is able to provide for example  a single java jvm can run several threads  each in a different protection class it enforces the resource requests through sophisticated stack inspection and via the type safety of the language  14.1 consider a computer system in which computer games can be played by students only between 10 p.m and 6 a.m  by faculty members between 5 p.m and 8 a.m  and by the computer center staff at all times suggest a scheme for implementing this policy efficiently  14.2 the rc 4000 system  among others  has defined a tree of processes  called a process tree  such that all the descendants of a process can be given resources  objects  and access rights by their ancestors only thus  a descendant can never have the ability to do anything that its ancestors can not do the root of the tree is the operating system  which has the ability to do anything assume the set of access rights is represented by an access matrix  a a  x,y  defines the access rights of process x to object y if xis a descendant of z  what is the relationship between a  x,y  and a  z,y  for an arbitrary object y 14.3 how are the access-matrix facility and the role-based access-control facility similar how do they differ 14.4 discuss the need for rights amplification in hydra how does this practice compare with the cross-ring calls in a ring-protection scheme 617 14.5 explain why a capability-based system such as hydra provides greater flexibility than the ring-protection scheme in enforcing protection policies  14.6 consider the ring-protection scheme in multics if we were to implement the system calls of a typical operating system and store them in a segment associated with ring 0  what should be the values stored in the ring field of the segment descriptor what happens during a system call when a process executing in a higher-numbered ring invokes a procedure in ring 0 14.7 discuss the strengths and weaknesses of implementing an access matrix using capabilities that are associated with domains  14.8 discuss the strengths and weaknesses of implementing an access matrix using access lists that are associated with objects  14.9 the access-control matrix can be used to determine whether a process can switch from  say  domain a to domain b and enjoy the access privileges of domain b is this approach equivalent to including the access privileges of domain b in those of domain a 14.10 how can systems that implement the principle of least privilege still have protection failures that lead to security violations 14.11 how does the principle of least privilege aid in the creation of protection systems 14.12 what protection problems may arise if a shared stack is used for parameter passing 14.13 if all the access rights to an object are deleted  the object can no longer be accessed at this point the object should also be deleted  and the space it occupies should be returned to the system suggest an efficient implementation of this scheme  14.14 discuss which of the following systems allow module designers to enforce the need-to-know principle  a the multics ring-protection scheme b hydra 's capabilities c jvm 's stack-inspection scheme 618 chapter 14 14.15 consider a computing environment where a unique number is associated with each process and each object in the system suppose that we allow a process with number n to access an object with number m only if n m what type of protection structure do we have 14.16 what is the need-to-know principle why is it important for a protection system to adhere to this principle 14.17 what hardware features does a computer system need for efficient capability manipulation can these features be used for memory protection 14.18 describe how the java protection model would be compromised if a java program were allowed to directly alter the annotations of its stack frame  14.19 a burroughs b7000/b6000 mcp file can be tagged as sensitive data  when such a file is deleted  its storage area is overwritten by some random bits for what purpose would such a scheme be useful the access-matrix model of protection between domains and objects was developed by lampson  1969  and lampson  1971   popek  1974  and saltzer and schroeder  1975  provided excellent surveys on the subject of protection  harrison et al  1976  used a formal version of this model to enable them to prove properties of a protection system mathematically  the concept of a capability evolved from iliffe 's and jodeit 's codewords  which were implemented in the rice university computer  iliffe and jodeit  1962    the term capability was introduced by dennis and horn  1966   the hydra system was described by wulf et al  1981   the cap system was described by needham and walker  1977   organick  1972  discussed the multics ring-protection system  revocation was discussed by redell and fabry  1974   cohen and jefferson  1975   and ekanadham and bernstein  1979   the principle of separation of policy and mechanism was advocated by the designer of hydra  levin et al   1975    the confinement problem was first discussed by lampson  1973  and was further examined by lipner  1975   the use of higher-level languages for specifying access control was suggested first by morris  1973   who proposed the use of the seal and unseal operations discussed in section 14.9 kieburtz and silberschatz  1978   kieburtz and silberschatz  1983   and mcgraw and andrews  1979  proposed various language constructs for dealing with general dynamic-resource-management schemes jones and liskov  1978  considered how a static access-control scheme can be incorporated in a programming language that supports abstract data types the use of minimal operating-system support to enforce protection was advocated by the exokernel project  ganger et al  2002   kaashoek et al  1997    15.1 protection  as we discussed in chapter 14  is strictly an internal problem  how do we provide controlled access to programs and data stored in a computer system on the other hand  requires not only an adequate protection system but also consideration of the external environment within which the system operates a protection system is ineffective if user authentication is compromised or a program is run by an unauthorized user  computer resources must be guarded against unauthorized access  malicious destruction or alteration  and accidental introduction of inconsistency  these resources include information stored in the system  both data and code   as well as the cpu  memory  disks  tapes  and networking that are the computer  in this chapter  we start by examining ways in which resources may be accidentally or purposely misused we then explore a key security enabler -cryptography finally  we look at mechanisms to guard against or detect attacks  to discuss security threats and attacks  to explain the fundamentals of encryption  authentication  and hashing  to examine the uses of cryptography in computing  to describe various countermeasures to security attacks  in many applications  ensuring the security of the computer system is worth considerable effort large commercial systems containing payroll or other financial data are inviting targets to thieves systems that contain data pertaining to corporate operations may be of interest to unscrupulous competitors  furthermore  loss of such data  whether by accident or fraud  can seriously impair the ability of the corporation to function  in chapter 14  we discussed mechanisms that the operating system can provide  with appropriate aid from the hardware  that allow users to protect 621 622 chapter 15 their resources  including programs and data these mechanisms work well only as long as the users conform to the intended use of and access to these resources we say that a system is if its resources are used and accessed as intended under all circumstances unfortunately total security can not be achieved nonetheless  we must have mechanisms to make security breaches a rare occurrence  rather than the norm  security violations  or misuse  of the system can be categorized as intentional  malicious  or accidental it is easier to protect against accidental misuse than against malicious misuse for the most part protection mechanisms are the core of protection from accidents the following list includes several forms of accidental and malicious security violations we should note that in our discussion of security  we use the terms intruder and cracker for those attempting to breach security in addition  a is the potential for a security violation  such as the discovery of a vulnerability  whereas an is the attempt to break security  breach of confidentiality this type of violation involves 1mauthorized reading of data  or theft of information   typically  a breach of confidentiality is the goal of an intruder capturing secret data from a system or a data stream  such as credit-card information or identity information for identity theft  can result directly in money for the intruder  breach of integrity this violation involves unauthorized modification of data such attacks can  for example  result in passing of liability to an innocent party or modification of the source code of an important commercial application  breach of availability this violation involves unauthorized destruction of data some crackers would rather wreak havoc and gain status or bragging rights than gain financially web-site defacement is a common example of this type of security breach  theft of service this violation involves unauthorized use of resources  for example  an intruder  or intrusion program  may install a daemon on a system that acts as a file server  denial of service this violation involves preventing legitimate use of the system or attacks are sometimes accidental the original internet worm turned into a dos attack when a bug failed to delay its rapid spread we discuss dos attacks further in section 15.3.3  attackers use several standard methods in their attempts to breach security the most common is in which one participant in a communication pretends to be someone  another host or another person   by masquerading  attackers breach the correctness of identification ; they can then gain access that they would not normally be allowed or escalate their privileges-obtain privileges to which they would not normally be entitled another common attack is to replay a captured exchange of data a consists of the malicious or fraudulent repeat of a valid data transmission sometimes the replay comprises the entire attackfor example  in a repeat of a to transfer money but frequently it is done along with again to escalate privileges consider 15.1 623 normal attacker masquerading attacker man-in-the-middle attacker figure 15.1 standard security attacks  the damage that could be done if a request for authentication had a legitimate user 's information with an unauthorized user 's yet another kind of attack is the in which an attacker sits in the data flow of a communication  masquerading as the sender to the receiver  and vice versa in a network communication  a man-in-the-middle attack may be preceded by a in which an active communication session is intercepted several attack methods are depicted in figure 15.1  as we have already suggested  absolute protection of the system from malicious abuse is not possible  but the cost to the perpetrator can be made sufficiently high to deter most intruders in some cases  such as a denial-ofservice attack  it is preferable to prevent the attack but sufficient to detect the attack so that cmmtermeasures can be taken  to protect a system  we must take security measures at four levels  physical the site or sites containing the computer systems must be physically secured against armed or surreptitious entry by intruders  both the machine rooms and the terminals or workstations that have access to the machines must be secured  624 chapter 15 must be done carefully to assure that only access to the system even authorized users/ to let others use their access  in exchange they may also be tricked into allowing one type of social-engineering attack here  a legitimate-looking e-mail or web page misleads a user into entering confidential information another teclucique is human authorization appropriate users have however  may be for a bribe  for access via is a general term for attempting to gather information in order to gain unauthorized access to the computer  by looking through trash  finding phone books  or finding notes containing passwords  for example   these security problems are management and personnel issues  not problems pertaining to operating systems  operating system the system must protect itself from accidental or purposeful security breaches a runaway process could constitute an accidental denial-of-service attack a query to a service could reveal passwords  a stack overflow could the launching of an unauthorized process list of possible breaches is almost endless  network much computer data in modern systems travels over private leased lines  shared lines like the internet  wireless connections  or dial-up lines intercepting these data could be just as harmful as breaking into a computer ; and interruption communications could constitute a remote denial-of-service attack  diminishing users ' use of and trust in the system  at the first two levels must be inaintained if operating-system is to be ensured a weakness at a high security  physical or allows circumvention of strict low-level  operating-system  security measures the old adage that a chal.'l is as weak as its weakest link is true of system security all these aspects must be addressed for to be maintained  to allow the more is as intruders countermeasures are created and deployed  this causes intruders to become more in their attacks for incidents include the use of to section tools needed to block the in the remainder of this 15 15.2 625 ways  ranging from passwords for authentication through guarding against viruses to detecting intrusions we start with an exploration of security threats  processes  along with the kernel  are the only means of accomplishing work on a computer therefore  writing a program that creates a breach of security  or causing a normal process to change its behavior and create a breach  is a common goal of crackers in fact even most nonprogram security events have as their goal causing a program threat for example  while it is useful to log in to a without authorization  it is quite a lot more useful to leave behind a daemon that provides information or allows easy access even if the original exploit is blocked in this section  we describe common methods which programs cause security breaches note that there is considerable variation in the naming conventions of security holes and that we use the most common or descriptive terms  15.2.1 horse systems have mechanisms for allowing programs written by users to be executed by other users if these programs are executed in a domain that provides the access rights of the executing user  the other users may misuse these rights a text-editor program  for example  may include code to search the file to be edited for certairl keywords if any are found  the entire file may be to a special area accessible to the creator of the text editor  a code segment that misuses its environment is called a long search paths  as are common on unix systems  exacerbate the trojanhorse the search path lists the set of directories to search when an program name is given the is searched for a file of that name  and the file is executed all the directories in such a search path must be secure  or a horse could be slipped into the user 's and executed consider the use of the  character in a search path the  to include the current directory in the search if a user has  in her search has set her current to a friend 's directory  and enters the name of a normal system commanct be executed from the friend 's instead the program would run the user 's to do anything that the user is allowed to the user 's instance  horse is a that emulates a what 626 chapter 15 such as the control-alt-delete conlbination used by all modern windows operating systems  another variation on the trojan horse is spyware sometimes accompanies a program that the user has chosen to install most frequently  it comes along with freeware or shareware programs  but sometimes it is included with commercial software the goal of spyware is to download ads to display on the user 's system  create when certain sites are visited  or capture information from the user 's system and return it to a central site this latter is an example of a general category of attacks known as in which surreptitious communication occurs for example  the installation of an innocuous-seeming program on a windows system could result in the loading of a spyware daemon the spyware could contact a central site  be given a message and a list of recipient addresses  and deliver the spam message to those users from the windows machine this process continues until the user discovers the spyware frequently  the spyware is not discovered  in 2004  it was estimated that 80 percent of spam was being delivered by this method this theft of service is not even considered a crime in most countries ! spyware is a micro example of a macro problem  violation of the principle of least privilege under most circumstances  a user of an operating system does not need to install network daemons such daemons are installed via two mistakes first  a user may run with more privileges than necessary  for example  as the administrator   allowing programs that she runs to have more access to the system than is necessary this is a case of human error-a common security weakness second  an operating system may allow by default more privileges than a normal user needs this is a case of poor operating-system design decisions an operating system  and  indeed  software in general  should allow fine-grained control of access and security  but it must also be easy to manage and understand inconvenient or inadequate security measures are bound to be circumvented  causing an overall weakening of the security they were designed to implement  15.2.2 trap door the designer of a program or system might leave a hole in the software that only he is capable of using this type of security breach  or was shown in the movie war games for instance  the code inight check a specific user id or password  and it might circumvent normal security procedures programmers have been arrested for embezzling from banks by including rounding errors in their code and having the occasional half-cent credited to their accounts  this account credititrg can add up to a large amount of money  considering the number of transactions that a large bank executes  a clever trap door could be included in a compiler the compiler could generate standard object code as well as a trap door  regardless of the source code being compiled this activity is particularly nefarious  since a search of the source code of the program will not reveal any problems only the source code of the compiler would contain the information  trap doors pose a difficult problem because  to detect them  we have to analyze all the source code for all components of a system given that software systems may consist of millions of lines of code  this analysis is not done frequently  and frequently it is not done at all ! 15.2 627 15.2.3 logic bomb consider a program that initiates a security incident only under certain circltmstances it would be hard to detect because under normal operations  there would be no security hole however  when a predefined set of parameters were met  the security hole would be created this scenario is known as a a programmer  for example  might write code to detect whether was still employed ; if that check failed  a daemon could be spawned to allow remote access  or code could be launched to cause damage to the site  15.2.4 stack and buffer overflow the stack or buffer-overflow attack is the most common way for an attacker outside the system  on a network or dial-up connection  to gain unauthorized access to the target system an authorized user of the system may also use this exploit for privilege escalation  essentially  the attack exploits a bug in a program the bug can be a simple case of poor programming  in which the programmer neglected to code bounds checking on an input field in this case  the attacker sends more data than the program was expecting by using trial and error  or by examining the source code of the attacked program if it is available  the attacker determines the vulnerability and writes a program to do the following  overflow an input field  command-line argument  or input buffer-for example  on a network daemon-wl.til it writes into the stack  overwrite the current return address on the stack with the address of the exploit code loaded in step 3  write a simple set of code for the next space in the stack that includes the commands that the attacker wishes to execute-for instance  spawn a shell  the result of this attack program 's execution will be a root shell or other privileged command execution  for instance  if a web-page form expects a user name to be entered into a field  the attacker could send the user name  plus extra characters to overflow the buffer and reach the stack  plus a new return address to load onto the stack  plus the code the attacker wants to run when the buffer-reading subroutine returns from execution  the return address is the exploit code  and the code is run  let 's look at a buffer-overflow exploit in more detail consider the simple c program in fig1-1re 15.2 this program creates a character array size buffer_size and copies the contents of the parameter provided on the command line-argv  1   as long as the size of this parameter is less than buffer_size  we need one byte to store the null terminator   this program works properly but consider what happens if the parameter provided on the command line is longer than buffer_size in this scenario  the strcpy   function will begin copying from argv  1  until it encounters a null terminator  \ 0  or until the program crashes thus  this program suffers from a potential problem in which copied data overflow the buffer array  628 chapter 15 # include stdio.h # define buffer_size 256 int main  int argc  char argv      char buffer  buffer_size  ; if  argc 2  return -1 ; else  strcpy  buffer,argv  1   ; return 0 ;  figure 15.2 c program with buffer-overflow condition  note that a careful programmer could have performed bounds checking on the size of argv  1  by using the strncpy   function rather than strcpy    replacing the line strcpy  buffer  argv  1   ; with strncpy  buffer  argv  1   sizeof  buffer  -1  ; .unfortunately  good bounds checking is the exception rather than the norm  furthermore  lack of bounds checking is not the only possible cause of the behavior of the program in figure 15.2 the program could instead have been carefully designed to compromise the integrity of the system we now consider the possible security vulnerabilities of a buffer overflow  when a function is invoked in a typical computer architecture  the variables defined locally to the function  sometimes known as automatic variables   the parameters passed to the function  and the address to which control returns once the function exits are stored in a stack frame the layout for a typical stack frame is shown in figure 15.3 examining the stack frame from top to bottom  we first see the parameters passed to the function  followed by any automatic variables declared in the function we next see the frame pointer  which is the address of the beginning of the stack frame finally  we have the return bottom ~ frame pointer grows top figure 15.3 the layout for a typical stack frame  15.2 629 address  which specifies where to return control once the function exits the frame pointer must be saved on the stack  as the value of the stack pointer can vary during the function call ; the saved frame pointer allows relative access to parameters and automatic variables  given this standard memory layout  a cracker could execute a bufferoverflow attack i  ter goal is to replace the return address in the stack frame so that it now points to the code segment containing the attacking program  the programmer first writes a short code segment such as the following  # include stdio.h int main  int argc  char argv     execvp  ' ' \ bin \ sh' '  ' ' \ bin \ sh' '  null  ; return 0 ; using the execvp   system call  this code segment creates a shell process  if the program being attacked runs with system-wide permissions  this newly created shell will gain complete access to the system of course  the code segment could do anything allowed by the privileges of the attacked process  this code segment is then compiled so that the assembly language instructions can be modified the primary modification is to remove unnecessary features in the code  thereby reducing the code size so that it can fit into a stack frame  this assembled code fragment is now a binary sequence that will be at the heart of the attack  refer again to the program shown in figure 15.2 let 's assume that when the main   function is called in that program  the stack frame appears as shown in figure 15.4  a   using a debugger  the programmer then finds the copied  a   b  figure 15.4 hypothetical stack frame for figure 15.2   a  before and  b  after  630 chapter 15 address of buffer  0  in the stack that address is the location of the code the attacker wants executed the binary sequence is appended with the necessary amount of no-op instructions  for no-operation  to fill the stack frame up to the location of the return address ; and the location of buffer  0   the new return address  is added the attack is complete when the attacker gives this constructed binary sequence as input to the process the process then copies the binary sequence from argv  1  to position buffer  0  in the stack frame  now  when control returns from main    instead of returning to the location specified by the old value of the return address  we return to the modified shell code  which runs with the access rights of the attacked process ! figure 15.4  b  contains the modified shell code  there are many ways to exploit potential buffer-overflow problems in this example  we considered the possibility that the program being attackedthe code shown in figure 15.2-ran with system-wide permissions however  the code segment that runs once the value of the return address has been modified might perform any type of malicious act  such as deleting files  opening network ports for further exploitation  and so on  this example buffer-overflow attack reveals that considerable knowledge and programming skill are needed to recognize exploitable code and then to exploit it unfortunately  it does not take great programmers to launch security attacks rather  one cracker can determine the bug and then write an exploit anyone with rudimentary computer skills and access to the exploita so-called then try to launch the attack at target systems  the buffer-overflow attack is especially pernicious because it can be run between systems and can travel over allowed communication channels such attacks can occur within protocols that are expected to be used to communicate with the target machine  and they can therefore be hard to detect and prevent  they can even bypass the security added by firewalls  section 15.7   one solution to this problem is for the cpu to have a feature that disallows execution of code in a stack section of memory recent versions of sun 's sparc chip include this setting  and recent versions of solaris enable it the return address of the overflowed routine can still be modified ; but when the return address is within the stack and the code there attempts to execute  an exception is generated  and the program is halted with an error  recent versions of amd and intel x86 chips include the nx feature to prevent this type of attack the use of the feature is supported in several x86 operating systems  including linux and windows xp sp2 the hardware implementation involves the use of a new bit in the page tables of the cpus this bit marks the associated page as nonexecutable  so that instructions can not be read from it and executed as this feature becomes prevalent  buffer-overflow attacks should greatly diminish  15.2.5 viruses another form of program threat is a a virus is a fragment of code embedded in a legitimate program viruses are self-replicating and are designed to infect other programs they can wreak havoc in a system by modifying or destroying files and causing system crashes and program malfunctions as with most penetration attacks  viruses are very specific to architectures  operating systems  and applications viruses are a particular problem for users of 15.2 631 pcs unix and other multiuser operating systems generally are not susceptible to viruses because the executable programs are protected from writing by the operating system even if a virus does infect such a progran  1  its powers usually are limited because other aspects of the system are protected  viruses are usually borne via e-mail  with spam the most comrnon vector  they can also spread when users download viral programs internet file-sharing services or exchange infected disks  another common form of virus transmission uses microsoft office files  such as microsoft word documents these documents can contain macros visual basic programs  that programs in the office suite powerpoint  and excel  will execute automatically because these programs run under the user 's own account  the macros can run largely unconstrained  for example  deleting user files at will   commonly  the virus will also e-mail itself to others in the user 's contact list here is a code sample that shows the simplicity of writing a visual basic macro that a virus could use to format the hard drive of a windows computer as soon as the file containing the macro was opened  sub autoopen   dim ofs set ofs = createobject  ' 'scripting.filesystemobject' '  vs = shell  ' 'c  command.com /k format c  ' ',vbhide  end sub how do viruses work once a virus reaches a target machine  a program known as a inserts the virus into the system the virus dropper is usually a trojan horse  executed for other reasons but installing the virus as its core activity once installed  the virus may do any one of a number of things there are literally thousands of viruses  but they fall into several main categories note that many viruses belong to more than one category  file a standard file virus infects a system by appending itself to a file  it changes the start of the program so that execution jumps to its code  after it executes  it returns control to the program so that its execution is not noticed file viruses are sometimes known as parasitic viruses  as they leave no full files behind and leave the host program still functional  boot a boot virus infects the boot sector of the system  executing every time the system is booted and before the operating system is loaded it watches for other boatable media  that is  floppy disks  and infects them  these viruses are also known as memory viruses  because they do not appear in the file system figure 15.5 shows how a boot virus works  macro most viruses are written in a low-levellanguage  such as assembly or c macro viruses are written in a high-level language  such as visual basic these viruses are triggered when a program capable of executing the macro is run for example  a macro virus could be contained in a spreadsheet file  source code a source code virus looks for source code and modifies it to include the virus and to help spread the virus  632 chapter 15 figure i 5.5 a boot-sector computer virus  polymorphic a polymorphic virus changes each time it is installed to avoid detection by antivirus software the changes do not affect the virus 's functionality but rather change the virus 's signature a a pattern that cili'i be used to identify a virus  typically a series make up the virus code  encrypted an encrypted virus includes decryption code along with the encrypted virus  again to avoid detection the virus first decrypts and then executes  stealth this tricky virus attempts to avoid detection by modifying parts of the system that could be used to detect it for example  it could modify the read system call so that if the file it has modified is read  the original form of the code is returned rather than the infected code  tunneling this virus attempts to bypass detection by an anti virus scanner by installing itself in the interrupt-handler chain similar viruses install themselves in device drivers  15.3 15.3 633 multipartite a virus of this type is able to infect nmltiple parts of a system  including boot sectors  memory  and files this makes it difficult to detect and contain  armored an armored virus is coded to ncake it hard for antivirus researchers to unravel and understand it can also be compressed to avoid detection and disinfection in addition  virus droppers and other full files that are part of a virus infestation are frequently hidden via file attributes or unviewable file names  this vast variety of viruses is likely to continue to grow in fact  in 2004 a new and widespread virus was detected it exploited three separate bugs for its operation this virus started by infecting hundreds of windows servers  including many trusted sites  running microsoft internet information server  iis   any vulnerable microsoft explorer web browser visiting those sites received a browser virus with any download the browser virus installed several back-door programs  including a which records all things entered on the keyboard  including and credit-card numbers   it also installed a daemon to allow unlimited remote access by an intruder and another that allowed an intruder to route spam through the infected desktop computer  generally  viruses are the most disruptive security attacks ; and because they are effective  they will continue to be written and to spread the active debates within the computing community is whether a jth'-yhcn ; _ in which many systems run the same hardware  operating system  and/ or application software  is increasing the threat of and damage caused by security intrusions this monoculture supposedly consists of microsoft products  and part of the debate concerns whether such a monoculture even exists today  program threats typically use a breakdown in the protection mechanisms of a system to attack programs in contrast  system and network threats involve the abuse of services and network comcections system and network threats create a situation in which operating-system resources and user files are inisused  sometimes a system and network attack is used to launch a program attack  and vice versa  the more an operating system is-the more services it has enabled and the more functions it allows-the more likely it is that a is available to exploit increasingly  operating systems strive to be for example  solaris 10 moved from a model in which many services  ftp  telnet  and others  were enabled by default when the system was installed to a model in which almost all services are disabled at installation time and must specifically be enabled system administrators such changes reduce the system 's set of ways in which an attacker can to break into the system  in the remainder of this section  we discuss some examples of system and network threats  including worms  port scamcing  and denial-of-service attacks it is important to note that masquerading and replay attacks are also 634 chapter 15 commonly launched over netvvorks between systems in fact  these attacks are more effective and harder to counter when multiple systems are involved  for example  within a computer  the operating system usually can determine the sender and receiver of a message even if the sender changes to the id of someone else  there may be a record of that id change when multiple systems are involved  especially systems controlled by attackers  then such tracing is much more difficult  in general  we can say that sharing secrets  to prove identity and as keys to encryption  is required for authentication and encryption  and sharing secrets is easier in environments  such as a single operating system  in which secure sharing methods exist these methods include shared memory and interprocess comnmnications creating secure communication and authentication is discussed in sections 15.4 and 15.5  15.3.1 worms a is a process that uses the mechanism to ravage system performance the worm spawns copies of itself  using up system resources and perhaps locking out all other processes on computer networks  worms are particularly potent  since they may reproduce themselves among systems and thus shut down an entire network such an event occurred in 1988 to unix systems on the internet  causing the loss of system and system-administrator time worth millions of dollars  at the close of the workday on november 2  1988  robert tappan morris  jr  a first-year cornell graduate student  unleashed a worm program on one or more hosts corm.ected to the internet targeting sun microsystems ' sun 3 workstations and vax computers running variants of version 4 bsd unix  the worm quickly spread over great distances ; within a few hours of its release  it had consumed system resources to the point of bringing down the infected machines  although robert morris designed the self-replicating program for rapid reproduction and distribution  some of the features of the unix networking environment provided the means to propagate the worm throughout the system  it is likely that morris chose for in.itial infection an internet host left open for and accessible to outside users from there  the worm program exploited flaws in the unix operating system 's security routines and took advantage of unix utilities that simplify resource sharing in local-area networks to gain unauthorized access to thousands of other connected sites morris 's methods of attack are outlined next  the worm was made up of two programs  a  also called a or program and the main program ll.c  the grappling hook consisted of 99 lines of c code compiled and run on each machine it accessed once established on the computer system under attack  the grappling hook connected to the machine where it originated and uploaded a copy of the main worm onto the hooked system  figure 15.6   the main program proceeded to search for other machines to which the newly infected system could connect easily in these actions  morris exploited the unix networking utility rsh for easy remote task execution by setting up special files that list host-login name pairs  users can omit entering a password each time they access a remote account on the paired list the worm searched these special files for site names 15.3 635 rsh attack finger attack sendmail attack worm sent target system infected system figure 15.6 the morris internet worm  that would allow remote execution without a password where remote shells were established  the worm program was uploaded and began executing anew  the attack via remote access was one of three infection methods built into the worm the other two methods involved operating-system bugs in the unix finger and sendmail programs  the finger utility functions as an electronic telephone directory ; the command finger user-name hostname returns a person 's real and login names along with other information that the user may have provided  such as office and home address and telephone number  research plan  or clever quotation finger runs as a background process  or daemon  at each bsd site and responds to queries throughout the internet the worm executed a buffer-overflow attack on finger the program queried finger with a 536-byte string crafted to exceed the buffer allocated for input and to overwrite the stack frame instead of returning to the main routine where it resided before morris 's calt the finger daemon was routed to a procedure within the invading 536-byte string now residing on the stack the new procedure executed /bin/ sh  which  if successful  gave the worm a remote shell on the machine under attack  the bug exploited in sendmail also involved using a daemon process for malicious entry sendmail sends  receives  and routes electronic mail  debugging code in the utility permits testers to verify and display the state of the ncail system the debugging option was useful to system administrators and was often left on morris included in his attack arsenal a call to debug that -instead of specifying a user address  as would be normal in testing-issued a set of cornmands that mailed and executed a copy of the grappling-hook program  once in place  the main worm systematically attempted to discover user passwords it began by trying simple cases of no password or passwords constructed of account-user-name combinations  then used comparisons with an internal dictionary of 432 favorite password choices  and then went to the 636 chapter 15 final stage of trying each word in the standard unix on-line dictionary as a possible password this elaborate and efficient three-stage password-cracking algorithm enabled the worm to gain access to other user accounts on the infected system the wontt then searched for rsh data files in these newly broken accounts and used them as described previously to gain access to user accounts on remote systems  with each new access  the worm program searched for already active copies of itself if it found one  the new copy exited  except in every seventh instance had the worm exited on all duplicate sightings  it might have remained undetected allowing every seventh duplicate to proceed  possibly to confound efforts to stop its spread baiting with fake worms  created a wholesale infestation of sun and vax systems on the internet  the very features of the unix network environment that assisted il l the worm 's propagation also helped to stop its advance ease of electronic communication  mechanisms to copy source and binary files to remote machines  and access to both source code and human expertise allowed cooperative efforts to develop solutions quickly by the evening of the next day  november 3  methods of halting the invading program were circulated to system administrators via the internet within days  specific software patches for the exploited security flaws were available  why did morris unleash the worm the action has been characterized as both a harmless prank gone awry and a serious criminal offense based on the complexity of the attack  it is unlikely that the worm 's release or the scope of its spread was unintentional the worm program took elaborate steps to cover its tracks and to repel efforts to stop its spread yet the program contained no code aimed at damaging or destroying the systems on which it ran the author clearly had the expertise to include such commands ; in fact  data structures were present in the bootstrap code that could have been used to transfer trojan-horse or virus programs the behavior of the program may lead to interesting observations  but it does not provide a sound basis for inferring motive what is not open to speculation  however  is the legal outcome  a federal court convicted morris and handed down a sentence of three years ' probation  400 hours of community service ; and a $ 10,000 fine morris 's legal costs probably exceeded $ 100,000  security experts continue to evaluate methods to decrease or eliminate worms a more recent event ; though  shows that worms are still a fact of life on the internet it also shows that as the internet grows  the damage that even harmless worms can do also grows and can be significant this example occurred during august 2003 the fifth version of the sobig worm  more properly known as w32.sobig.f @ mm  was released by persons at this time unknown it was the fastest-spreading worm released to date  at its peak mfecting hundreds of thousands of computers and one in seventeen e-mail messages on the internet it clogged e-mail inboxes  slowed networks  and took a huge number of hours to clean up  sobig.f was launched by being uploaded to a pornography newsgroup via an account created with a stolen credit card it was disguised as a photo the virus targeted microsoft windows systems and used its own smtp engine to e-mail itself to all the addresses found on an infected system it used a variety of subject lines to help avoid detection  including thank you ! your details,' ' and re  approved it also used a random address on the host as the from  15.3 637 address  making it difficult to determine from the message which machine was the infected source sobig.f included an attachment for the target e-mail reader to click on  again with a variety of names if this payload was executed  it stored a program called winppr32.exe in the default windows directory  along with a text file it also modified the windows registry  the code included in the attachment was also programmed to periodically attempt to connect to one of twenty servers and download and execute a program from them fortunately  the servers were disabled before the code could be downloaded the content of the program from these servers has not yet been determined if the code was malevolent  untold damage to a vast number of machines could have resulted  15.3.2 port scanning port scanning is not an attack but rather a means for a cracker to detect a system 's vulnerabilities to attack port scanning typically is automated  involving a tool that attempts to create a tcp lip connection to a specific port or a range of ports for example  suppose there is a known vulnerability  or bug  in sendmail a cracker could launch a port scanner to try to connect  say  to port 25 of a particular system or to a range of systems if the connection was successful  the cracker  or tool  could attempt to communicate with the answering service to determine if the service was indeed sendmail and  if so  if it was the version with the bug  now imagine a tool in which each bug of every service of every operath g system was encoded the tool could attempt to connect to every port of one or nwre systems for every service that answered  it could try to use each known bug frequently  the bugs are buffer overflows  allowing the creation of a privileged command shell on the system from there  of course  the cracker could install trojan horses  back-door programs  and so on  there is no such tool  but there are tools that perform subsets of that functionality for example  nmap  from http  / /www.insecure.org/mrtap/  is a very versatile open-source utility for network exploration and security auditing when pointed at a target  it will determine what services are n.1n..tling  including application names and versions it can identify the host operating system it can also provide information about defenses  such as what firewalls are defending the target it does not exploit any known bugs  nessus  from http  / /www.nessus.org/  performs a similar function  but it has a database of bugs and their exploits it can scan a range of systems  determine the services running on those systems  and attempt to attack all appropriate bugs it generates reports about the results it does not perform the final step of exploiting the found bugs  but a knowledgeable cracker or a script kiddie could  because port scans are detectable  section 15.6.3   they frequently are launched from such systems are previously compromised  independent systems that are serving their owners while being used for nefarious purposes  including denial-of-service attacks and spam relay zombies make crackers particularly difficult to prosecute because determining the source of the attack and the person that launched it is challenging this is one of many reasons for securing inconsequential systems  not just systems containing valuable information or services  638 chapter 15 15.3.3 denial of service as mentioned earlier  denial-of-service attacks are aimed not at gaming information or stealing resources but rather at disrupting legitimate use of a system or facility most such attacks involve systems that the attacker has not penetrated indeed  launching an attack that prevents legitimate use is frequently easier than breaking into a machine or facility  denial-of-service attacks are generally network based they fall into two categories attacks in the first category use so many facility resources that  in essence  no useful work can be done for example  a web-site click could download a java applet that proceeds to use all available cpu time or to pop up windows infinitely the second category involves disrupting the network of the facility there have been several successful denial-of-service attacks of this kind against major web sites these attacks result from abuse of some of the fundamental functionality of tcp lip for instance  if the attacker sends the part of the protocol that says i want to start a tcp connection  but never follows with the standard the connection is now complete  the result can be partially started tcp sessions if enough of these sessions are launched  they can eat up all the network resources of the system  disabling any further legitimate tcp connections such attacks  which can last hours or days  have caused partial or full failure of attempts to use the target facility the attacks are usually stopped at the network level until the operating systems can be updated to reduce their vulnerability  generally  it is impossible to prevent denial-of-service attacks the attacks use the same mechanisms as normal even more difficult to prevent and resolve are these attacks are launched from multiple sites at once  toward a common target  typically by zombies ddos attacks have become more comncon and are sometimes associated with blackmail attempts a site comes under attack  and the attackers offer to halt the attack in exchange for money  sometimes a site does not even know it is under attack it can be difficult to determine whether a system slowdown is an attack or just a surge in system use consider that a successful advertising campaign that greatly increases traffic to a site could be considered a ddos  there are other interesting aspects of dos attacks for example  if an authentication algorithm locks an account for a period of time after several incorrect attempts to access the account  then an attacker could cause all authentication to be blocked by purposely making incorrect attempts to access all accounts similarly  a firewall that automatically blocks certain kinds of traffic could be induced to block that traffic when it should not these examples suggest that programmers and systems managers need to fully understand the algorithms and technologies they are deploying finally  computer science classes are notorious sources of accidental system dos attacks consider the first programming exercises in which students learn to create subprocesses or threads a common bug involves spawning subprocesses infinitely the system 's free memory and cpu resources do n't stand a chance  there are many defenses against computer attacks  running the gamut from methodology to technology the broadest tool available to system designers 15.4 639 and users is cryptography in this section  we discuss the details of cryptography and its use in computer security  in an isolated computer  the operating system can reliably determine the sender and recipient of ali interprocess communication  since it controls all communication channels in the computer in a network of computers  the situation is quite different a networked computer receives bits from the wire with no immediate and reliable way of determining what machine or application sent those bits similarly  the computer sends bits onto the network with no of knowing who might eventually receive them  commonly  network addresses are used to infer the potential senders and receivers of network messages network packets arrive with a source address  such as an ip address and when a computer sends a message  it names the intended receiver by specifying a destination address however  for applications where security matters  we are asking for trouble if we assume that the source or destination address of a packet reliably determines who sent or received that packet a rogue computer can send a message with a falsified source address  and numerous computers other than the  one specified by the destination address can  and typically do  receive a packet for example  all of the routers on the way to the destination will receive the packet  too how  then  is an operating system to decide whether to grant a request when it can not trust the named source of the request and how is it supposed to provide protection for a request or data when it can not determine who will receive the response or message contents it sends over the network it is generally considered infeasible to build a network of any scale in which the source and destination addresses of packets can be trusted in this sense therefore  the only alternative is somehow to eliminate the need to trust the network this is the job of cryptography abstractly  ~ ' ' ~ .,-ro.rnron ~ used to constrain the potential senders and/ or receivers of a message  cryptography is based on secrets called that are selectively distributed to computers in a network and used to process messages cryptography enables a recipient of a message to verify that the message was created by some computer possessing a certain key-the key is the source of the message similarly  a sender can encode its message so that only a computer with a certain key can decode the message  so that the key becomes the destination unlike network addresses  however  keys are designed so that it is not computationally feasible to derive them from the messages they were used to generate or from any other public information thus  they provide a much more trustworthy means of constraining senders and receivers of messages note that cryptography is a field of study unto itself  with large and small complexities and subtleties  here  we explore the most important aspects of the parts of cryptography that pertain to operating systems  15.4.1 encryption because it solves a wide variety of communication security problems  is used frequently in many aspects of modern computing encryption a means for constraining the possible receivers of a message an encryption algorithm enables the sender of a message to ensure that only a computer possessing a certain key can read the message encryption of messages is an ancient practice  of course  and there have been many encryption algorithms  640 chapter 15 dating back to ancient times in this section  we describe important modern encryption principles and algorithms  figure 15.7 shows an example of two users communicating securely over an insecure channel we refer to this figure throughout the section note that the key exchange can take place directly between the two parties or via a trusted third party  that is  a certificate authority   as discussed in section 15.4.1.4  an encryption algorithm consists of the following components  a set k of keys  a set m of messages  a set c of ciphertexts  a function e  k  +  m  + c   thatis  for each k e k  e  k  is a function for generating ciphertexts from messages both e and e  lc  for any k should be efficiently computable functions  a function d  i  +  c  + m   thatis  for eachlc e i  d  k  is a function for generating messages from ciphertexts both d and d  lc  for any k should be efficiently computable functions  an encryption algorithm must provide this essential property  given a ciphertext c e c a computer can compute m such that e  lc   m  = c only if it possesses write ----1 rnessage rn 1 i ii 12  0 ~  z ~ -x ~  read 1 message ml figure i5.7 a secure communication over an insecure medium  15.4 641 d  lc   thus  a computer holding d  lc  can decrypt ciphertexts to the plaintexts used to produce them  but a computer not holding d  lc  can not decrypt ciphertexts  since ciphertexts are generally exposed  for example  sent on a network   it is important that it be infeasible to derive d  lc  from the ciphertexts  there are two main types of encryption algorithms  symmetric and asymmetric we discuss both types in the following sections  15.4.1.1 symmetric encryption in a the same key is used to encrypt and to decrypt that is  e can be from d  lc   and vice versa therefore  the secrecy of e  lc  must be protected to the same extent as that of d  lc   for the past several decades  the most commonly used symmetric encryption algorithm in the united states for civilian applications has been the adopted by the national institute of stantechxwlogy  nist   des works by taking a 64-bit value and a 56-bit key and performing a series of transformations these transformations are based on substitution and permutation operations  as is generally the case for symmetric encryption transformations some of the transformations are in that their algorithms are hidden in fact  these so-called s-boxes are classified by the united states government messages longer than 64 bits are broken into 64-bit chunks because des works on a chunk of bits at a time  is known as a cipher if the same key is used for encrypting an extended anwunt of data  it becomes vulnerable to attack  consider  for example  that the same source block would result in the same ciphertext if the same key and encryption algorithm were used therefore  the chunks are not just encrypted but also exclusive-or'ed  xored  with the ciphertext block before encryption this is known as des is now considered insecure for many applications because its keys can be exhaustively searched with moderate computing resources rather than giving up on des  though  nist created a modification called in which the des algorithm is repeated three times  two encryptions and one decryption  on the same plaintext usli'lg two or three keys-for example  c = e  k3   d  lc2   e  k1   m     when three keys are used  the effective key length is 168 bits triple des is in widespread use today  in 2001  nist a new encryption algorithm  called the to replace des aes is another symmetric block cipher it can use key lengths of 128  192  and 256 bits and works on 128-bit blocks it works by performing 10 to 14 rounds of transformations on a matrix formed from a block generally  the algorithm is compact and efficient  several other symmetric block encryption algorithms in use today bear mentioning the algorithm is fast compact  and easy to implement it can use a variable key length of up to 256 bits and works on 128-bit blocks  can vary in key length  number of transformations  and block size because it uses only basic computational operations  it can run on a wide variety of crus  is perhaps the most common stream cipher a is designed to encrypt and decrypt a stream of bytes or bits rather than a block  this is useful when the length of a communication would make a block cipher too slow the key is input into a pseudo-random-bit generator  which is an 642 chapter 15 algorithm that attempts to produce random bits the output of the generator when fed a key is a keystream a is an infinite set of keys that can be used for the input plaintext stream rc4 is used in encrypting steams of data  such as in wep  the wireless lan protocol lt is also used in communications between web browsers and web servers  as we discuss below unfortunately  rc4 as used in wep  ieee standard 802.11  has been found to be breakable in a reasonable amount of con1.puter time in fact rc4 itself has vulnerabilities  15.4.1.2 asymmetric encryption in an there are different encryption and decryption keys here  we one such algorithm  known as rsa after the names of its inventors  rivest  shamir  and adleman   the rsa cipher is a block-cipher public-key algorithm and is the most widely used asymmetrical algorithm asymmetrical algorithms based on elliptical curves are gaining ground  however  because the key length of such an algorithm can be shorter for the same amount of cryptographic strength  it is computationally infeasible to derive d  kd  n  from e  lee  n   and so e  ke  n  need not be kept secret and can be widely disseminated ; thus  e  lee  n   or just ice  is the and d  kd  n   or just led  is the n is the write 'i messlge 69/ 0 isl l m ~ encryption_... 695 mod 91 key k5.91 ~  + ' gl ~ gl  j c uc m m  f  .r     ~ 0 ' 1      n read ~ figure 15.8 encryption and decryption using rsa asymmetric cryptography  15.4 643 product of two large  randomly chosen prime numbers p and q  for example  p andq are512bitseach   theencryptionalgorithmis e  kc  n   rn  = mk  mod n  where icc satisfies leekd mod  p -1   q -1  = 1 the decryption algorithm is then d  kd  n   c  = ckd mod n an example using small values is shown in figure 15.8 in this example  we make p = 7 and q = 13 we then calculate n = 7 13 = 91 and  p-1   q -1  = 72  we next select kc relatively prime to 72 and 72  yielding 5 finally  we calculate kd such that kekrt mod 72 = 1  yielding 29 we now have our keys  the public key  lee  n = 5  91  and the private key  led  n = 29  91 encrypting the message 69 with the public key results in the message 62  which is then decoded by the receiver via the private key  the use of asymmetric encryption begins with the publication of the public key of the destination for bidirectional communication  the source also must publish its public key publication can be as simple as handing over an electronic copy of the key  or it can be more complex the private key  or secret key  must be jealously guarded  as anyone holding that key can decrypt any message created by the matching public key  we should note that the seemingly small difference in key use between asymmetric and symmetric cryptography is quite large in practice asymmetric cryptography is based on mathematical functions rather than transformations  inaking it much more computationally expensive to execute it is much faster for a computer to encode and decode ciphertext by using the usual symmetric algorithms than by using asymmetric algorithms why  then  use an asymmetric algorithm in truth  these algorithms are not used for generalpurpose encryption of large amounts of data however  they are used not only for encryption of small amounts of data but also for authentication  confidentiality  and key distribution  as we show in the following sections  15.4.1.3 authentication we have seen that encryption offers a way of constraining the set of possible receivers of a message constraining the set of potential senders of a message is called authentication is thus complementary to encryption in fact  sometimes their functions overlap consider that an encrypted message can also prove the identity of the sender for example  if d  kd  n   e  ke n   m   produces a valid message  then we know that the creator of the message must hold ke authentication is also useful for proving that a message has not been modified in this section  we discuss authentication as a constraint on possible receivers of a message note that this sort of authentication is similar to but distinct from user authentication  which we discuss in section 15.5  an authentication algorithm consists of the following components  a set k of keys  a set m of messages  a set a of authenticators  a functions  k    m + a   that is  for each k e k  s  k  is a function for generating authenticators from messages both sand s  k  for any k should be efficiently computable functions  644 chapter 15 a function v     +  m x a +  true  false    that is  for each lc e k  v  lc  is a function for verifying authenticators on messages both v and v  lc  for any lc should be efficiently computable functions  the critical property that an authentication algorithm must possess is this  for a message m  a computer can generate an authenticator a e a such that v  lc   m  a  = true only if it possesses s  lc   thus  a computer holding s  lc  can generate authenticators on messages so that any computer possessing v  lc  can verify them however  a computer not holding s  lc  can not generate authenticators on messages that can be verified using v  lc   since authenticators are generally exposed  for example  sent on a network with the messages themselves   it must not be feasible to derive s  lc  from the authenticators  just as there are two types of encryption algorithms  there are two main varieties of authentication algorithms the first in understanding these algorithms is to explore hash functions a h  m  creates a small  fixed-sized block of data  known as a or from a message m hash functions work by taking a message in n-bit blocks and processing the blocks to produce an n-bit hash h must be collision resistant on m-that is  it must be infeasible to find an 1111 # m such that h  m  = h  n/   now  if h  m  = h  m1   we know that m m1 -that is  we know that the message has not been modified common message-digest functions include which produces a 128-bit hash  and which outputs a 160-bit hash  message digests are useful for detecting changed messages but are not useful as authenticators for example  h  m  can be sent along with a message ; but if his known  then someone could modify m and recompute h  m   and the message modification would not be detected therefore  an authentication algorithm takes the message digest and encrypts it  the first main type of authentication algorithm uses symmetric encryption  in a a cryptographic checksum is generated from message using a secret key knowledge of v  lc  and knowledge of s  lc  are equivalent  one can be derived from the other  so lc must be kept secret a simple example of a mac defines s  lc   m  = f  k  h  m    where f is a function that is one-way on its first argument  that is  k cam10t be derived from f  k  h  m     because of the collision resistance in the hash function  we are reasonably assured that no other message could create the same mac a suitable verification algorithm is then v  lc   m  a  =  j  lc  m  = a   note that k is needed to compute both s  lc  and v  lc   so anyone able to compute one can compute the other  the second main type of authentication algorithm is a and the authenticators thus produced are called in a digital-signature algorithm  it is computationally to derive s  ks  from v  lcv  ; in particular  vis a one-way function thus  kv is the public key and lc5 is the private key  consider as an example the rsa digital-signature algorithm it is similar to the rsa encryption algorithm  but the key use is reversed the digital signature of a message is derived by computing s  lcs   m  = h  m  s mod n  the key /c5 again is a pair  d  n   where n is the product of two large  randomly chosen prime numbers p and q the verification algorithm is then v  kv   m  a  =  ak mod n = h  m    where kv satisfies lc   c5 mod  p  1   q  1  = 1  15.4 645 lf encryption can prove the identity of the sender of a m ~ essage  then why do we need separate authentication algorithms there are three primary reasons  authentication algorithms generally require fewer computations  with the notable exception of h.sa digital signatures   over large amounts of plaintext  this efficiency can make a huge difference in resource use and the time needed to authenticate a message  the authenticator of a message is almost always shorter than the message and its ciphertext this improves space use and transmission time efficiency  sometimes  we want authentication but not confidentiality for example  a company could provide a software patch and could sign that patch to prove that it came from the company and that it has n't been modified  authentication is a component of many aspects of security for example  it is the core of which supplies proof that an entity performed an action a typical example of nonrepudiation involves the filling out of electronic forms as an alternative to the signing of paper contracts nonrepudiation assures that a person filling out an electronic form can not deny that he did so  15.4.1.4 key distribution certainly  a good part of the battle between cryptographers  those inventing ciphers  and cryptanalysts  those trying to break them  involves keys with symmetric algorithms  both parties need the key  and no one else should have it the delivery of the symmetric key is a huge challenge sometimes it is performed cut-or-band -say  via a paper document or a conversation  these methods do not scale well  however also consider the key-management challenge suppose a user wanted to communicate with n other users privately  that user would need n keys and  for more security  would need to change those keys frequently  these are the very reasons for efforts to create asymmetric key algorithms  not only can the keys be exchanged in public  but a given user needs only one private key  no matter how many other people she wants to communicate with there is still the matter of managing a public key for each party to be communicated with  but since public keys need not be secured  simple storage can be used for that unfortunately  even the distribution of public keys requires some care  consider the man-in-the-middle attack shown in figure 15.9 here  the person who wants to receive an encrypted message sends out his public key  but an attacker also sends her bad public key  which matches her private key   the person who wants to send the encrypted message knows no better and so uses the bad key to encrypt the message the attacker then happily decrypts it  the problem is one of authentication-what we need is proof of who  or what  owns a public one to solve that problem involves the use of digital certificates a is a public key digitally signed by a trusted party the trusted party receives proof of identification from some entity 646 chapter 15 encryption __  key kbad attacker decryption key kd __  co ' -.u ' c ; aq cd 0 _ decryption ...  key kbad .,......_read  + message m i figure 15.9 a man-in-the-middle attack on asymmetric cryptography  and certifies that the public key we can trust the certifier these have their public keys i.j.l.cluded within web browsers  and other consumers of certificates  before they are distributed the certificate authorities can then vouch for other authorities  digitally signing the public keys of these other authorities   and so on  creating a web of trust the certificates can be distributed in a standard x.509 digital certificate format that can be parsed by computer this scheme is used for secure web communication  as we discuss in section 15.4.3  15.4.2 implementation of cryptography network protocols are typically organized in each layer acting as a client of the one below it that is  when one protocol generates a message to send to its protocol peer on another machine  it hands its message to the protocol below it in the network-protocol stack for delivery to its peer on that machine  for example  in an ip network  tcp  a transport-layer protocol  acts as a client of ip  a network-layer protocol   tcp packets are passed down to ip for delivery to the tcp peer at the other end of the tcp connection ip encapsulates the tcp 15.4 647 packet in an ip packet  which it similarly passes down to the data-link layer to be transmitted across the network to its ip peer on the destination computer this ip peer then delivers the tcp up to the tcp peer on that machine all in all  the which has been almost universally adopted as a model for data networking  defines seven such protocol layers  you will read more about the iso model of networking in chapter 16 ; figure 16.6 shows a diagram of the model  cryptography can be inserted at almost any layer in the iso model ssl  section 15.4.3   for example  provides security at the transport layer networklayer security generally has been standardized on which defines ip packet formats that allow the insertion of authenticators and the encryption of packet contents it uses symmetric encryption and uses the protocol for key ipsec is becoming widely used as the basis for in which all traffic between two ipsec endpoints is encrypted to make a private network out of one that may otherwise be public numerous protocols also have been developed for use by applications  but then the applications themselves must be coded to implement security  where is cryptographic protection best placed in a protocol stack in general  there is no definitive answer on the one hand  more protocols benefit from protections placed lower in the stack for example  since ip packets encapsulate tcp packets  encryption of ip packets  using ipsec  for example  also hides the contents of the encapsulated tcp packets similarly  authenticators on ip packets detect the modification of contaii1.ed tcp header information  on the other hand  protection at lower layers in the protocol stack may give insufficient protection to higher-layer protocols for example  an application server that runs over ipsec might be able to authenticate the client computers from which requests are received however  to authenticate a user at a client computer  the server may need to use an application-level protocol-for example  the user may be required to type a password also consider the problem of e-mail e-mail delivered via the industry standard smtp protocol is stored and forwarded  frequently multiple times  before it is delivered each of these transmissions could go over a secure or an insecure network for e-mail to be secure  the e-mail message needs to be encrypted so that its security is independent of the transports that carry it  15.4.3 an example  ssl ssl 3.0 is a cryptographic protocol that enables two computers to corrumjj1icate securely-that is  so that each can limit the sender and receiver of to the other it is perhaps the most commonly used cryptographic on the internet today  since it is the standard protocol by which web communicate securely with web servers for completeness  we should note that ssl was designed by netscape and that it evolved into the industry standard tls protocol in this discussion  we use ssl to mean both ssl and tls  ssl is a complex protocol with many options here  we present only a single variation of it  and even then in a very simplified and abstract form  so as to maintain focus on its use of cryptographic primitives what we are about to see is a complex dance in which asymmetric cryptography is used so that a client and a server can establish a secure  -cey that can be used for symmetric encryption of the session between the two-all of this while 648 chapter 15 avoiding man-in-the-middle and replay attacks for added cryptographic strength  the session keys are forgotten once a session is completed another communication between the two will generation of new session keys  the ssl protocol is initiated by a c to communicate securely with a prior to the protocol 's use  the server s is assumed to have obtained a certificate  denoted cert  from certification authority ca this certificate is a structure containing the following  various attributes attrs of the server  such as its unique distinguished name and its common  dns  name the identity of a public encryption algorithm e   for the server the public key kc of this server a validity interval interval durirtg which the certificate should be considered valid a digital signature a on the above information made by theca-that is  a = s  kca    attrs  e  ke   interval   in addition  prior to the protocol 's use  the client is presumed to have obtained the public verification algorithm v  kca  for ca in the case of the web  the user 's browser is shipped from its vendor containing the verification algorithms and public keys of certain certification authorities the user can add or delete these for certification authorities as she chooses  when c connects to s 1 it sends a 28-byte random value nc to the server  which responds with a random value n5 of its own  plus its certificate cert5 the client verifies that v  kca    attrs  e  lee   interval   a  = true and that the current time is in the validity interval interval if both of these tests are satisfied  the server has proved its identity then the client generates a random 46-byte and sends cpms = e  ks   pms  to the server the server recovers pms = d  kd   cpms   now both the client and the server are in possession of nc  n5  and pms  and each can cmnpute a shared 48-byte l ' '' ' c f  nc  715  pms   where f is a one-way and collision-resistant function  server and client can compute ms  since only they know pms  dependence of ms on nc and n5 ensures that ms is a fresh value-that is  a session key that has not been used in a previous communication at this point  the client and the server both compute the keys the ms  a symmetric encryption key k ~  ypt for encrypting messages from to the server client a symmetric encryption to the client lc ~ rypt for encrypting messages from the server a mac generation jc ~ ac generating authenticators on from the client to the server a mac generation k ~ ~ ac for generating authenticators on from the server to the to send a message m to the server  the client sends 15.5 15.5 649 upon receiving c  the server recovers  m a  = d  jc ~ pt   c  and accepts m if v  lc ~ ac   m  a  = true similarly  to send a message m to the client  the server sends and the client recovers and accepts m if v  k ~ ac   m  a  = true  this protocol enables the server to limit the recipients of its messages to the client that generated pms and to limit the senders of the messages it accepts to that same client similarly  the client can limit the recipients of the messages it sends and the senders of the messages it accepts to the party that knows s  kd   that is  the party that can decrypt cpms   in many applications  such as web transactions  the client needs to verify the identity of the party that knows s  lcd   this is one purpose of the certificate cert5 ; in particular  the attrs field contains information that the client can use to determine the identityfor example  the domain name-of the server with which it is communicating  for applications in which the server also needs information about the client  ssl supports an option by which a client can send a certificate to the server  in addition to its use on the internet  ssl is being used for a wide variety of tasks for example  ipsec vpns now have a competitor in ssl vpns ipsec is good for point-to-point encryption of traffic-say  between two company offices ssl vpns are more flexible but not as efficient  so they might be used between an individual employee working remotely and the corporate office  our earlier discussion of authentication involves messages and sessions but what about users if a system can not authenticate a user  then authenticating that a message can'le from that user is thus  a major security problem for operating systems is the protection system depends on the ability to identify the programs and processes currently executing  which in turn depends on the ability to identify each user the system users identify themselves how do we determine a user 's is authentic generally  user authentication is based on one or more things  the user 's possession of something  a or card   the user 's of something user identifier and password   an attribute retina or signature   15.5.1 passwords the most comm.on when the user a user is the use of user id or account name  she 650 chapter 15 is asked for a password i the user-supplied password matches the password stored in the system  the system assumes that the account is being accessed by the owner of that account  passwords are often used to protect objects in the computer system  in the absence of more complete protection schemes they can be considered a special case of either keys or capabilities for instance  a password may be associated with each resource  such as a file   whenever a request is made to use the resource  the password nmst be given if the password is correct  access is granted different passwords may be associated with different access rights  for example  different passwords may be used for reading files  appending files  and updating files  in practice  most systems require only one password for a user to gain full rights although more passwords theoretically would be more secure  such systems tend not to be implemented due to the classic trade-off between security and convenience if security makes something inconvenient then the security is frequently bypassed or otherwise circumvented  15.5.2 password vulnerabilities passwords are extremely common because they are easy to understand and use  unfortunately  passwords can often be guessed  accidentally exposed  sniffed  or illegally transferred from an authorized user to an unauthorized one  as we show next  there are two common ways to guess a password one way is for the intruder  either human or program  to know the user or to have information about the user all too frequently  people use obvious information  such as the names of their cats or spouses  as their passwords the other way is to use brute force  trying enumeration-or all possible combinations of valid password characters  letters  numbers  and punctuation on some systems  -until the password is found short passwords are especially vulnerable to this method  for example  a four-character password provides only 10,000 variations on average  guessing 5,000 times would produce a correct hit a program that could try a password every millisecond would take only about 5 seconds to guess a four-character password enumeration is less successful where systems allow longer passwords that include both uppercase and lowercase letters  along with numbers and all punctuation characters of course  users must take advantage of the large password space and must not  for example  use only lowercase letters  in addition to being guessed  passwords can be exposed as a result of visual or electronic monitoring an intruder can look over the shoulder of a user when the user is logging iil and can learn the password easily by watching the keyboard alternatively  anyone with access to the network on which a computer resides can seamlessly add a network monitor  allowing him to watch all data being transferred on the network including user ids and passwords encrypting the data stream containing the password solves this problem even such a system could have passwords stolen  however for example  if a file is used to contain the passwords  it could be copied for off-system analysis or consider a trojan-horse program installed on the system that captures every keystroke before sending it on to the application  15.5 651 exposure is a particularly severe problem if the password is written down where it can be read or lost as we shall see  some systems force users to select hard-to-remember or long passwords  which may cause a user to record the password or to reuse it as a result  such systems provide much less security than systems that allow users to select easy passwords ! the final type of password compromise  illegal transfer  is the result of human nature most computer installations have a rule that forbids users to share accounts this rule is sometimes implemented for accounting reasons but is often aimed at improving security for instance  suppose one user id is shared by several users  and a security breach occurs from that user id it is impossible to know who was using the id at the time the break occurred or even whether the user was an authorized one with one user per user id  any user can be questioned directly about use of the account ; in addition  the user might notice something different about the account and detect the break-in sometimes  users break account-sharing rules to help friends or to circumvent accounting  and this behavior can result in a system 's being accessed by unauthorized users -possibly harmful ones  passwords can be either generated by the system or selected by a user  system-generated passwords may be difficult to remember  and thus users may write them down as mentioned  however  user-selected passwords are often easy to guess  the user 's name or favorite car  for example   some systems will check a proposed password for ease of guessing or cracking before accepting it  at some sites  administrators occasionally check user passwords and notify a user if his password is easy to guess some systems also age passwords  forcing users to change their passwords at regular intervals  every three months  for instance   this method is not foolproof either  because users can easily toggle between two passwords the solution  as implemented on some systems  is to record a password history for each user for instance  the system could record the last n passwords and not allow their reuse  several variants on these simple password schemes can be used for example  the password can be changed more frequently in the extren  1e  the password is changed from session to session a new password is selected  either by the system or by the user  at the end of each session  and that password must be used for the next session in such a case  even if a password is misused  it can be used only once when the legitimate user tries to use a now-invalid password at the next session  he discovers the security violation steps can then be taken to repair the breached security  15.5.3 encrypted passwords one problem with all these approaches is the difficulty of keeping the password secret within the computer how can the system store a password securely yet allow its use for authentication when the user presents her password the unix system uses encryption to avoid the necessity of keeping its password list secret each user has a password the system contains a function that is extremely difficult-the designers hope impossible-to invert but is simple to compute that is  given a value x  it is easy to compute the function value f  x   given a function value j  x   however  it is impossible to compute x this function is used to encode all passwords only encoded passwords are stored  when a user presents a password  it is encoded and compared against the 652 chapter 15 stored encoded password even if the stored encoded password is seen  it cam1ot be decoded  so the password can not be determined thus  the password file does not need to be kept secret the functionf  x  is typically an encryption algorithm that has been designed and tested rigorously  the flaw in this method is that the system no longer has control over the passwords although the passwords are encrypted  anyone with a copy of the password file can run fast encryption routines against it-encrypting each word in a dictionary  for instance  and comparing the results against the passwords if the user has selected a password that is also a word in the dictionary  the password is cracked on sufficiently fast computers  or even on clusters of slow computers  such a comparison may take only a few hours  furthermore  because unix systems use a well-known encryption algorithm  a cracker might keep a cache of passwords that have been cracked previously  for these reasons  new versions of unix store the encrypted password entries in a file readable only by the the programs that compare a presented password to the stored password run setuid to root ; so they can read this file  but other users can not they also include a salt  or recorded random number  in the encryption algorithm the salt is added to the password to ensure that if two plaintext passwords are the same  they result in different ciphertexts  another weakness in the unix password methods is that many unix systems treat only the first eight characters as significant it is therefore extremely important for users to take advantage of the available password space to avoid the dictionary encryption method  some systems disallow the use of dictionary words as passwords a good technique is to generate your password by using the first letter of each word of an easily remembered phrase using both upper and lower characters with a number or punctuation mark thrown in for good measure for example  the phrase my mother 's name is katherine might yield the password mmn.isk !  the password is hard to crack but easy for the user to remember  15.5.4 one-time passwords to avoid the problems of password sniffing and shoulder surfing  a system could use a set of paired when a session begins  the system randomly selects and presents one part of a password pair ; the user must supply the other part in this system  the user is challenged and must with the correct answer to that challenge  this approach can be generalized to the use of an algorithm as a password  the algorithm might be an integer function  for example the system selects a random integer and presents it to the user the user applies a function and replies with the correct result the system also applies the function if the two results match  access is allowed  such algorithmic passwords are not susceptible to reuse ; that is  a user can type in a password  and no entity intercepting that password will be able to reuse it in this scheme  the system and the user share a secret the secret is never transmitted over a medium that allows exposure rather  the secret is used as input to the function  along with a shared seed a is a random number or alphanumeric sequence the seed is the authentication challenge from the computer the secret and the seed are used as input to the function f  secret  seed   the result of this function is transmitted as the password to the 15.5 653 computer becallse the computer also knows the secret and the seed  it can perform the same computation if the results match  the user is authenticated  the next time the user needs to be authenticated  another seed is generated  and the same ensue this time  the password is different  in this system  the password is different in each instance anyone capturing the password from one session and trying to reuse it in another session will fail one-time passwords are among the only ways to prevent improper authentication clue to password exposure  one-time password systems are implemented in various ways commercial implementations  such as securid  use hardware calculators most of these calculators are shaped like a credit card  a key-chain dangle  or a usb device ; they include a display and may or may not also have a keypad some use the current time as the random seed others the user to enter the shared secret  also known as a or on the keypad the display then shows the one-time password the use of both a one-time password generator and a pin is one form of n ! jn  ' ' ' ' two different types of components are needed in this case two-factor authentication offers far better authentication protection than single-factor authentication  another variation on one-time passwords uses a or which is a list of single-use passwords each password on the list is used once and then is crossed out or erased the commonly used s/key system uses either a software calculator or a code book based on these calculations as a source of one-time passwords of course  the user must protect his code book  15.5.5 biometrics yet another variation on the use of passwords for authentication involves the use of biometric measures palm or hand-readers are commonly used to secure physical access-for example  access to a data center these readers match stored parameters against what is being read from hand-reader pads  the parameters can include a temperature map  as well as finger length  finger width  and line patterns these devices are currently too large and expensive to be used for normal computer authentication  fingerprint readers have become accurate and cost-effective and should become more common in the future these devices read finger ridge patterns and convert them into a sequence of numbers over time  they can store a set of sequences to adjust for the location of the finger on the reading pad and other factors software can then scan a finger on the pad and compare its features with these stored sequences to determine if they match of course  multiple users can have profiles stored  and the scanner can differentiate among them  a very accurate two-factor authentication scheme can result from requiring a password as well as a user name and fingerprint scan if this information is encrypted in transit  the system can be very resistant to spoofing or replay attack  is better still consider how strong authentication can be with a usb device that must be plugged into the system  a pin  and a fingerprint scan except for the user 's having to place her finger on a pad and plug the usb into the system  this authentication method is no less convenient 654 chapter 15 15.6 that using normal passwords recall  though  that strong authentication by itself is not sufficient to guarantee the id of the user an authenticated session can still be hijacked if it is not encrypted  just as there are myriad threats to system and network security  there are many security solutions the solutions run the gamut from improved user education  through technology  to bug-free software most security professionals subscribe to the theory of which states that more layers of defense are better than fewer layers of course  this theory applies to any kind of security consider the security of a house without a door lock  with a door lock  and with a lock and an alarm in this section  we look at the major methods  tools  and techniques that can be used to improve resistance to threats  15.6.1 security policy toward improving the security of any aspect of computing is to have a  policies vary widely but generally include a statement of what is being secured for example  a policy might state that all outsideaccessible applications must have a code review before being deployed  or that users should not share their passwords  or that all connection points between a company and the outside must have port scans nm every six months without a policy in place  it is impossible for users and administrators to know what is permissible  what is required  and what is not allowed the policy is a road map to security  and if a site is trying to move from less secure to more secure  it needs a map to know how to get there  once the security policy is in place  the people it affects should know it well it should be their guide the policy should also be a that is reviewed and updated periodically to ensure that it is still pertinent and still followed  15.6.2 vulnerability assessment how can we determine whether a security policy has been correctly implemented the best way is to execute a vulnerability assessment such assessments can cover broad ground  from social engineering through risk assessment to port scans rlsl for example  endeavors to value the assets of the entity in question  a program  a management team  a system  or a facility  and determine the odds that a security incident will affect the entity and decrease its value when the odds of suffering a loss and the amount of the potential loss are known  a value can be placed on trying to secure the entity  the core activity of most vulnerability assessments is a '., ~ '''--in which the entity is scanned for known vulnerabilities because this book is concerned with operating systems and the software that runs on them  we concentrate on those aspects of vulnerability assessment  vulnerability scans typically are done at times when computer use is relatively low  to minimize their impact when appropriate  they are done on 15.6 655 test systems rather than production systems  because they can induce unhappy behavior from the target systems or network devices  a scan within an individual system can check a variety of aspects of the system  short or easy-to-guess passwords unauthorized privileged programs  such as setuid programs unauthorized programs in system directories unexpectedly long-running processes improper directory protections on user and system directories improper protections on system data files  such as the password file  device drivers  or the operating-system kernel itself dangerous entries in the program search path  for example  the trojan horse discussed in section 15.2.1  changes to system programs detected with checksum values unexpected or hidden network daemons any problems found by a security scan can be either fixed automatically or reported to the managers of the system  networked computers are much more susceptible to security attacks than are standalone systems rather than attacks from a known set of access points  such as directly connected terminals  we face attacks from an unknown and large set of access points-a potentially severe security problem to a lesser extent  systems connected to telephone lines via modems are also more exposed  in fact  the u.s government considers a system to be only as secure as its most far-reaching connection for instance  a top-secret system may be accessed only from within a building also considered top-secret the system loses its topsecret rating if any form of communication call occur outside that environment  some government facilities take extreme security precautions the connectors that plug a terminal into the secure computer are locked in a safe in the office when the terminal is not in use a person must have proper id to gain access to the building and her office  must know a physical lock combination  and must know authentication information for the computer itself to gain access to the computer-an example of multifactor authentication  unfortunately for systems administrators and computer-security professionals  it is frequently impossible to lock a machine in a room and disallow all remote access for instance  the internet network currently connects millions of computers it is becoming a mission-critical  indispensable resource for many companies and individuals if you consider the internet a club  then  as in any club with millions of members  there are many good members and some bad members the bad members have many tools they can use to attempt to gain access to the interconnected computers  just as morris did with his worm  vulnerability scans can be applied to networks to address some of the problems with network security the scans search a network for ports that respond to a request if services are enabled that should not be  access to them can be blocked  or they can be disabled the scans then determine the details of 656 chapter 15 the application listening on that port and try to determine if it has any known vulnerabilities testing those vulnerabilities can determine if the system is ncisconfigured or lacks needed patches  finally though  consider the use of port scanners in the hands of a cracker rather than someone trying to improve security these tools could help crackers find vulnerabilities to attack  fortunately  it is possible to detect port scans through anomaly detection  as we discuss next  it is a general challenge to security that the same tools can be used for good and for harm in fact  some people advocate stating that no tools should be written to test security  because such tools can be used to find  and exploit  security holes others believe that this approach to security is not a valid one  pointing out  for example  that crackers could write their own tools it seems reasonable that security through obscurity be considered one of the layers of security only so long as it is not the only layer for example  a company could publish its entire network configuration ; but keeping that information secret makes it harder for intruders to know what to attack or to determine what might be detected even here  though  a company assuming that such information will remain a secret has a false sense of security  15.6.3 intrusion detection and facilities is intimately linked to intrusion detection  as its name suggests  strives to detect attempted or successful intrusions into computer systems and to initiate appropriate responses to the intrusions intrusion detection encompasses a wide array of techniques that vary on a number of axes  including the following  the time at which detection occurs detection can occur in real time  while the intrusion is occurring  or after the fact  the types of inputs examined to detect intrusive activity these may include user-shell commands  process system calls  and network packet headers or contents some forms of intrusion might be detected only by correlating information from several such sources  the range of response capabilities simple forms of response include alerting an administrator to the potential intrusion or somehow halting the potentially intrusive activity-for example  killing a process engaged in such activity in a sophisticated fonn of response ; a system might transparently divert an intruder 's activity to a false resource exposed to the attacker the resource appears real to the attacker and enables the system to monitor and gain information about the attack these degrees of freedom in the design space for detecting intrusions have a wide range of solutions  known as and ids systems raise an alarm when an intrusion is detected  while idp systems act as routers  passing traffic unless an intrusion is detected  at which point that traffic is blocked   but just what constitutes an intrusion defining a suitable specification of intrusion turns out to be quite difficult  and thus automatic idss and idps today settle for one of two less ambitious approaches in the first  called system input or network traffic is examined for 15.6 657 specific behavior patterns  or known to indicate attacks a simple example of signature-based detection is scanning network packets for the string /etc/passwd/ targeted for a unix systenl another example is virus-detection software  which scans binaries or network packets for lmown viruses  the second approach  typically called attempts through various techniques to detect anomalous behavior within computer systen s of course  not all anomalous system activity indicates an intrusion  but the presumption is that intrusions often induce anomalous behavior an example of anomaly detection is monitoring system calls of a daemon process to detect whether the system-call behavior deviates from normal patterns  possibly indicating that a buffer overflow has been exploited in the daemon to corrupt its behavior another example is monitoring shell commands to detect anomalous commands for a given user or detecting an anomalous login time for a user  either of which may indicate that an attacker has succeeded in gaining access to that user 's account  signature-based detection and anomaly detection can be viewed as two sides of the same coin  signature-based detection attempts to characterize dangerous behaviors and to detect when one of these behaviors occurs  whereas anomaly detection attempts to characterize normal  or non dangerous  behaviors and to detect when something other than these behaviors occurs  these different approaches yield idss and idps with very different properties  however in particular  anomaly detection can find previously unknown methods of intrusion  so-called signature-based detection  in contrast  will identify only known attacks that can be codified in a recognizable pattern thus  new attacks that were not contemplated when the signatures were generated will evade signature-based detection this problem is well known to vendors of virus-detection software  who must release new signatures with great frequency as new viruses are detected manually  anomaly detection is not necessarily superior to signature-based detection  however indeed  a significant challenge for systems that attempt anomaly detection is to benchmark normal system behavior accurately if the system has already been penetrated when it is benchmarked  then the intrusive activity may be included in the normal benchmark even if the system is benchinarked cleanly  without influence from intrusive behaviorf the benchmark must give a fairly complete picture of normal behavior otherwise  the number of  false alarms  orf worse   missed intrusions  will be excessive  to illustrate the impact of even a marginally high rate of false alarms  consider an installation consisting of a hundred unix workstations from which security-relevant events are recorded for purposes of intrusion detection a small installation such as this could easily generate a million audit records per day only one or two might be worthy of an administrator 's investigation if we suppose  optimistically  that each actual attack is reflected in ten audit recordsf we can roughly compute the rate of occurrence of audit records reflecting truly intrusive activity as follows  2 intrusions  10  recor s mtrus10n 0.00002  658 chapter 15 interpreting this as a probability of occurrence of intrusive records/ ' we denote it as p  i  ; that is  event i is the occurrence of a record reflecting truly intrusive behavior since p  i  = 0.00002  we also know that p  ~ i  = 1-p  i  = 0.99998 now we let a denote the raising of an alarm by an ids an accurate ids should maximize both p  i la  and p  ~ i i ~ a  -that is  the probabilities that an alarm indicates an intrusion and that no alarm indicates no intrusion focusil g on p  i i a  for the moment  we can compute it using p  iia  p  i  p  aii  p  i  p  aii  + p  ~ i  p  ai ~ i  0.00002 p  aii  0.00002 p  aii  + 0.99998 p  ai ~ i  now consider the impact ofthe false-alarm rate p  ai ~ i  on p  iia   even with a very good true-alarm rate of p  ail  = 0.8  a seemingly good falsealarm rate of p  ai ~ i  = 0.0001 yields p  iia  ~ 0.14 that is  fewer than one ill every seven alarms indicates a real intrusion ! in systems where a security administrator ilwestigates each alarm  a high rate of false alarms-called a christmas tree effect -is exceedingly wasteful and will quickly teach the admilcistrator to ignore alarms  this example illustrates a general principle for idss and idps  for usability  they must offer an extremely low false-alarm rate achieving a sufficiently low false-alarm rate is an especially serious challenge for anomaly-detection systems  as mentioned  because of the difficulties of adequately benchmarking normal system behavior however  research contil ues to improve anomalydetection techniques intrusion detection software is evolving to implement signatures  anomaly algorithms  and other algorithms and to combine the results to arrive at a more accurate anomaly-detection rate  15.6.4 virus protection as we have seen  viruses can and do wreak havoc on systems protection from viruses thus is an important security concern antivirus programs are often used to provide this protection some of these programs are effective against only particular known viruses they work by searching all the programs on a system for the specific pattern of instructions known to make up the virus  when they find a known pattern  they remove the instructions  the program antivirus programs may have catalogs of thousands of viruses for which they search  both viruses and antivirus software continue to become more sophisticated  some viruses modify themselves as they infect other software to avoid the basic pattern-match approach of antivirus programs antivirus programs ill turn now look for families of patterns rather than a single pattern to identify a virus  in fact  some antivirus programs implement a variety of detection algorithms  they can decompress compressed viruses before checking for a signature  some also look for process anomalies a process opening an executable file for writing is suspicious  for example  unless it is a compiler another popular teducique is to run a program in a which is a controlled or emulated 15.6 659 the tripwire file system an example of an anomaly-detection tool is the checking tool for unix  developed at purdue university tripwire operates on the premise that many intrusions result in modification of system directories and files for example  an attacker might modify the system programs  perhaps inserting copies with trojan horses  or might insert new programs into directories commonly found in user-shell search paths or an intruder might remove system log files to cover his tracks tripwire is a tool to monitor file systems for added  deleted  or changed files and to alert system administrators to these modifications  the operation of tripwire is controlled by a configurationfile tw.config that enumerates the directories and files to be monitored for changes  deletions  or additions each entry in this configuration file includes a selection mask to specify the file attributes  inode attributes  that will be monitored for changes for example  the selection mask might specify that a file 's permissions be monitored but its access time be ignored in addition  the selection mask can instruct thatthe file be monitored for changes monitoring the hash of a file for changes is as good as monitoring the file itselt but storing hashes of files requires far less room than copying the files themselves  when run initially  tripwire takes as input the tw.config file and computes a sign.ature for each file or directory consisting of its monitored attributes  inode attributes and hash values   these signatures are stored in a database when run subsequently  tripwire inputs both tw.config and the previously stored database  recomputes the signature for each file or directory named in tw.conf ig  and compares this signature with the signature  if any  in the previously compl.j-ted database events reported to an administrator include any monitored file or directory whose signature differs from that in the database  a changed file   any file or directory in a monitored directory for which a signature does not exist in the database  an added file   and any signature in the database for which the corresponding file or directory no longer exists  a deleted file   although effective for a wide class of attacks  tripwire does have limitations  perhaps the most obvious is the need to protect the tripwire program and its associated files  especially the database file  from unauthorized modification  for this reason  tripwire and its associated files should be stored on some tamper-proof medium  such as a write-protected disk or a secure server where logins can be tightly controlled unforhm.ately  this makes it less convenient to update the database after authorized updates to monitored directories and files a second limitation is that some security-relevant files-for example  system log files-are supposed to change over time  and tripwire does not provide a way to distinguish between an authorized and an unauthorized change so  for example  an attack that modifies  without deleting  a system log that would normally change anyway would escape tripwire 's detection capabilities the best tripwire can do in this case is to detectcertain obvious inconsistencies  for example  a shrinking log file   free and commercial versions of tripwire are available from http  / /tripwire.org and.http  / /tripwire.com  660 chapter 15 section of the system the antivirus software analyzes the behavior of the code in the sandbox before letting it run unmonitored some antivirus programs also put up a complete shield rather than just scanning files within a file system  they search boot sectors  menlory  inbound and outbound e-mail  files as they are downloaded  files on removable devices or media  and so on  the best protection against computer viruses is prevention  or the practice of purchasing unopened software from vendors and avoiding free or pirated copies from public sources or disk exchange offer the safest route to preventing infection however  even new copies of legitimate software applications are not immune to virus infection  in a few cases  disgruntled employees of a software company have infected the master copies of software programs to do economic harm to the company for macro viruses  one defense is to exchange microsoft word documents in an alternative file format called unlike the native word format rtf does not include the capability to attach macros  another defense is to avoid opening any e-mail attachments from unknown users unfortunately  history has shown that e-mail vulnerabilities appear as fast as they are fixed for example  in 2000  the love bug virus became very widespread by traveling in e-mail messages that pretended to be love notes sent by friends of the receivers once a receiver opened the attached visual basic script  the virus propagated by sending itself to the first addresses in the receiver 's e-mail contact list fortunately  except for clogging e-mail systems and users ' inboxes  it was relatively harmless it did  however  effectively negate the defensive strategy of opening attachments only from people known to the receiver a more effective defense method is to avoid opening any e-mail attachment that contains executable code some companies now enforce this as policy by removing all incoming attachments to e-mail messages  another safeguard  although it does not prevent infection  does permit early detection a user must begin by completely reformatting the hard disk  especially the boot sector  which is often targeted for viral attack only secure software is uploaded  and a signature of each program is taken via a secure message-digest computation the resulting filename and associated messagedigest list must then be kept free from unauthorized access periodically  or each time a program is run  the operating system recomputes the signature and compares it with the signature on the original list ; any differences serve as a warning of possible infection this technique can be combined with others for example  a high-overhead antivirus scan  such as a sandbox  can be used ; and if a program passes the test  a signature can be created for it if the signatures match the next time the program is run  it does not need to be virus-scanned again  15.6.5 auditing  accounting  and logging auditing  accounting  and logging can decrease system performance  but they are useful in several areas  including security logging can be general or specific all system-call executions can be logged for analysis of program behavior  or misbehavior   more typically  suspicious events are logged  authentication failures and authorization failures can tell us quite a lot about break-in attempts  15 15.7 661 accounting is another potential tool in a security administrator 's kit it can be used to find performance changes  which in tum can reveal security problems one of the early unix computer break-ins was detected by cliff stoll when he was exam5ning accounting logs and spotted an anomaly  we turn next to the question of how a trusted computer can be connected safely to an untrustworthy network one solution is the use of a firewall to separate trusted and unh usted systems a is a computer  appliance  or router that sits between the trusted and the untrusted a network firewall limits network access between the two and monitors and logs all connections it can also limit coru1.ections based on source or destination address  source or destination port  or direction of the connection for instance  web servers use http to communicate with web browsers a firewall therefore may allow only http to pass from all hosts outside the firewall to the web server within the firewall the morris internet worm used the finger protocol to break into computers  so finger would not be allowed to pass  for example  in fact  a network firewall can separate a network into multiple domains  a common implementation has the internet as the untrusted domain ; a semitrusted and semisecure network  called the as another domain ; and a company 's computers as a third domain  figure 15.10   coru1.ections are allowed from the internet to the dmz computers and from the company computers to the internet but are not allowed from the internet or dmz computers to the company computers optionally  controlled commurucations may be allowed between the dmz and one company computer or more for instance  a web server on the dmz may need to query a database server on the corporate network with a firewall  however  access is contained  and any dmz systems that are broken into still are unable to access the company computers  internet internet access from company 's computers r---------i company computers access between dmz and company 's computers figure 15.10 domain separation via firewall  662 chapter 15 15.8 of course  a firewall itself must be secure and attack-proof ; otherwise  its ability to secure connections can be compromised furthermore  firewalls do not prevent attacks that or travel within protocols or com1ections that the firewall allows a buffer-overflow attack to a web server will not be stopped by the firewall  for example  because the http connection is allowed ; it is the contents of the http connection that house the attack likewise  denialof service attacks can affect firewalls as much as any other machines another vulnerability of firewalls is in which an unauthorized host pretends to be an authorized host by meeting some authorization criterion for example  if a firewall rule allows a connection from a host and identifies that host by its ip address  then another host could send packets using that same address and be allowed through the firewall  in addition to the most common network firewalls  there are other  newer kinds of firewalls  each with its pros and cons a is a software layer either included with the operating system or added as an application rather than limiting communication between security domains  it limits communication to  and possibly from  a given host a user could add a personal firewall to her pc so that a trojan horse would be denied access to the network to which the pc is connected  for example an prex-y understands the protocols that applications speak across the network  for example  smtp is used for mail transfer an application proxy accepts a com1ection just as an smtp server would and then initiates a connection to the original destination smtp server it can monitor the traffic as it forwards the message  watching for and disabling illegal commands  attempts to exploit bugs  and so on some firewalls are designed for one specific protocol an for example  has the specific purpose of analyzing xml traffic and blocking disallowed or malformed xml sit between applications and the kernel  monitoring system-call execution for example  in solaris 10  the least privilege feature implements a list of more than fifty system calls that processes may or may not be allowed to make a process that does not need to spawn other processes can have that ability taken away  for instance  the u.s department of defense trusted computer system evaluation criteria specify four security classifications in systems  a  b  c  and d this specification is widely used to determine the security of a facility and to model security solutions  so we explore it here the lowest-level classification is division d  or minimal protection division d includes only one class and is used for systems that have failed to meet the requirements of any of the other security classes  for instance  ms-dos and windows 3.1 are in division d  division c  the next level of security  provides discretionary protection and accountability of users and their actions through the use of audit capabilities  division c has two levels  c1 and c2 a c1-class system incorporates some form of controls that allow users to protect private information and to keep other users from accidentally reading or destroying their data a c1 environment is one in which cooperating users access data at the same levels of sensitivity most versions of unix are c1 class  15.8 663 the total of all protection systems within a computer system  hardware  software  firmware  that correctly enforce a security policy is known as a the tcb of a cl system controls access between users and files by allowing the user to specify and control sharing of objects by named individuals or defined groups in addition  the tcb requires that the users identify themselves before they start any activities that the tcb is expected to mediate this identification is accomplished via a protected mechanism or password ; the tcb protects the authentication data so that they are inaccessible to unauthorized users  a c2-class system adds an individual-level access control to the requirements of a cl system for example  access rights of a file can be specified to the level of a single individual in addition  the system adrninistrator can selectively audit the actions of any one or more users based on individual identity the tcb also protects itself from modification of its code or data structures in addition  no information produced by a prior user is available to another user who accesses a storage object that has been released back to the system some speciat secure versions of unix have been certified at the c2 level  division-b mandatory-protection systems have all the properties of a classc2 system ; in addition  they attach a sensitivity label to each object the bl-class tcb maintains the security label of each object in the system ; the label is used for decisions pertaining to mandatory access control for example  a user at the confidential level could not access a file at the more sensitive secret level the tcb also denotes the sensitivity level at the top and bottom of each page of any human-readable output in addition to the normal user-namepassword authentication information  the tcb also maintains the clearance and authorizations of individual users and will support at least two levels of security these levels are hierarchicat so that a user may access any objects that carry sensitivity labels equal to or lower than his security clearance for example  a secret-level user could access a file at the confidential level in the absence of other access controls processes are also isolated through the use of distinct address spaces  a b2-class system extends the sensitivity labels to each system resource  such as storage objects physical devices are assigned minimum and maximum security levels that the system uses to enforce constraints imposed by the physical environments in which the devices are located in addition  a b2 system supports covert channels and the auditing of events that could lead to the exploitation of a covert channel  a b3-class system allows the creation of access-control lists that denote users or groups not granted access to a given named object the tcb also contains a mechanism to monitor events that may indicate a violation of security policy the mechanism notifies the security administrator and  if necessary  terminates the event in the least disruptive manner  the highest-level classification is division a architecturally  a class-al system is functionally equivalent to a b3 system  but it uses formal design specifications and verification techniques  granting a high degree of assurance that the tcb has been implemented correctly a system beyond class al might be designed and developed in a trusted facility by trusted personnel  the use of a tcb merely ensures that the system can enforce aspects of a security policy ; the tcb does not specify what the policy should be typically  664 chapter 15 15.9 a given computing environment develops a security policy for and has the plan by a security agency  such as the national computer security center certain computing environments may require other certification  such as that supplied by tempest  which guards against electronic eavesdropping for example  a tempest-certified system has terminals that are shielded to prevent electromagnetic fields from escaping this shielding ensures that equipment outside the room or building where the terminal is housed camwt detect what information is being displayed by the terminal  microsoft windows xp is a general-purpose operating system designed to support a variety of security features and methods in this section  we examine features that windows xp uses to perform security functions for more information and background on wilcdows xp  see chapter 22  the windows xp security model is based on the notion of windows xp allows the creation of any number of user accounts  which can be grouped in any manner access to system objects can then be permitted or denied as desired users are identified to the system by a unique security id  when a user logs on  windows xp creates a that includes the security id for the user  security ids for any groups of which the user is a member  and a list of any special privileges that the user has examples of special privileges include backing up files and directories  shutting down the compute1 ~ logging on interactively  and changing the system clock every process that windows xp runs on behalf of a user will receive a copy of the access token the system uses the security ids in the access token to permit or deny access to system objects whenever the use1 ~ or a process on behalf of the user  attempts to access the object authentication of a user account is typically accomplished via a user name and password  although the modular design of windows xp allows the development of custom authentication packages for example  a retinal  or eye  scanner might be used to verify that the user is who she says she is  windows xp uses the idea of a subject to ensure that programs run by a user do not get greater access to the system than the user is authorized to have  a is used to track and manage permissions for each program that a user runs ; it is composed of the user 's access token and the program acting on behalf of the user since windows xp operates with a client-server model  two classes of subjects are used to control access  simple subjects and server subjects an example of a is the typical application program that a user executes after she logs on simple subject is assigned a based on the security access token of the user a is a process implemented as a protected server that uses the security context of the client when acting on the client 's behalf  as mentioned in section 15.7  auditing is a useful security technique  windows xp has bl1ilt-in auditing that allows many common security threats to be monitored examples include failure auditing for login and logoff events to detect random password break-ins  success auditing for login and logoff events to detect login activity at strange hours  success and failure write-access auditing for executable files to track a virus outbreak  and success and failure auditing for file access to detect access to sensitive files  15.10 15.10 665 security attributes of an object in windows xp are described by a the security descriptor contains the security id of the owner  who can change the access permissions   a group security id used the posix subsystem  a discretionary access-control list that identifies users or groups are allowed  and which are not allowed  access  and a system access-control list that controls which auditing messages the system will generate for example  the security descriptor of the file foo.bar might have owner avi and this discretionary access-control list  a vi -all access group cs-read-write access user cliff-no access in addition  it might have a system access-control list of audit writes by everyone  an access-control list is composed of access-control entries that contain the security id of the individual and an access mask that defines all possible actions on the object  with a value of accessallowed or accessdenied for each action files in windows xp may have the following access types  readdata  writedata,appenddata  execute,readextendedattribute  writeextendedattribute  readattributes  and wri teattributes we can see how this allows a fine degree of control over access to objects  windows xp classifies objects as either container objects or noncontainer objects such as directories  can logically contain other objects by default  an object is created within a container object  the new object inherits permissions from the parent object similarly  if the user copies a file from one directory to a new directory  the file will inherit the permissions of the destination directory inherit no other permissions  furthermore  if a permission is changed on a directory  the new permissions do not automatically apply to existing files and subdirectories ; the user may explicitly apply them if she so desires  the system administrator can prohibit printilig to a printer on the system for all or part of a day and can use the windows xp performance monitor to help her spot approaching problems in general  windows xp does a good job of providing features to help ensure a secure computing environment many of these features are not enabled by default  however  which may be one reason for the myriad security breaches on windows xp systems another reason is the vast number of services windows xp starts at system boot tiine and the number of applications that typically are installed on a windows xp system  for a real multiuser environment  the system administrator should formulate a security plan and implement it  using the features that windows xp provides and other security tools  protection is an internal problem security  in contrast  must consider both the computer system and the environment-people  buildings  businesses  valuable objects  and threats-within which the system is used  666 chapter 15 the data stored in the computer system must be protected from unauthorized access  malicious destruction or alteration  and accidental introduction of inconsistency it is easier to protect against accidental loss of data consistency than to protect against malicious access to the data absolute protection of the information stored in a computer system from malicious abuse is not possible ; but the cost to the perpetrator can be made sufficiently high to deter most  if not all  attempts to access that information without proper authority  several types of attacks can be launched against programs and agaitlst individual computers or the masses stack and buffer-overflow techniques allow successful attackers to change their level of system access viruses and worms are self-perpetuating  sometimes infecting thousands of computers  denial-of-service attacks prevent legitimate use of target systems  encryption limits the domain of receivers of data  while authentication limits the domain of senders encryption is used to provide confidentiality of data being stored or transferred symmetric encryption requires a shared key  while asymn'letric encryption provides a public key and a private key  authentication  when combined with hashing  can prove that data have not been changed  user authentication methods are used to identify legitimate users of a system in addition to standard user-name and password protection  several authentication methods are used one-time passwords  for example  change from session to session to avoid replay attacks two-factor authentication requires two forms of authentication  such as a hardware calculator with an activation pin multifactor authentication uses three or more forms these methods greatly decrease the chance of authentication forgery  methods of preventing or detecting security incidents include intrusiondetection systems  antivirus software  auditing and logging of system events  monitoring of system software changes  system-call monitoring  and firewalls  15.1 argue for or against the judicial sentence handed down against robert morris  jr  for his creation and execution of the internet worm discussed in section 15.3.1  15.2 discuss a means by which managers of systems connected to the internet could design their systems to limit or eliminate the damage done by worms what are the drawbacks of making the change that you suggest 15.3 what commonly used computer programs are prone to man-in-themiddle attacks discuss solutions for preventing this form of attack  15.4 the unix program cops scans a given system for possible security holes and alerts the user to possible problems what are two potential hazards of using such a system for security how can these problems be limited or eliminated 667 15.5 make a list of six security concerns for a bank 's computer system for each item on your list  state whether this concern relates to physicat human  or operating-system ~ security  15.6 an experimental addition to unix allows a user to connect a program to a file the watchdog is invoked whenever a program requests access to the file the watchdog then either grants or denies access to the file discuss two pros and two cons of using watchdogs for security  15.7 discuss how the asymmetric encryption algorithm can be used to achieve the following goals  a authentication  the receiver knows that only the sender could have generated the message  b secrecy  only the receiver can decrypt the message  c authentication and secrecy  only the receiver can decrypt the message  and the receiver knows that only the sender could have generated the message  15.8 why does n't d  lce  n   e  /cd  n   m   provide authentication of the sender to what uses can such an encryption be put 15.9 consider a system that generates 10 million audit records per day also assume that there are on average 10 attacks per day on this system and that each such attack is reflected in 20 records if the intrusion-detection system has a true-alarm rate of 0.6 and a false-alarm rate of 0.0005  what percentage of alarms generated by the system correspond to real intrusions 15.10 what is the purpose of using a salt along with the user-provided password where should the salt be stored  and how should it be used general discussions concerning security are given by hsiao et al  1979l landwehr  1981   deru  1ing  1982   pfleeger and pfleeger  2003   tanenbaum 2003  and russell and gangemi  1991   also of general interest is the text by lobel  1986   computer networking is discussed in kurose and ross  2005   issues concernin ~ g the design and verification of secure systems are discussed by rushby  1981  and by silverman  1983   a security kernel for a multiprocessor microcomputer is described by schell  1983   a distributed secure system is described by rushby and randell  1983   morris and thompson  1979  discuss password security morshedian  1986  presents methods to fight password pirates password authentication 668 chapter 15 with insecure communications is considered by lamport  1981   the issue of password cracking is examined by seely  1989   cmnputer break-ins are discussed by lehmann  1987  and by reid  1987   issues related to trusting computer programs are discussed in thompson  1984   discussions concerning unix security are offered by grampp and morris  1984 l wood and kochan  1985   farrow  1986b   farrow  1986a   filipski and hanko  1986   hecht et al  1988   kramer  1988   and garfinkel et al  2003   bershad and pinkerton  1988  present the watchdog extension to bsd unix the cops security-scanning package for unix was written by farmer at purdue university it is available to users on the internet via the ftp program from host ftp.uu.net in directory /pub i security i cops  spafford  1989  presents a detailed technical discussion of the internet worm the spafford article appears with three others in a special section on the morris internet worm in communications of the acm  volume 32  number 6  june 1989   security problems associated with the tcp /ip protocol suite are described in bellovin  1989   the mechanisms commonly used to prevent such attacks are discussed in cheswick et al  2003   another approach to protecting networks from insider attacks is to secure topology or route discovery kent et al  2000   hu et al  2002   zapata and asokan  2002   and hu and perrig  2004  present solutions for secure routing savage et al  2000  examine the distributed denialof service attack and propose ip trace-back solutions to address the problem  perlman  1988  proposes an approach to diagnose faults when the network contains malicious routers  information about viruses and worms can be found at http  / /www.viruslist.com  as well as in ludwig  1998  and ludwig  2002   other web sites containing up-to-date security information include http  / /www.trusecure.com and httpd  / /www.eeye.com a paper on the dangers of a computer monoculture can be found at http  / /www.ccianet.org/papers/cyberinsecurity.pdf  diffie and hellman  1976  and diffie and hellman  1979  were the first researchers to propose the use of the public-key encryption scheme the algorithm presented in section 15.4.1 is based on the public-key encryption scheme ; it was developed by rivest et al  1978   lempel  1979   simmons  1979   denning and demting  1979   gifford  1982   denning  1982   ahituv et al   1987   schneier  1996   and stallings  2003  explore the use of cryptography in computer systems discussions concerning protection of digital signatures are offered by akl  1983   davies  1983   denning  1983   and denning  1984   the u.s government is  of course  concerned about security the department of defense trusted computer system evaluation criteria  dod  1985    known also as the orange book  describes a set of security levels and the features that an operating system must have to qualify for each security rating reading it is a good starting point for understanding security concerns the microsoft windows nt workstation resource kit  microsoft  1996   describes the security inodel of nt and how to use that model  the rsa algorithm is presented in rivest et al  1978   information about nist 's aes activities can be found at http  / /www.nist.gov/aes/ ; information about other cryptographic standards for the united states can also be found at that site more complete coverage of ssl 3.0 can be found at 669 http  / /home.netscape.com/eng/ssl3/ in 1999  ssl 3.0 was modified slightly and presented in an ietf request for comments  rfc  under the name tls  the example in section 15.6.3 illustrating the impact of false-alarm rate on the effectiveness of idss is based on axelsson  1999   the description of tripwire in section 15.6.5 is based on kim and spafford  1993   research into system-call-based anomaly detection is described in forrest et al  1996   part seven a distributed system is a collection of processors that do not share memory or a clock instead  each processor has its own local memory  and the processors communicate with one another through communication lines such as local-area or wide-area networks the processors in a distributed system vary in size and function such systems may include small handheld or real-time devices  personal computers  workstations  and large mainframe computer systems  a distributed file system is a file-service system whose users  servers  and storage devices are dispersed among the sites of a distributed system accordingly  service activity has to be carried out across the network ; instead of a single centralized data repository  there are multiple independent storage devices  the benefits of a distributed system include giving users access to the resources maintained by the system and thereby speeding up computation and improving data availability and reliability because a system is distributed  however  it must provide mechanisms for process synchronization and communication  for dealing with the deadlock problem  and for handling failures that are not encountered in a centralized system  16.1 a distributed system is a collection of processors that do not share memory or a clock instead  each processor has its own local memory the processors communicate with one another through various communication networks  such as high-speed buses or telephone lines in this chapter  we discuss the general structure of distributed systems and the networks that interconnect them we contrast the main differences in operating-system design between these systems and centralized systems in chapter 17  we go on to discuss distributed file systems then  i11 chapter 18  we describe the methods necessary for distributed operating systems to coordinate their actions  to provide a high-level overview of distributed systems and the networks that interconnect them  to discuss the general structure of distributed operating systems  a is a collection of loosely coupled processors interconnected by a communication network from the point of view of a specific processor in a distributed system  the rest of the processors and their respective resources are remote  whereas its own resources are local  the processors in a distributed system may vary in size and function  they may include small microprocessors  workstations  minicomputers  and large general-purpose cornputer systems these processors are referred to by a number of names  such as sites  nodes  computers  machines  and hosts  depending on the context in which they are mentioned we mainly use site to indicate the location of a machine and host to refer to a specific system at a site generally  one host at one site  the server  has a resource that another host at another site  the client  or user   would like to use a general structure of a distributed system is shown in figure 16.1  673 674 chapter 16 site a site c network communication site b figure 16.1 a distributed system  d d d d l resources l there are four major reasons for building distributed systems  resource sharing  computation speedup  reliability  and communication in this section  we briefly discuss each of them  16.1.1 resource sharing if a number of different sites  with different capabilities  are connected to one another  then a user at one site may be able to use the resources available at another for example  a user at site a may be using a laser printer located at site b meanwhile  a user at b may access a file that resides at a in general  in a distributed system provides mechanisms for sharing files at remote sites  processing information in a distributed database  printing files at remote sites  using remote specialized hardware devices  such as a high-speed array processor   and performing other operations  16.1.2 computation speedup if a particular computation can be partitioned into subcomputations that can run concurrently  then a distributed system allows us to distribute the subcomputations among the various sites ; the subcomputations can be run concurrently and thus provide in addition  if a particular site is currently overloaded with jobs  some of them can be moved to other  lightly loaded sites this movement of jobs is called automated load sharing  in which the distributed operating system automatically moves jobs  is not yet comnlon in commercial systems  16.1.3 reliability if one site fails in a distributed system  the remammg sites can continue operating  giving the system better reliability if the system is composed of multiple large autonomous installations  that is  general-purpose computers   the failure of one of them should not affect the rest if  however  the system 16.2 16.2 675 is composed of sncall machines  each of which is responsible for some crucial system function  such as tenninal character i/0 or the file system   then a single failure may halt the operation of the whole system in general  with enough redundancy  in both hardware and data   the system can continue operation  even if some of its sites have failed  the failure of a site must be detected by the system  and appropriate action may be needed to recover from the failure the system must no longer use the services of that site in addition  if the function of the failed site can be taken over by another site  the system must ensure that the transfer of function occurs correctly finally  when the failed site recovers or is repaired  mechanisms must be available to integrate it back into the system smoothly as we shall see in chapters 17 and 18  these actions present difficult problems that have many possible solutions  16.1.4 communication when several sites are connected to one another by a communication network  users at the various sites have the opportunity to exchange information at a low level  are passed between systems  much as messages are passed between processes in the single-computer message system discussed in section 3.4 given message passing  all the higher-level flmctionality found in standalone systems can be expanded to encompass the distributed system  such functions include file transfer  login  mail  and remote procedure calls  rpcs   the advantage of a distributed system is that these functions can be carried out over great distances two people at geographically distant sites can collaborate on a project  for example by transferring the files of the project  logging in to each other 's remote systems to run programs  and exchanging mail to coordinate the work  users minimize the limitations inherent in longdistance work we wrote this book by collaborating in such a manner  the advantages of distributed systems have resulted in an industry-wide trend toward dovmslzing many companies are replacing their mainframes with networks of workstations or personal computers companies get a bigger bang for the buck  that is  better functionality for the cost   more flexibility in locating resources and expanding facilities  better user interfaces  and easier maintenance  in this section  we describe the two general categories of network-oriented operating systems  network operating systems and distributed operating systems network operating systems are simpler to implement but generally more difficult for users to access and utilize than are distributed operating systems  which provide more features  16.2.1 network operating systems a operating provides an environment in which users  who are aware of the multiplicity of machines  can access remote resources by either 676 chapter 16 logging in to the appropriate remote machine or transferring data from the remote machine to their own machines  16.2.1.1 remote login an important function of a network operating system is to allow users to log in remotely the internet provides the telnet facility for this p1.npose to illustrate this facility  lets suppose that a user at westminster college wishes to compute on cs.yale.edu  a computer that is located at yale university to do so  the user must have a valid account on that machine to log in remotely  the user issues the command telnet cs.yale.edu this command results in the formation of a socket connection between the local machine at westminster college and the cs.yale.edu computer after this connection has been established  the networking software creates a transparent  bidirectional link so that all characters entered by the user are sent to a process on cs.yale.edu and all the output from that process is sent back to the user the process on the remote machine asks the user for a login name and a password  once the correct information has been received  the process acts as a proxy for the use1 ~ who can compute on the remote machine just as any local user can  16.2.1.2 remote file transfer another major function of a network operating system is to provide a mechanism for remote file transfer from one machine to another in such an enviromnent  each computer maintains its own local file system if a user at one site  say  cs.uvm.edu  wants to access a file located on another computer  say  cs.yale.edu   then the file must be copied explicitly from the computer at yale to the computer at the university of vermont  the internet provides a mechanism for such a transfer with the file transfer protocol  ftp  program suppose that a user on cs.uvm.edu wants to copy a java program server java that resides on cs.yale.edu the user must first invoke the ftp program by executing ftp cs.yale.edu the program then asks the user for a login name and a password once the correct information has been received  the user must connect to the subdirectory where the file server java resides and then copy the file by executing get server java in this scheme  the file location is not transparent to the user ; users must know exactly where each file is moreover  there is no real file sharing  because a user can only copy a file from one site to another thus  several copies of the same file may exist  resulting in a waste of space tn addition  if these copies are modified  the vario-us copies will be inconsistent  16.2 677 notice that  in our example  the user at the university of vermont must have login permission on cs.yale.edu ptp also provides a way to allow a user who does not have an account on the yale computer to copy files remotely this remote copying is accomplished through the anonymous ft'p method  which works as follows the file to be copied  that is  server java  must be placed in a special subdirectory  say  jtp  with the protection set to allow the public to read the file a user who wishes to copy the file uses the ftp command as before when the user is asked for the login nan'le  the user supplies the name anonymous and an arbitrary password  once anonymous login is accomplished  care must be taken by the system to ensure that this partially authorized user does not access inappropriate files generally  the user is allowed to access only those files that are in the directory tree of user anonymous any files placed here are accessible to any anonymous users  subject to the usual file-protection scheme used on that machine anonymous users  however  cam'lot access files outside of this directory tree  implementation of the ftp mechanism is similar to telnet implementation  a daemon on the remote site watches for requests to coru'lect to the system 's ptp port login authentication is accomplished  and the user is allowed to execute commands remotely unlike the telnet daemon  which executes any command for the user  the ptp daemon responds only to a predefined set of file-related commands these include the following  get-transfer a file from the remote machine to the local machine  put-transfer from the local machine to the remote machine  ls or dir-list files in the current directory on the remote machine  cd -change the current directory on the remote machine  there are also various commands to change transfer modes  for binary or ascii files  and to determine connection status  an important point about telnet and ptp is that they require the user to change paradigms ptp requires the user to know a command set entirely different from the normal operating-system commands telnet requires a smaller shift  the user must know appropriate commands on the remote system  for instance  a user on a windows machine who teh'lets to a unix machine must switch to unix commands for the duration of the telnet session facilities are more convenient for users if they do not require the use of a different set of commands distributed operating systems are designed to address this problem  16.2.2 distributed operating systems in a distributed operating system  users access remote resources in the same way they access local resources data and process migration from one site to another is under the control of the distributed operating system  16.2.2.1 data migration suppose a user on site a wants to access data  such as a file  that reside at site b the system can transfer the data by one of two basic methods one approach 678 chapter 16 to is to transfer the entire file to site a from that point on  all access to the file is local when the user no longer needs access to the file  a copy of the file  if it has been modified  is sent back to site b even if only a modest change has been made to a large file  all the data must be transferred  this mechanism can be thought of as an automated ftp system this approach was used in the andrew file system  as we discuss in chapter 17  but it was found to be too inefficient  the other approach is to transfer to site a only those portions of the file that are actually necessary for the immediate task if another portion is required later  another transfer will take place when the user no longer wants to access the file  any part of it that has been modified must be sent back to site b  note the similarity to demand paging  the sun microsystems network file system  nfs  protocol uses this method  chapter 17   as do newer versions of andrew  the microsoft smb protocol  running on top of either tcp /ip or the microsoft netbeui protocol  also allows file sharing over a network smb is described in appendix c.6.1  clearly  if only a small part of a large file is being accessed  the latter approach is preferable if significant portions of the file are being accessed  however  it is more efficient to copy the entire file in both methods  data migration includes more than the mere transfer of data from one site to another  the system must also perform various data translations if the two sites involved are not directly compatible  for instance  if they use different character-code representations or represent integers with a different number or order of bits   16.2.2.2 computation migration in some circumstances  we may want to transfer the computation  rather than the data  across the system ; this approach is called for example  consider a job that needs to access various large files that reside at different sites  to obtain a summary of those files it would be more efficient to access the files at the sites where they reside and return the desired results to the site that il itiated the computation generally  if the time to transfer the data is longer than the time to execute the remote cmmnand  the remote command should be used  such a computation can be carried out in different ways suppose that process p wants to access a file at site a access to the file is carried out at site a and could be il itiated by an rpc an rpc uses a  udp on the internet  to execute a routine on a remote system  section 3.6.2   process p invokes a predefilced procedure at site a the procedure executes appropriately and then returns the results to p  alternatively process p can send a message to site a the operatil g system at site a then creates a new process q whose function is to carry out the designated task when process q completes its execution  it sends the needed result back to p via the message system in this scheme  process p may execute concurrently with process q ; in fact  it may have several processes running concurrently on several sites  either method could be used to access several files residing at various sites  one rpc might result in the ilwocation of another rpc or even in the transfer of messages to another site similarly  process q could  duril g the course of its 16.3 16.3 679 execution  send a message to another site  which in turn would create another process this process might either send a message back to q or repeat the cycle  16.2.2.3 process migration a logical extension of computation migration is na  ' ' ~ c process is submitted for execution  it is not always executed at it is initiated the entire process  or parts of it  may be executed at different sites this scheme may be used for several reasons  load balancing the processes  or subprocesses  may be distributed across the network to even the workload  computation speedup if a single process can be divided into a number of subprocesses that can run concurrently on different sites  then the total process turnaround time can be reduced  hardware preference the process may have characteristics that make it more suitable for execution on some specialized processor  such as matrix inversion on an array processor  rather than on a microprocessor  software preference the process may require software that is available at only a particular site  and either the software can not be moved  or it is less expensive to move the process  data access just as in computation migration  if the data being used in the computation are numerous  it may be more efficient to have a process run remotely than to transfer all the data  we use two complementary techniques to move processes in a computer network in the first  the system can attempt to hide the fact that the process has migrated from the client this scheme has the advantage that the user does not need to code her program explicitly to accomplish the migration this method is usually employed for achieving load balancing and computation speedup among homogeneous systems  as they do not need user input to help them execute programs remotely  the other approach is to allow  or require  the user to specify explicitly how the process should migrate this method is usually employed when the process must be moved to satisfy a hardware or software preference  you have probably realized that the web has many aspects of a distributedcomputing environment certainly it provides data migration  between a web server and a web client   it also provides computation migration for instance  a web client could trigger a database operation on a web server finally  with java  it provides a form of process migration  java applets are sent from the server to the client  where they are executed a network operating system provides most of these features  but a distributed operating system makes them seamless and easily accessible the result is a powerful and easy-to-use facility-one of the reasons for the huge growth of the world wide web  there are basically two types of networks  and the main difference between the two is the way in 680 chapter 16 which they are geographically distributed local-area networks are composed of processors distributed over small areas  such as a single building or a number of adjacent buildings   whereas wide-area networks are composed of a number of autonomous processors distributed over a large area  such as the united states   these differences imply major variations in the speed and reliability of the communications networks  and they are reflected in the distributed operating-system design  16.3.1 local-area networks local-area networks emerged in the early 1970s as a substitute for large mainframe computer systems for many enterprises  it is more economical to have a number of small computers  each with its own self-contained applications  than to have a single large system because each small computer is likely to need a full complement of peripheral devices  such as disks and printers   and because some form of data sharing is likely to occur in a single enterprise  it was a natural step to connect these small systems into a network  lans  as mentioned  are usually designed to cover a small geographical area  such as a single building or a few adjacent buildings  and are generally used in an office environment all the sites in such systems are close to one another  so the communication links tend to have a higher speed and lower error rate than do their cou.rjerparts in wide-area networks high-quality  expensive  cables are needed to attain this higher speed and reliability it is also possible to use the cable exclusively for data network traffic over longer distances  the cost of using high-quality cable is enormous  and the exclusive use of the cable tends to be prohibitively expensive  the most conunon links in a local-area network are twisted-pair and fiberoptic cabling the most common configurations are multiaccess bus  ring  and star networks communication speeds range from 1 megabit per second  for networks such as appletalk  infrared  and the new bluetooth local radio network  to 1 gigabit per second for ethernet ten megabits per second is the speed of requires a higher-quality cable but runs at 100 m ~ egabits per second and is common also growing is the use of optical-fiber-based fddi networking the fddi network is token-based and runs at over 100 megabits per second  a typical lan may consist of a number of different computers  from mainframes to laptops or pdas   various shared peripheral devices  such as laser printers and magnetic-tape drives   and one or more gateways  specialized processors  that provide access to other networks  figure 16.2   an ethernet scheme is commonly used to construct lans an ethernet network has no central controller  because it is a multiaccess bus  so new hosts can be added easily to the network the ethernet protocol is defined by the ieee 802.3 standard  there has been significant growth in using the wireless spectrum for designing local-area networks wireless  or wifi  networks allow constructing a network using only a wireless router for transmitting signals between hosts  each host has a wireless adapter networking card which allows it to join and use the wireless network however  where ethernet systems often run at 100 megabits per second  wifi networks typically run at slower speeds there are 16.3 681 workstation workstation workstation printer laptop file server figure 16.2 local-area network  several ieee standards for wireless networks  802.11g can theoretically run at 54 megabits per second  although ilc practice data rates are often less than half that amount the recent 802.11n standard provides theoretically much higher data rates than 802.11g  although in actual practice 802.11n networks have typical data rates of around 75 megabits per second data rates of wireless networks are heavily influenced by the distance between the wireless router and the host as well as interference in the wireless spectrum wireless networks often have a physical advantage over wired ethernet networks as no cabling needs to be run to connect communicatilcg hosts as a result  wireless networks are popular in homes as well as public areas such as libraries and internet cafes  16.3.2 wide-area networks wide-area networks emerged in the late 1960s  mainly as an academic research project to provide efficient communication among sites  allowing hardware and software to be shared conveniently and economically by a wide community of users the first wan to be designed and developed was the arpanet begun in 1968  the arpanet has grown from a four-site experimental network to a worldwide network of networks  the internet  comprising millions of computer systems  because the sites in a wan are physically distributed over a large geographical area  the communication links are  by default  relatively slow and unreliable  typical links are telephone lines  leased  dedicated data  lines  microwave links  and satellite channels these communication links are controlled by special  figure 16.3   which are responsible for defilcing the interface through which the sites communicate over the network  as well as for transferring information among the various sites  682 chapter 16 communication subsystem h h netwot k host communication processor figure 16.3 communication processors in a wide-area network  for example  the internet wan enables hosts at geographically separated sites to communicate with one another the host computers typically differ from one another in type  speed  word length  operatil1.g system  and so on hosts are generally on lans  which are  in turn  connected to the internet via regional networks the regional networks  such as nsfnet il1  the northeast united states  are interlinked with  section 16.5.2  to form the worldwide network connections between networks frequently use a telephone-system service called t1  which provides a transfer rate of 1.544 megabits per second over a leased line for sites requiring faster internet access  tls are collected into multiple-t1 units that work in parallel to provide more throughput for instance  a t3 is composed of 28 t1 connections and has a transfer rate of 45 megabits per second the routers control the path each message takes through the net this routing may be either dynamic  to increase commmlication efficiency  or static  to reduce security risks or to allow communication charges to be computed  other wans use standard telephone lines as their primary means of communication  are devices that accept digital data from the computer side and convert it to the analog signals that the telephone system uses a modem at the destination site converts the analog signal back to digital form  and the destination receives the data the unix news network  uucp  allows systems to communicate with each other at predetermined times  via modems  to exchange messages the messages are then routed to other nearby systems and in this way either are propagated to all hosts on the network  public messages  or are transferred to specific destinations  private messages   wans are generally slower than lans ; their transmission rates range from 1,200 bits 16.4 16.4 683 per second to over 1 megabit per second uucp has been superseded by ppp  the point-to-point protocol ppp functions over modem coru1ections  allowing home computers to be fully connected to the internet  the sites in a distributed system can be connected physically in a variety of ways each configuration has advantages and disadvantages we can compare the configurations by using the following criteria  installation cost the cost of physically linking the sites in the system communication cost the cost in time and money to send a message from site a to site b availability the extent to which data can be accessed despite the failure of some links or sites the various topologies are depicted in figure 16.4 as graphs whose nodes correspond to sites an edge from node a to node b corresponds to a direct communication link between the two sites in a fully connected network  each site is directly connected to every other site however  the number of links grows as the square of the number of sites  resulting in a huge installation cost  therefore  fully connected networks are impractical in any large system  in a pc  ntially direct links exist between some-but not all-pairs of sites hence  the installation cost of such a configuration is lower than that of the fully connected network however  if two sites a and b are not directly connected  messages from one to the other must be through a sequence of communication links this requirement results in a higher communication cost  if a communication link fails  messages that would have been transmitted across the link must be rerouted in some cases  another route through the network may be found  so that the messages are able to reach their destination  in other cases  a failure may mean that no connection exists between some pair  or pairs  of sites when a system is split into two  or more  unconnected subsystems  it is partitioned under this definition  a subsystem  or partition  may consist of a single node  the various partially connected network types include tree-structured networks  ring networks  and star networks  as shown in figure 16.4 these types have different failure characteristics and installation and communication costs installation and communication costs are relatively low for a treestructured network however  the failure of a single link in such a network can result in the network 's becoming partitioned in a ring network  at least two links must fail for partition to occur thus  the ring network has a higher degree of availability than does a tree-structured network however  the communication cost is high  since a message may have to cross a large number of links in a star network  the failure of a single link results in a network partition  but one of the partitions has only a single site such a partition can be treated as a single-site failure the star network also has a low communication cost  since each site is at most two links away from every other site howeve1 ~ 684 chapter 16 16.5 fully connected network partially connected network b d f tree-structured network star network f ring network figure 16.4 network topology  if the central site fails  all the sites in the system become disconnected from one another  now that we have discussed the physical aspects of networking  we turn to the internal workings the designer of a communication network must address five basic issues  naming and name resolution how do two processes locate each other to communicate routing strategies how are messages sent through the network packet strategies are packets sent individually or as a sequence connection strategies how do two processes send a sequence of messages 16.5 685 contention how do we resolve conflicting demands for the network 's lise  given that it is a shared resource in the following sections  we elaborate on each of these issues  16.5.1 naming and name resolution the first component of network communication is the naming o the systems in the network for a process at site a to exchange information with a process at site b  each must be able to specify the other within a computer system  each process has a process identifier  and messages may be addressed with the process identifier beca use networked systems share no memory  however  a host within the system initially has no knowledge about the processes on other hosts  to solve this problem  processes on remote systems are generally identified by the pair host name  identifier  where host name is a name unique within the network and identifier may be a process identifier or other unique number within that host a host name is usually an alphanumeric identifier  rather than a number  to make it easier for users to specify for instance  site a might have hosts named homer  marge  bart  and lisa bart is certainly easier to remember than is 12814831100  names are convenient for humans to use  but computers prefer numbers for speed and simplicity for this reason  there must be a mechanism to !  '  the host name into a that describes the destination system to the networking hardware this mechanism is similar to the name-to-address binding that occurs during program compilation  linking  loading  and execution  chapter 8   in the case of host names  two possibilities exist first  every host may have a data file containing the names and addresses of all the other hosts reachable on the network  similar to binding at compile time   the problem with this model is that adding or removing a host from the network requires updati.n.g the data files on all the hosts the alternative is to distribute the information among systems on the network the network must then use a protocol to distribute and retrieve the information this scheme is like execution-time binding the first method was the one originally used on the internet ; as the internet rnarr-,,or it became untenable  so the second method  the domain-name ' ~ ' ' is now in use  dns specifies the naming structure of the hosts  as well as name-to-address resolution hosts on the internet are logically addressed with multipart names known as ip addresses the parts of an ip address progress frorn the most specific to the most general part  with periods separating the fields for instance  bob.cs.brown.edu refers to host bob in the depattment of science at brown university within the top-level domain edu  domains include com for commercial sites and for organizations  as well as connected to the for systems the resolves in reverse order each a a process on a a name and returns the address of the name server as the final the name server for the host in host-id is returned for a made communicate with bob.cs.brown.edu would result in 686 chapter 16 the kernel of system a issues a request to the name server for the edu domain  asking for the address of the name server for brown.edu the name server for the edu domain must be at a known address  so that it can be queried  the edu nance server returns the address of the host on which the brown.edu name server resides  the kernel on system a then queries the name server at this address and asks about cs.brown.edu  an address is returned ; and a request to that address for bob.cs.brown.edu now  finally  returns an host-id for that host  for example  128.148.31.100   this protocol may seem inefficient  but local caches are usually kept by each name server to speed the process for example  the edu name server would have brown.edu in its cache and would inform system a that it could resolve two portions of the name  returning a pointer to the cs.brown.edu name server  of course  the contents of these caches must be refreshed over time in case the name server is moved or its address changes in fact  this service is so important that many optimizations have occurred in the protocol  as well as many safeguards consider what would happen if the primary edu name server crashed it is possible that no edu hosts would be able to have their addresses resolved  making them all lmreachable ! the solution is to use secondary  back-up name servers that duplicate the contents of the primary servers  before the domain-name service was introduced  all hosts on the internet needed to have copies of a file that contained the names and addresses of each host on the network all changes to this file had to be registered at one site  host sri-nic   and periodically all hosts had to copy the updated file from sri-nic to be able to contact new systems or find hosts whose addresses had changed  under the domain-name service  each name-server site is responsible for updating the host information for that domain for instance  any host changes at brown university are the responsibility of the name server for brown.edu and need not be reported anywhere else dns lookups will automatically retrieve the updated information because they will contact brown.edu directly  within domains  there can be autonomous subdomains to further distribute the responsibility for host-name and host-id changes  java provides the necessary api to design a program that maps ip names to ip addresses the program shown in figure 16.5 is passed an ip name  such as bob.cs.brown.edu  on the command line and either outputs the ip address of the host or returns a message indicating that the host name could not be resolved  an inetaddress is a java class representing an ip name or address the static method getbyname   belonging to the inetaddress class is passed a string representation of an ip name  and it returns the corresponding inetaddress  the program then invokes the gethostaddress   method  which internally uses dns to look up the ip address of the designated host  generally  the operating system is responsible for accepting from its processes a message destined for host name  identifier and for transferring that message to the appropriate host the kernel on the destination host is then responsible for transferring the message to the process named by the identifier  this exchange is by no means trivial ; it is described in section 16.5.4  16.5 i usage  java dnslookup ip name i.e java dnslookup www.wiley.com i public class dnslookup   public static void main  string   args   inetaddress hostaddress ; try   hostaddress = inetaddress.getbyname  args  o   ; system.out.println  hostaddress.gethostaddress    ; catch  unknownhostexception uhe     system err println  unknown host  + args  0   ; figure 16.5 java program illustrating a dns lookup  16.5.2 routing strategies 687 when a process at site a wants to communicate with a process at site b  how is the message sent if there is only one physical path from a to b  such as in a star or tree-structured network   the message must be sent through that path however  if there are multiple physical paths from a to b  then several routing options exist each site has a indicating the alternative paths that can be used to send a message to other sites the table may include information about the speed and cost of the various communication paths  and it may be updated as necessary  either manually or via programs that exchange routing information the three most common routing schemes are td ~ .-i  ja  and fixed routing a path from a to b is specified in advance and does not change unless a hardware failure disables it usually  the shortest path is chosen  so that communication costs are minimized  virtual routing a path from a to b is fixed for the duration of one different sessions involving messages from a to b may use different paths  a session could be as short as a file transfer or as long as a remote-login period  dynamic routing the path used to send a message from site a to site b is chosen only when the message is sent because the decision is made dynamically  separate messages may be assigned different paths site a will make a decision to send the message to site c ; c  in turn  will decide to send it to sited  and so on eventually  a site will deliver the message to b usually  a site sends a message to another site on whatever link is the least used at that particular time  there are tradeoffs among these three schem.es fixed routing can not adapt to link failures or load changes in other words  if a path has been established 688 chapter 16 between a and b  the messages must be sent along this path  even if the path is down or is used more heavily than another possible path we can partially remedy this problem by using virtual routing and can avoid it completely by using dynamic routing fixed routing and virtual routing ensure that ncessages from a to b will be delivered in the order in which they were sent in dynamic routing  messages may arrive out of order we can remedy this problem by appending a sequence number to each message  dynamic routing is the most complicated to set up and run ; however  it is the best way to manage routing in complicated environments unix provides both fixed routing for use on hosts within simple networks and dynamic routing for complicated network environments it is also possible to mix the two within a site  the hosts may just need to know how to reach the system that connects the local network to other networks  such as company-wide networks or the internet   such a node is known as a each individual host has a static route to the gateway  although the gateway itself uses dynamic routing to reach any host on the rest of the network  a router is the entity within the computer network responsible for routing messages a router can be a host computer with routing software or a special-purpose device either way  a router must have at least two network cmmections  or else it would have nowhere to route messages a router decides whether any given message needs to be passed from the network on which it is received to any other network connected to the router it makes this determination by examining the destination internet address of the message  the router checks its tables to determine the location of the destination host  or at least of the network to which it will send the message toward the destination host in the case of static routing  this table is changed only by manual update  a new file is loaded onto the router   with dynamic routing  a is used between routers to inform them of network changes and to allow them to update their routing tables automatically gateways and routers typically are dedicated hardware devices that run code out of firmware  16.5.3 packet strategies messages generally vary in length to simplify the system design  we commonly implement communication with fixed-length messages called or a communication incplemented in one packet can be sent to its destination in a a connectionless message can be in which case the sender has no guarantee that  and can not tell whether  the packet reached its destination alternatively  the packet can be usually  in this case  a packet is returned from the destination indicating that the packet arrived  of course  the return packet could be lost along the way  if a message is too long to fit within one packet  or if the packets need to how back and forth between the two communicators  a connection is established to allow the reliable exchange of multiple packets  16.5.4 connection strategies c ~ uuc'' ' ~ u are able to reach their destinations  processes can institute to exchange information pairs of processes that want to communicate over the network can be connected in a number of ways  16.5 689 the three most common schemes are and circuit switching if two processes want to con1municate  a permanent physical link is established between them tl1is link is allocated for the duration of the communication session  and no other process can use that link during this period  even if the two processes are not actively communicating for a while   this scheme is similar to that used in the telephone system once a communication line has been opened between two parties  that is  party a calls party b   no one else can use this circuit until the communication is terminated explicitly  for example  when the parties hang up   message switching if two processes want to communicate  a temporary link is established for the duration of one message transfer physical links are allocated dynamically among correspondents as needed and are allocated for only short periods each message is a block of data with system information-such as the source  the destination  and errorcorrection codes  ecc  -that allows the communication network to deliver the message to the destination correctly this scheme is similar to the post-office mailing system each letter is a message that contains both the destination address and source  return  address many messages  from different users  can be shipped over the same link  packet switching one logical message may have to be divided into a number of packets each packet may be sent to its destination separately  and each therefore must include a source and a destination address with its data furthermore  the various packets may take different paths through the network the packets must be reassembled into messages as they arrive note that it is not harmful for data to be broken into packets  possibly routed separately  and reassembled at the destination breaking up an audio signal  say  a telephone communication   in contrast  could cause great confusion if it was not done carefully  there are obvious tradeoffs among these schemes circuit switching requires substantial set-up time and may waste network bandwidth  but it incurs less overhead for shipping each message conversely  message and packet switching require less set-up time but incur more overhead per message also  in packet switching  each message must be divided into packets and later reassembled packet switching is the method most commonly used on data networks because it makes the best use of network bandwidth  16.5.5 contention depending on the network topology  a link may cmmect more than two sites in the computer network  and several of these sites may want to transmit information over a link simultaneously this situation occurs mainly in a ring or multiaccess bus network in this case  the transmitted information may become scrambled if it does  it must be discarded ; and the sites must be notified about the problem so that they can retransmit the information if no special provisions are made  this situation may be repeated  resulting in degraded performance  690 chapter 16 16.6 several techniques have been developed to avoid repeated collisions  including collision detection and token passing  csma/cd before transmitting a message over a link  a site must listen to determine whether another message is currently being transmitted over that link ; this technique is called -uvith  if the link is free  the site can start transmitting otherwise  it must wait  and continue to listen  until the link is free if two or more sites begin transmitting at exactly the same time  each thinking that no other site is using the link   then they will register a and will stop transmitting each site will try again after some random time interval  the main problem with this approach is that  when the system is very busy  many collisions may occur  and thus performance may be degraded  nevertheless  csma/cd has been used successfully in the ethernet system  the most common local area network system one strategy for limiting the number of collisions is to limit the number of hosts per ethernet network  adding more hosts to a congested network could result in poor network throughput as systems get faster  they are able to send more packets per time segment as a result  the number of systems per ethernet network generally is decreasing so that networking performance is kept reasonable  token passing a unique message type  known as a continuously circulates in the system  usually a ring structure   a site that wants to transmit information must wait until the token arrives it then removes the token from the ring and begins to transmit its messages when the site completes its round of message passing  it retransmits the token this action  in turn  allows another site to receive and remove the token and to start its message transmission if the token gets lost  the system must detect the loss and generate a new token it usually does that by declaring an to choose a unique site where a new token will be generated  later  in section 18.6  we present one election algorithm a token-passing scheme has been adopted by the ibm and hp i apollo systems the benefit of a token-passing network is that performance is constant adding new sites to a network may lengthen the waiting time for a token  but it will not cause a large performance decrease  as may happen on ethernet on lightly loaded networks  however  ethernet is more efficient  because systems can send messages at any time  when we are designing a communication network  we must deal with the inherent complexity of coordinating asynchronous operations communicating in a potentially slow and error-prone environment in addition  the systems on the network must agree on a protocol or a set of protocols for determining host names  locating hosts on the network  establishing connections  and so on we can simplify the design problem  and related implementation  by partitioning the problem into multiple layers each layer on one system communicates with the equivalent layer on other systems typically  each layer has its own protocols  and communication takes place between peer layers 16.6 691 network environment iso environment real systems environment figure 16.6 two computers communicating via the iso network model  using a specific protocol the protocols may be implemented in hardware or software for instance  figure 16.6 shows the logical communications between two computers  with the three lowest-level layers implemented in hardware  following the international standards organization  iso   we refer to the layers as follows  physical layer the physical layer is responsible for handling both the mechanical and the electrical details of the physical transmission of a bit stream at the physical layer  the communicating systems must agree on the electrical representation of a binary 0 and 1  so that when data are sent as a stream of electrical signals  the receiver is able to interpret the data properly as binary data this layer is implemented in the hardware of the networking device  data-link layer the data-link layer is responsible for handlingfi'ames  or fixed-length parts of packets  including any error detection and recovery that occurs in the physical layer  network layer the network layer is responsible for providing connecti01cs and for routing packets in the communication network  including handling the addresses of outgoing packets  decoding the addresses of incoming packets  and maintaining routing information for proper response to changing load levels routers work at this layer  transport layer the transport layer is responsible for low-level access to the network and for transfer of messages between clients  including partitioning messages into packets  maintaining packet order  controlling flow  and generating physical addresses  session layer the session layer is responsible for implementing sessions  or process-to-process communication protocols typically  these protocols are the actual communications for remote logins and for file and mail transfers  692 chapter 16 presentation layer the presentation layer is responsible for resolving the differences in formats among the various sites in the network  including character conversions and half duplex-full duplex modes  character echoing   application layer the application layer is responsible for interacting directly with users this layer deals with file transfe1 ~ remote-login protocols  and electronic mail  as well as with schemas for distributed databases  figure 16.7 summarizes the set of cooperating protocols-showing the physical flow of data as mentioned  logically each layer of a protocol stack communicates with the equivalent layer on other systems but physically  a message starts at or above the application layer and end-user application process distributed information transfer-syntax negotiation data-representation transformations dialog and synchronization control for application entities network-independent message-interchange service j end-to ~ end message transfer  connection management  error control  fragmentation  flow control  network routing  addressing  call set-up and clearing application layer presentation layer session layer transport layer network layer data-link control  framing  data transparency  error control  link layer mechanical and electrical networkcinterface connections physical connection to network termination equipment physical layer 16.7 the iso protocol stack  16.6 data-link -layer header network-layer header transport-layer header f-------1 session-layer header f-------1 presentation layer f-------1 application layer message l_ _ _____j data-link -layer trailer figure 16.8 an iso network message  693 is passed through each lower level in turn each layer may modify the message and il1.clude message-header data for the equivalent layer on the receiving side ultimately  the message reaches the data-network layer and is transferred as one or more packets  figure 16.8   the data-lil1.k layer of the target system receives these data  and the message is moved up through the protocol stack ; it is analyzed  modified  and stripped of headers as it progresses it fu1.ally reaches the application layer for use by the receiving process  the iso model formalizes some of the earlier work done in network protocols but was developed in the late 1970s and is currently not in widespread use perhaps the most widely adopted protocol stack is the tcp /ip model  which has been adopted by virtually all internet sites the tcp /ip protocol stack has fewer layers than does the iso model theoretically  because it combilles several functions ill each layer  it is more difficult to implement but more efficient than iso networking the relationship between the iso and tcp /ip models is shown in figure 16.9 the tcp /ip application layer identifies several protocols ill widespread use ill the internet  illcluding http  ftp  telnet  dns  and smtp the transport layer identifies the unreliable  connectionless user datagram protocol  udp  and the reliable  connection-oriented transmission control protocol  tcp   the internet protocol  ip  is responsible for routing ip datagrams through the internet the tcp /ip model does not formally identify a link or physical laye1 ~ allowing tcp /ip traffic to run across any physical network in section 16.9  we consider the tcp /ip model running over an ethernet network  security should be a concern in the design and implementation of any modern communication protocol both strong authentication and encryption are needed for secure communication strong authentication ensures that the sender and receiver of a communication are who or what they are supposed to be encryption protects the contents of the communication from eavesdropping weak authentication and clear-text communication are still very common  however  for a variety of reasons when most of the 694 chapter 16 16.7 iso presentation session physical tcp/ip http  dns  telnet smtp  ftp not defined not defined tcp-udp not defined not defined figure 16.9 the iso and tcp/ip protocol stacks  common protocols were designed  security was frequently less important than performance  simplicity  and efficiency  strong authentication requires a multistep handshake protocol or authentication devices  adding complexity to a protocol modern cpus can efficiently perform encryption  and systems frequently offload encryption to separate cryptography processors  so system performance is not compromised longdistance communication can be made secure by authenticating the endpoints and encrypting the stream of packets in a virtual private network  as discussed in 15.4.2 lan communication remains unencrypted at most sites  but protocols such as nfs version 4  which includes strong native authentication and encryption  should help improve even lan security  a distributed system may suffer from various types of hardware failure the failure of a link  the failure of a site  and the loss of a message are the most common types to ensure that the system is robust  we must detect any of these failures  reconfigure the system so that computation can continue  and recover when a site or a link is repaired  16.7.1 failure detection in an environment with no shared memory  we are generally unable to differentiate among link failure  site failure  and message loss we can usually detect only that one of these failures has occurred once a failure has been 16.7 695 detected  appropriate action must be taken what action is appropriate depends on the particular application  to detect link and site failure  we use a procedure suppose that sites a and b have a direct physical link between them  at fixed intervals  the sites send each other an j-am-up m.essage if site a does not receive this message within a predetermined time period  it can assume that site b has failed  that the link between a and b has failed  or that the message from b has been lost at this point  site a has two choices it can wait for another time period to receive an j-am-up message from b  or it can send an are-you-up message to b  if time goes by and site a still has not received an j-am-up message  or if site a has sent an are-you-up message and has not received a reply  the procedure can be repeated again  the only conclusion that site a can draw safely is that some type of failure has occurred  site a can try to differentiate between link failure and site failure by sending an are-you-up message to b by another route  if one exists   if and when b receives this message  it immediately replies positively this positive reply tells a that b is up and that the failure is in the direct link between them since we do not know in advance how long it will take the message to travel from a to b and back  we must use a at the time a sends the are-you-up message  it specifies a time interval during which it is willing to wait for the reply from b if a receives the reply message within that time interval  then it can safely conclude that b is up if not  however  that is  if a time-out occurs   then a may conclude only that one or more of the following sih1ations has occurred  site b is down  the direct link  if one exists  from a to b is down  the alternative path from a to b is down  the message has been lost  site a can not  however  determine which of these events has occurred  16.7.2 reconfiguration suppose that site a has discovered  through the mechanism described in the previous section  that a failure has occurred it must then initiate a procedure that will allow the system to reconfigure and to continue its normal mode of operation  if a direct link from a to b has failed  this information must be broadcast to every site in the system  so that the various routing tables can be updated accordingly  if the system believes that a site has failed  because that site can be reached no longer   then all sites in the system must be so notified  so that they will no longer attempt to use the services of the failed site the failure of a site that serves as a central coordinator for some activity  such as deadlock detection  requires the election of a new coordinator similarly  if the failed 696 chapter 16 site is part of a logical ring  then a new logical ring must be constructed  note that  if the site has not failed  that is  if it is up but camwt be reached   then we may have the undesirable situation in which two sites serve as the coordinator when the network is partitioned  the two coordinators  each for its own partition  may initiate conflicting actions for example  if the coordinators are responsible for implementing mutual exclusion  we may have a situation in which two processes are executing simultaneously in their critical sections  16.7.3 recovery from failure when a failed link or site is repaired  it must be integrated into the system gracefully and smoothly  suppose that a link between a and b has failed wlcen it is repaired  both a and b must be notified we can accomplish this notification by continuously repeating the handshaking procedure described in section 16.7.1  suppose that site b has failed wlcen it recovers  it must notify all other sites that it is up again site b then may have to receive information from the other sites to update its local tables ; for example  it may need routing-table information  a list of sites that are down  or mcdelivered messages and mail if the site has not failed but simply could not be reached  then this information is still required  16.7.4 fault tolerance a distributed system must tolerate a certain level of failure and continue to function normally when faced with various types of failures making a facility fault tolerant starts at the protocol level  as described above  but continues through all aspects of the system we use the term fault tolerance in a broad sense communication faults  machine failures  of type fail-stop where the machine stops before performing an erroneous operation that is visible to other processors   storage-device crashes  and decays of storage media should all be tolerated to some extent a should continue to function  perhaps in a degraded form  when faced with such failures the degradation can be in performance  in functionality  or in both it should be proportional  however  to the failures that caused it a system that grinds to a halt when only one of its components fails is certainly not fault tolerant  unfortunately  fault tolerance can be difficult and expensive to implement  at the network layer  multiple redundant communication paths and network devices such as switches and routers are needed to avoid a cmnmunication failure a storage failure can cause loss of the operating system  applications  or data storage units can include redundant hardware components that automatically take over from each other in case of failure in addition  raid systems can ensure continued access to the data even in the event of one or more disk failures  section 12.7   a system failure without redundancy can cause an application or an entire facility to stop operation the inost simple system failure involves a system running only stateless applications these applications can be restarted without 16.8 16.8 697 compromising the operation ; so as long as the applications can run on more than one computer  node   operation can continue such a facility is commonly known as a because it is computation-centric  in contrast  systems involve running applications that access and modify shared data as a result  data-centric computing facilities are more difficult to make fault tolerant they failure-monitoring software and special infrastructure for instance  such as veritas cluster and sun cluster include two or more computers and a set of shared disks any given application can be stored on the computers or on the shared disk  but the data must be stored on the shared disk the running application 's node has exclusive access to the application 's data on disk the application is monitored by the cluster software  and if it fails it is automatically restarted  if it camwt be restarted  or if the entire computer fails  the node 's exclusive access to the application 's data is terminated and is granted to another node in the cluster the application is restarted on that new node the application loses whatever state information was in the failed system 's memory but can continue based on whatever state it last wrote to the shared disk from a user 's point of view  a service was interrupted and then restarted  possibly with some data missing  specific applications may improve on this functionality by implementing lock management along with clustering with lock management  section 18.4.1   the application can run on multiple nodes and can use the same data on shared disks concurrently clustered databases frequently implement this functionality if anode fails  transactions can continue on other nodes  and users notice no interruption of service  as long as the client is able to automatically locate the other nodes in the cluster any noncommitted transactions on the failed node are lost  but again  client applications can be designed to retry noncommitted transactions if they detect a failure of their database node  making the multiplicity of processors and storage devices to the users has been a key challenge to many designers ideally  a distributed system should look to its users like a conventional  centralized system the user interface of a transparent distributed system should not distinguish between local and remote resources that is  users should be able to access remote resources as though these resources were local  and the distributed system should be responsible for locating the resources and for arranging for the appropriate interaction  another aspect of transparency is user mobility it would be convenient to allow users to log into any machine in the system rather than forcing them to use a specific machine a transparent distributed system facilitates user mobility by bringiicg over the user 's environment  for example  home directory  to wherever he logs in both the andrew file system from cmu and project athena from mit provide this functionality on a large scale ; nfs can provide it on a smaller scale  still another issue is l ;  -the capability of a system to adapt to increased service load systems have bounded resources and can become completely saturated under increased load for example  with respect to a file 698 chapter 16 system  saturation occurs either when a server 's cpu runs at a high utilization rate or when disks are almost full scalability is a relative property  but it can be measured accurately a scalable system reacts more gracefully to increased load than does a nonscalable one first  its performance degrades more moderately ; and second  its resources reach a saturated state later even perfect design can not accommodate an ever-growing load adding new resources might solve the problem  but it might generate additional indirect load on other resources  for example  adding machines to a distributed system can clog the network and increase service loads   even worse  expanding the system can call for expensive design modifications a scalable system should have the potential to grow without these problems in a distributed system  the ability to scale up gracefully is of special importance  since expanding the network by adding new machines or interconnecting two networks is commonplace in short  a scalable design should withstand high service load  accommodate growth of the user community  and enable simple integration of added resources  scalability is related to fault tolerance  discussed earlier a heavily loaded component can become paralyzed and behave like a faulty component also  shifting the load from a faulty component to that component 's backup can saturate the latter generally  having spare resources is essential for ensuring reliability as well as for handling peak loads gracefully an inherent advantage of a distributed system is a potential for fault tolerance and scalability because of the multiplicity of resources however  inappropriate design can obscure this potential fault-tolerance and scalability considerations call for a design demonstrating distribution of control and data  very large-scale distributed systems  to a great extent  are still only theoretical no magic guidelines ensure the scalability of a system it is easier to point out why current designs are not scalable we next discuss several designs that pose problems and propose possible solutions  all in the context of scalability  one principle for designing very large-scale systems is that the service demand from any component of the system should be bounded by a constant that is independent of the number of nodes in the system any service mechanism whose load demand is proportional to the size of the system is destined to become clogged once the system grows beyond a certain size  adding more resources will not alleviate such a problem the capacity of this mechanism simply limits the growth of the system  another principle concerns centralization central control schemes and central resources should not be used to build scalable  and fault-tolerant  systems examples of centralized entities are central authentication servers  central naming servers  and central file servers centralization is a form of functional asyrrunetry among machines constituting the system the ideal alternative is a functionally symmetric configuration ; that is  all the component machines have an equal role in the operation of the system  and hence each machine has some degree of autonomy practically  it is virtually impossible to comply with such a principle for instance  incorporating diskless machines violates functional symmetry  since the workstations depend on a central disk however  autonomy and symmetry are important goals to which we should aspire  deciding on the process structure of the server is a major problem in the design of any service servers are supposed to operate efficiently in peak 16.9 16.9 699 periods  when hundreds of active clients need to be served simultaneously a single-process server is certainly not a good choice  since whenever a request necessitates disk i/0  the whole service will be blocked assigning a process for each client is a better choice ; however  the expense of frequent context switches between the processes must be considered a related problem occurs because all the server processes need to share information  one of the best solutions for the server architecture is the use of lightweight processes  or threads  which we discuss in chapter 4 we can think of a group of lightweight processes as multiple threads of control associated with some shared resources usually  a lightweight process is not bound to a particular client instead  it serves single requests of different clients scheduling of threads can be preemptive or nonpreemptive if threads are allowed to run to completion  nonpreemptive   then their shared data do not need to be protected explicitly otherwise  some explicit locking mechanism must be used  clearly  some form of lightweight-process scheme is essential if servers are to be scalable  we now return to the name-resolution issue raised in section 16.5.1 and examine its operation with respect to the tcf /if protocol stack on the internet  we consider the processing needed to transfer a packet between hosts on different ethernet networks  in a tcf /if network  every host has a name and an associated if address  or host-id   both of these strings must be unique ; and so that the name space can be managed  they are segmented the name is hierarchical  as explained in section 16.5.1   describing the host name and then the organization with which the host is associated the host-id is split into a network number and a host number the proportion of the split varies  depending on the size of the network once the internet adrninistrators assign a network number  the site with that number is free to assign host-ids  the sending system checks its routing tables to locate a router to send the frame on its way the routers use the network part of the host-id to transfer the packet from its source network to the destination network the destination system then receives the packet the packet may be a complete message  or it may just be a component of a message  with more packets needed before the message can be reassembled and passed to the tcf /udf layer for transmission to the destination process  now we know how a packet moves from its source network to its destination within a network  how does a packet move from sender  host or router  to receiver ethernet device has a unique byte number  called the assigned to it for addressing two devices on a lan communicate with each other only with this number if a system needs to send data to another system  the networking software generates an containing the if address of the destination system this packet is to all other systems on that ethernet network  a broadcast uses a special network address  usually  the maximum address  to signal that all hosts should receive and process the packet the 700 chapter 16 broadcast is not re-sent by gateways  so only systems on the local network receive it only the system whose ip address matches the ip address of the arp request responds and sends back its mac address to the system that initiated the query for efficiency  the host caches the ip-mac address pair in an internal table the cache entries are so that an entry is eventually removed from the cache if an access to that system is not required within a given time in this way  hosts that are removed from a network are eventually forgotten for added performance  arp entries for heavily used hosts may be hardwired in the arp cache  once an ethernet device has announced its host-id and address  communication can begin a process may specify the name of a host with which to communicate networking software takes that name and determines the ip address of the target  using a dns lookup the message is passed from the application laye1 ~ through the software layers  and to the hardware layer at the hardware layer  the packet  or packets  has the ethernet address at its start ; a trailer indicates the end of the packet and contains a for detection of packet damage  figure 16.10   the packet is placed on the network by the ethernet device the data section of the packet may contain some or all of the data of the original message  but it may also contain some of the upper-level headers that compose the message in other words  all parts of the original message must be sent from source to destination  and all headers above the 802.3layer  data-link layer  are included as data in the ethernet packets  if the destination is on the same local network as the source  the system can look in its arp cache  find the ethernet address of the host  and place the packet on the wire the destination ethernet device then sees its address in the packet and reads in the packet passing it up the protocol stack  if the destination system is on a network different from that of the source  the source system finds an appropriate router on its network and sends the packet there routers then pass the packet along the wan 1-mtil it reaches its bytes 7 2 or 6 2 or 6 2 0-1500 0-46 4 pt.e ~ ~ n1bh  l ~ s.tartfc1ft  r = ccll et  1 each byte pattern 1010101 o data pattern 10101011 ethernet address or broadcast ethernet address length in bytes message data message must be 63 bytes long for error detection figure i 6.10 an ethernet packet  16.10 701 destination network the router that connects the destination network checks its arp cache  finds the ethernet number of the destination  and sends the packet to that host through all of these transfers  the data-link-layer header may change as the ethernet address of the next router in the chain is used  but the other headers of the packet remain the same until the packet is received and processed by the protocol stack and finally passed to the receiving process by the kernel  a distributed system is a collection of processors that do not share memory or a clock instead  each processor has its own local memory  and the processors communicate with one another through various communication lines  such as high-speed buses and telephone lines the processors in a distributed system vary in size and function they may include small microprocessors  workstations  minicomputers  and large general-purpose computer systems  the processors in the system are connected through a communication network  which can be configured in a number of ways the network may be fully or partially connected it may be a tree  a star  a ring  or a multiaccess bus the communication-network design must include routing and com1ection strategies  and it must solve the problems of contention and security  a distributed system provides the user with access to the resources the system provides access to a shared resource can be provided by data migration  computation migration  or process migration  protocol stacks  as specified by network layering models  massage the message  adding information to it to ensure that it reaches its destination a naming system  such as dns  must be used to translate from a host name to a network address  and another protocol  such as arp  may be needed to translate the network number to a network device address  an ethernet address  for instance   if systems are located on separate networks  routers are needed to pass packets from source network to destination network  a distributed system may suffer from various types of hardware failure  for a distributed system to be fault tolerant  it must detect hardware failures and reconfigure the system when the failure is repaired  the system must be reconfigured again  16.1 what are the advantages of using dedicated hardware devices for routers and gateways what are the disadvantages of using these devices compared with using general-purpose computers 16.2 why would it be a bad idea for gateways to pass broadcast packets between networks what would be the advantages of doing so 16.3 consider a network layer that senses collisions and retransmits immediately on detection of a collision what problems could arise with this strategy how could they be rectified 702 chapter 16 16.4 even though the iso model of networking specifies seven layers of functionality  most computer systems use fewer layers to implement a network why do they use fewer layers what problems could the use of fewer layers cause 16.5 the lower layers of the iso network model provide datagram service  with no delivery guarantees for messages a transport-layer protocol such as tcp is used to provide reliability discuss the advantages and disadvantages of supporting reliable message delivery at the lowest possible layer  16.6 what are the advantages and the disadvantages of making the computer network transparent to the user 16.7 under what circumstances is a token-passing network more effective than an ethernet network 16.8 process migration within a heterogeneous network is usually impossible  given the differences in architectures and operating systems  describe a method for process migration across different architectures running  a the same operating system b different operating systems 16.9 contrast the various network topologies in terms of the following attributes  a reliability b available bandwidth for concurrent communications c installation cost d load balance in routing responsibilities 16.10 how does using a dynamic routing strategy affect application behavior for what type of applications is it beneficial to use virtual routing instead of dynamic routing 16.11 the original http protocol used tcp /ip as the underlying network protocol for each page  graphic  or applet  a separate tcp session was constructed  used  and torn down because of the overhead of building and destroying tcp lip connections  performance problems resulted from this implementation method would using udp rather than tcp be a good alternative what other changes could you make to improve http performance 16.12 what are the advantages and disadvantages of using circuit switching for what kinds of applications is circuit switching a viable strategy 16.13 in what ways is using a name server better than using static host tables what problems or complications are associated with name servers what methods could you use to decrease the amount of traffic name servers generate to satisfy translation requests 703 16.14 of what use is an address-resolution protocol why is it better to use such a protocol than to make each host read each packet to determine that packet 's destination does a token-passing network need such a protocol explain your answer  16.15 what is the difference between computation migration and process migration which is easier to implement  and why 16.16 run the program shown in figure 16.5 and determine the ip addresses of the following host names  www.wiley.com www.cs.yale.edu www.apple.com www.westminstercollege.edu www.ietf.org 16.17 to build a robust distributed system  you must know what kinds of failures can occur  a list three possible types of failure in a distributed system  b specify which of the entries in your list also are applicable to a centralized system  16.18 explain why doubling the speed of the systems on an ethernet segment may result in decreased network performance what changes could help solve this problem 16.19 name servers are organized in a hierarchical manner what is the purpose of using a hierarchical organization 16.20 consider a distributed system with two sites  a and b consider whether site a can distinguish among the following  a b goes down  b the link between a and b goes down  c b is extremely overloaded  and its response time is 100 times longer than normal  what implications does your answer have for recovery in distributed systems tanenbaum  2003   stallings  2000a   and kurose and ross  2005  provide general overviews of computer networks williams  2001  covers computer networking from a computer-architecture viewpoint  the internet and its protocols are described in comer  1999  and comer  2000   coverage of tcp /ip can be found in stevens  1994  and stevens  1995   704 chapter 16 unix network programming is described thoroughly in stevens  1997  and stevens  1998   discussions concerning distributed operating-system structures have been offered by coulouris et al  2001  and tanenbaum and van steen  2002   load balancing and load sharing are discussed by i-iarchol-balter and downey  1997  and vee and i-isu  2000   i-iarish and owens  1999  describes load-balancing dns servers process migration is discussed by jul et al  1988   douglis and ousterhout  1991   han and ghosh  1998   and milojicic et al   2000   issues relating to a distributed virtual machine for distributed systems are examined in sirer et al  1999   17.1 in the previous chapter  we discussed network construction and the low-level protocols needed to transfer between systems now we examine one use of this infrastructure a is a distributed implementation of the classical time-sharing of a file system  where multiple users share files and storage resources  chapter 11   the purpose of a dfs is to support the same kind of sharing when the files are physically dispersed among the sites of a distributed system  in this chapter  we describe how a dfs can be designed and implemented  first  we discuss common concepts on which dfss are based then  we illustrate our concepts by examining one influential dfs-the andrew file system  afs   to explain the naming mechanism that provides location transparency and independence  to describe the various methods for accessing distributed files  to contrast stateful and stateless distributed file servers  to show how replication of files on different machines in a distributed file system is a useful redundancy for improving availability  to introduce the andrew file system  afs  as an example of a distributed file system  as we noted in the preceding chapter  a distributed system is a collection of loosely coupled computers interconnected by a communication network  these computers can share physically dispersed files by using a distributed file system  dfs   in this chapter  we use the term dfs to mean distributed file systems in general  not the commercial transarc dfs product ; we refer to the latter as transarc dfs also  nfs refers to nfs version 3  unless otherwise noted  705 706 chapter 17 to explain the structure of a dfs  we need to define the terms service  server  and client a is a software entity running on one or more machines and providing a particular type of function to clients a is the service software running on a single machine a is a process that can invoke a service using a set of operations that form its sometimes a lower-level interface is defined for the actual cross-machine interaction ; it is the using this terminology  we say that a file system provides file services to clients a client interface for a file service is formed by a set of primitive file operations  such as create a file  delete a file  read from a file  and write to a file  the primary hardware concponent that a file server controls is a set of local secondary-storage devices  usually  magnetic disks  on which files are stored and from which they are retrieved according to the clients ' requests  a dfs is a file system whose clients  servers  and storage devices are dispersed among the machines of a distributed system accordingly  service activity has to be carried out across the network instead of a single centralized data repository  the system frequently has multiple and independent storage devices as you will see  the concrete configuration and implementation of a dfs may vary from system to system in some configurations  servers run on dedicated machines ; in others  a machine can be both a server and a client a dfs can be implemented as part of a distributed operating system or  alternatively  by a software layer whose task is to manage the communication between conventional operating systems and file systems the distinctive features of a dfs are the multiplicity and autonomy of clients and servers in the system  ideally  a dfs should appear to its clients to be a conventional  centralized file system the multiplicity and dispersion of its servers and storage devices should be made invisible that is  the client interface of a dfs should not distinguish between local and remote files it is up to the dfs to locate the files and to arrange for the transport of the data a dfs facilitates user mobility by bringing a user 's environment  that is  home directory  to wherever the user logs in  the most important performance measure of a dfs is the amount of time needed to satisfy service requests in conventional systems  this time consists of disk-access time and a small amount of cpu-processing time in a dfs  however  a remote access has the additional overhead attributed to the distributed structure this overhead includes the time to deliver the request to a server  as well as the time to get the response across the network back to the client for each direction  in addition to the transfer of the information  there is the cpu overhead of running the communication protocol software the performance of a dfs can be viewed as another dimension of the dfs 's transparency that is  the performance of an ideal dfs would be comparable to that of a conventional file system  the fact that a dfs manages a set of dispersed storage devices is the dfs ' s key distinguishing feature the overall storage space managed by a dfs is composed of different and remotely located smaller storage spaces usually  these constituent storage spaces correspond to sets of files a cmnpm1.c  nt is the smallest set of files that can be stored on a single machine  independently from other units all files belonging to the same component unit must reside in the same location  17.2 17.2 707 is a mapping between logical and physical objects for instance  users deal with logical data objects represented by file nances  whereas the system manipulates physical blocks of data stored on disk tracks usually  a user refers to a file by a textual name the latter is mapped to a lower-level numerical identifier that in turn is mapped to disk blocks this multilevel mapping provides users with an abstraction of a file that hides the details of how and where on the disk the file is stored  in a transparent dfs  a new dimension is added to the abstraction  that of hiding where in the network the file is located in a conventional file system  the range of the naming mapping is an address within a disk in a dfs  this range is expanded to include the specific machine on whose disk the file is stored  going one step further with the concept of treating files as abstractions leads to the possibility of given a file name  the mapping returns a set of the locations of this file 's replicas in this abstraction  both the existence of multiple copies and their locations are hidden  17.2.1 naming structures we need to differentiate two related notions regarding name mappings in a dfs   the name of a file does not reveal any hint of the file 's physical storage location  'j ' ~ ' '''  ' ' ' ' the name of a file does not need to be changed when the file 's physical storage location changes  both definitions relate to the level of naming discussed previously  since files have different names at different levels  that is  user-level textual names and system-level numerical identifiers   a location-independent naming scheme is a dynamic mapping  since it can map the same file name to different locations at two different times therefore  location independence is a stronger property than is location transparency  in practice  most of the current dfss provide a static  location-transparent mapping for user-level names these systems  however  do not support that is  changing the location of a file automatically is impossible  hence  the notion of location independence is irrelevant for these systems  files are associated permanently with a specific set of disk blocks files and disks can be moved between machines manually  but file migration implies an automatic  operating-system-initiated action only afs and a few experimental file systems support location independence and file mobility afs supports file mobility mainly for administrative purposes a protocol provides migration of afs component units to satisfy high-level user requests  without changing either the user-level names or the low-level names of the corresponding files  a few aspects can further differentiate location independence and static location transparency  divorce of data from location  as exhibited by location independence  provides a better abstraction for files a file name should denote the file 's 708 chapter 17 most significant attributes  which are its contents ratber than its location  location-independent files can be viewed as logical data containers that are not attached to a specific storage location if only static location transparency is supported  the file name still denotes a specific  although hidden  set of physical disk blocks  static location transparency provides users with a convenient way to share data users can share remote files by simply naming the files in a locationtransparent manner  as though the files were local nevertheless  sharing the storage space is cumbersome  because logical names are still statically attached to physical storage devices location independence promotes sharing the storage space itself  as well as the data objects when files can be mobilized  the overall  system-wide storage space looks like a single virtual resource a possible benefit of such a view is the ability to balance the utilization of disks across the system  location independence separates the naming hierarchy from the storagedevices hierarchy and from the intercomputer structure by contrast  if static location transparency is used  although names are transparent   we can easily expose the correspondence between component units and machines the machines are configured in a pattern similar to the naming structure this configuration may restrict the architecture of the system um1.ecessarily and conflict with other considerations a server in charge of a root directory is an example of a structure that is dictated by the naming hierarchy and contradicts decentralization guidelines  once the separation of name and location has been completed  clients can access files residing on remote server systems in fact  these clients may be and rely on servers to provide all files  including the operatingsystem kernel special protocols are needed for the boot sequence  however  consider the problem of getting the kernel to a diskless workstation the diskless workstation has no kernel  so it cam1.ot use the dfs code to retrieve the kernel instead  a special boot protocol  stored in read-only memory  rom  on the client  is invoked it enables networking and retrieves only one special file  the kernel or boot code  from a fixed location once the kernel is copied over the network and loaded  its dfs makes all the other operating-system files available the advantages of diskless clients are many  including lower cost  because the client machines require no disks  and greater convenience  when an operating-system upgrade occurs  only the server needs to be modified   the disadvantages are the added complexity of the boot protocols and the performance loss resulting from the use of a network rather than a local disk  the current trend is for clients to use both local disks and remote file servers  operating systems and networking software are stored locally ; file systems containing user data-and possibly applications-are stored on remote file systems some client systems may store commonly used applications  such as word processors and web browsers  on the local file system as well other  less commonly used applications may be from the remote file server to the client on demand the main reason for providing clients with local file systems rather than pure diskless systems is that disk drives are rapidly increasing in capacity and decreasing in cost  with new generations appearing every year or so the same can not be said for networks  which evolve every few years  17.2 709 overall  systems are growing more quickly than are networks  so extra work is needed to limit network access to improve system throughput  17.2.2 naming schemes there are three main approaches to naming schemes in a dfs in the simplest approach  a file is identified by some combination of its host name and local name  which guarantees a unique system-wide name in ibis  for instance  a file is identified uniquely by the name host  local-name  where local-name is a unix-like path this naming scheme is neither location transparent nor location independent nevertheless  the same file operations can be used for both local and remote files the dfs is structured as a collection of isolated component units  each of which is an entire conventional file system in this first approach  component 1-mits remain isolated  although means are provided to refer to a remote file we do not consider this scheme any further in this text  the second approach was popularized by sun 's network file system  nfs  nfs is the file-system component of onc +  a networking package supported by many unix vendors nfs provides a means to attach remote directories to local directories  thus giving the appearance of a coherent directory tree  early nfs versions allowed only previously mmmted remote directories to be accessed transparently with the advent of the feature  mounts are done on demand  based on a table of mount points and file-structure names components are integrated to support transparent sharing  although this integration is limited and is not uniform  because each machine may attach different remote directories to its tree the resulting structure is versatile  we can achieve total integration of the component file systems by using the third approach here  a single global name structure spans all the files in the system ideally  the composed file-system structure is the same as the structure of a conventional file system in practice  however  the many special files  for example  unix device files and machine-specific binary directories  make this goal difficult to attain  to evaluate naming structures  we look at their the most complex and most difficult-to-maintain structure is the nfs structure  because any rem.ote directory can be attached anywhere onto the local directory tree  the resulting hierarchy can be highly m1structured if a server becomes unavailable  some arbitrary set of directories on different machines becomes unavailable in addition  a separate accreditation mechanism controls which machine is allowed to attach which directory to its tree thus  a user might be able to access a remote directory tree on one client but be denied access on another client  17.2.3 implementation techniques implementation of transparent naming requires a provision for the mapping of a file naine to the associated location to keep this mapping manageable  we must aggregate sets of files into component units and provide the mapping on a component-unit basis rather than on a single-file basis this aggregation serves administrative purposes as well unix-like systems use the hierarchical directory tree to provide name-to-location mapping and to aggregate files recursively into directories  710 chapter 17 17.3 to enhance the availability of the crucial mapping information  we can use replication  local caching  or both as we noted  location independence means that the mapping changes over time ; hence  replicating the mapping makes a simple yet consistent update of this information impossible a teclllcique to overcome this obstacle is to introduce low-level me textual file names are mapped to lower-level file identifiers that indicate to which component unit the file belongs these identifiers are still location independent they can be replicated and cached freely without being invalidated by migration of component units the inevitable price is the need for a second level of mapping  which maps component units to locations and needs a simple yet consistent update mechanism implementing unix-like directory trees using these low-levet location-independent identifiers makes the whole hierarchy invariant under component-unit migration the only aspect that does change is the component-unit location mapping  a common way to implement low-level identifiers is to use structured names these names are bit strings that usually have two parts the first part identifies the component unit to which the file belongs ; the second part identifies the particular file within the unit variants with more parts are possible the invariant of structured names  however  is that individual parts of the name are unique at all times only within the context of the rest of the parts we can obtain uniqueness at all times by taking care not to reuse a name that is still in use  by adding sufficiently more bits  this method is used in afs   or by using a timestamp as one part of the name  as done in apollo domain   another way to view this process is that we are taking a location-transparent system  such as ibis  and adding another level of abstraction to produce a location-independent naming scheme  aggregating files into component units and using lower-level locationindependent file identifiers are techniques exemplified in afs  consider a user who requests access to a remote file the server storing the file has been located by the nanling scheme  and now the actual data transfer must take place  one way to achieve this transfer is through a whereby requests for accesses are delivered to the server  the server machine performs the accesses  and their results are forwarded back to the user one of the most common ways of implementing remote service is the remote procedure call  rpc  paradigm  which we discussed in chapter 3 a direct analogy exists between disk-access methods in conventional file systems and the remote-service method in a dfs  using the remote-service method is analogous to performing a disk access for each access request  to ensure reasonable performance of a remote-service mechanism  we can use a form of caching in conventional file systems  the rationale for caching is to reduce disk i/0  thereby increasing performance   whereas in dfss  the goal is to reduce both network traffic and disk i/0 in the following discussion  we describe the implementation of caching in a dfs and contrast it with the basic remote-service paradigm  17.3 711 17.3.1 basic caching scheme the concept of caching is simple if the data needed to satisfy the access request are not already cached  then a copy of those data is brought from the server to the client system accesses are performed on the cached copy the idea is to retain recently accessed disk blocks in the cache  so that repeated accesses to the same information can be handled locally  without additional network traffic  a replacement policy  for example  the least-recently-used algorithm  keeps the cache size bounded no direct correspondence exists between accesses and traffic to the server files are still identified with one master copy residing at the server machine  but copies  or parts  of the file are scattered in different caches  when a cached copy is modified  the changes need to be reflected on the master copy to preserve the relevant consistency semantics the problem of keeping the cached copies consistent with the master file is the which we discuss in section 17.3.4 dfs caching could just as easily be called  it acts sincilarly to demand-paged virtual memory  except that the backing store usually is not a local disk but rather a remote server nfs allows the swap space to be mounted remotely  so it actually can implement virtual memory over a network  notwithstanding the resulting performance penalty  the granularity of the cached data in a dfs can vary from blocks of a file to an entire file usually  more data are cached than are needed to satisfy a single access  so that many accesses can be served by the cached data this procedure is much like diskread-ahead  section 11.6.2   afs caches files in large chunks  64 kb   the other systems discussed in this chapter support caching of individual blocks driven by client demand increasing the caching unit increases the hit ratio  but it also increases the miss penalty  because each miss requires more data to be transferred it increases the potential for consistency problems as well selecting the unit of caching involves considering parameters such as the network transfer unit and the rpc protocol service unit  if an rpc protocol is used   the network transfer unit  for ethernet  a packet  is about 1.5 kb  so larger units of cached data need to be disassembled for delivery and reassembled on reception  block size and total cache size are obviously of importance for blockcaching schemes in unix-like systems  common block sizes are 4 kb and 8 kb for large caches  over 1mb   large block sizes  over 8 kb  are beneficial for smaller caches  large block sizes are less beneficial because they result in fewer blocks in the cache and a lower hit ratio  17.3.2 cache location where should the cached data be stored-on disk or in main memory disk caches have one clear advantage over main-memory caches  they are reliable  modifications to cached data are lost in a crash if the cache is kept in volatile memory moreove1 ~ if the cached data are kept on disk  they are still there during recovery  and there is no need to fetch them again main-memory caches have several advantages of their own  however  main-memory caches permit workstations to be diskless  data can be accessed more quickly from a cache in main memory than from one on a disk  712 chapter 17 technology is moving toward larger and less expensive memory the resulting performance speedup is predicted to outweigh the advantages of disk caches  the server caches  used to speed up disk i/0  will be in main memory regardless of where user caches are located ; if we use main-memory caches on the user machine  too  we can build a single caching nl.echanism for use by both servers and users  many remote-access implementations can be thought of as hybrids of caching and remote service in nfs  for instance  the implementation is based on remote service but is augmented with client and server-side memory caching for performance similarly sprite 's implementation is based on caching ; but under certain circumstances  a remote-service method is adopted thus  to evaluate the two methods  we must evaluate the degree to which either method is emphasized  the nfs protocol and most implementations do not provide disk caching  recent solaris implementations ofnfs  solaris 2.6 and beyond  include a clientside disk-caching option  the  file system once the nfs client reads blocks of a file from the serve1 ~ it caches them in memory as well as on disk  if the memory copy is flushed  or even if the system reboots  the disk cache is referenced if a needed block is neither in memory nor in the cachefs disk cache  an rpc is sent to the server to retrieve the block  and the block is written into the disk cache as well as stored in the memory cache for client use  17.3.3 cache-update policy the policy used to write modified data blocks back to the server 's master copy has a critical effect on the performance and reliability the simplest policy is to write data to disk as soon as they are placed in any cache  the advantage of a is reliability  little information is lost when a client system crashes however  this policy requires each write access to wait until the information is sent to the server  so it causes poor write performance caching with write-through is equivalent to using remote service for write accesses and exploiting caching for read accesses  an alternative is the also known as where we delay updates to the master copy modifications are written to the cache and then are written through to the server at a later time this policy has two advantages over write-through first  because writes are made to the cache  write accesses complete much more quickly second  data may be overwritten before they are written back  in which case only the last update needs to be written at all unfortunately  delayed-write schemes introduce reliability problems  since unwritten data are lost whenever a user machine crashes  variations of the delayed-write policy differ in when modified data blocks are flushed to the server one alternative is to flush a block when it is about to be ejected from the client 's cache this option can result in good performance  but some blocks can reside in the client 's cache a long time before they are written back to the server a compromise between this alternative and the write-through policy is to scan the cache at regular intervals and to flush blocks that have been modified since the most recent scan  just as unix scans 17.3 713 nfs server network workstation figure 17.1 cachefs and its use of caching  its local cache sprite uses this policy with a 30-second interval nfs uses the policy for file data  but once a write is issued to the server durilcg a cache flush  the write must reach the server 's disk before it is considered complete  nfs treats meta data  directory data and file-attribute data  differently any metadata changes are issued synchronously to the server thus  file-structure loss and directory-structure corruption are avoided when a client or the server crashes  for nfs with cachefs  writes are also written to the local disk cache area when they are written to the server  to keep all copies consistent thus  nfs with cachefs improves performance over standard nfs on a read request with a cachefs cache hit but decreases performance for read or write requests with a cache miss as with all caches  it is vital to have a high cache hit rate to gain performance figure 17.1 shows how cachefs uses write-through and write-back caching  yet another variation on delayed write is to write data back to the server when the file is closed this is used in afs in the case of files that are open for short periods or are modified rarely  this policy does not significantly reduce network traffic in addition  the write-on-close policy requires the closing process to delay while the file is written through  which reduces the performance advantages of delayed writes for files that are open for long periods and are modified frequently  however  the performance advantages of this policy over delayed write with more frequent flushing are apparent  17.3.4 consistency a client machine is faced with the problem of deciding whether a locally cached copy of the data is consistent with the master copy  and hence can be used   if 714 chapter 17 the client machine determines that its cached data are out of date  accesses can no longer be served by those cached data an up-to-date copy of the data needs to be cached there are two approaches to verifying the validity of cached data  client-initiated approach the client initiates a validity check  in which it contacts the server and checks whether the local data are consistent with the master copy the frequency of the validity checking is the crux of this approach and determines the resulting consistency semantics it can range from a check before every access to a check only on first access to a file  on file open  basically   every access coupled with a validity check is delayed  compared with an access served immediately by the cache  alternatively  checks can be initiated at fixed time intervals depending on its frequency  the validity check can load both the network and the server  server-initiated approach the server records  for each client  the files  or parts of files  that it caches when the server detects a potential inconsistency  it must react a potential for inconsistency occurs when two different clients in conflicting modes cache a file if unix semantics  section 10.5.3  is implemented  we can resolve the potential inconsistency by having the server play an active role the server must be notified whenever a file is opened  and the intended mode  read or write  must be indicated for every open the server can then act when it detects that a file has been opened simultaneously in conflicting modes by disabling caching for that particular file actually  disabling caching results in switching to a remote-service mode of operation  17.3.5 a comparison of caching and remote service essentially  the choice between caching and remote service trades off potentially increased performance with decreased simplicity we evaluate this tradeoff by listing the advantages and disadvantages of the two methods  when caching is used  the local cache can handle a substantial number of the remote accesses efficiently capitalizing on locality in file-access patterns makes caching even more attractive thus  most of the remote accesses will be served as fast as will local ones moreover  servers are contacted only occasionally  rather than for each access consequently  server load and network traffic are reduced  and the potential for scalability is enhanced by contrast  when the remote-service method is used  every remote access is handled across the network the penalty in network traffic  server load  and performance is obvious  total network overhead is lower for transmitting big chunks of data  as is done in caching  than for transmitting series of responses to specific requests  as in the remote-service method   furthermore  disk-access routines on the server may be better optimized if it is known that requests will always be for large  contiguous segments of data rather than for random disk blocks  the cache-consistency problem is the major drawback of caching when access patterns exhibit infrequent writes  caching is superior however  17.4 17.4 715 when writes are frequent  the mechanisms employed to overcome the consistency problem incur substantial overhead in terms of performance  network traffic  and server load  so that caching will confer a benefit  execution should be carried out on machines that have either local disks or large main memories remote access on diskless  small-memory-capacity machines should be done through the remote-service method  in caching  since data are transferred en masse between the server and the client  rather than in response to the specific needs of a file operation  the lower-level intermachine interface is different from the upper-level user interface the remote-service paradigm  in contrast  is just an extension of the local file-system interface across the network thus  the intermachine interface mirrors the user interface  there are two approaches for storing server-side information when a client accesses remote files  either the server tracks each file being accessed by each client  or it simply provides blocks as they are requested by the client without knowledge of how those blocks are used in the former case  the service provided is stateful ; in the latter case  it is stateless  the typical scenario involving a is as follows  a client must perform an open   operation on a file before accessing that file the server fetches information about the file from its disk  stores it in its memory  and gives the client a connection identifier that is unique to the client and the open file   in unix terms  the server fetches the inode and gives the client a file descriptor  which serves as an index to an in-core table of inodes  this identifier is used for subsequent accesses ru1.til the session ends a stateful service is characterized as a connection between the client and the server during a session either on closing the file or through a garbage-collection mechanism  the server must reclaim the main-memory space used by clients that are no longer active the key point regarding fault tolerance in a stateful service approach is that the server keeps main-memory information about its clients afs is a stateful file service  a avoids state information by making each request self-contained that is  each request identifies the file and the position in the file  for read and write accesses  in full the server does not need to keep a table of open files in main memory  although it usually does so for efficiency reasons moreover  there is no need to establish and terminate a com1.ection through open   and close   operations they are totally redundant since each file operation stands on its own and is not considered part of a session a client process would open a file  and that open would not result in the sending of a remote message reads and writes would take place as remote messages  or cache lookups   the final close by the client would again result in only a local operation nfs is a stateless file service  the advantage of a stateful over a stateless service is increased performance  file information is cached in main memory and can be accessed easily via the connection identifier  thereby saving disk accesses in addition  a stateful 716 chapter 17 5 server knows whether a file is open for sequential access and can therefore read ahead the next blocks stateless servers cmmot do so  since they have no knowledge of the purpose of the client 's requests  the distinction between stateful and stateless service becomes more evident when we consider the effects of a crash that occurs during a service activity a stateful server loses all its volatile state in a crash ensuring the graceful recovery of such a server involves restoring this state  usually by a recovery protocol based on a dialog with clients less graceful recovery requires that the operations that were underway when the crash occurred be aborted  a different problem is caused by client failures the server needs to become aware of such failures so that it can reclaim space allocated to record the state of crashed client processes this phenomenon is sometimes referred to as a stateless computer server avoids these problems  silcce a newly reincarnated server can respond to a self-contained request without any difficulty  therefore  the effects of server failures and recovery are almost unnoticeable  there is no difference between a slow server and a recovering server from a client 's point of view the client keeps retransmitting its request if it receives no response  the penalty for using the robust stateless service is longer request messages and slower processing of requests  since there is no in-core i,_'lformation to speed the processing in addition  stateless service imposes additional constraints on the design of the dfs first  since each request identifies the target file  a uniform  system-wide  low-level naming scheme should be used translating remote to local names for each request would cause even slower processing of the requests second  since clients retransmit requests for file operations  these operations must be idempotent ; that is  each operation must have the same effect and return the same output if executed several times consecutively  self-contained read and write accesses are idempotent  as long as they use an absolute byte count to indicate the position withilc the file they access and do not rely on an incremental offset  as is done in unix read   and write   system calls   however  we must be careful when implementing destructive operations  such as deleting a file  to make them idempotent  too  in some environments  a stateful service is a necessity if the server employs the server-initiated method for cache validation  it camcot provide stateless service  since it maintains a record of which files are cached by which clients  the way unix uses file descriptors and implicit offsets is inherently stateful  servers must mailctain tables to map the file descriptors to inodes and must store the current offset within a file this requirement is why nfs  which employs a stateless service  does not use file descriptors and does include an explicit offset in every access  replication of files on different machines in a distributed file system is a useful redundancy for improving availability multimachine replication can benefit performance too  selecting a nearby replica to serve an access request results in shorter service time  the basic requirement of a replication scheme is that different replicas of the same file reside on failure-independent machines that is  the availability 17.5 717 nfsv4 ourcov,era.geofnfs thus .far has 0p1y considered version 3  orv3  nf  the mostrecentnps standard is version 4  v 4   and it differs fundanrentaljy from pr ~ vious versimi.s jhe most significant ch9nge is that the protocol is now stqteful,meaping tha  tthesetv ~ er maintains the.state ofthe client session from the  time the r ~ j1lote file is 0pt   ned untij itis closed th.t1s  thenfs protocol now provides open   ar ; td c1o $ e   operations ; previous v  ersions of nfs  v \ thich are stateless  .proyide .np such operations furthen   ore  preytous \ cersions specify s ~ parate protocols or j  lounting remote fil ~ systews and for lockii1g remote files  v4 provides ali of these features under l phlgle prqtocol in patticular ; the 1nrnmt protocol was elimin ; 1.ted  allowing 1 \  fs to work with network fitewalls  the nj.ount protocol was a notorimis security hole in nps implem,entations ~         .additionally  v4 has enhan edthe ability of.dients jo cache file data local  y  this feature ; i ~ prov ~ s tne performapc ~ of the disttil  mt  3d file system  a.s plients are able to reso ~ ve more file a9cesses from the loc ~ l c ~ che rgther thanh ; lvingto gpjhroughthe s ~ !  ver '   4 ali   ws diel   tsjo req  c ! estfile jocks from s ~ rvers as we 'll .if the  senr ~ r grqrct ~  the request the client maintains the loci ,tmtil it is released or its lea.st = expires  clier ~ ts ar,e als   permitted to r ~ new ex ~ stil'tg least = s  traditiora11y1 unix ~ ba ~ ~ d  systems .provide advisory jile locking  whereas windows operatirg systen1 ~ use mandat   rylockil1g to allov \ t l ' \  ps towm  wellwithnon-unixsyste1fts  v4t1qw.p.rovides mandatory locking as vvell the new lockinga11d caching mec ~ anisms are based on the concept   f d ~ legahon  whe ~ eby the server delegates responsibilities for a file 's lockand contents to .the client thatrequested tnel   ckcrhat delegated client maintains in cache the .current version of .the file  and  other clients can  ask that deleg21ted client for lock access  a1i.d filt = confentsuntifthe del  egated client reli11quishesth ~ lock andde ~ egation               finally  whereas.preyiousversionb   f npsarebasedon the udj network ptqtocol  \  4 is based on .tcp,whioh allows itto betteraclj \ lstto varying traffic loads on thel  etwork  peleg2lting these responsibilities to cliel ! cts reduces the foado11the s.eryet and.i ~ proves.cache.coherency  of one replica is not affected by the availability of the rest of the replicas  this obvious requirement implies that replication management is inherently a location-opaque activity provisions for placing a replica on a particular machine must be available  it is desirable to hide the details of replication from users mapping a replicated file name to a particular replica is the task of the naming scheme  the existence of replicas should be invisible to higher levels at lower levels  however  the replicas must be distinguished from one another by different lower-level names another transparency requirement is providing replication control at higher levels replication control includes determination of the degree of replication and of the placement of replicas under certain circumstances  we may want to expose these details to users locus  for instance  provides users and system administrators with mechanisms to control the replication scheme  718 chapter 17 17.6 the main problem ~ associated with replicas is updating from a user 's point of view  replicas of a file denote the same logical entity  and thus an update to any replica must be reflected on all other replicas more precisely  the relevant consistency sen1antics must be preserved when accesses to replicas are viewed as virtual accesses to the replicas ' logical files if consistency is not of primary incportance  it can be sacrificed for availability and performance in this fundamental tradeoff in the area of fault tolerance  the choice is between preserving consistency at all costs  thereby creating a potential for indefinite blocking  and sacrificing consistency under some  we hope  rare  circumstances for the sake of guaranteed progress locus  for example  employs replication extensively and sacrifices consistency in the case of network partition for the sake of availability of files for read and write accesses  ibis uses a variation of the primary-copy approach the domain of the name mapping is a pair primary-replica-identifier  local-replica-identifier  if no local replica exists  a special value is used thus  the mapping is relative to a machine if the local replica is the primary one  the pair contains two identical identifiers ibis supports demand replication  an automatic replication-control policy similar to whole-file caching under demand replication  reading of a nonlocal replica causes it to be cached locally  thereby generating a new nonprimary replica updates are performed only on the primary copy and cause all other replicas to be invalidated through the sending of appropriate messages atomic and serialized invalidation of all nonprimary replicas is not guaranteed hence  a stale replica may be considered valid to satisfy remote write accesses  we migrate the primary copy to the requesting machine  andrew is a distributed computing environment designed and implemented at carnegie mellon university the andrew file system  afs  constitutes the underlying information-sharing mechanism among clients of the environment  the transarc corporation took over development of afs and then was purchased by ibm ibm has since produced several commercial implementations of afs afs was subsequently chosen as the dfs for an industry coalition ; the result was part of the distributed computing environment  dce  from the osf organization  in 2000  ibm 's transarc lab announced that afs would be an open-source product  termed openafs  available under the ibm public license  and transarc dfs was canceled as a commercial product openafs is available under most commercial versions of unix as well as linux and microsoft windows systems  many unix vendors  as well as microsoft  support the dce system and its dfs  which is based on afs  and work is ongoing to make dce a cross-platform  universally accepted dfs as afs and transarc dfs are very similar ~ we describe afs throughout this section  unless transarc dfs is named specifically  afs seeks to solve many of the problems of the simpler dfss  such as nfs  and is arguably the most feature-rich nonexperimental dfs it features a uniform name space  location-independent file sharing  client-side caching with cache consistency  and secure authentication via kerberos it also includes server-side caching in the form of replicas  with high availability through automatic switchover to a replica if the source server is unavailable one of the 17.6 719 most formidable attributes of afs is scalability  the andrew system is targeted to span over 5,000 workstations between afs and transarc dfs  there are hundreds of implementations worldwide  17.6.1 overview afs distinguishes between client machines  sometimes referred to as workstations  and dedicated server machines servers and clients originally ran only 4.2 bsd unix  but afs has been ported to many operating systems the clients and servers are interconnected by a network of lans or wans  clients are presented with a partitioned space of file names  a and a dedicated servers  collectively called vice after the name of the software they run  present the shared name space to the clients as a homogeneous  identical  and location-transparent file hierarchy  the local name space is the root file system of a workstation  from which the shared name space descends workstations run the virtue protocol to communicate with vice  and each is required to have a local disk where it stores its local name space servers collectively are responsible for the storage and management of the shared name space the local name space is small  is distinct for each workstation  and contains system programs essential for autonomous operation and better performance also local are temporary files and files that the workstation owner  for privacy reasons  explicitly wants to store locally  viewed at a finer granularity  clients and servers are structured in clusters interconnected by a wan each cluster consists of a collection of workstations on a lan and a representative of vice called a and each cluster is com1.ected to the wan by a router the decomposition into clusters is done primarily to address the problem of scale for optimal performance  workstations should use the server on their own cluster most of the time  thereby making cross-cluster file references relatively infrequent  the file-system architecture is also based on considerations of scale the basic heuristic is to offload work from the servers to the clients  in light of experience indicating that server cpu speed is the system 's bottleneck following this heuristic  the key mechanism for remote file operations is to cache files in large chunks  64 kb   this feature reduces file-open latency and allows reads and writes to be directed to the cached copy without frequently involving the servers  briefly  here are a few additional issues in the design of afs  client mobility clients are able to access any file in the shared name space from any workstation a client may notice some initial performance degradation due to the caching of files when accessil g files a workstation other than the usual one  security the vice interface is considered the boundary of trustworthiness  because no client programs are executed on vice machines authentication and secure-transmission functions are provided as part of a connectionbased communication package based on the rpc paradigm after mutual authentication  a vice server and a client communicate via encrypted messages encryption is performed by hardware devices or  more slowly  720 chapter 17 in software information about clients and groups is stored in a protection database replicated at each server  protection afs provides for protecting directories and the regular unlxbits for file protection the access list ncay contain information about those users allowed to access a directory  as well as information about those users not allowed to access it thus  it is simple to specify that everyone except  say  jim can access a directory afs supports the access types read  write  lookup  insert  administer  lock  and delete  heterogeneity defining a clear interface to vice is a key for integration of diverse workstation hardware and operating systems so that heterogeneity is facilitated  some files in the local /bin directory are symbolic links pointing to machine-specific executable files residing in vice  17.6.2 the shared name space afs 's shared name space is made up of component units called the volumes are unusually small component units typically  they are associated with the files of a single client few volumes reside within a single disk partition  and they may grow  up to a quota  and shrink in size conceptually  volumes are glued together by a mechanism similar to the unix m01mt mechanism however  the granularity difference is significant  since in unix only an entire disk partition  containing a file system  can be mounted volumes are a key administrative unit and play a vital role in identifying and locating an individual file  a vice file or directory is identified by a low-level identifier called a fid  each afs directory entry maps a path-name component to a fid a fid is 96 bits long and has three equal-length components  a volume number  a vnode number  and a uniquifier the vnode number is used as an index into an array containing the inodes of files in a single volume the allows reuse of vnode numbers  thereby keeping certain data structures compact fids are location transparent ; therefore  file movements from server to server do not invalidate cached directory contents  location information is kept on a volume basis in a replicated on each server a client can identify the location of every volume in the system by querying this database the aggregation of files into volumes makes it possible to keep the location database at a manageable size  to balance the available disk space and utilization of servers  volumes need to be migrated among disk partitions and servers when a volume is shipped to its new location  its original server is left with temporary forwarding information  so that the location database need not be updated synchronously  while the volume is being transferred  the original server can still handle updates  which are shipped later to the new server at some point  the volume is briefly disabled so that the recent modifications can be processed ; then  the new volume becomes available again at the new site the volume-movement operation is atomic ; if either server crashes  the operation is aborted  read-only replication at the granularity of an entire volume is supported for system-executable files and for seldom-updated files in the upper levels of the vice name space the volume-location database specifies the server 17.6 721 contammg the only read-write copy of a volume and a list of read-only replication sites  17.6.3 file operations and consistency semantics the fundamental architectural principle in afs is the caching of entire files from servers accordingly  a client workstation interacts with vice servers only during opening and closing of files  and even this interaction is not always necessary reading and writing files do not cause remote interaction  in contrast to the remote-service n lethod   this key distinction has far-reaching ramifications for performance  as well as for semantics of file operations  the operating system on each workstation intercepts file-system calls and forwards them to a client-level process on that workstation this process  called venus  caches files from vice when they are opened and stores modified copies of files back on the servers from which they came when they are closed venus may contact vice only when a file is opened or closed ; reading and writing of individual bytes of a file are performed directly on the cached copy and bypass venus as a result  writes at some sites are not visible immediately at other sites  caching is further exploited for future opens of the cached file venus assumes that cached entries  files or directories  are valid unless notified otherwise therefore  venus does not need to contact vice on a file open to validate the cached copy the mechanism to support this policy  called callback  dramatically reduces the number of cache-validation requests received by servers it works as follows when a client caches a file or a directory  the server updates its state information to record this caching we say that the client has a callback on that file the server notifies the client before allowing another client to modify the file in such a case  we say that the server removes the callback on the file for the former client a client can use a cached file for open purposes only when the file has a callback if a client closes a file after modifying it  all other clients caching this file lose their callbacks therefore  when these clients open the file later  they have to get the new version from the server  readin.g and writing bytes of a file are done directly by the kernel without venus 's intervention on the cached copy venus regains control when the file is closed if the file has been modified locally  it updates the file on the appropriate server thus  the only occasions on which venus contacts vice servers are on opens of files that either are not in the cache or have had their callback revoked and on closes of locally modified files  basically  afs implements session semantics the only exceptions are file operations other than the primitive read and write  such as protection changes at the directory level   which are visible everywhere on the network immediately after the operation completes  in spite of the callback mechanism  a small amount of cached validation traffic is still present  usually to replace callbacks lost because of machine or network failures when a workstation is rebooted  venus considers all cached files and directories suspect  and it generates a cache-validation request for the first use of each such entry  the callback mechanism forces each server to maintain callback information and each client to maintain validity information if the amount of callback 722 chapter 17 information maintained by a server is excessive  the server can break callbacks and reclaim some storage by unilaterally notifying clients and revoking the validity of their cached files if the callback state maintained by venus gets out of sync with the corresponding state maintained by the servers  some inconsistency may result  venus also caches contents of directories and syncbolic links  for pathname translation each component in the path name is fetched  and a callback is established for it if it is not already cached or if the client does not have a callback on it venus does lookups on the fetched directories locally  using fids no requests are forwarded from one server to another at the end of a path-name traversal  all the intermediate directories and the target file are in the cache with callbacks on them future open calls to this file will involve no network communication at all  unless a callback is broken on a component of the path name  the only exception to the caching policy is a modification to a directory that is made directly on the server responsible for that directory for reasons of integrity the vice interface has well-defined operations for such purposes  venus reflects the changes in its cached copy to avoid re-fetching the directory  17.6.4 implementation client processes are interfaced to a unix kernel with the usual set of system calls the kernel is modified slightly to detect references to vice files in the relevant operations and to forward the requests to the client-level venus process at the workstation  venus carries out path-name translation component by component  as described above it has a mapping cache that associates volumes to server locations in order to avoid server interrogation for an already known volume location if a volume is not present in this cache  venus contacts any server to which it already has a comcection  requests the location information  and enters that information into the mapping cache unless venus already has a connection to the server  it establishes a new comcection it then uses this connection to fetch the file or directory connection establishment is needed for authentication and security purposes when a target file is found and cached  a copy is created on the local disk venus then returns to the kernel  which opens the cached copy and returns its handle to the client process  the unix file system is used as a low-level storage system for both afs servers and clients the client cache is a local directory on the workstation 's disk within this directory are files whose names are placeholders for cache entries both venus and server processes access unix files directly by the latter 's inodes to avoid the expensive path-name-to-inode translation routine  namei   because the internal inode interface is not visible to client-level processes  both venus and server processes are client-level processes   an appropriate set of additional system calls was added dfs uses its own journaling file system to improve performance and reliability over ufs  venus manages two separate caches  one for status and the other for data  it uses a simple least-recently-used  lru  algorithm to keep each of them bounded in size when a file is flushed from the cache  venus notifies the appropriate server to remove the callback for this file the status cache is kept in virtual memory to allow rapid servicing of stat    file-status-returning  17.7 17.7 723 system calls the data cache is resident on the local disk  but the unix i/0 buffering mechanism does some caching of disk blocks in memory that is transparent to venus  a single client-level process on each file server services all file req1.1ests from clients this process uses a lightweight-process package with non-preemptible scheduling to service many client requests concurrently the rpc package is integrated with the lightweight-process package  thereby allowing the file server to concurrently make or service one rpc per lightweight process the rpc package is built on top of a low-level datagram abstraction whole-file transfer is implemented as a side effect of the rpc calls one rpc connection exists per client  but there is no a priori binding of lightweight processes to these connections instead  a pool of lightweight processes services client requests on all connections the use of a single multithreaded server process allows the caching of data structures needed to service requests on the negative side  a crash of a single server process has the disastrous effect of paralyzing this particular server  a dfs is a file-service system whose clients  servers  and storage devices are dispersed among the sites of a distributed system accordingly  service activity has to be carried out across the network ; instead of a single centralized data repository  there are multiple independent storage devices  ideally  a dfs should look to its clients like a conventional  centralized file system the multiplicity and dispersion of its servers and storage devices should be made transparent that is  the client interface of a dfs should not distinguish between local and remote files it is up to the dfs to locate the files and to arrange for the transport of the data a transparent dfs facilitates client mobility by bringing the client 's environment to the site where the client logs in  there are several approaches to naming schemes in a dfs in the simplest approach  files are named by some combination of their host name and local name  which guarantees a unique system-wide name another approach  popularized by nfs  provides a means to attach remote directories to local directories  thus giving the appearance of a coherent directory tree  requests to access a remote file are usually handled by two complementary methods with remote service  requests for accesses are delivered to the server  the server machine performs the accesses  and their results are forwarded back to the client with caching  if the data needed to satisfy the access request are not already cached  then a copy of the data is brought from the server to the client accesses are performed on the cached copy the idea is to retain recently accessed disk blocks in the cache  so that repeated accesses to the same information can be handled locally  without additional network traffic a replacement policy is used to keep the cache size bounded the problem of keeping the cached copies consistent with the master file is the cache-consistency problem  there are two approaches to server-side information either the server tracks each file the client accesses  or it simply provides blocks as the client requests them without knowledge of their use these approaches are the stateful versus stateless service paradigms  724 chapter 17 replication of files on different machines is a useful redundancy for improving availability multimachine replication can benefit performance  too  since selecting a nearby replica to serve an access request results in shorter service time  afs is a feature-rich dfs characterized by location independence and location transparency it also imposes significant consistency semantics caching and replication are used to improve performance  17.1 discuss whether afs and nfs provide the following   a  location transparency and  b  location independence  17.2 discuss whether clients in the following systems can obtain inconsistent or stale data from the file server and  if so  under what scenarios this could occur  a afs b sprite c nfs 17.3 consider afs  which is a stateful distributed file system what actions need to be performed to recover from a server crash in order to preserve the consistency guaranteed by the system 17.4 discuss the advantages and disadvantages of path-name translation in which the client ships the entire path to the server requesting a translation for the entire path name of the file  17.5 under what circumstances would a client prefer a locationtransparent dfs under what circumstances would she prefer a location-independent dfs discuss the reasons for these preferences  17.6 v'lhich of the example dfss discussed in this chapter would handle a large  multiclient database application most efficiently explain your answer  17.7 what are the benefits of mapping objects into virtual memory  as apollo domain does what are the drawbacks 17.8 compare and contrast the teclmiques of caching disk blocks locally  on a client system  and remotely  on a server  17.9 what aspects of a distributed system would you select for a system running on a totally reliable network 725 consistency and recovery control for replicated files are examined by davcev and burkhard  1985   management of replicated files in a unix environncent is covered by brereton  1986  and purdin et al  1987   wah  1984  discusses the issue of file placement on distributed computer systems a detailed survey of mainly centralized file servers appears in svobodova  1984   sun 's network file system  nfs  is described by callaghan  2000  and sandberg et al  1985   the afs system is discussed by morris et al  1986   howard et al  1988   and satyanarayanan  1990   information about openafs is available from http  / /www.openafs.org many different and interesting dfss are not covered in detail in this text  including unix united  sprite  and locus unix united is described by brownbridge et al  1982   the locus system is discussed by popek and walker  1985   the sprite system is described by ousterhout et al  1988  and nelson et al  1988   distributed file systems for mobile storage devices are discussed in kistler and satyanarayanan  1992  and sobti et al  2004   considerable research has also been performed on cluster-based distributed file systems  anderson et al  1995   lee and thekkath  1996   thekkath et al  1997   and anderson et al  2000    distributed storage systems for large-scale  wide-area settings are presented in dabek et al  2001  and kubiatowicz et al  2000   18.1 in chapter 6  we described various mechanisms that allow processes to synchronize their actions we also discussed a number of schemes to ensure the atomicity of a transaction that executes either in isolation or concurrently with other transactions in chapter 7  we described various methods that an operating system can use to deal with the deadlock problem in this chapter  we examine how centralized synchronization mechanisms can be extended to a distributed environment we also discuss methods for handling deadlocks in a distributed system  to describe various methods for achieving mutual exclusion in a distributed system  to explain how atomic transactions can be implemented in a distributed system  to show how some of the concurrency-control schemes discussed in chapter 6 can be modified for use in a distributed environment  to present schemes for handling deadlock prevention  deadlock avoidance  and deadlock detection in a distributed system  in a centralized system  we can always determine the order in which two events occurred  since the system has a single common memory and clock  many applications may require us to determine order for example  in a resource-allocation scheme  we specify that a resource can be used only aft-er the resource has been granted a distributed system  however  has no common memory and no common clock therefore  it is sometimes impossible to say which of two events occurred first the happened-before relation  discussed next  is only a partial ordering of the events in distributed systems since 727 728 chapter 18 the ability to define a total ordering is crucial in many applications  we present a distributed algorithm for extending the happened-before relation to a consistent total ordering of all the events in the system  18.1.1 the happened-before relation since we are considering only sequential processes  all events executed in a single process are totally ordered also  by the law of causality  a message can be received only after it has been sent therefore  we can define the happenedbefore relation  denoted by  +  on a set of events as follows  assuming that sending and receiving a message constitutes an event   1 if a and bare events in the same process  and a was executed before b  then a  + b  if a is the event of sending a message by one process and b is the event of receiving that message by another process  then a + b  3 if a + band b + c  then a + c  since an event can not happen before itself  the  + relation is an irreflexive partial ordering  if two events  a and b  are not related by the  + relation  that is  a did not happen before b  and b did not happen before a   then we say that these two events were executed in this case  neither event can causally affect the other if  however  a  + b  it is possible for event a to affect event b causally  a space-time diagram  such as that in figure 18.1  can best illustrate the definitions of concurrency and happened-before the horizontal direction represents space  that is  different processes   and the vertical direction represents time the labeled vertical lines denote processes  or processors   the labeled dots denote events a wavy line denotes a message sent from one process to another events are concurrent if and only if no path exists between them  for example  these are some of the events related by the happened-before relation il figure 18.1  p1  + q2 ro  + q4 q3 + r4 p1  + q4  sil ce p1  + q2 and q2  + q4  these are some of the concurrent events in the system  qo and p2 ro and q3 ro and p3 q3 and p3 we can not know which of two concurrent events  such as qo and p2  happened first however  since neither event can affect the other  there is no way for one of them to know whether the other has occurred yet   it is not important which 18.1 729 p 0 figure 18.1 relative time for three concurrent processes  happened first it is important only that any processes that care about the order of two concurrent events agree on son1.e order  18.1.2 implementation to determine that an event a happened before an event b  we need either a common clock or a set of perfectly synchronized clocks since neither of these is available in a distributed system  we must defil e the happened-before relation without the use of physical clocks  first we associate with each system event a we can then define the requirement  for every pair of events a and b  if a + b  then the timestamp of a is less than the timestamp of b  below  we will see that the converse need not be true  how do we enforce the global ordering requirement in a distributed environment we define within each process pi a logical lci the logical clock can be implemented as a simple counter incremented between any two successive events executed within a process since the logical clock has a increasing value  it assigns a unique number to every event  and if an event a occurs before event bin process pi  then lci  a  lc  b   the timestamp for an event is the value of the logical clock for that event this scheme ensures that for any two events in the same process the global orderil g requirement is met  unfortunately  this scheme does not ensure that the global ordering requirement is met across processes to illustrate the problem  consider two processes p1 and p2 that communicate with each other suppose that p1 sends a message to p2  event a  with lc1  a  = 200  and p2 receives the message  event b  with lc2  b  = 195  because the processor for p2 is slower than the processor for p1  its logical clock ticks more slowly   this situation violates our requirement  since a + b but the timestamp of a is greater than the timestamp of b  to resolve this difficulty  we require a process to advance its logical clock when it receives a message whose timestamp is greater than the current value of its logical clock in particulm ~ if process pi receives a message  event b  with timestamp t and lc  b     ; t  tlcenit should advance its clock so that lci  b  = t + 1 thus  in our example  when p2 receives the message from p1  it will advance its logical clock so that lc2  b  = 201  730 chapter 18 18.2 finally  to realize a total ordering  we need only observe that  with our timestamp-ordering scheme  if the timestamps of two events  a and b  are the same  then the events are concurrent in this case  we may use process identity numbers to break ties and to create a total ordering the use of tirnestamps is further discussed in section 18.4.2  in this section  we present a number of different algorithms for implementing mutual exclusion in a distributed environment we assume that the system consists of n processes  each of which resides at a different processor to simplify our discussion  we assume that processes are numbered uniquely from 1 to n and that a one-to-one mapping exists between processes and processors  that is  each process has its own processor   18.2.1 centralized approach in a centralized approach to providing mutual exclusion  one of the processes in the system is chosen to coordinate the entry to the critical section each process that wants to invoke mutual exclusion sends a request message to the coordinator when the process receives a reply message from the coordinator  it can enter its critical section after exiting its critical section  the process sends a release message to the coordinator and proceeds with its execution  on receiving a request message  the coordinator checks to see whether some other process is in its critical section if no process is in its critical section  the coordinator immediately sends back a reply message otherwise  the request is queued when the coordinator receives a release message  it removes one of the request messages from the queue  in accordance with some scheduling algorithm  and sends a reply message to the requesting process  it should be clear that this algorithm ensures mutual exclusion in addition  if the scheduling policy within the coordinator is fair-such as first-come  firstserved  fcfs  scheduling-no starvation can occur this scheme requires three messages per critical-section entry  a reques  a reply  and a release  if the coordinator process fails  then a new process must take its place  in section 18.6  we describe some algorithms for electing a unique new coordinator once a new coordinator has been elected  it must poll all the processes in the system to reconstruct its request queue once the queue has been constructed  the computation can resume  18.2.2 fully distributed approach if we want to distribute the decision making across the entire system  then the solution is far more complicated one approach  described next  uses an algorithm based on the event-ordering scheme described in section 18.1  when a process g wants to enter its critical section  it generates a new timestamp  ts  and sends the message request  p ;  ts  to all processes in the system  including itself   on receiving a request message  a process may reply immediately  that is  send a reply message back to p ;   or it may defer sending a reply back  because it is already in its critical section  for example   a process that has received a reply message from all other processes in the system can 18.2 731 enter its critical section  queueing incmning requests and deferring them after exiting its critical section  the process sends reply messages to all its deferred requests  the decision whether process pi replies immediately to a request  pj  ts  message or defers its reply is based on three factors  if process pi is in its critical section  then it defers its reply to pj  if process pi does not want to enter its critical section  then it sends a reply immediately to p j  if process p ; wants to enter its critical section but has not yet entered it  then it compares its own request timestamp with the timestamp of the incoming request made by process pj if its own request timestamp is greater than that of the incoming request  then it sends a reply immediately to pj  pj asked first   otherwise  the reply is deferred  this algorithm exhibits the following desirable behavior  mutual exclusion is obtained  freedom from deadlock is ensured  freedom from starvation is ensured  since entry to the critical section is scheduled according to the timestamp ordering the timestamp ordering ensures that processes are served in fcfs order  the number of messages per critical-section entry is 2 x  n-1   this number represents the minimum number of required messages per critical-section entry when processes act independently and concurrently  to illustrate how the algorithm functions  we consider a system consisting of processes p1  p2  and p3  suppose that processes p1 and p3 want to enter their critical sections process p1 then sends a message request  p1  timestamp = 10  to processes p2 and p3  while process p3 sends a message request  p3  timestamp = 4  to processes p1 and p2   the timestamps 4 and 10 were obtained from the logical clocks described in section 18.1  when process p2 receives these request messages  it replies immediately when process p1 receives the request from process p3  it replies immediately  since the timestamp  10  on its own request message is greater than the timestamp  4  for process p3 when process p3 receives the request message from process p1  it defers its reply  since the timestamp  4  on its request message is less than the timestamp  10  for the message from process p1  on receiving replies from both process p1 and process p2  process p3 can enter its critical section after exiting its critical section  process p3 sends a reply to process p1  which can then enter its critical section  because this scheme requires the participation of all the processes in the system  it has three undesirable consequences  the processes need to know the identity of all other processes in the system when a new process joins the group of processes participating in the mutual-exclusion algorithm  the following actions need to be taken  732 chapter 18 a the process must receive the names of all the other processes in the gro11p  b the name of the new process must be distributed to all the other processes in the group  this task is not as trivial as it may seem  since some request and reply messages may be circulating in the system when the new process joins the group the interested reader is referred to the bibliographical notes at the end of the chapter  if one process fails  then the entire scheme collapses we can resolve this difficulty by continuously monitoring the state of all processes in the system if one process fails  then all other processes are notified  so that they will no longer send request messages to the failed process when a process recovers  it must initiate the procedure that allows it to rejoin the group  processes that have not entered their critical section must pause frequently to assure other processes that they intend to enter the critical section  because of these difficulties  this protocol is best suited for small  stable sets of cooperating processes  18.2.3 token-passing approach another method of providilcg mutual exclusion is to circulate a token among the processes ilc the system a is a special type of message that is passed from process to process possession of the token entitles the holder to enter the critical section since there is only a single token  only one process can be in its critical section at a time  we assume that the processes in the system are logically organized ilc a the physical communication network need not be a ring as long as the processes are connected to one another  it is possible to implement a logical ring to implement mutual exclusion  we pass the token around the ring when a process receives the token  it may enter its critical section  keeping the token  after the process exits its critical section  the token is passed around again  if the process receiving the token does not want to enter its critical section  it passes the token to its neighbor this scheme is similar to algorithm 1 in chapter 6  but a token is substituted for a shared variable  if the ring is unidirectional  freedom from starvation is ensured the number of messages required to implement mutual exclusion may vary from one message per entry  in the case of high contention  that is  every process wants to enter its critical section   to an infinite number of messages  in the case of low contention  that is  no process wants to enter its critical section   two types of failure must be considered first  if the token is lost an election must be called to generate a new token second  if a process fails  a new logical ring must be established in section 18.6  we present an election algorithm ; others are possible the development of an algorithm for reconstructing the ring is left to you in exercise 18.4  18.3 18.3 733 in chapter 6  we introduced the concept of an atomic transaction  which is a program unit that must be executed  that is  either all the operations associated with it are executed to completion  or none are performed when we are dealing with a distributed system  ensuring the atomicity of a transaction becomes much more complicated than in a centralized system this difficulty occurs because several sites may be participating in the execution of a single transaction the failure of one of these sites  or the failure of a communication link connecting the sites  may result in erroneous computations  ensuring that the execution of transactions in the distributed system preserves atomicity is the function of the each site has its own local transaction coordinator  which is responsible for coordinating the execution of all the transactions initiated at that site for each such transaction  the coordinator is responsible for the following  starting the execution of the transaction breaking the transaction into a number of subtransactions and distributing these subtransactions to the appropriate sites for execution coordinating the termination of the transaction  which may result in the transactions being committed at all sites or aborted at all sites we assume that each local site maintains a log for recovery purposes  18.3.1 the two-phase commit protocol for atomicity to be ensured  all the sites in which a transaction t has executed must agree on the final outcome of the execution t must either commit at all sites  or it must abort at all sites to ensure this property  the transaction coordinator of t must execute a among the simplest and most widely used commit protocols is the which we discuss next  assume that t is a transaction initiated at site s ; and that the transaction coordinator at s ; is c ;  when t completes its execution-that is  when all the sites at which t has executed inform c ; that t has completed -then c starts the 2pc protocol  phase 1 c adds the record prepare t to the log and forces the record onto stable storage it then sends a prepare  t  message to all the sites at which t has executed on receiving the message  the transaction manager at each of these sites determines whether it is willing to commit its portion oft if the answer is no  it adds a record no t to the log  and then it responds by sending an abort  t  message to c ;  if the answer is yes  it adds a record ready t to the log and forces all the log records corresponding to t onto stable storage the transaction manager then replies with a ready  t  message to c  phase 2 when c ; has received responses to the prepare  t  message from all the sites  or when a pre-specified interval of time has elapsed since the prepare  t  message was sent out  c ; can determine whether the transaction 734 chapter 18 t can be committed or aborted transaction t can be committed if c ; has received a ready  t  m ~ essage from all the participating sites otherwise  transaction t must be aborted depending on the verdict  either a record commit t or a record abort t is added to the log and forced onto stable storage at this point  the fate of the transaction has been sealed  following this  the coordinator sends either a commit  t  or an abort  t  message to all participating sites when a site receives that message  it records the message in the log  a site at which t has executed can unconditionally abort tat any time prior to its sending the message ready  t  to the coordinator the ready  t  message is  in effect  a promise by a site to follow the coordinator 's order to commit tor to abort t a site can make such a promise only when the needed information is stored in stable storage otherwise  if the site crashes after sending ready  t   it may be unable to make good on its promise  since unanimity is required to commit a transaction  the fate of t is sealed as soon as at least one site responds with abort  t   note that the coordinator site 5 ; can decide unilaterally to abort t  as it is one of the sites at which t has executed the final verdict regarding t is determined at the time the coordinator writes that verdict  commit or abort  to the log and forces it to stable storage  in some implementations of the 2pc protocol  a site sends an acknowledge  t  message to the coordinator at the end of the second phase of the protocol  when the coordinator has received the acknowledge  t  message from all the sites  it adds the record complete t to the log  18.3.2 failure handling in 2pc we now examine in detail how 2pc responds to various types of failures as we shall see  one major disadvantage of the 2pc protocol is that coordinator failure may result in blocking  and a decision either to commit or to abort t may have to be postponed until the coordinator recovers  18.3.2.1 failure of a participating site when a participating site s ~ c recovers from a failure  it must examine its log to determine the fate of those transactions that were in the midst of execution when the failure occurred suppose that t is one such transaction how will s ~ c deal with t we consider each of the possible alternatives  the log contain ~ s a commit t record in this case  the site executes redo  t   the log contains an abort t record in this case  the site executes undo  t   the log contains a ready t record in this case  the site must consult c ; to determine the fate oft if c ; is up  it tells s ~ c whether t committed or aborted in the former case  it executes redo  t  ; in the latter case  it executes undo  t   if c ; is down  s ~ c must try to find out the fate of t from other sites it does so by sending a query-status  t  message to all the sites in the system on receiving such a message  a site must consult 18.3 735 its log to determine whether t has executed there and  if so  whether t committed or aborted it then notifies s  about this outcome if no site has the appropriate information  that is  whether t corrnni tted or aborted   then s ~ c can neither abort nor commit t the decision concerning t is postponed until s1c can obtain the needed information thus  s ~ c must periodically resend the query-status  t  message to the other sites it does so until a site responds with the needed information the site at which c ; resides always has this inforn1.ation  the log contains no control records  abort  commit  ready  concerning t  the absence of control records implies that s1c failed before responding to the prepare  t  message from c ;  since the failure of s1c means that it could not have sent such a response  by our algorithm  c ; must have aborted t  hence  s1c must execute undo  t   18.3.2.2 failure of the coordinator if the coordinator fails in the midst of the execution of the commit protocol for transaction t  then the participating sites must decide the fate oft we shall see that  in certain cases  the participating sites can not decide whether to commit or abort t  and therefore these sites must wait for the recovery of the failed coordinator  if an active site contains a commit t record in its log  then t must be committed  if an active site contains an abort t record in its log  then t must be aborted  if some active site does not contain a ready t record in its log  then the failed coordinator c ; can not have decided to commit t we can draw this conclusion because a site that does not have a ready t record in its log can not have sent a ready  t  message to c ;  however  the coordinator may have decided to abort t rather than wait for c ; to recover  it is preferable to abort tin this case  if none of the preceding cases holds  then all the active sites must have a ready t record in their logs  but no additional control records  such as abort t or commit t   since the coordinator has failed  it is impossible to determine whether a decision has been made-or  if so  what that decision is-until the coordinator recovers thus  the active sites must wait for c to recover as long as the fate oft remains in doubt  t may continue to hold system resources for example  if locking is used  t may hold locks on data at active sites such a situation is undesirable because hours or days may pass before c ; is again active during this time  other transactions may be forced to wait for t as a result  data are unavailable not only on the failed site  c ;  but on active sites as well the amount of unavailable data increases as the downtime of c ; grows this situation is called the blocking problem  because t is blocked pending the recovery of site c ;  736 chapter 18 18.4 18.3.2.3 failure of the network when a link fails  the messages in the process of being routed through the link do not arrive at their destinations intact from the viewpoint of the sites connected throughout that link  the other sites appear to have failed thus  our previous schemes apply here as well when a number of links fail  the network may partition in this case  two possibilities exist the coordinator and all its participants may remain in one partition ; in this case  the failure has no effect on the commit protocol alternatively  the coordinator and its participants may belong to several partitions ; in this case  messages between the participant and the coordinator are lost  reducing the case to a link failure  we move next to the issue of concurrency control in this section  we show how certain of the concurrency-control schemes discussed in chapter 6 can be modified for use in a distributed environment  the transaction manager of a distributed database system manages the execution of those transactions  or subtransactions  that access data stored in a local site each such transaction may be either a local transaction  that is  a transaction that executes only at that site  or part of a global transaction  that is  a transaction that executes at several sites   each transaction manager is responsible for maintaining a log for recovery purposes and for participating in an appropriate concurrency-control scheme to coordinate the concurrent execution of the transactions executing at that site as we shall see  the concurrency schemes described in chapter 6 need to be modified to accommodate the distribution of transactions  18.4.1 locking protocols the two-phase locking protocols described in chapter 6 can be used in a distributed environment the only change needed is iil the way the lock manager is implemented here  we present several possible schemes the first deals with the case where no data replication is allowed the others apply to the more general case where data can be replicated in several sites as in chapter 6  we assume the existence of the shared and 18.4.1.1 nonreplicated scheme if no data are replicated in the system  then the locking schemes described in section 6.9 can be applied as follows  each site maintains a local lock manager whose function is to administer the lock and unlock requests for those data items stored in that site when a transaction wishes to lock data item qat site s ;  it simply sends a message to the lock manager at site s ; requesting a lock  in a particular lock mode   if data item q is locked in an incompatible mode  then the request is delayed until that request can be granted once it has been determined that the lock request can be granted  the lock n1.anager sends a message back to the initiator indicating that the lock request has been granted  18.4 737 this scheme has the advantage of simple implementation it requires two message transfers for handling lock requests and one ncessage transfer for handling unlock requests however  deadlock handling is more complex since the lock and unlock requests are no longer made at a single site  the various deadlock-handling algorithms discussed in chapter 7 must be modified ; these modifications are discussed in section 18.5  18.4.1.2 single-coordinator approach several concurrency-control schemes can be used in systems that allow data replication under the single-coordinator approach  the system maintains a single lock manager that resides in a single chosen site-say  si all lock and unlock requests are made at site si when a transaction needs to lock a data item  it sends a lock request to si the lock manager determines whether the lock can be granted immediately if so  it sends a message to that effect to the site at which the lock request was initiated otherwise  the request is delayed until it can be granted ; and at that time  a message is sent to the site at which the lock request was initiated the transaction can read the data item from any one of the sites at which a replica of the data item resides in the case of a write operation  all the sites where a replica of the data item resides must be involved in the writing  the scheme has the following advantages  simple implementation this scheme requires two messages for handling lock requests and one message for handling lmlock requests  simple deadlock handling since all lock and unlock requests are made at one site  the deadlock-handling algorithms discussed in chapter 7 can be applied directly to this environment  the disadvantages of the scheme include the following  bottleneck the site si becomes a bottleneck  since all requests must be processed there  vulnerability if the site si fails  the concurrency controller is lost either processing must stop or a recovery scheme must be used  a compromise between these advantages and disadvantages can be achieved through a in which the lockmanager function is distributed over several sites each lock manager administers the lock and unlock requests for a subset of the data items this distribution reduces the degree to which the coordinator is a bottleneck  but it complicates deadlock handling  since the lock and unlock requests are not made at a single site  18.4.1.3 majority protocol the majority protocol is a modification of the nonreplicated data scheme presented earlier the system maintains a lock manager at each site each manager controls the locks for all the data or replicas of data stored at that site  when a transaction wishes to lock a data item q that is replicated inn different 738 chapter 18 sites  it must send a lock request to nlore than one-half of then sites in which q is stored each lock manager detern1.ines whether the lock can be granted immediately  as far as it is concerned   as before  the response is delayed until the request can be granted the transaction does not operate on q until it has successfully obtained a lock on a majority of the replicas of q this scheme deals with replicated data in a decentralized manner  thus avoiding the drawbacks of central control however  it suffers from its own disadvantages  implementation the majority protocol is more complicated to implement than the previous schemes it requires 2  n/2 + 1  messages for handling lock requests and  n/2 + 1  messages for handling unlock requests  deadlock handling since the lock and unlock requests are not made at one site  the deadlock-handling algorithms must be modified  section 18.5   in addition  a deadlock can occur even if only one data item is being locked to illustrate  consider a system with four sites and full replication  suppose that transactions t1 and t2 wish to lock data item q in exclusive mode transaction t1 may succeed in locking q at sites 51 and 53  while transaction t2 may succeed in locking q at sites 52 and 54 each then must wait to acquire the third lock  and hence a deadlock has occurred  18.4.1.4 biased protocol the biased protocol is similar to the majority protocol the difference is that requests for shared locks are given more favorable treatment than are requests for exclusive locks the system maintains a lock manager at each site each manager manages the locks for all the data items stored at that site shared and exclusive locks are handled differently  shared locks when a transaction needs to lock data item q  it simply requests a lock on q from the lock manager at one site containing a replica of q exclusive locks when a transaction needs to lock data item q  it requests a lock on q from the lock manager at each site containing a replica of q as before  the response to the request is delayed until the request can be granted  the scheme has the advantage of imposing less overhead on read operations than does the majority protocol this advantage is especially significant in common cases in which the frequency of reads is much greater than the frequency of writes however  the additional overhead on writes is a disadvantage  furthermore  the biased protocol shares the majority protocol 's disadvantage of complexity in handling deadlock  18.4.1.5 primary copy yet another alternative is to choose one of the replicas as the primary copy  thus  for each data item q  the primary copy of q must reside in precisely one site  which we call the primary site of q when a transaction needs to lock a data 18.4 739 item q  it requests a lock at the primary site of q again  the response to the request is delayed until the request can be granted  this scheme enables us to handle concurrency control for replicated data in much the same way as for unreplicated data implementation of the method is simple however  if the primary site of q fails  q is inaccessible even though other sites containing a replica may be accessible  18.4.2 timestamping the principal idea behind the timestamping scheme discussed in section 6.9 is that each transaction is given a unique timestamp  which is used to decide the serialization order our first task  then  in generalizing the centralized scheme to a distributed scheme is to develop a method for generating unique timestamps  our previous protocols can then be applied directly to the nonreplicated environment  18.4.2.1 generation of unique timestamps two primary methods are used to generate unique timestamps ; one is centralized  and one is distributed in the centralized scheme  a single site is chosen for distributing the timestamps the site can use a logical counter or its own local clock for this purpose  in the distributed scheme  each site generates a local unique timestamp using either a logical counter or the local clock the global unique timestamp is obtained by concatenation of the local unique timestamp with the site identifier  which must also be unique  figure 18.2   the order of concatenation is important ! we use the site identifier in the least sign.ificant position to ensure that the global timestamps generated in one site are not always greater than those generated in another site compare this technique for generating unique timestamps with the one presented in section 18.1.2 for generating unique na1nes  we may still have a problem if one site generates local timestamps at a faster rate than do other sites in such a case  the fast site 's logical counter will be larger than those of other sites therefore  all timestamps generated by the fast site will be larger than those generated by other sites a mechanism is needed to ensure that local timestamps are generated fairly across the system  to accomplish the fair generation of timestamps  we define within each site si a logical clock  lc   which generates the local timestamp  see section 18.1.2   to ensure that the various logical clocks are synchronized  we require that a site local unique timestamp site identifier ll 0 global unique identifier figure 18.2 generation of unique timestamps  740 chapter 18 18.5 si advance its logical clock whenever a transaction ~ with timestarnp x,y visits that site and xis greater than the current value of lc in this case  site si advances its logical clock to the value x + 1  if the system clock is used to generate timestamps  then timestamps are assigned fairly  provided that no site has a system clock that runs fast or slow  since clocks may not be perfectly accurate  a technique similar to that used for logical clocks must be used to ensure that no clock gets far ahead or far behind another clock  18.4.2.2 timestamp-ordering scheme the basic timestamp scheme introduced in section 6.9.4.3 can be extended in a straightforward mam'ler to a distributed system as in the centralized case  cascading rollbacks may result if no mechanism is used to prevent a transaction from reading a data item value that is not yet committed to eliminate cascading rollbacks  we can con bine the basic timestamp scheme of section 6.9 with the 2pc protocol of section 18.3 to obtain a protocol that ensures serializability with no cascading rollbacks we leave the development of such an algorithm to you  the basic timestamp scheme just described suffers from the undesirable property that conflicts between transactions are resolved through rollbacks  rather than through waits to alleviate this problem  we can buffer the various read and write operations  that is  delay them  until a time when we are assured that these operations can take place without causing aborts a read  x  operation by ~ must be delayed if there exists a transaction tj that will perform a wri te  x  operation but has not yet done so and ts  tj  ts  ~   similarly  a wri te  x  operation by ~ must be delayed if there exists a transaction tj that will perform either a read  x  or a wri te  x  operation and ts  tj  ts  t ;   various methods are available for this property one such method  called the scheme  requires each site to maintain a read queue and a write queue consisting of all the read and write requests that are to be executed at the site and that must be delayed to preserve the property just described we shall not present the scheme here again  we leave the development of the algorithm to you  the deadlock-prevention  deadlock-avoidance  and deadlock-detection algorithms presented in chapter 7 can be extended so that they can be used in a distributed system in this section  we describe several of these distributed algorithms  18.5.1 deadlock prevention and avoidance the deadlock-prevention and deadlock-avoidance algorithms presented in chapter 7 can be used in a distributed system  provided that appropriate modifications are made for example  we can use the resource-ordering deadlock-prevention technique by simply defining a global ordering among the system resources that is  all resources in the entire system are assigned unique numbers  and a process may request a resource  at any processor  with 18.5 741 unique nl1mber i only if it is not holding a resource with a unique number greater than i similarly  we can use the banker 's algorithm in a distributed systen'l by designating one of the processes in the system  the banker  as the process that maintains the information necessary to carry out the banker 's algorithm every resource request must be channeled through the banker  the global resource-ordering deadlock-prevention scheme is simple to implement in a distributed environment and requires little overhead the banker 's algorithm can also be implemented easily  but it may require too much overhead the banker may become a bottleneck  since the number of messages to and from the banker may be large thus  the banker 's scheme does not seem to be of practical use in a distributed system  we turn next to a new deadlock-prevention scheme based on a timestampordering approach with resource preemption although this approach can handle any deadlock situation that may arise in a distributed system  for simplicity we consider only the case of a single instance of each resource type  to control the preemption  we assign a unique priority number to each process these numbers are used to decide whether a process p ; should wait for a process pj for example  we can let p ; wait for pj if p ; has a priority higher than that of pj ; otherwise  p ; is rolled back this scheme prevents deadlocks because  for every edge p ;  + pi in the wait-for graph  p ; has a higher priority than p ;  thus  a cycle can not exist  one difficulty with this scheme is the possibility of starvation some processes with extremely low priorities may always be rolled back this difficulty can be avoided through the use of timestamps each process in the system is assigned a unique timestamp when it is created two complementary deadlock-prevention schemes using timestamps have been proposed  1 the wait-die scheme this approach is based on a nonpreemptive teclmique when process p ; requests a resource currently held by pj  p ; is allowed to wait only if it has a smaller timestamp than does pj  that is  p ; is older than pj   otherwise  p ; is rolled back  dies   for example  suppose that processes p1  p2  and p3 have timestamps 5  10  and 15  respectively  if p1 requests a resource held by p2  p1 will wait if p3 requests a resource held by p2  p3 will be rolled back  the wound-wait scheme this approach is based on a preemptive technique and is a counterpart to the wait-die approach when process p ; requests a resource currently held by pj  p ; is allowed to wait only if it has a larger timestamp than does pi  that is  p ; is younger than pi   otherwise  pi is rolled back  pj is wounded by p ;   returning to our previous example  with processes p1  p2  and p3  if p1 requests a resource held by p2  then the resource will be preempted from p2  and p2 will be rolled back if p3 requests a resource held by p2  then p3 will wait  both schemes can avoid starvation provided that  when a process is rolled back  it is not assigned a new timestamp since timestamps always increase  a process that is rolled back will eventually have the smallest timestamp thus  it will not be rolled back again there are  however  significant differences in the way the two schemes operate  742 chapter 18 in the wait-die scheme  an older process must wait for a younger one to release its resource thus  the older the process gets  the more it tends to wait by contrast  in the wound-wait scheme  an older process never waits for a younger process  in the wait-die scheme  if a process pi dies and is rolled back because it has requested a resource held by process pj  then pi n ay reissue the same sequence of requests when it is restarted if the resource is still held by pj  then pi will die again thus  pi may die several times before acquiring the needed resource contrast this series of events with what happens in the wound-wait scheme process pi is wounded and rolled back because pi has requested a resource it holds when pi is restarted and requests the resource now being held by pi  pi waits thus  fewer rollbacks occur in the wound-wait scheme  the major problem with both schemes is that unnecessary rollbacks may occur  18.5.2 deadlock detection the deadlock-prevention algorithm may preempt resources even if no deadlock has occurred to prevent um ecessary preemptions  we can use a deadlockdetection algorithm we construct a wait-for graph describing the resourceallocation state since we are assuming only a single resource of each type  a cycle in the wait-for graph represents a deadlock  the main problem in a distributed system is deciding how to maintain the wait-for graph we illustrate this problem by describing several common techniques to deal with this issue these schemes require each site to keep a local wait-for graph the nodes of the graph correspond to all the processes  local as well as nonlocal  currently holding or requesting any of the resources local to that site for example  in figure 18.3 we have a system consisting of two sites  each maintaining its local wait-for graph note that processes p2 and p3 appear in both graphs  indicating that the processes have requested resources at both sites  these local wait-for graphs are constructed in the usual manner for local processes and resources when a process pi in site 51 needs a resource held by process pj in site 52  a request message is sent by pi to site 52  the edge pi  + pj is then inserted in the local wait-for graph of site 52  figure 18.3 two local wait-for graphs  18.5 743 figure 18.4 global wait-for graph for figure 18.3  clearly  if any local wait-for graph has a cycle  deadlock has occurred the fact that we find no cycles in any of the local wait-for graphs does not mean that there are no deadlocks  however to illustrate this problem  we consider the system depicted in figure 18.3 each wait-for graph is acyclic ; nevertheless  a deadlock exists in the system to prove that a deadlock has not occurred  we must show that the of all local graphs is acyclic the graph  figure 18.4  that we obtain from the union of the two wait-for graphs of figure 18.3 does indeed contain a cycle  implying that the system is in a deadlocked state  a number of methods are available to organize the wait-for graph in a distributed system we describe several common schemes here  18.5.2.1 centralized approach in the centralized approach  a global wait-for graph is constructed as the tmion of all the local wait-for graphs it is maintained in a single process  the since there is communication delay in the system  we must distinguish between two types of wait-for graphs the real graph describes the real but unknown state of the system at any point in time  as would be seen by an ormliscient observer the constructed graph is an approximation generated by the coordinator during the execution of its algorithm the constructed graph must be generated so that  whenever the detection algorithm is invoked  the reported results are correct by correct we mean the following  if a deadlock exists  then it is reported properly  if a deadlock is reported  then the system is indeed in a deadlocked state  as we shall show  it is not easy to construct such correct algorithms  the wait-for graph may be constructed at three different points in time  whenever a new edge is inserted in or removed from one of the local wait-for graphs periodically  when a number of changes have occurred in a wait-for graph whenever the deadlock-detection coordinator needs to invoke the cycledetection algorithm when the deadlock-detection algorithm is invoked  the coordinator searches its global graph if a cycle is found  a victim is selected to be rolled back the 744 chapter 18 coordinator must notify all the sites that a particular process has been selected as victiitl the sites  in turn  roll back the victim process  next  we consider each of the three graph-construction options listed above with option 1  whenever an edge is either inserted in or removed from a local graph  the local site must also send a message to the coordinator to notify it of this modification on receiving such a message/ the coordinator updates its global graph  alternatively  option 2  1 a site can send a number of such changes in a single message periodically returning to our previous example/ the coordinator process will maintain the global wait-for graph as depicted in figure 18.4  when site 52 inserts the edge p3  + p4 in its local wait-for graph/ it also sends a message to the coordinator similarly/ when site 51 deletes the edge p5  + p1 because p1 has released a resource that was requested by ps 1 an appropriate message is sent to the coordinator  note that no matter which of these two options is used/ unnecessary rollbacks may occur  as a result of two situations  1 false may exist in the global wait-for graph to illustrate this point we consider a snapshot of the system as depicted in figure 18.5 suppose that p2 releases the resource it is holding in site 511 resulting in the deletion of the edge p1  + p2 in site 51 process p2 then requests a resource held by p3 at site 521 resulting in the addition of the edge p2  + p3 in site 52 if the insert p2  + p3 message from site 52 arrives before the delete p1  + p2 message from site 51/ the coordinator may discover the false cycle p1  + p2  + p3  + p1 after the insert  but before the delete   deadlock recovery may be initiated  although no deadlock has occurred  2 unnecessary rollbacks may also result when a deadlock has indeed occurred and a victim has been picked1 but at the same time one of the processes has been aborted for reasons unrelated to the deadlock  as when a process has exceeded its allocated time   for example/ suppose that site 51 in figure 18.3 decides to abort p2 at the same time/ the coordinator has discovered a cycle and picked p3 as a victim both p2 and p3 are now rolled back1 although only p2 needed to be rolled back now consider a centralized deadlock-detection algorithm using option 3 that detects all deadlocks that actually occur and does not detect false deadlocks to avoid the report of false deadlocks/ we require that requests coordinator figure 18.5 local and global wait-for graphs  18.5 745 from different sites be appended with unique identifiers  or timestamps   when process p ;  at site 511 requests a resource from pj  at site 52  a request message with timestamp t5 is sent the edge p ; ~ p1 with the label t5 is inserted in the local wait-for graph of 51 this edge is inserted in the local wait-for graph of site 52 only if site 52 has received the request message and can not immediately grant the requested resource a request from p ; to p1 in the same site is handled in the usual manner ; no timestamps are associated with the edge p ; ~ pi  the detection algorithm is as follows  the controller sends an initiating message to each site in the system  on receiving this message  a site sends its local wait-for graph to the coordinator each of these wait-for graphs contains all the local information the site has about the state of the real graph the graph reflects an instantaneous state of the site  but it is not synchronized with respect to any other site  when the controller has received a reply from each site  it constructs a graph as follows  a the constructed graph contains a vertex for every process in the system  b the graph has an edge p ; ~ p1 if and only if there is an edge p ; ~ p1 in one of the wait-for graphs or an edge p ; ~ pj with some label t5 in more than one wait-for graph  if the constructed graph contains a cycle  then the system is in a deadlocked state if the constructed graph does not contain a cycle  then the system was not in a deadlocked state when the detection algorithm was invoked as result of the initiating messages sent by the coordinator  in step 1   18.5.2.2 fully distributed approach in the all controllers share equally the responsibility for detecting deadlock every site constructs a waitfor graph that represents a part of the total graph  depending on the dynamic behavior of the system the idea is that  if a deadlock exists  a cycle will appear in at least one of the partial graphs we present one such algorithm  which involves construction of partial graphs in every site  each site maintains its own local wait-for graph a local wait-for graph in this scheme differs from the one described earlier in that we add one additional node pex to the graph an arc p ; ~ pex exists in the graph if p ; is waiting for a data item in another site being held by any process similarly  an arc pex ~ pi exists in the graph if a process at another site is waiting to acquire a resource currently being held by p1 in this local site  to illustrate this situation  we consider again the two local wait-for graphs of figure 18.3 the addition of the node pex in both graphs results in the local wait-for graphs shown in figure 18.6  if a local wait-for graph contains a cycle that does not involve node pm then the system is in a deadlocked state if  however  a local graph contains a cycle involving pex  then this implies the possibility of a deadlock  746 chapter 18 figure 18.6 augmented local wait-for graphs from figure 18.3  to ascertain whether a deadlock does exist  we must invoke a distributed deadlock -detection algorithm  suppose that  at site si  the local wait-for graph contains a cycle involving node pex this cycle must be of the form which indicates that process pk  in site si is waiting to acquire a data item located in some other site-say  sj on discovering this cycle  site si sends to site si a deadlock-detection message containing information about that cycle  when site si receives this deadlock-detection message  it updates its local wait-for graph with the new information then it searches the newly constructed wait-for graph for a cycle not involving pex if one exists  a deadlock is found  and an appropriate recovery scheme is iiwoked if a cycle involving pex is discovered  then si transmits a deadlock-detection message to the appropriate site-say  sk site sic ! in return  repeats the procedure  thus  after a finite number of rounds  either a deadlock is discovered or the deadlock-detection computation halts  to illustrate this procedure  we consider the local wait-for graphs of figure 18.6 suppose that site 51 discovers the cycle since p3 is waiting to acquire a data item in site 52  a deadlock-detection message describing that cycle is transmitted from site 51 to site 52  when site 52 receives this message  it updates its local wait-for graph  obtaining the wait-for graph of figure 18.7 this graph contains the cycle which does not include node pex therefore  the system is in a deadlocked state  and an appropriate recovery scheme must be invoked  note that the outcome would be the same if site 52 discovered the cycle first in its local wait-for graph and sent the deadlock-detection message to site 51  in the worst case  both sites will discover the cycle at about the same time  and two deadlock-detection messages will be sent  one by 51 to 52 and another by s2 to 51 this situation results in unnecessary message transfer and overhead in updating the two local wait-for graphs and searching for cycles in both graphs  18.6 18.6 747 figure 18.7 augmented local wait-for graph in site 5.z of figure 18.6  to reduce message traffic  we assign to each process pi a unique identifier  which we denote id  pi   when site sk discovers that its local wait-for graph contains a cycle involving node pex of the form it sends a deadlock-detection message to another site only if id  pi j id  pr j  otherwise  site s1c continues its normal execution  leaving the burden of initiating the deadlock-detection algorithm to some other site  to illustrate this scheme  we consider again the wait-for graphs maintained at sites 51 and 52 as shown in figure 18.6 suppose that suppose both sites discover these local cycles at about the same time the cycle in site 51 is of the form since id  p3  id  p2   site 51 does not send a deadlock-detection message to site 52  the cycle in site 52 is of the form since id  p2  id  p3   site 52 does send a deadlock-detection message to site 51  which  on receiving the message  updates its local wait-for graph site s1 then searches for a cycle in the graph and discovers that the system is in a deadlocked state  as we pointed out in section 18.3  many distributed algorithms employ a coordinator process that performs functions needed by the other processes in 748 chapter 18 the systenc these functions include enforcing mutual exclusion  maintaining a global wait-for graph for deadlock detection  replacing a lost token  and controlling an input or output device in the system if the coordinator process fails due to the failure of the site at which it resides  the system can continue execution only by restarting a new copy of the coordinator on some other site  the algorithms that determine where a new copy of the coordinator should be restarted are called election algorithms assume that a unique priority number is associated with each active process in the system for ease of notation  we assume that the priority number of process g is i to simplify our discussion  we assume a one-to-one correspondence between processes and sites and thus refer to both as processes the coordinator is always the process with the largest priority number hence  when a coordinator fails  the algorithm must elect that active process with the largest priority number this number must be sent to each active process in the system in addition  the algorithm must provide a mechanism for a recovered process to identify the current coordinator  in this section  we present examples of election algorithms for two different configurations of distributed systems the first algorithm applies to systems where every process can send a message to every other process in the system  the second algorithm applies to systems organized as a ring  logically or physically   both algorithms require n2 messages for an election  where n is the number of processes in the system we assume that a process that has failed knows on recovery that it has indeed failed and thus takes appropriate actions to rejoin the set of active processes  18.6.1 the bully algorithm suppose that process p ; sends a request that is not answered by the coordinator within a time interval t in this situation  it is assumed that the coordinator has failed  and p ; tries to elect itself as the new coordinator this task is completed through the following algorithm  process p ; sends an election message to every process with a higher priority number process p ; then waits for a time interval t for an answer from any one of these processes  if no response is received within time t  p ; assumes that all processes with numbers greater than i have failed and elects itself the new coordinator process p ; restarts a new copy of the coordinator and sends a message to inform all active processes with priority numbers less than i that p ; is the new coordinator  howeve1 ~ if an answer is received  p ; begins a time interval t '  waiting to receive a message informing it that a process with a higher priority number has been elected  that is  some other process is electing itself coordinator and should report the results within time t  if no message is received within t '  then the process with a higher number is assumed to have failed  and process p ; should restart the algorithm  if p ; is not the coordinator  then  at any time during execution  p ; may receive one of the following two messages from process pr p1 is the new coordinator  j i   process p ;  in turn  records this information  18.6 749 pi has started an election  j i   process p ; sends a response to pi and begins its own election algorithm  provided that p ; has not already initiated such an election  the process that completes its algorithm has the highest number and is elected as the coordinator it has sent its number to all active processes with lower numbers after a failed process recovers  it immediately begins execution of the same algorithm if there are no active processes with higher numbers  the recovered process forces all processes with lower numbers to let it become the coordinator process  even if there is a currently active coordinator with a lower number for this reason  the algorithm is termed the we can demonstrate the operation of the algorithm with a simple example of a system consisting of processes p1 through p4  the operations are as follows  all processes are active ; p4 is the coordinator process  p1 and p4 fail p2 determines that p4 has failed by sending a request that is not answered within time t p2 then begins its election algorithm by sending a request to p3  p3 receives the request  responds to p2  and begins its own algorithm by sending an election request to p4  p2 receives p3 's response and begins waiting for an interval t'  p4 does not respond within an interval t  so p3 elects itself the new coordinator and sends the number 3 to p2 and p1  p1 does not receive the number  since it has failed  later  when p1 recovers  it sends an election request to p2  p3  and p4  p2 and p3 respond to p1 and begin their own election algorithms p3 will again be elected  through the same events as before  finally  p4 recovers and notifies p1  p2  and p3 that it is the current coordinator  p4 sends no election requests  since it is the process with the highest number in the system  18.6.2 the ring algorithm the assumes that the links between processes are unidirectional and that each process sends its messages to the neighbor on the right the main data structure used by the algorithm is the lisi   a list that contains the priority numbers of all active processes in the system when the algorithm ends ; each process maintains its own active list the algorithm works as follows  if process p ; detects a coordinator failure  it creates a new active list that is initially empty it then sends a message elect  i  to its neighbor on the right and adds the number i to its active list  750 chapter 18 18.7 if pi receives a message elect  j  from the process on the left  it must respond in one of three ways  a if this is the first elect message it has seen or sent  pi creates a new active list with the numbers i and j it then sends the message elect  i   followed by the message elect  j   b ifi    f j-thatis,ifthe message received does not contain pi'snumber -then pi adds j to its active list and forwards the message to its neighbor on the right  c if i = j-that is  if pi receives the message elect  i  -then the active list for pi now contains the numbers of all the active processes in the system process g can now determine the largest number in the active list to identify the new coordinator process  this algorithm does not specify how a recovering process determines the number of the current coordinator process one solution requires a recovering process to send an inquiry message this message is forwarded around the ring to the current coordinator  which in turn sends a reply containing its number  for a system to be reliable  we need a mechanism that allows a set of processes to agree on a common value such an agreement may not take place  for several reasons first  the communication medium may be faulty  resulting in lost or garbled messages second  the processes themselves may be faulty  resultilcg lie unpredictable process behavior the best we can hope for in this case is that processes fail in a clean way  stopping their execution without deviating from their normal execution pattern in the worst case  processes may send garbled or incorrect messages to other processes or even collaborate with other failed processes in an attempt to the integrity of the system  the provides an analogy for this situation  several divisions byzantine army  each commanded by its own general  surround an enemy camp the byzantine generals must reach agreement on whether or not to attack the enemy at dawn it is crucial that all generals agree  since an attack by only some of the divisions would result lie defeat the various divisions are geographically dispersed  and the generals can communicate with one another only via messengers who run from camp to camp the generals may not be able to reach agreement for at least two major reasons  messengers may get caught by the enemy and thus may be unable to deliver their messages this situation corresponds to unreliable communication in a computer system and is discussed further in section 18.7.1  generals may be traitors  trying to prevent the loyal generals from reaching an agreement this situation corresponds to faulty processes lie a computer system and is discussed further in section 18.7.2  18.7 751 18.7.1 unreliable communications let us first assume that  if processes fail  they do so in a clean way and that the communication medium is unreliable suppose that process p ; at site 51  which has sent a message to process pj at site 52  needs to know whether p1 has received the message so that it can decide how to proceed with its computation for example  p ; may decide to compute a functionfoo if pj has received its message or to compute a function boo if p1 has not received the message  because of some hardware failure   to detect failures  we can use a similar to the one described in section 16.7.1 when p ; sends out a message  it also specifies a time interval during which it is willing to wait for an acknowledgment message from pi when pi receives the message  it immediately sends an acknowledgment to p ;  if p ; receives the acknowledgment message within the specified time interval  it can safely conclude that p1 has received its message  if  however  a time-out occurs  then p ; needs to retransmit its message and wait for an acknowledgment this procedure continues until p ; either gets the acknowledgment message back or is notified by the system that site 52 is down  in the first case  it will compute foo ; in the latter case  it will compute boo note that  if these are the only two viable alternatives  p ; must wait until it has been notified that one of the situations has occurred  suppose now that p7 also needs to know that p ; has received its acknowledgment message  so that it can decide how to proceed with its computation  for example  p1 may want to compute foo only if it is assured that p ; got its acknowledgment in other words  p ; and pi will compute foo if and only if both have agreed on it it turns out that  in the presence of failure  it is not possible to accomplish this task more precisely  it is not possible in a distributed environment for processes p ; and p1 to agree completely on their respective states  to prove this claim  let us suppose that a minimal sequence of message transfers exists such that  after the messages have been delivered  both processes agree to compute foo let m ' be the last message sent by p ; to pj since g does not know whether its message will arrive at pj  since the message may be lost due to a failure   p ; will execute foo regardless of the outcome of the message delivery thus  m ' could be removed from the sequence without affecting the decision procedure hence  the original sequence was not minimal  contradicting our assumption and showing that there is no sequence  the processes can never be sure that both will compute foo  18.7.2 faulty processes now let us assume that the communication medium is reliable but that processes can fail in unpredictable ways consider a system of n processes  of which no more than m are faulty suppose that each process p ; has some private value of v ;  we wish to devise an algorithm that allows each nonfaulty process p ; to construct a vector x ; =  a ; ,l  a ; ,2    a ;  11  such that the following conditions exist  1 if pi is a nonfaulty process  then a ; ,i = vi if p ; and p1 are both nonfaulty processes  then x ; = x1  752 chapter 18 18.8 there are many solutions to this problem  and they share the following properties  a correct algorithm can be devised only if n 3 x m + 1  the worst-case delay for reaching agreement is proportionate tom + 1 message-passing delays  the number of messages required for reaching agreement is large no single process is trustworthy  so all processes must collect all information and make their own decisions  rather than presenting a general solution  which would be complicated  we present an algorithm for the simple case where m = 1 and n = 4 the algorithm requires two rounds of information exchange  each process sends its private value to the other three processes  each process sends the information it has obtained in the first round to all other processes  a faulty process obviously may refuse to send messages in this case  a nonfaulty process can choose an arbitrary value and pretend that the value was sent by the faulty process  once these two rounds are completed  a nonfaulty process pi can construct its vector x ; =  ai,l  a,2  ai,3  a4  as follows  ai = v ;  for j -1 i  if at least two of the three values reported for process pj  in the two rounds of exchange  agree  then the majority value is used to set the value of ai,f ' otherwise  a default value-say  nil-is used to set the value of ai,f ' in a distributed system with no common memory and no common clock  it is sometimes impossible to determine the exact order in which two events occur the happened-before relation is only a partial ordering of the events in a distributed system timestamps can be used to provide a consistent event ordering  mutual exclusion in a distributed environment can be implemented in a variety of ways in a centralized approach  one of the processes in the system is chosen to coordinate the entry to the critical section in the fully distributed approach  the decision making is distributed across the entire system a distributed algorithm  which is applicable to ring-structured networks  is the token-passing approach  for atomicity to be ensured  all the sites in which a transaction t has executed must agree on the final outcome of the execution t either commits at all sites or aborts at all sites to ensure this property  the transaction coordinator 753 of t must execute a commit protocol the most widely used commit protocol is the 2pc protocol  the various concurrency-control schemes that can be used in a centralized system can be modified for use in a distributed environment in the case of locking protocols  we need only change the way the lock manager is implemented in the case of timestamping and validation schemes  the only change needed is the development of a mechanism for generating unique global timestamps the mechanism can either concatenate a local timestamp with the site identification or advance local clocks whenever a message arrives that has a larger timestamp  the primary method for dealing with deadlocks in a distributed environment is deadlock detection the main problem is deciding how to maintain the wait-for graph methods for organizing the wait-for graph include a centralized approach and a fully distributed approach  some distributed algorithms require the use of a coordinator if the coordinator fails because of the failure of the site at which it resides  the system can continue execution only by restarting a new copy of the coordinator on some other site it can do so by maintaining a backup coordinator that is ready to assume responsibility if the coordinator fails another approach is to choose the new coordinator after the coordinator has failed the algorithms that determine where a new copy of the coordinator should be restarted are called election algorithms two algorithms  the bully algorithm and the ring algorithm  can be used to elect a new coordinator in case of failures  18.1 why is deadlock detection much more expensive in a distributed environment than in a centralized environment 18.2 consider a hierarchical deadlock-detection algorithm in which the global wait-for graph is distributed over a number of different controllers  which are organized in a tree each non-leaf controller maintains a wait-for graph that contains relevant information from the graphs of the controllers in the subtree below it in particular  let sa  sp  and sc be controllers such that sc is the lowest common ancestor of sa and sp  sc must be unique  since we are dealing with a tree   suppose that node ti appears in the local wait-for graph of controllers sa and sp  then ~ must also appear in the local wait-for graph of controller sc every controller in the path from sc to sa every controller in the path from sc to sp in addition  if ~ and tj appear in the wait-for graph of controller so and there exists a path from ti to ti in the wait-for graph of one of the children of s0  then an edge ~  + tj must be in the wait-for graph of so  show that  if a cycle exists in any of the wait-for graphs  then the system is deadlocked  754 chapter 18 18.3 your company is building a comp1.1ter network  and you are asked to develop a scheme for dealing with the deadlock problem  a would you use a deadlock-detection scheme or a deadlockprevention scheme b if you used a deadlock-prevention scheme  which one would you use explain your choice  c if you used a deadlock-detection scheme  which one would you use explain your choice  18.4 derive an election algorithm for bidirectional rings that is more efficient than the one presented in this chapter how many messages are needed for n processes 18.5 your company is building a computer network  and you are asked to write an algorithm for achieving distributed mutual exclusion which scheme will you use explain your choice  18.6 consider the following failure model for faulty processors processors follow the prescribed protocol but may fail at unexpected points in time when processors fail  they simply stop functioning and do not continue to participate in the distributed system given such a failure model  design an algorithm for reaching agreement among a set of processors discuss the conditions under which agreement could be reached  18.7 under what circumstances does the wait-die scheme perform better than the wound-wait scheme for granting resources to concurrently executing transactions 18.8 consider a failure that occurs during 2pc for a transaction for each possible failure  explain how 2pc ensures transaction atomicity despite the failure  18.9 the logical clock timestamp scheme presented in this chapter provides the following guarantee  if event a happens before event b  then the timestamp of a is smaller than the timestamp of b note  however  that we cam1.0t order two events based only on their timestamps the fact that an event c has a timestamp that is smaller than the timestamp of event d does not necessarily mean that event c happened before event d ; c and d could be concurrent events in the system discuss ways in which the logical clock timestamp scheme could be extended to distinguish concurrent events from events that can be ordered by the happened-before relation  the distributed algorithm for extending the happened-before relation to a consistent total ordering of all the events in the system was developed by lamport  1978b   further discussions of using logical time to characterize the behavior of distributed systems can be found in fidge  1991   raynal and 755 singhal  1996   babaoglu and marzullo  1993   schwarz and mattern  1994   and mattern  1988   the first general algorithm for implementing mutual exclusion in a distributed environment was also developed by lamport  1978b   lamport 's scheme requires 3 x  n  1  messages per critical-section entry subsequently  ricart and agrawala  1981  proposed a distributed algorithm that requires only 2 x  n 1  messages their algorithm is presented in section 18.2.2 a squareroot algorithm for distributed mutual exclusion is described by maekawa  1985   the token-passing algorithm for rilcg-structured systems presented ilc section 18.2.3 was developed by lann  1977   carvalho and roucairol  1983  discusses mutual exclusion in computer networks  and agrawal and abbadi  1991  describes an efficient and fault-tolerant solution of distributed mutual exclusion a simple taxonomy for distributed mutual-exclusion algorithms is presented by raynal  1991   the issue of distributed synchronization is discussed by reed and kanodia  1979   shared-memory environment   lamport  1978b   lamport  1978a   and sclmeider  1982   totally disjoint processes   a distributed solution to the dilling-philosophers problem is presented by chang  1980   the 2pc protocol was developed by lampson and sturgis  1976  and gray  1978   two modified versions of 2pc  called presume commit and presume abort  reduce the overhead of 2pc by defilling default assumptions regarding the fate of transactions  mohan and lindsay  1983    papers dealing with the problems of implementillg the transaction concept in a distributed database were presented by gray  1981   traiger et al  1982   and spector and schwarz  1983   bernsteill et al  1987  offer comprehensive discussions of distributed concurrency control rosenkrantz et al  1978  report on the timestamp distributed deadlock-prevention algorithm the fully distributed deadlock-detection scheme presented ill section 18.5.2 was developed by obermarck  1982   the hierarchical deadlock-detection scheme of exercise 18.1 appears in menasce and muntz  1979   knapp  1987  and singhal  1989  offer surveys of deadlock detection in distributed systems  deadlocks can also be detected by takilcg global snapshots of a distributed system  as discussed ill chandy and lamport  1985   the byzantine generals problem is discussed by lamport et al  1982  and pease et al  1980   the bully algorithm is presented by garcia-molina  1982   and the election algorithm for a ring-structured system was written by larue  1977   part eight our coverage of operating-system issues thus far has focused mainly on general-purpose computing systems there are  however  specialpurpose systems with requirements different from those of many of the systems we have described  a real-time system is a computer system that requires not only that computed results be correct but also that the results be produced within a specified deadline period results produced after the deadline has passed-even if correct-may be of no real value for such systems  many traditional operating-system scheduling algorithms must be modified to meet the stringent timing deadlines  a multimedia system must be able to handle not only conventional data  such as text files  programs  and word-processing documents  but also multimedia data multimedia data consist of continuous-media data  audio and video  as well as conventional data continuous-media data-such as frames of video-must be delivered according to certain time restrictions  for example  30 frames per second   the demands of handling continuous-media data require significant changes in operatingsystem structure  most notably in memory  disk  and network management  19.1 r our coverage of operating-system issues thus far has focused mainly on general-purpose computing systems  for example  desktop and server systems   we now turn our attention to real-time computing systems the requirements of real-time systems differ from those of many of the systems we have described  largely because real-time systems must produce results within certain time limits in this chapter  we provide an overview of real-time computer systems and describe how real-time operating systems must be constructed to meet the stringent timing requirements of these systems  to explain the timing requirements of real-time systems  to distinguish between hard and soft real-time systems  to discuss the defining characteristics of real-time systems  to describe scheduling algorithms for hard real-time systems  a real-time system is a computer system that requires not only that the computing results be correct but also that the results be produced within a specified deadline period results produced after the deadline has passedeven if correct-may be of no real value to illustrate  consider an autonomous robot that delivers mail in an office complex if its vision-control system identifies a wall after the robot has walked into it  despite correctly identifying the wall  the system has not met its requirement contrast this timing requirement with the much less strict demands of other systems in an interactive desktop computer system  it is desirable to provide a quick response time to the interactive user  but it is not mandatory to do so some systems -such as a batch-processing system-m.ay have no timing requirements whatsoever  real-time systems executing on traditional computer hardware are used in a wide range of applications in addition  many real-time systems are 759 760 chapter 19 19.2 embedded in specialized devices  such as ordinary home appliances  for example  microwave ovens and dishwashers   consumer digital devices  for exarnple  cameras and mp3 players   and communication devices  for example  cellular telephones and blackberry handheld devices   they are also present in larger entities  such as automobiles and airplanes an embedded system is a computing device that is part of a larger system in which the presence of a computing device is often not obvious to the user  to illustrate  consider an embedded system for controlling a home dishwasher  the embedded system may allow various options for scheduling the operation of the dishwasher-the water temperature  the type of cleaning  light or heavy   even a timer indicating when the dishwasher is to start most likely  the user of the dishwasher is unaware that there is in fact a computer embedded in the appliance as another example  consider an embedded system controlling antilock brakes in an automobile each wheel in the automobile has a sensor detecting how much sliding and traction are occurring  and each sensor continually sends its data to the system controller taking the results from these sensors  the controller tells the braking mechanism in each wheel how much braking pressure to apply again  to the user  in this instance  the driver of the automobile   the presence of an embedded computer system may not be apparent it is important to note  however  that not all embedded systems are real-time for example  an embedded system controlling a home furnace may have no real-time requirements whatsoever  some real-time systems are identified as safety-critical systems in a safety-critical system  incorrect operation-usually due to a missed deadline -results in some sort of catastrophe examples of safety-critical systems include weapons systems  antilock brake systems  flight-management systems  and health-related embedded systems  such as pacemakers in these scenarios  the real-time system must respond to events by the specified deadlines ; otherwise  serious injury-or worse-might occur however  a significant majority of embedded systems do not qualify as safety-critical  including fax machines  microwave ovens  wristwatches  and networking devices such as switches and routers for these devices  missing deadline requirements results in nothing more than perhaps an unhappy user  real-time computing is of two types  hard and soft a hard real-time system has the most stringent requirements  guaranteeing that critical realtime tasks be completed within their deadlines safety-critical systems are typically hard real-time systems a soft real-time system is less restrictive  simply providing that a critical real-time task will receive priority over other tasks and that it will retain that priority until it completes many commercial operating systems-as well as linux-provide soft real-time support  in this section  we explore the characteristics of real-time systems and address issues related to designing both soft and hard real-time operating systencs  the following characteristics are typical of many real-time systems  single purpose small size inexpensively mass-produced specific timing requirements 19.2 we next examine each of these characteristics  761 unlike pcs  which are put to many uses  a real-time system typically serves only a single purpose  such as controlling antilock brakes or delivering music on an mp3 player it is unlikely that a real-time system controlling an airliner 's navigation system will also play dvds ! the design of a real-time operating system reflects its single-purpose nature and is often quite simple  many real-time systems exist in environments where physical space is constrained consider the amount of space available in a wristwatch or a microwave oven-it is considerably less than what is available in a desktop computer as a result of space constraints  most real-time systems lack both the cpu processing power and the amount of memory available in standard desktop pcs whereas most contemporary desktop and server systems use 32 or 64-bit processors  many real-time systems run on 8 or 16-bit processors  similarly  a desktop pc might have several gigabytes of physical memory  whereas a real-time system might have less than a megabyte we refer to the amount of memory required to run the operating system and its applications as the footprint of a system because the amount of memory is limited  most real-time operating systems must have small footprints  next  consider where many real-time systems are implemented  they are often found in home appliances and consumer devices devices such as digital cameras  microwave ovens  and thermostats are mass-produced in very costconscious environments thus  the microprocessors for real-time systems must also be inexpensively mass-produced  one technique for reducing the cost of an embedded controller is to use an alternative technique for organizing the components of the computer system rather than organizing the computer around the structure shown in figure 19.1  where buses provide the mechanism intercom1ectin.g individual components  many embedded system controllers use a strategy known as here  the cpu  memory  including cache   memorymouse keyboard printer figure 19.1 bus-oriented organization  762 chapter 19 19.3 management-unit  mmu   and any attached peripheral ports  such as usb ports  are contained in a single integrated circuit the soc strategy is typically less expensive than the bus-oriented organization of figure 19.1  we turn now to the final characteristic identified above for real-time systems  specific timing requirements it is  in fact  the defining characteristic of such systems accordingly  the primary task of both hard and soft real-time operating systems is to support the timing requirements of real-time tasks  and the remainder of this chapter focuses on this issue real-time operating systems meet tim.ing requirements by using scheduling algorithms that give real-time processes the highest schedulil1.g priorities furthermore  schedulers must ensure that the priority of a real-time task does not degrade over time  another technique for addressing timing requirements is by minimizing the response time to events such as il1.terrupts  in this section  we discuss the features necessary for designing an operating system that supports real-time processes before we begin  though  let 's consider what is typically not needed for a real-time system we begin by examining several features provided in many of the operatil1.g systems discussed so far in this text  including linux  unix  and the various versions of windows these systems typically provide support for the following  a variety of peripheral devices  such as graphical displays  cd drives  and dvd drives protection and security mechanisms multiple users supporting these features often results in a sophisticated -and large-kernel  for example  windows xp has over forty million lines of source code in contrast  a typical real-time operating system usually has a very simple design  often written in thousands rather than millions of lines of source code we would not expect these simple systems to il1.clude the features listed above  but why do n't real-time systems provide these features  which are crucial to standard desktop and server systems there are several reasons  but three are most prominent first  because most real-time systems serve a single purpose  they simply do not require many of the features found il1 a desktop pc  consider a digital wristwatch  it obviously has no need to support a disk drive or dvd  let alone virtual memory furthermore  a typical real-time system does not include the notion of a user the system simply supports a small number of tasks  which often await input from hardware devices  sensors  vision identification  and so forth   second  the features supported by standard desktop operating systems are impossible to provide without fast processors and large amounts of memory both of these are unavailable in real-time systems due to space constraints  as explained earlier in addition  many realtime systems lack sufficient space to support peripheral disk drives or graphical displays  although some systems may support file systems using nonvolatile memory  nvram   third  supporting features common in standard desktop 19.3 p = l relocation register r p physical memory figure 19.2 address translation in real-time systems  763 computing environments would greatly increase the cost of real-time systems  which could make such systems economically impractical  additional considerations arise when we consider virtual memory in a real-time system providing virtual memory features as described in chapter 9 requires that the system include a memory-management unit  mmu  for translating logical to physical addresses however  mmus typically increase the cost and power consumption of the system in addition  the time required to translate logical addresses to physical addresses-especially in the case of a translation look-aside buffer  tlb  miss-may be prohibitive in a hard real-time environment in the following discussion  we examine several approaches for translating addresses in real-time systems  figure 19.2 illustrates three different strategies for managing address translation available to designers of real-time operating systems in this scenario  the cpu generates logical address l  which must be mapped to physical address p the first approach is to bypass logical addresses and have the cpu generate physical addresses directly this teclmique-kn.own as real-addressing mode-does not employ virtual memory techniques and is effectively stating that p equals l one problem with real-addressil1.g mode is the absence of memory protection between processes real-addressing mode may also require that programmers specify the physical location where their programs are loaded into memory however  the benefit of this approach is that the system is quite fast  as no time is spent on address translation  real-addressing mode is quite common in embedded systems with hard real-time constraints in fact  some real-time operating systems rum1.ing on microprocessors containing an mmu actually disable the mmu to gain the performance benefit of referencing physical addresses directly  a second strategy for translating addresses is to use an approach similar to the dynamic relocation register shown in figure 8.4 in this scenario  a relocation register r is set to the memory location where a program is loaded  the physical address p is generated by adding the contents of the relocation register r to l some real-time systems configure the mmu to perform this way  the obvious benefit of this strategy is that the mmu can easily translate logical addresses to physical addresses using p = l + r however  this system still suffers from a lack of memory protection between processes  764 chapter 19 19.4 the last approach is for the real-time system to provide full virtual memory functionality as described in chapter 9 in this instance  address translation takes place via page tables and a translation look-aside buffer  or tlb in addition to allowing a program to be loaded at any memory location  this strategy also provides memory protection between processes for systems without attached disk drives  demand paging and swapping may not be possible however  systems may provide such features using nvram flash memory the lynxos and on core systems are examples of real-time operating systems providing full support for virtual memory  keeping in mind the many possible variations  we now identify the features necessary for implementing a real-time operating system this list is by no means absolute ; some systems provide more features than we list below  while other systems provide fewer  preemptive  priority-based scheduling preemptive kernel minimized latency one notable feature we omit from this list is networking support however  deciding whether to support networking protocols such as tcp /ip is simple  if the real-time system must be connected to a network  the operating system must provide networking capabilities for example  a system that gathers real-time data and transmits it to a server must obviously include networking features alternatively  a self-contained embedded system requiring no interaction with other computer systems has no obvious networking requirencent  in the remainder of this section  we examine the basic requirements listed above and identify how they can be implemented in a real-time operating system  19.4.1 priority-based scheduling the most important feature of a real-time operating system is to respond immediately to a real-time process as soon as that process requires the cpu  as a result  the scheduler for a real-time operating system must support a priority-based algorithm with preemption recall that priority-based scheduling algorithms assign each process a priority based on its importance ; more important tasks are assigned higher priorities than those deemed less important  if the scheduler also supports preemption  a process currently running on the cpu will be preempted if a higher-priority process becomes available to run  preemptive  priority-based scheduling algorithms are discussed in detail in chapter 5  where we also present examples of the soft real-time scheduling features of the solaris  windows xp  and linux operating systems each of these systems assigns real-time processes the highest scheduling priority for 19.4 765 example  windows xp has 32 different priority levels ; the highest levelspriority values 16 to 31-are reserved for real-time processes solaris and linux have similar prioritization schemes  note  however  that providing a preemptive  priority-based scheduler only guarantees soft real-time functionality hard real-time systems must further guarantee that real-time tasks will be serviced in accord with their deadline requirem ~ ents  and making such guarantees may require additional scheduling features in section 19.5  we cover scheduling algorithms appropriate for hard real-time systems  19.4.2 preemptive kernels nonpreemptive kernels disallow preemption of a process running in kernel mode ; a kernel-mode process will run until it exits kernel mode  blocks  or voluntarily yields control of the cpu in contrast  a preemptive kernel allows the preemption of a task running in kernel mode designing preemptive kernels can be quite difficult  and traditional user-oriented applications such as spreadsheets  word processors  and web browsers typically do not require such quick response times as a result  some commercial desktop operating systems-such as windows xp-are nonpreemptive  however  to meet the timing requirements of real-time systems-in particular  hard real-time systems-preemptive kernels are mandatory otherwise  a real-time task might have to wait an arbitrarily long period of time while another task was active in the kernel  there are various strategies for making a kernel preemptible one approach is to insert preemption points in long-duration system calls a preemption point checks to see whether a high-priority process needs to be nm if so  a context switch takes place then  when the high-priority process terminates  the interrupted process continues with the system call preemption points can be placed only at safe locations in the kernel-that is  only where kernel data structures are not being modified a second strategy for making a kernel preemptible is through the use of synchronization mechanisms  discussed in chapter 6 with this method  the kernel can always be preemptible  because any kernel data being updated are protected from modification by the high-priority process  19.4.3 minimizing latency consider the event-driven nature of a real-time system the system is typically waiting for an event in real time to occur events may arise either in software -as when a timer expires-or in hardware-as when a remote-controlled vehicle detects that it is approaching an obstruction when an event occurs  the system must respond to and service it as quickly as possible we refer to event latency as the amount of time that elapses from when an event occurs to when it is serviced  figure 19.3   usually  different events have different latency requirements for example  the latency requirement for an antilock brake system might be three to five milliseconds  meaning that from the time a wheel first detects that it is sliding  the system controlling the antilock brakes has three to five milliseconds to respond to and control the situation any response that takes longer might result in the automobile 's veering out of control in contrast  an embedded 766 chapter 19 event e first occurs event latency real-time system responds to e time figure 19.3 event latency  system controlling radar in an airliner might tolerate a latency period of several seconds  two types of latencies affect the performance of real-time systems  interrupt latency dispatch latency interrupt latency refers to the period of time from the arrival of an interrupt at the cpu to the start of the routine that services the interrupt when an interrupt occursf the operating system must first complete the instruction it is executing and determine the type of interrupt that occurred it must then save the state of the current process before servicing the interrupt using the specific interrupt service routine  isr   the total time required to perform these tasks is the interrupt latency  figure 19.4   obviouslyf it is crucial for real-time task t running interrupt 1 determine or interrupt type o'witocontiex t interrupt latency time i ish i figure 19.4 interrupt latency  19.4 767 operating systems to minimize interrupt latency to ensure that real-time tasks receive immediate attention  one important factor contributing to interrupt latency is the amomrt of time interrupts may be disabled while kernel data structures are being updated real-time operating systems require that interrupts be disabled for very short periods of time howeve1 ~ for hard real-time systems  interrupt latency must not only be minimized  it must in fact be bounded to guarantee the deterministic behavior required of hard real-time kernels  the amount of time required for the schedulil g dispatcher to stop one process and start another is known as dispatch latency providing real-time tasks with immediate access to the cpu mandates that real-time operating systems minimize this latency the most effective technique for keeping dispatch latency low is to provide preemptive kernels  in figure 19.5  we diagram the makeup of dispatch latency the conflict phase of dispatch latency has two components  preemption of any process running in the kernel release by low-priority processes of resources needed by a high-priority process as an example  in solaris  the dispatch latency with preemption disabled is over a hundred milliseconds with preemption enabled  it is reduced to less than a millisecond  one issue that can affect dispatch latency arises when a higher-priority process needs to read or modify kernel data that are currently beil g accessed by a lower-priority process-or a chain of lower-priority processes as kernel event response to event ~ --------response interval + ~ process made interrupt available processing ! +  dispatch latency ----1 ~ _....  time figure 19.5 dispatch latency  real-time process execution 768 chapter 19 19.5 data are typically protected with a lock  the higher-priority process will have to wait or a lower-priority one to finish with the resource the situation becomes more complicated i the lower-priority process is preempted in favor of yet another process with a higher priority as an example  assume we have three processes  l  m  and h  whose priorities follow the order l m h also assume that process h requires resource r  which is currently being accessed by process l ordinarily  process h would wait for l to finish using resource r  howevet ~ now suppose that process m becomes runnable  thereby preempting process l indirectly  a process with a lower priority -process m-has affected how long process h must wait for l to relinquish resource r  this problem  known as priority inversion  can be solved by use of the priority-inheritance protocol according to this protocol  all processes that are accessing resources needed by a higher-priority process inherit the higher priority until they are finished with the resources in question when they are finished  their priorities revert to their original values in the example above  a priority-inheritance protocol allows process l to temporarily inherit the priority of process h  thereby preventing process m from preempting its execution when process l has finished using resource r  it relinquishes its inherited priority from h and assumes its original priority as resource r is now available  process h -not m -will run next  our coverage of scheduling so far has focused primarily on soft real-time systems as mentioned  though  scheduling for such systems provides no guarantee on when a critical process will be scheduled ; it guarantees only that the process will be given preference over noncritical processes hard real-time systems have stricter requirements a task must be serviced by its deadline ; service after the deadline has expired is the same as no service at all  we now consider scheduling for hard real-time systems before we proceed with the details of the individual schedulers  however  we must define certain characteristics of the processes that are to be scheduled first  the processes are considered periodic that is  they require the cpu at constant intervals  periods   each periodic process has a fixed processing timet once it acquires the cpu  a deadline d by which time it must be serviced by the cpu  and a period p the relationship of the processing time  the deadline  and the period can be expressed as 0    ; t    ; d    ; p the rate of a periodic task is 1 i p figure 19.6 p d period1 d period2 d ~ i lc = j figure 19.6 periodic task  time period3 19.5 769 illustrates the execution of a periodic process over time schedulers can take advantage of this relationship and assign priorities according to the deadline or rate requirements of a periodic process  what is unusual about this form of scheduling is that a process may have to announce its deadline requirements to the scheduler then  using a technique known as an admission-control algorithm  the scheduler either admits the process  guaranteeing that the process will complete on time  or rejects the request as impossible if it can not guarantee that the task will be serviced by its deadline  in the following sections  we explore scheduling algorithms that address the deadline requirements of hard real-time systems  19.5.1 rate-monotonic scheduling the rate-monotonic scheduling algorithm schedules periodic tasks using a static priority policy with preemption if a lower-priority process is running and a higher-priority process becomes available to run  it will preempt the lower-priority process upon entering the system  each periodic task is assigned a priority inversely based on its period the shorter the period  the higher the priority ; the longer the period  the lower the priority the rationale behind this policy is to assign a higher priority to tasks that require the cpu more often  furthermore  rate-monotonic scheduling assumes that the processing time of a periodic process is the same for each cpu burst that is  every time a process acquires the cpu  the duration of its cpu burst is the same  let 's consider an example we have two processes p1 and p2 the periods for p1 and p2 are 50 and 100  respectively-that is  pl = 50 and p2 = 100 the processil1.g times are t1 = 20 for p1 and t2 = 35 for p2  the deadline for each process requires that it complete its cpu burst by the start of its next period  we must first ask ourselves whether it is possible to schedule these tasks so that each meets its deadlines if we measure the cpu utilization of a process pi as the ratio of its burst to its period -ti i pi -the cpu utilization of p1 is 20j50 = 0.40 and that of p2 is 35/100 = 0.35  for a total cpu utilization of 75 percent therefore  it seems we can schedule these tasks in such a way that both meet their deadlines and still leave the cpu with available cycles  first  suppose we assign p2 a higher priority than p1 the execution of p1 and p2 is shown in figure 19.7 as we can see  p2 starts execution first and completes at time 35 at this point  p1 starts ; it completes its cpu burst at time 55 however  the first deadline for p1 was at time 50  so the scheduler has caused p1 to miss its deadline  now suppose we use rate-monotonic scheduling  in which we assign p1 a higher priority than p2  since the period of p1 is shorter than that of p2  deadlines 0 1 0 20 30 40 50 60 70 80 90 1 00 11 0 120 figure 19.7 scheduling of tasks when p2 has a higher priority than p1  770 chapter 19 deadlines 0 1 0 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 figure '19.8 rate-monotonic scheduling  the execution of these processes is shown in figure 19.8 p1 starts first and completes its cpu burst at time 20  thereby meeting its first deadline p2 starts running at this point and runs until time 50 at this time  it is preempted by p1  although it still has 5 milliseconds remaining in its cpu burst p1 completes its cpu burst at time 70  at which point the scheduler resumes p2 p2 completes its cpu burst at time 75  also meeting its first deadline the system is idle until time 100  when p1 is scheduled again  rate-monotonic scheduling is considered optimal in that if a set of processes can not be scheduled by this algorithm  it can not be scheduled by any other algorithm that assigns static priorities let 's next examine a set of processes that can not be scheduled using the rate-monotonic algorithm  assume that process p1 has a period of p1 = 50 and a cpu burst of t1 = 25  for p2  the corresponding values are p2 = 80 and t2 = 35 rate-monotonic scheduling would assign process p1 a higher priority  as it has the shorter period the total cpu utilization of the two processes is  25 /50  +  35 j80  = 0.94  and it therefore seems logical that the two processes could be scheduled and still leave the cpu with 6 percent available time figure 19.9 shows the scheduling of processes p1 and p2  initially  p1 runs until it completes its cpu burst at time 25 process p2 then begins running and n.ms until time 50  when it is preempted by p1 at this point  p2 still has 10 milliseconds remaining in its cpu burst process p1 n.ms until time 75 ; consequently  p2 misses the deadline for completion of its cpu burst at time 80  despite being optimal  then  rate-monotonic scheduling has a limitation  cpu utilization is bounded  and it is not always possible to fully maximize cpu resources the worst-case cpu utilization for scheduling n processes is 2  21111  1   with one process in the system  cpu utilization is 100 percent  but it falls to approximately 69 percent as the number of processes approaches infinity with two processes  cpu utilization is bounded at about 83 percent combined cpu utilization for the two processes scheduled in figures 19.7 and 19.8 is 75 percent ; therefore  the rate-monotonic scheduling algorithm is guaranteed to schedule deadlines 0 1 0 20 30 40 50 60 70 80 90 1 00 11 0 120 130 140 150 1 60 figure 19.9 missing deadlines with rate-monotonic scheduling  19.5 771 them so that they can meet their deadlines for the two processes scheduled in figure 19.9  combined cpu utilization is approximately 94 percent ; therefore  rate-monotonic scheduling can not guarantee that they can be scheduled so that they meet their deadlines  19.5.2 earliest-deadline-first scheduling earliest-deadline-first  edf  scheduling dynamically assigns priorities according to deadline the earlier the deadline  the higher the priority ; the later the deadline  the lower the priority under the edf policy  when a process becomes runnable  it must announce its deadline requirements to the system priorities may have to be adjusted to reflect the deadline of the newly rmmable process  note how this differs from rate-monotonic scheduling  where priorities are fixed  to illustrate edf scheduling  we again schedule the processes shown in figure 19.9  which failed to meet deadline requirements under rate-monotonic scheduling recall that p1 has values of p1 = 50 and t1 = 25 and that p2 has values of p2 = 80 and t2 = 35 the edf scheduling of these processes is shown in figure 19.10 process p1 has the earliest deadline  so its initial priority is higher than that of process p2 process p2 begins rmming at the end of the cpu burst for p1 however  whereas rate-monotonic scheduling allows p1 to preempt p2 at the beginning of its next period at time 50  edf scheduling allows process p2 to continue running p2 now has a higher priority than p1 because its next deadline  at time 80  is earlier than that of p1  at time 100   thus  both p1 and p2 meet their first deadlines process p1 again begins running at time 60 and completes its second cpu burst at time 85  also meeting its second deadline at time 100 p2 begins rum1ing at this point only to be preempted by p1 at the start of its next period at time 100 p2 is preempted because p1 has an earlier deadline  time 150  than p2  time 160   at time 125  p1 completes its cpu burst and p2 resumes execution  finishing at time 145 and meeting its deadline as well the system is idle until time 150  when p1 is scheduled to run once again  unlike the rate-monotonic algorithm  edf scheduling does not require that processes be periodic  nor must a process require a constant amount of cpu time per burst the only requirement is that a process a1mom1ce its deadline to the scheduler when it becomes runnable the appeal of edf scheduling is that it is theoretically optimal-theoretically  it can schedule processes so that each process can meet its deadline requirements and cpu utilization will be 100 percent in practice  however  it is impossible to achieve this level of cpu utilization due to the cost of context switching between processes and interrupt handling  deadlines 0 1 0 20 30 40 50 60 70 80 90 1 00 11 0 120 130 140 150 160 figure 19.10 earliest-deadline-first scheduling  772 chapter 19 19.5.3 proportional share scheduling proportional share schedulers operate by allocating t shares among all applications an application can receive n shares of timef thus ensuring that the application will have n 1 t of the total processor time as an examplef assume that a total oft = 100 shares is to be divided among three processesf af b f and c a is assigned 50 share sf b is assigned 15 sharesf and c is assigned 20 shares this scheme ensures that a will have 50 percent o total processor time  b will have 15 percent  and c will have 20 percent  proportional share schedulers must work in conjunction with an admission control policy to guarantee that an application receives its allocated shares of time an admission control policy will only admit a client requesting a particular number of shares if sufficient shares are available in our current example  we have allocated 50 + 15 + 20 = 85 shares of the total of 100 shares  if a new process d requested 30 shares  the admission controller would deny d entry into the system  19.5.4 pthread scheduling the posix standard also provides extensions for real-time computingposix  1b in this section  we cover some of the posix pthread api related to scheduling real-time threads pthreads defines two scheduling classes for real-time threads  sched __fifo sched_rr sched__fifo schedules threads according to a first-come  first-served policy using a fifo queue as outlined in section 5.3.1 however  there is no time slicing among threads of equal priority therefore  the highest-priority real-time thread at the front of the fifo queue will be granted the cpu until it terminates or blocks sched_rr  for round-robin  is sincilar to sched_fifo except that it provides time slicing among threads of equal priority pthreads provides an additional scheduling class-scheddther-but its implementation is undefined and system specific ; it may behave differently on different systems  the pthread api specifies the following two functions for getting and setting the scheduling policy  pthread_attr_getsched_policy  pthread_attr_t attr  int policy  pthread_attr_setsched_policy  pthread_attr_t attr  int policy  the first parameter to both functions is a pointer to the set of attributes for the thread the second parameter is either  1  a pointer to an integer that is set to the current scheduling policy  for pthread_attr_getsched_policy    or  2  an integer value  sched_fifo  sched_rr  or sched_other  for the pthread_attr setsched_policy   function both functions return non-zero values if an error occurs  19.5 # include pthread.h # include stdio.h # define num_threads 5 int main  int argc  char argv     int i  policy ; pthread_t tid  num_threads  ; pthread_attr_t attr ; i get the default attributes i pthread_attr_init  &attr  ; i get the current scheduling policy i if  pthread_attr_getschedpolicy  &attr  &policy  ! = 0  fprintf  stderr  unable to get policy \ n  ; else   if  policy = = sched_other  printf  sched_other \ n  ; else if  policy = = sched_rr  printf  sched_rr \ n  ; else if  policy = = sched_fifo  printf  sched_fifo \ n  ; i set the scheduling policy  fifo  rr  or other i 773 if  pthread_attr_setschedpolicy  &attr  sched_other  ! = 0  fprintf  stderr  unable to set policy \ n  ;  i create the threads i for  i = 0 ; i num_threads ; i + +  pthread_create  &tid  i  ,&attr,runner,null  ; i now join on each thread i for  i = 0 ; i num_threads ; i + +  pthread_join  tid  i   null  ; i each thread will begin control in this function i void runner  void param   i do some work  i pthread_exi t  0  ;  figure 19.11 pthread scheduling api  774 chapter 19 19.6 in figure 19.11  we illustrate a pthread program using this api this program first determines the current scheduling policy and then sets the scheduling algorithm to scheddther  in this section  we describe vxworks  a popular real-time operating system providing hard real-time support vxworks  commercially developed by wind river systems  is widely used in automobiles  consumer and industrial devices  and networking equipment such as switches and routers vxworks is also used to control the two rovers-spirit and opportunity-that began exploring the planet mars in 2004  the organizationofvxworks is shown in figure 19.12 vxworks is centered on the wind microkernel recall from our discussion in section 2.7.3 that microkernels are designed so that the operating-system kernel provides a bare minimum of features ; additional utilities  such as networking  file systems  and graphics  are provided in libraries outside of the kernel this approach offers many benefits  including minimizing the size of the kernel-a desirable feature for an embedded system requiring a small footprint  the wind microkernel supports the following basic features  processes the wind microkernel provides support for individual processes and threads  using the pthread api   however  similar to linux  vxworks does not distinguish between processes and threads  instead referring to both as tasks  embedded real-time application figure 19.12 the organization of vxworks  19.6 775 thelinux  operating .systent isbe ~ ng us ~ cl incre ~ singlyinreai-time enviroft ~ ments.yve  hav ~ alreadycovered its softr ~ fil-ti,nteschedv,ling.fe ~ tur ~ s  secti   n 5.6.3  ,whereby real-time tasl sareassignt ; d.thehj.ghe ~ tpriorityi ~ the s  cst ~ m   additionaueatures in the 2,6 release ofthe kernel makelinux even,more suitilb le fot .embedded systems thes ~  features i.nclud ~ ia fullr pree ptive kernel an ~ la more .efficient sched'-lli11g  ~ l ~ orith ! fl   'vhic  t p.ll1sij     1  tinte regardless ofthe number  of tas ~ s  ac.tive in the s ~ .stero  the2.6release also   k ;  r  ei ~    o ~ j ~ ; t ~ ~ ; ; ~ ~  ~ enthardlare arc ~ itecttlresbydi \ tlcljng another strategy jew  integrah1lg linux iftto reanill1e .envirof \ me ~ ts involves combini  \ g th ~  linux operat ~ s,.systt ; it1 vith a mallreal-.ti ! fle ~ ernet tht ; reby  providing  a systell1 th ~ t ~ cts as botl      l gene ~ al 7p ~ rp   s.e   tnd  ~ real-tipte ~ ystem this .is theappro ~ c  h tctken1  jy.t  te ~ ~ lil_1cux ppt ; rat ~ g systerrl in .rtlinux  the standard lirn.tx kernel run ~ as a tas  in ~ sll1ctll re ~ l-timeoperating  syste1n   the real ~ htt1eikerl'lelj  an ~ les alliftterrupts.-,-directing each interrupt to ct handler in the st ~ ndarclkerrtel.or to anintef ~ rupt.randler in the real7tirl'le ketnel fi1jrt,h_ermore/ ~ tlin ~ x  prevents the sh'tndar \ i.linuxkernel  from ever disablb ; j  g intefrl_lpts  t  tus.ens11rirtg th ~ t itc tl1rlotac1c11atencyto.thereal-timesystem.rj  lirtux.ctls0 pr   yi ies difft ; rent schedulingpolicies  includingrate-'mo1lot  i1  i  'schec1ul ~ j1g  sechonj9.5.1  c ; tnd earliest ~ deadline-first scheduling  section19 ; 5.2  scheduling the wind microkernel provides two separate scheduling models  preemptive scheduling and nonpreemptive round-robin scheduling with 256 different priority levels the scheduler also supports the posix api for real-time threads  covered in section 19.5.4  interrupts the wind microkernel also manages interrupts to support hard real-time requirements  interrupt and dispatch latency times are bounded  interprocess communication the wind micro kernel provides both shared memory and message passing as mechanisms for communication between separate tasks it also allows tasks to communicate using a technique known as pipes-a mechanism that behaves in the same way as a fifo queue but allows tasks to communicate by writing to a special fik the pipe  to protect data shared by separate tasks  vxworks provides semaphores and mutex locks with a priority inheritance protocol to prevent priority mversion  outside the microkernet vxworks includes several component libraries that provide support for posix  java  tcp /ip networking  and the like all components are optionat allowing the designer of an embedded system to customize the system according to its specific needs for example  if networking is not required  the tcp /ip library can be excluded from the image of the operating system this strategy allows the operating-system designer to 776 chapter 19 19.7 include only required features  thereby minimizing the size-or footprint-of the operating system  vxworks takes an interesting approach to memory management  supporting two levels of virtual memory the first level  which is quite simple  allows for control of the cache on a per-page basis this policy enables an application to specify certain pages as non-cacheable when data are being shared by separate tasks running on a multiprocessor architecture  it is possible that shared data can reside in separate caches local to individual processors unless an architecture supports a cache-coherency policy to ensure that the same data residing in two caches will not be different  such shared data should not be cached and should instead reside only in main memory so that all tasks maintain a consistent view of the data  the second level of virtual memory requires the optional virtual memory component vxvmi  figure 19.12   along with processor support for a memorymanagement unit  mmu   when this optional component is loaded on a system with an mmu  vxworks allows a task to mark certain data areas as private a data area marked as private may only be accessed by the task it belongs to  furthermore  vxworks allows pages containing kernel code along with the interrupt vector to be declared as read-only this is useful  as vxworks does not distinguish between user and kernel modes ; all applications run in kernel mode  giving an application access to the entire address space of the system  a real-time system is a computer system requiring that results arrive within a deadline period ; results arriving after the deadline has passed are useless  many real-time systems are embedded in consumer and industrial devices  there are two types of real-time systems  soft and hard real-time systems  soft real-time systems are the least restrictive  assigning real-time tasks higher scheduling priority than other tasks hard real-time systems must guarantee that real-time tasks are serviced within their deadline periods in addition to strict timing requirements  real-time systems can further be characterized as having only a single purpose and running on small  inexpensive devices  to meet timing requirements  real-time operating systems must employ various techniques the scheduler for a real-time operating system must support a priority-based algorithm with preemption furthermore  the operating system must allow tasks running in the kernel to be preempted in favor of higher-priority real-time tasks real-time operating systems also address specific timing issues by minimizing both interrupt and dispatch latency  real-time scheduling algorithms include rate-monotonic and earliestdeadline first scheduling rate-monotonic scheduling assignb tasks that require the cpu more often a higher priority than tasks that require the cpu less often earliest-deadline-first scheduling assigns priority according to upcoming deadlines-the earlier the deadline  the higher the priority  proportional share scheduling uses the technique of dividing up processor time into shares and assigning each process a number of shares  thus guaranteeing each process its proportional share of cpu time the pthread api provides various features for scheduling real-time threads as well  777 19.1 explain why interrupt and dispatch latency times must be bounded in a hard real-time system  19.2 identify whether hard or soft real-time scheduling is more appropriate in the following environments  a thermostat in a household b control system for a nuclear power plant c fuel economy system in an automobile d landing system in a jet airliner 19.3 consider two processes  p1 and p2  where p1 = 50  t1 = 25  p2 = 75  and t2 = 30  a can these two processes be scheduled using rate-monotonic scheduling illustrate your answer using a gantt chart such as the ones in figures 19.7-19.10  b illustrate the scheduling of these two processes using earliestdeadline first  edf  scheduling  19.4 discuss ways in which the priority inversion problem could be addressed in a real-time system also discuss whether the solutions could be implemented within the context of a proportional share scheduler  19.5 under what circumstances is rate-monotonic scheduling inferior to earliest-deadline-first scheduling in meeting the deadlines associated with processes the scheduling algorithms for hard real-time systems  such as rate monotonic scheduling and earliest-deadline-first scheduling  are presented in liu and layland  1973   other scheduling algorithms and extensions to previous algorithms are presented in jensen et al  1985   lehoczky et al  1989   audsley et al  1991   mok  1983   and stoica et al  1996   mok  1983  describes a dynamic priority-assignment algorithm called least-laxity-first scheduling stoica et al   1996  analyze the proportional share algorithm useful information regarding various popular operating systems used in embedded systems can be obtained from http  / /rtlinux.org  http  / /windriver.com  and http  / /qnx.com future directions and important research issues in the field of embedded systems are discussed in a research article by stankovic  1996   in earlier chapters  we generally concerned ourselves with how operating systems handle conventional data  such as text files  programs  binaries  wordprocessing documents  and spreadsheets however  operating systems may have to handle other kinds of data as well a relatively recent trend in technology is the incorporation of multimedia data into computer systems  multimedia data consist of continuous-media  audio and video  data as well as conventional files continuous-media data differ from conventional data in that continuous-media data-such as frames of video-must be delivered  streamed  according to certain time restrictions  for example  30 frames per second   in this chapter  we explore the demands of continuous-media data  we also discuss in more detail how such data differ from conventional data and how these differences affect the design of operating systems that support the requirements of multimedia systems  chapter objectives to identify the characteristics of multimedia data  to examine several algorithms used to compress multimedia data  to explore the operating  system requirements of multimedia data  including cpu and disk scheduling and network management  20.1 what is multimedia the term multimedia describes a wide range of applications that are in popular use today these include audio and video files such as mp3 audio files  dvd movies  and short video clips of movie previews or news stories downloaded over the internet multimedia applications also include live web casts  broadcast over the world wide web  of speeches or sporting events and even live webcams that allow a viewer in manhattan to observe customers at a cafe in paris multimedia applications need not be either audio or video ; rather  a multimedia application often includes a combination of both for example  a movie may consist of separate audio and video tracks  779 780 chapter 20 nor must multimedia applications be delivered only to desktop personal computers increasingly  they are being directed toward smaller devices  including personal digital assistants  pdas  and cellular telephones for example  a stock trader may have stock quotes delivered in real time to her pda  in this section  we explore several characteristics of multimedia systems and examine how multimedia files can be delivered from a server to a client system we also look at common standards for representing multimedia video and audio files  20.1.1 media delivery multimedia data are stored in the file system just like any other data the major difference between a regular file and a multimedia file is that the multimedia file must be accessed at a specific rate  whereas accessing the regular file requires no special timing let 's use video as an example of what we mean by rate  video is represented by a series of images  formally known as that are displayed in rapid succession the faster the frames are displayed  the smoother the video appears in general  a rate of 24 to 30 frames per second is necessary for video to appear smooth to human eyes  the eye retains the image of each frame for a short time after it has been presented  a characteristic known as a rate of 24 to 30 frames per second is fast enough to appear continuous  a rate lower than 24 frames per second will result in a choppy-looking presentation the video file must be accessed from the file system at a rate consistent with the rate at which the video is being we refer to data with associated rate requirements as multimedia data may be delivered to a client either from the local file system or from a remote server when the data are delivered from the local file system  we refer to the delivery as examples include watching a dvd on a laptop computer or listening to an mp3 audio file on a handheld mp3 player in these cases  the data comprise a regular file that is stored on the local file system and played back  that is  viewed or listened to  from that system  multimedia files may also be stored on a remote server and delivered to a client across a network using a technique known as a client may be a personal computer or a smaller device such as a handheld computer  pda  or cellular telephone data from live continuous media -such as live webcams -are also streamed from a server to clients  there are two types of streaming techniques  progressive download and real-time streaming with a download  a media file containing audio or video is downloaded and stored on the client 's local file system as the file is being downloaded  the client is able to play back the media file without having to wait for the file to be downloaded in its entirety because the media file is ultimately stored on the client system  progressive download is most useful for relatively small media files  such as short video clips  differs from progressive download in that the media file is streamed to the client but is only played -and not stored -by the client  because the media file is not stored on the client system  real-time streaming is preferable to progressive download for media files that might be too large 20.1 781 for storage on the system  such as long videos and internet radio and tv broadcasts  both progressive download and real-time streaming may allow a client to move to different points in the stream  just as you can use the fast-forward and rewind operations on a dvd controller to move to different points in the dvd disc for example  we could move to the end of a 5-minute streaming video or replay a certain section of a movie clip the ability to move around within the media stream is known as two types of real-time streaming are available  live streaming and ondemand streaming is used to deliver an event  such as a concert or a lecture  live as it is actually occurring a radio program broadcast over the internet is an example of a live real-time stream in fact  one of the authors of this text regularly listens to a favorite radio station from vermont while at his home in utah as it is streamed live over the internet live real-time streaming is also used for applications such as live webcams and video conferencing due to its live delivery  this type of real-time streaming does not allow clients random access to different poil1.ts in the media stream in addition  live delivery means that a client who wishes to view  or listen to  a particular live stream already in progress will join the session late ; ' thereby missing earlier portions of the stream the same thing happens with a live tv or radio broadcast if you start watching the 7  00p.m news at 7  10p.m  you will have missed the first 10 minutes of the broadcast  on-demand streaming is used to deliver media streams such as full-length movies and archived lectures the difference between live and on-demand streaming is that on-demand streaming does not take place as the event is occurring thus  for example  whereas watching a live stream is like watching a news broadcast on tv  watching an on-demand stream is like viewing a movie on a dvd player at some convenient time-there is no notion of arriving late  depending on the type of on-demand streaming  a client may or may not have random access to the stream  examples of well-known streaming media products include realplayer  apple quicktime  and windows media player these products include both servers that stream the media and client media players that are used for playback  20.1.2 characteristics of multimedia systems the demands of multimedia systems are unlike the demands of traditional applications in general  multimedia systems may have the following characteristics  multimedia files can be quite large for example  a 100-minute mpeg-1 video file requires approximately 1.125gb of storage space ; 100 minutes of high-defuution television  hdtv  requires approximately 15 gb of storage a server storing hundreds or thousands of digital video files may thus require several terabytes of storage  continuous media may require very high data rates consider digital video  in which a frame of color video is displayed at a resolution of 800 x 600 if we use 24 bits to represent the color of each pixel  which allows us to have 224  or roughly 16 million  different colors   a single 782 chapter 20 20.2 frame requires 800 x 600 x 24 = 11,520  000 bits of data if the frames are displayed at a rate of 30 frames per second  a bandwidth in excess of 345 mbps is required  multimedia applications are sensitive to timing delays during playback  once a continuous-media file is delivered to a client  delivery must continue at a certain rate during playback of the media ; otherwise  the listener or viewer will be subjected to pauses during the presentation  20.1.3 operating-system issues for a computer system to deliver continuous-media data  it must guarantee the specific rate and timing requirements-also known as of or qos  requirements-of continuous media  providing these qos guarantees affects several components in a computer system and influences such operating-system issues as cpu scheduling  disk scheduling  and network management specific examples include the following  compression and decoding may require significant cpu processing  multimedia tasks must be scheduled with certain priorities to ensure meeting the deadline requirements of continuous media  similarly  file systems must be efficient to meet the rate requirements of continuous media  network protocols must support bandwidth requirements while minimizing delay and jitter  which we discuss further later in the chapter   in later sections  we explore these and several other issues related to qos  first  however  we provide an overview of various techniques for compressing multimedia data as suggested above  compression makes significant demands on the cpu  because of the size and rate requirements of multimedia systems  multimedia files are often compressed from their original form to a much smaller form  once a file has been compressed  it takes up less space for storage and can be delivered to a client more quickly compression is particularly important when the content is beilcg streamed across a network cormection in discussing file compression  we often refer to the which is the ratio of the original file size to the size of the compressed file for example  an 800-kb file that is compressed to 100 kb has a compression ratio of 8  1  once a file has been compressed  encoded   it must be decompressed before it can be accessed a feature of the algorithm used to compress the file affects the later decompression compression algorithms are classified as either or with lossy compression  some of the original data are lost when the file is decoded  whereas lossless compression ensures that 20.2 783 the compressed file can always be restored to its original form in generat lossy techniques provide much higher cone pression ratios obviously  though  only certain types of data can tolerate lossy compression-namely  images  audio  and video lossy compression algorithms often work by eliminating certain data  such as very high or low frequencies that a human ear can not detect some lossy compression algorithms used on video operate by storing only the differences between successive frames lossless algorithms are used for compressing text files  such as computer programs  for example  files   because we want to restore these compressed files to their state  a number of different lossy compression schemes for continuous-media data are commercially available in this section  we cover one used by the moving picture experts group  better known as mpeg  mpeg refers to a set of file formats and compression standards for digital video because digital video often contains an audio portion as well  each of the standards is divided into three layers layers 3 and 2 apply to the audio and video portions of the media file layer 1  known as the layer  contains timing information to allow the mpeg player to multiplex the audio and video portions so that they are synchronized during playback there are three major mpeg standards  mpeg-1  mpeg-2  and mpeg-4  mpeg-1 is used for digital video and its associated audio stream the resolution of mpeg-1 is 352 x 240 at 30 frames per second with a bitrate of up to 1.5 mbps this provides a quality slightly lower than that of conventional vcr videos mp3 audio files  a popular medium for storing music  use the audio layer  layer 3  of mpeg-1 for video  mpeg-1 can achieve a compression ratio of up to 200  1  although in practice compression ratios are much lower because mpeg-1 does not require high data rates  it is often used to download short video clips over the internet  mpeg-2 provides better quality than mpeg-1 and is used for compressing dvd movies and digital television  including high-definition television  or hdtv   mpeg-2 identifies a number of levels and profiles of video compression  the refers to the resolution of the video ; the characterizes the video 's quality in general  the higher the level of resolution and the better the quality of the video  the higher the required data rate typical bit rates for mpeg-2 encoded files are 1.5 mbps to 15 mbps because mpeg-2 requires higher rates  it is often lmsuitable for delivery of video across a network and is generally used for local playback  mpeg-4 is the most recent of the standards and is used to transmit audio  video  and graphics  including two-dimensional and three-dimensional animation layers animation makes it possible for end users to interact with the file during playback for example  a potential home buyer can download an mpeg-4 file and take a virtual tour through a home she is considering purchasing  moving from room to room as she chooses another appealing feature of mpeg-4 is that it provides a scalable level of quality  allowing delivery over relatively slow network connections such as 56-i bps modems or over high-speed local area networks with rates of several megabits per second  furthermore  by providing a scalable level of quality  mpeg-4 audio and video files can be delivered to wireless devices  including handheld computers  pdas  and cell phones  all three mpeg standards discussed here perform lossy compression to achieve high compression ratios the fundamental idea behind mpeg 784 chapter 20 20.3 compression is to store the differences between successive frames we do not cover further details of how mpeg performs compression but rather encourage the interested reader to consult the bibliographical notes at the end of this chapter  as a result of the characteristics described in section 20.1.2  multimedia applications often require levels of service from the operating system that differ from the requirements of traditional applications  such as word processors  compilers  and spreadsheets tin1.ing and rate requirements are perhaps the issues of foremost concern  as the playback of audio and video data demands that the data be delivered within a certain deadline and at a continuous  fixed rate traditional applications typically do not have such time and rate constraints  tasks that request data at constant intervals-or known as ' ' ; ~ ' ' ' for example  an mpeg-1 video might require a rate of 30 frames per second during playback maintaining this rate requires that a frame be delivered approximately every l/301h  or 3.34 hundredths  of a second to put this in the context of deadlines  let 's assume that frame f1 succeeds frame fi in the video playback and that frame fi was displayed at time t0 the deadline for displaying frame f i is 3.34 hundredths of a second after time t0 if the operating system is unable to display the frame by this deadline  the frame will be omitted from the stream  as mentioned earlier  rate requirements and deadlines are known as quality of service  qos  requirements there are three qos levels  the system makes a best-effort attempt to satisfy the requirements ; however  no guarantees are made  this level treats different types of traffic in different ways  giving certain traffic streams higher priority than other streams however  just as with best-effort service  no guarantees are made  the quality-of-service requirements are guaranteed  traditional operating systems-the systems we have discussed in this text so far-typically provide only best-effort service and rely on ' ' ' ' that is  they simply assume that the total amount of resources available will tend to be larger than a worst-case workload would demand if demand exceeds resource capacity  manual intervention must take place  and a process  or several processes  must be removed from the system however next-generation multimedia systems can not make such assumptions these systems must provide continuous-media applications with the guarantees made possible by hard qos therefore  in the remainder of this discussion  when we refer to qos  we mean hard qos next  we explore various techniques that enable multimedia systems to provide such service-level guarantees  there are a number of parameters defining qos for multimedia applications  including the following  20.3 785 throughput is the total amount of work done during a certain interval for multimedia applications  throughput is the required data rate   delay refers to the elapsed time from when a request is first submitted to when the desired result is produced for example  the time from when a client requests a media stream to when the stream is delivered is the delay  jitter is related to delay ; but whereas delay refers to the time a client must wait to receive a stream  jitter refers to delays that occur during playback of the stream certain multimedia applications  such as on-demand real-time streaming  can tolerate this sort of delay jitter is generally considered unacceptable for continuous-media applications  howeve1 ~ because it may mean long pauses-or lost frames-during playback clients can often compensate for jitter by buffering a certain amount of data-say  5 seconds ' worth-before beginning playback  reliability refers to how errors are handled during transmission and processing of continuous media errors may occur due to lost packets in the network or processing delays by the cpu in these-and other-scenarios  errors can not be corrected  since packets typically arrive too late to be useful  the quality of service may be between the client and the server  for example  continuous-media data may be compressed at different levels of quality  the higher the quality  the higher the required data rate a client may negotiate a specific data rate with a server  thus agreeing to a certain level of quality during playback furthermore  many media players allow the client to configure the player according to the speed of the client 's connection to the network this allows a client to receive a streaming service at a data rate specific to a particular connection thus  the client is negotiating quality of service with the content provider  to provide qos guarantees  operating systems often use which is simply the practice of admitting a request for service only if the server has sufficient resources to satisfy the request we see admission control quite often in our everyday lives for example  a movie theater only admits as many customers as it has seats in the theater  there are also many situations in everyday life where admission control is not practiced but would be desirable !  if no admission-control policy is used in a multimedia environment  the demands on the system might become so great that the system becomes unable to meet its qos guarantees  in chapter 6  we discussed using semaphores as a method of implementing a simple admission-control policy in this scenario  there exist a finite number of nonshareable resources when a resource is requested  we grant the request only if sufficient resources are available ; otherwise  the requesting process must wait until a resource becomes available we can use semaphores to implement an admission-control policy by first initializing a semaphore to the number of resources available every request for a resource is made through a wait   operation on the semaphore ; a resource is released with an invocation of signal   on the semaphore once all resources are in use  subsequent calls to wait   block until there is a corresponding signal    786 chapter 20 20.4 figure 20.1 resources on a file server  a common technique for implementing admission control is to use for example  resources on a file server may include the cpu  memory  file system  devices  and network  figure 20.1   note that resources may be either exclusive or shared and that there may be either single or multiple instances of each resource type to use a resource  a client must make a reservation request for the resource il1 advance if the request can not be granted  the reservation is denied an admission-control scheme assigns a to each type of resource requests for resources have associated qos requirements-for example  required data rates when a request for a resource arrives  the resource manager determines if the resource can meet the qos demands of the request if it can not  the request may be rejected  or a lower level of qos may be negotiated between the client and the server if the request is accepted  the resource manager reserves the resources for the requesting client  thus assuring the client that the desired qos requirements will be met in section 20.7.2  we examine the admission-control algorithm used to ensure qos guarantees in the cineblitz multimedia storage server  19  which covers real-time systems  we distinguished between and soft real-time systems simply give scheduling priority to critical processes a soft real-time system ensures that a critical process will be given preference over a noncritical process but provides no guarantee as to when the critical process will be scheduled  a typical requirement of continuous media  however  is that data must be delivered to a client by a certain deadline ; data that do not arrive by the deadline are unusable multimedia systems thus require hard real-time scheduling to ensure that a critical task will be serviced withii1 a guaranteed period of time  20.5 20.5 787 another scheduling issue concerns whether a scheduling algorithm uses or distinction first discussed in chapter 5 the difference between the two is that the priority of a process will remain unchanged if the scheduler assigns it a static priority scheduling algorithms that assign dynamic priorities allow priorities to change over time most operating systems use dynamic priorities when scheduling non-real-time tasks with the intention of giving higher priority to interactive processes however  when scheduling real-time tasks  most systems assign static priorities  as the design of the scheduler is less complex  several of the real-time scheduling strategies discussed in section 19.5 can be used to meet the rate and deadline qos requirements of continuous-media applications  we first discussed disk scheduling in chapter 12 there  we focused primarily on systems that handle conventional data ; for these systems  the scheduling goals are fairness and throughput as a result  most traditional disk schedulers employ some form of the scan  section 12.4.3  or c-scan  section 12.4.4  algorithn l  continuous-media files  however  have two constraints that conventional data files generally do not have  timing deadlines and rate requirements  these two constraints must be satisfied to preserve qos guarantees  and diskscheduling algorithms must be optimized for the constraints unfortunately  these two constraints are often in conflict continuous-media files typically require very high disk-bandwidth rates to satisfy their data-rate requirements  because disks have relatively low transfer rates and relatively high latency rates  disk schedulers must reduce the latency times to ensure high bandwidth  however  reducing latency times may result in a scheduling policy that does not prioritize according to deadlines in this section  we explore two diskscheduling algorithms that meet the qos requirements for continuous-media systems  20.5.1 earliest-deadline-first scheduling we first presented the earliest-deadline-first  edf  algorithm in section 19 .5.2 as an example of a cpu-scheduling algorithm that assigns priorities according to deadlines edf can also be used as a disk-scheduling algorithm ; in this context  edf uses a queue to order requests according to the time each request must be completed  its deadline   edf is similar to shortest-seek-time-first  sstf   discussed in section 12.4.2  except that instead of servicing the request closest to the current cylinder  we service requests according to deadline-the request with the closest deadline is serviced first  a problem with this approach is that servicing requests strictly according to deadline may result in higher seek tim.es  since the disk heads may move randomly throughout the disk without any regard to their current position  for example  suppose a disk head is currently at cylinder 75 and the queue of cylinders  ordered according to deadlines  is 98  183  105 under strict edf scheduling  the disk head will move from 75  to 98  to 183  and then back to 788 chapter 20 105 note that the head passes over cylinder 105 as it travels from 98 to 183 it is possible that the disk scheduler could have serviced the request for cylinder 105 en route to cylinder 183 and still preserved the deadline requirement for cylinder 183  20.5.2 scan-edf scheduling the fundamental problem with strict edf scheduling is that it ignores the position of the read-write heads of the disk ; it is possible that the heads will swil g wildly to and fro across the disk  leading to unacceptable seek times that negatively affect disk throughput recall that this is the same issue faced with fcfs scheduling  section 12.4.1   in the context of cpu scheduling  we can address this issue by adopting scan schedulil g  whereil the disk arm moves in one direction across the disk  servicing requests according to their proximity to the current cylinder once the disk arm reaches the end of the disk  it begins moving in the reverse direction this strategy optimizes seek times  scan-edf is a hybrid algorithm that combines edf with scan scheduling  scan-edf starts with edf ordering but services requests with the same deadline usil g scan order what if several requests have different deadlines that are relatively close together in this case  scan-edf may batch requests  usil g scan ordering to service requests in the same batch there are many techniques for batching requests with similar deadlines ; the only requirement is that reordering requests within a batch must not prevent a request from being serviced by its deadline if deadlines are equally distributed  batches can be organized in groups of a certain size-say  10 requests per batch  another approach is to batch requests whose deadlines fall within a given time threshold-say  100 milliseconds let 's consider an example in which we batch requests in this way assume we have the followil g requests  each with a specified deadline  in milliseconds  and a requested cylinder  ~ ; ' ' ; ! 1  -'  ~  ~ '   ; ~  ;    '  r ' k'j ~ lr         /1  -1 r ~ 'i ! ~  ~ ~ ~ ' k ;  ci ; h  a 150 25 b 201 112 c 399 95 d 94 31 e 295 185 f 78 85 g 165 150 h 125 101 i 300 85 j 210 90 suppose we are at time0  the cylinder currently being serviced is 50  and the disk head is moving toward cylinder 51 according to our batching scheme  requests d and f will be in the first batch ; a g  and h in batch 2 ; b  e  and j il batch 3 ; and c and i il the last batch requests within each batch will 20.6 789 be ordered according to scan order thus  in batch 1  we will first service request f and then request d note that we are moving downward in cylinder nun1.bers  fron 85 to 31 in batch 2  we first service request a ; then the heads begin moving upward in cylinders  servicing requests hand then g batch 3 is serviced in the order e  b  j requests i and care serviced in the final batch  perhaps the foremost qos issue with multimedia systems concerns preserving rate requirements for example  if a client wishes to view a video compressed with mpeg-1  the quality of service greatly depends on the system 's ability to deliver the frames at the required rate  our coverage of issues such as cpu and disk-scheduling algorithms has focused on how these techniques can be used to better meet the quality-ofservice requirements of multimedia applications however  if the media file is being streamed over a network-perhaps the internet-issues relating to how the network delivers the multimedia data can also significantly affect how qos demands are met in this section  we explore several network issues related to the unique demands of continuous media  before we proceed  it is worth noting that computer networks in general -and the internet in particular currently do not provide network protocols that can ensure the delivery of data with timing requirements  there are some proprietary protocols-notably those running on cisco routers-that do allow certain network traffic to be prioritized to meet qos requirements  such proprietary protocols are not generalized for use across the internet and therefore do not apply to our discussion  when data are routed across a network  it is likely that the transmission will encounter congestion  delays  and other network traffic issues-issues that are beyond the control of the originator of the data for multimedia data with timing requirements  any timing issues must be synchronized between the end hosts  the server delivering the content and the client playing it back  one protocol that addresses timing issues is the  real-thne  rrrf '   rtp is an internet standard for delivering real-time data  including audio and video it can be used for transporting media formats such as mp3 audio files and video files compressed using mpeg rtf does not provide any qos guarantees ; rathe1 ~ it provides features that allow a receiver to remove jitter introduced by delays and congestion in the network  in following sections  we consider two other approaches for handling the unique requirements of continuous media  20.6.1 unicasting and multicasting in general  there are three methods for delivering content from a server to a client across a network  the server delivers the content to a single client if the content is being delivered to more than one client  the server must establish a separate unicast for each client  790 chapter 20 '  ' ',''' ' 'nh  the server delivers the content to all clients  regardless of whether they wish to receive the content or not  the server delivers the content to a group of receivers that indicate they wish to receive the content ; this method lies somewhere between unicasting and broadcasting  an issue with unicast delivery is that the server must establish a separate unicast session for each client this seems especially wasteful for live real-time streaming  where the server must make several copies of the same content  one for each client obviously  broadcasting is not always appropriate  as not all clients may wish to receive the stream  suffice it to say that broadcasting is typically only used across local area networks and is not possible across the public internet  multicasting appears to be a reasonable compromise  since it allows the server to deliver a single copy of the content to all clients indicating that they wish to receive it the difficulty with multicasting from a practical standpoint is that the clients must be physically close to the server or to intermediate routers that relay the content from the originating server if the route from the server to the client must cross intermediate routers  the routers must also support multicasting if these conditions are not met  the delays incurred during routing may result in violation of the timing requirements of the continuous media in the worst case  if a client is connected to an intermediate router that does not support multicasting  the client will be unable to receive the multicast stream at all ! currently  most streaming media are delivered across unicast channels ; however  multicasting is used in various areas where the organization of the server and clients is known in advance for example  a corporation with several sites across a country may be able to ensure that all sites are connected to multicasting routers and are within reasonable physical proximity to the routers the organization will then be able to deliver a presentation from the chief executive officer using multicasting  20.6.2 real-time streaming protocol in section 20.1.1  we described some features of streaming media as we noted there  users may be able to randomly access a media stream  perhaps replaying or pausing  as they would with a dvd controller how is this possible to answer this question  let 's consider how streaming media are delivered to clients one approach is to stream the media from a standard web server using the hypertext transport protocol  or http-the protocol used to deliver documents from a web server quite often  clients use a such as quicktime  realplaye1 ~ or windows media player  to play media streamed from a standard web server typically  the client first requests a  ,e  cajj.u   which contains the location  possibly identified by a uniform resource locatm ~ or url  of the streaming media file this metafile is delivered to the client 's web browser  and the browser then starts the appropriate media player according to the type of media specified by the metafile for example  a real audio stream would require the realplayer  while the windows media player would be used to play back streaming windows media the media player then contacts the web server and requests the streaming media the stream 20.6 791 figure 20.2 streaming media from a conventional web server  is delivered from the web server to the media player using standard http requests this process is outlined in figure 20.2  the problem with delivering streaming media from a standard web server is that http is considered a protocol ; thus  a web server does not maintain the state  or status  of its connection with a client as a result  it is difficult for a client to pause during the delivery of streaming media content  since pausing would require the web server to know where in the stream to begin when the client wished to resume playback an alternative strategy is to use a specialized streaming server that is designed specifically for streaming media one protocol designed for communication between streaming servers and media players is known as the or  the significant advantage rtsp provides over http is a connection between the client and the server  which allows the client to pause or seek to random positions in the stream ~ during playback delivery of streaming media using rtsp is similar to delivery using http  figure 20.2  in that the metafile is delivered using a conventional web server however  rather than a web server  the streami.j.1.g media is delivered from a streaming server using the rtsp protocol the operation of rtsp is shown in figure 20.3  rtsp defines several commands as part of its protocol ; these commands are sent from a client to an rtsp streaming server the commands i.j.1.clude the following   the server allocates resources for a client session   the server delivers a stream to a client session established from a setup command  the server suspends delivery of a stream but maintains the resources for the session  the server breaks down the connection and frees up resources allocated for the session  792 chapter 20 20.7 figure 20.3 real-time streaming protocol  rtsp   the commands can be illustrated with a state machine for the server  as shown in figure 20.4 as you can see in the figure  the rtsp server may be in one of three states  and transitions between these three states are triggered when the server receives one of the rtsp commands from the client  using rtsp rather than http for streaming media offers several other advantages  but they are primarily related to networking issues and are therefore beyond the scope of this text we encourage interested readers to consult the bibliographical notes at the end of this chapter for sources of further information  the cineblitz multimedia storage server is a high-performance media server that supports both continuous media with rate requirements  such as video and audio  and conventional data with no associated rate requirements  such as text and images   cineblitz refers to clients with rate requirements as whereas have no rate constraints cine blitz guarantees to meet the rate requirements of real-time clients by implementing an admission controller  admitting a client only if there are sufficient resources to allow data retrieval at the required rate in this section  we explore the cineblitz disk-schedulu1.g and admission-control algorithms  setup play tear down pause figure 20.4 finite-state machine representing rtsp  20.7 793 20.7.1 disk scheduling the cineblitz disk scheduler services requests in at the beginning of each service cycle  requests are placed inc-scan  section 12.4.4   recall from our earlier discussions of c-scan that the disk heads move from one end of the disk to the other however  rather than reversing direction when they reach the end of the disk  as in pure scan disk scheduling  section 12.4.3   the disk heads move back to the beginr1ing of the disk  20.7.2 admission control the admission-control algorithm in cil eblitz must monitor requests from both real-time and non-real-time clients  ensuring that both classes of clients receive service furthermore  the admission controller must provide the rate guarantees required by real-time clients to ensure fairness  only a fraction p of time is reserved for real-time clients  while the remainder  1 p  is set aside for non-real-time clients here  we explore the admission controller for real-time clients only ; thus  the term client refers to a real-time client  the admission controller in cineblitz monitors various system resources  such as disk bandwidth and disk latency  while keeping track of available buffer space the cineblitz admission controller admits a client only if there is enough available disk bandwidth and buffer space to retrieve data for the client at its required rate  cineblitz queues requests for continuous media files  where r1  r2  r3    r11 are the requests and r ; is the required data rate for a given request r requests in the queue are served in cyclic order in rounds of time-length t  t being tpically measured in seconds   the scheme is using a technique known as double wherein a buffer is allocated for each request r ; of size 2 x t x r ;  during each cycle i  the server must for each request r j  retrieve the data from disk to buffer  i mod 2   transfer data from the   i + 1  mod 2  buffer to the client  this process is illustrated in figure 20.5 for n clients  the total buffer space b required is n l2 x t x r ;     b  i = l  20.1  the fundamental idea behil d the admission controller in cineblitz is to bound requests for entry into the queue according to the following criteria  the service time for each request is first estimated  a request is admitted only if the sum of the estimated service times for all admitted requests does not exceed the duration of service cycle t  lett x r ; bits be retrieved during a cycle for each real-time client r ; with rater ;  if r1  r2    r11 are the clients currently active in the system  then the admission controller must ensure that the total time for retrieving t x r1  t x r2    t x r11 794 chapter 20 0 f---1 double buffer total buffer space  b  figure 20.5 double buffering in cinebiitz  bits for the corresponding real-time clients does not exceed t we explore the details of this admission policy in the remainder of this section  if b is the size of a disk block  then the maximum number of disk blocks thatcanberetrievedforrequest r1c duringeachcycleis i  t xr ~ c  /bl + 1 the 1 in this formula comes from the fact that if t x yjc is less than b  then it is possible for t x r1c bits to span the last portion of one disk block and the beginning of another  causing two blocks to be retrieved we know that the retrieval of a disk block involves  a  a seek to the track contailling the block and  b  the rotational delay as the data in the desired track arrive under the disk head as described  cineblitz uses a c-scan disk-scheduling algorithm  so disk blocks are retrieved in the sorted order of their positions on the disk  if tscek and trot refer to the worst-case seek and rotational delay times  the maximum latency incurred for servicil1g n requests is n  t x r ;  2 x tseek + l 1-b-l + 1 x trot 1 = 1  20.2  in this equation  the 2 x tseek component refers to the maximum disk-seek latency incurred in a cycle the second component reflects the sum of the retrievals of the disk blocks multiplied by the worst-case rotational delay  if the transfer rate of the disk is r dislu then the time to h ansfer t x yic bits of data for request r1c is  t x r ~ c  /rdisk as a result  the total time for retrieving t x r1  t x r2    t x r11 bits for requests r1  r2    r11 is the sum of equation 20.2 and n t x r ; t ; raisk  20.3  therefore  the admission controller in cineblitz only admits a new client r if at least 2 x t x r ; bits of free buffer space are available for the client and the following equation is satisfied  n  t x r ;  n t x r ; 2 x tsee/c + l 1-b-l + 1 x trot + l ~ s t  i = l i = l diok  20.4  20.8 795 multimedia applications are in common use in modern computer systems  multimedia files include video and audio filesf which may be delivered to systems such as desktop computersf personal digital assistantsf and cell phones the primary distinction between multimedia data and conventional data is that multimedia data have specific rate and deadline requiren ents  because multimedia files have specific timing requirementsf the data must often be compressed before delivery to a client for playback multimedia data may be delivered either from the local file system or from a multimedia server across a network connection using a technique known as streaming  the timing requirements of multimedia data are known as qualityof service requirementsf and conventional operating systems often can not make quality-of-service guarantees to provide quality of servicef multimedia systems must provide a form of admission control whereby a system accepts a request only if it can meet the quality-of-service level specified by the request  providing quality-of-service guarantees requires evaluating how an operating system performs cpu schedulingf disk schedulingf and network management  both cpu and disk scheduling typically use the deadline requirements of a continuous-media task as a scheduling criterion network management requires the use of protocols that handle delay and jitter caused by the network as well as allowing a client to pause or move to different positions in the stream during playback  20.1 explain why the traditional internet protocols for transmitting data are not sufficient to provide the quality-of-service guarantees required for a multimedia system discuss what changes are required to provide the qos guarantees  20.2 contrast unicastingf multicastingf and broadcasting as techniques for delivering content across a computer network  20.3 assume that we wish to compress a digital video file using mpeg-1 technology the target bit rate is 1.5 mbps if the video is displayed at a resolution of 352 x 240 at 30 frames per second using 24 bits to represent each colorf what is the necessary compression ratio to achieve the desired bit rate 20.4 assume that a digital video file is being displayed at a rate of 30 frames per second ; the resolution of each frame is 640 x 480f and 24 bits are being used to represent each color assuming that no compression is being usedf what is the bandwidth necessary to deliver this file next assuming that the file has been compressed at a ratio of 200  1f what is the bandwidth necessary to deliver the compressed file 20.5 a multimedia application consists of a set containing 100 imagesf 10 minutes of videof and 10 minutes of audio the compressed sizes of the imagesf videof and audio are 500 mbf 550 mbf and 8 mbf respectively  the images were compressed at a ratio of 15  1f and the video and 796 chapter 20 audio were compressed at 200  1 and 10  1  respectively what were the sizes of the images  video  and audio before compression 20.6 which of the following types of real-time streaming applications can tolerate delay which can tolerate jitter live real-time streaming on-demand real-time streaming 20.7 distinguish between progressive download and real-time streaming  20.8 the following table contains a number of requests with their associated deadlines and cylinders requests with deadlines occurring within 100 milliseconds of each other will be batched the disk head is currently at cylinder 94 and is moving toward cylinder 95 if scan-edf disk scheduling is used  how are the requests batched together  and what is the order of requests within each batch  o ,j,c ~ ;   ; ;      ;  ~    ' '  !  ;  ;  ~  '..ji  ; ; _ ~  3      ~ tp ~ li ~ ~ ~ ~  ~ r1 57 77 r2 300 95 r3 250 25 r4 88 28 r5 85 100 r6 110 90 r7 299 50 r8 300 77 r9 120 12 r10 212 2 20.9 repeat exercise 20.8  but this time batch requests that have deadlines occurring within 75 milliseconds of each other  20.10 what operating principle is used by the cine blitz system in performing admission control for requests for media files 20.11 consider two processes  p1 and p2  where p1 = 50  t1 = 25  p2 = 75  and t2 = 30  a can these two processes be scheduled using rate-monotonic scheduling illustrate your answer using a gantt chart  b illustrate the schedulil1.g of these two processes using earliestdeadline first  edf  scheduling  797 fuhrt  1994  provides a general overview of multimedia systems topics related to the delivery of multimedia through networks can be found in kurose and ross  2005   operating-system support for multimedia is discussed in steinmetz  1995  and leslie et al  1996   resource management for resources such as processing capability and memory buffers is discussed in mercer et al   1994  and druschel and peterson  1993   reddy and wyllie  1994  give a good overview of issues relating to the use of i/0 in a multimedia system  discussions regarding the appropriate programming model for developing multimedia applications are presented in regehr et al  2000   an admission control system for a rate-monotonic scheduler is considered in lauzac et al   2003   bolosky et al  1997  present a system for serving video data and discuss the schedule-management issues that arise in such a system the details of a real-time streaming protocol can be found at http  / /www.rtsp.org tudor  1995  gives a tutorial on mpeg-2 a tutorial on video compression techniques can be found at http  / /www.wave-report.com/tutorials/vc.htm  part nine we can now integrate the concepts described in this book by describing real operating systems two such systems are covered in great detaiilinux and windows xp we chose linux for several reasons  it is popular  it is freely available  and it represents a full-featured unix system this gives a student of operating systems an opportunity to read-and modifyrea/ operating-system source code  we also cover windows xp in great detail this recent operating system from microsoft is gaining popularity  not only in the stand-alonemachine market  but also in the workgroup-server market we chose windows xp because it provides an opportunity for us to study a modern operating system that has a design and implementation drastically different from those of unix  in addition  we briefly discuss other highly influential operating systems  we have chosen the order of presentation to highlight the similarities and differences among the systems ; it is not strictly chronological and does not reflect the relative importance of the systems  finally  we provide on-line coverage of three other systems the freebsd system is another unix system however  whereas linux combines features from several unix systems  freebsd is based on the bsd model of unix freebsd source code  like linux source code  is freely available the mach operating system is a modern operating system that provides compatibility with bsd unix windows is another modern operating system from microsoft for intel pentium and later microprocessors ; it is compatible with ms-dos and microsoft windows applications  21.1 this chapter presents an in-depth examination of the linux operating system  by exam.ining a complete  real system  we can see how the concepts we have discussed relate both to one another and to practice  linux is a version of unix that has gained popularity in recent years in this chapter  we look at the history and development of linux and cover the user and programmer interfaces that linux presents-interfaces that owe a great deal to the unix tradition we also discuss the internal methods by which linux implements these interfaces linux is a rapidly evolving operating system  this chapter describes developments through the linux 2.6 kernel  which was released in late 2003  to explore the history of the unix operating system from which linux is derived and the principles upon which linux is designed  to examine the linux process model and illustrate how linux schedules processes and provides interprocess communication  to look at memory management in linux  to explore how linux implements file systems and manages 110 devices  linux looks and feels much like any other unix system ; indeed  unix compatibility has been a major design goal of the linux project however  linux is much younger than most unix systems its development began in 1991  when a finnish student  linus torvalds  wrote and christened linux  a small but self-contained kernel for the 80386 processor  the first true 32-bit processor in intel 's range of pc-compatible cpus  early in its development  the linux source code was made available free on the internet as a result  linux 's history has been one of collaboration by many users from all around the world  corresponding almost exclusively over the internet from an initial kernel that partially implemented a small subset of 801 802 chapter 21 the unix system services  the linux system has grown to include much unix functionality  in its early days  linux development revolved largely around the central operating-system kernel-the core  privileged executive that n1.anages all system resources and that interacts directly with the computer hardware  we need much more than this kernel to produce a full operating systeitl  o course it is useful to make the distinction between the linux kernel and a linux system the is an entirely original piece of software developed from scratch by the linux con1.munity the as we know it today  includes a multitude of components  some written from scratch  others borrowed from other development projects  and still others created in collaboration with other teams  the basic linux system is a standard environment for applications and user programming  but it does not enforce any standard means of managing the available functionality as a whole as linux has matured  a need has arisen for another layer of functionality on top of the linux system this need has been met by various linux distributions a includes all the standard components of the linux system  plus a set of administrative tools to simplify the initial installation and subsequent upgrading of linux and to manage installation and removal of other packages on the system a modern distribution also typically includes tools for management of file systems  creation and management of user accounts  administration of networks  web browsers  word processors  and so on  21.1.1 the linux kernel the first linux kernel released to the public was version 0.01  dated may 14  1991 it had no networking  ran only on 80386-compatible intel processors and pc hardware  and had extremely limited device-driver support the virtual memory subsystem was also fairly basic and included no support for memorymapped files ; however  even this early incarnation supported shared pages with copy-on-write the only file system supported was the minix file system -the first linux kernels were cross-developed on a minix platform however  the kernel did implement proper unix processes with protected address spaces  the next milestone version  linux 1.0  was released on march 14  1994  this release culminated three years of rapid development of the limlx kernel  perhaps the single biggest new feature was networking  1.0 included support for unix 's standard tcp lip networking protocols  as well as a bsd-compatible socket interface for networking programming device-driver support was added for running ip over an ethernet or  using ppp or slip protocols  over serial lines or modems  the 1.0 kernel also included a new  much enhanced file system without the limitations of the original minix file system and supported a range of scsi controllers for high-performance disk access the developers extended the virtual memory subsystem to support paging to swap files and memory mapping of arbitrary files  but only read-only memory mapping was implemented in 1.0   a range of extra hardware support was also included in this release  although still restricted to the intel pc platform  hardware support had grown to include floppy-disk and cd-rom devices  as well as sound cards  a range of mice  and international keyboards floating-point emulation was provided 21.1 803 in the kernel for 80386 users who had no 80387 math coprocessor ; system v unix-style inclllding shared memory  semaphores  and message queues  was implemented simple support for dynamically loadable and unloadable kernel modules was supplied as well  at this point  development started on the 1.1 kernel stream  but numerous bug-fix patches were released subsequently against 1.0 a pattern was adopted as the standard numbering convention for linux kernels kernels with an odd minor-version number  such as 1.1  1.3  and2.1  are evennumbered minor-version numbers are stable updates against the stable kernels are intended only as remedial versions  whereas the development kernels may include newer and relatively untested functionality  in march 1995  the 1.2 kernel was released this release did not offer nearly the same improvement in functionality as the 1.0 release  but it did support a much wider variety of hardware  including the new pci hardware bus architecture developers added another pc-specific feature-support for the 80386 cpu 's virtual8086 mode-to allow emulation of the dos operating system for pc computers they also updated the networking stack to provide support for the ipx protocol and made the ip implementation more complete by including accounting and firewalling functionality  the 1.2 kernel was the final pc-only linux kernel the source distribution for linux 1.2 included partially implemented support for sparc  alpha  and mips cpus  but full integration of these other architectures did not begin until after the 1.2 stable kernel was released  the linux 1.2 release concentrated on wider hardware support and more complete implementations of existing functionality much new functionality was under development at the time  but integration of the new code into the main kernel source code had been deferred until after the stable 1.2 kernel had been released as a result  the 1.3 development stream saw a great deal of new functionality added to the kernel  this work was finally released as linux 2.0 in jmce 1996 this release was given a major version-number increment on account of two major new capabilities  support for multiple architectures  including a 64-bit native alpha port  and support for multiprocessor architectures linux distributions based on 2.0 are also available for the motorola 68000-series processors and for sun 's sparc systems a derived version of linux running on top of the mach microkernel also runs on pc and powermac systems  the changes in 2.0 did not stop there the memory-management code was substantially improved to provide a unified cache for file-system data independent of the caching of block devices as a result of this change  the kernel offered greatly increased file-system and virtual memory performance  for the first time  file-system caching was extended to networked file systems  and writable memory-mapped regions also were supported  the 2.0 kernel also included much improved tcp /ip performance  and a number of new networking protocols were added  including apple talk  ax.25 an'lateur radio networking  and isdn support the ability to mount remote netware and smb  microsoft lanmanager  network volumes was added  other major improvements in 2.0 were support for internal kernel threads  for handling dependencies between loadable modules  and for automatic loading of modules on demand dynamic configuration of the kernel at run time was much improved through a new  standardized configuration interface  804 chapter 21 additional new features included file-system quotas and posix-compatible real-time process-scheduling classes  improvements continued with the release of linux 2.2 in january 1999 a port for ultrasparc systems was added networking was enhanced with more flexible firewalling  better routing and traffic management  and support for tcp large window and selective acks acorn  apple  and nt disks could now be read  and nfs was enhanced and a kernel-mode nfs daemon added signal handling  interrupts  and some i/0 were locked at a finer level than before to improve symmetric multiprocessor  smp  performance  advances in the 2.4 and 2.6 releases of the kernel include increased support for smp systems  journaling file systems  and enhancements to the memorymanagement system the process scheduler was modified in version 2.6  providing an efficient 0  1  scheduling algorithm in addition  the limix 2.6 kernel is now preemptive  allowing a process to be preempted while running in kernel mode  21.1.2 the linux system in many ways  the linux kernel forms the core of the linux project  but other components make up the complete linux operating system whereas the linux kernel is composed entirely of code written from scratch specifically for the linux project  much of the supporting software that makes up the linux system is not exclusive to linux but is common to a number of unix-like operating systems in particular  linux uses many tools developed as part of berkeley 's bsd operating system  mit 's x window system  and the free software foundation 's gnu project  this sharing of tools has worked in both directions the main system libraries of linux were originated by the gnu project  but the linux community greatly improved the libraries by addressing omissions  li1.efficiencies  and bugs other components  such as the l were already of sufficiently high quality to be used directly in linux the networkingadministration tools under linux were derived from code first developed for 4.3 bsd  but more recent bsd derivatives  such as freebsd  have borrowed code from linux in return examples include the intel floating-point-emulation math library and the pc sound-hardware device drivers  the linux system as a whole is maintained by a loose network of developers collaborating over the internet  with small groups or individuals having responsibility for maintaining the integrity of specific components  a small number of public internet file-transfer-protocol  ftp  archive sites act as de facto standard repositories for these components the document is also maintained by the linux community as a means of ensuring compatibility across the various system components  this standard specifies the overall layout of a standard linux file system ; it determines under which directory names configuration files  libraries  system binaries  and run-time data files should be stored  21.1.3 linux distributions in theory  anybody can install a linux system by fetching the latest revisions of the necessary system components from the ftp sites and compiling them  in linux 's early days  this operation was often precisely what a linux user 21.1 805 had to carry out as linux has matured  however  various individuals and groups have attempted to make this job less painful by providing standard  precompiled sets of packages for easy installation  these collections  or distributions  include much more than just the basic linux system they typically include extra system-installation and management utilities  as well as precompiled and ready-to-install packages of many of the common unix tools  such as news servers  web browsers  text-processing and editing tools  and even games  the first distributions managed these packages by simply providing a means of unpacking all the files into the appropriate places one of the important contributions of modem dish ibutions  however  is advanced package management today ' s linux distributions include a package-tracking database that allows packages to be installed  upgraded  or removed painlessly  the sls distribution  dating back to the early days of linux  was the first collection of linux packages that was recognizable as a complete distribution  although it could be installed as a single entity  sls lacked the packagemanagement tools now expected of linux distributions the distribution represented a great improvement in overall quality  even though it also had poor package management ; in fact  it is still one of the most widely installed distributions in the linux community  since slackware ' s release  many commercial and noncommercial limlx distributions have become available and are particularly popular distributions ; the first comes from a commercial linux support company and the second from the free-software linux community other commercially supported versions of linux include distributions from cz,ldera  and a large linux following in germany has resulted in several dedicated german-language distributions  including versions from and there are too many linux distributions in circulation for us to list all of them here the variety of distributions does not prohibit compatibility across linux distributions  however the rpm package file format is used  or at least understood  by the majority of distributions  and commercial applications distributed in this format can be installed and run on any distribution that can accept rpm files  21.1.4 linux licensing the linux kernel is distributed under the gnu general public license  gpl   the terms of which are set out the free software fmmdation linux is not public-domain software implies that the authors have waived copyright rights in the software  but copyright rights in linux code are still held by the code 's various authors linux is free software  however  in the sense that people can copy it  modify it  use it in any manner they want  and give away their own copies  without any restrictions  the main implications of linux 's licensing terms are that nobody using liimx  or creating a derivative of linux  a legitimate exercise   can make the derived product proprietary software released under the gpl camwt be redistributed as a binary-only product if you release software that includes any components covered by the gpl  then  under the gpl  you must make source code available alongside any binary distributions  this resh iction does 806 chapter 21 21.2 not prohibit ntaking-or even selling-binary-only software distributions  as long as anybody who receives binaries is also given the opportunity to get source code for a reasonable distribution charge  in its overall design  linux resembles any other traditional  nonmicrokernel unix implementation it is a multiuser  multitasking system with a full set of unix-compatible tools linux 's file system adheres to traditional unix semantics  and the standard unix networking model is implemented fully  the internal details of linux ' s design have been influenced heavily by the history of this operating system 's development  although linux runs on a wide variety of platforms  it was developed exclusively on pc architecture a great deal of that early development was carried out by individual enthusiasts  rather than by well-funded development or research facilities  so from the start linux attempted to squeeze as much functionality as possible from limited resources today  linux can run happily on a multiprocessor machine with hundreds of megabytes of main memory and many gigabytes of disk space  but it is still capable of operating usefully in under 4 mb of ram  as pcs became more powerful and as memory and hard disks became cheaper  the original  minimalist limix kernels grew to implement more unix functionality speed and efficiency are still important design goals  but much recent and current work on linux has concentrated on a third major design goal  standardization one of the prices paid for the diversity of unix implementations currently available is that source code written for one may not necessarily compile or run correctly on another even when the same system calls are present on two different unix systems  they do not necessarily behave in exactly the same way the posix standards comprise a set of specifications for different aspects of operating-system behavior there are posix documents for common operating-system functionality and for extensions such as process threads and real-time operations linux is designed to be compliant with the relevant posix documents ; at least two linux distributions have achieved official posix certification  because it gives standard interfaces to both the programmer and the user  linux presents few surprises to anybody familiar with unix we do not detail these interfaces here the sections on the programmer interface  section a.3  and user interface  section a.4  of bsd apply equally well to linux by default  howeve1 ~ the linux programming interface adheres to svr4 unix semantics  rather than to bsd behavior a separate set of libraries is available to implement bsd semantics in places where the two behaviors differ significantly  many other standards exist in the unix world  but full certification of linux with respect to these standards is sometimes slowed because certification is often available only for a fee  and the expense involved in certifying an operating system 's compliance with most standards is substantial however  supporting a wide base of applications is important for any operating system  so implementation of standards is a major goal for linux development  even if the implementation is not formally certified in addition to the basic posix 21.2 807 standard  linux currently supports the posix threading extensions-pthreads -and a subset of the posix extensions for real-time process control  21.2.1 components of a linux system the linux system is composed of three main bodies of code  in line with most traditional unix implementations  kernel the kernel is responsible for maintaining all the important abstractions of the operating system  including such things as virtual memory and processes  system libraries the system libraries define a standard set of functions through which applications can interact with the kernel these functions implement much of the operating-system functionality that does not need the full privileges of kernel code  system utilities the system utilities are programs that perform individual  specialized management tasks some system utilities may be invoked just once to initialize and configure some aspect of the system ; othersknown as daemons in unix terminology -may run permanently  handling such tasks as responding to incoming network connections  accepting logon requests from terminals  and updating log files  figure 21.1 illustrates the various components that make up a full linux system the most important distinction here is between the kernel and everything else all the kernel code executes in the processor 's privileged mode with full access to all the physical resources of the computer linux refers to this privileged mode as kernel under linux  no user-mode code is built into the kernel any operating-system-support code that does not need to run in kernel mode is placed into the system libraries instead  although various modern operating systems have adopted a messagepassing architecture for their kernel internals  linux retains un ' lx ' s historical model  the kernel is created as a single  monolithic binary the main reason is to improve performance because all kernel code and data structures are kept iil a single address space  no context switches are necessary when a process calls an operating-system function or when a hardware interrupt is delivered  linux kernel loadable kernel modules figure 21.1 components of the linux system  808 chapter 21 this single address space contains not only the core scheduling and virtual memory code but all kernel code  including all device drivers  file systems  and networking code  even though all the kernel components share this same m.elting pot  there is still room for modularity in the san1.e way that user applications can load shared libraries at run time to pull in a needed piece of code  so the linux kernel can load  and unload  modules dynamically at run time the kernel does not necessarily need to know in advance which modules may be loaded -they are truly independent loadable components  the linux kernel forms the core of the linux operating system it provides all the functionality necessary to run processes  and it provides system services to give arbitrated and protected access to hardware resources the kernel implements all the features required to qualify as an operating system on its own  however  the operating system provided by the linux kernel looks nothing like a unix system it is missing many of the extra features of unix  and the features that it does provide are not necessarily in the format in which a unix application expects them to appear the operating-system interface visible to running applications is not maintained directly by the kernel rather  applications make calls to the system libraries  which in turn call the operatingsystem services as necessary  the system libraries provide many types of functionality at the simplest level  they allow applications to make kernel-system service requests making a system call involves transferring control from tmprivileged user mode to privileged kernel mode ; the details of this transfer vary from architecture to architecture the libraries take care of collecting the system-call arguments and  if necessary  arranging those arguments in the special form necessary to make the system call the libraries may also provide more complex versions of the basic system calls for example  the c language 's buffered file-handling functions are all implemented in the system libraries  providing more advanced control of file i/ 0 than the basic kernel system calls the libraries also provide routines that do not correspond to system calls at all  such as sorting algorithms  mathematical functions  and string-manipulation routines all the functions necessary to support the running of unix or posix applications are implemented here in the system libraries  the limix system includes a wide variety of user-mode programs-both system utilities and user utilities the system utilities include all the programs necessary to initialize the system  such as those to configure network devices and to load kernel modules continually running server programs also com1.t as system utilities ; such programs handle user login requests  incoming network connections  and the printer queues  not all the standard utilities serve key system-administration functions  the unix user environment contains a large number of standard utilities to do simple everyday tasks  such as listing directories  moving and deleting files  and displaying the contents of a file more complex utilities can perform text-processing functions  such as sorting textual data and performing pattern searches on input text together  these utilities form a standard tool set that users can expect on any unix system ; although they do not perform any operating-system function  they are an important part of the basic limix system  21.3 21.3 809 the linux kernel has the ability to load and unload arbitrary sections of kernel code on demand these loadable kernel modules run in privileged kernel mode and as a consequence have full access to all the hardware capabilities of the machine on which they run in theory  there is no restriction on what a kernel module is allowed to do ; typically  a module might implement a device driver  a file system  or a networking protocol  kernel modules are convenient for several reasons linux ' s source code is free  so anybody wanting to write kernel code is able to compile a modified kernel and to reboot to load that new functionality ; however  recompiling  relinking  and reloading the entire kernel is a cumbersome cycle to undertake when you are developing a new driver if you use kernel modules  you do not have to make a new kernel to test a new driver-the driver can be compiled on its own and loaded into the already-rmming kernel of course  once a new driver is written  it can be distributed as a module so that other users can benefit from it without having to rebuild their kernels  this latter point has another implication because it is covered by the gpl license  the linux kernel can not be released with proprietary components added to it  unless those new components are also released under the gpl and the source code for them is made available on demand the kernel 's module interface allows third parties to write and distribute  on their own terms  device drivers or file systems that could not be distributed under the gpl  kernel modules allow a linux system to be set up with a standard minimal kernet without any extra device drivers built in any device drivers that the user needs can be either loaded explicitly by the system at startup or loaded automatically by the system on demand and unloaded when not in use for example  a cd-rom driver might be loaded when a cd is mounted and unloaded from memory when the cd is dismounted from the file system  the module support under linux has three components  the allows modules to be loaded into memory and to talk to the rest of the kernel  the allows modules to tell the rest of the kernel that a new driver has become available  a allows different device drivers to reserve hardware resources and to protect those resources from accidental use by another driver  21.3.1 module management loading a module requires more than just loading its binary contents into kernel memory the system must also make sure that any references the module makes to kernel symbols or entry points are updated to point to the correct locations in the kernel 's address space linux deals with this reference updating by splitting the job of module loading into two separate sections  the management of sections of module code in kernel memory and the handling of symbols that modules are allowed to reference  810 chapter 21 linux maintains an internal syncbol table in the kernel this symbol table does not contain the full set of symbols defined in the kernel during the latter 's compilation ; rather  a symbol must be exported explicitly by the kernel the set of exported symbols constitutes a well-defined interface by which a module can interact with the kernel  although exporting symbols from a kernel function requires an explicit request by the programmer  no special effort is needed to import those symbols into a module a module writer just uses the standard external linking of the c language  any external symbols referenced by the module but not declared by it are simply marked as unresolved in the final module binary produced by the compiler when a module is to be loaded into the kernel  a system utility first scans the module for these unresolved references all symbols that still need to be resolved are looked up in the kernel 's symbol table  and the correct addresses of those symbols in the currently running kernel are substituted into the module 's code only then is the module passed to the kernel for loading if the system utility can not resolve any references in the module by looking them up in the kernel 's symbol table  then the module is rejected  the loading of the module is performed in two stages first  the moduleloader utility asks the kernel to reserve a continuous area of virtual kernel memory for the module the kernel returns the address of the memory allocated  and the loader utility can use this address to relocate the module 's machine code to the correct loading address a second system call then passes the module  plus any symbol table that the new module wants to export  to the kernel the module itself is now copied verbatim into the previously allocated space  and the kernel 's symbol table is updated with the new symbols for possible use by other modules not yet loaded  the final module-management component is the module requestor the kernel defines a communication interface to which a module-management program can connect with this connection established  the kernel will inform the management process whenever a process requests a device driver  file system  or network service that is not currently loaded and will give the manager the opportunity to load that service the original service request will complete once the module is loaded the manager process regularly queries the kernel to see whether a dynamically loaded module is still in use and unloads that module when it is no longer actively needed  21.3.2 driver registration once a module is loaded  it remains no more than an isolated region of memory until it lets the rest of the kernel know what new functionality it provides  the kernel maintains dynamic tables of all known drivers and provides a set of routines to allow drivers to be added to or removed from these tables at any time the kernel makes sure that it calls a module 's startup routine when that module is loaded and calls the module 's cleanup routine before that module is unloaded  these routines are responsible for registering the module 's functionality  a module may register many types of drivers and may register more than one driver if it wishes for example  a device driver might want to register two separate mechanisms for accessing the device registration tables include the following items  21.3 811 device drivers these drivers include character devices  such as printers/ terminals/ and mice  1 block devices  including all disk drives  1 and network interface devices  file systems the file system may be anything that implements linux 's virtual-file-system calling routines it might implement a format for storing files on a disk  but it might equally well be a network file system  such as nfs1 or a virtual file system whose contents are generated on demand/ such as linux 's /proc file system  network protocols a module may implement an entire networking protocot such as ipx1 or simply a new set of packet-filtering rules for a network firewall  binary format this format specifies a way of recognizing/ and loading/ a new type of executable file  in addition/ a module can register a new set of entries in the sysctl and/proc tables  to allow that module to be configured dynamically  section 21.7.4   21.3.3 conflict resolution commercial unix implementations are usually sold to run on a vendor/s own hardware one advantage of a single-supplier solution is that the software vendor has a good idea about what hardware configurations are possible pc hardware/ however/ comes in a vast number of configurations  with large numbers of possible drivers for devices such as network cards  scsi controllers/ and video display adapters the problem of managing the hardware configuration becomes more severe when modular device drivers are supported/ since the currently active set of devices becomes dynamically variable  linux provides a central conflict-resolution mechanism to help arbitrate access to certain hardware resources its aims are as follows  to prevent modules from clashing over access to hardware resources to prevent aui  oprobes-device-driver probes that auto-detect device configuration-from interfering with existing device drivers to resolve conflicts among multiple drivers trying to access the same hardware-for example  as when both the parallel printer driver and the parallel-line ip  pup  network driver try to talk to the parallel printer port to these ends/ the kernel maintains lists of allocated hardware resources  the pc has a limited number of possible i/0 ports  addresses in its hardware i/0 address space   interrupt lines/ and dma channels when any device driver wants to access such a resource  it is expected to reserve the resource with the kernel database first this requirement incidentally allows the system administrator to determine exactly which resources have been allocated by which driver at any given point  a module is expected to use this mechanism to reserve in advance any hardware resources that it expects to use if the reservation is rejected because the resource is not present or is already in use  then it is up to the module 812 chapter 21 21.4 to decide how to proceed it may fail its initialization and request that it be unloaded if it can not continue  or it may carry on  using alternative hardware resources  a process is the basic context within which all user-requested activity is serviced within the operating system to be compatible with other unix systems  linux must use a process model similar to those of other versions of unix linux operates differently from unix in a few key places  however in this section  we review the traditional unix process model  section a.3.2  and introduce linux 's own threading model  21.4.1 the fork   and exec   process model the basic principle of unix process management is to separate two operations  the creation of a process and the rum1.ing of a new program a new process is created by the fork   system call  and a new program is run after a call to exec    these are two distinctly separate functions a new process may be created with fork   without a new program being run-the new subprocess simply continues to execute exactly the same program that the first  parent  process was running equally  running a new program does not require that a new process be created first  any process may call exec   at any time the currently rumung program is immediately terminated  and the new program starts executing in the context of the existing process  this model has the advantage of great simplicity it is not necessary to specify every detail of the environment of a new program in the system call that runs that program ; the new program simply runs in its existing environment  if a parent process wishes to modify the environment in which a new program is to be run  it can fork and then  still running the original program in a child process  make any system calls it requires to modify that child process before finally executing the new program  under unix  then  a process encompasses all the information that the operating system must maintain to track the context of a single execution of a single program under linux  we can break down this context into a number of specific sections broadly  process properties fall into three groups  the process identity  environment  and context  21.4.1.1 process identity a process identity consists mainly of the following items  process id  pid   each process has a lmique identifier the pid is used to specify the process to the operating system when an application makes a system call to signal  modify  or wait for the process additional identifiers associate the process with a process group  typically  a tree of processes forked by a single user command  and login session  credentials each process must have an associated user id and one or more group ids  user groups are discussed in section 10.6.2  that determine the rights of a process to access system resources and files  21.4 813 personality process personalities are not traditionally found on unix systems  but under linux each process has an associated personality identifier that can slightly modify the semantics of certain system calls  personalities are primarily used by emulation libraries to request that system calls be compatible with certain varieties of unix  most of these identifiers are under the limited control of the process itself the process group and session identifiers can be changed if the process wants to start a new group or session its credentials can be changed  subject to appropriate security checks howeve1 ~ the primary pid of a process is unchangeable and uniquely identifies that process until termination  21.4.1.2 process environment a process 's environment is inherited from its parent and is composed of two null-terminated vectors  the argument vector and the enviromnent vector the argument vector simply lists the command-line arguments used to invoke the running program ; it conventionally starts with the name of the program itself  the environment vector is a list of name = value pairs that associates named environment variables with arbitrary textual values the environment is not held in kernel memory but is stored in the process 's own user-mode address space as the first datum at the top of the process 's stack  the argument and environment vectors are not altered when a new process is created ; the new child process will inherit the environment that its parent possesses however  a completely new environment is set up when a new program is invoked on calling exec   ,a process must supply the environment for the new program the kernel passes these enviromnent variables to the next program  replacing the process 's current environment the kernel otherwise leaves the environment and command-line vectors alone-their interpretation is left entirely to the user-mode libraries and applications  the passing of environment variables from one process to the next and the inheriting of these variables by the children of a process provide flexible ways to pass information to components of the user-mode system software various important environment variables have conventional meanings to related parts of the system software for example  the term variable is set up to name the type of terminal com1.ected to a user 's login session ; many programs use this variable to determine how to perform operations on the user 's display  such as moving the cursor and scrolling a region of text programs with multilingual support use the lang variable to determine in which language to display system messages for programs that include multilingual support  the environment-variable mechanism custom-tailors the operating system on a per-process basis  rather than for the system as a whole users can choose their own languages or select their own editors independently of one another  21.4.1.3 process context the process identity and environment properties are usually set up when a process is created and not changed until that process exits a process may choose to change some aspects of its identity if it needs to do so  or it may alter its environment in contrast  process context is the state of the running 814 chapter 21 program at any one time ; it changes constantly process context includes the following parts  scheduling context the most important part of the process context is its scheduling context-the information that the scheduler needs to suspend and restart the process this information includes saved copies of all the process 's registers floating-point registers are stored separately and are restored only when needed  so that processes that do not use floating-point arithmetic do not incur the overhead of saving that state the scheduling context also includes information about scheduling priority and about any outstanding signals waiting to be delivered to the process a key part of the scheduling context is the process 's kernel stack  a separate area of kernel memory reserved for use exclusively by kernel-mode code both system calls and interrupts that occur while the process is executing will use this stack  accounting the kernel maintains accounting information about the resources currently being consumed by each process and the total resources consumed by the process in its entire lifetime so far  file table the file table is an array of pointers to kernel file structures  when making file-i/o system calls  processes refer to files by their index into this table  file-system context whereas the file table lists the existing open files  the file-system context applies to requests to open new files the current root and default directories to be used for new file searches are stored here  signal-handler table unix systems can deliver asynchronous signals to a process in response to various external events the signal-handler table defines the routine in the process 's address space to be called when a specific signal arrives  virtual memory context the virtual memory context describes the full contents of a process 's private address space ; we discuss it in section 21.6  21.4.2 processes and threads linux provides the fork   system call with the traditional functionality of duplicating a process linux also provides the ability to create threads using the clone   system call however  linux does not distinguish between processes and threads in fact  linux generally uses the term task-rather than process or thread-when referring to a flow of control within a program when clone   is invoked  it is passed a set of flags that determine how much sharing is to take place between the parent and child tasks some of these flags are  21.5 21.5 815 thus  if clone   is passed the flags clone_fs  clone_vm  clone_sighand  and clone_files  the parent and child tasks will share the same file-system information  such as the current working directory   the same memory space  the same signal handlers  and the same set of open files using clone   in this fashion is equivalent to creating a thread in other systems  since the parent task shares most of its resources with its child task however  if none of these flags is set when clone   is invoked  no sharing takes place  resulting in functionality similar to the fork   system call  the lack of distinction between processes and threads is possible because linux does not hold a process 's entire context within the main process data structure ; rather  it holds the context within independent subcontexts thus  a process 's file-system context  file-descriptor table  signal-handler table  and virtual memory context are held in separate data structures the process data structure simply contains pointers to these other structures  so any number of processes can easily share a subcontext by pointing to the same subcontext  the arguments to the clone   system call tell it which subcontexts to copy  and which to share  when it creates a new process the new process always is given a new identity and a new scheduling context according to the arguments passed  however  it may either create new subcontext data struch1res initialized to be copies of the parent 's or set up the new process to use the same subcontext data structures being used by the parent the fork   system call is nothing more than a special case of clone   that copies all subcontexts  sharing none  scheduling is the job of allocating cpu time to different tasks within an operating system normally  we think of scheduling as being the running and interrupting of processes  but another aspect of scheduling is also important to linux  the running of the various kernel tasks kernel tasks encompass both tasks that are requested by a running process and tasks that execute internally on behalf of a device driver  21.5.1 process scheduling linux has two separate process-scheduling algorithms one is a time-sharing algorithm for fair  preemptive scheduling among multiple processes ; the other is designed for real-time tasks  where absolute priorities are more important than fairness  the scheduling algorithm used for routine time-sharing tasks received a major overhaul with version 2.5 of the kernel earlier versions of the linux kernel ran a variation of the traditional unix scheduling algorithm  which does not provide adequate support for smp systems and does not scale well as the number of tasks on the system grows the overhaul of the linux scheduler with version 2.5 of the kernel provides a scheduling algorithm that runs in constant time-known as 0  1  -regardless of the number of tasks on the system the new scheduler also provides increased support for smp  including processor affin.ity and load balancing  as well as maintaining fairness and support for interactive tasks  816 chapter 21 numeric priority 0 99 100 140 relative priority highest lowest time quantum 200 ms 10 ms figure 21.2 the relationship between priorities and time-slice length  the linux scheduler is a preemptive  priority-based algorithm with two separate priority ranges  a real-time range from 0 to 99 and a nice value ranging from 100 to 140 these two ranges map into a global priority scheme whereby numerically lower values indicate higher priorities  unlike schedulers for many other systems  the linux scheduler assigns higher-priority tasks longer time quanta and lower-priority tasks shorter time quanta because of the unique nature of the scheduler  this is appropriate for lim.ix  as we shall soon see the relationship between priorities and time-slice length is shown in figure 21.2  a rum able task is considered eligible for execution on the cpu so long as it has time remaining in its time slice when a task has exhausted its time slice  it is considered expired and is not eligible for execution again until all other tasks have also exhausted their time quanta the kernel maintains a list of all runnable tasks in a runqueue data structure because of its support for smp  each processor maintains its own runqueue and schedules itself independently  each runqueue contains two priority arrays-active and expired the active array contains all tasks with time remaining in their time slices  and the expired array contains all expired tasks each of these priority arrays includes a list of tasks indexed according to priority  figure 21.3   the scheduler chooses the task with the highest priority from the active array for execution on the cpu  on multiprocessor machines  this means that each processor is scheduling the highest-priority task from its own runqueue structure when all tasks have exhausted their time slices  that is  the active array is empty   the two priority arrays are exchanged as the expired array becomes the active array and vice-versa  tasks are assigned dynamic priorities that are based on the nice value plus or minus a value up to the value 5 w1 ether a value is added to or subtracted from a task 's nice value depends on the interactivity of the task a task 's interactivity is determined by how long it has been sleeping while waiting for i/0 tasks that are more interactive typically have longer sleep times and therefore are more likely to have adjustments closer to -5  as the scheduler favors such interactive tasks conversely  tasks with shorter sleep times are often cpu-bound and thus will have their priorities lowered  a task 's dynamic priority is recalculated when the task has exhausted its time quantum and is to be moved to the expired array thus  when the two active array priority  oj  1   140  task lists o-o o-o-o 0 21.5 expired array priority  0   1   140  task lists o-o-o 0 o-o figure 21.3 list of tasks indexed according to priority  817 arrays are exchanged  all tasks in the new active array have been assigned new priorities and corresponding time slices  linux ' s real-time scheduling is simpler still linux implements the two realtime scheduling classes required by posix.lb  first-come  first-served  fcfs  and round-robin  sections 5.3.1 and 5.3.4  respectively   in both cases  each process has a priority irt addition to its scheduling class processes with different priorities can compete with one another to some extent in time-sharing scheduling ; in real-time scheduling  however  the scheduler always runs the process with the highest priority among processes of equal priority  it runs the process that has been waiting longest the only difference between fcfs and round-robin scheduling is that fcfs processes continue to nm until they either exit or block  whereas a round-robill process will be preempted after a while and will be moved to the end of the scheduling queue  so round-robin processes of equal priority will automatically time-share among themselves  unlike routine time-sharing tasks  real-time tasks are assigned static priorities  linux 's real-time scheduling is soft-rather than hard-real time the scheduler offers strict guarantees about the relative priorities of real-time processes  but the kernel does not offer any guarantees about how quickly a reahim.e process will be scheduled once that process becomes runnable  21.5.2 kernel synchronization the way the kernel schedules its own operations is fundamentally different from the way it schedules processes a request for kernel-mode execution can occur in two ways a running program may request an operating-system service  either explicitly via a system call or implicitly-for example  when a page fault occurs alternatively  a device controller may deliver a hardware interrupt that causes the cpu to start executing a kernel-defined handler for that interrupt  the problem posed to the kernel is that all these tasks may try to access the sanl.e internal data structures if one kernel task is in the middle of accessing some data structure when an interrupt service routine executes  then that service routine can not access or modify the same data without risking data corruption this fact relates to the idea of critical sections-portions of code that access shared data and that must not be allowed to execute concurrently  as a result  kernel synchronization involves much more than just process scheduling a framework is required that allows kernel tasks to run without violating the integrity of shared data  818 chapter 21 prior to version 2.6  linux was a nonpreernptive kernet meaning that a process running in kernel mode could not be preempted -even if a higherpriority process became available to run with version 2.6  the linux kernel became fully preemptive ; so a task can now be preempted when it is running in the kernel  the limix kernel provides spinlocks and semaphores  as well as readerwriter versions of these two locks  for locking in the kernel on smp machines  the fundamental locking mechanism is a spinlock ; the kernel is designed so that the spinlock is held only for short durations on single-processor machines  spinlocks are inappropriate for use and are replaced by enabling and disabling kernel preemption that is  on single-processor machines  rather than holding a spinlock  the task disables kernel preemption when the task would otherwise release the spinlock  it enables kernel preemption this pattern is summarized below  linux uses an interesting approach to disable and enable kernel preemption  it provides two simple system calls-preempldisable   and preemplenable   -for disabling and enabling kernel preemption however  in addition  the kernel is not preemptible if a kernel-mode task is holding a lock  to enforce this rule  each task in the system has a thread-info structure that includes the field preempt_count  which is a counter indicating the number of locks being held by the task the counter is incremented when a lock is acquired and decremented when a lock is released if the value of preemplcount for the task currently running is greater than zero  it is not safe to preempt the kernet as this task currently holds a lock if the count is zero  the kernel can safely be interrupted  assuming there are no outstanding calls to preempt_disable    spinlocks-along with enabling and disabling of kernel preemption -are used in the kernel only when the lock is held for short durations when a lock must be held for longer periods  semaphores are used  the second protection technique used by linux applies to critical sections that occur in interrupt service routines the basic tool is the processor 's interrupt-control hardware by disabling interrupts  or using spinlocks  during a critical section  the kernel guarantees that it can proceed without the risk of concurrent access to shared data structures  however  there is a penalty for disabling interrupts on most hardware architectures  interrupt enable and disable instructions are expensive furthermore  as long as interrupts remain disabled  all i/0 is suspended  and any device waiting for servicing will have to wait until interrupts are reenabled ; so performance degrades the linux kernel uses a synchronization architecture that allows long critical sections to run for their entire duration without having interrupts disabled this ability is especially useful in the networking code an interrupt in a network device driver can signal the arrival of an entire network packet  which may result in a great deal of code being executed to disassemble  route  and forward that packet within the interrupt service routine  21.5 819 kernel-system service routines  preemptible  user-mode programs  preemptible  figure 21.4 interrupt protection levels  linux implements this architecture by separating interrupt service routines into two sections  the top half and the bottom half the is a normal interrupt service routine that runs with recursive interrupts disabled ; interrupts of a higher priority may interrupt the routine  but interrupts of the same or lower priority are disabled the of a service routine is run  with all interrupts enabled  by a miniature scheduler that ensures that bottom halves never interrupt themselves the bottom-half scheduler is invoked automatically whenever an interrupt service routine exits  this separation means that the kernel can complete any complex processing that has to be done in response to an interrupt without worrying about being interrupted itself if another interrupt occurs while a bottom half is executing  then that interrupt can request that the same bottom half execute  but the execution will be deferred until the one currently running completes each execution of the bottom half can be interrupted by a top half but can never be interrupted by a similar bottom half  the top-half/bottom-half architecture is completed by a mechanism for disabling selected bottom halves while executing normal  foreground kernel code the kernel can code critical sections easily using this system interrupt handlers can code their critical sections as bottom halves ; and when the foreground kernel wants to enter a critical section  it can disable any relevant bottom halves to prevent any other critical sections from interrupting it at tl1e end of the critical section  the kernel can reenable the bottom halves and run any bottom-half tasks that have been queued by top-half interrupt service routines during the critical section  figure 21.4 summarizes the various levels of interrupt protection within the kernel each level may be interrupted by code running at a higher level but will never be interrupted by code running at the same or a lower level ; except for user-mode code  user processes can always be preempted by another process when a time-sharing scheduling interrupt occurs  21.5.3 symmetric multiprocessing the linux 2.0 kernel was the first stable linux kernel to support hardware  allowing separate processes to execute in parallel on separate processors originally  the implementation of smp imposed the restriction that only one processor at a time could be executing kernel-mode code  820 chapter 21 21.6 in version 2.2 of the kernel  a single kernel spinlock  sometimes termed bkl for big kernel lock  was created to allow multiple processes  running on different processors  to be active in the kernel concurrently however  the bkl provided a very coarse level of locking granularity later releases of the kernel made the smp implementation more scalable by splitting this single kernel spinlock into multiple locks  each of which protects only a small subset of the kernel 's data structures such spinlocks are described in section 21.5.2  the 2.6 kernel provided additional smp enhancements  including processor affinity and load-balancing algorithms  memory management under linux has two components the first deals with allocating and freeing physical memory-pages  groups of pages  and small blocks of memory the second handles virtual memory  which is memory mapped into the address space of running processes in this section  we describe these two components and then examine the mechanisms by which the loadable components of a new program are brought il lto a process 's virtual memory in response to an exec   system call  21.6.1 management of physical memory due to specific hardware characteristics  linux separates physical memory into three different zones  or regions  zone_dma zone_normal zone_highmem these zones are architecture specific for example  on the intel 80x86 architecture  certain isa  industry standard architecture  devices can only access the lower 16 mb of physical memory using dma on these systems  the first 16 mb of physical memory comprise zonldma zone_normal identifies physical memory that is mapped to the cpu 's address space this zone is used for most routine memory requests for architectures that do not limit what dma can access  zonldma is not present  and zone_normal is used  finally  zone_highmem  for high memory  refers to physical memory that is not mapped into the kernel address space for example  on the 32-bit intel architecture  where 232 provides a 4-gb address space   the kernel is mapped into the first 896 mb of the address space ; the remaining memory is referred to as high memory and is allocated from zone_highmem the relationship of zones and physical addresses on the intel80x86 architecture is shown in figure 21.5 the kernel maintains a list of free pages for each zone when a request for physical memory arrives  the kernel satisfies the request using the appropriate zone  the priinary physical-memory manager in the lil lux kernel is the page allocator each zone has its own allocator  which is responsible for allocating and freeing all physical pages for the zone and is capable of allocating ranges 21.6 821 figure 21.5 relationship of zones and physical addresses on the lntel80x86  of physically contiguous pages on request the allocator uses a buddy system  section 9.8.1  to keep track of available physical pages in this scheme  adjacent units of allocatable memory are paired together  hence its name   each allocatable memory region has an adjacent partner  or buddy   whenever two allocated partner regions are freed up  they are combined to form a larger region-a buddy heap that larger region also has a partner  with which it can combine to form a still larger free region conversely  if a small memory request can not be satisfied by allocation of an existing small free region  then a larger free region will be subdivided into two partners to satisfy the request separate linked lists are used to record the free memory regions of each allowable size ; under linux  the smallest size allocatable under this mechanism is a single physical page figure 21.6 shows an example of buddy-heap allocation a 4-kb region is being allocated  but the smallest available region is 16 kb the region is broken up recursively until a piece of the desired size is available  ultim.ately  all memory allocations in the linux kernel are ncade either statically  by drivers that reserve a contiguous area of memory during system boot time  or dynamically  by the page allocator however  kernel functions do not have to use the basic allocator to reserve memory several specialized memory-management subsystems use the underlying page allocator to manage their own pools of memory the most important are the virtual memory system  described in section 21.6.2 ; the kmalloc   variable-length allocator ; the slab allocator  used for allocating memory for kernel data structures ; and the page cache  used for caching pages belonging to files  many components of the linux operating system need to allocate entire pages on request  but often smaller blocks of memory are required the kernel 8kb 8kb 16kb 4kb figure 21.6 splitting of memory in the buddy system  822 chapter 21 provides an additional allocator for arbitrary-sized requests  where the size of a request is not known in advance and may be only a few bytes analogous to the c language 's malloc   function  this kmalloc   service allocates entire pages on demand but then splits them into smaller pieces the kernel maintains lists of pages in use by the kmalloc   service allocating memory involves determining the appropriate list and either taking the first free piece available on the list or allocating a new page and splitting it up memory regions clain'led by the kmalloc   system are allocated permanently until they are freed explicitly ; the kmalloc   system can not reallocate or reclaim these regions in response to memory shortages  another strategy adopted by linux for allocating kernel memory is known as slab allocation a slab is used for allocating memory for kernel data structures and is made up of one or more physically contiguous pages a consists of one or more slabs there is a single cache for each unique kernel data structure -a cache for the data structure representing process descriptors  a cache for file objects  a cache for semaphores  and so forth each cache is populated with that are instantiations of the kernel data structure the cache represents for example  the cache representing semaphores stores instances of semaphore objects  and the cache representing process descriptors stores instances of process descriptor objects the relationship among slabs  caches  and objects is shown in figure 21.7 the figure shows two kernel objects 3 kb in size and three objects 7 kb in size these objects are stored in the respective caches for 3-kb and 7-kb objects  the slab-allocation algorithm uses caches to store kernel objects when a cache is created  a number of objects are allocated to the cache the number of objects in the cache depends on the size of the associated slab for example  a 12-kb slab  made up of three contiguous 4-kb pages  could store six 2-kb objects initially  all the objects in the cache are marked as free when a new object for a kernel data structure is needed  the allocator can assign any free kernel objects 3-kb i objects l 7-kb objects figure 21.7 slab allocator in linux  physically contiguous pages 21.6 823 object from the cache to satisfy the request the object assigned from the cache is marked as used  let 's consider a scenario in which the kernel requests memory from the slab allocator for an object representing a process descriptor in linux systems  a process descriptor is of the type struct task_struct  which requires approximately 1.7 kb of memory when the linux kernel creates a new task  it requests the necessary memory for the struct task_struct object from its cache the cache will fulfill the request using a struct task_struct object that has already been allocated in a slab and is marked as free  in linux  a slab may be in one of three possible states  1 full all objects in the slab are marked as used  empty all objects in the slab are marked as free  partial the slab consists of both used and free objects  the slab allocator first attempts to satisfy the request with a free object in a partial slab if none exist  a free object is assigned from an empty slab if no empty slabs are available  a new slab is allocated from contiguous physical pages and assigned to a cache ; memory for the object is allocated from this slab  two other main subsystems in linux do their own management of physical pages  the page cache and the virtual memory system these systems are closely related to one another the page cache is the kernel 's main cache for block devices  section 21.8.1  and memory-mapped files and is the main mechanism through which i/0 to these devices is performed both the native linux diskbased file systems and the nfs networked file system use the page cache  the page cache stores entire pages of file contents and is not limited to block devices ; it can also cache networked data the virtual memory system manages the contents of each process 's virtual address space these two systems interact closely with one another because reading a page of data into the page cache requires mapping pages in the page cache using the virtual memory system in the following section  we look at the virtual memory system in greater detail  21.6.2 virtual memory the limn virtual memory system is responsible for maintaining the address space visible to each process it creates pages of virtual memory on demand and manages loading those pages from disk and swapping them back out to disk as required under linux  the virtual memory manager maintains two separate views of a process 's address space  as a set of separate regions and as a set of pages  the first view of an address space is the logical view  describing instructions that the virtual memory system has received concerning the layout of the address space in this view  the address space consists of a set of nonoverlapping regions  each region representing a continuous  page-aligned subset of the address space each region is described internally by a single vm_area_struct structure that defines the properties of the region  including the process 's read  write  and execute permissions in the region as well as information about any files associated with the region the regions for each 824 chapter 21 address space are linked into a balanced binary tree to allow fast lookllp of the region corresponding to any virtual address  the kernel also n laintains a second  physical view of each address space  this view is stored in the hardware page tables for the process the pagetable entries identify the exact current location of each page of virtual mernory  whether it is on disk or in physical memory the physical view is managed by a set of routines  which are invoked from the kernel 's software-interrupt handlers whenever a process tries to access a page that is not currently present in the page tables each vm_area_struct in the address-space description contains a field that points to a table of functions that implement the key page-management functions for any given virtual memory region all requests to read or write an unavailable page are eventually dispatched to the appropriate handler in the function table for the vm_area_struct  so that the central memorymanagement routines do not have to know the details of managing each possible type of memory region  21.6.2.1 virtual memory regions linux implements several types of virtual memory regions one property that characterizes virtual memory is the backing store for the region  which describes where the pages for the region come from most memory regions are backed either by a file or by nothing a region backed by nothing is the simplest type of virtual memory region such a region represents demand-zero memory  when a process tries to read a page in such a region  it is simply given back a page of memory filled with zeros  a region backed by a file acts as a viewport onto a section of that file  whenever the process tries to access a page within that region  the page table is filled with the address of a page within the kernel 's page cache corresponding to the appropriate offset in the file the same page of physical memory is used by both the page cache and the process 's page tables  so any changes made to the file by the file system are immediately visible to any processes that have mapped that file into their address space any number of processes can map the same region of the same file  and they will all end up using the same page of physical memory for the purpose  a virtual memory region is also defined by its reaction to writes the mapping of a region into the process 's address space can be either private or shared if a process writes to a privately mapped region  then the pager detects that a copy-on-write is necessary to keep the changes local to the process in contrast  writes to a shared region result in updating of the object mapped into that region  so that the change will be visible immediately to any other process that is mapping that object  21.6.2.2 lifetime of a virtual address space the kernel will create a new virtual address space in two situations  when a process runs a new program with the exec   system call and when a new process is created by the fork   system call the first case is easy when a new program is executed  the process is given a new  completely empty virtual address space it is up to the routines for loading the program to populate the address space with virtual memory regions  21.6 825 the second case  creating a new process with fork    involves creating a concplete copy of the existing process 's virtual address space the kernel copies the parent process 's vm_area_struct descriptors  then creates a new set of page tables for the child the parent 's page tables are copied directly into the child 's  and the reference count of each page covered is incremented ; thus  after the fork  the parent and child share the same physical pages of memory in their address spaces  a special case occurs when the copying operation reaches a virtual memory region that is mapped privately any pages to which the parent process has written within such a region are private  and subsequent changes to these pages by either the parent or the child must not update the page in the other process 's address space when the page-table entries for such regions are copied  they are set to be read only and are marked for copy-on-write as long as neither process modifies these pages  the two processes share the same page of physical memory however  if either process tries to modify a copy-on-write page  the reference count on the page is checked if the page is still shared  then the process copies the page 's contents to a brand-new page of physical memory and uses its copy instead this mechanism ensures that private data pages are shared between processes whenever possible ; copies are made only when absolutely necessary  21.6.2.3 swapping and paging an important task for a virtual memory system is to relocate pages of memory from physical memory out to disk when that inemory is needed early unix systems performed this relocation by swapping out the contents of entire processes at once  but modern versions of unix rely more on paging-the movement of individual pages of virtual memory between physical memory and disk linux does not implement whole-process swapping ; it uses the newer paging mechanism exclusively  the paging system can be divided into two sections first  the decides which to write out to disk and when to write them  second  the carries out the transfer and pages data back into physical memory when they are needed again  linux 's pageuut policy uses a modified version of the standard clock  or second-chance  algorithm described in section 9.4.5.2 under linux  a multiplepass clock is used  and every page has an age that is adjusted on each pass of the clock the age is more precisely a measure of the page 's youthfulness  or how much activity the page has seen recently frequently accessed pages will attain a higher age value  but the age of infrequently accessed pages will drop toward zero with each pass this age valuing allows the pager to select pages to page out based on a least frequently used  lfu  policy  the paging mechanism supports paging both to dedicated swap devices and partitions and to normal files  although swapping to a file is significantly slower due to the extra overhead incurred by the file system blocks are allocated from the swap devices according to a bitmap of used blocks  which is maintained in physical memory at all times the allocator uses a next-fit algorithm to try to write out pages to continuous runs of disk blocks for improved performance the allocator records the fact that a page has been paged out to disk by using a feature of the page tables on modern processors  826 chapter 21 the page-table entry 's page-not-present bit is set  allowing the rest of the pagetable entry to be filled with an index identifying where the page has been written  21.6.2.4 kernel virtual memory linux reserves for its own internal use a constant  architecture-dependent region of the virtual address space of every process the page-table entries that map to these kernel pages are marked as protected  so that the pages are not visible or modifiable when the processor is running in user mode this kernel virtual memory area contains two regions the first is a static area that contains page-table references to every available physical page of memory in the system  so that a simple translation from physical to virtual addresses occurs when kernel code is run the core of the kernel  along with all pages allocated by the normal page allocator  resides in this region  the remainder of the kernel 's reserved section of address space is not reserved for any specific purpose page-table entries in this address range can be modified by the kernel to point to any other areas of memory the kernel provides a pair of facilities that allow processes to use this virtual memory the vmalloc   function allocates an arbitrary number of physical pages of memory that may not be physically contiguous into a single region of virtually contiguous kernel memory the vremap   function maps a sequence of virtual addresses to point to an area of memory used by a device driver for memory-mapped i/0  21.6.3 execution and loading of user programs the linux kernel 's execution of user programs is triggered by a call to the exec   system call this exec   call commands the kernel to run a new program within the current process  completely overwriting the current execution context with the initial context of the new program the first job of this system service is to verify that the calling process has permission rights to the file being executed once that matter has been checked  the kernel invokes a loader routine to start running the program the loader does not necessarily load the contents of the program file into physical memory  but it does at least set up the mapping of the program into virtual memory  there is no single routine in linux for loadil1.g a new program instead  linux maintains a table of possible loader functions  and it gives each such function the opportunity to try loading the given file when an exec   system call is made the initial reason for this loader table was that  between the releases of the 1.0 and 1.2 kernels  the standard format for linux 's binary files was changed older linux kernels understood the a out format for binary files-a relatively simple format common on older unix systems newer linux systems use the more modern elf format  now supported by most current unix implementations elf has a number of advantages over a out  including flexibility and extensibility new sections can be added to an elf binary  for example  to add extra debugging information  without causing the loader routines to become confused by allowing registration of multiple loader routines  linux can easily support the elf and a out binary formats in a single rmming system  21.6 827 in sections 21.6.3.1 and 21.6.3.2  we concentrate exclusively on the loading and running of elf-format binaries the procedure for loading a out binaries is simpler but is similar in operation  21.6.3.1 mapping of programs into memory under linux  the binary loader does not load a binary file into physical memory  rather  the pages of the binary file are mapped into regions of virtual memory  only when the program tries to access a given page will a page fault result in the loading of that page into physical memory using demand paging  it is the responsibility of the kernel 's binary loader to set up the initial memory mapping an elf-format binary file consists of a header followed by several page-aligned sections the elf loader works by reading the header and mapping the sections of the file into separate regions of virtual memory  figure 21.8 shows the typical layout of memory regions set up by the elf loader in a reserved region at one end of the address space sits the kernet in its own privileged region of virtual memory inaccessible to normal user-mode programs the rest of virtual memory is available to applications  which can use the kernel 's memory-mapping functions to create regions that map a portion of a file or that are available for application data  the loader 's job is to set up the initial memory mapping to allow the execution of the program to start the regions that need to be initialized il1.clude the stack and the program 's text and data regions  the stack is created at the top of the user-mode virtual memory ; it grows downward toward lower-numbered addresses it includes copies of the arguments and environment variables given to the program il1 the exec 0 system call the other regions are created near the bottom end of virtual memory the sections of the binary file that contail1 program text or read-only 