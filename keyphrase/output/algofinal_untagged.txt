design and analysis of algorithms prof abhiram ranade computer science engineering department indian institute of technology  bombay lecture  1 overview of the course welcome to the course on design and analysis of the algorithms today ? s lecture is going to be an overview of the course and i want to introduce you to the main problems in the course and also give also convey a spirit of the course let us start with the fundamental question given a certain problem how do you solve it on a computer ? for many problems it is relatively easy to design algorithms that will somehow solve them however  quite some cleverness is needed and designing algorithms that are also fast that is algorithms which give answers very quickly this will be the major challenge in the course  refer slide time  01  34  our main course goal as said is to design algorithms which are fast design of fast algorithms as you may realize designing anything be it computers  be it cars  be it clothes is an art so  in some sense you have to be creative and it can not be taught  but in some other sense there are also some very well defined design techniques which have evolved for this purpose and the goal of this course is to study these techniques we will also have lots of exercises in which you can apply these ideas and our hope is at the end of the course you will be able to solve algorithm design problems that may that you may encounter later on in your life there are some prerequisites for this course and let me just state them  refer slide time  02  36  the prerequisites are that you should be familiar with some programming you should have done some amount of programming in some common language say c or scheme or basic you should also have done a course on data structures and you should have some discrete mathematics background as you may see this is not an elementary course and this background is going to be necessary for us  refer slide time  03  27  our approach is this course is going to be analytical in the sense that will build the model of a computer analytical will build a model of a computer that is mathematical build a mathematical model and on this mathematical model we will be designing algorithms so  we design algorithms and we study their properties of algorithms on this model the whole point is to reason about algorithms so  this is very important if we want to mathematically prove properties and we want to prove properties the emphasis on this course is proof prove properties about speed say prove facts about time taken all this so there is going to be a number of things that we have to do  and of course there is a whole course for this  refer slide time  05  04  so  i want to give an overview of the course in the next few lectures we will develop the basic framework what i mean by this is that we will define the mathematical model we will say what fast means when i say fast algorithm what do i mean ? this is what we will say then we will embark on a fairly long stretch which involves techniques for designing fast algorithms while doing this we will also be surveying many problems so  we will look at problems from optimization graph theory some problems from geometry also some others it will turn out that there are some problems which do not really quite respond to our algorithm design techniques of course  we will see many problems for which we can design really god algorithms our techniques work beautifully  but there are also some problems where our techniques do not work so well and for these problems a fairly intricate theory has developed over the last over the last several year ? s 10 20 30 years and this theory is the  so called theory of n p completeness so  we will be studying this theory as well  refer slide time  06  59  let me now  come to the topic for today the main topic for today so  today i do not want to be too formal  but i nevertheless want to convey the spirit of the course to you so  here is what we are going to do ? we are going to take a fairly simple problem which everybody will have no should have no trouble in understanding and let me state that problem right away the problem is given 2 numbers m and n find their greatest common divisor so  everybody understands this problem you are going to see 2 algorithms for this one is a very simple algorithm which you probably have learnt in school it is probably the algorithm that you were taught in first standard or fourth standard or something like that and which probably most of us used to solve this to find the greatest common divisor where we need to such as when simplifying fractions then we will study another algorithm and this is one of the earliest algorithms which was ever invented and this was invented by the mathematician euclid who you might know from plane geometry yes euclid did invent this algorithm even though there were no computers in his time so  we will study euclid ? s algorithm and from this we will get a sense of what a fast algorithm is you will see that euclid ? s algorithm even though we have not defined what fast is you will intuitively understand that euclid ? s algorithm must be much faster will be certainly much faster than the simple school level algorithm that we are talking about and euclid ? s algorithm is also clever and that makes it more exciting and that is also again the spirit of this course  refer slide time  09  31  so  let me begin with the simple school level algorithm for factoring so  everybody knows this  but let me state it anyway so  basically there are 2 or 3 steps  let me write down the input it is 2 integers m and n what we need is the greatest common divisor which is also the largest integer that divides both and when i say divides i mean without leaving a reminder so  here is what the algorithm will look like so  step 1 we are going to factorize m what does it mean ? it means finding primes that we call them m 1 m 2 till m k such that m is m 1 times m 2 times all of this the next step is to factorize n what does that mean ? again find break n into its factors so  write n as n 1 time n 2 times till some n j note that the same factor may appear several times and of course  we are here to write it separately the next step is to identify common factors and then multiply and return the result we are going to take an example of this and in a minute and we will see we will see what exactly these steps do  refer slide time  12  19  let me now  state euclid ? s algorithm so  i am going to write this as a procedure so  euclid is a procedure which will take 2 arguments m and n and i am going to invent a pseudo language as i go along and that is again going to be in the spirit of the course we are not going to be too picky about how we write down algorithms ? so  long as what i mean is clear to you perfect everything is fine so  you could express your algorithms in the most in the most suitable nicest syntax so  that it is easy for you to get your meaning across we will come to all this in somewhat more detail in the next lecture or  so what does euclid do ? so  the first is a check so  we are going to check whether m divides n while m does not divide n we are going to do the following so  first we are going to calculate the remainder so  we are going to calculate r of n mod m then we are going to set n equals m and then we are going to set m to be r al these steps are going to be done inside the loop so  that is the end of the loop over here we will prove soon that eventually this loop will have to terminate and after it terminates all that euclid ? s algorithm does is to return the value that m has at that point and that is it that is all there is to the algorithm it is not clear when you first look at this algorithm that in fact  this algorithm works it seems to be doing some divisions and taking some remainders  but it is not actually factorizing any of the numbers that you want who is greatest common divisors you want so  let us now  take some examples and see whether or not this algorithm works  refer slide time  14  19  so  let us first take let us first take a small example where we have m equals say 36 n equals 48 how will you do it using our simple algorithm well we factor m is equal to a product of prime factors so  the idea is going to be that we are going to test numbers one after another so  we start with say 2 and yes 2 is a factor so  we have to write it as 2.so  then we divides 36 by 2 we get 18 so in fact  now  you have to factorize 18 well 2 is again a factor so  then what remains is 9 now  2 is not a factor so  you go out to the next prime number which is 3 which is a factor and then only the factor 3 remains so  we have factorized m what about n ? again we do the same thing we start with 2 yes 2 is a factor so  that gives us 24 again 2 is a factor that gives us 12 again 2 is a factor so  that leaves us 6 again 2 is a factor and then that leaves us 3 so  then we want to identify what factors are common well these 2 are common and then 1 3 is also common so  common the common factors are 2 2 and 3 and  so g c d is equal to 2 into 2 into 3 or 12 nothing terribly difficult in this this is all of course  school level stuff so  let us contrast this with what euclid ? s algorithm does so  let me bring euclid ? s algorithm back so  we are going to start with m equals 36  n equals 48  refer slide time  17  11  so  let me write it down as we go along so  doe ? s m we have to check whether m does not divide n so  36 divide n no  so when  so therefore we enter the loop so  now we calculate r equals n mod m so  in other words r is equal to 48 mod 36 that is equal to 12 then we calculate n is equal to m let me write down over here that well we know that m is equal to 36 and n is 48 so  n takes the value that m originally had so  n is now  going to become 36 and m takes the value which r has  so r is going to become 12 so  at this point the iteration has ended so  at this point our iteration has ended and we have left with m equals 36 and n equals 36 and m equals 12 but now  we have to go and check out the loop again  because well that is what the while loop says so  this time n equals 36 and m equals 12 and again we had to check whether m does or does not divide 12 m r does not divide n  refer slide time  18  43  so  does 12 divides 36 this time the answer is yes and therefore  we are going to exit the loop and in fact  at the end of it we are going to return the current value of m which is equal to 12  so we will be returning 12 so  let us now  compare these 2 algorithms so  here you can see roughly what the work done by the 2 algorithms is the simple algorithm had to do factorizations then it had to collect common factors and it had to do it had to multiply the factors together and return the answer euclid ? s algorithm on the other hand did 1 division so  first time it divided 36 it divided it checked if 36 divides 48 which is the value of n that we had and when it found that answer was false the division was not possible then it took the remainder then it simply exchanged the numbers basically and then again it did 1 more division so  i can summarize the work over here by saying that this did roughly 2 divisions how much work does this do ? well it calculated these  so many factors so  it calculated 4 factors  so it had to at least do 3 divisions to get each factor here it calculated 5 factors so  it had to at least do 4 divisions so  it at least did 9 divisions probably it did more you can see in a quick example immediately where it will have to do many more than these as you can see in this school level very simple factoring algorithm we use 9 divisions whereas  in this somewhat sophisticated algorithm we use really 2 divisions so  clearly you have done less work and although i have not proved yet that euclid ? s algorithm in fact  works you can see that it is returning the correct answer so  but we will show later on that in fact  euclid ? s algorithm does do does work correctly  refer slide time  21  17  so  let me take one more example m equals 434  n equals 966 so  these are somewhat large numbers and let us see what will happen over here so  suppose you want to factorize these numbers first of all it will take some work  but let us see what the answer is going to be so  m can be written as 2 times 7 times 31  n can be factorized as 2 times 7 times 139 what is the answer ? the common factors are 2 and 7 and the g c d therefore  is 14  let us now  come back to euclid ? s algorithm  refer slide time  22  31  so  let me write down again m is equal to 434 and n is equal to 966 the first step is to compute the reminder of n mod m so  then r is going to be according to this step r is going to be 966 mod 434 well 966 is equal to 2 times 434 which is 868 plus 98 under 4 i can write down r as equal to 98 after this we just had to exchange values basically so  now  n is going to take the old value of m so  n is going to be 434 and m is going to take the value of r  so which is 98 so  at the end of one iteration of euclid ? s algorithm we have new values of m and n and these are the new values so  we started off with the values 434 and 966 after 1 iteration we have the values 434 98 and 434 in that order m and n so  now  we just have to repeat the same thing with these new values  so this is the end of the first iteration and in the second iteration we are again going to calculate r equals n mod m  refer slide time  24  26  so  let us do that so  if you calculate r equals n mod m we are going to get the new value of r to be equal to 434 mod 98 so  you have to do the division over here and which is 434 can be written as 98 times 4  so this is 392  so plus we are going to get 42 so  r is going to be equal to 42  but of course  when we began this iteration we had to check whether m does or does not divide n or whether 98 does or does not divide 434  but clearly it is leaving the remainder and therefore  we will enter this iteration after that we are going to set n equal to m  so m the value of n is 98 and m will be equal to r which is 42 so  at the end of the second iteration these are the values that we have so  n is equal to 98 and m is equal to 42 so  we will have to go into the third iteration for the beginning of the third iteration  let me just write down the values  refer slide time  25  56  so  we have m equals 42 n equals 98 again we are going to check does m divide n it does not and  so we will enter the loop when we enter the loop we have to find out r equals n mod m so  that is equal to 98 mod 42 42 times 2 is 84  so 98 mod 42 is 14 then there is just a matter of setting n and m correctly so  n will now  equal the old value of m so  this is 42 and m will equal the value of r we just calculated so  it is going to be 14 so  these are the values at the end of the third iteration so  now  again we are going to come back and execute this loop and again we are going to check does m divide n or not and this time we will see that 14  in fact does divide 42 and at this point the loop will be exited  refer slide time  27  20  and at the end we are going to return the result as m the value of m right now is 14 and therefore  14 will be returned so  what has happened ? we took 3 iterations so  summary of this all this is we took 3 iterations what happened in each iteration well we did 1 division per iteration and before quitting from the loop we have to do one more iteration one more division and therefore  that took 4 divisions essentially so  euclid ? s algorithm for this somewhat complicated problem m equals 434 n equals 966 where we are asking to find the greatest common divisor of m equals 434 and n equals 966 took 4 divisions let us now  check what happens with the simple algorithm  refer slide time  28  36  so  here where the factors that we found so  how many divisions did it take ? at first glance you might see you might think that it is only going to take 2 divisions to calculate m and 2 more divisions to calculate n  so which is 4 divisions  but that is not quite correct to check that there is no factor between 2 and 7 we would require checking for 2 3 for 3 five as well now  after dividing by 7 we had to check whether 139 is a prime so  that would also involve checking all the numbers until 39 so  that would require substantially many divisions so  here again we require many divisions many more than euclid ? s algorithm in fact  you will see that if we take bigger and bigger numbers factoring them becomes much harder you will have to do a lot more work whereas  in euclid ? s case we will just do divisions and we are we will in fact  show now that as we do the divisions then numbers will become smaller and smaller and the algorithm will terminate so  the next topic that we are going to get into is i want to argue that euclid ? s algorithm actually works  refer slide time  30  10  i am not going to give a fairly very detail proof i just want to indicate the main idea the main idea is really a fact about divisibility and remainder and things like that  so let me indicate that so  this idea says that if m divides n then g c d of m and n must ; obviously  be equal to m  because clearly m will be the largest number which divides both m and n if not then we can write g c d of m and n is the same as g c d of n it is the same as the g c d of n mod m and m how do you prove this ? i will leave it as an exercise  but let me just mention the main idea the main idea is suppose g is the g c d then we can write m as a times g and n as b times g where a and b are relatively prime having written them in this manner you should be able to just substitute into what we have in the fact and then you should be able to get the answer actually the fact itself is very similar to what we had in euclid ? s algorithm in the fact that we have written down so  let me just show you the iteration the loop part of euclid ? s algorithm so  the fact says that if you want to calculate the g c d of m and n then you might as well calculate d g c d of n mod m and m what does our loop do ? it wants to calculate we want to calculate d g c d of m and n it first checks what the remainder is and then it calculates the fact is essentially n mod m so  it is essentially this term over here and we need and then it sets m to be equal to r so  the first argument is set to r and the second argument is set to the old value of m  but this is precisely what the fact says the fact says that if you want to calculate g c d of m and n instead calculate g c d of n mod m and m in fact  once we have once we are given this fact the proof of euclid ? s algorithm the correctness of euclid ? s algorithm is at least partially done because what we have accomplished is that we are sure that as we go through iterations we will never be we will always maintain integer ? s m and n and who ? s g c d will be the g c d of the original values of m and n so  this is the invariant that we are going to maintain so  m the specific value of m and n might change  but their g c d is never going to change so  let me write this down  refer slide time  34  45  so  as the loop executes m and n might change  but their g c d does not and hence what we have is that we will preserve the g c d and eventually when we exit the loop will exit with the same g c d we just established that as the loop executes m and n might change  but curiously enough their g c d continues to remain the same as a result if we ever get out of the loop then that will be  because m divides n  but notice that even at this point the g c d will still be the same as the old g c d the g c d of the old m and n but if m divides n then the g c d will simply be n and that is what you will return and therefore  in fact  we have established that if the loop in fact  terminates then we will be returning the correct value now  you want to argue why the loop should terminate ? why we need to exit ? why we will always exit from the loop ? this is actually fairly straight forward let us examine euclid ? s algorithm again and see what happens in each iteration so  long as the loop is executing so  initially the values of m and n might be taking some values what are the values of m and n after one iteration ? well  the first step is calculating r  but notice that immediately afterwards we are setting n to have the value m  refer slide time  36  42  so  if the original values are m and n then at the end of 1 iteration n will have the value m and what will the value of m be m will have the value r so  it will be n mod m so  what has happened is that the values m and n have now  changed to n mod m and m  but n mod m anything mod m is actually going to be smaller than m itself so  notice that the first argument is always continuously is going to decrease in every iteration of the loop the value of m is always going to decrease how long can it keep on decreasing ? well  it has to remain positive it can not even become zero and it might decrease it will have to decrease at least by one in each iteration as a result of this we can conclude that the loop has to exit at some point and we know that if the loop exits then the correct value is returned so  that proves that euclid ? s algorithm is correct our next step is to argue that euclid ? s algorithm in fact  runs reasonably quick so  the basic idea we want to prove is going to be something like this well before that we are going to assume we are going to we are going to prove something about cost euclid ? s algorithm and in doing this we are going to assume that the first argument that we sent to euclid is always smaller than the second  refer slide time  38  21  so  this means that we will always call g c d 36 48 and not g c d of 48 36 this is only for the purpose of analysis actually the algorithm as we wrote will work fine if make this call as well if we make this call you will see that internally we will start with m equals 48 n equals 36  but in one iteration we will be exchanging these values the first iteration will only be spent for exchanging these values so  m will become 36 and n will become 48 so  then we can as good as assume that in fact  we will start off our execution in this manner so  we will assume in fact  that we are not analyzing this because that only adds 1 iteration so  our assumption is that we are analyzing calls of this kind where m is less than n well of course  if m is equal to n then there is no question so  the loop wills the euclid will immediately return  so  let me now state the main result that we want to prove the main result that we want to prove is something like this  refer slide time  39  44  so  if euclid is called with values m with values p and q that is euclid of p q is called i want to make a distinction between the variables m n and their values so  we will think of p q as values and m n will be variables so  you are going to call euclid with values p and q then if p is less than q then in each iteration the sum of the values of variables m and n will decrease at least by a factor 3 halves so  initially if the sum is 6 then in one iteration the sum must go below 4 become 4 or go below 4 if it is 16 in 1 iteration it must go below 40 or stay or become 40 what is the implication of this theorem ? this theorem establishes that the sum will drop very fast and in fact  the number of iterations this establishes that the number of iterations is equal to log to the base 3 halves of p plus q well  it is less than equal to  so this theorem will put a good upper bound on the number of iterations that euclid is going to execute and therefore  all that we need to do is to prove this theorem and we have a bound on how many iterations euclid takes so  our goal now  is to estimate what happens to the sum of the values taken by m and n in each iteration so  let us say that we start off the whole process and at the beginning of the iteration m and n take values p and q  refer slide time  42  27  so  at the beginning of iteration m is equal to p and n is equal to q after the iteration and i mean just one iteration let us say m takes the value p prime n takes that value q prime so  what is our goal we want to estimate by how much the whole thing drops so  you want to estimate p plus q upon p rime plus q prime so  if we prove that this ratio is at most 3 halves then we are done so  all that remains now  is to express p prime and q prime in terms of p and q so  for that will need our euclid procedure again so  what does the euclid procedure do if it does not terminate and that is the case that we are looking at currently ? then it computes r equals n mod m and then set n equals m so  in terms of this the new value of n is the old value of m so  going back to this the new value of n must be the old value of m so  therefore we get q prime must be equal to p the new value of m is the value of r which is the old value of m mod old value of n mod old value of m the old value of n is simply q mod m the old value of m is p and therefore  this whole thing p prime is q mod p we need one more fact in addition to all this which concerns p prime plus q prime  refer slide time  45  26  well  what is p prime plus q prime ? p prime is the remainder when q is divided by p and q prime is p itself so  this is remainder plus divisor and we know that the divisor is strictly less than the dividend divisor in our case is p and the divided is q and we assume that p is in fact  less than q and therefore  we can conclude that this whole thing has to be less than or equal to the dividend in other words this is q  so here are the 3 facts that we wanted p prime plus q prime is less than q let me align it q prime is equal to p and p prime is less than q mod p if p prime is less than q mod p then i can conclude that p prime has to be less than both in fact  it has to be less than p so  now  it is just a matter of algebra we are going to add this and this and this last inequality  but just to get the terms right we will multiply this last inequality by 2 times so  if we do that let me adjust this  refer slide time  47  50  so  if we do that addition we are going to get p prime plus q prime plus p prime plus q prime this comes from over here the whole thing is less than this p plus this p plus this q  but let us takes this 2 times therefore  we will get 2 q so  now it is just a matter of simplifying this so  what is this equal to this is nothing but 3 times p prime plus q prime and we have shown that it is less than 2 times p plus q and this is exactly what we wanted so  we have been we have showed that the original value of the sum has reduced by a factor of two-thirds so  p prime plus q prime is less than p plus q upon 3 halves that concludes the analysis of the algorithm and we have been able to show that in fact  the algorithm will execute a fairly small number of steps  refer slide time  49  19  so  let me now conclude this lecture and highlight the main points the very first point that i want to make is that is the difference between the 2 algorithms the school level algorithm basically uses the definition euclid ? s algorithm uses some more interesting deeper properties deeper mathematical properties of the quantities that we are going to calculate so  we study properties of whatever we computing and this helps in designing fast algorithms there is also one more point that i want to make which is that the basic way in which we did this analysis counting iterations will be useful in the rest of the course and the precise details of all of this we will cover in the subsequent lectures so  that marks the end of this lecture design and analysis of algorithms prof abhiram ranade department of computer science engineering indian institute of technology  bombay lecture  2 framework for algorithms analysis welcome to the second lecture of the course on design and analysis of algorithms in today ? s lecture  we are going to develop a framework for algorithm analysis in this course  we will be designing many algorithms for solving many different kinds of problems we will want to compare these algorithms and even just plain evaluate them and for this  we will need some sound mathematical bases and that is  what we are going to do we are going to design a formal framework using which we can evaluate algorithms and we will and also compare them this is going to the topic of this lecture and also the next few lectures the framework that we designed could be used not only for comparing the execution time of algorithms  which is what we primarily mean ; when we say algorithm analysis but  it could also be used for comparing other resources that an algorithm might use for example  an algorithm might use varying amounts of memory so  we could use essentially the same framework that we are going to discuss very soon and use that framework to formally compare the memory requirements of different programs or different algorithms the basic idea in designing in the framework that we are going to discuss today  is actually very related to the kind of analysis that we did in the first lecture except we are going to make it a little bit more formal essentially we are going to make a mathematical model of a computer and then  we are going to take our algorithm and mentally execute that algorithm on that model and then through this execution and by mentally executing  we will be able to tell how much time the algorithm takes and that is essentially going to be involved in doing the analysis that is what the analysis is going to have  refer slide time  02  58  so  let me write that down the basic idea we will make a mathematical model for computer and we are not going to execute our algorithm on any specific real computer but  will execute it mentally will imagine it is execution on this mathematical model so  let me write that down as well  mentally execute algorithm on computer model and evaluate the time this is the basic scheme ; this is the basic idea that we are going to develop in order to develop it  we need to answer several questions so  the first question naturally is what is this mathematical model going to be ? which essentially is the same thing as saying how long should be assign  what time should  we assign for each of the operations that is comprised that is used in our algorithm so  we need to answer questions like what is the time required on the model for every operation that an algorithm might perform then  we also said that we need to execute we need to mentally execute the algorithm on this model however  every algorithm or most interesting algorithms will require data some input that needs to be given to this algorithm so  an important question that we need to answer is  what data should we be giving  what should be the input data this is an extremely important question  because the time of execution in general will depend upon the input so  when we say we want to estimate the time taken by an algorithm we have to be very clear in saying what input is being given to that algorithm we may make mathematical models and we may develop them and we may estimate the time taken on those models of course  there is the important question  which we need to answer  which is how does all this relate to real computers ?  refer slide time  06  17  so  how does our model relate to real computers ? if our model is terribly different  then our conclusions for the model might not be too useful for real computers and of course  we do not really care that much about the mathematical model we want our conclusions to eventually apply to real computers and therefore  this is an extremely important question that we need to consider over the next few lectures  we are going to answer these questions and also many of the other relevant questions and you will see that all these questions can be answered nicely and in that  we will be we will finish our development of our framework  refer slide time  07  32  here is roughly what i am going to talk about in the next few lectures i am going to start by discussing some fairly basic terms so  we will try to formally define or at least semi formally some basic terms then  we will present our mathematical model after that i will discuss  the general overall analysis strategy which we are going to use these will for example  answer questions like what should be the input we will also be taking a number of examples of algorithms and their analysis and finally  we will conclude with a discussion of the limitations of the model this will essentially be an answer to the question of how well do our conclusions to the conclusions that we draw for the mathematical model relate to real computers and of course  i do not strictly i would not strictly discuss these points in the order i have written them i will discuss examples and i will discuss limitations and may be alternate a little bit but  this is basically going to be the gist of this lecture and the next let us  begin now with some basic terms that we are going to use in day to day life  we often use the same term to mean different things in scientific discussion  it is important to fix the meanings for every term so that we do not confuse ourselves later on and we do not end up with fallacies of any kind  refer slide time  10  06  so  let us start by discussing the very first very common term that we are going to use which is problem before  i give a definition of a problem i would like to give some examples and from those examples  i will try to motivate this definition when i say problem in this course i will mean  what we usually mean is  in the sense of the problem of computing the gcd of two numbers or say something like the problem of finding the shortest path on a map or maybe say finding the meaning of a word in a dictionary or may be even something like given an x-ray determine if there is any disease you may notice that when we are talking about a problem there is typically certain input which is which needs to be supplied and a certain output that needs to be generated let us take an example of this so for example  if you are asking about the gcd of two numbers we could say that the input consists of say numbers like say may be 36 and 48 the gcd of which will obviously  be the number 12 say for the problem of finding the shortest path in a map may be the input could look like say name of a city may be mumbai and say a city say aurangabad and we would have to supply which map we are going to use so maybe  we use the western india automobile association map and that will also have to be supplied that will also that map will also have to be supplied as a part of the problem definition for finding a word in a dictionary may be we have to supply the word say for example  we take the word evolution and we will also have to name what dictionary we use  say may be the oxford dictionary or something like that for the last problem  we will have to supply an actual x ray say some actual picture and in this case the output would be something like either there is disease say we just a yes or no for the shortest path  the output would be say the actual map  the actual path on the map for the evolution for finding the meaning of the word evolution the output would have to be the actual meaning that you would get while after looking at the dictionary at this point  we have i think we have a good sense of what a problem is and we can write down a reasonably simple definition  refer slide time  14  19  so  let us do that so  when we say problem in this course we will mean a specification of what are valid inputs and what constitute acceptable outputs ? acceptable outputs for each valid input so  we looked at this earlier so for example  for the gcd problem 36 and 48 constitute valid inputs and for these the acceptable input is 12 finding the shortest path names of two cities in the map constitutes a valid input and acceptable output would be the description of the path and so on of course input which is valid for one problem need not be valid for another problem and typically is not so  numbers will not make sense as input for say the dictionary problem or and words will not make sense as inputs for the gcd problem ; obviously we often use the phrase input instance and this is nothing but a valid input value for a given problem so  i will say that a value x is a input instance for problem p if x is a valid input as per the specification so  36 and 48 are 36 and 48 together constitute an instance for the gcd problem mumbai aurangabad and map constitute an instance for the gcd problem for the shortest path problem and so on  refer slide time  17  17  another important term that we need is that of a size of an instance we will often not necessarily use the term input instance  but we will just stick with instance instance will always mean input instance or we could even say problem instance so  when we say the size of an input instance  we mean in a formal sense we will mean a following will mean the number of bits needed to represent the input  the input instance let me just clarify that so  a specific input instance will have a certain specific size so  again let us go back to our examples  refer slide time  10  06  so for example  if you look at 36 48 which constitutes the input instance for the gcd problem  then we will have to ask the question  how many bits are needed to represent 36x and 48 ? so  here there is a question of how we represent numbers in the first place so suppose  we say numbers are going to be represented in binary then  36 will require 6 bits and so will 48 so in this case  the input instance will have length 6 plus 6 or 12 as far as  the shortest path in the map is concerned  somehow or the other we will have to represent the map there are various ways of this representation we will see some of them later on in the course in general a map can be thought of as a graph which you have probably seen in the prerequisites for this course and a map could be represented as a matrix and a matrix could be represented as an array bits if you like and in that way we can represent maps as well this definition that  we have given this formal definition that we have given is often a little bit inconvenient for directives so often  we settle for a somewhat more informal definition but in fact  this typically is something that is more useful and informally we might say the size of an instance and we might mean any parameter which roughly grows with the official definition of the size of the instance  refer slide time  17  17  so  let me write that down any parameter which grows roughly the growth may not be exactly predicable with the formal notion of size so  let us go back to our gcd problem there we said that the size was the size for 36 48 was 12 bits but  instead of making this is the definition of size we could say that the size is simply the sum of the numbers so  36 plus 48  which is 84 so  we could think of 84 itself as our notion of size of the input instance rather than 12 bits in fact  if you go back to the first lecture you will see that this was the parameter that we used when we analyzed the gcd algorithm in the first lecture so  we said that the size the sum of the numbers u and v will keep on decreasing and in fact  this is really the reason why we are interested in the notion of size usually we will expect that the time taken by an algorithm will increase with the size of the instance and therefore  if you are going to evaluate an algorithm it is only natural and it is only fair in some sense that we also mention what the size of the input instance is so  if an algorithm takes a long time on a large instance on an instance of large size then  that is but if it takes large time on an instance of a small size then  we should potentially say that that algorithm is not a good algorithm or at least it is not a fast algorithm so  let us go back to the other problems and maybe we can think of what the notion of size is going to be over there so  going back to our shortest path problem  a notion of size could be the size of the map  so the number of roads in the map or the number of roads and the number of cities together so  clearly finding a shortest path in a map which only involves one road is going to be really easy and therefore  we should expect we should an algorithm which takes us takes us short time on such a small instance should not really be thought of as a great algorithm on the other hand if an algorithm takes a small amount of time on a map which consists of 1000 cities and 2000 roads then  that algorithm we should certainly say is a god algorithm so  essentially that is the idea  we want to when we evaluate algorithms we want to evaluate that in comparison evaluate the time taken in comparison to the size of the input instance for the dictionary problem the size of the dictionary the number of words in the dictionary that is would be a good indication of the size and for the x ray problem  we will somehow have to take that x ray and convert it into bits of some kind so  we could say for example  that the size of the x ray say the number of the if the x ray is has a resolution 1000 by 1000 then  they could say that the size is a million or something like that we often use the phrase problem size also to denote the size of the instance so  if you say if you hear the phrase problem size it is really talking about the instance of the problem rather than the problem directly but  that is a term which is very commonly used on the literature  refer slide time  24  37   they next important term that we need to discuss is algorithm when i say algorithm  i mean an abstract computational procedure which takes some value or values as input and produces a value or values as output i use the term abstract in order to denote that an algorithm can be expressed in many ways so  a program is an expression of an algorithm so  the same algorithm might give rise to different programs say in different languages a program has been concrete and algorithms has been abstract of course even for eve for discussing algorithms  we will need to have an ocean of a language so ; however  this notion is not going to be as rigid or as strict as the notion that we have when we discuss programs when we discuss programs  we have a very well defined very  very strict language which has very  very strict rules for syntax we will not be worrying about all of that when we discuss algorithms we would like to think of algorithms as the idea behind the program and so long as we are able to convey that idea in as in very clear terms you will happy so  the basic our goal in this course is going to be description of algorithms so that human beings can understand  what is being said and we will not worry so much about the precise syntax that is used initially  we will describe algorithms at a fairly great level of detail as the course progresses  we will abbreviate our descriptions and it will become clear to somebody who has gone through the course exactly what is being met the reason for describing algorithms is of course  one reason is to convey what is the idea and the other reason why we will be discussing algorithms in this course is of course to evaluate their time so  i tell you what an algorithm is it should be clear to you  what exactly are the operations that i have in mind and you should be able to write the program  but not only that it should also be clear to you how that program will execute on a machine and especially  on the model machine that we are going to talk about and that is going to be another important purpose another important point that we want to keep in mind  when we discuss algorithms so  we have to describe algorithms at such a level of detail that it is fairly easy to analyze how long they will take on our mathematical model all these issues will become clear  when we describe our mathematical model which we will do right now  refer slide time  28  48  the mathematical model of a computer that we are going to use in this course is called ram and ram stands for random access machine this is a very simplified computer model and it only consists of basically consists of two parts so  there is a processor which will be executing programs and then  there is going to a memory the memory is going to be a correction of locations and in fact  it is convenient to think of the memory as an array with numbers on it so  the locations start with a certain say 0 and there might be say m minus 1 the last number could be m minus 1  if there are m locations overall so  each location has a number which is also called its address so  we can refer to locations by assigning by describing the number of course that is going to extremely inconvenient in general and so while writing algorithms  we will want to do something which is more pleasant and let me start describing  how we i will describe  what exactly  how we are going to refer to the locations  refer slide time  30  58  and in fact  as we describe the ram model i will also be describing how we program the ram model or how we design algorithms for the ram model so  the first thing to notice that although the ram model contains locations which are addressed by numbers we will in fact allow variable names so when we describe algorithms  we can say that say the value is contained in this variable a a certain value is contained in variable rather than a certain value we stored in the location fifty three or something like that in fact  we will allow a variety of data types say  we will allow plain  simple plain simple variables but  will also allow say arrays and will also allow structures i would like to think of these two as sort of the primitive data types and of course  let me write down simple variables along with them in addition of course  we will allow other things like trees lists and so on as well you will be able to build your own data structures as well  but somehow or the other they will have to be built out of these data structures so  this is as far as the memory is concerned there will be a memory which will store the program as well but  we will think of it as being quite separate so  the program and data do not mix so  here is again our picture  refer slide time  28  48  of the ram model so  there is a memory and then there is a processor  refer slide time  33  10   now  i have to tell you what the processor can do in each step so  basically this is going to be a description of the instruction set of the processor so  the processor is going to have a number of instructions and will assume for simplicity that all instructions execute in one step there are basically three  four groups of instructions that we will have so  one group is arithmetic and logical operations so in this  you will be allowed in your program to say take two locations from memory add their contents and deposit them in a third location let me  write down how you will actually express this  when you write programs and do not worry ; it is going to quite in a quite friendly this is going this can be represented in a very friendly pleasant manner so  for example  you could say a equals b plus c as a part of your algorithm and this is going to be one instruction as we said an instruction is going to be taking two operands b and c which are stored in two locations  add them up and put them back so  this will happen in one step then  you will be allowed to have conditions jumps and conditional jumps and this will also execute in one step so  correspondingly as a part of your program you will be allowed to write something like go to this will happen in one step or you will be allowed to write say something like if a greater than b then go to this will all happen in one step defining our model  we want to keep this definition reasonably simple you may be wondering at a stage  real computers probably do not look like this and you are right and we will take that question a little bit later i would like to make another comment over here although  the very second group of instructions that i am talking about concerns go to is this does not suggest this should not suggest to you in any way that when we design algorithms  we recommend use of go to is far from that algorithms as i said are intended to be read mostly by human beings and therefore  structured programming presenting the algorithms in a nice readable manner is extremely important however  when we talk about machines go to is are a very convenient mechanism and that is the reason  why we have go to is in our instruction set we will soon come to instructions which are more structured which  but that will be built out of those will be built out of our basic instruction set so  they will take several instructions and several cycles of execution we will come to that very soon there is a third group of instructions which is important and which i will call as pointer instructions so  these are simply operations of the form say b equals star c  where i am using c style pointer notation so  i am going to think of c as a pointer or c itself contains the address and i am going to fetch that location whose value is contained that location whose address is contained in c and b will get that value i can also have a store based on pointers so say for example  i could write star c is equal to b and all b ? s and both of these will also be executed in a single step all of these algorithmic actions will take just one step pointers and arrays are very related and the c language in particular mixes pointers and arrays a lot and in fact  our machine our random access machine is also going to treat pointers and arrays in a very similar consistent manner so in fact  in this group itself i will put down array operations and here i mean one dimensional array so for example  you are allowed to say a of i equals b or b equals c of i i do not mean this c to be the same as this c just c is just some array which you have declared going back to arrays  let me just make one comment about that so  we said that arrays that our machine will contain arrays and structures will assume the usual c like representation of arrays so  if an array has size 100  then we will assume that the array is stored in 100 contiguous locations in memory similarly  if an array if we have a structure which consists of three components then the structure will be stored in three contiguous locations in the memory so coming back  we have a processor and a memory a processor whose basic instruction set i have just described  a memory which consists of a location i have not told you what a location is really a location simply is a collection of bits it has to be a fixed number of bits say does not really matter what number it is  but it has to be fixed once and for all say it could be a number like 64 which is the number which is used in most modern computers so  there is a notion of a word and a notion of a word really goes along with this notion of a location  refer slide time  40  38  so  we looked at the basic instruction set and the basic algorithmic actions that are possible  what i am going to do next is think about more complex algorithmic actions more complex algorithmic statements so  we said that for example  we allow these instructions  but naturally these should suggest to you some more complex instructions or statements that you would like to have for example  we would like to write down a statement which looks like this so  say a equals b plus c times d minus f  how long does this take well our rule is very simple we will have to break this down into out elementary instructions so here  we have three operations and therefore  this will take three elementary steps the three elementary instructions and therefore  this will take three steps so  we will allow use of such statements in our algorithm but  when we count the time we will have to count 3 we will also want to use arrays in such expressions so for example  we might want to say a of i equals b of i plus c of i again  we have to see  how this statement is going to be represented in our basic instruction set so  here is how this could be represented so  first we need to fetch the value of b  the element of b so  this could get translated to say something like x equals b of i and this  itself is a primitive statement that we allowed and we said this takes one step then  we can similarly fetch c equals sorry say y equals c of y and this can also again be done in a single step  because this is our basic instruction itself then  we could compute z equals x plus y so now  we have ahead of the two values one of b of i and c of j and the second of c of j and we have computed their sum and all that remains is that this sum needs to be stored into a of i so now  we can write a of i equals z so  this simple statement well simple which we wrote down very simply in that sense really has to be translated into four machine instructions so to say and therefore  this entire statement will take four steps during execution we could also have multi dimensional arrays so for example  we could have an instruction which looks like c equals a of i comma j so here  we will have to decide  how two dimensional arrays are stored but  we will assume that two dimensional arrays are built on top of one dimensional arrays  refer slide time  44  19   so for example  if we have a two dimensional array which looks like this so  say this is an array a which has two rows and four columns say here at two rows and four columns let us say it stores elements a b c d in the first row  e f g h in the second row let us say that the array is indexed c style so  let us say this is row index 0  row index 1 this is column index 0 column index 1  2 and 3 now  on our machine on our ram  we are going to store this using one dimensional array we are essentially going to simulate it using the one dimensional array but of course  the array must have the same number of elements and so that is 8 2 3 4 5 6 7 8 and these elements will have to appear in this one dimensional array somehow let us say they are stored row wise many possibilities are there  but we are just picking one so  a b c d e f g h  so essentially every time you want to access this two dimensional array  we will be accessing some element of this one dimensional array and by the way remember that the index set of this is 0 1 2 3 4 5 6  how do we know  which element of this array we access in order to access a particular element of array of this array well there is a very simple correspondence so  a of i j if you want to access the i  jth element so  row i column j  then that corresponds to an element of a prime which element well it is simply i times m plus j where m is the number of columns in a which in this case is 4 so  wherever we see a i j we really should be reading it as this as far as the problem of accounting how much time it takes  refer slide time  46  54  so  let us do that so  c of a i j well we should really be thinking of as c equals a prime of i minus 1 times m plus j but once  we think of this statement in this manner then  estimating the time taken for it is fairly straight forward because now we know that we have to do one subtraction we have to subtract 1 from i then  we have to do one multiplication in the multiplication with m then  we have to do one addition and then we have to do a plain indirect axis so  this whole thing will take 4 steps let us  now turn to some structured computing statements  refer slide time  47  54  let us take a loop so  let us say we have for loop for i equals 1 to n  let us say we do something like c of i equals a of i plus b of i so as i said  we have to translate these instructions into our basic constructions and here is a possible translation so  we are going to have to start off by initializing so  we will write i equal 1 that is how the initialization step will be then  we have to have the loop test so  we will write this as if i greater than 1 greater than n  then go to end of loop then  we will write something like fetching the ith element of a so  we write something like x equals a of i then  we will write y equals b of i and then we will write say z equals x plus y in the manner that we just discussed and now we have to store it back so  we will write c of i equals z at this point we have to step through the loop so  we will write i equal i plus 1 and now we will just go back so  we will go to let me number these statements 1 2 3 4 5 6 7 8 and so we are going to go back to 2 and this should jump out of loop so  this has to go to 9 so  this is going to be a translation of this so  although in our algorithms  we will write this for statement and just for completeness may be we will have an end for as well but  as far as the purpose of accounting is concerned  this is the time  this is the code that we should be considering so  when we want to analyze the time taken  this is the code that is going to be of interest  refer slide time  50  44  so  let us try and analyze this code so  the analysis is going to be reasonably straight forward we are just going to go over each statement and we have to see how many times it is going to be executed so  let us try statement number  how many times will statement number one we executed well this will just be executed 1 time this part a of i to this part is what might be thought of as the body of the loop this will be executed n times this loop counting and this jump back will also be executed n times and this loop test let me write that down here this will be executed once for each iteration but  it will also be executed one more time because  that is when the machine is going to determine that we need to exit so in fact  this statement will be executed n plus 1 time so  the total number of steps taken for all of this is going to be b times n  where b is the number of instructions in the body plus these 2 steps plus these 2 n steps plus 2 n plus these n plus 1 plus this extra 1 so  this is going to be nothing but 2 plus n times b plus 3 so  this is our final answer let me write that down in big letters over here 2 plus n times b plus 3 this is going to the number of steps needed to execute this loop of course for this loop b has the value 4 and therefore  that is going to be the time  refer slide time  52  48  we will also allow functions calls in our programs and when we will assume for simplicity that the number of steps is needed for the function calls are going to be the number of arguments passed so  time equal to number of arguments of course i should say something about the syntax of the function calls and the syntax is going to be pretty much like the c language  what i mean by this  is that arrays and structures will be passed by reference which means that you can modify them in the called procedure and the modifications will be seen by the calling procedure  whereas variables will be passed by value so  this basically concludes the description of our random access machine there are a number of issues that we need to consider and i will mention some of them  refer slide time  54  17  now and these have to do with how the how this machine relates to reality are real computers like this or are they different then  we have to ask questions about we will want to take algorithms and see their complete analysis and finally  we will want to say what part of our analysis is really interesting  when we consider real computers  what features of our analysis are really relevant for real computers so this  we will do in the next lectures design and analysis of algorithms prof abhiram ranade department of computer science engineering indian institution of technology  bombay lecture  3 algorithm analysis framework ? ii welcome to the course on design and analysis of algorithms this is the second lecture on the algorithm analysis framework that we are going to be developing algorithm analysis framework part two  refer slide time  00  51  let me quickly remind you  what we did last time so last time  we defined an abstract computer model called ram or the random access machine this was the model which we said we were going to be using throughout this course somewhat implicitly  but at the beginning  we want to make it fairly we want to discuss it in fair detail ; so that it is well understood so  we said that this model consisted of a processor and a memory and we said that the data would be stored in memory the memory would include would be able to contain things like variables structures  arrays the usual  data structures and there would a separate program memory then  we said that the instruction set would be reasonably simple so  let me just mention which instructions are important so  basically the instruction set would be performing operations of the form a equals operator c so  this would be the first kind of instructions  where a b c are simple variables this is one kind of instruction then  we would have control instructions or jump instructions or control transfer instructions rather so  jump instructions and conditional jumps and then  we would have array operations or array or pointer operations so  these would be instructions of the form say a equals star b where we are using the c like syntax or it could also be something like a equals b index i so  this instruction is sort of a standard array axis and we said last time that these instructions would be single would take a single step would take a single cycle so  the idea was that we were going to design algorithms and then  you would reason about what instructions those algorithms would need to execute and then  we would estimate the total time taken  refer slide time  03  50  the goal for today is to do the following so  let me write that down so  the outline for today so  we are going to start by comparing our ram model with real computers actually  it is not just real computers  but also real compilers  because compilers also will make a difference in this comparison will see what parts of a ram model are of interest and which parts or which details is really something that we need to define for the sake of completeness but  which really should not be taken too seriously then  we are going to define our general strategy for algorithm analysis and then we will take some examples and after that we will have summary as usual  refer slide time  05  25   so  let us now come back to our random access machine and try to find out in what sense this differs from real computers so  first of all unlike our random access machine in which we said that there was a single processor and a single memory real computer have a much more complicated architecture much more complicated so  for example  there are many different kinds of memory so  there could be our standard memory which we will call main memory then  there could be a memory called cache memory i am sure you have heard of cache memory most computer advertisements do talk about cache memory there could be memory which is called registers register memory then  a very tricky complication arises in a very tricky idea is used in designing computers these days and that idea is called pipelining  what this means is that ? several instructions might be executed simultaneously at in ; they will be at different stages of execution not only there is pipelining  but there is also something called superscalar execution essentially this means that say somewhere between 1 and 4 instructions might be simultaneously executed this is a fairly complicated idea and analyzing this idea is definitely beyond the scope of this course however  you will see that to a certain extent these complications do not matter we said we defined an instruction set for our ram machine the instruction set for real computers is somewhat different in the sense that there are instructions which transfer data between different memories in the system so  there is memory to memory instructions memory or memory to register instructions  memory to register copying instructions and then arithmetic or any computation is only done in registers so  as you can see this architecture is much more complicated  refer slide time  08  42  and i will just take one more example just to illustrate what i mean by the use of registers so  this will drive home one of the points so  we said that on a ram suppose you have an instruction like a equals b plus c so  this really happens in one step on a ram our abstract on our abstract computer  this happens in a single step on the real computer this actually needs several steps so  a might be a will be a variable in memory so  first you will have to load it into a register so  on real computers this is what you will do so  we will load b into say register 1 then  you will load c into register 2 and then  you will add register 2 register 1 into register 3 so  the value contained in register 2 is added to the value contained in the register 1 and the result is placed in register 3 and then finally  you will store register 3 into location a of main memory so  a is a main memory location just stress b and c as well b and c are also main memory locations but since  we can not deal directly with data stored in main memory we will have to deal with we will have to first move them into registers and then deal with them this is the not the only way in which real computation is different from our idealized memory there can be differences between real computation and our idealized computation  refer slide time  10  42  also because of the use of compilers or because of intelligent compilers  let us take a simple example ; say a program which consists of just two lines so  this is a program say which looks like if x of i equals 1 then  do something or then go to if x of i equals 2 then do something a simple translator for this  a simple translator would operate for this would have to extract the value of i because remember  we can not do conditional operations directly on array elements we first have to get them into some variable so  this first statement would be translated into some fragment which will cause x of i to be loaded say temp is the name and here we load x of i this can be done in one step on a ram as well as on a real computer very almost certainly so  this is going to be one of the instructions in translation of or compilation of this first statement a simple minded translator would also do a similar translation for the second statement which would also require 10 equals or temp 1 equals x of i  because even in the second statement we are accessing x of i the fact that x of i has not is not changing in between these two statements may or may not be noticed by the compilers so  as result a high level language statement such as this one or a sequence of high level language statement such as this one could be translated into a different number of statements depending upon the compiler intelligence and a simple compiler would translate the first statement and a first statement translation would require a fetching of x of i and the second statement would be translated separately and that would also require a fetch of x of i but  an intelligent transfer an intelligent compiler would realize that this fetch is really not needed because  it is already there and it can already use the value which is present earlier because that value is not being modified in between so  in general  when we write algorithms an intelligent compiler would be between us and the machine on which the algorithm executes and therefore  we really should be thinking about what are the capabilities of that compiler so in summary  i would like to say that we have an idealized model of what a computer is it differs from what a computer what a real computer actually is it is it is also idealized  because we are not really talking too much about the compiler which sets between a high level language algorithm or a high level language program and what actually gets executed on the machine so in some sense now you might be thinking that it is actually a big surprise that whatever we say about real computers whatever we say about our idealized model applies to real computers as well and indeed we have to be somewhat careful about how we make this application and we are going to see that next  refer slide time  14  50  so  now let me indicate our general analysis strategy our measure for each algorithm is going to be a function t of n  where t of n is going to denote the maximum time taken by the algorithm on the ram for any instance of size n so  this is a very crucial definition t of n is going to be maximum time taken by our algorithm here  let me call it algorithm a and let me say t sub a of n maximum time taken by our algorithm a to solve any instance of size n so  as we noted in the last lecture  there could be several instances having the same size so  if you are sorting a data set the size typically could be thought of as a number of values in that data set and for different values even if the number of values is the same the time taken could be different so  when we talk about t of n or t of n for this algorithm a we are asking  what is the largest time taken  say for sorting or whatever that algorithm is doing for all the input instances of size n and that is what we define as t of n so  we will have a value which will be defined for t of 1 t of 2 t of 3 t of 4 for every n we will have a value in principle  we can determine that value by executing our algorithm on all possible instances of size n this is only for the definition  but i just wanted to say that just to indicate that our that this definition is very down to earth and this expression is very well defined so  t of a t sub a of n is going to be our measure of goodness of this algorithm a notice that  we are not saying we are not evaluating a single number for our algorithm so  you give me an algorithm and you ask me how good is this algorithm i am going to tell you here is a function this is how good this algorithm is you might want me to tell you a number  but sorry i can not do that i am just going to tell you a function and now for every algorithm that you might give me  i will indicate a certain function and it is your job to find out which algorithm is actually good by looking at those functions carefully  refer slide time  18  28  so  i would like to make a few more remarks on this definition the first remark is that this definition is a conservative definition so  it says that the time that i am going to think is significant for a certain input size is the worst case time is sort of the maximum possible time i could have said  well let me consider all possible inputs of size n and let me take their average time this would be a reasonable definition however  this averaging of often turns out to be turns out to be difficult to perform primarily the reason is that you might enumerate all possible instances of size n but  those instances may not appear in actual day to day life with equal probability so unless  they appear with exactly the same probability for all instances taking that average is usually not so significant even if they did appear with equal probability even then taking an average introduces an extra complexity to this which we will deal with once in a while but by and large  we will stick to this reasonably simple definition which happens to be conservative so  this is sort of the worst so  this is a conservative definition in the sense that if i say that t of n is 5 i know that no instance of size n will take time less than 5 will take time more than 5 and so  it provides me sort of a very solid guarantee and since  i am talking about the worst possible time that an instance of size n can take this measure is also called the worst case measure now  i would like to comment on how the time taken on a ram relates to real machines so  we said earlier we pointed out the differences between the real computers and rams and we saw there that a single instruction on the ram might correspond to several instructions on the real computers and also perhaps vice versa and also because of compiler technology we might get differences so then  the question arises whatever analysis we do ? for the ram is it really of consequence for real computers so  the point the important point that we will see soon but  which i would like to just mention right now is that the precise value of t of n the precise the precise numbers in the expression that we get for t of n may not be of great consequence of real computers and real computations  what is going to be of consequence ; however  is the form of t of n or let me write this as the functional form of t of n  what i mean by the functional form is the following so  i am going to ask questions such as is t of n linear function of n by linear i mean functions which are of the form a n plus b or is it a quadratic function of n by this i mean functions which are of the form a n square plus b n plus c and so on so  say cubic and other complex functions as well but essentially  i am not talking about the precise values of a and b when  i talk about a function being linear  but i am just talking about the shape of that function so  they crucial idea is going to be that the shape that this function t of n has is going to be independent of what computer it is on which you run your algorithm and since  we are really interested in characterizing the algorithm rather than the computer itself it is important for us to pick a measure which depends entirely on the algorithm and in fact  it is going this going to be the shape or this functional form  whether t of n is a linear function or a quadratic function or a cubic function and we will we will soon see examples of this in a minute very often  we will not be able to exactly estimate t of n we will not be able to give exact bounds on t of n here  we will say something like what is an upper bound or we will say something like what is the lower bound if the upper bound and lower bound match then great our analysis is complete but  this does not always necessarily happen so in that case well  we will have to live with incomplete our work done incomplete and bridging the difference between the lower bound and the upper bound will be subject of research finally  i will also comment that we will mostly be concerned or it is customary to be concerned about getting good estimates of on t of n for large n so  we always have in our mind that we are solving large problems as our problems become larger and larger which algorithm does better that is the question that we should be asking so  let me answer that question very quickly if an algorithm has time quadratic  then it is going to take more time than an algorithm which has time linear certainly as the problem size n goes to infinity and so in the end  what we will end up saying is that a quadratic time algorithm is worse than a linear time algorithm does not matter  what the precise constants are whether it is 3x square plus 9 or whether it is 1000x plus 56  because as n goes to infinity as n goes to infinity linear time algorithm is going to take less time eventually as n goes to infinity than a quadratic time algorithm and this idea is justified  because typically we use computers to solve large problems if a problem is small  then either we solve it by hand or it does not take too much time on a computer computers are very fast these days and so  analyzing it is really not of interest so  this is our overall analysis strategy and this is a general idea about when we say an algorithm is good when we say an algorithm is bad and so on  refer slide time  26  21  so  let us now take some examples to make these ideas more concrete the first example is going to be matrix multiplication our input is going to be 2n by n matrices say a and b which are n by n matrices output is a matrix c which is also a n by n matrix and c will be a times b  where a times b is the usual matrix product so  in particular c i j is defined as summation over k of a i k b k j everybody knows this definition i am just writing it down just for completeness there are many algorithms for matrix multiplication but i am going to consider relatively simple the most natural algorithm which just uses this definition that is all nothing more  refer slide time  27  30  so  here is that simple algorithm so  we are going to have three loops so  for i equal 1 to n for j equals 1 to n and then  we are going to set c i j equals 0 over here so  c i j is an entry in a two dimensional array n by n of size of dimension n by n and so  we will assume that all over matrices are represented by two dimensional arrays and they are in memory and having set each c i j to 0 we will just use our definition of c i j to calculate the value of it so  for k going from 1 to n c i j equals c i j plus a i k times b k j and then we just close all the loops so  this is our algorithm we would like to find out how long this takes on our random access machine ram  refer slide time  29  13  let me just point out that the problem size can be thought of as just n itself so  the first thing to note in analyzing this algorithm is that for every instance of the same problem size the time taken is the same this is not typical of algorithms  but we have chosen this algorithm just to illustrate the analysis ideas and later on we will see algorithms where the time taken is different and we will analyze those as well then  this algorithm also has a very simple structure again we will be analyzing more complex structures later on this basic structure in this algorithm is that of a loop and we have already seen in the last lecture how to analyze loops  refer slide time  30  05  so  let us write that down so  we said last time that if i have a loop in which the body takes b steps and the number of iterations is n so  we have a for loop in which the body takes b steps and the number of iterations is n then  we said that the total time taken is going to be 2 plus n times b plus 3 steps this is what we concluded in the last lecture if you do not remember these numbers 2 and 3 do not worry about it so much we are in the end going to see that these two numbers are not important but right now  we are going to do pretty exact analysis knowing how to do this exact analysis is important for the sake of completeness and we will do it just once it might look it might feel a little bit painful  but it is important to do it once so that  we are sure that we have understood the ram model completely later on we will see why not  worry about some details is important and in fact  the second algorithm that we will analyze we will analyze without doing without paying too much attention to all the small detail but  for now we will stick to exact analysis and will note that for an exact analysis the time taken for in a for loop is going to be 2 plus n times b plus 3  where b is the time required for the body the second idea that we needed from last lecture was that if i am accessing a two dimensional array an element of a two dimensional array it takes me 4 steps  refer slide time  32  22  so  we will use these ideas to analyze our program our algorithm over here so  let us look at this statement so  this statement has four accesses to two dimensional arrays so  this is one access this is another access this is another access and this is another access and then there are two arithmetic operations that it does so  the time it takes must be 4 times 4 for the array accesses plus 2 so  10 cycles totally so  a single a single execution of this statement itself forget the loops just a single execution of the statement itself will take time 10 steps no matter what the values of c i j a i k and b k j are but once  we know the time required for this single statement we can calculate the time required for this outer loop over here this loop  refer slide time  30  05  how do we do that ? well we wrote down that if you have a for loop in which the body takes b steps and if there are n iterations  then the time taken is 2 plus n times b plus 3 as a result of which this for loop will take time so  it is 2 plus n is the same over here it is still has n iterations b ; however is 10 plus 3 so  it is going to take time 13 and plus 2 18  so this will take time this so  in the reverse it will take time 21 and plus 2 but now  this is the time for this particular part so  we just have to worry about this additional time over here so this entire portion  all the way from here to here comprises the body of for this loop for the  for loop on j this takes an additional 4 steps for this access and 1 step for this assignment so  this will take time 21 n n plus 7 so  that is the time for the body of this loop the total time taken for this outer loop now this one over here is again going back to our formula for body the time required for the for a loop in terms of its body and number of iterations well the body of this loop requires time 21 n plus 7 so that is all that we need to plug in into this expression over here so  if we do that then for this loop we are going to get 2 plus the number of iterations is still n times 21 n plus 7 plus that additional 3 so  this is the time that we are going to get for this outer loop in j and now all that remains is to do this outer loop in i itself so  the total time for the outer loop well we know the time required for the body of that outer loop and that is going to be so now  it is going to be 2 plus n times whatever time the body of this loop takes and that is nothing but so if i substitute this i am going to get 21 n squared plus i will get a 10 n out of this and plus i will get a 2 so  this is going to be the time taken for the entire program and in fact  this is going to be the time taken for our for our algorithm so  let me write that down so  it is going to be something like 21 n cube  total time is going to be 21 n cube plus 10 n square plus 2 n plus 2 something like that the important point you will see are not in fact  these very numbers  but the fact that there is a n cube term in all this but  anyway unless i have made a mistake in doing this arithmetic the expression should come out to be something like this some number times n cube plus some number times n square plus some number times n plus some number now  let me ask you what is going to happen if i execute this algorithm on another computer which has a different instruction set  what well change ? well the time required for each of these individual instructions could change  because that would correspond to a different number of basic instructions on that computer there might be some intelligent compiler which might realize that for example  fetching once you fetch this then storing it to the same position you do not need to recalculate where you are going to store it something like that but  the important point is that no compiler is going to be able to change the fact that this in a loop must execute n times on no computer and with no compiler this fact is going to change similarly  with no computer and with no compiler  the fact that this outer the second inner most loop is going to execute n times is also going to change or that the first loop is going to execute the outer most loop is going to execute n times that is neither going to change so  no matter where we run this program on what compiler we use the time taken is going to involve this n cube term so  it will be 21 n cube or it could be some 36 n cube or something like that  but there is going to be a n cube term  refer slide time  39  23  so  let me write down sort of the major conclusion so  the main conclusion is that the time is cubic in n so  what i mean what do i mean by cubic let me write that down again that the time is going to be some function of the form a times n cube plus b times n square plus c times n plus d we do not really care so much about what the values of a b c d are when we say the time taken is cubic in n and in fact  saying that it is cubic in n provides us  precisely the level of the level of exactness that we are looking for because  we really do not know  what these constants are going to be  unless we look at the precise architecture of a computer so  if we are going to analyze an algorithm if we are not going to analyze a computer itself but  if we are going to restrict ourselves to the analysis of the algorithm itself then all that we can say is that the time taken is going to be a n cube plus b n square plus c n plus d or make this statement which sounds almost qualitative which is that the time taken is cubic in n but  this is exactly what we want to be saying in this course we want to say whether the time taken is cubic or quadratic or linear or something like that because this weak looking statement is exactly the kind of statement which we can justify and which we can claim about all possible computers the important point is that although we are making a weak statement when we say we do not know what the constant a is we are making at the same time our statement is actually a very strong statement  because we are saying this about all computers if we made on down this a to be 105  then that statement we would have to make about a specific computer say some specific pentium whatever computer or some other architecture computer and that is not what we want to make we do not want to talk about computers in this course or of precise computer architectures in this course  but we want to talk about algorithms and therefore  we are going to restrict ourselves to making statements of this kind that the time taken is cubical and or quadratic and so on and our methodology is going to be similar to what we followed just down we are going to execute we are going to mentally think about how long an algorithm executes and then  we will estimate the time is cubic or quadratic or whatever in fact  our methodology is going to be simpler than what we just saw because now we know that we are really not interested in whether it is 21 or 2 plus n or whatever 2 plus n square or whatever we can just say we will just ask about is it cubic is it quadratic so and will take it in that manner  refer slide time  42  46  i will take one more example  to illustrate this idea of analysis of algorithms and this example is that of finding the median the input in this case is n numbers so  let us call them say x 1 x 2 all the way till x n and our goal is to find out the median so  the median element is the element which appears in the middle in all those numbers or in general  it is defined as that element which is smaller than utmost n over two elements and larger than utmost n over two elements so  let us first try to it is a very simple program for finding the median this is not the best possible program  but it is actually it is a very simple program  refer slide time  43  46   so  let me indicate that so  let us say let me just write it separately over here this program will look like this so  let us assume that the input that is given to us is stored in this array x so  the idea of the program is going to be i am going to look at successive elements of x and i am going to check whether it satisfies the median definition  what ; that means  is i am going to compare it every other element and see if the number of smaller elements is utmost n by 2 and the number of larger elements is also utmost n by 2 if we find that both of these conditions match then we are done so  here is the code which does this because we are going to say n equals length of x then  we are going to have a loop where we consider each element in turn so  for i equals 1 to n  we are going to say the number of smaller elements is equal to 0 we are going to initialize the number of larger elements to also 0 then  we are going to check so now  we are going to check the i th element with the every j th element so  for j equal to 1 to n  if x of i is greater than x of j then we will increment smaller if x of i is strictly less than x of j then  we have found an element which is larger and therefore  will increment the count for larger that ends this for loop and at this point we will check  whether we have median in fact so  the check is going to be something like this if smaller is less than n b y 2 and larger is also lesser than n by 2  then exit because  we have found  otherwise we repeat we repeat with the rest of the loop so  this matches this for over here that is the very simple algorithm  refer slide time  46  50  now  the question is how do we analyze it ? so  we are going to do that analysis in the abbreviated style which i indicated so  again we are going to look at the inner most loop so  here is the inner most loop  how long does this take the body of this take well the body of this is going to take some c steps i will leave it to you as an exercise to show that c further will be no larger than 7 but  i am not going to worry about whether it is 7 or 8 or some or whatever it is  but i will just say it is some c so then  what can i say about the time taken for a single execution of this loop well it executes n times and so i can write down this time as being say some say linear in n  what does linear in n mean it means that it is going to be some a n plus b sometime which is a n plus b now  we will focus our attention on this outer loop  how many times is this outer loop going to be executed well we said that it is going to be executed utmost n times  but it could execute fewer than n times as well but  let us make a simple assumption a simplifying assumption and say that it never exits early or in the worse case or let us say since we do not know whether it exits earlier or not we will just calculate an upper bound so  now  we are in the business of calculating an upper bound since  we are not being too clever in analyzing when it will when it will exit early if it will exit early so  if we make this assumption that it does not exit early  then the time taken for this is going to be linear in the time taken for the body of this loop but  that itself is linear in n and therefore  the time taken for this loop is going to be quadratic in n but  that finishes the analysis of the entire algorithm because in fact  the time taken for the entire algorithm will have just this extra step and therefore  the time taken is going to be quadratic in n  refer slide time  49  07  so  what have we concluded ? we have concluded that the time is utmost quadratic in n so  remember again the time when i say the time i mean a function so that function is smaller than some function which is quadratic in n that is what i am saying over here  what do we need to do ? in order to complete this analysis we need to ask is there a lower bound we can actually put so for that  we need to we need to figure out what happens to this exit condition is there a situation in which this exit condition is not really met or met very  very few times ?  refer slide time  50  04  so in fact  i will leave this as an exercise for you and that exercise is if x 1 x 2 to x n is the input and if this last number happens to be the median and all other numbers are distinct then  this exit condition will be met only at the very end and then therefore  we will say that for this particular instance the time taken is going to be at least quadratic in n as well so  let me write that down so  this is our conclusion it is at least quadratic as well as utmost quadratic this does not mean for every instance it takes time at least quadratic this just means for the worst instance it takes time at least quadratic  refer slide time  51  00  so  let me now conclude this lecture so  what have seen in this lecture ? in these two lectures well we have seen in the ram model then  we have indicated an analysis strategy this strategy is based on the notion of worst case analysis we said that the ram model is not equal to real computers but  conclusions about the form of t of n are still valid and that is what is going to be of importance ? and finally  last point for most of the most of the course this is going to be this is going to be our framework  refer slide time  52  02  we will consider worst case  but once in a while we will also look at the average case so  let me just leave it leave you with this summary and that is the end of the lecture design and analysis of algorithms prof abhiram ranade department of computer science engineering indian institute of technology  bombay lecture  4 asymptotic notation welcome to the course on design and analysis of algorithms our topic today is asymptotic notation let me begin by setting down this topic  in the context of our overall course goals we said last time that one of the main course goals was to well first design algorithms  refer slide time  01  10  and then  we want to analyze their time analyze the time taken on the ram model  which we defined we also said that the results of the analysis are not directly applicable so  there is some care needed in understanding  how to interpret the time taken on the ram model ? and how to use that predict  what happens on other computers so  for example  so in all this we need some level of  we need to be a little bit imprecise we need to throw away some details of our analysis  in order to predict what happens on other computers so  the entire analysis or what i really mean the entire detailed analysis not applicable to other computers we said that suppose the time taken in the ram was something like say 10 n cube plus 5 n square plus 7 then  all that we can say for other computers is that the time is going to be cubic in n however  the same conclusion would be arrived at say of the time was 2 n cube plus 3 n plus 79 even here  our interpretation our conclusions for the computers at large would be that the time taken is only cubic in n so  this is what our conclusions will be  for any computer or all computers so  you see that we start off with the precise number over here but  over here we are going down to a very rough statement and in some sense  we are saying in all this that this function  this expression which we are going to think of is a function in n n is the problem size so  this expression 10 n cube plus 5 n square plus 7 and this expression or this function  2 n cube plus 3 n plus 79 are really in the same class so  you want to define the notion of classes of functions  refer slide time  04  07  so  the idea is that we want to put functions in the same class and really think about the entire class so  our conclusion will be that instead of saying that the time taken is 10 n cube plus 5 n square plus 7 we really want to say something like cubic  but we want to be a lot more systematic and formal about it so  that is really the goal of today ? s lecture so  we would like to develop the notation  which allows us to talk nicely about classes of functions so  asymptotic notation  so is a formal way or formal notation to speak about functions and classify them asymptotic analysis refers to the question of classifying functions or classifying the behavior of anything  but in this not too precisely  but by putting them into classes so  let me start out by writing down what do  we need from the classes  that we are going to define so  we want really two kinds of features  refer slide time  06  05  so one  we would like to put functions such as say 10 n cube plus 5 n square plus 17 and 2 n cube plus 3 n plus 79  should belong to the same class because  we said that in some sense we are going to be classifying these as cubic and we want them to get together another way of saying this is that constant multipliers should be ignored so  the constant multiplier over here is 10 the constant multiplier over here is 2 so  we are going to ignore that and that is our desire because  eventually what we can really say is that  the time taken on the ram is this and the time taken on any computer has to have the form  like something times n cube so  we want a class notation  which allows us to nicely ignore constant multipliers our class notation should also really worry about  what happens as n tends to infinity so  we should give more importance to behavior as n tends to infinity so  it is also seen in this example itself as n tends to infinity really the 5 n square plus 17 and this 3 n plus 79  these two parts of these functions will go out and therefore  we will really be worrying about 10 n cube versus 2 n cube and then  out first property or first feature which we said  we want in our class definition will take over and it will say that  really 10 n cube and 2 n cube are really the same thing so  that is the spirit so  we want a notation a class notation  which will allow us to conclude that say functions of this kind are really similar or are in the same class so  let me give an outline of today ? s lecture  refer slide time  08  43  so  we are going to define three main kinds of notation today so  one is the theta notation one is the o notation this is the capital or the big o and then  there is the omega notation and these will define function classes and they will do exactly  what we said we want and we will have lots of examples throughout but  at the end we will also have a series of examples so  let us go in order and let me start off with the theta notation  refer slide time  09  38  so  in what follows we are going to have functions  say something like f and g and these functions are always going to be say non-negative functions  which will take non negative values functions of non negative arguments this is natural in the sense that  we are going to be talking about time or may be sometimes the memory or any kind of resource and these values will not really  there would not be any occasion when functions will need to take negative values so  now theta of g where remember g is a function  is the following class so  it is the class of all functions f where so  let me write that down so  f is a non negative function such that  there exists constant c 1 c 2 and n naught such that c 1 times g of n is less than or equal to f of n less than or equal to c 2 times g of n and this is true  not necessarily for all values of n  but certainly for n greater than or equal to n naught this is bit of a big definition but  let me just indicate the spirit of it so  let us go back to the properties that we wanted so  we said that whenever whatever class structure we defined  should give importance to behavior as n tends to infinity so  this is the part of the behavior this is the part of that requirement we are saying that for  only that we are only really bothered about  what happens as n is bigger than some n naught so  we are not worried about smaller values of n then we said that we should really not be worrying so much about constant multipliers so  this is also what is going on over here so  it says that we want f of n to be sandwiched between c 1 time g of n and c 2 times g of n so  let me draw a picture here  refer slide time  12  35  so  this is where i plot the function values and this is n then  c 1 times g of n will look something like this say in general  that is going to be something like this is c 1 times g of n this is going to be c 2 times g of n and let us say this is n naught then  our claim our requirement is that  if f occupies this region entirely and does not go beyond this region  go outside this region this is the region for f  somewhere inside anywhere inside so  then it is sandwiched between c 1 times g of n and c 2 times g of n then we will put f in the class theta times g so  notice that we are not caring what happens  if for values of n below over here  f could go outside this that is ok we are not worried about that but  beyond n naught f must only lie between this sandwich region so  essentially we are saying that  we do not worry about what that constant factor is so  it is bracketed below by some multiple of g it is bracketed above by another multiple of g so  essentially it behaves like g and that for large n that is exactly  that is exactly in the consistent with the features that we wanted so  let us take a few examples  refer slide time  14  12  let us take our two functions  which we started so  i will write down say f of n equals or let me call it f 1 n equals  say 10 n cube plus 5 n square plus 17 then  this function belongs to theta of n cube let me write down i will prove this  but let me write down the other claim as well so  let me write f 2 of n another function  which is say 2 n cube plus 3 n plus 79 and this also belongs to the same class n cube so  i just want to reassure you that the goals with which we started namely  developing a class notation which will enable us to put these two functions in the same class or those that goal is actually being met so  let me now say in one what basis i am concluding something like this so  let me go back to the old definition so  in order to classify a function as being a member of this classed functions  this set of functions all i need to do is  to find suitable constants c 1 c 2 and n naught so  if i find these suitable a constant c 1 c 2 n naught such that  these properties are met then  i am really done so  let us take one so  let me write this down as proof of one so  clearly 10 n cube is less than or equal to f 1 n because  there is a that additional 5 n square plus 17 term i can write down f 1 of n is certainly less than i will just raise all these to n cubes  instead of keeping them n to the zero n square over here so  this is certainly less than 10 plus 5 plus 17 times n cube or this is equal to 22 n cube or 32 n cube really and this is true for all n so  i have established that if i take c 1 equals 10 and c 2 equals 32 then  c 1 times n cube is less than or equal to f 1 of n less than or equal to c 2 of n for all n greater than or equal to say even 1 that does not really matter so  these constants c 1 c 2 and n  and 1 is equal to n naught having found and we know that the functions and these constants satisfy the properties that  we wanted and therefore  we can legitimately claim  that f 1 belongs to this class so  the conclusion from this is that f 1 belongs to the class n cube and in fact  the same kind of analysis where example is to prove  that f 2 also belongs to the same class let me take one more example  which actually illustrates that our notation is a bit more is going beyond what we really started off  refer slide time  17  54  so  i am going to argue now so  that suppose i take f 3 of n is equal to n cube plus  say n log n then  even this is in the class theta of n cube this is not something that you would classify  as being cubic  because cubic has the connotation of cubic polynomial so  it has to it has to have the form something times n cube plus something times n square plus something times n plus a constant whereas here there is something funny that is there is a term  which is not a polynomial term however  note that it still is true that 10 n cube is less than equal to f 3 of n  because n log n is certainly always at least zero or certainly greater than zero in fact and in fact  i can since i know that  n log n less than n cube i can also write this as less than or equal to 11 n cube so  i have found c 1 equals 10 c 2 equals 11 and say n naught equals 1 for which  my whole definition holds and therefore  i can write fully claim that this function f 3 also belongs to the class n cube so  this is a good idea it is good that in fact  this function belongs to this class n cube because  as n increases as n tends to infinity then  this function really is the same as 10 n cube essentially  because this term is going to be negligible as for large n as compared to this term and so therefore  since it is essentially the same as n cube it is a good thing that our classification system  is putting it in the same class let me write down a few more examples actually before that  let us come back to this definition itself so  theta of g is a set of functions or class of functions and we are going to think of g as being sort of a representative or v g s being a sort of a prototypical function so  instead of talking about a very detailed function like  say 10 n cube plus n log n or 10 n cube plus 5 n square plus 17 we will say  we are roughly going to say that it is n cube if we are ignoring constant factors and as n goes to infinity and we will instead be saying that  it is that the class theta of n cube so  think of g also as a representative of all this  all these functions so  let us take a few more examples  refer slide time  20  53  some  i will write down may be that 5 n log n plus 10 n  belongs to the class theta of n log n so  the important point over here is that  this 10 n is grows slower so  as n becomes large this term is going to dominate this term and therefore  its behavior should be essentially the same as that of n log n and therefore  it is in the same class we should really prove this and i will leave that as an exercise we should really prove that  a function of n which is 5 n log n plus 10 n belongs to this class theta of n log n and by that  i mean you should exhibit constant c 1 c 2 and n naught such that c 1 times n log n is less than or equal to this  which in turn is less than or equal to c 2 times n log n  for all n greater than the n naught that we defined that is a fairly easy task  but if you certainly do it so  as to make sure that you are fully conversant with this definition let us take a slightly more complicated  but complicated looking example so  let us say we have a polynomial a of n which goes something like say summation i going from zero to k of a i  a i n to the power i and let us assume that  a k is greater than 0 other terms could be smaller  but a k is bigger than 0 then  i will claim that this function a of n in general  any polynomial of k th degree is in the class theta of n to the power k again you should prove this but  this proof is really similar to what we have done earlier and there should not any difficulty what so ever again the message is the same that  if you have a function you look at the most  the largest determinate and that is really is  it is class that really is its asymptotic complexity class let me now write down  some properties of this definition so  suppose we have f belonging to theta of g 1 and say h belonging to theta of g 2 then  i claim that f plus h belongs to theta of g 1 plus g 2 this also you should be able to verify  fairly straight forward and furthermore  if say g 1 this is the same as g 2 then f plus h when i write f plus h  i really mean f of n plus h of n  the function which returns for every n  f of n plus h of n and this function belongs to theta of g this is also understandable  because if you believe the previous result so  g 1 plus g 1 equal to g 2 equal to g so  g 1 plus g 2 will be equal to twice g and the class two n cube theta of two n cube is really  the same as the class theta of n cube again you should be able to verify this you should verify this and it is really not surprising  because we started off by saying that we really do not want to worry about constant factors and therefore  it does make sense or it should make sense  to have theta of n cube with the same as theta of 2 n cube we however  never ever write theta of 2 n cube and that is  because it is much simpler and much nicer to say  theta of n cube so  when we write theta of g we do not  we drop off the constant multipliers  if any that might be present in g so  we have defined the first class of functions that we wanted and it does indeed  it is indeed consistent with our intuition and the goals we set ourselves however  although this is a class of functions  in computer science and mathematics there is a funny style evolved  as far as writing down these classes is concerned  refer slide time  25  38  so  let me write that let me write that down so  i am going to write down a note  on writing style so  suppose f belongs to the class theta of g now  very often or most commonly this is not written in this manner but  the more common writing style is  to write f is equal to theta of g unfortunately  the assignment operator is again being badly abused over here this seems to be a tradition in computer science we use assignment to mean  we use the equal to operator to mean assignment we use it to mean well  first of all it has the value equality then  it has this value it has  it is used to indicate assignment and here  we are actually using it to denote inclusion however  you will see that you will not really be bothered by this it will become very clear by the context that is  what we mean actually the situation is really similar to our use of english language words so  for example  we might write rose is red when we write this  we really mean that rose belongs to the class of red things or the set of red things so  as you can see even in the english language  the verb is used to indicate equality that is perhaps the more common use but  it is also used to indicate some kind of say conclusion so  anyway instead of saying f belongs to theta of g  it is very common to say f equals to theta of g we never however  write theta of g equal to f this is never written just as  it does not make sense to say red  well i guess in poetic english it does make sense to say red is rose  but we never write this in computer science i will add one more note on the writing style so  i have been writing functions as functions by their names directly so  i might write something like f equals theta of g but  from time to time i might also write f of n is equal to theta g of n  refer slide time  28  34  if n is it is sort of understood  n is clearly understood as an unbound variable the argument  the possible argument that f can take so  these two really will be think of these as being the same this i might write it in this manner just to emphasize the fact that  f is a function but  if it is clear that  f is a function then i am it might be god to write it in this manner let me take one more example  refer slide time  29  07  let me define f 5 of n as 2 plus 1 over n so  what can we what class can we put this in so  it turns out that there is actually a nice class into  which we can put this and this class is simply the theta of 1 class so  here let us say g of n is always equal to 1 and then  we have to argue that in fact  f 5 belongs to theta of g  which is what i have written over here let us just do this just to make sure  we understand this so  clearly 2 is less than or equal to 2 plus 1 over n  which is equal to f 5 of n and in fact  this is less than or equal to say 3 and therefore  we have c 1 equals 2 c 2 equals 3 and this is true for all n so  we can have n naught equals 1 and we have these three constants  satisfying our basic definition of theta and therefore  we can write f 5 as belonging to theta of 1 so  theta of 1 is the class of all functions  which are essentially constant they may have some minor perturbations  but they really are like constants so  now we will come back we will come to our other two definitions our other two classes and we will define those so  the first class is o of g  refer slide time  30  53  so  this is a class of functions f  where such that where f is a non negative function and there exist c 2 and n naught  such that f of n is less than or equal to c 2 times g of n  for all n greater than n naught omega of g is the same thing as above so  it is f but so  f is non negative but  now we are worried about c 1 and n naught such that f of n is less than c 1 times g of n is less than or equal to f of n and we do not have anything on the upper for all n greater than n naught and let me just refresh you  what the theta of g definition was the only difference was that  we wanted there to exist c 1 c 2 and n naught such that  c 1 times g of n is less than f of n and is less than c 2 times g of n so  you can see that the class omega relaxes one of the conditions  which was present in the class theta and so it does o o relaxes the lower bound condition and omega relaxes the upper bound condition so  in omega the lower bound condition is present but  we are not saying anything about  whether f of n is bounded above by some g most of an algorithm analysis  we find it is easy it is reasonably easy to say that the time taken is at most something like this  at most this function so  for example in last lecture in the last lecture  we said something like if we look at this program and we count the number of iterations then  we can certainly argue that there are at most n cube iterations or something like that and therefore  the time taken has to be at most cubic in n there we later on did argue that  the time has to be at least cubic but  suppose we just had argued that it was utmost cubic then  we would have used this o notation so  we would have said that the time taken belongs to o of n cube so  let me write that down  refer slide time  34  02  so  if we know say the time taken as a function of n is less than or equal to say 15 n cube plus 17 plus 7 n square plus 35 or something like that then  we can conclude that t of n belongs to o of n cube if in addition  we prove that t of n is also greater than say 2 n cube plus 37 then  this would imply let us go back to our definition so  let me put it on top so  let us see so  here we are establishing that t of n is bigger than 2 n cube plus 37 or i can write this as  say 2 n cube is less than or equal to t of n so  which is exactly the condition that  we wanted over here and therefore  we could argue that t of n belongs to the class omega of n cube over here in the first case  in the first case we said that t of we know that of  n is less than 15 n cube plus 7 n square plus 35 and which i can write down as in fact  less than 35 plus 7 plus 15  so 57 n cube and that is really satisfying this condition and therefore  i can conclude that t of n is belonging to this class but  what happens as a result of both of these so  i have really established that  this t of n is bounded below by c 1 times g of n simply  this 2 n cube and it is bounded above by c 2 times g of n  which is simply this 57 n cube  refer slide time  36  29  so  as a result what has happened is that  i can conclude from both of these things that t of n is belonging t of n belongs to theta of n cube as well so  let me make this point again  because it is an important point the class o of g is the class of functions  which are bounded above by g so  if we know something about what is bigger than these  than the function that we are considering if we know a function  which is bigger then  we can say we can put it put the unknown function in this class  in this o of g class so  if you know an upper bound on a function then  we should be looking at expressing that upper bound as o of g if we know a lower bound on that function  we should be looking at expressing this knowledge as this f belongs to omega of g and if we know both upper bound and lower bounds in terms of the same function g then  we should write that this function f belongs to theta of g  refer slide time  37  48  so  as you must have already guessed the class theta of g is simply the union of the classes o of g  the intersection of the classes o of g and omega of g so  let us take some examples of o  refer slide time  38  13  this is something like 3 n square belongs to o n square 3 n square in fact  belongs to theta of n square and therefore  it certainly belongs to o of n square  because o of n square in fact  is bigger than theta of n square here is another example so  say 10 n cube plus 5 n plus 7 belongs to o of n cube similar logic  but something like 10 n cube plus 5 n plus 7 also belongs to o of n to the fourth why is this ? because  10 n cube plus 5 n plus 7 is less than or equal to i can certainly write this as being less than or equal to say 32 n to the fourth and that is all my definition of o really cares about so  this also belongs to this this function belongs to this class no surprise  you should be surprised by this because  we are really saying that g serves like an upper bound on this function and if i can  if n cube is an upper bound then certainly n fourth is an upper bound as well  refer slide time  39  45  let me summarize that f belongs to theta of g  should be read as f is nearly the same or let me write it as similar to g f belongs to o of g  should be read as f dominated by g actually  not dominated f is no larger than g sort of like less than or equal to  but it is not exactly less than or equal to because  we are ignoring constants just as this is sort of like equal to f belongs to omega of g  like y should be thought of as f greater than or equal to g but  again we are ignoring constants and also lower order terms so  we now have defined our three main functions classes  theta of g  o of g are also called big o of g and omega of g i have defined these classes in the context of the times taken by algorithms but of course  these are just plain old function classes and the functions could denote  not necessarily the time taken  but any old thing so  for example let me define a general function which just  it is the sum of n numbers  refer slide time  41  33  so  let me say s of n is equal to summation i going from 1 to n of i itself well you do know  from say some of the mathematics courses that you have done that s of n is nothing  but n into n plus 1 upon 2 however  getting into a result like this  requires some amount of cleverness  this result is a very precise result if you prove that s of n is exactly this  it is a very precise result but  sometimes you might say  you might not have enough time or you might not have enough cleverness to get on exact result like this  which is to say that s of n is exactly n into n plus 1 upon 2 but  suppose you might be happy with a weaker results so  you might want to know well  does s of n grow or does s of n belong to n square into the class theta of n square or does it belong to the class theta of n at first glance just by looking at this  it should it is not clear at all whether s of n belongs to the class theta of n square or whether it belongs to class theta of n cube or anything like that so  what i want to stress or what i want to give an example of right now is that  even without getting to this exact expression  you might be able to determine the class to which a function belongs and in fact  we are going to prove that s of n belongs to the class theta of n square  without actually calculating s of n precisely and this is an instructive example  because something like this will happen when we analyze algorithms so  s of n is equal to summation i going from 1 to n of i but  note that  this i is always going to be at most n and therefore  i can write this as summation i going from 1 to n of n itself but  what is this ? this is just n plus n plus n n times  because every term in this sum is n and therefore  this is nothing  but n square so  what have you established ? we have established that s of n is less than or equal to n square but  that right away puts s in the class o of n square so  this implies that s of n belongs to the class o of n square can we argue that s of n belongs to the class omega of n square ? that is  what we are going to do next so  again we observe something very simple so  s of n is summation i going from 1 to n of i now  i am going to ignore the first in our two terms so  this certainly is greater than or equal to if i ignore the first in our two terms  this is going to be i going from say n over 2 plus 1 to n of i itself but  now note that this i is always going to be at least n over 2  because it starts at n over 2 and goes all the way till n so  therefore  i can write this as summation i going from n over to 2 plus 1 to n of n over 2 itself so  term by term this series this sum is bigger than the corresponding term over here but  what is this ? this is simply n by 2 added to itself n by 2 times so  therefore  i will write this as equal to n by 2 times n by 2  which is n square by 4 so  i have argued that s of n is bigger than or equal to n square by 4 and at most n square so  we have bracketed s of n well before that  just once we argue that s of n is bigger than n square by 4 we can conclude that  s of n belongs to omega of n square just going back to our definition  we have proved that let us just do this once  refer slide time  45  59  so  we need to argue that f of n which is s of n now  is greater than or equal to n square by 4 and the c 1 is now 1 by 4  but that is we do not  we did not necessarily say we did not say over here that  c 1 has to be greater than 1 or anything like that any real number  any positive real number is fine there exist  i should write this as there exists c 1 on n naught positive all through all the c 1 naught that  i have written should be positive that is what  that is exactly what we have proved over here s of n so  we have argued that s of n belongs to n square as well and from this and this  what can i conclude ?  refer slide time  46  48  well from those two things  i can conclude that s of n must be theta of n square as well so  notice that in doing any of this i did not actually exactly evaluate s of n in this case evaluating s of n exactly is possible  but very often it is not as i said  it may require exceptional cleverness once in a while to exactly evaluate a function but  giving bounds on it is easy and the bounds on it  can be nicely stated in terms of the class notation that we have right now so  if you know an upper bound we can state it as s of n belongs to o of something if you know a lower bound  we can state it as s of n belongs to omega of something and if we know both  we can state s of n as theta of something in this manner  i would like you to prove so  let me write this down as home work prove that  say t of n if t of n is equal to summation i going from 1 to n of say i square then  prove that t of n belongs to theta of n cube the proof is really more or less identical  but you should but  again you should persuade yourself of this let me take one more example  refer slide time  48  34  so  let us look at the fibonacci series which is defined by f of n equals f of n minus 1 plus f of n minus 2 and f of 1 equals f of 0 equals 1 let me claim that  f of n is always greater than or equal to 2 to the power n by 2 this is also home work what can you conclude from this ? you can conclude from this that  f of n is an omega of 2 to the n by 2 let me make  one more claim well  let me not make that claim let me instead tell you the real result  actual result it is possible to show that f of n is theta of 1 plus root 5 upon 2 whole to the power n so  it is not exactly this number  but it is within a constant multiplier of this the nth fibonacci number is within a constant multiplier of this proving this exact bound  takes a lot more work but  by something really simple we have at least argued that f of n is actually going to grow at least as 2 to the n by 2 or in fact  i can write this as omega of root 2 to the n so  something like this is commonly called exponential growth some by a very easy logic  by very easy reasoning we can argue that the fibonacci number grow exponentially by more complicated reasoning and by reasoning  which involves essentially involves finding out a precise formula for the nth term we can get a much tighter results but  the importance right now is that our theta notation and our omega notation allow us to express our knowledge or lack of it  in a very compact manner so  let me summarize now  refer slide time  50  59  so  one we have defined theta  o and omega notation these capture the idea that  ignore constant multipliers consider n goes to infinity  which is equivalent to saying that ignore consider leading terms so  our time estimates of algorithms will be expressed using these notations the second thing that i want to mention is that  in fact this notation is just a general notation on functions and it can be used for other things as well and sometimes and it really allows us to express  what we know and what we do not know in a very compact manner so  even if we do not know even if we know a little bit about it then  we can put it in a certain class and that  this partial information that we have can be nicely expressed so  this is useful for thinking about functions in general thank you design and analysis of algorithms prof sunder vishwanathan department of computer science engineering indian institute of technology  bombay lecture  5 algorithm design techniques  basics we begin with discussion of some basic design techniques for algorithms we will start with fairly simple problems that many of which you may have seen but  we will hopefully see solutions to non trivial problems  which you may not have seen the first example  i would like to start with is finding the minimum element in an array  refer slide time  01  21  so  the input problem is find min the input is an array a and the output is a minimum element in a a element with whose value is the minimum this is a problem  which most of you would have see earlier  in your perhaps the first programming course may be later even in your course on data structures let us anyway go over the solution so  the standard solution is  you have you take a temporary variable there is a temporary variable  which contains the current minimum so  this contains the current minimum and it is updated at each step so  you start with the first element in the array and then  you scan the array element by element the first element then the second element third element and so on up to the last element in the array each time  you compare the array element with temp and update temp if necessary this simple technique actually is used is very powerful and used often  which is if your input is in the form of an array or a list  then if you can solve the problem for the first n minus 1 elements of the array can you solve it for the n th element ? this is the step  which is put in a loop so  let me just write this down so  the important step is this if a i is less than temp  then temp is set to a i this is the crucial step and this is usually put in a loop i varies from 1 to n  where let us say n is a size of the array and at the end of this temp will have  the minimum element in the array so  i will be when i discuss algorithms  often i may not write the full code in fact  i will not write the full code the idea is to give you the main ideas  behind the design of this algorithm i may not take care of some of the stray cases i may not initialize variables properly etcetera  etcetera so  all these programming details i will not get into the main idea is to  get the main the techniques  and the ideas behind the algorithm which solves the problem and you should be able to sort of take these ideas together and write the program  which actually works yes so  if you look at this  this is the level at which we will describe algorithms may be even less i could just say  scan the array element by element and update the current minimum as required  refer slide time  05  15  there are  so there is two things couple of things that i would like to point out so  this thing this design technique is often called well induction both recursion and iteration actually go with this i will explain this a bit more the second technique  the second sort of thing we will keep in mind is ordering and the third thing is to store value  already computed i must add store necessary this value so  let us go each of these one by one so  induction what you mean by induction is this you want to solve a problem your input has some size the array has say size n now  the one way to solve it is  supposing you can solve this problem for smaller values of input if this problem can be solved for smaller values of input  now can i extend these solutions to a solution for the bigger input if you can solve the problem for the smaller values of input  can you extend the solution to a bigger value of the input for instance  if i can solve a problem for all arrays of size n minus 1  can i solve this problem for an array of size n ? now  this step which takes you from n minus 1 to n is often put is the crucial step and once you come up with this step  we just put this step in a loop or you use recursion you first recurse on an array of size n minus 1  and then extend it to an array of size n or you scan the array one by one  element by element and you update the solution as you go along so  this is what i mean by induction and it is at the base of every algorithm design technique you can call it the mother of all algorithm design techniques you will learn some more fancier things later  but this is the very crux of design of every algorithms second point that  i would like to make is ordering the input looking at the input in the right order often helps in this case  it is simple you know  you look at the array elements in the increasing order of the array index in fact  you could look at the array in any order it really does not matter but  there are cases  we will see cases where ordering plays a very crucial role in solving the problem the third thing  which is also fairly simple here is to store some of the values that you already computed that is the third point  we want to make now  even in this case it is simple you just store the previous minimum  the variable temp the temporary variable which we had so  these are values that you would like to use in future almost every algorithm design technique that  we will study are a combination of these three some of them will use just induction and storing whole values some of them will use all three some of them will just order the input and use induction so  these three are things that you must keep at the back of your mind  when you design any algorithm let us analyze this algorithm  refer slide time  09  23  so  here is the here is an analysis every algorithm that we design  we will analyze analysis is as important a part of this subject as design and we would like algorithms to be as fast as possible  in the worst case so  all our analysis pertain to worst case and we would like to design algorithms  which are as fast as possible so  in this case we look at every array element one and we make one comparison for array element this leads to  we make n minus one comparison we do not really compare anything with the first element but  with each of these subsequent we make one comparison n is the size of the array so  we make n minus 1 comparison along with comparison  we need to we also store this new value in temp but  the number of times we do this exactly equals the number of mean it is let us say one more than the number of comparisons  we make so  if you are not really worried about constants  then you can just focus on the number of comparisons as long as i bound the number of comparisons  the other small operations that i do  like incrementing the index variable of the array or storing the value in temp they all are of the same order as the number of comparisons so  the time taken here we would say is order n the time taken is order n  but our focus is just on the number of comparisons i think the question that one can ask and one should ask is this the best ? can i find the minimum element in an array in using less than n minus one comparison ? well  we just think about it or even  if you do not think about it i guess  most of you will jump up and say  of course you need n minus 1 comparison that is the sort of first reaction that  most people will have without making n minus 1 comparison  how can you find the minimum ? why is this so ? can you logically argue that  you actually need n minus 1 and you know you can not do with less than n minus 1 this argument is easy  but not absolutely trivial let us see so  why should we make n minus 1 can we do it with less ? well  every element you have to look at least every element in the array must be compared to something or the other if it is not compared then  you may actually make it the minimum it could be the minimum if you had output something else as a minimum then  this could have been made the minimum or you can easily make some other element of the array the minimum so  without comparing if you do not compare an element then  you can not tell the minimum element in an array now  this gives you this does not give you n minus 1 it gives you n by 2  assuming n is even so  at least n by 2 comparisons are needed the reason every element must be compared  must be in some comparison what i mean is  every element must be compared with some other element this can be done with n by 2  by the way so  the first element is compared with the second element the third element is compared with the fourth element and so on the fifth element is compared with the sixth element and so on so  with n 2 two comparisons  if n is even or n by 2 plus 1  if n is odd well  ceiling of n by 2 comparisons are all that is necessary to sort of satisfy this condition that  every element must be in some comparison now  clearly n by 2 is not the right answer n minus 1 is the right answer and why is it that 1 is n minus 1 ? so  here is an argument see initially  when you have not made any comparison there are n candidates for the minimum each element in the array can actually be a minimum you do not know  which of these n elements are minimum now  when you make a comparison  you can get rid of only one of these candidates when you compare  let us say at some stage you have candidates x 1 x 2 up to x k  some k elements are candidates for the minimum each of them based on the comparisons that you have made  previously each of them is equally likely  i mean each of them can be a minimum now  if you compare two of these candidates you can get rid of one whichever one is smaller  that still remains a candidate whichever one was larger  that no longer remains the candidate but  you can only get rid of one candidate so  with each comparison i can only get rid of one candidate i have n candidates to start with so  i need n minus 1 comparison this is actually a complete proof  though a bit hang wavy you can make it more rigorous also here is one more way of looking at it supposing  you draw the following graph so  initially we have i have all these nodes let us say the elements x 1 x 2 up to x n these are elements and the array and also vertices in our graph let me just put a circle around these two  indicate that these are also vertices now  when you compare x i and x j here is x i and here is x j  i draw an edge between these two now  when you compare let us say x 1 and x 2 and x n  i draw this edge now  x j and x n are compared i draw this edge and so on as you make comparisons  i keep drawing these edges so  once your program ends terminates  you have done all these comparisons now  i look at this graph and i look at each connected component in this graph if there are more than one connected components in this graph  then i will not be able to tell the minimum now  in each connected component i know which is the minimum ? there will be one which is a minimum  but i can surely give values to these so  that i can pick the global minimum from  any one of these connected components here  this is arguments that after the comparisons are over  once you finished all comparisons i better have one connected component this means from a discrete structure ? s class  you know that you need n minus 1 edges so  this is just the same argument both of them have the same idea behind both for instance  here you start with n connected components and each time you add an edge  you can decrease the number of connected components by 1 at most 1 so  it is the same argument so  you need n minus 1 and this sort of simple scan of the array does it with n minus 1 comparison let us look at a slight variation of this problem now  i want to find not just the minimum in the array  but also the maximum  refer slide time  18  29  so  this problem is called max min so  the input is in array a and the output is the maximum and the minimum elements in a so  i want both the maximum and the minimum element now  if i just wanted to find the maximum  the procedure is clearly the same as the one for the minimum element what if i want to find both the maximum and the minimum ? well i could first find the minimum and then  i can find the maximum how many comparisons does this take ? well n minus 1 for the maximum n minus 1 for the minimum and that makes it 2 n minus 2 so  the number of comparisons that a na ? ve algorithm so  this is the number of comparisons that  the na ? ve algorithm makes we can ask the same question is this the best ? you can try the previous argument that  we had of connected components and so  you can show that you need m minus 1 that is fine  but when you see more than  that is very difficult to prove it is not absolutely impossible  but it is difficult and if you try to certainly increase it to 2 n minus 2  it is impossible you will not be able to prove it let us look at some small values supposing i have four elements  i have let us say x 1 x 2 x 3 and x 4 the na ? ve algorithm took x 1 compared with all of them found the minimum and then we were done then  we took again took x 1 we compared it with all of them and while maintaining the maximum  the temporary maximum now  many of these comparisons are repeated so  you see that many of some of these comparisons that you make  when you on roll the whole thing out  are repeated so  our aim is to sort of get rid of these unnecessary comparisons now  in four elements here is what you can do i first compare x 1 and x 2 so  supposing x 1 is less than x 2 so  this is the first comparison x 1 is less than x 2 now  i compare x 3 and x 4 supposing x 3 is greater than x 4 this is my second comparison now  where do i find the minimum ? i mean  how do i find the minimum ? the minimum clearly is either x 1 or x 4 it is the smaller of x 1 and x 4 so  i compare these two and find the minimum similarly  i compare these two and find the maximum so  how many comparisons i have made ? i have made 1 2 3 and 4  so four comparisons what does our old algorithm say ? it says 2 n minus 2 n is 4 so  this is 6 when n is 4 so  we seem to have done certainly  better than 2 n minus 2 when there are four elements  we have certainly done better than 2 n minus 2 and well the trick was  once we found the minimum and maximum between x 1 x 2 and x 3 x 4 then  for the minimums i only need to look at x 1 and x 4 the smaller on the left hand side and the smaller on the right hand side i do not have to bother about the bigger one and similarly for the maximum so  this trick can be applied recursively well  it is certainly worth trying and let us do it so  what we do is this  refer slide time  22  57  here is the let us say i have you have an array of size n so  let us divide this into two parts recursively find the maximum in this part let us say max l this is the left part and that is the right part and min l recursively  find the maximum minimum on the right hand side so  that is max r and min r now  how do i find the maximum and minimum ? well  i need to compare these two to find the minimum i need to compare those two to find the maximum so  here is an algorithm that seems natural i divide this into two parts divide this into two parts let us say two equal parts two equal halves halves are always equal i find the maximum and the minimum on the left hand side the left half  i find the maximum and minimum in the right hand side now  i compare the two minimums to output the minimum of the array i compare the two maximums  to find the maximum of the array how many comparisons does this algorithm take ? let me write down the algorithm but  in future with this explanation you should be able to write the algorithm so  divide into halves let us say left and right then  recurse on both parts on the left and on the right and the answers are max l and min l and max r and min r then  put these things together  to get the minimum and maximum and then  compute final solution from the solution of the two parts well  i have written the essence of the algorithm without really writing details i hope  you can fill in the details define procedures and write down recursive calls and you know  do these two comparisons and output the minimum maximum do how many comparisons does this take ? that is the question  we need to answer  refer slide time  25  53  so  let us say t n is the time taken by max min on arrays of size n it is the time taken by max min on arrays of size n then  t n is there are two problems of half the size you solve two problems of half the size there is 2 n by 2 plus ; two more comparisons  one between the 2 maxs to get the new max  one between the 2 minus to get the new minimum these are this is for the recursive call so  you call the left hand side  that is n by 2 right hand side n by 2 and then 2 well  if n is odd i would have a ceiling and floor somewhere but  let us not worry about it for the time being let us assume that  n is even you can assume n is the power of 2 we also know that  t 2 is 1 for two elements  i can find it in one comparison so  what is the solution to this recurrence ? let us see so  the easiest way to solve all this is to check  how this recurrence behaves so  t n is nothing but 2 and now  i open this out this is 2 t n by 4 plus 2 plus 2  which is 2 square t n by 2 square plus 2 square plus 2 you can right this down  once more and essentially we want to see how  what pattern this follows ? well  it is not too difficult to guess what the pattern is ? the pattern is this  refer slide time  27  55  so  t n is 2 to the i t n by 2 to the i plus 2 to the i plus 2 to the i minus 1 and so on all the way up to 2 so  now we set n by 2 to the i to be 2  because we know that t of 2 is 1 then  we have t of n is 2 to the i this t  this becomes 2 so  t of 2 is 1plus 2 to the i plus 2 to the i minus 1 and so on up to 2 so  this is nothing but 2 to the i plus 1 plus well 2 to the i minus 1 and so on up to 2 you can check that this is nothing but we also know that 2 to the i plus 1 is n and i hope you can solve this i will leave it for you  to solve this this is nothing but n and this sum  you will get as n by 2 minus 2 if you sum this up  using the usual geometric series and use this fact  that n is 2 to the i plus 1 you will get that  this sum is nothing but n by 2 minus 2 well  put this together you get t of n is 3 n by 2 minus 2 you can check that this  when n is 2 this is 1  which is what we want and this also satisfies the recurrence that we had the recurrence was let me refresh your memory the recurrence was t n is twice t n by 2 plus 2 so  if i put t n equals 3 n by 2 minus 2  this satisfies the recurrence you can prove that t n is this   refer time  30  12    well  so the number of comparisons that we seem to make using this method is 3 n by 2 minus 2  which is certainly better than 2 n minus 2 we seem to have done something fairly mechanically and we seem to have improved the number of comparisons made  quite drastically so  this technique is called divide and conquer  is used by the british in the last century  i mean last century and even before that we will put it to good use  in designing algorithms so  let me write down the main steps of this technique  refer slide time  31  02  the first step is to divide the problem into  i will say two parts often we will want these parts to have equal sizes  often of equal sizes the next step is recurse on each part so  you recurse on each part and solve each of them both the parts  if there are two and the final step is put these solutions together put these solutions together  to get a solution for the original problem often  you can just do this blindly in fact  for the max min we could have done it blindly take this array of size n  divide that divide this array into two arrays to size n by 2 find the maximum and minimum on the left array  the maximum and minimum on the right array and once you have these two solutions together  now you find the maximum of the whole array comparing the two maximums find the minimum of the array by comparing two minimums again the essence is this induction in the sense that  if you could solve problems of smaller size which is what you are doing in this recursion you are somehow putting these together to get a solution  for the big problem how you will find these small problems varies from problem to problem let us look at another problem  where we will apply this method blindly and we will see what we get ?  refer slide time  33  14  this problem is to find both the minimum and second minimum the minimum is the smallest element in the array the second minimum is the next one  the second smallest element in the array so  your input is an array and you are going to find both the minimum and the second minimum the usual way you would do it is  you first scan the array and find the minimum now  you again scan the array and find the second minimum the other way to do it is  to have two temporary variables temp 1 and temp 2 in temporary 1  i store the current minimum in temporary 2  i store the current second minimum now  when i get to an array element i let us say the ith element i compare this first with the minimum then  with the second minimum that i have and based on the result of these two comparisons  i update minimum and second minimum now  this will take we have seen how many comparisons this will take it is 2 n minus 2 as it was in the max min case and let us just apply our divide and conquer paradigm  blindly to this problem and see what you get so  how would we do this ? so  here is the array the array is of size n i divide this equally into two paths i find a minimum and second minimum here so  let us say min left and the second min left  min right and s min right so  i have found these four values and now  i want to find the minimum and second minimum for the entire array the minimum is not a problem so  i just compare min l and these two values and i can output the minimum the smaller of these two is the minimum now  what do i do about the second minimum now  supposing min l was smaller than min r  so without loss of generality this min l so  assume min l was less than min r  which means at this point min l has been output now  what are the candidates for the second minimum ? clearly  min r is still a candidate for the second minimum second minimum of the left hand side s min l is also a candidate for the second minimum but  one of these elements we have sort of thrown out  which is s min r this does not figure in the picture at all so  we need exactly one more comparison to get the second minimum  which is supposing this is true then compare the second minimum on the left and the minimum from the right so  you need to compare just these two elements and you can see that  the minimum of these two will give me the second minimum the minimum of the entire array i get by comparing these two minimums  the minimum the left hand side minimum the right hand side minimum and the second minimum i can get by comparing the minimum element  which lost the first comparison which was larger and the second minimum of the element that one so  that will give me the second minimum so  how many comparisons have we does this take well  if you write down the recurrence this seems to be very similar to the previous one so  what is ?  refer slide time  37  23  so  if t n is the time taken by the algorithm  we have two sub problems each of size n by 2 that takes time t of n by 2 and then  we have two more comparisons  one with the two minimums and one to find the second minimum we also know that  t of 2 is 1 if i have two elements  one comparison suffices to find both the minimum and second minimum and these set of equations are exactly the same as the set of equations  we had before so  the solution is t n is 3 n by 2 minus 2 so  the number of comparisons we make is 3 half n minus 2 and it is not 2 n minus 2 it is much less one can ask  is this the best ? can we do better than 3 half n minus 2 ? this question can be asked both for max min and also for min and second min well  it turns out that these two problems behave differently for max min 3 half n minus 2  is the best we can do so  3 half n comparisons is the best we need 3 half n comparisons while in this case  when minimum and second minimum you can do with actually less the divide and conquer sort of paradigm gave us 3 half n  but that is not the best so  why ? so  let me give you a reason why you can do better here to better this  you need to understand a bit more as to how this algorithm works ? let us unfold the recurrent  the recursion out and see what this looks like  refer slide time  39  18  initially  i have array elements x 1 x 2 x 3 x 4 and so on  all the way up to x n what we do is  we just divide it into two parts we divide it down the middle and then  you recurse on these two on the left half  we divide it again into two halves  recurse divide recurse divide recurse now  we come down all the way down to  when they avail as size 2 this is when the comparisons start happening the recurrence sort of bottoms down  till you reach arrays of size 2 now  i will find the minimum of x 1 and x 2 that goes up the minimum of x 3 and x 4 is pushed up also the second minimum is pushed up but  let us not worry about that for the minute the maximum case also it works very similar you put the max min  max mins are pushed up at each level we just focus on the minimum element so  the minimum element is pushed up from here  the minimum element is pushed up from there at the next level  i will compare the minimum of these two this contains a minimum of x 1 x 2 this contains a minimum of x 3 x 4 this contains a minimum of these two  which is actually the minimum of these four and so on this would be a bigger tree and so on all the way up to the root and this root  you can the minimum is known so  for instance here x n minus 1 and x n are compared this is the minimum at the next instance  you compare it with the other two this would be x n minus 2 and x n minus 3 and so on all the way up to the root  where the minimum is known now  this looks like if n was say power of 2 this looks like very familiar complete binary tree so  there are log n levels there are n leaves there are log n levels and in each level  we sort of have some sort of minimum and some portions of the array and these are pushed up now  where was the minimum element ? the minimum element sits somewhere in this array and at each stage  it is pushed up it sort of wins  its comparison each time at each level of this tree and it finds its way to the top somewhere with perhaps  came from the left may be it came from the right  came from the left came from the left and so on so  it does traverse some root all the way from the root node to a leaf and this is where the minimum element resided now  what can you say of the second minimum element now  well the crucial sort of observation that you need to make to speed up the algorithm is that  at some stage the second minimum must have been compared with the minimum element this is absolutely crucial if the second minimum element were never compared with the minimum element then  you really do not know which of these two is the minimum  because in each comparison the second minimum one  it was smaller than every other element so  was the minimum element which of these two is minimum  to know that you must have compared the second minimum element with the minimum element now let us look at this picture here is the picture how many elements did the minimum element  you know win against how many elements were compared with the minimum element ? if you look at this picture and you follow this  at each stage in this path down from the loop to the leaf  the minimum element was compared with exactly one element the length of this path is log n so  the minimum element was compared with at most log n elements in this tree  which means log n elements in this array were compared with a minimum element and one of these log n elements  remember must be the second minimum so  to find the second minimum all we do is this find the minimum using this tree you can do it recursively  if you want once you find the minimum element  collect all elements that the minimum element one against was compared against if you have this tree in front of you  you can certainly go down the tree and figure out  which were these elements among these elements  find out which is the minimum ? and that will give you a second one there are log n elements  refer slide time  44  24  \ so  initially you made n comparisons may be  it is n minus 1  n minus one comparisons to find a minimum and then  you need about log n minus 1 comparison more to find the second minimum this then is actually optimum  though we will not do it in this course there is an argument  which shows that you need n plus log n but  it is surprising that you can actually do this in n plus log n and this problem in this way  it differs from the previous problem a straight forward application of divide and conquer does not work you need to use some more intuition you need to understand a problem a bit more come up with new ideas and that is what algorithm design is all about often  there are problems which are hard you really do not know what to do ? and when you come up with a smart answer to an algorithm  you feel really you feel nice design and analysis of algorithms prof sunder viswanathan department of computer science engineering indian institute of technology  bombay lecture  6 divide and conquer ? i  refer slide time  01  11  the three of them are first 1 is induction then  ordering the input and the third is storing old values so  let me sort of let us talk about all these three once more induction essentially says  supposing you can solve the problem for smaller inputs how do you solve it for larger inputs ? so  this is the first term design principle so  supposing you know  you can put together  these solutions for these smaller inputs and somehow generate a solution for the larger input then  your algorithm is clear you sort of recurse on the smaller inputs solve them and when you get the solutions back you put them together and get the solution to the larger input that is induction  you could use either recursion or it could even be once you know what the smaller inputs it could even be   refer time  02  42    the next point is ordering the input  which is you look at the input in some order for instance  if it is an array you look at it let us say  in increasing order of indices for other structures it is could be different if it is a list again  you will have to look at it element by element for other data input and other data structures  you could this could vary the other thing is store values  which you may use later on if you are not going to use a value  which you have computed later on then you can of course  discard it otherwise  this very simple principle will help us design algorithms in the future good so  we have seen  some algorithms simple algorithms for simple problems using these  refer slide time  03  44  the paradigm  that we considered last time was divide and conquer so  the basic idea was this  if i have an input let us say an array but  it could absolutely anything divide it into two parts solve for the left  solve for the right say and then  put the solution back together so  you divide input solve each part and put them together to get a solution for the big problem and well often it pays to divide it into equal parts we will in fact  see an example why this is so ? but  even in the previous cases for instance max min or finding the second minimum the two examples that we saw  you could try breaking at breaking the input up into unequal pieces and then  try and solve the recurrence and see  what answer you get so  you should i sort of encourage you to try this out so  the next problem  we are going to consider is the very familiar  should be very familiar to all of us it is sorting  refer slide time  05  22  only we are going to look at it now from the new sort of design principles  that we have learnt and we are going to try and apply those to sorting so  let us take the simplest sort of case you have an array an array of n elements and you would like to sort the elements in increasing order let us  assume that elements are distinct this is not going to change any of our design principle or that or the algorithm we come up with  but it will just  it is just to keep your mind less cluttered so  you have an array of let us say  n elements and you would like to sort them in increasing order let us put the first design principle into practice supposing you can solve it for a smaller array  how do you extend it to a bigger array ? and the most natural smaller arrays let us say  the first n minus 1 elements supposing  i could solve this problem of sorting the array for the first n minus 1 elements how do i  now extend it to the bigger array so  here the problem  here is what the input looks like this will be first n minus 1 elements and this is the last element so  you first sort this is by recursion let us  say and now i want to place this in the right position now  i guess most of you know binary search by now so  the position is very easily identified so  let us say  this is now sorted so  this portion is sorted and this position now is identified let us say  they are you do a binary search and identify exactly where this goes in well now  what you have to do is to insert this in the right position you have to move  you have to make essentially make space so  from this point onwards  you need to move everything one step to the right every element here  you have to move one step to the right move one step and then insert the element in the right order so  here is an algorithm so  let us go over this again you recursively  sort the first n minus 1 elements figure out where the last element sits and then put it in place this is called insertion sort so  in you previous courses  you have dealt with this algorithm you have learnt insertion sort there people i guess told you what insertion sort is all about here  we are trying to understand  how it is that people come up with these algorithms so that  when you are faced with a new problem  you can come up with an algorithm of your own that is the idea so  this is how you come up with an insertion sort it is just putting you know the principle  we had into practice   refer time  09  03   how much time does this take well  if we just look at number of comparisons if just want to find out the number of comparisons the last element  you use the number of comparisons is log n  because you are just doing a binary search so  and every other element use less time it was for the i th element it was log i the time taken was log i so  the total time is in fact  roughly order log n  order n log n i mean it is order log n per element  there are n elements and it is n log n but  the mains step here  which takes time is not comparison  but this movement let us  look at the last step the last step says  once i find this place i have to move everything to the right now  this place could actually be the beginning of the array  in which case you are going to shift the entire array by 1 so  while you just took log n time to find the place to move elements  you may take you know n units of time this is the expansive step in insertion sort and if  implemented in this in this manner  each time you take order n steps for the i th step  you take i units of time to move it to the right you may take i units in the worst case  in which case the time will be order n square so  i will leave it for you to see if you can use better data structures so  that you can sort of avoid this movement but  implemented this way insertion sort takes order n square time though the number of comparisons here is still n log n let us  put our other design paradigms into play into practice one was to divide the array equally work one each piece and then put them together let us look at this  refer slide time  11  15  so  here is my so sorting so  this is the second algorithm we are going to try so  here is my array you divide it into two equal halves  two halves sort each piece  sort each piece and then you have to put these things together so  let us  write this down so  divide into two pieces sort each piece and then put them together so  the algorithm is clear  except for this step what does it mean to put them together ? so  far it is okay dividing into two pieces no sweat  sort each recursion you recurse on each piece you sort each well  here is the crucial step that we need to implement  which is putting two sorted arrays together let me  also say that this you could work with other data structures instead of arrays you could have list and you can apply the same design principles to list too in fact  we will do this with arrays but  you it is really does not matter so  when i mention array  you could you could put a list there equally well especially  for this method  so we divide into two pieces we sort each of them by recursion and then we put the solutions together what does this step entail ? well  this let us focus on this step because  once we implement this the algorithm is ready and you can program it at your leisure so  this step entails the following i have two sorted arrays  we sort it i need to combine this to get one big sorted array this is what i want i have these two pieces  which are sorted this piece is sorted  so is that piece we need to put these two sorted pieces together to get a big sorted piece so  this is the problem that we would like to solve so  this is called merging two sorted arrays or lists it does not matter  refer slide time  13  54  so  we are given elements these are sorting let us say  sorted in ascending order and i would like to build a bigger structure  which is which contains elements in both of them and which is sorted so  i guess the procedure should be fairly natural and most of you should have gotten this by now which is well  what is the smallest  so here is my new list or array this is my output what is which element will occur will occupy the first position well it has to be the smaller of these two elements so  what i do  i first compare the first elements of these two lists or arrays or sub arrays and put the smaller of them here so  i can have two pointers or two temporary variables  which point to some places in the array i compare these two put the smaller one here and move that corresponding pointer let us  call this a and b  supposing a 1 is smaller than b 1 then c of 1 i should assign the smaller of 2  which is a 1 and i should move this pointer so  let us call these two pointers something so  small a and small b so  c small c is small a and then i need to increment so  increment so a  which is pointing here will point to the next element and so on so  i just keep doing this at any stage so  what is the generic step at some stage i have these arrays a and b  refer slide time  16  24  i have pointers small a and small b and then i have pointer small c i compare these two elements  put the smaller of them here and increment the corresponding pointer this is the algorithm so  i just essentially  somehow scan these two and keep filling in these elements in this big array so  this is merging i hope you can write code  if need be if the idea is very simple and if they are less of course  it is it is trivial  you just you have these two pointers you sort of compare these two values put the new value in a new list and increment this pointer that is the way it is done and arrays if you want to you may actually  have sub arrays of the original array  where you are doing this work you could have a temporary array  where you create this merge list and then write it back into the original array so  this is the algorithm so  the merge step is also now done this is the algorithm how much time does this take ? this algorithm is called merge sort by the way so  it is called merge sort and how much time does this take let us go back to this   refer time  17  52   scribble piece of paper so  here is merge sort it says  divide into two pieces sort each of them and put them together this is essentially  essentially merging two sorted sequences into one big sequence that is the third step the divide step well does not take any time sorting each is a recursive step we sort on roughly half the array and we need to figure out how much time does it take to put these things together good so  what is the time taken ? supposing t n is the time for an array of size n  refer slide time  18  43  by time again  we will focus on the number of comparisons  because even here every other operation is big o of number of comparisons so  the number of  if i can bound the number of comparisons  the total time is a constant times the number of comparisons so  please check this for instance moving the pointer etcetera  etcetera  etcetera and if  you have list copying one list to the other etcetera they are all bounded by the number of comparisons so  i can just focus on this one quantity  which is number of comparisons so  let t n be the number of comparisons that we make  we would now like to bound the total number of comparisons needed for an array of size n well the divide step does not take any time so  but there are two sub problems of roughly equal size  so that that gives you 2 t n by 2 again  we will not worry about floors and ceilings for ease of calculation and then  there is a merge step how much time does merge step take on an array of size n so  this is something we need to do so  what is the merge step on an array of size n  refer slide time  20  20  the merge  i have two arrays of size n by 2 two sub arrays of size n by 2 or two lists of size n by 2 and i merge this to get something of size n how much time does this take ? well  there are many ways of figuring this out so  let me tell you one way of doing it this is a smart way of doing it and then  i will tell you a way  which is just using the  we will again design this algorithm using design principles  that we have already followed and then analyze it that way also so  we will do it two ways so  what is each step ? each step you compare two elements and an element gets filled in the array c the crucial sort of observation here is that each time you make an  each time you make a comparison one element in c in this new array  the final sorted array gets filled once again  one for each comparison an extra element a new element gets filled in the total array how many elements get filled ? the total number of elements that gets filled into a new array is n so  the total number of comparisons you make is utmost n let me say  state this analysis again  because this is this is quite important each time you make a comparison an element of c gets filled so  if i make k comparisons  then i fill k elements in c the total number of elements in c is n so  the number of comparisons i make must also be bounded by n so  the total number of comparisons is n the reason is for each comparison  we add an element to call this array c from these two smaller arrays a and b  i pick an element and add it to c the total number of elements in c is n so  the total number of comparisons i make is also n let us  do this merge differently so  that total time now is n  we now this and we can now  write the recurrence and we will solve it that is fine but  let us again do this merge a bit differently so  we are now going to apply our design principles  which is induction like a merge to smaller arrays how do i merge larger arrays ?  refer slide time  23  40  so  let us look at this again so  here are my two arrays   refer time  23  46    now  i know that the first element in c is the smaller of these two elements the first element here is the smaller of these two elements so  let us say this is smaller now  i look at the input without this the input now consists of one less element from here and some elements from here now  the sizes need not be same so  i can supposing i solve my problem for this smaller input can i solve the problem for the larger input ? well  the answer is should be you know vociferous yes it can be solved this is the smaller of this shaded is the smaller of these two and now  i have two arrays here so  the total size is smaller one element i have removed so  these two i merge to get this portion and the smaller element i put here  this goes in here and recursively  i can sort of merge these two to get this portion so  this is the other way of doing this here  you would naturally use recursion and if not iteration at least initially  may be later on you will see that this can be implemented iteratively and you could put them together iteratively so  now how much time does this take ? if there are two elements so  let us write the recurrence here so  t  so this could be n and m  because they could have different sizes so  if it is  let me not use n here let us say m 1 and m 2 t of m 1 m 2 is what  well one of them decreases by 1 we do not know which one so may be  i can just write a recurrence on the sum of these two sizes so  i can actually  write it on n this n stands for the sum of these two sizes  which is the total number of elements in the input this is nothing but t n minus 1 plus 1 where does this 1 come from ? this is two  when i have two lists for the first comparison i make i make i compare these two elements remove the smaller 1 and apply recursion this 1 comes for the first comparison i make and you can check that t n the solution to this so  t of 2 is 1 the solution to this is t n equals n minus 1 so  to merge two arrays the sum of two sizes is n the number of comparisons is n minus 1 so  these are two ways to do this merge when you write the code actually  the comparisons they  the both the algorithms do will be  if you unroll the recursion you will actually get the iterative process back so now  we can go back to merge sort and look for the what is the time that we need  refer slide time  27  19  so  it is 2 twice t n by 2 plus order n so  in fact  well i just put n here for simplicity you can put a constant times n and calculate this  it will make no difference these constant will just come out of the calculation so now  we need to solve this recurrence i guess most of you would have hopefully do know how to solve this let us  anyway do this so  t 2 is 1 well  i should strictly write t n is less than or equal to 2 t n by 2 plus order n if i am writing comparisons for the simple reason that  if i have two arrays whose let us say  they are not of equal size then i could use less than n in this case  it is n and it is equal but  often you should sort of check whether  this should be equal to or less than or equal to in this case  it is in fact  equal  refer slide time  28  39  and t n is twice t n by 2 plus n so  this then is so let us open this out  twice t n by 2 square plus n plus n so  this t n by 2  i have replaced i have again used this recurrence i am sorry so  this should be n by 2 i have just used recurrence i use the same recurrence on t n by 2 so  that will be twice t n by 4 plus n by 2  but just using this with n by 2 in place of n so  that is what i get so  what is this  this is 2 square t n by 2 square plus n plus n so  if i do this i times  well you could do it once more to see what the pattern looks like if i do it i times  i get 2 to the i times t n by 2 to the i  you can plus there will be a number of n so  how many times will i have n ? s here  it will be i times n  this you can check you should do it once more  you will get 3 times n so  here i have 2 times n if i do it once more i get 3 times n the next step you get 4 times n so  you check  you sort of guess that this is what is going to after i steps and you can actually check this by induction on i once  you have guessed this you can check that t n is in fact  equal to this by induction on i that i will let you know now what  well we continue till n by 2 to the i so  let me let us start here  refer slide time  30  58  now  we know t of n is 2 to the i times t n by 2 to the i plus i times n now  i know t of 2 is 1 so  i will let n by 2 to the i to be 2 so  then what do i get if i choose an i  so that this is true then  i get t n is well 2 to the i is n by 2 this is t by t of 2  which is 1 plus i times n what is i well  i know 2 to the i plus 1 is n so  i is log base 2 of n by 2 this is what i is  so plus n times log base 2 n by 2 this is order n log n so  the time taken by merge sort is n log n there is one more way to do this divide and conquer business on arrays to sort and it in fact  gives rise to another well known algorithm  which you may have studied let us see  what this idea is so  here is the array  refer slide time  32  37  i still want to do divide and conquer so  i still want to divide the array into two parts somehow work with each part and then put the solutions back together but  the way i do  it will be slightly different in this case so  what i do is this so  what i do is in the divide part  i   refer time  33  02   start rearranging the elements of the array so  i now have two parts to the array but  these are not divided as in the previous case remember in the previous case  we just picked up the array and just divided arbitrarily into two parts this time we are going to be more careful what we would like is every element landing up in the left is smaller than every element landing up in the right so  there is a x here and y here i know that x is less than y again  every element in the array must occur in one of these so  somehow  i have divided this array into two parts left and right so  that elements in the left are smaller than the elements in the right this somehow we have done this now  what we do is just recurse on these two parts so  you sort the left separately you sort the right separately let us see  what happens once you do this once you do this  i claim that the entire array is now sorted why is that so ? well this portion is sorted right this left portion is sorted this right portion is sorted i also know that every element on the left is smaller than every element on the right so  the largest element here is smaller than the smallest element here so  when i look at the entire array  you can check that the entire array is now sorted so  this is also an example of divide and conquer the divided step is where we did work in the divide step  which i have still to specify you somehow divided this array into two parts left and right so  that elements in the left are smaller than the elements in the right this was the divide step now  it is recursion   refer time  35  06   sweat you just recurse on these two parts and recursion does it for you and you sort these two putting things together is trivial you just have to do no work both of them are sorted  when you put them together in fact  the entire array is sorted the only thing we need to figure out  it how to divide the input so  that everything on the left is smaller than everything on the right well  some of you may have noticed that  this is an algorithm that you have seen before and it is called quick sort again  let me sort of emphasize that earlier on you were told what these algorithms are you were given code for   refer time  35  54   while this is quick sort  that this sorts an array our emphasis here is to see  how do people come up with these algorithms how does somebody come up with an algorithm called quick sort well  this is how they come up with so  you start with divide and conquer at the back of your mind and you figure out  how do you put this paradigm into place so  now how do people  how do we split an array into left and right well  you pick a pivot  xome element in the array  which you call the pivot left consists of every element  which is smaller than the pivot and right consists of every element  which is greater than or equal to the pivot so  the smallest element on the right is the pivot and you know that the left of the array every element is smaller than the pivot so  that does it let me just write this down so  that we have quick sort in front of us  refer slide time  37  03  so  pick a pivot this is quick sort pick a pivot divide into two parts smaller than the pivot elements let us  call this p pivot p and elements greater than or equal to p these are elements less than p and elements greater than or equal to p these are the two parts recurse and put them together that is the last step well  unlike the difference between merge sort and quick sort is essentially this in quick sort this is where you spend most of your time putting them together is easy in merge sort dividing into two parts is easy putting them together is where you take time so  this is the this is this is quick sort how much time does quick sort take ? well  it depends on which element you pick as a pivot so  let us see  what really happens so  supposing you pick the i th element what do i mean by i th element  means in the sorted order it is the i th element so  let me make a definition here  which we will use later on  refer slide time  38  53  the rank of an element is it is position in the sorted order remember recall that  we are assuming that each element in the array is distinct so  each element has a distinct rank if there are i elements smaller than x  say x is an element i elements are smaller than x the rank is i plus 1 the minimum element has a rank 1 and the maximum element has a rank n  which is the size of the array so  the rank of an element is it is position in the array so  if everything depends on the rank of the pivot so  if the rank of the pivot is i then  the time taken there will be i minus 1 on 1 side and n minus i plus 1 on the other side right so  is t of i minus 1 plus t n minus i plus 1 plus the time taken to partition the array what was this time ? how did we partition the array ? well  we compared each element to the pivot put the ones on the left on the  put the lesser ones on the left and the larger ones on the right so  we compared every element to the pivot so  number of comparisons we made is n minus 1 for ease of calculation  we will just say this is less than or equal to n it will not bother us much what is the constant between   refer time  40  54    so  if the rank is i then it is utmost t of i minus 1 plus t n minus i plus 1 plus n this is the time taken to partition the array  because every element was compared to the pivot ones exactly once so  what is the solution to this recurrence clearly sort of it depends on i it depends on what i is and various values of i it varies  for instance if i picked the minimum element if i picked the minimum element then  then what happens then i was 1  oh sorry this should be minus 1   refer time  41  47   n minus i minus 1 or it should be n minus i plus 1 that is what fits in here  n minus i minus 1 so  if i were 1 then there is nothing on the left and there are n minus 1 elements on your right hand side there is something wrong here this should just be n minus i there is no 1 here this is exactly n minus i i have the pivot i have i minus 1 here and i have n minus i elements on this side the pivot sits at the center so  the right hand side had size n minus i so  if i has chosen the minimum element  then i get t of n to be t of 0  which is 0 plus t of n minus 1 plus n and if i kept picking the minimum element as the pivot what is this time so  let us write this again  refer slide time  43  14  so  t n looks to be t of n minus 1 plus n let us expand this further it is n minus 2 plus n minus 1 plus n so on and so on well  it comes down to 1 plus 2 plus so on up to n this is n times n plus 1 by 2 this is order n square so  if the pivot turns out to be the minimum element in each case  then the total time taken is order n square for quick sort and you can check that if i start increasing now let us say  the pivot was a second element there is a time decreases it will decrease till you reach the middle element let us see what happens  if i pick the roughly the middle element then  t of n roughly  i will have two problems of equal size that is 2 t n by 2 plus n this does not change because  every element i have to compare with the pivot anyway this recurrence i know what then solution is this looks like order n log n so  let us sort of write all this on one slide and look at it  refer slide time  44  47  i started out with saying let us say  t n with t i plus t n minus i plus n now  if i equals 1  there is i minus 1 here if i is 1  then i get order n square this i is roughly n by 2 then i get order n log n well  one can check that  if i plot t n versus i then it initially falls if we start with n square it initially starts falling  till i roughly n by 2 and then  again it starts rising so  if i pick the maximum it is very similar to picking the minimum the array sizes split much the same way and it again goes back to n square so  the worst case time is n square and if you manage to pick the pivots correctly  i mean if you pick the middle element at each stage then the time taken by quick sort is n log n so  we are only worried about the worst case time so  if somebody asks you  what is the time taken by quick sort well  it is n square this implementation of quick sort  takes order n square time  because that is the worst case time if somehow  we could pick an element of rank n by 2 efficiently if you can pick an element of rank n by 2 efficiently  what do you mean  what do i mean by efficiently here it means the recursion should not change much so  if i know i get n log n with the following recurrence t n is 2 t n by 2 plus n in fact  i could add a constant here and it would still be n log n this will still give me order n log n so  you can check this this constant will just come out of the calculation if i have c n then i get n log n now  if i can somehow find the middle element of an array  which means an element of rank roughly n by 2 so  if i can find an element of rank n by 2 in constant times n time then  i can implement quick sort to take time order n log n the way i do it is i first pick i spend c n time pick this middle element the element of rank roughly n by 2 now  i do the usual use this pivot divide the array into two parts now  the array is divided into two roughly equal parts once  i divide them into equal parts now  it is the usual recursion and i am done if i take n log n so  i can implement quick sort there is an implementation of quick sort there will be an implementation of quick sort  which takes n log n time  provided i can find this element of rank n by 2 in linear time which is what we do next ? before we do that let me so point this fact out  that in the recurrence  it was desirable that the two parts were of roughly equal size if they were not of equal size  then the time was proportionally it was higher so  often in these divide and conquer kind of situations we try and see  if we can somehow split the input into exactly two parts in two halves so  once we have halves  some because of the way this recurrence these recurrences work the time taken is usually smaller so  our job next is to figure out  how to pick this element of rank n by 2 in linear time so  let me state this problem so  this element of rank n by 2 is called the median  refer slide time  49  28  so  the median is an element of rank n by 2 let us say  floor of n by 2 you could take the ceiling of n by 2 it does not matter so  n is odd you get the floor of n by 2 and our job so  what we want to do is this the algorithmic problem given an array find the median how do you find this element of rank roughly n by 2 ? well  the easy way to do it is sort the array and pick the middle element that takes time n log n the time for that is n log n so  if you for instance use merge sort that is not good enough can we do it faster ? the answer is yes you can in fact  do it find median in linear time and this will be the first non trivial algorithm that you will see it is a really smart algorithm and as we present  i hope you appreciate the beauty of the solution design and analysis of algorithms prof sunder viswanathan department of computer science engineering indian institute of technology  bombay lecture ? 7 divide and conquer ? ii median finding we had looked at algorithms for sorting  especially quick sort the way quick sort worked was you picked a pivot  and compared each element in the array with the pivot and split the array into two parts those elements  which were less than the pivot and elements  which were greater than the pivot  and then we recursed down these two parts and you recursed on these two parts and recursively sorted them and this then gave you the sorted order of the entire array the time taken by quick sort  crucially depends on the position of the pivot on the array we would like to pick an element  which is the middle of the array it is called the median so  if we could always pick the median fast then perhaps we can make quick sort work in n log n time and that is our goal that is the next goal to see if we can pick we can given an array  we can find the median in time linear in the size of the array so  let me make a few definitions just to set the ball rolling  refer slide time  02  25  the rank of an element is it is position in the sorted array so  given an array and an element to find the rank of this element sort the array and the position of this element in the array   refer time  02  57    we will assume for simplicity that elements in the array are distinct every element is distinct all algorithms that we designed will also work when the elements are not distinct but  this is just for simplicity this is the rank and rank of an element and the median is an element of rank n by 2 that is the floor of n by 2  an element of rank floor of n by 2 our objective is to find the median in an array how fast can we do this ?  refer slide time  03  52  so  the problem is this the input is an array a  the output the median now  clearly this can be done in order n log n time  by just sorting the array and picking the middle element sort the array pick the middle element that gives you the median and sorting takes n log n time our objective of course  is to do this faster we want to find the median faster how does one go about doing this ? let can we apply divide and conquer for instance supposing  we want to apply divide and conquer how do we like to divide the array ? how do you divide the array ? again  it seems sort of to think about this and you are up against the wall so  supposing somehow  you can find a let us say  not the exact median but  some kind of approximate median what do i mean by an approximate median ? let us say an element whose rank is greater than n by 4 and rank of x and it is greater than by 4 that is greater than equal to n by 4 less than equal to 3 n by 4  n is the size of the array so  these elements i will call approximate medians they are not quite the middle element but  they roughly sit around the middle element supposing  you could find supposing somebody gives you an approximate median then  what can you do i mean can does this help well  it may help i mean in the following sense that  so here is my array i take the array  refer slide time  06  24  this is my array a x is an approximate median let us  not lose focus of our goal  which is to find the exact median x is an approximate median and well you just come to know of it somehow what you could do is this you split the array as less than x and greater than x so  here are elements less than x here  are elements greater than x this is less than x this is greater than x once  you do this  you know where the median falls if this is less than n by 2  if the rank of x is less than n by 2 then the median falls on the right it sits in this portion of the array on the other hand  if the rank of x is greater than n by 2  then the median falls in the left portion of the array you know where  it which portion of the array it sits so  the other good thing about this partitioning is that  see both these parts have size at least n by 4  the elements which are less than x  as size at least n by 4 the elements greater than x has size at least n by 4 so  if you can get rid of one part  let us say the smaller part then the bigger part is of size utmost 3 n by 4 so  you have shrunk the size of the array by a constant factor does this is that does this help or actually it does supposing i can find this approximate median in let us say  linear time somehow may be you do not know how that is done so  let us see  how much time for finding the exact median will take so  what we would like to do is this we have an array of size n  so let us say the time is t n we find approximate median  which takes n time and then  we would like to recurse on a small part that part has size 3 n by 4 well there is still a problem  which we need to address but  supposing you can do this then  you check that this recurrence gives you order n you can just open out a few terms here and you can prove that this in fact is order n there is a problem the problem is this i want to find the median of the big array i somehow get this approximate median in linear time this we still do not know how this is done i know that the median is here somewhere this is the element i want to find the element could sit here not exactly in the middle so  what i want to find in this array is not exactly the median the element that i want to find depends the rank of the element i want to find depends on the left hand side of the array so  what i want to find let us say that the numbers so  let us call these two things let us say  a left and a right let us  assume that the size of a left is less than n by 2  floor of n by 2 we will assume this without loss of generality now  what i want is the median of the entire array what is it is rank in a r so  it is the element that i want what is the rank in a r supposing that rank is r what i want is r plus 1 plus size of a l should be equal to n by 2 in other words  i want r to be n by 2 minus size of a l minus 1 so  r is the rank of the median of a in a r the small r is the rank of the median of a in this array a r let me  draw a picture and sort of explain this again  refer slide time  11  32  this is a   refer time  13  34   this is sorted here is my middle element this is n by 2 now  my element x sits somewhere here this is this is element x  which is approximate median and this portion is a l this portion is a r when the size is a l this portion is a r now  the rank of the median is just the rank  the number of elements in this portion what is that  n by 2 minus a l plus 1 so  it is nothing but n by 2 minus size of a l minus 1 so  the rank of this element  which is the median  this is the median that we want to find so  this is the median a the rank of this element in a r is this so  the problem when we recurse we do not really want the median in a we are not looking for the median we are looking for an element of rank this we are looking for an element of rank n by 2 minus a l minus 1 and this need not be the middle element in a l this is the first problem so  when we recurse  we do not really want the i mean it is not just the median but  we want an element of some particular   refer time  13  15    well  why not look for a procedure to find an element of any rank so  pay careful attention here this is a very sort of important design principle we started off with trying to find the median we tried to find the recursive procedure along the way we have encountered a problem  where we actually want to find an element of some other rank not just rank n by 2  but some other rank so  this seems to be a more general problem finding the median a special case of this problem sometimes  it is easier to solve a more general problem  than a specific problem and in this case  that is what we will do so  the problem we would like to tackle is not just finding the median it is finding the element of any rank so  the input let me write this problem now  refer slide time  14  17  input is array a and r the output is an element of rank r in a now  when r is n by 2 we get the median this is a more general problem  than the one we started out with and this is what  now we would like to solve the reason this problem came up is that during the recursion  i mean at least the recursion that we tried we started out with the median but  when we try and get the recursion going the problem we end up with is this so  this is the problem we will try and solve now  you see that there is no problem with the recursion at least so  if i solve  if i find an array a  i start with an array a supposing  this is still a step which is left undone i get x  which is an approx median i split the array into two parts a l and a r and x i want to find an element of rank r if r is less than the size of a l  i recurse on a l if r is greater than size of a l plus x  i recurse on a r that is it that is the algorithm so  as long as i can find a approximate median  i can find the element of any rank  by this divide and conquer scheme you can write down the recurrence relation and see that  if we can find this approximate median in linear time then  you can find the an element of any rank till linear time so  how will you find the approximate medium this is what  this is seems to be as difficult as finding the exact median of any rank so  it is and the solution that i am going to present now  is really smart so  the way you find the approximate median is this  refer slide time  16  49  first take the array a so  unfortunately i can not tell you exactly how people came up with this i am just going to present the solution to you sometimes  we just have to be really clever and come up the solution there is no real recipe for finding algorithms all the time there are the few general recipes that one follows  but often you know these things are so problem dependent that you just have to think about it think about it and at some point  you know some light bulb goes off somewhere and you know you come up the algorithm that works so  here is the algorithm that works so  take the array a split up the elements in the following form  refer slide time  17  32  so  here is a  i split the elements into groups of 5 so  these are 5 elements these are the first 5 elements  the next 5 elements and so on and i have n by 5 groups i have n by 5 groups in a and each of them has 5 elements now  find the median of these 5 elements this will give me some median y 1 this will be some median y 2 and the last one will give me some median y n by 5 these are medians of the groups there are n by groups each of each consisting of 5 elements and this is the median of these groups now  the approximate median  which we will pull out is the exact median  of these n by 5 elements approximate median that will output exact median of these n by elements so  let me repeat this again you split them into groups of five there are n by 5 groups for each group you find the median y 1  y 2 up to y n by 5 each group  you find the median now  you find the exact median of these n by 5 elements and this is an approximate median there are number of questions why should this be an approximate median ? that is the first question second question  we started off trying to find the exact median we ended with trying to find the approximate median and we are again finding the exact median to find the approximate median  we find these y 1 through y n by 5 and again we are finding the exact median what is going on well  the second thing is not so serious in the sense  that initially we started out trying to find the median of n elements we are now finding the exact of median on n by 5 elements so  the size has reduced and we can apply the  we can recurse on this smaller size remember i said  we can find solve the problem for a smaller size you know  you can put the solution back up so  the exact median that we want to find among these n by 5 elements we just do it by recursion we just recurse and find the exact median here that gives you what we want we said  we want to find an element of any rank this is what we want to do if somehow  we can find the approximate median we have done to find the approximate median  we have to solve an exact median problem on an input of smaller size this can also be done by recursion i have to still convince you  that the exact median of these n by 5 elements will be an approximate median let us see why this is to see this  let me arrange these 5 elements in each group in descending order  refer slide time  21  49  so  smallest element will be at the bottom say a 1 1  then the next element a 1 2 then  a 1 3  a 1 4  a 1 5 so  all of these  i will all of these small groups  let me first write them down in increasing order upwards or decreasing order downwards the next thing i am going to do is look at these middle elements  a you know the medians of these groups and i will write them in sorted order towards the right so  a 1 3 will be the smallest element among these medians so  a 1 3 is less than a 2 3  less than a 3 3 so on and a n by 5  3 so  this is also part of a group  which i have written in this order this is a 2 2 this is a 2 1  a 2 4  a 2 5 similarly  here a n by 5  2  a n by 5  1  a n by 5  4 and a n by 5  5 so  the entire array  i have written out this way well the last row may not be one of these rows  may not may not have all 5 elements but if the size of the array is divisible by 5 all of these columns will have sorry all of these columns will have 5 elements so  let me again explain this picture you first  split each of them into 5 parts there are n by 5 groups now  you sort  you sort them in increasing order look at the middle elements sort them middle elements in increasing order  lay them out and lay out the entire array in this fashion so  a 1 3 is this smallest element among the middle elements it comes in some group it may be the 20th group or the 15th group or whatever whatever it is  i will call it a 1 3 so  the first group here i get by taking that part of the array  where the middle element was the smallest when  i look at the middle elements this has to be the smallest so  then comes the second group and then the third group and so on and this is the last i will just arrange the elements of the array this way now  what is the median of this  this portion remember  that we said that the median of this portion is an approximate median median of this middle portion is what we claim to be an approximate medium so  what is this middle what is this so  it sits somewhere in the middle here so  it is the way we have written this  it is a n by 10  3 that is the element  which is the median of this  this portion now  how many elements are smaller than this medium ? where do you find elements in the array  which is smaller than this medium well  if you look at this middle row  everything to the left here is smaller also  everything down here is also smaller  which means this entire portion consists of elements  which are smaller these are smaller elements similarly  if you look to the right these are all larger elements and also  if i go right and up i also get larger elements this is also larger than this so  this portion again consists of larger elements this portion consists of larger elements this portion consists of smaller elements and you can see that  each of these this portion is at least 1 4th in fact  it is greater for our purposes this is at least 1 4th of the entire array this portion is at least 1 4th of the entire array that is the reason  this is an approximate median we have just shown that  at least n by 4 elements in the array are smaller than this at least n by 4 elements are larger than this that is why this is an approximate median and once  we have the approximate median  well we know what to do next we take this approximate median and recurse on these two parts and i have well in sort of vague disjoint way i have given you all the ideas needed to design this algorithm to find a median let us just put all of this together let us just write down each step put all these steps together stare at it and then  see if you can analyze it and see what the running time so  i will not be giving you code but  i will write down each step carefully so  here goes  refer slide time  28  10  i want to find an element of rank r in an array a so  let us call this find rank  find element let us say  r  a this finds an element of rank r in array a now  what do we do well the first thing is to somehow gets this approximate median so  for that well  the first step is this so  partition array  partition a into n by 5 groups of size 5 find the median of each group now  i want to recurse i want to find the median of these medians there are n by 5 groups so  there are n by 5 elements so  each group and put them in an array put them in array b say array b has size n by 5 now  i want to find element essentially  i want to find the median in this array so  find element of rank n by 10 in the array b this is again recursion so  this portion  find the approximate median this well this should return an element so  call this x so  this returns x this element x is returned by this call and this x is the approximate median that we want now  what do ? well  we use this is something we have done before we use x partition array into two parts and then recurse on the right part  that is it  refer slide time  31  14  so  partition a into a l and a r so  elements in a l are less than x elements in a r are greater than x that is how you get a l and a r now  let us see  so now  we are left with cases depending on what the rank of x is we have to either recurse on a l or a r so  there are 3 cases the first case is if  x luckily happens to be the element we are looking for so  when does that happen ? so  if size of a l is let us say  n by 2 minus 1 then  return x  x is the median that we are looking for now  if i am sorry  so size of a l should be r minus 1 we are looking for an element of rank r so  we go back to here   refer time  32  40    what we want to find is an element of rank r in a  not just the median so  if the size of a l is r minus 1  then rank of x is nothing but r so  x is the element of rank r and we return x so  if size of a l is greater than r minus 1 then  we know that this element of rank r sits in array a l the element of rank r sits in array a l so  we recurse on array a l so  we say find the element so  we want still to find an element of rank r and we now recurse on a l an element of rank r in a l will also be an element of rank r in a that is easy to see and the last case is when size of a l smaller  refer slide time  34  02  the last case is size of a l is smaller than r minus 1 then  we need to find an element of rank so  here is a let us quickly draw picture to find this out so  here is x this is a l and i know that r is somewhere here this is what i want now  this is r  this there are r elements here there are size of a l elements here  one element here so  the rank of this element in this portion of the array is nothing but this distance that is nothing but r minus size of a l minus 1 this entire distance is r this distance is size of a l and x is just one element so  we want to find now an element to find element the rank is r minus size of a l minus 1 and we want to do this in a r this portion of the array  that portion of the array is a r that is it that ends the description of the algorithm so  let us look at this algorithm again we want to find an element of rank r in array a partition a into   refer time  36  01   n 5 n by 5 groups of size 5 find the median of each group now  find the median of these medians this returns an element x  which will be an approximate median and we are now going to partition with respect to x so  partition a into a l and a r with respect to x   refer time  36  27    these are elements smaller than x and those larger than x and now are going to recurse on one of these two parts that depends on the rank of this element x the rank of this element x is nothing but size of a l plus 1 so  if the rank of the element x is r exactly r then we return x as the answer if the rank of element is x is larger than r that means  the element we are looking for is on the left half of the array so  we just find an element of rank r in the left half of the array if this does not work  well the element that we are looking for lies on the right half of the array and well this is what we do if the rank is less than r minus 1  then we find an element of rank r minus a l minus 1 in the right half of the array this is the entire procedure how do we analyze this ? we have to analyze this procedure so  we have seen reasons  why this procedure works why this we expect that this procedure should run in linear time our expectation at this procedure runs in linear time and we will see why  this procedure actually works in linear time so  to once so the once you have designed an algorithm how do you analyze  the easiest way to do it is write down the algorithm write down perhaps code for the algorithm look at each step and figure out how many times each step is executed the step could be a recursive step  refer slide time  38  38  so  here is let us say the  you have written an algorithm  for some problem there are there are many steps one of them could be recursive so  this is an algorithm call it a so  you recurse on a with smaller input and then  there is a loop may be and there are steps inside the loop this is your normal structure of a program to analyze this  i need to you need to find out  how many times is each step executed those outside a loop are executed once that is fine for the recursive call  you need to estimate the size of this input so  for instance  if initially the input size were n and this is let us say n by 10 this could be n by 10 and now  you have these things in a loop let us say and you have to find out how many times each of them is executed now  supposing there are you know 5 of them  5 steps here and each is executed n times then the total time that you will take t n well t n by 10  that comes in the recursive step and these  totally the total time is 5 times n there are 5 steps each takes   refer time  40  14    each of them is executed n times and that gives you 5 n so  we are going to do something similar to our algorithm for the median look at these each of these steps and we will see how much time each of them takes so  t of n is the time taken on an array of size n  refer slide time  40  40  so  we are going to write a recurrence for t of n so  let us look at the first slide   refer time  40  45    partition a into n by 5 groups of size 5 how much time does this take nothing this partition is easily done find the median of each group how much time does this take ? well each of those 5 elements  each of those n by 5 groups i could even sort that will take let us say 25 sort of comparisons for each group i mean even if i do bubble sort and there are n by 5 such groups  so it is some constant times n so  this step finding the median of each group takes constant times n for the entire array so  let me write this down so  this is the first  first step to the cost c 1 times n this is to find the median of these groups of each group there are n by 5 groups for each of them i spend some constant time and that gives me a total of c 1 times n what about this what about this step well  i recurse on something of size n by 10 so  sorry i recurse on b this is the rank i want to find i recurse on b and b had size n by 5 so  i recurse on a problem of size n by 5 so  that is t n by 5 plus now  we get here we partition   refer time  42  41   this into two parts so  how much time does this take this takes n time  because each element is compared with x so  this takes an additional n this is to partition this array into two parts once we find this x after  we do this partitioning  now we recurse well  if we are in this case we are done  we return if we are in this case or if we are in the next case  we recurse on a smaller part of the array so  we recurse on either a l or we recurse on a r depending on where this what was the rank what was r and what was rank of x we either recurse on a l or we recurse on a r now  we know so  this is the last step so  that is what i need to write so  this t of something this is the recurrent so  what do i put here well  both a l and a r notice  the size of a l i know is at least n by 4 size of a r i know is at least n by 4 or this implies that size of a l is less than equal to 3 n by 4 why is this this is because size of a r is at least n by 4 so  size of a l is utmost 3 n by 4 similarly  size of a r is utmost 3 n by 4 so  whichever side  i recurse the size the maximum size it can be is 3 n by 4 so  i can put 3 n by 4 and i put less than equal to what i put here the size  can only be less than or equal to 3 n by 4 so  the maximum size an array can have when i do this recursion is 3 n by 4 so  the recurrence equation i get is this t n is less than equal to c 1 n this is to find the median of those 5 of these groups n by 5 groups here is a recurrent recursion to find the exact median this is the time for portioning and this is the last recursion we recurse either on a l or a r and a maximum size of a l and a r is 3 n by 4 so  this is the recurrent so  t n is less than equal to let us say  c 1 plus 1 n plus t of n by 5 plus t of 3 n by 4 so  what is the solution to this recurrence ? that is what we want to see so  here is something i want to sort of share with you so  look at the recurrence look at where we recurse one is a problem of size n by 5 this is a problem of size n by 5 the other is a problem of size 3 n by 4 the sum these two is less than n n by 5 plus 3 n by 4 is strictly less than n in such cases usually  you will end up with something which is linear so  let us try and prove this so  we know and in such cases instead of writing down this recurrence the way we have been doing there is an easier way to do this so  i will assume that t n is less than equal to c n i want to prove this so  i want to prove this so  let us just substitute this into the previous equation and see what we get so  i get t n  so let me do this on a new sheet  refer slide time  46  59  i know this t n is less than equal to c 1 plus 1 n plus t n by 5 plus t 3 n by 4 and i want t n is less than equal to c n i want to find a right constant here c so  let us just plug this into the original thing i get t n is less than equal to c 1 plus 1 n plus well c n by 5 by plugging in c n plus 3 by 4 times c n now  if this is less than or equal to c n if i can find c  so that this is less than equal to c n then we are done if this is true  we are done because  we can actually  prove this by induction even  if you do not understand why we are done at this point we would like to find a c which satisfies this so  let us find that so  we want c 1 plus 1 n plus this plus this less than equal to c n so  n cancels from these so  i want c 1 plus 1 less than equal to c minus c by 5 minus 3 c by 4 we want c plus c 1 plus 1 utmost c minus c by 5 minus 3 c by 4 so  this will imply the previous statement and just simplifying this  this is nothing but c by 20 c times on by 20 so  as long as c is at least 20 times c 1 plus 1 we are done so  if c is greater than equal to 20 times c 1 plus 1 we see that  this quantity is less than equal to c n so  we can choose c to be 20 c 1 plus 1  refer slide time  49  32  so now  it looks like t n is less than equal to 20 times c 1 plus 1 times n so  this is a constant this is the constant c this now  we can prove by induction on n  that t n is less than or equal to this c times n we can prove now by induction of n for n equals 2 it is true  because you make just one comparison and for the inductive step for the inductive step we just observe that  t n by the recurrence is utmost c 1 plus 1 n plus t n by 5 plus 3 n by 4 and now i can use induction so    refer time  50  23   t n by 5 us utmost c n by 5 and this is utmost this much and the way we have chosen c  this sum is utmost c n so  the proof now follows by induction in fact  initially we guessed that t n is utmost c times n and the plan was to use induction somehow  use induction and prove this and you just follow your nose in the induction substitute c and find out   refer time  50  54   what is the c that helps you finish the inductive   refer time  50  58    so  this is the other way of solving recurrence relations if you can guess the answer somehow  then you know that you want to prove this by induction so  try proving it by induction and come up with the right constants and then you can once you have found it  once you plug it in and then find out what and figure out what constants to use then you can write the whole thing by induction so  this proves that the time taken by our algorithm is in fact  linear in n so  you can find the median in time  which is linear in the size of the array if you use this  use this algorithm with quick sort then the worst case time for quick sort would be n log n but that is if you pick the pivot as a median each time the normal way you do it of course  it turns out to be order n square design and analysis of algorithms prof sunder vishwanathan department of computer science engineering indian institute of technology  bombay lecture  8 divide and conquer  iii surfing lower bounds we will again look at sorting many of these algorithms merge sort  quick sort and some of the other sorting algorithms that we have studied all of them seem to take n log n time i think it is a good question to ask  if we can do faster and we this n log n  which most of the sorting algorithms seem to take and this is the question  we would like to address in this part of the course in fact  we will show that for a large family you have sorting algorithms n log n is the best that they can do if we sort of restrict  the kinds of things that algorithms can do on inputs and n log n is the best that we can do now  we look at merge sort let us take merge sort  but whatever i say is actually true even for  let us say quick sort for merge sort the what really happens you split the array into two parts and then  recursively sort these things and then  you merge these two sorted arrays that is the crucial step what happens when you merge two sorted sub arrays  or when you merge two sorted lists well  you compare two elements and then  move them out compare the first element in the list that is the smallest and we put it as a first element then  move your pointers and so on  and so forth so  the most important step there is comparing two elements and then  some reordering so  the only operation that  you actually do on these elements in the sorted  in the array is comparing two elements and moving them out so  this property actually holds even for quick sort so  always quick sort term  you pick a pivot  by your favorite procedure and now you compare every element with the pivot once you compare every element with the pivot and then  you move these elements around so  the basic operation on two elements of the array is compare these two figure out which one is larger  which one is smaller so  again the usual sort of operation is compared  is looking at two elements in the array  comparing them and then  based on the result  i mean if one is smaller than the other or first one is bigger than the other  then you sort of either switch them around or change the order so  this is the crucial operation  that you do on the array element you do not add them  subtract them  divide them so  the operations that  we use in at least these popular sorting algorithms  is compare two elements may be swap them or move them around so  if these are the only things  that you can do with array elements then  you need n log n comparisons to sort an array so  this is our goal that is what we are going to show then  how do we go about doing this well  even if you recall  we showed that  find the minimum you needed n minus 1 comparison and even there  it was not an obvious  sort of the solution was not absolutely obvious you had to do little bit of work and for this actually we need to do a little bit more of work  but it is not difficult so  the first thing is to notice that these algorithms can be written as flowcharts  or comparison trees what is a comparison tree ? well  basic block is comparison between two elements  refer slide time  04  52   so  i compare two elements  a i and a j upon array  for a branch this way  if a i less than a j i branch this way  if a j is greater than a i we will assume that  array elements are distinct  which means i will never have two of them equal so  compare two elements  branch this way if a i is less than a j  branch the other way if a i is greater than a j now  using these as building blocks  i can build the large flow chart so  somewhere here  maybe i can compare two elements  two other elements a k and a l then again branch and so on and at the end  i will have these leaves where i will output so  the output for us is the order  is the sorted order so  we will look at flow charts  which look like this the input to a flow chart is just an array a  the input is an array a and you go through these flow charts and at these leaves  will output the sorted order this is the model that we are looking at let us do a small example  refer slide time  06.35  so  for instance if i have two elements  then let us say a 1 and a 2  i compare a 1 and a 2 if it is less than the order output is a 1  a 2 this is in sorted order if a 1 is larger  the order i output is a 2  a 1 so  here is a small flow chart  that sorts 2 numbers so  it sorts two elements an array  basically works on arrays of size 2 and sorts these arrays so  let us say 3  what if the array size is 3 ? so  let us see what to do  refer slide time  07  20  so  you compare  let us say a 1 and a 2 now  if a 1 is less let us say i compare a 1 with a 3  say a 1 is less now  i know at this point  that a 1 is the smallest so  i would like to compare a 2 and a 3 so  here a 2 is greater than a 3  here a 2 is less than a 3 the sorted order here is a 1  a 2  a 3 here it is a 1  a 3  a 2 and so on so  you can sort of fill this up so  you can write down a flow chart like this  with these are the leaves where you get the answer  which is the array in sorted order now  merge sort can also be depicted like this so  let us see  why that is do  refer slide time  08  34  so  let us look at merge sort or when four elements so  i have four elements a 1  a 2  a 3  a 4 and i want to sort of run merge sort on this well  we divide this into two  recurs on this part  then recurs on this part and then  there is a merge when we recurs on this part  the first comparison we do is between a 1 and a 2 so  this is the first comparison we do ; and this could be a 1 is less than a 2 this is a 1 could be greater than a 2  we do not know what this is here the sorted order which is returned here is a 1  a 2 on this side it is a 2  a 1 whichever branch we choose  now we have to recurs on the right hand side so  this is the next comparison we do  a 3 and a 4 ; and well these give again two sort of outputs now  at this step  we are up to the merge the lower things return  these two sorted orders and now we are down to the merge  the top level of recursion so  let us take this portion what are the two elements  that are compared ? here i know that a 1 is greater than a 2 so  the first one returns  this side returns a 2 a 1 and here  i know a 3 is less than a 4 so  the second list returned is a 3 a 4 these are the two lists so  the two elements compared here  are a 2 and a 3 the smallest elements from the two lists are compared now so  this is what is compared here so  let us go down one more step and see how this looks like so  here a 2 is less or a 3 is less supposing  we followed this branch  what is the next clue  which are compared for merge sort well  a 3 is less than a 2 so  a 3 is moved to the new list and our pointers are at a 2 and a 4 so  the next two things  which are compared are a 2 and a 4 and so on i can draw this tree out and at the leaves of this tree  are the solution i mean at each leaf  i will tell you what is the output  what is the sorted order which it output in fact  if you look at this tree  this tree is a binary tree at each internal load  we have a comparison so  we compare two elements  that you branch out into two and for each branch again there will an internal node  you compare two elements branch into two  that is it so  if i just look at it as  if i forget that  there these comparisons etcetera  etcetera the whole structure looks like a tree in fact  the binary tree and there are leaves  and at these leaves we have the output output mean in the sorted order so  when i sort of follow this tree down to a leaf  i get the answer this leaf whatever is the answer is the output the one thing to notice is that  this tree is different for different sizes of the input for four elements  i can tell you what the merge sort tree looks like the comparison tree for merge sort  it has some structure and in fact  just the way i have written part for the tree here  you can actually write down the entire tree it will take some time and space  but if you have the patience  you could try this out for five elements again  there is a tree for six elements  there is a different tree and so on and so forth if i take  if i fix the number of elements to n  then for merge sort i can write down the comparison tree  which does the comparisons in the same order as merge sort so  if i follow the tree down for a particular input  the comparisons made will be the same the order of comparisons will be the same  as that made in merge sort and the answer will be as given by merge sort so  how much time  what is the notion of time required on this tree well  the time taken for  let us say given an input  the time taken is just the number of comparisons you make till you hit a leaf  in this tree so  given an input you make comparisons  then you branch either on the left or right then  you make some other comparisons branch etcetera  etcetera  finally you hit a leaf the number of comparisons is just a path in this tree from the root to a leaf so  the time taken is the number of comparisons  which is the length of the path followed from the root to a leaf so then  the worst case time  what is the worst case time by merge sort  would be just the height of the tree the worst case time for merge sort will just be the height of this tree ; which is the longest root leaf path in this tree i can show that this is long enough then  the time taken for merge sort will also be that much and that is the goal ; and we have most of the ingredients in place we just have to make certain observations here are two critical observations so  we have an algorithm as a flowchart we have an algorithm which sorts n numbers and it is depicted as a flow chart where or a comparison tree  where at each node you compare to a level that you branch  depending on which one is greater at leaves we output sorted out  this is the thing the question we ask is  how  what is the height of such a tree  how tall is such a tree  refer slide time  15  55  here are two sort of crucial observations  if this flow chart does sort then  every input order must leads to a leaf different input orders lead to different leaves so  why is this true ? well  firstly we will assume that inputs  in our input every element is distinct we do not have two copies of the same element there are n factorial orders  that are possible as input and for each order you make certain comparisons and you trace the path to a leaf and at the leaf  we have to give the right order  which is a permutation of the input and for every input order  you must end up with a different leaf  because the permutations are different so  for instance  if i look at 1 2 3 this is already sorted if i look at 1 3 2  the sorted order is 1 2 3 so  i have to interchange the last two elements this order must appear at a different leaf  which means all of these n factorial permutations ; must appear in one of these leaves and different permutations must appear at different leaves so  because of the order of the input is different and must land up in different leaf i land up in the same leaf  i will give the same answer for both orders  which is incorrect so  this means  there must be n factorial leaves in this binary tree and each of these different orders  must land up in the leaf and different orders must land up in different leaves  which means there must be n factorial leaves in this binary tree so  intuitively you can see  why the height must be large so  if i have a binary tree with large number of leaves  the height better be large so  we will invoke theorems  which you must have seen in discrete structures ; and we will finish the proof so  if i look at this flow chart or this decision tree  refer slide time  19  06  then  i have a binary tree and i know that number of leaves is at least n factorial using these  i want to conclude that the height is at least something so  if i can put something here  the height at least something then  i know that the time taken by the algorithm must also be at least this much so  why can i say this is the height is most   refer time  19  42    well  supposing i have a binary tree of height h  what is the maximum number of leaves  that this can have this is something which you have done in discrete structures  so let us state this so  binary tree of height h has at most  well 2 to the h leaves the number of leaves can be at most 2 to the h well  let us i hope you can sort of prove this let us give a proof anyway so  proof is by induction on h  so the base case i will let you do this h equals 1  i will let you do this  so the inductive step so  how do we   refer time  20  46   the two steps  refer slide time  20  59  well  let us look at a tree  that t be a binary tree of height  let us say k plus 1 so  we will assume that  the statement is true for all binary trees of height k or smaller and we would like to prove  the statement for a height k plus 1 so  the inductive hypothesis is that  for every tree of height k or smaller  the statement holds and we would like to prove this for tree of height k plus 1 so  if i look at t  t has a root and then  there are two sub trees this is what it looks like t 1 and t 2 now  well i remove the root and now i am going to apply induction to t 1 and t 2 apply the inductive hypothesis to t 1 and t 2 well  this had height k plus 1  which means both t 1 and t 2 have height k or less one of them has height k  the other could be k or it could be less so  the height of t 1 is less than equal to k similarly  height of t 2 is less than equal to k that is the reason  we can apply the inductive hypothesis both of them have height less now  we can apply the inductive hypothesis so  the number of leaves in t 1  is less than equal to 2 to the k similarly  number of leaves in t 2 is less than equal to 2 to the k and now you can see  basically the number of leaves in t  is number of leaves in t 1 plus the number of leaves in t 2 ; which is this plus this which is at most 2 to the k plus 1 and that is the end of the proof  refer slide time  23  28  so  let us just write this  so number of leaves t equals number of leaves t 1 plus number of leaves t 2 that is less than 2 to the k plus 2 to the k 2 to the k plus 1 that finishes the inductive proof  that on a tree of height h when i have a binary tree of height h  the number of leaves is at most 2 to the h  refer slide time  24  02  number of leaves is less than equal to 2 to the h well taking logs  i get log the number of leaves is the lower bound on the height well we know remember that in our binary tree  the number of leaves was n factorial so  the height this means  the height is at least log base 2 of n factorial well  this is the height of any of the tree  which is related to any this flowcharts  which sort flowcharts for sorting we looked at these flowcharts so  we said that number of leaves is n factorial and we have just seen that  the height must be at least the log of n factorial so  what is this  we are very close log of n factorial is at least log base 2 n by 2 to the n by 2  why is this true ? well  this is true  because n factorial is n into n minus 1  somewhere n by 2 and then  all the way up to 1  now if you look at the top n by 2 terms  all of them are greater than n by 2 there are n by 2 terms and all of them are at least n by 2  all of whom are greater than equal to n by 2 so  n factorial is at least n by 2 to the n by 2 well  that is why this is true and this is nothing but n by 2 log n by 2 this is omega n log n  so the time taken is at least n log n this is what we have proved some constant times n log n so  this shows that merge sort  for instance we know it takes maximum time n log n but  there are inputs on which it takes  i mean log n  it must take so  similarly quick sort and any other algorithm in this model  which is if the only thing which the algorithm can do  is look at two element  compare them and perhaps move them around  swap them for instance or move them around if this is the only thing  that is possible then  the time taken by any such algorithm for sorting must be n log n there are sorting algorithms which do not fit into this model we will not cover those  hopefully we have covered some of them in your data structures course these are bucket sort radix sort  well these algorithms differ in that they you can look at the least significant digit you can divide  you can take floors of these numbers if they are integers  you can take floors and then  do funny things with them the difference between these algorithms and let us say something like merge sort is merge sort  when you look at 2 numbers the only operation  that you can do is compare them on any element  if i take an element of the array only thing i am allowed to do on this element is compare this element with some other element in the array i can not sort of divide it by 10  20  100 whatever i can not look at it is least significant bit etcetera  etcetera that is as far as sorting is concerned we will continue with our discussion of algorithm design techniques  by looking at some other problems  which are fairly interesting and where this divide and conquer paradigm sort of works  and gives you better results in some cases perhaps you the algorithm  that we desire  refer slide time  28  58  the first one is this  so you are you are given a 2 k so  this is the input by 2 k square and what you want as output is a tiling with  i will tell you what all this means  with this what do i mean by tiling  a tiling essentially means  i must fill up all these squares and try and fill up as many squares as possible with pieces  which look like this so  each piece is  let us say a kind of a domino  which sort of looks is of this shape and i want to try and fill up  as many squares as possible in this so  well you can see immediately  i may not be able to fill up all square even for k equals 1  if i have a 2 by 2 square  i have a 2 by 2 square well i can put one of them so  i can put something which fills up these three squares and this will be an empty square so  here is a tiling of a 2 by 2 square with a domino of this form  which does everything except one square so  what about 2 to the k cross 2 to the k well  the number of squares  we look at the number of squares is 2 to the 2 k these are the number of squares  the number of squares and this is 1 mod 3 i will let you figure this one out the number of squares is always 1 mod 3 well it is 2 here and you can see  that it is always two if you want you can prove it by induction on k  but even otherwise you can sort of check that  the number of squares will be always 1 mod 3 maybe we will just look at a equals 2 so  then it is a 4 by 4  how does this look like so  this is a 4 by 4 thing and you have well 16 squares ; and that is 1 mod 3 when i divide this by 3 i get 1  so 3 times 5 is 15 and 1  one gets left out so  that is true whatever k i choose  it is always 3 times something plus 1 so  that is what i mean by 1 modulo 3 when i divide by 3 i get 1  the remainder is 1 so  can i sort of now put these dominos so that  all except 1 square is covered so  i have this 2 by 2 k 2 to the k by 2 to the k square so  i will square with side length to the k i have these dominos  which each of them has 3 squares put together can i arrange these on these squares  so that all except 1 square is covered  this is the question ; the answer is yes and we would like an algorithm to do this and well  we would like to do it using divide and conquer that is the goal  let us do this 4 by 4 and then  we will see what is the scene can i do a 4 by 4 with  so here is a 4 by 4 square  can i do this well  since i am going to do something like divide and conquer i would like to divide this into two parts  or more in this case four parts  tile each of them and put them together  refer slide time  33  24  well  let us take a bigger one and see how this is done i know how to tile this  so that one is left  i know how to tile this  so that one is left and so on now  how do i tile the whole thing  so that one is left well supposing the one left here is this one left here is this and one left here is this now  i am able to tile all four of them so  i tile this  so that this is my first domino  i use my first domino there  i put it around like this this is my second domino  i put it around like this here is 3  it does not matter how i put my fourth as of now we will we will change this later  supposing the fourth is like this well now i can put the fifth domino here  here 5 5 5 so  i managed to sort of tile at least 4 by 4 using 5 dominos i have just one left which is what i want you would like to repeat this for 2 k by 2 k so  how does that look so  here we again would like to do a divide and conquer  we would like to divide this i would like to first tile this  tile these 4 and then put them together well  by looking at this  this example i can do it if provided this square left off here is this the square left off in this  is this the square left off here is this here i really do not care if i can tile the entire region here  except this corner and so on for these 3  then i am able to tile the entire region on top leaving one squares that square left will be in this big sort of region this is 2 to the k minus 1  2 to the k minus 1  2 to the k minus 1 and so on so  this whole thing is 2 to the k so  if i can do this  then i can fit in this entire  this extra sort of domino here there will be one square left over in this region  good so  again notice a crucial thing  to be able to carry this inductive process  to completion i should be able to have the extra square i mean the square left over as a corner square when i tile a region which is like this  the left over square must be a corner square so  this is a square left over or left un tiled i have tiled the entire region  except this small square if i can do this  then clearly i can do the big thing so  i started out my initial objective was to tile to 2 to the k by 2 to the k that is what i started out with  i want to apply this divide and conquer kind of thing well  actually the process is really induction always if i can do it for a small square  can i push it up to a bigger square  that is the goal well  we want to do divide and conquer so  we saw that  we break it up into these 4 smaller squares now  i want to tile these smaller squares and put them up  to a tiling for the bigger square well  each of these smaller squares  that is one square  one small square which is not tiled one piece which is not tiled now  this happens to be the corner  then i can push the induction upwards but  for the induction to work even the final  so let us look at this again for the induction to work in this big square  i want the one square which is not tiled to be a corner well  it can not be corner here  because everything else is tiled  this can not be a corner  it can not be a corner well  can it be this corner  the answer is yes it is like i leave this square un tiled  so leave this now  when i look at these 4 small problems  they all look absolutely alike i want to tile everything  except a small square in the corner i want to tile all of it  except this square in the corner except this in the corner and except this in the corner and i put them together like this this tile is the entire big square  except for this one thing in the corner and well this is the algorithm  break it up into 2 tile these 4 recursively and put them together like this  and in the middle i just put one domino one domino fill up these three squares  so i have tile everything except this so  this is a well divide and conquer sort of strategy to tile squares so  this problem looks a bit different from the other problems and that is why i picked this  because the strategy can be used not just for computer algorithms or puzzles  or whatever you have and the other sort of lesson  that we learn through this example is that you start with some problem you want to solve we have seen this before  along the way you would like to apply  you want to find solutions to smaller sub problems you want to put these together to get a solution to a larger problem and while doing this  you may want to solve something slightly more general and then  you see this general problem can be solved so  you try and solve the general problem now  again using the same technique the smaller sub problems  you solve the general problem and now you see  if you can put them together  to get a solution to the general problem for the big input and often this sort of gives you the solution that want we saw this in two occasions  one was this tiling problem  the other was a i hope you can remember it was when we looked at this median we wanted to find the median  but we ended up looking at the problem of finding an element of given an array and a positive integer r so  we found the element of rank r  that is what we did so  let us look at one more problem  yet another problem rather  which uses divide and conquer so  this is a familiar problem of multiplying to two integers so  let us look at this  refer slide time  40  58  so  i have 2 n bit integers  x and y  this is x  this is y  these are n bits each is n bit long  i want to find the product of x and y the normal way is well  we formed the product of each of these bits with lsb  then with the next one and so on  and so forth so  there are n square multiplications well  in this case actually it is not  you can just do are and since they are bits but  you can think of these as instead of bits  you can think of them as digits  it really does not matter so  each of them could be n digits long and we multiply these digits so  there are n square multiplications  this is the high school multiplication algorithm  that we know can we do faster  that is the question let us try applying divide and conquer i have divide this into two parts this is x left x right  y left y right and now i want to compute the product of these two  well what is the product first of all x is nothing but 2 to n by 2 times x l plus x r y is 2 to the n by 2 y l plus so  this means x y is 2 to n x l y l plus 2 to the n by 2 x l y r x l y r plus y l x r plus x r times y r  this is the product x and y so  we could recursively sort of find these products  these four products and then  compute this this is just shifting by n bits  we are just shifting by n by 2 bits so  that is an order n operation  so if t n is the time taken  this what is the recurrence there are 1  2  3  4  4 problems  so 4 times t n by 2 plus order n this is some constant times n  which is for shifting and adding adding two n bit numbers  it takes alternately so  what is the solution to this recurrence ? well  i will let you do this and the solution to this recurrence you do a solution to this recurrence  it is order n square so  we are back to where we started compare these two  well we have done divide and conquer  but we do not seem to have conquered anything we are back to spending order n square times  even with this divide and conquer approach  but well all is not lost i will first indicate this algorithm and then  we will analyze  and see how we can decrease the number of multiplications so  well the trick is this  let us see  refer slide time  45  02  so  the terms that we want to compute are x l y l and then  x l y r plus y l x r and x r y r these are the three terms  that we would like to compute the way we did it  was each of them separately that gave over problems of size n by 2 so  here is a smart way of doing this well  these two i compute as they are  the third thing i compute is not quite these two  it is the following i computer x l plus x r times y l plus y r  i add x l and x r  i add y l and y r and then  multiply them together what is this  this is x l y l plus x l y r plus x r y l plus x r y r  that is what this is so  i do three multiplications and not four  this is the first multiplication x l y l x r y r is my second multiplication and this is the third this i do not do  as of now i have not touched this  this is the third multiplication i do well  if you look at this x l y l is present  x r y r is present ; and if i remove these two what is left is this x l y r plus x r y l is exactly this well  i have just written x r y l  but it is same as y l x r good so  how do i compute this  well from this product i just subtract these two to get this  so call this z now  i just do z minus x l y l minus x r y r i get this  good what have i done well i have a few more additions  i have 1  2  3  4 more additions the number of multiplications has gone down by 1 multiplying 2  n by 2 bit numbers i do 1  2 and 3 there are now only 3 multiplications that i do so  what is the recurrence now  well t n is 3 t n by 2 plus order n earlier remember it was 4 t n by 2 plus order n now  it is 3 t n by 2 plus order n and it is not surprising  that the solution to this recurrence is smaller in fact  we will show the solution to this recurrence  is order n to the log 3 base 2 roughly it grows as n to the 1.59  which is certainly less than n square  which is what we started out with so  this the number of multiplications one makes is significantly smaller  refer slide time  48  23  is 3 t n by 2 plus some let us this take n  it does not matter  so t n is this we will do the usual thing which is 3  so 3 t n by 2 square plus n by 2 plus n this is 3 square t n by 2 square plus 3 times n by 2 plus n let us do it once more  to see what happens  so this is 3 square t well n by 2 cube i am expanding this out  so there should be a 3 here  3 times plus n by 2 square plus 3 times n by 2 plus n so  this is now 3 cube t n by 2 cube plus  well 3 by 2 whole square times n plus 3 by 2 n plus n now  one can guess the general term  in this when i go down i steps  refer slide time  50  11  so  t n will be 3 to the i t of n by 2 to the i plus 3 by 2 to the i minus 1 n plus 3 by 2 i minus 2 n so on  to n  good so  now we would like n by 2 to the i should be 2 or say 1  for ease of computation if this is one  if i have one bit and the time is 1  so the time is just this becomes t of 1 is 1 so  if this is true and t of n is 3 to the i plus  well n times 1 plus 3 half and so on  up to 3 half to the i minus 1 well  let us do this  so this is 3 to the i plus n times  well what is this ? this is just a geometric series so  it is a roughly some constant times 3 half to the i you can do it more a bit better by doing the  writing the exact formula  which is 3 half to the i minus 1 by 3 half minus 1 so  it is at most  let me put it at most this is perhaps 3 half minus 1 which is half  so c could be 2  so c is most probably 2 in any case  this is bounded by something like this  3 to the i plus so  what is i ? well  2 to the i is n  so i is nothing but log base 2 n so  this is 3 to the log base 2 n plus n times constant times  well 3 half to the i  which is 3 half to the log based 2 n now  2 to the log base 2 of n is nothing but n  these two things cancel so  i have 3 to the log base 2 of n times constant and 3 to the log base 2 of n here so  this is nothing but some c prime times 3 to the log base 2 of n  this is the i time taken  refer slide time  53  26  so  what is this  well so the time  the total number of multiplications is 3 to the log base 2 of n this is 3 to the log base 3 of n divided by log base 3 of 2  i could take this up so  this is nothing but 3 to the log base 2 of 3 times  use just some general manipulations so  that is 3 to the log base 3 of n  i take this inside  n to the log base 2 of 3 this is nothing but n to the log 2 of 3 as promised so  this is roughly n to the 1.59 so  the time number of multiplications  we got it down from n square so  the initial thing to roughly n to the 1.59 of course  we increase the number of additions  but if multiplications are more expensive than additions then  we have saved on time this trick of using three multiplications instead of 2 is actually a trick it is something you have to really need to come up with somehow there is no real sort of way in which you can do it there is no method and that is the beauty of algorithm design that somehow not every problem can be sort of tackled using everything that  you could come up often you come up with  all the time you come up problems which require you to design new methods  derive new methods  find new methods rack your brain and sort of come up with good algorithms and when you do come up with these new algorithms  you feel again  the kick that you derive  the pleasure that you derive is something else this trick by the way or this multiplication is due to euler and he used this for the multiplying complex numbers we have just used this trick was then used by other people  who saw that it can be applicable in multiplying 2 n bit numbers thanks design and analysis of algorithms prof sunder vishwanathan department of computer science engineering indian institute of technology  bombay lecture  8 divide and conquer  iii surfing lower bounds we will again look at sorting many of these algorithms merge sort  quick sort and some of the other sorting algorithms that we have studied all of them seem to take n log n time i think it is a good question to ask  if we can do faster and we this n log n  which most of the sorting algorithms seem to take and this is the question  we would like to address in this part of the course in fact  we will show that for a large family you have sorting algorithms n log n is the best that they can do if we sort of restrict  the kinds of things that algorithms can do on inputs and n log n is the best that we can do now  we look at merge sort let us take merge sort  but whatever i say is actually true even for  let us say quick sort for merge sort the what really happens you split the array into two parts and then  recursively sort these things and then  you merge these two sorted arrays that is the crucial step what happens when you merge two sorted sub arrays  or when you merge two sorted lists well  you compare two elements and then  move them out compare the first element in the list that is the smallest and we put it as a first element then  move your pointers and so on  and so forth so  the most important step there is comparing two elements and then  some reordering so  the only operation that  you actually do on these elements in the sorted  in the array is comparing two elements and moving them out so  this property actually holds even for quick sort so  always quick sort term  you pick a pivot  by your favorite procedure and now you compare every element with the pivot once you compare every element with the pivot and then  you move these elements around so  the basic operation on two elements of the array is compare these two figure out which one is larger  which one is smaller so  again the usual sort of operation is compared  is looking at two elements in the array  comparing them and then  based on the result  i mean if one is smaller than the other or first one is bigger than the other  then you sort of either switch them around or change the order so  this is the crucial operation  that you do on the array element you do not add them  subtract them  divide them so  the operations that  we use in at least these popular sorting algorithms  is compare two elements may be swap them or move them around so  if these are the only things  that you can do with array elements then  you need n log n comparisons to sort an array so  this is our goal that is what we are going to show then  how do we go about doing this well  even if you recall  we showed that  find the minimum you needed n minus 1 comparison and even there  it was not an obvious  sort of the solution was not absolutely obvious you had to do little bit of work and for this actually we need to do a little bit more of work  but it is not difficult so  the first thing is to notice that these algorithms can be written as flowcharts  or comparison trees what is a comparison tree ? well  basic block is comparison between two elements  refer slide time  04  52   so  i compare two elements  a i and a j upon array  for a branch this way  if a i less than a j i branch this way  if a j is greater than a i we will assume that  array elements are distinct  which means i will never have two of them equal so  compare two elements  branch this way if a i is less than a j  branch the other way if a i is greater than a j now  using these as building blocks  i can build the large flow chart so  somewhere here  maybe i can compare two elements  two other elements a k and a l then again branch and so on and at the end  i will have these leaves where i will output so  the output for us is the order  is the sorted order so  we will look at flow charts  which look like this the input to a flow chart is just an array a  the input is an array a and you go through these flow charts and at these leaves  will output the sorted order this is the model that we are looking at let us do a small example  refer slide time  06.35  so  for instance if i have two elements  then let us say a 1 and a 2  i compare a 1 and a 2 if it is less than the order output is a 1  a 2 this is in sorted order if a 1 is larger  the order i output is a 2  a 1 so  here is a small flow chart  that sorts 2 numbers so  it sorts two elements an array  basically works on arrays of size 2 and sorts these arrays so  let us say 3  what if the array size is 3 ? so  let us see what to do  refer slide time  07  20  so  you compare  let us say a 1 and a 2 now  if a 1 is less let us say i compare a 1 with a 3  say a 1 is less now  i know at this point  that a 1 is the smallest so  i would like to compare a 2 and a 3 so  here a 2 is greater than a 3  here a 2 is less than a 3 the sorted order here is a 1  a 2  a 3 here it is a 1  a 3  a 2 and so on so  you can sort of fill this up so  you can write down a flow chart like this  with these are the leaves where you get the answer  which is the array in sorted order now  merge sort can also be depicted like this so  let us see  why that is do  refer slide time  08  34  so  let us look at merge sort or when four elements so  i have four elements a 1  a 2  a 3  a 4 and i want to sort of run merge sort on this well  we divide this into two  recurs on this part  then recurs on this part and then  there is a merge when we recurs on this part  the first comparison we do is between a 1 and a 2 so  this is the first comparison we do ; and this could be a 1 is less than a 2 this is a 1 could be greater than a 2  we do not know what this is here the sorted order which is returned here is a 1  a 2 on this side it is a 2  a 1 whichever branch we choose  now we have to recurs on the right hand side so  this is the next comparison we do  a 3 and a 4 ; and well these give again two sort of outputs now  at this step  we are up to the merge the lower things return  these two sorted orders and now we are down to the merge  the top level of recursion so  let us take this portion what are the two elements  that are compared ? here i know that a 1 is greater than a 2 so  the first one returns  this side returns a 2 a 1 and here  i know a 3 is less than a 4 so  the second list returned is a 3 a 4 these are the two lists so  the two elements compared here  are a 2 and a 3 the smallest elements from the two lists are compared now so  this is what is compared here so  let us go down one more step and see how this looks like so  here a 2 is less or a 3 is less supposing  we followed this branch  what is the next clue  which are compared for merge sort well  a 3 is less than a 2 so  a 3 is moved to the new list and our pointers are at a 2 and a 4 so  the next two things  which are compared are a 2 and a 4 and so on i can draw this tree out and at the leaves of this tree  are the solution i mean at each leaf  i will tell you what is the output  what is the sorted order which it output in fact  if you look at this tree  this tree is a binary tree at each internal load  we have a comparison so  we compare two elements  that you branch out into two and for each branch again there will an internal node  you compare two elements branch into two  that is it so  if i just look at it as  if i forget that  there these comparisons etcetera  etcetera the whole structure looks like a tree in fact  the binary tree and there are leaves  and at these leaves we have the output output mean in the sorted order so  when i sort of follow this tree down to a leaf  i get the answer this leaf whatever is the answer is the output the one thing to notice is that  this tree is different for different sizes of the input for four elements  i can tell you what the merge sort tree looks like the comparison tree for merge sort  it has some structure and in fact  just the way i have written part for the tree here  you can actually write down the entire tree it will take some time and space  but if you have the patience  you could try this out for five elements again  there is a tree for six elements  there is a different tree and so on and so forth if i take  if i fix the number of elements to n  then for merge sort i can write down the comparison tree  which does the comparisons in the same order as merge sort so  if i follow the tree down for a particular input  the comparisons made will be the same the order of comparisons will be the same  as that made in merge sort and the answer will be as given by merge sort so  how much time  what is the notion of time required on this tree well  the time taken for  let us say given an input  the time taken is just the number of comparisons you make till you hit a leaf  in this tree so  given an input you make comparisons  then you branch either on the left or right then  you make some other comparisons branch etcetera  etcetera  finally you hit a leaf the number of comparisons is just a path in this tree from the root to a leaf so  the time taken is the number of comparisons  which is the length of the path followed from the root to a leaf so then  the worst case time  what is the worst case time by merge sort  would be just the height of the tree the worst case time for merge sort will just be the height of this tree ; which is the longest root leaf path in this tree i can show that this is long enough then  the time taken for merge sort will also be that much and that is the goal ; and we have most of the ingredients in place we just have to make certain observations here are two critical observations so  we have an algorithm as a flowchart we have an algorithm which sorts n numbers and it is depicted as a flow chart where or a comparison tree  where at each node you compare to a level that you branch  depending on which one is greater at leaves we output sorted out  this is the thing the question we ask is  how  what is the height of such a tree  how tall is such a tree  refer slide time  15  55  here are two sort of crucial observations  if this flow chart does sort then  every input order must leads to a leaf different input orders lead to different leaves so  why is this true ? well  firstly we will assume that inputs  in our input every element is distinct we do not have two copies of the same element there are n factorial orders  that are possible as input and for each order you make certain comparisons and you trace the path to a leaf and at the leaf  we have to give the right order  which is a permutation of the input and for every input order  you must end up with a different leaf  because the permutations are different so  for instance  if i look at 1 2 3 this is already sorted if i look at 1 3 2  the sorted order is 1 2 3 so  i have to interchange the last two elements this order must appear at a different leaf  which means all of these n factorial permutations ; must appear in one of these leaves and different permutations must appear at different leaves so  because of the order of the input is different and must land up in different leaf i land up in the same leaf  i will give the same answer for both orders  which is incorrect so  this means  there must be n factorial leaves in this binary tree and each of these different orders  must land up in the leaf and different orders must land up in different leaves  which means there must be n factorial leaves in this binary tree so  intuitively you can see  why the height must be large so  if i have a binary tree with large number of leaves  the height better be large so  we will invoke theorems  which you must have seen in discrete structures ; and we will finish the proof so  if i look at this flow chart or this decision tree  refer slide time  19  06  then  i have a binary tree and i know that number of leaves is at least n factorial using these  i want to conclude that the height is at least something so  if i can put something here  the height at least something then  i know that the time taken by the algorithm must also be at least this much so  why can i say this is the height is most   refer time  19  42    well  supposing i have a binary tree of height h  what is the maximum number of leaves  that this can have this is something which you have done in discrete structures  so let us state this so  binary tree of height h has at most  well 2 to the h leaves the number of leaves can be at most 2 to the h well  let us i hope you can sort of prove this let us give a proof anyway so  proof is by induction on h  so the base case i will let you do this h equals 1  i will let you do this  so the inductive step so  how do we   refer time  20  46   the two steps  refer slide time  20  59  well  let us look at a tree  that t be a binary tree of height  let us say k plus 1 so  we will assume that  the statement is true for all binary trees of height k or smaller and we would like to prove  the statement for a height k plus 1 so  the inductive hypothesis is that  for every tree of height k or smaller  the statement holds and we would like to prove this for tree of height k plus 1 so  if i look at t  t has a root and then  there are two sub trees this is what it looks like t 1 and t 2 now  well i remove the root and now i am going to apply induction to t 1 and t 2 apply the inductive hypothesis to t 1 and t 2 well  this had height k plus 1  which means both t 1 and t 2 have height k or less one of them has height k  the other could be k or it could be less so  the height of t 1 is less than equal to k similarly  height of t 2 is less than equal to k that is the reason  we can apply the inductive hypothesis both of them have height less now  we can apply the inductive hypothesis so  the number of leaves in t 1  is less than equal to 2 to the k similarly  number of leaves in t 2 is less than equal to 2 to the k and now you can see  basically the number of leaves in t  is number of leaves in t 1 plus the number of leaves in t 2 ; which is this plus this which is at most 2 to the k plus 1 and that is the end of the proof  refer slide time  23  28  so  let us just write this  so number of leaves t equals number of leaves t 1 plus number of leaves t 2 that is less than 2 to the k plus 2 to the k 2 to the k plus 1 that finishes the inductive proof  that on a tree of height h when i have a binary tree of height h  the number of leaves is at most 2 to the h  refer slide time  24  02  number of leaves is less than equal to 2 to the h well taking logs  i get log the number of leaves is the lower bound on the height well we know remember that in our binary tree  the number of leaves was n factorial so  the height this means  the height is at least log base 2 of n factorial well  this is the height of any of the tree  which is related to any this flowcharts  which sort flowcharts for sorting we looked at these flowcharts so  we said that number of leaves is n factorial and we have just seen that  the height must be at least the log of n factorial so  what is this  we are very close log of n factorial is at least log base 2 n by 2 to the n by 2  why is this true ? well  this is true  because n factorial is n into n minus 1  somewhere n by 2 and then  all the way up to 1  now if you look at the top n by 2 terms  all of them are greater than n by 2 there are n by 2 terms and all of them are at least n by 2  all of whom are greater than equal to n by 2 so  n factorial is at least n by 2 to the n by 2 well  that is why this is true and this is nothing but n by 2 log n by 2 this is omega n log n  so the time taken is at least n log n this is what we have proved some constant times n log n so  this shows that merge sort  for instance we know it takes maximum time n log n but  there are inputs on which it takes  i mean log n  it must take so  similarly quick sort and any other algorithm in this model  which is if the only thing which the algorithm can do  is look at two element  compare them and perhaps move them around  swap them for instance or move them around if this is the only thing  that is possible then  the time taken by any such algorithm for sorting must be n log n there are sorting algorithms which do not fit into this model we will not cover those  hopefully we have covered some of them in your data structures course these are bucket sort radix sort  well these algorithms differ in that they you can look at the least significant digit you can divide  you can take floors of these numbers if they are integers  you can take floors and then  do funny things with them the difference between these algorithms and let us say something like merge sort is merge sort  when you look at 2 numbers the only operation  that you can do is compare them on any element  if i take an element of the array only thing i am allowed to do on this element is compare this element with some other element in the array i can not sort of divide it by 10  20  100 whatever i can not look at it is least significant bit etcetera  etcetera that is as far as sorting is concerned we will continue with our discussion of algorithm design techniques  by looking at some other problems  which are fairly interesting and where this divide and conquer paradigm sort of works  and gives you better results in some cases perhaps you the algorithm  that we desire  refer slide time  28  58  the first one is this  so you are you are given a 2 k so  this is the input by 2 k square and what you want as output is a tiling with  i will tell you what all this means  with this what do i mean by tiling  a tiling essentially means  i must fill up all these squares and try and fill up as many squares as possible with pieces  which look like this so  each piece is  let us say a kind of a domino  which sort of looks is of this shape and i want to try and fill up  as many squares as possible in this so  well you can see immediately  i may not be able to fill up all square even for k equals 1  if i have a 2 by 2 square  i have a 2 by 2 square well i can put one of them so  i can put something which fills up these three squares and this will be an empty square so  here is a tiling of a 2 by 2 square with a domino of this form  which does everything except one square so  what about 2 to the k cross 2 to the k well  the number of squares  we look at the number of squares is 2 to the 2 k these are the number of squares  the number of squares and this is 1 mod 3 i will let you figure this one out the number of squares is always 1 mod 3 well it is 2 here and you can see  that it is always two if you want you can prove it by induction on k  but even otherwise you can sort of check that  the number of squares will be always 1 mod 3 maybe we will just look at a equals 2 so  then it is a 4 by 4  how does this look like so  this is a 4 by 4 thing and you have well 16 squares ; and that is 1 mod 3 when i divide this by 3 i get 1  so 3 times 5 is 15 and 1  one gets left out so  that is true whatever k i choose  it is always 3 times something plus 1 so  that is what i mean by 1 modulo 3 when i divide by 3 i get 1  the remainder is 1 so  can i sort of now put these dominos so that  all except 1 square is covered so  i have this 2 by 2 k 2 to the k by 2 to the k square so  i will square with side length to the k i have these dominos  which each of them has 3 squares put together can i arrange these on these squares  so that all except 1 square is covered  this is the question ; the answer is yes and we would like an algorithm to do this and well  we would like to do it using divide and conquer that is the goal  let us do this 4 by 4 and then  we will see what is the scene can i do a 4 by 4 with  so here is a 4 by 4 square  can i do this well  since i am going to do something like divide and conquer i would like to divide this into two parts  or more in this case four parts  tile each of them and put them together  refer slide time  33  24  well  let us take a bigger one and see how this is done i know how to tile this  so that one is left  i know how to tile this  so that one is left and so on now  how do i tile the whole thing  so that one is left well supposing the one left here is this one left here is this and one left here is this now  i am able to tile all four of them so  i tile this  so that this is my first domino  i use my first domino there  i put it around like this this is my second domino  i put it around like this here is 3  it does not matter how i put my fourth as of now we will we will change this later  supposing the fourth is like this well now i can put the fifth domino here  here 5 5 5 so  i managed to sort of tile at least 4 by 4 using 5 dominos i have just one left which is what i want you would like to repeat this for 2 k by 2 k so  how does that look so  here we again would like to do a divide and conquer  we would like to divide this i would like to first tile this  tile these 4 and then put them together well  by looking at this  this example i can do it if provided this square left off here is this the square left off in this  is this the square left off here is this here i really do not care if i can tile the entire region here  except this corner and so on for these 3  then i am able to tile the entire region on top leaving one squares that square left will be in this big sort of region this is 2 to the k minus 1  2 to the k minus 1  2 to the k minus 1 and so on so  this whole thing is 2 to the k so  if i can do this  then i can fit in this entire  this extra sort of domino here there will be one square left over in this region  good so  again notice a crucial thing  to be able to carry this inductive process  to completion i should be able to have the extra square i mean the square left over as a corner square when i tile a region which is like this  the left over square must be a corner square so  this is a square left over or left un tiled i have tiled the entire region  except this small square if i can do this  then clearly i can do the big thing so  i started out my initial objective was to tile to 2 to the k by 2 to the k that is what i started out with  i want to apply this divide and conquer kind of thing well  actually the process is really induction always if i can do it for a small square  can i push it up to a bigger square  that is the goal well  we want to do divide and conquer so  we saw that  we break it up into these 4 smaller squares now  i want to tile these smaller squares and put them up  to a tiling for the bigger square well  each of these smaller squares  that is one square  one small square which is not tiled one piece which is not tiled now  this happens to be the corner  then i can push the induction upwards but  for the induction to work even the final  so let us look at this again for the induction to work in this big square  i want the one square which is not tiled to be a corner well  it can not be corner here  because everything else is tiled  this can not be a corner  it can not be a corner well  can it be this corner  the answer is yes it is like i leave this square un tiled  so leave this now  when i look at these 4 small problems  they all look absolutely alike i want to tile everything  except a small square in the corner i want to tile all of it  except this square in the corner except this in the corner and except this in the corner and i put them together like this this tile is the entire big square  except for this one thing in the corner and well this is the algorithm  break it up into 2 tile these 4 recursively and put them together like this  and in the middle i just put one domino one domino fill up these three squares  so i have tile everything except this so  this is a well divide and conquer sort of strategy to tile squares so  this problem looks a bit different from the other problems and that is why i picked this  because the strategy can be used not just for computer algorithms or puzzles  or whatever you have and the other sort of lesson  that we learn through this example is that you start with some problem you want to solve we have seen this before  along the way you would like to apply  you want to find solutions to smaller sub problems you want to put these together to get a solution to a larger problem and while doing this  you may want to solve something slightly more general and then  you see this general problem can be solved so  you try and solve the general problem now  again using the same technique the smaller sub problems  you solve the general problem and now you see  if you can put them together  to get a solution to the general problem for the big input and often this sort of gives you the solution that want we saw this in two occasions  one was this tiling problem  the other was a i hope you can remember it was when we looked at this median we wanted to find the median  but we ended up looking at the problem of finding an element of given an array and a positive integer r so  we found the element of rank r  that is what we did so  let us look at one more problem  yet another problem rather  which uses divide and conquer so  this is a familiar problem of multiplying to two integers so  let us look at this  refer slide time  40  58  so  i have 2 n bit integers  x and y  this is x  this is y  these are n bits each is n bit long  i want to find the product of x and y the normal way is well  we formed the product of each of these bits with lsb  then with the next one and so on  and so forth so  there are n square multiplications well  in this case actually it is not  you can just do are and since they are bits but  you can think of these as instead of bits  you can think of them as digits  it really does not matter so  each of them could be n digits long and we multiply these digits so  there are n square multiplications  this is the high school multiplication algorithm  that we know can we do faster  that is the question let us try applying divide and conquer i have divide this into two parts this is x left x right  y left y right and now i want to compute the product of these two  well what is the product first of all x is nothing but 2 to n by 2 times x l plus x r y is 2 to the n by 2 y l plus so  this means x y is 2 to n x l y l plus 2 to the n by 2 x l y r x l y r plus y l x r plus x r times y r  this is the product x and y so  we could recursively sort of find these products  these four products and then  compute this this is just shifting by n bits  we are just shifting by n by 2 bits so  that is an order n operation  so if t n is the time taken  this what is the recurrence there are 1  2  3  4  4 problems  so 4 times t n by 2 plus order n this is some constant times n  which is for shifting and adding adding two n bit numbers  it takes alternately so  what is the solution to this recurrence ? well  i will let you do this and the solution to this recurrence you do a solution to this recurrence  it is order n square so  we are back to where we started compare these two  well we have done divide and conquer  but we do not seem to have conquered anything we are back to spending order n square times  even with this divide and conquer approach  but well all is not lost i will first indicate this algorithm and then  we will analyze  and see how we can decrease the number of multiplications so  well the trick is this  let us see  refer slide time  45  02  so  the terms that we want to compute are x l y l and then  x l y r plus y l x r and x r y r these are the three terms  that we would like to compute the way we did it  was each of them separately that gave over problems of size n by 2 so  here is a smart way of doing this well  these two i compute as they are  the third thing i compute is not quite these two  it is the following i computer x l plus x r times y l plus y r  i add x l and x r  i add y l and y r and then  multiply them together what is this  this is x l y l plus x l y r plus x r y l plus x r y r  that is what this is so  i do three multiplications and not four  this is the first multiplication x l y l x r y r is my second multiplication and this is the third this i do not do  as of now i have not touched this  this is the third multiplication i do well  if you look at this x l y l is present  x r y r is present ; and if i remove these two what is left is this x l y r plus x r y l is exactly this well  i have just written x r y l  but it is same as y l x r good so  how do i compute this  well from this product i just subtract these two to get this  so call this z now  i just do z minus x l y l minus x r y r i get this  good what have i done well i have a few more additions  i have 1  2  3  4 more additions the number of multiplications has gone down by 1 multiplying 2  n by 2 bit numbers i do 1  2 and 3 there are now only 3 multiplications that i do so  what is the recurrence now  well t n is 3 t n by 2 plus order n earlier remember it was 4 t n by 2 plus order n now  it is 3 t n by 2 plus order n and it is not surprising  that the solution to this recurrence is smaller in fact  we will show the solution to this recurrence  is order n to the log 3 base 2 roughly it grows as n to the 1.59  which is certainly less than n square  which is what we started out with so  this the number of multiplications one makes is significantly smaller  refer slide time  48  23  is 3 t n by 2 plus some let us this take n  it does not matter  so t n is this we will do the usual thing which is 3  so 3 t n by 2 square plus n by 2 plus n this is 3 square t n by 2 square plus 3 times n by 2 plus n let us do it once more  to see what happens  so this is 3 square t well n by 2 cube i am expanding this out  so there should be a 3 here  3 times plus n by 2 square plus 3 times n by 2 plus n so  this is now 3 cube t n by 2 cube plus  well 3 by 2 whole square times n plus 3 by 2 n plus n now  one can guess the general term  in this when i go down i steps  refer slide time  50  11  so  t n will be 3 to the i t of n by 2 to the i plus 3 by 2 to the i minus 1 n plus 3 by 2 i minus 2 n so on  to n  good so  now we would like n by 2 to the i should be 2 or say 1  for ease of computation if this is one  if i have one bit and the time is 1  so the time is just this becomes t of 1 is 1 so  if this is true and t of n is 3 to the i plus  well n times 1 plus 3 half and so on  up to 3 half to the i minus 1 well  let us do this  so this is 3 to the i plus n times  well what is this ? this is just a geometric series so  it is a roughly some constant times 3 half to the i you can do it more a bit better by doing the  writing the exact formula  which is 3 half to the i minus 1 by 3 half minus 1 so  it is at most  let me put it at most this is perhaps 3 half minus 1 which is half  so c could be 2  so c is most probably 2 in any case  this is bounded by something like this  3 to the i plus so  what is i ? well  2 to the i is n  so i is nothing but log base 2 n so  this is 3 to the log base 2 n plus n times constant times  well 3 half to the i  which is 3 half to the log based 2 n now  2 to the log base 2 of n is nothing but n  these two things cancel so  i have 3 to the log base 2 of n times constant and 3 to the log base 2 of n here so  this is nothing but some c prime times 3 to the log base 2 of n  this is the i time taken  refer slide time  53  26  so  what is this  well so the time  the total number of multiplications is 3 to the log base 2 of n this is 3 to the log base 3 of n divided by log base 3 of 2  i could take this up so  this is nothing but 3 to the log base 2 of 3 times  use just some general manipulations so  that is 3 to the log base 3 of n  i take this inside  n to the log base 2 of 3 this is nothing but n to the log 2 of 3 as promised so  this is roughly n to the 1.59 so  the time number of multiplications  we got it down from n square so  the initial thing to roughly n to the 1.59 of course  we increase the number of additions  but if multiplications are more expensive than additions then  we have saved on time this trick of using three multiplications instead of 2 is actually a trick it is something you have to really need to come up with somehow there is no real sort of way in which you can do it there is no method and that is the beauty of algorithm design that somehow not every problem can be sort of tackled using everything that  you could come up often you come up with  all the time you come up problems which require you to design new methods  derive new methods  find new methods rack your brain and sort of come up with good algorithms and when you do come up with these new algorithms  you feel again  the kick that you derive  the pleasure that you derive is something else this trick by the way or this multiplication is due to euler and he used this for the multiplying complex numbers we have just used this trick was then used by other people  who saw that it can be applicable in multiplying 2 n bit numbers thanks design and analysis of algorithms prof sunder vishwanathan department of computer science engineering indian institute of technology  bombay lecture  9 divide and conquer  iv closest pair we will continue with our study of algorithm design techniques we had looked at some examples or which we used this divide and conquer strategy we also looked at sorting the strategy  applied to sorting to give a lower bound of n log n for certain sorting based algorithm we continue along the same way in fact we look at yet another problem which some meanable to divide and conquer this problem comes from computational geometry so  it has the basic is bit different  but the same strategies  algorithm design strategies applied even to field so  the problem is this  refer slide time  01  54  the input the set of points in the plane say the output i want the closest pair of points so  given some points  i can compute the distance  we can compute the distance and among these points  that you have given in the plane  you want to find the closest way for instance  the points could be given with the x y coordinates  that what we will assume so  for each point  i give you the x and y coordinate and actually there are n points and given these n points  with their x y coordinates i want to find a pair of points  which are closest to each other so  given any two points the distance ; the distance is square root of x 1 minus x 2 squared plus y 1 minus y 2 square so  the usual notion of distance that you have in the plane so  for each plane of points  there is a distance and among there are n choosable distances so  between these pairs of points  if i have n points  there are n choosable distances i want to pick the minimum distance and the points  which actually witness this minimum distance so  there is an obvious way to do it it is to compute for each pair a distance  for each pair you compute the distance now  you have n choose to distances and you find the minimum  so that takes n choosable time  can you do faster  can you do better so  for instance  do you really have to compute all distances to find the closest pair well  that is the big question there  and the answer is no if the answer is yes  then i guess i would not be discussing this problem here   refer time  04  30    so  the answer is yes and in fact  we will do it much faster than  then n square so  somehow you do not really need to compute all pairs  distances of all pairs  that is the moral of the story let us see how we go about designing well  on the face of it  if you would think a bit you start wonder  why should i need to compute every pair of distance what can i mean  can i for instance  look at geometry of the points to certain distances  maybe there are many ways of thinking about it we should also be thinking of an inductive approach  which is supposing  i remove one point to compute a distance  shortest distance between the other points now  adding this point  can i get rid of some points  may be yes  may be no i mean  so let us look at this approach this will not be the approach finally follow  but this is something that i wanted to think about for every problem so  that is the reason i wanted to go over this again so  the classic sort of inductive approach to this any such problem is remove a point the recurs some the rest  put the point back and see what you get  good  refer slide time  06  13  so  let us see this is the point i have to move  here the point set in this  let us say in this region there are other points  this huge big star  this dot is what i have remove now  i find the smallest distance  now what can i do ? i mean although i compute  what is distance well  it looks like  i mean the obvious thing is to compute distances from this to every other point but  well that is not true way you are back to the original to where you started because  there are n minus 1  such other points and you will compute n minus 1 distances and this recurrence you will see  will lead to in order n square solution  because your recurrence will be t n is n minus 1 plus t of n minus 1 this will lead to order n square  so you can check this well  can i get rid of some of these points now if you could do the following  suppose in the minimum distance here  over the entire set  except this point this is delta the minimum distance  you have calculated recursively is delta then  you know that any point  the points that are actually in contention  are points in a circle of radius delta  around this point only points in this circle are in contention the other points are well too far away if you can quickly compute these points  then you are in business i took n minus 1 times to extra time  but supposing i can do this much faster  somehow i do not know how but  if you can compute this much faster  then you are in good shape then  you can may be reduce this n minus 1 somehow to something else for instance  if you reduce this to log n  your n really good shape then  maybe you can push this further  so your think is should be along these lines how do i cut this n minus 1 log  refer slide time  09  02  so  before we go further  let us do this in one dimension so  what is this ? that mean  you are given a line and some points you are given some points and you want to find the closest pair along this line you want to find the closest pair  how will you do this ? well  i think some of you should see that  you do not need to really calculate all the n chooseful distances there are n points  there are n choose to sort of distances this may be you do not really need to do n chooseful  look at all n chooseful so  how will you sort of do this  well let think to notice in all these things  you need to notice something about the problem some property of the problem  that sort of pushes you up that let us you do things faster so  in this case  well the thing to notice is that  supposing i am looking at this point  the only candidate points are the adjacent ones these are the only candidate points if for each of these points  i can identify these candidates then  i am done because  for each point i need only to check two distances so  that leads to 2 n for each point i need to check two distances so  for n points i need to check 2 n distances but  which 2 that is a big question for this point  how do i find these two  which are neighbors in some sense if i can do that  then just 2 n distances  then well looks linear well  how do i find these two  may be just solved them  so in one dimension  sort well  once you sort them  look at them in increasing order of from left to right and for each point look at the previous and then  the next one compute these distances and choose the minimum then  you choose the minimum by scanning these points  sorting takes order n log n so  we started out with n chooseful distances  but really time taken is order n log n sorting dominates this procedure and if i sort of first sort it and then  i do this then it takes the time taken is n log n i have reduced it from n square to n log n there are some morals in this story first one is you should always try a simpler problem you are given this problem in two dimension it always pays to first check out what the problem means in one dimension you will learn a little bit  that is the first thing and you at least now know that  may be it is possible to do it in less than n choosable time because  one dimension certainly you can do it in n log n question one asks now is about what about two dimension  can you do is there something like sorting that i can do  which will help me out the answer is actually  yes and let us see how this is done  it is quite a smart algorithm so  we will try and apply  divide and conquer approach  reasonably blindly but as we proceed this fact we use some some trick that speed up   refer time  13  16    so  the first step is something you can think of  i want to divide the point into two parts find the minimum on the left part  find the minimum on the right part then  once you find the minimum  now i am going to use these minimums to compute the overall minimum will this help  will this not help  well will have to see before  we do this i think it is possibly illustrative to do this on the line we first sorted in founded the minimum  that was n log n let us see what this gives  this divide and conquer strategy gives on the line  refer slide time  14  06  so  i am given these n points  can i get something  can i do something better what would the divide and conquer strategy be  well divide the points into two parts one on the left  one on the right and recurs on the both ; and what ? once we get the minimum distance from this and that now  the only sort of thing that we have not check  is if the minimum distance between points on the left and point on the right i find the minimum here  i find the minimum there and then  the only distance i need to now compute is the point on the left  the last point on the left the first point on the right  find this distance and check this against the minimum  that i have computed so  i found let us say  delta l  delta r and this is delta so  i need to find this minimum so  once i compute delta l and delta r and delta  i just check the minimum of these and return the minimum of these three i need to make sure  that i find this point on the left recursively and i make find this point on the right recursively so  again this  if i want to do by divide and conquer then  i need to strengthen in   refer time  15  39    which is that not only do i find minimum distances but  also to find the last point  right most point and the left most point once i do this  then i can sort of put these two things together to get a solution i do not want to get into with this in detail  because the problem here is again splitting the input into two parts now  how do you split this into two parts well  we need to split it into two equal halves  then you need two halves then  we need to find the median  that takes linear time so  if i look on the recurrence  it looks like t n  this is 2 t n by 2 plus order n and this we know is n log n the solution to this is order n log n this is the time taken to find the median and divide  this is the divide step so  this does not really use sorting in any sense  except to find the median so  this is another way of doing this  using this plane  divided and conquered on these points on the line let us do this to points in the plane you have given points on the plane and i am going to divide this into two parts let us say based on the x coordinate  the first n by 2  the left most n by 2 points  it will form one set the right most n by 2 points will form the other set i recursively find minimum distance  closest pair of points in the left  closest set of points in the right and now i need to find  closest pair for the entire thing and let see if this can be done fast that is the goal of this lecture  refer slide time  17  50  so  here let us see my points are points line this blob  divided into two parts there are n by 2 points here  so this is the left and that is the right there are n by 2 points  n by 2 points here now  i find closest pair points in the left  closest pair of points from the right let us say  that delta l is the closest  is the minimum distance on the left  delta r minimum distance on the right delta be minimum of delta l and delta r  this is the smaller of these two now  i need to find closest pair and i know that  if at all there are candidates or closest pair  it has to be that one point has to be on the left and one on the right any candidate pair has to be of this form they can not both be on the left and both on the right one of them has to be on the left ; and one of them has to be on the right what else can be said  let us also sort of at the same time the recurrence in place so  i am looking t n  so t n is the time i have already done this plus what ? well  i would like this ideally to be if this is say order n  then this recurrence is n log n  i mean good shape so  this is the amount of work that i will need  to find the closest pair one from this side and one from that side ; where one point is from this side  one point is from that side suppose  i do it well the natural way is to just pick every point on the left  every point on the right and sort of find the distances  for every pair this could be as large as  well that could be n by 2 there are n by 2 points and n by 2 points here  that gives rise to n square by 4 pairs so  if i compute n square by 4 distances i am   refer time  19  58    that is just too much in fact  we will back to our usual n square we will be  you can check that  you compute all distances  all pairs of distances  the n chooseful distances so  this is not what we want to do so  how can we speed things   refer time  20  20    this is the question  that you need to answer now  at this point  it is left to your ingenuity it is left to little bit of luck  but there are no clear it is not as if at least i do not know of any thing that i can teach you  which you can use this is left to just  your own intelligence and perseverance you can try small problem  small examples etcetera  etcetera but  the idea from this point onwards are completely new they are problem dependent  depends on the problem and there are no general techniques  that i can tell you which you can use there are no formulae 's that you can apply and this in fact  is the beauty of the speed  that it is not that i pump you full of ideas and then  you are all ready to take on a problem that is not the way to us each problem has a flavor of it is own and each candidate also has his own capability and each person has his own capabilities and is a good chance that  if you try this problem  you will come up with your own algorithm  your own passed algorithm so  from this point onwards  i am going to tell you a few tricks by which we will reduce this these observations are which i will make or we will find reasonably simple initially  but these we have to find on your own there is no teacher to teach you this  you have to learn it yourself it just depends on your analytical skills  good so  let us now get back to this problem the first thing i am going to note is  where do these supposing i have these candidate points on the left and where can they live  so let us look at this so  this is my dividing line  dividing vertical line things are the left  where can they live  well they can not  they all have to be  we draw a straight lines here they have to be within a distance delta of this middle point  similarly on this side if a point lies here  then the distance between this point and any point on the right is greater than delta if it lies on this side  this point and anything on the other side  this distance is greater than delta so  i can forget about these points so  the only points of interest to us are points in this small  on these small bands on either side of the middle line each of these bands as bridge delta  these are the points of interest that is the first observation    refer time  23  19   so far it is all fairly simple now  just this is not enough the same problem could apply in the sense that  all n by 2 points on the left could lie in this band and all n by 2 points on the right could lie  you know the other band in which case we are again back to computing n square  roughly n square distance is the other smart thing the intuition is this  so let us magnify this small band around the centre  refer slide time  24  00  so  here my  this is the centre line and here are the two bands this is delta and this is also delta so  when i look to the left  if i take any point here  then i know that  if i draw a circle of radius delta  around this there is no point inside this circle the reason is this closest pair of points on the left  this is the left  this is delta so  there is nothing closer than delta to this there could be something at distance delta  that is fine so  there is nothing closer than delta these points are actually spread  they can not be a large number of points clustered together they are all spread out nicely  distance delta similarly  the points on the right we would like to use this fact  this is point number 1 that these points on both left and right are sort of spread apart so  there can not be too many points close by  somehow we need to use this what else can be said  well let us look at this point on the left so  let us look at the point on the left  what about points on the right where can they live  let us draw this line well  i know that any candidates for this point will lie over this does not look like a square  but think of this as square so  any candidates for p  must lie within either this square or this square please strain your imagination and imagine this to be a square then  any candidate for this point must lie  either in this square or that square they can not be  if it is outside here  then the distance is greater than delta in fact  distance to this line is greater than delta so  this point is also greater than delta so  everything has to lie here  and that is the first the second thing we know  that if i take any point here  then this precludes many points from appearing in region of size delta if i have a point  i have a point in there then  in a circle of radius delta no other point appears so  there can not be  i mean intuitively one feels that  there can not be too many points inside each square this is delta by delta square  if there are too many points inside this there are bound to be two points which are close by i can not have every point as far away as delta this is what we are going to use  but how do we use this the intuitively you feel that  there can not be too many points inside the square the reason is essentially this  that if i have a point  there is nothing within a distance delta  good so  how do we put this intuition into practice well  we use a very simple principle  that you have heard many time it seems very simple  but putting it to practice often requires some thought the principle is called pigeon hole principle  well what does it say it says that  if there are n plus 1 pigeons and there are n holes and you stuff these pigeons into these holes  then at least one hole  must have two pigeons there are more pigeons and holes so  if you stuff these pigeons into holes ; and at least one hole must contain two pigeons very simple principle  we all immediately understand the statement  but often very difficult to figure out where to apply and how to apply really nice problem  solutions based on this principle  which smart people have figured out so  let us go back to this square problem  refer slide time  28  37  so  i have this square of size delta and i know that  if i have any point in this square then  no other point is within a circle of radius ; which means given any two points  the distance is at least delta between any pair so  the distance between any pair of points in this square  is at least delta how many points can i put in inside this square well  here is the trick  what you do is divide this into four parts these will be our holes  you have to see where the pigeons are well  what is the maximum distance inside the small square well  this small square as side length delta by 2 maximum distance is along the diagonal  which is square root of delta square by 4 plus delta square by 4 so  this is the square root of delta square by 2 this twice this which is nothing but delta by square root 2  which is strictly less than delta which is what we want so  if i take one of these small squares  then the maximum distance is delta  which means i can not have two points sitting inside this small square if i take any two points  the distance between them is less than delta so  at most one point can sit inside the small square  which means in the big square i can not at most four points if i say the distance between any pair of points is at least delta inside this big square  i can have at most four  this is the big trick  that we will use so  this is the trick we will use  so let us get back to the original picture here  refer slide time  30  58  so  i have p and how many points on the right  do i have to really compare well  four inside this  there could be at most 4 candidates in each square  if at all 4 or less than equal to 4  to be less it looks like as of now  for each point i need to at most look at 8 other points and compute this much and this certainly looks to be linear for each point i need to compute at max  let us say  8 other distances then 8  10 distances is what i need to compute and my recurrence i will have 2 t n plus by 2 plus t n ; and this is n log n how will we do this  we still not done i mean  we have all the ideas in place now  it is implementation smaller implementation details are we need to worry about for instance  for each point on the left  how do you compute these 8 points  you have to do this fast remember you can not take a lot of time to we look at all the points for each points  then your sort  because that is n square already but  again looking at this picture  you should again get ideas so  let us go back to this picture so  here is the point  the 8 points are somehow located in the neighborhood of this point they are at distance delta from here delta from here delta these are the points that i need to and this   refer time  32  44   i need to pick up so  let me put up another picture and after this picture hopefully the algorithm will be clear  refer slide time  32  54  so  here is my centre line and here are these various points the left in the right here are the various points if i take any point on this side  if i consider this then  i only need to look at a window size to delta  delta on this side  delta on that side this is what i need to do ? as i go up  if i look at this point this window shifts up  it somehow if i can scan them upwards  in this order  scan from bottom top if i can scan this points from bottom to top  somehow this window also keeps you think from bottom to top and it again looks like i can do this efficiently  all we need is these things altered by y coordinates if i have these points altered by y coordinates  then i can scan the left side from bottom to top  the right side from bottom to top and i can maintain this window easily  this is the last trick in the algorithm there is a detailed left so  if i have these points sorted by the y coordinates then this is what i do ? i start looking at the points on the left in the narrow band i look at these points  look at them in increasing y coordinates and for each point i have i also maintain this band of this sort of window on the right so  when i look at a point  i have a corresponding window on the right when i move to the next point i correspondingly move them window up this is all i do  so somehow i need to move this window up and you can see that  it is not it should not be too difficult  to move the window up and the time that it takes is linear with and the very similar to the merge sort idea the window moves up each time on the left each time  you know a pointer moves up  on the right side the window could move up  but it can move up only n time the window can move up only n time  because there are only n points so  at least the movement  the number of times the pointer here moves or pointer here moves  that is linear and once you have fixed a window and a point on the left  i need to compute distances in this window and i know  because of the geometry that there are at most eight points in this window  four on top four on bottom there are at most eight points i need to compute these eight distances and choose the minimum that is it so  let us put all this together and see what we get ? that is before we do that  i just want to point out one thing so  we need the points sorted on to five coordinate now  if we do it on the fly  once we partitioned now if we look at points in the each band and now we sort these points along the y coordinate you take n log n time sorting takes n logging time and then  the recurrence looks like this  refer slide time  36  50  your recurrence would look like t n equals t n by 2 well  2 t n by 2 plus n log n  because this sorting will dominate once  we sort we can go over them in linear time but  this sorting will dominate and take n log n time this the solution to this recurrence is still not n long n  the solution you can check that the time to find what the solution recurrence is n log n square and it is not n log  we would like to be positive in fact  you would like to get rid of this n log n and replace this with order n so  that overall we get n log n and again the trick is we tree sort the points in the x and y coordinates we do not do it at each recursive call so  all i want is the point sorted on y coordinate suppose  i can do it once in for all initially and use this effectively and that is what i want to this n log n for sorting along both x and y coordinates i do right in the beginning  that is the one time cost i do not repeat this in each literature  it is a onetime cost i sort them i am done with and now i have this sorted order each time i make a recursive call so  each time i make a recursive call i have this sorted order in place in which case  perhaps i can put order n instead of n log n and you get order n log n in total time so  let us look at this algorithm  now we have i have told you all the pieces  we just have to put them together and analyze the algorithm and see how much time it takes  refer slide time  38  52  we first sort to  so input a set of points first step 1  sort them on both x and y coordinates at this point i would like to make an assumption  which is that each point has distinct every point has a distinct x coordinate and every point has a distinct y coordinate  which means if i take any vertical line at most one point lies on it if you take any horizontal line at most one point lie on that this is not really a big assumption it will not it affect us for instance what i could do is take the initial set of points now  pert up each points slightly  add a small epsilon to each point it sort of move them around a little bit so  that the minimum distance does not change  these perturbations are very  very small ; these perturbations are very small the minimum distance to change was a points do not change the points participate in the minimum distance will that does not change while this perturbation  if i make sure that if i take any vertical line two points do not lie on this vertical line actually the algorithm i give right now  will assume that you know no two points have the same x coordinate or y coordinate you can  then try and modify this algorithm to work even in the case there are points on with the same   refer time  40  36    so  i sort the points on both x and y co-ordinates so  i have two arrays the first array where points are sorted on x coordinates second where points are sorted on y coordinates this is done right at the beginning i am not going to do this recursively now  the actual procedure starts so  the input to this procedure  the input is points sorted on x and y co-ordinates points sorted on x coordinates  same points sorted on y coordinates so  i have two sorted arrays this is the input  what i do the first step  let me call this step 0 this is the initialization phase step 1 here is divide  step 2 recurse on both  divide into halves and step 3 put them back look at these two solutions and now look at these points and find the minimum and   refer time  42  06   broadly these are the three steps so  let us expand on all these steps  here is step 1  refer slide time  42  19  step 1 is the divide step  this is the divide step well  i have the point sorted on x coordinates so  i can find the left most  so since points are sorted on x coordinates  find the left most n by 2 elements and right most n by 2 points  these are points this sort this give you the two points  the two sets now  i need to do this even for the points sorted on y coordinates so  i have two arrays remember so  let me call them  let us say a and b so  a is sorted on x coordinate  b is sorted on y coordinate splitting a is easy  leftmost n by 2 elements and right most n by   refer time  43  22    so  this algorithm that we have just seen is perhaps as you realize is more complicated  than what we have seen so far and we seen all the pieces actually i have given you all the details what they are going to now is summarize the whole thing  put them together and then  right out the recurrence  which will not be difficult once we put them on the steps and once we do that  we will see that this actually runs in n log n time so  let us summarize and write down this algorithm in one piece  refer slide time  44  06  the first step is initialization  it is initialization and here we sort the points on x coordinates and on y coordinate so  given the set of input points we form two lists so  we have two arrays one array which will store the points in increasing order of x coordinates the other array will store the points in increasing order of y coordinates and this will be the input to the procedure this initialization is done before we start the algorithm and each recursive step will pass down this information so  the next step is the divide step  so it is step 2  it is divide step so  here we first look at the median so  let the value of the median  when sorted on x coordinates  let us say x median so  now we split the input into two parts  points to the left of x median and points to the right of x median and then  we would like to recurse on these two parts to do this  we have to split both the arrays into two parts  both the arrays which sorted on x coordinates and the array which is sorted on y coordinates the x coordinate business is easy  because you know where the median is right it is n half the element so  you just split the array into two parts in a very natural rate for the y coordinates what you need to do is go through the entire array look at each point  look at it is x coordinate and then put it into an appropriate array so  now let us say these two arrays  where it is points are x coordinate so  these two arrays are called  let us say capital x and capital y so  split let me write this in a new paper  refer slide time  47  18  so  split x into x l for x left and x r points in x l have their x coordinate less than equal to x median  the median value and the others are in x r  the other points x r contains the other points similarly  split y into y l and y r remember  y sorted on y coordinates  y l and y r will also be sorted on y coordinates y l contains the same points as x l only sorted in y coordinates similarly  y r so  y r contains the same points as x r so  this actually divides the input into two parts initially you had this sorted arrays x and y and now you have created x l  x r  y l  y r the next step is the recurse step so  let us write that  so pass so  the input is divided into two parts x l  y l and x r  y r these are the same set of points  in x l they sorted in increasing x coordinates and y l they sorted in increasing y coordinates so  here is the recurse step so  you recurse on these two  when you get the shortest the distance of the shortest the closest pair on both sides  on either sides of x median  refer slide time  49  56  so  this is the recurse step so  step 3 is the recurse step so  what you get is let us say d l is the distance of the closest pair on input capital x l capital y l similarly  d r is the distance of the closest pair on the input x r  y r on input x r  y r the distance of the closest pair on the right hand side is d r the one on the left hand side is d l so  now you look at these two distances d l and d r and d compute d as the minimum of d l and d r so  that is the next step  where you compute the minimum of d l and d r now  let me quickly review what we have to do next around x median  we look at a band of size d on both sides so  let me draw a picture so  here is x median around this on both sides at a distance of d  we look at the band this is the on the left side  that is on the right side now  we look at points on these two sides and we know that  if you need to find points which are closer than d then  one point should come from the left one point should come from the right and they must be in this band so  let us first prove x l  y l  x r  y r to only points in these bands  refer slide time  52  27  so  step 4 is putting them together  is putting the answers together  so that is the next step so  x l prime will be points from x l whose x coordinates are  well greater than or equal to x median minus d so  just look at this figure   refer time  53  13    this is x median  we need points in this range so  any point will have it is x coordinate at most x median minus d  so that is step 4 so  similarly y l prime  so the y l prime same y coordinates of the same set of points  as in x l prime sorted on y coordinates so  you get these by pruning x l and y l so  you look at x l and y l and only look at these points in the band and you prove  remove some of those points so  you are left with x l prime and y l prime do a similar thing for right hand side  so you get x r prime and y r prime so  x r prime will be points from x r whose x coordinates are so  these are points from x r with x coordinates  which are less than equal to x median plus d these are points from y r with x coordinates less than equal to x median plus d so  these are the same set of points  only x r prime is sorted on x coordinates ; y r prime is sorted on y coordinates our focus from now will be on y l prime and y r prime we will only look at these points from increasing y coordinates so  we are still into step 4 so  let me recall what we do now so  here are the points let us say we have x r prime and y r prime  x l prime and y l prime  refer slide time  55  28  so  i look at  so here is my array x l prime and here is my array  let us say x r prime these arrays could have different sizes  because we have proved points from x l and y l  x l and x r this should be y l prime  because you want to look at the points and increasing y coordinates so  y l prime and y r prime are what we are concerned with so  maintain three pointers  let us say p  q and r p will point to something there  that is p  q and r will be pointers to y r prime so  let me quickly recall what we want a do for each point here  so here is what we are going to do so  recall that  for each point in y l prime  we will find at most 10 points this at most 10 follows from a discussion that we did earlier that in a square with 5 points  there will be two who are close as half the diagonal so  we will find at most 10 points whose y coordinates are  so supposing this point is  so for each point  let us say this point is some x prime  prime y coordinates are in the range y prime minus d to y prime plus d so  we find these points and then  compute the distance from x prime y prime to these points and this will add 10 more distances we do it for every point in this array once you do that among all these distances  10 times the size of number of points in y l prime  we compute the minimum that is our goal  refer slide time  58  17  so  a generic step is supposing  so a generic step is this  so i have let us say i have computed up to this point p is done  here is the letters q and r so  between q and r  we find the points with the correct range of y coordinates so  supposing we have done up to this now  in a generic step  i will raise p by 1  p is p plus 1  so this moves one step  now this is your new p so  now correspondingly q will move and r will move so  the next step is move q appropriately  then move r appropriately what do i mean by appropriately  you increment q till  if this value is not in the range y prime minus d to y prime plus d  which means it is smaller than y prime minus d so  increment as long as y coordinate is less than y prime minus d and similarly you move r  as long as it is less than y prime minus d finally  when you do this  then you will have these two values for the next value of d and now you compute the distances then  compute the distances of y prime l p and all points between y prime r q and y prime r small r so  then this is the generic step in the loop  put this in a loop and once you are done  you found these distances  compute the minimum so  the last step is compute the minimum of these distances  refer slide time  61  08  so  say it is some delta  then return min of delta and d and that is  ends the algorithm now  if you look at the analysis  then there is an initial so analysis  so the sorting initially is order n log n there are two recursive calls  so that is 2 t n by 2 and then  if you look at the whole analysis  the total time is o n so  we can quickly do this  while splitting does not take time the only problem is this pointer manipulation   refer time  62  13   in this pointer manipulation  how many times this p  q and r incremented well p can be incremented at most n times q can be incremented at most n times r can be incremented at most n times  so this is order n the total number of distances we know  we calculate is at most order n so  that is why this is order n and t n satisfies this recurrence and the solution is t n is o of n log n so  this is the analysis of this algorithm  it is pretty simple  and this ends our discussion on divide and conquer thank you design and analysis of algorithms prof sunder vishwanathan department of computer science engineering indian institute of technology  bombay lecture  10 greedy algorithms ? i  refer slide time  00  59  we will look at yet another algorithm design technique this one is interestingly called greedy algorithm or greedy techniques for designing algorithms so  the keyword here is greedy and well  it has something to do with greed as we know it is only a little bit the main technique main sort of technique for these for this algorithm design technique is the somehow piece together your optimum solution small piece by small piece by piece so  construct the optimum solution somehow by getting a small piece first then sort of enlarging it and so on and so forth till you get the final answer so  we will start with an example and we will see how this technique works in some sense it is a bit difficult to listen you have to some zero in on part of the optimum output the optimum has some output you have to somehow zero in on some part of it then slowly enlarge it and how do you end up with this small piece how do you enlarge this ? that is the crux of design technique here often there are no clear cut guiding rules that i can give you it is left to your sort of imagination and intuition hard work whatever perhaps luck that you come up with the right ideas that build in solutions so  let me write down the sort of basic ideas here  refer slide time  03  19  so  the greedy approach  the basic idea the basic step is construct the optimum solution or output piece by piece this is the basic step so  generic step in your algorithm which will typically be in a loop is having constructed some part of the optimum solution extend this you extend this by identifying the next part this is the crucial generic step which you will have to design so  once you get the problem you somehow have to get this generic step usually one starts off by figuring out what the first step is then the generic step and then you just put the generic step in a loop and that is how the algorithm looks like well  the crucial question that you will have to answer for each of these problem is how do you identify you know the next part of the optimum so  you have got some part of the optimum somehow how do you zero in on the next part ? this is the sort of crucial question that you will have to answer  refer slide time  05  41  we will have to answer as we go along for this course the question is how you zero in on this extension ? by extension i mean extension of this optimum solution like you is building so  how do you get this ? well  for this course its intuition perhaps hard work well  some amount of luck and i will sort of sort of just say local improvement do not worry too much about what this means i will explain as we go along this may be the only key i can give you at this point this local improvement trick is not very formal it is sort of an informal idea that i am going to give you and we will see that in many of these problems you can sort of use some of these small tricks that i give you to get the algorithm but often you know you just have to rely on your own engineering as a bigger hint as to where all these algorithms comes from there is some there are some there is some theory behind it which we will not which as this says beyond the scope of this course  but i will tell you where at least some of it comes from there are 2 subjects so  let me sort of tell you what both are and i will strongly encourage you to read about them the first so  this is for this course so  future i encourage you to read the following one is matroids theory this is a theory of matroids and the second one is linear programming duality so  that is so  called primary dual method in linear programming duality which gives you a sort of method in using that method many of these so  called greedy algorithms can be devised in the way you pick up the next part often is dictated by this by this geometric structure so  but this is for the future forget about this after this course we have the time   refer time  08  28    please do read about these 2 they are very important algorithm design technique so  here is the exchange trick which i am going to tell you in a very sort of in a hand wavy fashion  refer slide time  08  45  so  here is the exchange trick or the local improvement trick so  this trick in this trick what you do is this ? so  supposing you have some solution some generic output for the problem there is an input we want the output well  usually we want an output which satisfies which is maximum in some respect or minimum in some respect optimum in some respect so  supposing you take some solution which may be which may not be optimum well  the question asked is can you improve the solution by adding or subtracting let us say 1 element can you improve the solution by adding one element to it or subtracting one element from it ? so  this question will sort of dictate how the algorithm proceeds this question will keep asking and we will see how answers to this question we will sort have trigger our algorithms ; we will sort of push the algorithms forward so  in some sense we may have partial solution you see how to improve the solution improves it and so on so  that is for all the theory behind greedy algorithms we will now get down into business and solve a few problems the first problem is the input i need to define a term which you may have seen before  refer slide time  11  20  it is a notion of an independent set and independent set in a graph g is well  it is a subset of the vertex set such that the there are no edges between any pair of vertices it is a subset let us say u of the vertex at v of g such that if i take any 2 vertices in u there is no edge between them such that for any x y belonging to u  x and y are vertices in u x y does not belong to the edge set of g there is no edge between any pair so  let us take an example suppose this is a graph let us complicated a graph so  let us give them some labels a b c d e f g h while you look at this graph let us see 1 independent set could be a if i put a in the independent set then b and d can not be part of the independent set  because this is an edge between 2 of them so  i could have c in it and f this is an independent set  a c and f is an independent set so  i could have other independent for instance a and c by this is also an independent set a and c this is an independent set there is no edge between a and c similarly  i could have let me put one more let us say b f so  if i have b and f this is an independent set i could have e h let us see i have e and h then c a this is an independent that is c a that is another independent set so  i have sort of   refer time  14  18    so  there are many others for instance each vertex by itself is an independent set just take an vertex itself it is an independent set so  it is just a collection of vertices so  there is no edge between that is an independent set now  for instance if these are radio stations it is a mobile radio stations and they are transmitting they want to transmit and 2 of these cant transmit if they sort of if they interfere so  then only an independent set can transmit at a time if i take if a b c d e f g h are radio stations and i suppose i draw an edge showing that these 2 fellows can not transmit at the same time  because of the same frequency  because of interference then at one particular frequency or at one particular time only an independent set can transmit so  that is the  that is roughly the motivation where independent sets come in there are other motivations there are other places where one looks for independent sets so  general problem is given a graph you want to find an independent set large size that is the general problem in fact  an independent set of maximum size that is the problems in that have been traced this problem tells out to be hard and we look at it later on in this course for now we will restrict the graphs   refer time  16  00    so  here is the problem in totality  refer slide time  16  09  so  the input the tree t by tree i mean a graph that is connected and acyclic the output i want a maximum sized independent set in t i want a subset of the vertex set of maximum size so  that there is no edge between any pair of them so  this is the problem that we would like to solve well  how you solve such a problem ? how do you find maximum sized independent set in t ? well  if t has n vertices let us say t has n vertices one way to do it is too we sort of group force way   refer time  17  24   sort of compute one to one   refer time  17  28   is to look at all possible subset or vertex set and look at see they are independent and choose the one which has maximum size now  this is just too expensive the time taken is least number of subsets of n vertices 2 to the n let us do it we are not going to do it much faster what else could one do well  pick a vertex if you pick a vertex in the independent set then the neighbors of this vertex will not be in an independent set so  that is by definition so  you pick this pick a vertex throughout its neighbors we will look at this graph again pick any vertex that remains throughout its neighbors and so on this for you will get an independent set that is fine  but i do ensure that the size is maximum if you pick the wrong vertex at any point then you may end up in trouble so  here is simple example so  let us say this is the graph if i pick this vertex if i pick this vertex then what   refer time  18  48   you get is an independent set of size 1 the optimum is of size 3 the solution here the solution to this problem is a subset or a vertex set and you have to somehow pick the right subset and the greedy solution greedy way says pick it one by one somehow which vertex would you like to put in the independent set   refer time  19  39    the independent set is large will you think about it for a while intuition tells you that you should pick a vertex of smallest degree in this graph so  if a vertex has this small degree then the number of neighbors that you throw up remember when i pick a vertex i have to throw out its neighbors number of neighbors i throw up is small in particular if i leaf in the independent set if i pick a leaf in the independent set then the only vertex i just throw out one vertex per leaf right and this actually works in this case this algorithm works though we need a proof to show that this technique actually it produces maximum size independent set the algorithm is just this the just pick a leaf throw out its neighbor and continue at each stage you will have a forest if you have a vertex of degree 0 it is not adjacent to anything else which you may come across as the algorithm proceeds just put it in the independent set blindly if no such vertex exist then i pick a leaf from there will be many trees as the algorithm proceeds pick any leaf and put it in the independent set and continue this algorithm actually works it ; however  needs a proof you need to prove that this simple algorithm that we just described actually works it is greedy in the sense that we start you piece you put the solution together piece by piece you pick a piece and slowly sort of proceed in this way once you have a part of the solution you do not change it this is the other sort of property if i have a few word vertices in the solution i just build the solution up further i do not sort of throw somehow and then put something and then do back tracking in this case we sort of ; obviously  to extend the solution which is to pick the vertex a minimum degree it works we will see many examples where simple things are not working there is yet another way of looking at this algorithm which i would like to describe this is a bit more general than what we have studied and this is the sort of exchange trick i promised which we will use over and over again  refer slide time  22  28  so  another way to come up with the same algorithm so  let us take an example to each so  let us say this is a tree ; tree in question and we would like to pick a maximum independent set in this tree supposing you have some subset which is an independent set let us say this and so on this is the algorithm has not picked this is just to sort of get your thinking process going supposing you have you have let us say somebody gives you an independent set this is an independent set somebody gives you this independent set so  the question is when can i exchange can i sort of add and subtract to this solution to increase the number of vertices in the independent set ? can i add and subtract to this solution to increase the size of the independent set ? well  let us look at this here i can add these 2 and remove this i can add these 2 and remove this from this example actually its look like i can always do this with leaves the sense that i can add these two and remove this so  this is a natural way to lead on to the question will leaves always do all leaves always fall in some optimum independent set so  this is the question we would like to answer and you can see the answer is yes why is the answer yes ? well  supposing this is the lets say this is the tree this is any tree and i have a leaf here i have any tree and i have a leaf well  what i can say well  one of these 2 better be in the independent set in the optimum independent set one of these better be in  because if this is not in i can always put the leaf in into an independent set so  one of them always has to be in the maximum independent set well  supposing i have this in the independent set and this is not   refer time  25  07   and then you notice that i can remove this from the independent set and put the leaf in the size of the independent set does not change the set still remains independent and the size of the independent set does not change i have picked one element out and i have put a element in and the size does not change and this is the exchange which we did earlier on when we tried to increase the size of the independent set so  this in fact  is a one line proof all leaves must be in you know in some optimum independent set so  you can start of by putting all leaves into the independent set you know there is an optimum which contains these remove their neighbors now  you can recurse this is the good thing with greedy algorithm if you pick a part of the output then you have a smaller input to deal with and you can just apply the same technique over and over again and the proof follows by induction in fact  the algorithm also is inductive you can right it as a recursive algorithm you move part of the output now  you have a modified input on which to work on and   refer time  26  23    so  that is the algorithm for this problem we move put all the leaves in the optimum independent set remove their neighbors the tree will now break into many parts and you just recurs in each part so  that is the algorithm for this problem so  here is the next problem so  imagine that you are the systems are your administering a system let us say you have one really fancy computer at your command and a lot of people want to use it so  they come and give you one so  you have a 24 hour sort of time slot in which to schedule them and they give you each of them comes and give you a time slot somebody comes and says 3 am to 5 am  somebody else says 2 pm to 7 pm these intervals could vary somebody could 12  30  12  31 pm to 12  32 pm 1.05 am to 3.51 pm and so on so  they give you sort of intervals each person gives you exactly 1 interval we can not spread his job we can not each day he has just one interval in which he can work we can give 1 interval so  many people come and they give you intervals in which they would like to occupy this machine and they do not want shorter intervals they wanted for their entire duration which they have asked for your job is to pick these intervals some intervals essentially schedule people on this machine so  that firstly  at each time just 1 person on the machine 2 people can not work on the machine and the second thing is maximum number of people get to use the machine in a 24 hour slot there is a 24 hour slot your objective final objective is maximum number of people get to use the machine it should not your policy should not be dictated on by any other   refer time  28  58   like the amount of money they are willing to give you etcetera  refer slide time  29  17  so  here is the  an abstract sort of origin of this problem you are given a set of intervals so  the input is a set of intervals you can take them as closed intervals if you want the output is a subset of non overlapping intervals of maximum size you must output a subset of non overlapping intervals with maximum size so  let us see what this looks like so  here is let us say this is 0 to 24 hours so  the input looks like here is an interval so  people give you various intervals some of them should overlap all kind of funny ways 2 people may want the same thing that is also possible and so on so this is how the input looks like so  this is person a b q z each person gives you an interval and you have to pick a subset of this intervals so  that they do not overlap so  if i pick a i can not pick q i can pick b  but i can not pick q and i want to pick a maximum number of these intervals we said that i want to subset on these intervals which is of maximum size so  that if i look at the intervals in the subset they do not overlap so  a solution to this we will see in the in the next class so  given a set of intervals you want a subset of overlapping intervals of maximum size our maximum size i mean the subset of maximum size so  this is the size of the subset which means number of intervals in the subset that should be maximum for instance here i can if i can take 4 intervals then that are better than picking 3 intervals even though the 3 intervals may be longer that is not our goal our goal is to pick numbers just numbers as many intervals that i can schedule you should pick that is the problem so  the problem you are solving is this the input in a set of intervals and we would like to pick a maximum number of them so  that pair wise they do not overlap so  if i take 2 intervals they do not overlap they are separating so  how do we do this ? well  greedy technique the so  called greedy technique says builds a solution piece by piece so  i am going to build the optimum solution interval by interval so  which interval would you pick first which interval can you say in lie in some optimum solution which is what does your intuition say well  i want you want to pick intervals so  that a large number of intervals so  intuition i think the first thing that sort of strikes you is that if i pick intervals in small size so  that the span is small maybe i can pick a large number of them  refer slide time  33  33  so  let us consider this let us call this algorithm 1 pick intervals interval of smallest size smallest span let us say this span or the interval suppose the interval is let us say goes from a to b ; b minus a is the span of the interval so  pick the smallest interval with smallest span well  smallest length is the one which you want this seems to be a reasonable algorithm this is what perhaps intuition suggests  but this intuition is faulty it is very easy to construct examples where this algorithm fails here is on here is an example where the algorithm fails well  let us take a small interval then take 2 large intervals that intersect us the algorithm says pick this one so  this is the input ; this 3 intervals form the input the algorithm says pick this once you pick this you can not pick these 2 fellows clearly optimum consists of picking these 2 big fellows these 2 big intervals the optimum has value 2 value of this picked one so  here the first sort of first thing that you can try fails so  picking the interval of smallest length not fails well  let me think a bit more now  we start thinking a bit more what else could work here if you look at this example  this small even though this interval is small it overlaps 2 of them while these big ones even they are big they overlap only 1 so  may be the right thing to do is to pick interval that overlaps with smallest number of intervals  refer slide time  35  45  so  here is the algorithm 2 ; pick the interval with which overlaps the smallest overlaps with the smallest number of intervals so  you see that the first algorithm fails you construct an example where the algorithm fails and here is an algorithm that at least works on that example it seems fairly reasonable also that you pick an interval which overlaps the smaller so  when i pick this interval i throw away the smallest number of intervals i pick an interval i throw away the smallest number of intervals and now  i recurse once i pick an interval every interval that overlaps with this interval i am not going to pick so  i throw those away and i recurse this should look very familiar to the previous problem there you pick vertex throw away its neighbors and go forward here you pick an interval throw away the intervals with which this overlaps and you proceed forward well  they are extremely closely related in fact  we could form a graph right here for an interval you have for each interval i can you can have an vertex and 2 there is an edge between two vertices if the intervals overlap then all you are doing for this problem is picking an independent set of maximum size in this graph the way you get the graph is for each interval you have a vertex and you join 2 vertices in the intervals overlap and you are picking an independent set of maximum size that is what the problem is this is an independent set of maximum size so  like in the previous case why not pick a vertex with smallest degree that is what this is an interval with minimum overlap minimum number of intervals that it overlaps is nothing  but a vertex of smallest degree in this graph so  pick a vertex of smallest degree then you get its neighbors and recurse while i have written just one sentence there is a recursion going on you pick this you pick an interval of smallest degree throughout all the intervals that it overlaps and now you recurse you again pick an interval of so  that it overlaps the smallest number in what remains throughout everything that it overlaps and so  on and continue so  that is the algorithm does it this work ? well  it worked in the previous case in the first problem this worked unfortunately in this case it does not work so  here is the example so  this is a bit more complicated the example now  which i have to construct so  that this algorithm does not work for work in this case is a bit more complicated so  here is the example so  far it looks like the old 1 we are going to now  apply more intervals and interval is 2 this is let me it ends here i do not want this to overlap with this these 2 do not overlap with each other well  similar similarly here  i can have many copies let us say k copies similarly  here k copies we can have many copies i just want to make sure that this interval does not overlap with those let us look at this example so  these intervals there are k copies of these intervals so  each interval overlaps with you know k minus 1 over here and 2 so  k plus 1 intervals is what this overlaps this interval overlaps with k of them this interval overlaps with k plus 1 this similarly  these are sort of mirror image this side is a mirror image of this side so  these intervals have a large overlap the smallest overlap is this interval is a interval with smallest overlap with smallest overlap in fact  it is 2 these 2 so  your algorithm will say pick this and recurse let see what happens when you pick this ? so  when i pick this while i will throw away these 2  because these 2 overlap with this so  now  i am left with imagine that these three intervals have gone i am left with this side these intervals on this side you can say that i can pick only one of these all of them pair wise overlap so  i can pick only one of these i can pick only of these so  and i have pick any one it really does not matter all of them have the same degree so  the solution size of the solution i construct is 3 the number of intervals that i have picked is 3 ; the first interval that i have picked interval with the smallest overlap and then 2 others after 2 others well  if you stare in this paper a bit more carefully so  if you stare at this a bit more carefully you can see that there is an optimum which has value 4 1 2 3 4 so  these 4 intervals do not intersect with each other so  this algorithm produced this i mean 1 2 3 sorry 1 2 3 intervals which do not overlap with each other on the other hand optimum value is 4 1 2 3 4 so  this algorithm also does not work the algorithm for roughly the same kind of problem as the previous one does not work in this case and you can see that constructing examples become harder and harder i mean you can come up with really complicated algorithms for which it will be it may not work  but to prove that it does not work you have to really come up with all kind of complicated counter examples so  now  what do we do what is the algorithm that works ? does any algorithm work ? well  what does the optimum look like ? what is the picture of the optimum ?  refer slide time  43  21  let us look at this so  it looks like this the optimum looks like this so  any solution must be a set of intervals that do no overlap with each other so  they have to look like this now  the exchange trick what is the exchange trick here what do i mean by exchange trick here ? well  can i add an interval here can i add an interval to this solution well  certainly if i have space supposing let us say this is how the solution looks like i can certainly add one here if there were one supposing these spaces are filled out can i add i may not be able to add  but can i add and remove one is that possible ? well  let us see supposing i add let us say add something like this now  this could intersect with more than 1 intervals it can intersect with this it could intersect with well  it may be something like this in which case adding this makes no sense there is no exchange that sort of you know keep you going i can not exchange this with some others i mean if i exchange this then i need to remove more than one and the size of the optimum falls is there an interval that i can always add to this optimum solution and maybe we move at most one ? is there some interval means i can always do ? this i can always you know add to this solution that i have and maybe we move at most one this requires a bit of thought will tell you that there is an interval and this is the interval that ends first so  let us see what this mean ? so  supposing what do i know about the interval that ends first it has to end before this so  the interval sends somewhere here this is the interval that ends first well  now we notice that i can add this and remove at most interval that is the first interval this will certainly not intersect with the second interval  because the starting time of the second interval comes after ending time of the first interval and ending time of this interval is either this or before so i can always add the interval that ends first and remove the first interval it maybe if it ends before   refer time  46  20  and increasing the size of the solution so  exchange trick well  exchange trick plus some intuition plus whatever tells us that i can add the interval that ends first you give me any solution i can add an interval that ends first may be remove the first interval certainly the size does not go down and this is then this is what  now leads us to the algorithm 3 which actually works  refer slide time  47  00  algorithm 3 says add interval that ends first remove the intervals that overlap with this and recurse this is the algorithm 3 and this actually does give us what we want so  instead of recursing of course  you could sort the algorithm the intervals by ending time so  you can write this as a iterative procedure you sort the intervals by ending times and pick the first interval the interval that ends first throughout the interval that this overlaps and then continue the iteration so  this also this works so  why does this work ? actually it is this exchange trick that makes exchange trick that we saw that is what makes it work and it will prove it by induction really first thing to prove would be show the interval that there is some optimum solution which contains the interval that ends first and the proof we have actually seen so  what you do is look at any optimum and look at the interval that ends first go back to previous figure supposing this where the optimum solution ? here is supposing it already contain the interval that first we have done so  there is an optimum solution that contains the interval that ends first otherwise take the interval that ends first it looks like this it has to end either at the same point or before add this remove the first interval and we now have an optimum solution which contains the interval that ends first there is some optimum solution that contains the interval that ends first so  i pick this as part of my solution remove the intervals that overlap with this and now recurse on what remains  and since i am recursing on a smaller set by induction the algorithm will find the optimum in what remains and   refer time  49  40    so  when i remove this and when i remove the intervals that overlap the interval that ends first the size of the optimum falls by at most one that you can see just by looking at this figure that we have i added this interval to remove this and what remains was this you know the rest of it it just dropped the optimum drops by 1 by induction i will find something of this size the algorithm finds something of this size and the algorithm output this plus 1 that is this interval so  the size of the solution that   refer time  50  20   output is the same as the size of the optimum which is what we want design and analysis of algorithms prof sunder vishwanathan department of computer science engineering indian institute of technology  bombay lecture ? 11 greedy algorithms ? ii we are discussing greedy strategies for algorithm design the main principle is to somehow construct solution to your problem piece by piece we saw two examples  last time let us quickly short of revise the main lessons that will learn  refer slide time  01  18  so  greedy strategies for algorithm design  has been reviewed main point is  you somehow construct the solution  piece by piece so  two things are important one is  how you  what is the next piece of your solution that is the crucial question that you have to answer and once  you answer this question  you just put it around in the loop and you get your algorithm the other thing we solve  especially with the 2nd problem  2nd problem was to find the  input was a set of intervals and you are the output was  maximum number of intervals that do not overlap with each other pair wise  they do not overlap with each other now  we saw that two obvious strategies for this problem field one was to pick  smallest interval or the strategy of picking and interval which overlapped  smallest number of other intervals  both of these seen to be natural strategies to consider in this field the strategy that worked before last time was to pick first the interval that ends first to pick an interval that ends first  remove the intervals at overlap with this and when you request on the rest  again you pick an interval that ends first and so on so  the set of moral of that story was obvious strategies may not work what is obvious to one may not be obvious to the other but  certain obvious strategies  which appear obvious to most normal human beings  may turn out not to work so  the point that i want to stress is a proof of correctness is important you come up with the greedy strategy ; you need to argue logically  that this strategy works so  let me write system  this is important so  hence a proof of correctness is necessary and important because  obvious things do not work  if you are giving in obvious solution to your problem  then you go to say that proof that this obvious solution is indeed correct so  this is important there are usually two ways of writing a proof for these greedy strategies one is induction since  you are constructing the solutions  piece by piece you find the first piece  argue that this is correct and then you proof of correctness  usually works by induction on the rest of the input so  you argue that on the rest of the input  your strategy by induction works and added to the first thing that you pick  it gives your optimal solution this is  what we followed yesterday now  this one more way of writing this  which is also quite popular  which i will call exchange so  this is proof by the exchange techniques this is very similar to the exchange trick that i have been mentioning throughout and we look at it the bit more detail what we will do is  take up this 2nd problem that you look at an actually  write the proof derive a proof by using this exchange business  just you get gathered into   refer time  05  41    so  here is of the proof by exchange words  refer slide time  05  51  let us review the algorithm first it is says  pick  this is among the intervals that remain  the interval that ends first discard intervals that overlap with this interval the whole thing is in a loop this whole thing is in a loop and you are done  when if discarded all the intervals we have an empty set start out with the set of intervals  do this in a loop till you do not have intervals to deal with now  this was the algorithm  we know that  this works let us now proof  using this sort of exchange strict that disworks the proof is actually logically the same  only it is written the bit differently  refer slide time  07  18  how does this work  suppose  so here the proof of correctness it is the 2nd proof of correctness suppose  the algorithm picked intervals of say i 1  i 2  i 3 so on to i k let us also  suppose that  they are sorted  say sorted by does not matter  whether it is finishing times or finishing times whether  it is sorted by finishing times  they are also sorted by starting time  because these intervals are disjoint so  the intervals  we look like this so  this is i 1  it is i 2  i 3  so on  next i k so  the intervals are disjoint so  sorting by finishing times and starting times is the same thing suppose  the algorithm  pick these intervals i 1  i 2  i 3 up to i k now  we have to somehow of show  that there is an optimum solution  which also picks these intervals if this is lead to  take the optimum  if this is an optimum solution  which as i 1  i 2  i 3 up to i k  then you are done because  every other interval in the input  must overlapped with at least one of these inputs the way the algorithm works  it picks an interval  throughout the intervals  which overlap with this and finally  we have an empty set so  there is a set of intervals  that we picked as i 1 through i k  it is set the rest of it has been discarded because  they have been  they overlapped with one of these intervals so  if any optimum solution has i 1  i 2 to i k in it  then this is it  your optimum must be must have size k so  let us pick so  this is what we need to show that  you know optimum  this is an optimum solution suppose  it is not so  there is no optimum  which as i 1  i 2  i 3 up to i k then  pick the optimum solution  which has maximum number of initial intervals in common with for the algorithm pick let me write this term so  suppose  so the claim that you want to make is  there is an optimum solution containing i 1  i 2 and so on up to i k well  in this case  we just saw that  if contains these  it must be exactly this  which case this is optimum so  if you prove this claim  we have done  refer slide time  10  44  so  the proof is  why by contradiction  proof of the claim by contradiction so  we start of by saying  suppose not  which means ; suppose the statement is false  there is no optimum solution  which contains i 1  i 2 all the way up to i k that is  why this is so  then  pick an optimum solution  which has the maximum number of initial solutions  initial intervals in common with i 1 up to i k what do i mean  so there maybe some optimum solutions  which contain i 1  which do not contain i 1 there maybe some  which contain i 1 and i 2  but not i 3 there may be some with contain i 1  i 2  i 3 all the way up to i 17  but not i 18 and so on so  all these optimum solutions pick 1 so  that the maximum number of initial intervals or in this optimum solution so  suppose then  this optimum solution has i 1  i 2  i 3 up to i j so  this has i 1  i 2 up to i j  but not i j plus 1 so  this says that  there is an optimum solution  which has i 1  i 2 up to i j and no optimum solution has i j plus 1 when  i look at all these optimum solutions  there could be many  none of them has i j plus 1 and then but  there is one  which as i 1  i 2 up to i j so  let us look at  just picks this and look at it now  since it has i 1 through i j  every interval  which overlaps with these is not present in the optimum so  every interval  which overlaps with i 1  i 2 up to i j is not in the optimum present in this optimum in fact  it is not present  even in the algorithms so  let us look at i j plus 1  refer slide time  14  20  so  here the optimum  it has i 1  i 2 up to i j then  it has some other interval  kolid interval k  j plus 1th interval in the optimum is k so  let the j plus 1th interval in the opt solution be k so  it is look like this and then  there are others this is of the optimum looks now  i j plus  so k looks like this  this is k now  the claim is that  i j plus 1  ends before the ending time of k so  this is the crucial  let me write this down the ending time of i j plus 1  this is what the algorithm picked it is less than equal to ending time of this interval k any time of i j plus 1 is less than equal to ending time of k well  which means i j plus 1  it must look like this i have k  this is k and i j plus 1 should look like this it could be the same as k but  it is suddenly less than equal to this and i j sits like this this one  this is the picture that we have now  we notice that  i can put i j plus 1 into this optimum and remove k so  look at this picture so  in this picture  your optimum has other intervals k and so on this is what the optimum looks like i can insert i j plus 1  the only other interval  it intersect with this k so  i can insert i j plus 1 and i can remove k because  this is the way the algorithm picked i j plus 1 and because  you pick and interval with this smallest ending time that is the reason the ending time of i j plus 1 is smaller than equal to ending time of k so  i can insert i j plus 1  remove k and this is the contradiction because  now i have an optimum of the same size  which has i j plus 1  that is the contradiction let me just write the term  refer slide time  17  16  so  hence  replacing k with i j plus 1  gives yet another optimum solution  which contains i 1  i 2 up to i j plus 1 now  which is a contradiction ; that is it so  this is the proof by contradiction  proof by this  exchange techniques by somehow  the exchange what the optimum has  with what the algorithm constructor as i said earlier  you could write the same proof by induction 2 that is another way of writing the same proof and you can pick  whichever way you want to write  whichever way you like so  this was the previous example let us look at some more problems the idea is  to give you a flavor of some kinds of algorithms are problems and algorithms some of them are easy and you know  you follow your notes and you get  exactly what you want some of them are will like this  one they may be slightly more non-trivial so  this is just to give you a feel of behinds the things that even do with greedy algorithms  refer slide time  19  17  so  here is the next one  the input set of intervals as in the previous case the output is  to partition this set into a minimum number of parts  such that  in each part  you have  only have  non-overlapping intervals so  this is similar  the input is similar so  let us go back to the scheduling problem ; that we had previously previously  you had in the set of intervals each interval is associated with the user so  the user gives an interval  where she wants to work and your job is to sort of  you know  satisfy as many users as possible only  two users can not work on this machine at the same time so  the intervals should not overlap now  you still want to schedule them only  you have more machines  you have many machines  on the same machine  you can not schedule to users at the same time so  i can not schedule to intervals  which overlapped on the same machine and i want to minimize the number of machines so  users come with these requirements  which is an interval each user comes within interval you have a number of machines you want to minimize the number of machines that we use so  which means  have to tell  for each user  which machine  he or she should sit-down and it should be search that  for each machine  two users  whose times overlapped  should not sit on the same machine so  if i look at the schedules for one machine  the intervals should be non-overlapping or disjoint so  this is the problem  then abstract way the input is the set of intervals and the output is to partition this set of intervals into minimum number of parts these are thing of each part is the processor within each part  i must only have non-overlapping intervals in other words  within each part  i can not have to intervals at overlap so  this is the problem ; that we would like to solve so  how this one  go ahead doing this  again the sort of  since we are discussing greedy techniques i guess  one would like to do it by  you know  this keep partitioning these intervals putting them in different parts  one by one till you are done so  look at these intervals in some orders say and you know  this side  which part  which processor that person going to sit down again  the crucial question is  which orders do you look at intervals you can try ending time  through i mean  since the previous time  in the previous problem  you know  we looked at intervals on based on ending times for interval at end it first was schedule on  was looked at first in the interval that in the next  we looked at since  so on this is the order in which we looked at the intervals does this work  i let you figure out in this works so  that is an exercise for you  just see  if you can look at intervals based on ending times  when you get a new interval  put in on one of these processors  which processor  well we have to decide that the set of greedy way of putting  you know  putting these intervals on to processors this order these processors 1  2  3  4  5  6  7  8  9 and so on and when you get an interval  put it on the first processor ; that you can so  there is some interval on the first processor to which this overlaps then  i do not put it in and so on so  i just keep going on to 1  2  3  4 and the first place  where i can put it  i put it with this sort of greedy back drop  supposing you look at intervals by finishing times  this is what  i want to check  whether this will give you an optimum solution well  we look at it differently will first sort by starting times we will sort intervals by starting times ; we consider an inputs in this order and what will do is this  when i come to the ith interval  i check  i order these groups 1  2  3  4  5 i check  whether i can put it into the first group  if i can well and good if not  i will look at the second group and so on so  i put it into the first group  where i can  this is the algorithm so  let us write this down then  we will see  why if at all this is optimum they does not seem to be any  i mean on the top on set of on the phase of it there is no real intuition as of now guiding me mean  why should i choose intervals  why should i order them by the starting time i could order them by the ending time ; i could have order them by sizes or any other parameter that you may have but  well  we will do it by starting times  means see what happens  refer slide time  26  22  so  here is the algorithm  start consider the intervals in the order specified by increasing starting times and so for an interval i  let us say i t this is the t th interval  when considered in this order put it in the first ; put it in the first group or part ; that you can so  you consider intervals in this order and when i look at an interval i t  put into the first parts that you can  which means in the first part in which it does not have a non  it does not have an overlapping interval if i put it into part l  say ; that means  when i look at part 1  part 2  part 3 up to part l minus 1 ; in all these  there is some interval  which overlaps with i t so  this is algorithm and why does it work  does it work well  it does  the algorithms does work now  let us in fact  prove it does works so  well  you run the algorithms on a set of inputs so  initially  the first interval goes into the first part 2nd interval  if it does not overlap  if the first one goes into the first one if it does overlap  it goes into the 2nd part and so on so  you just sort of scan your objective is some of to figure out  what happens to this to the number of parts  you are minimizing the number of parts to supposing the number of parts working  the algorithm produced k parts ; it is broken up into k parts somehow  we need to argue  i mean  if it is in fact optimum  that any algorithm will produce these k parts  refer slide time  29  28  so  let us see why ? so  let us say suppose the algorithm partitioned a set of intervals into k parts somehow  we have to now argue ; that any algorithm on this ends set of inputs would use k parts why should this be true ? well  let us looks at small values of k so  often  the other sort of trick  which is never sort of mention in text books is you should always trial small values of these parameters  you see  what really happens for instance  if the algorithm took two parts  if it divided into two parts  then you can you can see that  even the optimum will take two parts why is that so ? where  look at anything that landed in the 2nd part why was this interval put into the second parts ? because  it overlaps with something different parts now  if you have two overlapping intervals  you better you two parts so  if you two parts  you are through  anybody else in this you know working with the same input will use two parts what about three parts ? well  what do you know ; you know that  if something was put into the 3rd part it must overlap with some interval in the first part and some interval in the 2nd part let us look at this picture  refer slide time  33  48  so  here let us say i have an interval so  this was put into the 3rd part this interval was put into the 3rd part  which means  there must be an interval that overlapped  which was the first part and second part now  what are the various things that can happen  then i can have  this was in the first part and let us say  this was in the 2nd part this is one seen so  what is the other seen in that possible let us say  this was in the first part and let us say  this was in the second part let us look at these 2 now  in this scenario  i claimed that  any other algorithm will also use three parts  why is that ? well  at this line here  when i look at this line here  all three of them intersects so  all three of them must be put in different locations in different parts so  if it is this situation  then we are done  we have doing optimally as usual let us look at this situation now  in this situation  we can not see the same thing and optimum could have up put these two into one part in which case we have   refer time  33  08    well  can this situation happen  elsewhere  we see  you may progress  because remember we started by  we sorted these by starting times so  it looks like this can not happen this 2nd scenario can not happen so  focus on this so  in this scenario  it looks like this fellow  starts after this  which means it should be consider afterwards so  this scenario can not happen and in fact  our ordering  when we order  the ordering thing is to in fact  get read of these so  i guess  you know  how will an algorithm design or go about doing this well  you say  let me look at in some order  intervals are looked at in some order now  look for a proof  when you look for a proof  you see  how you have to tune your algorithm so  that the proof works and that is how the algorithm develops so  here  we want to look at these intervals in some order and put them into parts  put them into the first parts that it can we see that  if there is a point  where all of them intersect  when you are through  the only way  this can not happen is  you know  if you have a figure like that so  have an interval and have two intervals at do not intersect into different parts but  if this happens  one of them must have started after this 3rd interval one of them must start  after this interval that i am looking at right now then  you go back and see  can i get read out this will  can i get read out this in my proof to get read of this  now you notice that  if you had looked at these intervals in increasing order of starting times  this case will not arise and so  in rank to prove  you are actually refining your algorithm  you see that  where you are previous algorithms can go wrong and you refine this algorithm to take care of this case and you know  hopefully come up with this right algorithm before  looking at the problem  where you have a set of intervals and you want to partition  this set into minimum number of parts so  that each part  you have non-overlapping intervals  if this is the problem and the algorithm  we will like to consider is sort in intervals by starting time  look at them in increasing order of sort in staring time and when you look at an interval  put it into the first part ; that you can the first interval goes into the first part is 2nd interval will go into the 2nd part  if it overlaps with the first and so on so  this is the algorithm and in fact  you would like to prove that  this algorithm is optimum so  to prove this  well we say supposing the algorithm produced k parts we need to show that  any algorithm will produce k parts so  let us look at this  refer slide time  36  40  since  there are k parts let i be  let us say the first interval  the rest put into the kth part the first interval does not matter  any interval you can pick from the kth part but  let us look at the first one  interval that the algorithm put in the kth part we would like to now argue that  any algorithm will use k parts  why is that so  here is i  so this my interval i well  if i take any other interval  it has to start before i any other interval  which is already been considered so  if i look at any other interval  that is already being consider just start before i  this much i know i also know  when that in each of these parts in the parts  1  2  3  4 up to k minus 1  first k minus 1 parts  there must be an interval  which intersects with i how is that look like ? so  let us say in the the first part  how can interval will be well  it has to start before this and it has to be intersect this so  it has to end after this point so  it has be something like this it could  where it could start here and it could end after i  that is fine but  it has to start before this point and it should end after this point that is the crucial thing so  these intervals in the first part 2nd part and so on that i overlap with  must start before this and end after this  which means  this point must be common to all of these intervals so  all of these intervals overlap at this point and they must be put into different parts and you have k parts there are k intervals  which overlap at this point and hence  they must be put into k different parts so  any algorithm will use k  k different parts  this is the ideal so  let us write  refer time  39  07     refer slide time  39  09  so  let i be the interval a b in every part and this is one less then equal to every part p so  one less than or equal to p  less than or equal to k minus 1  there is an interval  which overlaps with i  because i was not put into this part  which overlaps with i since  i was not put into any of these parts ; it was put into k  part k now  what can be say  this means this interval  let us say i p so  what can we say about i p  well i p starts before a and ends after a it is starts before a  because i consider the intervals in increasing order of starting times and it ends after a  because it has to overlap with i so  this point a  is in all of these intervals so  point a  is in the intervals  i 1  i 2 up to i k minus 1 and i so  there are k intervals  which has point a so  we optimum must have size k that is the end of the proof i am not hiding that last sentence  just fill it in  your optimum must have size k  because of this  because of k intervals  which has a point and all of them so  the algorithm produces an optimum solution so  that is what  we want to proof so  well  what i want to point out with this  this problem is that in the previous case  we looked at them in increasing order of ending times and this case  we look at them with increasing order sorting times  you could look at them with decreasing order of ending times and that will work i will leave it you  again is an exercise so  i can look at these intervals in decreasing order of ending times and then  do this partitioning and that will also work same proof  you just the whole thing is similar i guess you can see  because i can either look at the intervals from left to right or right to left  it really does not matter so  things that work in one problem  you know the ordering  which works in one problem  may not work in some other problem and this ordering of the input  the order in which you consider the input is a crucial strategy in greedy algorithms so  remember  when i started  i said there are three set of paradigms  there are three techniques  that is run through all algorithm design  all these algorithms that we design one is induction that is  we can solve it for a smaller problem  how do you extend the solution to a bigger problem 2nd is ordering the input  look at the input in the right order and the 3rd was storing old values  some of store value  which may be reusing in greedy strategy  this greedy strategy that you looked at  you can see that  there are two things induction and ordering  which has so for lead the crucial role so  let me go back and tell you  bit more about this exchange trick then  we will see a problem  where it couple of problem  where this works beautifully so  the exchange trick usually works  when your algorithm looks like this when your problem looks like this  refer slide time  43  37  so  you have a set of elements as input you want a subset as output and usually  you want this subset to satisfy some property so  this is  how the problem looks like  your input is the set of elements and you want a subset of this so  let us say  that is this  you want subset of this with satisfy some property as output the first two problems were absolutely like this what was your input  initially it was a 3 it is set of vertex and well there are  refer time  44  39    there is an input with some structure  embedded in you want a subset of the vertex set so  that you know they are independent  you want an independent set as output  which is maximum in size so  the first problem had this property this 2nd problem also had this property you want to set of intervals  you want to subset with maximum size so  that the intervals do not overlap both these problems are this property the third one was slightly different  you wanted to partition  means and intervals into many parts so  the first and second fitted into this roughly this frame work so  what is this exchange trick that i am saying  roughly  it is this  refer slide time  45  37  to supposing you have a solution  supposing you have  let us say candidate these things are to help you design the algorithms i am not giving you algorithm as search  but telling you tips as to  how do design this algorithm so  your candidate algorithm solution is a subset now  the exchange trick is this supposing  you remove  let us say an element  x from this solution so  called this t  so this is subset t from t and add  let us say y the question you ask you  does the solution improve  we ask this question so  if we can some more identify  when the solution improves that  you give as a clue as to how to design these algorithm and that happened in the first two cases so  let me look  tell you recall your   refer time  47  05   memory effect so  the first case  we set that  if we did not have a leaf so  i can put this leaf in and i remove something else  solution may improve in the 2nd case  i said that  you could pick  you could put  always put the interval that ended first into the solution and remove something else that is how  we sort of came off to that now  we will see examples  where this was  this exchange trick was  the problem is this so  actually fairly the simple problem and even without this exchange pick should be able to come up with algorithm  refer slide time  47  54  it is again from scheduling so  you have a set of jobs the input is set of jobs with processing times t 1  t 2  up to t n there are n jobs  they have processing times t 1  t 2 up to t n there is a single processor you have one processor  you want to schedule these so  obviously  will schedule them by one by one  there is no preemption in fact  in all over scheduling problems  that you have taken up so for ; there is no preemption so  you can not stop a job in the middle  you lock at the processor the processor runs to completion so  you want to schedule this  one processor  no preemption so  the output in the schedule on one processor  it is no preemption well and we beyond want to minimize something  want to we want to minimize  we want to minimize the some of the finishing times schedule of one processor  no preemption  minimize the sum of the finishing times so  what do i am mean by this  in fact  what do i am mean by schedule the schedule means  as to tell you  which order you put these jobs on to the processor so  supposing i put them in order t 1  t 2  t 3 up to t n  this is the order in which i schedule them on to the processor the finishing time on the first job is t 1 the finishing time of the 2nd job is t 1 plus t 2 the finishing time of the 3rd job is t 1 plus t 2 plus t 3 the ith job is  t 1 plus t 2 up to t i this is the finishing time of the ith job i want to minimize the sum of the finishing times so  each job as a finishing time  i add up all finishing times and i want to minimize the some of the finishing times this is the problem so  if i just look at this sum  what is this ? so just take a minute  refer slide time  51  13  so  the first finishing time f 1 is t 1  f 2 is t 1 plus t 2  so on  f i  the ith finishing time t 1 plus t 2 plus 2 t i now  what is sigma f i  f j  let us say j equal to 1 to n this is what i want to minimize well  it is t 1 plus t 1 plus t 2 and so on up to the last 1  f n is t 1 plus t 2 and so on plus t n it is this some  if you sort of look at this some  it is nothing but n times t 1 plus n minus 1 times t 2 and so on up to plus t n is 1 and when you look at this sum  immediately realize that i better have this modules job here  sitting here  t 1 should be minimum among the t ? s  t 2 should be the max minimum and so on  t n is the maximum time it is a just look at this  you see that is what  that is the way should be and in fact  the proof is also obvious and that is the algorithm so  you just sort of the jobs by increasing times and that is what ; it should be  just by looking at that is a very simple algorithm for this problem we will look at these problem  again just to sort of pushing in a point  when we start next time and then we look at you know slightly more complicated problems from the greedy perspective design and analysis of algorithms prof sunder vishwanathan department of computer science engineering indian institute of technology  bombay lecture  12 greedy algorithms ? iii  refer slide time  01  03  next problem  we want to discuss is called fractional knapsack you want to be correct ; you can call it the fractional knapsack problem well  imagine that you are a bugler and always wanted to start in algorithms lecture this way so  here goes  so imagine you are a bugler  it is say successful one at it you set up to bugler house and you have a bag with you or a knapsack that is why  the word knapsack comes on so  you have a sack with you and you enter the house so  you are a   refer time  01  45    so  you enter the house then  you find that  you are really hit the jackpot in the sense that houses all kinds of stuff in it but  you see your sack has limited capacity you can not just take everything away  which is  where in the house so  the crucial question  there you have to answer at this point is  what you stuff into your knapsack ? what do you stuff into your back ? now  each item as a size and there is some profit associated with it so  if you sell it  there is some cost of profit  that you some money  that you get out of it each item has a profit and size now  the total amount of item  set you can take back with you is  restricted by the size of your sack your objective is to fill this sack with items  such that  the total profit is maximized so  you like to somehow decide  which items to put into this sack so  that the profit is maximized well  this is the knapsack problem without the fractional part well  if you put take fractions of each item  then it is called the fraction in knapsack in the sense that  imagine that  you not bugling an ordinary house  you have bugling cake shops then  there are all kinds of cakes  fancy cakes tune around and you can take fractions of each cake and select you can not eat it so  you can not have the cake and eat it too so  you have to sell this cakes  you can take a fraction of each cake and you like your job is to sort of choose  those which can fit into your bag and make sure not to squish them and you will again to maximize profit so  this is the fractional knapsack problem so  let us get back to our real occasion  which is designing algorithms and let me define this problem formally  refer slide time  04  08  so  there are n items so  each item has a profit and size  associated with it so  this is p i and this is s i let us say  1 less than equal to i  less than equal to n each item has a profit p i and size associated with it you have knapsack of capacity b say  this is input so  this is the input  which is each item has a profit and size and knapsack of capacity b and the output is  for each item  you have to say  what fractions of this item  you going to take so  you are given the each item  there only unit  one unit of an item  which is available to you you have to take a fraction of this item so  each item  you have to say  what fraction so  that the total size is at most b  you can not take  more than what knapsack  the knapsack and contain and you want to maximize the profit so  let us again write this down mathematically let me recall  you are given  refer slide time  05  56  so  you are given p 1 through p n  s 1 through s n  these are profit and sizes and b so  this is the capacity of the knapsack what do you want is  so you are going to find x 1  x 2 up to x n that is  what for action of each item  that you want to pick so  what should be satisfy  well you want maximize the profit so  for one unit of item 1  the profit is p 1  for x 1 units  it is x 1 times p 1 for instance  if x 1 was half  the profit work p could be t 1 by 2 so  you want to maximize sigma x i p i  i going from 1 to n and the total size of these items ; that you pick should be at most b so  that  sigma x i s i is less than equal to b  i equal to 1 to n and let me remind you  that x i are fraction so  0 less than equal to x i  less than equal to 1 for all i so  mathematically  this is the problem you want to solve the input is p 1 through p n  s 1 through s n and b output is x 1  x 2 up to x n you want to maximize this profit function  which is sigma x i p i and the x i should satisfy these two constraints  which is  first one is this  that the total size that at most b and the 2nd is  they must live between 0 and 1 so  this is the fractional knapsack problem this is the problem ; that you like to solve so  how does one  go about doing this  where they still within the real muff greedy algorithms so  i guess  one just has to be greedy and pick as many cakes as one can so  what you like to do is  pick these items 1 by 1 so  that is what a greedy approach to this will be so  the output is  roughly looks like a subset it is a fractional subset not quite subset in the real terms but  still we would like to do this greedily  in the sense that ; we would like to pick elements by 1 by 1  refer slide time  08  48  so  we would like to use the greedy approach  which means pick items 1 by 1 so  the crucial question here is  the crucial decision that you want to make is  how do you order items to pick well  i can come up with all kinds of orderings you can say  so here is 1  for instance  decreasing profit so  you could look at items in decreasing profit  according to decreasing profit and pick them  well increasing sizes and you could sort of put profit and sizes together into various functions and let us say  profit by size or profit by size squared  profit squared by size  etcetera  etcetera and you know  you can ask  which of these work you can try  all of these  do these work well  that is one way of set of approaching this try all these  you know various possibilities that go through your hidden  do not go through your hide and you know  trying prove that  these possibilities work remember that  coming of the possibilities is not too difficult you just listed out many  coming out with the right kind of order is the crucial  is the sort of key here and so  how do you do this one is  when we try the sort of one way is to you know  come up with these things and see  if you can prove one way or the other either come up with examples  so that the strategy works of fails i mean strategy fails or you try and prove somehow  that this strategy works what we will like to do is  to use this in defined exchange trick ; that we mentioned and see  we can use act to design  this algorithm so  that is what we going to do so  we going to take this exchange trick somehow  you know exchange item basically  put one item in and exchange one item for another and see what happens to the profit and that  hopefully will guide us  as to how to come up with this order  ordering on a limits this does not always work  but in cases  where it works  it works so  let us try this so  supposing  let me raise  write this term so  we would like to use the exchange trick here  refer slide time  12  11  so  remember  this is just the way to design the algorithm this is some kind of trick that you can use and i am not guaranteeing that this will work always  it is just hints that i give you so  try this  what does exchange trick looks like  we would like to exchange  some portion of item i for item j see  there are two items i and j i would like to put more of item j and take out some amount of item i then  i will like to see  what happens to the profit of course  i have to make sure that  the bag does not bust so  we can assume that  you know you started out with solution in mind and you know  you can assume that the bag is full if it is not full  i can put  you know  i can start filling in arbitrarily some items in the bag is full so  we take  let us say y 1 of item 1  y 2 of item 2 is so on  y n of item n  such that  the bag is full and from now  with this in mind  now i will exchange one item for other for another so  that the bag still remains full and then  we will see what happens to the profit so  that is the sort of calculation that i would like to do so  will assume y t unit is of item t picked and so  what is the  we will also assume that sigma y t s t  t going from 1 to n  it is exactly b so  the bag is full so  what is the profit  the profit is nothing  but sigma y t p t  t going from 1 to n  this is the profit now  what would i like to do with this is  remove some of item  some amount of item or high an increase some amount of item j  refer slide time  15  06  so  in my new thing  initially i had y 1  y 2  somewhere had y i  somewhere had y j  y n this is what i have now  what i would like to do is  make this  let us say y i minus epsilon  for some small epsilon and this  i would like to increase by some amount let us say y j plus epsilon prime this is what i would like to do  which is i have replaced some portion of the ith element with some of the jth element i would like to maintain the bag to be full  what this transactions to be such that the bag is still full then  i would like to see what happens to the profit so  what it mean that the bag is full  it means  s 1  y 1 plus s 2  y 2 and so on  plus s i and y i minus epsilon plus 1 s j into y j plus epsilon plus s n y n equal to b so  this is sigma s i y i sorry  s t  let us say y t  t going from 1 to n  plus s j times  this is epsilon prime epsilon prime minus s i times epsilon equal to b well  this fellow i know already b  because  initially i started out with the bag being full so  we have that s j epsilon prime equals s i epsilon if i want the bag to be full  then i guide that epsilon prime and epsilon i can choose in the ratio of s j to s i  refer slide time  17  26  so  let us see  what happens to the profit so  what happens to the profit ? so  what is the new profit ? the old profit we know the new profit is  new is p 1  y 1 and so on  plus p i into y i minus epsilon so  on plus p j into y j plus epsilon prime and we go on p n times y n so  this is nothing  but the old profit which is p 1  y 1 and so on  p i y i etcetera  p j y j plus p n y n plus minus p i time epsilon plus p j times epsilon prime so  the new profit is nothing  but the old profit minus this plus  that good so  now the profit  there is a profit increase this is so  this is what we want when  is there profit increase  well this fellow is greater than 0  which means p j times epsilon prime is greater than p i times epsilon  which means p j by p i is greater than epsilon by epsilon prime so  there is a profit increase  this is 2 let me just full back and old slide   refer time  15  06   on this is s j times epsilon prime is s i times epsilon which means epsilon by epsilon prime is s j by s i now  so let us just put this bag into that other inequality we are going to put this equality into other inequality so  this epsilon by epsilon prime is nothing  but s j by s i so  there is a profit increase if p j by p i is greater than s j by s i or p j by s j is greater than pi by s i there is an increase in profit if p j by s j is greater than p i by s i this tells you  what to favor i remember this happen  if i replace part of item i with item j i should favor item j if p j by s j is greater than p i by s i that is what this calculation tells and that is what my algorithms will give  refer slide time  20  58  so  you can see  how just using the exchange trick we have actually device an algorithms and we will  in fact  also proof that this algorithm is optimum so  these score of an item is nothing  but profit divided by size and the algorithm is simple  you say  the algorithm says order items by let us say decreasing scores and when pick these items in this order and pick them in this order  till you fill the knapsack this is the algorithm in one sentence so  how will they output look like  well you pick the first item  the one with top score  you pick the item  you pick the next item and so on and at some point  you know you picked these items and the next item ; there may not be enough space in the knapsack so  you do not pick the entire item  you pick a fraction so  the first few items  the first  if i pick k items  k minus 1 item will be  i will pick the entire item unit and the kth item will be a fraction so  in my solution  there will be one fraction  which is a last item and the rest of them will be units that is what the outputs looks like so  the algorithm  if i look at  let us say x 1  x 2  so on  this is the output and let us say 1 is the item with maximum score so  this is items by decreasing scores this is the maximum score and so on this is the least score then and this is the output is output then  it is of the form  this sequence is and let us say  it is of the form 1  1  1 up to some level and then some let us say delta  which is between 0 and 1  and then 0  0 this is how the output looks likes we need to now proof that  this in the works this algorithm  the output that this algorithm gives you is optimal so  let us prove that this is optimum  refer slide time  23  51  so  we would like to compare this with any optimum and shows that  this is optimum this is the  next way we would like to do it and we would like to use an exchange trick again but  now to prove that it is indeed correct and we will use it the same way  we use it earlier so  please set of review that material  if you forgotten so  the proof goes this way so  supposing  so let z 1  z 2  z n be an optimum output let me recall that  our outputs looks like this    refer time  20  58   it is x 1  x 2  x n it looks like 1  till something these are in decreasing order of scores then  you have a delta and then 0 ? s this is how our outputs looks like good so  let z 1  z 2  z n be an optimum output again  the items are  the order of the item is at the same in both  z 1  the item 1 is an item with the top score this is the way the optimum picks  if it is of course  1  1  1  and then  delta and 0 then  you have done  because this exactly equals z exactly equals x now  let it is possible that z 1  equal to x 1  z 2  equal to x 2 but  if they are different  they will be first index  where they differ there will be a first index  where they differ so  what you are going to do is  we picked more of these item in the first index exit ith index they differ then  here we thick more of i and he picked less of i because  we picked one unit of i and he is picked  let say  less than one unit of i then  we would like to see what happens if we increase  we take the optimum increase the ith and we decrease something some other  where which as a lower score  we can do this and using the previous sort of calculation that we did during the exchange trick  we will see that the profit goes out this is the argument so  let us just formalize so  let j be the first coordinate  where z j is less than x j this is also the first coordinate  where they differ well  i will let you observed or proof that  this first coordinate  where they differ  z j will be less than x j  it will not be greater than x j that is the way  we have picked exist they are once still some stage and then  there is a delta total sum is b if i increase any one of them  it is can not be  the once i can not increase obviously and if all the initial things are 1  i can not increase this delta because  then it will be greater than b so  the first coordinate  where they differ z j will be less than x j so  then there is some i  which is greater than j  where z j is greater than x j  z i is greater than x i  some x i  where this is true because  the total of both of them is b now  what i do is  i increase the profit so  one can increase the profit here by doing this so  take z 1  z 2  z j plus epsilon and so on  z i minus epsilon and so on z n so  look at this epsilon prime  do the same calculation that you did and you can see  that the profit  remember as an end of it  we came up with the new profit  which is greater than the old profit of course  we need to choose epsilon and epsilon prime properly and i like to look at  i like to do this so  go back to this old calculation that we did figure out  what epsilon the epsilon prime should be i again have enough items to fill enough type size of b with the profit as strictly increased  which means at the original z 1  z 2  up to z n was not optimum that is the contradiction so  this is  for a contradiction and calculate the profit for a contradiction i am said towards  that is the end of the proof i let you sort of frame this better and write it properly that is one small thing about this proof which i should say  supposing  so let us say that  these items are same score they all had different scores and then  there is no problem in this proof  it was perfectly if the scores are the same  then you need to worry about it you need to modify the proof slightly basically  i could up chose in you know an item with  among items in the same score  i can sort of chose any one of them write and i may differ  from the optimum in this respect  but it is only in this respect that part of the proof  i will let you sort of film  but module that  this finishes the proof of correctness of the algorithm to fill up knapsack so  again  let me emphasis this  that the right order that we want is to pick to look at the profit by size this is also ; you could up come up with this intuitaly also you know  your intuition could have told you  in fact  even i mentioned initially  that you look at profit by size  as i measure  which we should follow but to sort of the way  we got it was just by trying out this exchange idea and just exchange two things and we saw what happened and profit by size  came out naturally from this from this exercise so  that is just see  that is the model that i want to commit you that often  when you are face the problem this kind just do the exchange trick run over this exchange tricks ; see  what is the quantity that comes out and that quantity  hopefully  will let you dictate  what your algorithm should be the next problem that we look at comes from the area of information transmission and it is related to codes so  we could like to design codes  which have some properties that is the next problem  refer slide time  32  50  so  the problem is like this so  you have symbols x 1  x 2 and so on up to x n so  you have n symbols  each of them has of frequency f 1  f 2  f n so  these are symbols that as sent on a channel from let us say place a to place b these are symbols  which are same from one place to another and this is the frequency at which they are send and what we would like  so this is the input so  what we would like is codes c 1  c 2 we would like to code them it is using c 1  c 2 up to c n these are  let us say  binary codes each of these is a binary string and we would like this binary strings associated with each of these symbols so  for instance  if i send c 1  c 1  c 2 ; it means  you have received  you send symbols x 1  x 2 ; the word x 1  x 1  x 2 so  it sends these symbols  i just send this codes  which a sort of decoded at the other end and what you send is not symbol at a time  but huge word at a time so  you have this huge sort of string  which keeps sending from the left another right  you sort of full of those strings and you decode it so  this code is the output so  if it not for the frequency  there is nothing much to this i have n symbols  i just write down ; you know and sort strings as scored then  i send them out the frequencies that make things are being difficult so  let us take an example so  for instance  i have symbols  whereas  in example  i have let us say a  b and c ; free symbols for frequencies are  100  2  which means  it is send 100 times 2nd ; b send 2 times the 2nd ; c 2 times the 2nd these are the frequencies that which average frequencies that which a send through now  we would like code  these are the frequencies we would like a code  well here is a good code  a is 0  this is 1 0 and this is 1 1 so  let us look at this code for a  b and c well  i use stands to resend that i would like to use small number of bits for a because  it sends many times so  i would like to send this smaller number of bits across so  this has 1 bit  then i have 2 bits for b and 2 bits for c so  then  it is sort of  i use lesser number of bits  which i transmitted across from from left to right now  this code has another property so  if i look at the strings of 0 ? s and 1 ? s going across  decoding is easy the reason is this as soon  supposing the first symbol is 0  i note it is an a for sure if i hit a 1  if the first symbol is 1  i just look at the next symbol  it is either 0 or 1,depending on whether  it is 0 or 1  i know it is b or c  so  let us do one example here  refer slide time  36  45  so  i have a  b and c  0  1 0 and 1 1 let us take a binary string  let us say 11001000101 so  how do i decoded so  i am getting the symbols from left to right i get 1  i check  i know it has to be either an a or b or c so  this gives me c i get this 0  this has to be an a this has to be an a  1 0 means b  0 is an a  0 is an a  1 0 is a b 1  well there has to be something else  here let us say 11 and that give you c i can just look at this symbol as they arrive the bits as they arrive and decode decoding is very easy this code  which have this actually has a property  which i will tell you  which is that none of these  is a prefix of another  0 is not a prefix of any of these and these two are suddenly not prefix of each other so  such codes are called prefix pre codes so  if i have code c 1  c 2  up to c n  c i is not a prefix of c j for any i j  then it is called a prefix pre code and these  you can see are easy to decode i just keep looking at the symbols and as soon as i get a code word  i am done i know  what about is so  i write the word  i remove these and i keep scanning so  we would like to construct prefix pre codes so  that is an objective  given these symbols  i would like to construct prefix pre codes and given the frequencies but  each frequency  there will be a  with each alphabet  there is a frequency i would like prefix pre codes  which minimize the average in your length of the message with send  which means  the length of a message  i will think of  the length frequency times  the length of the code word it has the code has some length l so  the frequency times length is a length is a average length and the some over all elements i would like to minimize so  before i state this formally  let us observe that  prefix pre codes have a close connection to binary trees so  for instance  this code a  b  c  i can represented this way  0 1  0 1  a  b  c so  basically i want to show that  given any prefix pre codes  code  there is a binary tree and given any binary tree like this  i can get a prefix pre codes  refer slide time  40  05  so  what is the secret of this transformation ? well  take any binary tree ; the crucial things are code words at leaves  the leaves  where you look at the code word left branch is 0 and the right branch is 1 so  when i take the left branch  i think of it has 0  right branch i think of it has 1 and when you reach a leaf  you know  you have traverse a binary string now  that will be the code word  associated with that with that symbol so  for by go left  left  right  left and there is a there is a code word there  will be 0 0  1 0 so  given a binary tree  with these code word  at leaves  with these words at leaves then  i can assign a code word to each of these symbols so  maybe  i should say symbols  given a binary tree with symbols at leaves  i can assign a code word to each symbol  using this 0 and 1 business you can check that  that this is prefix free so  why this so  because take any node in the tree so  this has some binary string  associated with it  which is you followed the path from the root to leaf and if you go right  you have a 1  if you go left  you have a 0 so  you get  there is a binary string associate now  what are the prefix of this binary string ? the prefix are exactly those string  which you get at nodes  which are in the path from the root to this node if i only take leaf of a binary tree  no tree  no leaf is a prefix of another leaf  no leaf sets on a path from another leaf to a root let us say just not on so  the codes that you get this way of prefix free and given any prefix pre code  you can construct the binary tree just the simple fashion you just go 01  01 at each stage and you follow your follow this 0 and pattern and when you hit a pattern  which you want  just remove everything below it and that will be your leaf i am the word associated there with that will be the code word will the symbol  which is associated with that code word so  these are prefix pre codes  what we want is  you want to construct prefix pre code before  this let me just take a new set of paper and write this entire problem properly  refer slide time  43  34  so  the input is frequencies f 1 and so on up to f n what is the output that we desire ? we desire prefix free codes or in other word a binary tree and we would like to minimize the following function sigma h i f i  i equal to 1 to n the frequency is come from there well  things with large frequencies must be very  must have sort codes  which means they should be close to the root from the tree so  h i in fact  is the height of symbol i in the binary tree so  the input of frequencies f 1 through f 1  f n  i would like to output a binary tree and with each leaf  i associate one of these works either1  2  3 up to n one of these symbols and i would like to minimize h i f i with the some h i f i  where h i is the height of the symbol i in the binary tree f i is the frequency of symbol i so  this is what i want now  how do i go about doing this ? so  given a binary tree  supposing that  i give you a binary tree with n leafs now  can you assign these words to this leaves well  or to minimize this function  when the answer is  it is easy so  given a given a tree  since i want to minimize the h i f i  i put items with the largest frequency as close to the root so  basically  i take these leafs  i sort them in decreasing order of height saying increasing order of height  i take frequency  i sort them into decreasing order of frequencies and i just tied them up so  if the tree is given  then this is easy the heaviest item of the most frequency must set close to the root and those with whose frequencies of the least will sit for just away from the root in this tree the question is  what the tree should look like ; there are many binary trees with n leaf it is a good exercise to see  how many if there are large number of binaries trees with n leafs  which of these threes we pick once  you pick a tree we are done  we know  how to sort of fit these symbols into the leafs so  how do we pick these trees ? well  do you like to do something some kind of exchange trick here even here  what could the exchange pick really mean  you know  you want to come up with the binary tree and you have these items  which set of these symbols  which are sitting what does mean to pick ? what is the exchange trick mean ? so  what we would like to do is  construct the correct tree that is  what we want to true  you want to do and so  what does it mean to do an exchange trick so  supposing you have some tree  you know that the bottom of the tree  which means node with the longest height you have the two items with the largest frequency  will sit there so  this much  we know  refer slide time  48  07  so  we know  we really know some part in the tree  which is  this is a binary tree now  the bottom most points  somewhere here it is a bottom most points i have  once with largest frequencies the two items with the largest frequencies will sit at the bottom on the tree the other leafs are somewhere there this much  i know so  i know part of the tree what about the rest of the tree can i sort of somehow compute the tree part by part  well  so that is one thing what could an exchange trick look like here ? what do i exchange items but  if i exchange items  remember the tree remains the same so  the trick  the exchange trick that you would like to try is exchange sub trees so  given a tree  i pick two sub trees and exchange them and see what happens so  this is the calculation we would like to do and once we do this calculation  we would like to see  what really happens to the average height and from this hopefully  will get a clue as to how to construct in the tree design and analysis of algorithms prof sunder vishwanathan department of computer science engineering indian institute of technology  bombay lecture  13 greedy algorithms ? iv we will look at this problem from information transmission let me recall the problem  we had n symbols x 1 through x n and each at a frequency associated with it  which was the frequency of occurrence we wanted to construct the code for this set of symbols so  for each symbol we read the binary string now  if the codeword such that   refer time  01  26    length of these string  whether same for all of them then  there is really no problem there is we do not have a problem to solve  but if you allow the coding to be such that  these codeword 's can have variable length then  we do have a problem to solve essentially  symbols which appear more frequently  we would like to give a shorter codeword i just one way to see this is for instance  if you had a large file and you had n words  which these word are repeated the all have frequency associated with them  which is nothing but the number of times that the codeword appears in a file  that file and we would like to encode this file as so  that in the codeword 's are prefix free  which means no codeword is a prefix of another this is how we want to encode this file and send it across then  the size of the file once encoded the size of the file is nothing but the frequency times for it is some over every word the frequency of occurrence of the word  times the size of the codeword and this quantity  which is the size of the file we would like to minimize so  this is exactly what we had so  let me put that put it file across  refer slide time  03  08  so  the input  the input is a set of frequencies f 1 through f n and we would like a prefix free code  this is from yesterday well  i just get to this binary tree in a minute essentially  we want to prefix free code and you want to minimize the following h i times f i  h i here we say it is the height of the symbol i in the binary tree but  this is the same as a length of the codeword h i is the same is also length of the code word for symbol i so  then this tree is nothing but a size of the file  f i is the number of times the word occurs  this is the length of the curve codeword so  this is the this quantity is nothing but the number of bits used to encode the file and you would like to minimize this so  we also saw that prefix free code of prefix free code corresponds to a binary tree there is a one to one correspondence so  given a binary tree and symbols at each leafs  you can construct the prefix free code now  this gives you a prefix free code for the symbols essentially  when you traverse right  you think of it as one  if you traverse left on the tree you think of it as a zero you traverse a path from root to this leaf and that gives you a codeword each time you going to right you right a one  each time you going to left to right a zero and when you end up with leaf this code word associated with it similarly  given prefix free code for n symbols  you can construct the binary tree with n leafs for which this is n or more leafs  which as which the same property so  essentially if your code codeword is say 1 1 1 1 0 then  you get to this leaf by traversing right  right  right and then left that gives you the position  similarly for any binary string  you sort of get two a leaf so  you traverse right if it is a one traverse left if it is a zero and then  gives you part of the binary tree  you can fill up the rest of the binary tree if you want since  it is prefix free each time you will end up with leaf so  all these symbols will sit on leafs so  you can check this using small example in fact  i encouraged do it  that will help you understand this better so  this is what we want to do so  we want to output prefix free code or a binary tree  which corresponds to this which minimize this something like this we saw that once this structure of the binary trees fixed then  we know how to associate these words to the binary tree  with two with the smallest frequencies  will see that leaves that two with smallest frequencies will sit at leafs  which are at the bottom  refer slide time  06  43  so  here is my tree  so it is a binary tree and let us say somewhere here at see it is important  it need not be sort of balance you can even be skew like this let us say these two are the bottom most leafs  then i know the these will have the least frequencies these two the frequencies will be the smallest because  take supposing it was not the case and you know somewhere here  you had you had a leaf now  the quantity of minimizing is sigma h i f i now  if this   refer time  07  37   like closure in the h of this was smaller  h of this was larger so  let us say this is the h 1 and this is the h 2  h 1 smaller than h 2 h 1 is smaller than h 2  this is lower down and that is higher up now  you like to put smaller frequency here so  that if i had f 1  f 2 supposing  let us say this way f 1 was smaller then  i could like f 1 this to sit there and this to sit there because  then it will be f 2 h 1 plus f 1 h 2 which is smaller then  f 1 h 1 plus f 2 h 2  you can check that this is smaller and that so  this is what we want  which means things which are smaller in frequency  must be lower down in the tree things  which are larger in frequency should be higher of in that tree  which stands to reason because  the larger the frequency  this smaller should be the codeword  which means should be higher up in the tree so  this much we know so  given the shape of the tree  we can certainly fill it up with these frequencies  that is easily done but  what is the shape of the tree  that is the question that we would like to answer yes so  that is what we would like to answer and we would like to use this exchange trick so  what you like to do this ? supposing you are given some tree and you filled it up somehow  refer slide time  09  29  now  suppose there are these two parts from the root this is let us say node i  next node j there is a tree sitting here call it t i and there is a tree sitting here call it t j  here is the root now  so what you like to do is exchange these two sub trees so  remember the or exchange trick was the somehow exchange some part of the output supposing  we had some constructed solution  we would like to change twist the solutions slightly and see what happens and earlier it was exchanging part to the solution with something  which is outside well  here since we are constructing trees  what you like to see is suppose your exchange to sub trees and what is the result of this action ? what happens with this action ? what happens to the you know the function we are trying to minimize  which is product to the height times the frequency so  we would like to exchange these two sub trees and see what really happens so  let us do this calculation so  let us say this is at height  so this let us say is height h  let us say this portion is h prime  this is all the way up to the root so  the root two node i is height h so  h is length of path from root to i h prime is from root 2 j  that is the length of the path from root 2 j at h prime and we would like to now compute this function so  see when you exchange these two sub trees  the rest of the elements remain that they are so  there contribution to this cost remains the same  we are not changing that at all so  the anything we need to worry about is the contribution change in the cost  because of the exchange so  let us calculate the old cost  because of these two sub trees the new cost because of these two sub trees and we will see what the differences so  here the old cost so  let me do it for t i so for every element e in t i so  i have the frequency times the length of the part from root 2 that element  which is h length root to i plus the length of the path inside this sub tree let me call this l e l e is the length of the path from i to the element e in t i so  this is the cost old cost for t i and the old cost for t j is sigma let us say f c h prime plus l prime c and c is in t j the similar cost for t j h prime is this l prime c is the length of the path from j to the element c f c is a frequency this is the old cost of elements in t i and elements in t j similarly  we can write the new cost  let me actually i guess i will have to write this again  refer slide time  13  50  so  let me try and write all this in one so  here the old cost sigma e belong to t i f e times h plus l e plus sigma c belongs to t j f c times h prime plus l prime this is old  a new cost  well here and just exchanging let us go back here   refer time  14  10   i am exchanging t i and t j so  this h will remain the same  but here i will have t j instead of t i sigma e belongs to t i now  t i shifted from h to h prime so  it will be f e times h prime plus l e plus sigma c belongs to t j f c remains the same h plus l prime so  this cost this length inside the tree  inside the sub tree remains fixed but  earlier it was attach to i that is why i add h prime here now  it is attach to j  so earlier it was attach to j  that is why it is h prime  now it is attach to i  so it is h so  this is new cost  so what is difference in cost so  i want to compute old minus new so  when i do old minus new  this f e l e cancels in both similarly  f c l primes c cancels  so with f c when i subtract h minus h prime is what remains so  this is nothing but h minus h prime time 's sigma f e plus h prime minus h times sigma f c so  this is nothing but and this rewriting this h prime minus h times sigma f c minus sigma f e this is c belong into t j and this is e belong into t i so  let us go back from slide h prime i will assume is greater than h   refer time  16  19   i have done a shift of this if h prime is greater than h what is the result ? so  h prime is greater than h  then this quantity is positive so  if this quantity is negative  then we have achieve something which is good  the cost as decreased  i mean in the cost as increased so  this is old minus new if old minus new is greater than 0 at means the old cost is greater than the new cost  we have decreased the cost so  which is what we want  so when is this  this is greater than 0 so  this is greater than 0  then we are in good shape this fellow is greater than 0  then we have achieve something so  which means  so let me just write this term  so just conclude  refer slide time  17  33  so  here is i t i and somewhere here is j and t j if sigma f c c belongs to t j is greater and sigma f e e belong to t i c belongs to t j e belongs to t i this is true is then  exchanging leads to smaller cost so  we have two sub trees  one is this t i that is t i and that is fellow t j if the some of the frequencies of the elements in t j is greater than the sum of the frequencies of elements in t i  then if i exchange i get a smaller cost remember  it is just this when i look at a sub tree i need to look at only if some of the frequencies inside  that is what these two things it only the some of the frequencies nothing to do the length inside the tree  etcetera i do not care what this tree looks like as long as the some of the frequencies of elements inside this sub tree is greater than this i should move it up that is what the exchange trick tells us and our algorithm in fact  we will use this fact so  when i look at a sub tree the quantity that i am going to keep looking at is the some of the frequencies of elements in this sub tree not the average length or anything of that kind so  this will be the mantra that we will follow during the course of this algorithm so  now what so we would like to follow greedy approach  which is we would like to build this sub trees one by one slowly initially each element will be a sub tree of it is own so  i am going to build this sub trees bottom up  slow increase the size of the sub trees that i build slowly initially  each element will be a sub tree of it is own this is the bottom thing and i know the first step  refer slide time  20  25  so  if i have let us say i have these elements  let us say this as frequency f 1  f 2  f n minus one and f n suppose these are the frequencies and let us say  they are in decreasing order so  f 1 is greater than equal to f 2 and so on two of the smallest once are f n minus 1 and f n then  i know part of the sub tree  i know that my next step i can have f 1  f 2 and so on i can join the last  this will be a sub tree i know so  this is f 1 minus 1 and this is f n the question is what next ? now  the crucial trick is rather than treat them as leafs  i treat all of them as sub trees this is one sub tree  this is another sub tree well a leaf is a sub tree this is sub tree  that is a sub tree  this is sub tree now  which are the two sub trees that i want to be at bottom remember  the previous exchange thing as the bottom i want the sub tree with whose some of the frequencies of the nodes inside it must be the smallest so  i now look at these frequencies i have f 1 f 2 and so on so  this is f n minus 2 now  this i treat us one sub tree and it is f n minus 1 plus f n so  this is my new f prime let say n minus 1 these frequencies remain the same  now this i look upon as a sub tree with this frequency among these i chose two of this smallest and i put a new node and join them together and this is my algorithm so  i just keep doing this as i go up so  my generic step of the algorithm is this  i have sub trees so  initially treat each element is treated as sub tree and as i go long i will have may sub trees  refer slide time  22  59  so  the intermediate stage is this i have sub trees  let us say t 1 t 2 so on up to t k now  i associate weight to each of them  which is just the some of the frequencies of the elements in side each so  let us say f 1 prime f 2 prime so on f k prime so  f i prime is the sum of frequencies of elements in t i or symbols or words so  just sum of all the frequencies and that is my f i prime now  i pick two of minimum f prime and join them together and make a new sub tree for instance  if f k prime and f k minus 1 prime where the smallest once then  i would take t k minus 1 and t k and join them together the others remain as they are t k minus 2 and so on this is if t k and f k prime was the smallest  let us say and f k minus 1 prime was the next smallest when  in that iteration i take this as input and i create this so  number of tree is as decreased by one i have just merge these two sub trees together the new frequency of this will be just the sum of the frequencies of these two things this will remain f 1 prime and so on because  here you have just the elements of these two just get union so  this is the generic step and i put this into a loop and this is my algorithm  you can see that the algorithm terminates ease to see because  initially start with each leaf  each element as a sub tree so  these will be the leafs and there are n of them  each time the number of sub trees decreases by one so  finally  i will just have one sub tree  this will be my binary tree  this will be a the binary tree that i want and this will give me the prefix free code that i am looking for so  why does this algorithm work so  let us write a proof of correctness and again it will just invoke the same exchange principle  that done to write a proof so  here is a proof  refer slide time  26  16  so  what we would like to argue is that  at each stage see at each stage you have   refer time  26  26   these sub trees t 1  t 2 up to t k will the proof statement  that we would like is that at each stage  there is an optimum tree which has the sub trees  that the algorithm constructs as sub trees so  what does it mean so  if the algorithm for instance here if the algorithm had sub trees   refer time  27  19   t 1 t 2 up to t k  there must be an optimum tree which has these as sub trees you do not know how these are connected  but the optimum must have these as sub trees  there must be some optimum tree with these a sub trees now  this statement is if you prove this statement  then we are done because  once algorithm terminates we just have one tree and we just said that an optimum must have this as a sub tree  which means optimum must be this tree so  if this statement is true for all stages of the algorithm  it should be true when the algorithm terminates in which case we have what we want  so why is this true so  again the proof you can write this proof by induction on the stages so  we would like to prove this maybe i should shift this here so  proof by induction on stages  the first stage at the beginning so  the base case well it is true because  what you have as sub trees i just the leaves and these should be leaves even in an optimum tree the base case you have n elements which are not connected to anything and this is true even in the optimum case so  the base case is so  what is what about the inductive step  well what happened in the inductive step so  here inductive step  refer slide time  29  13  well  you started out with trees let us say t 1 up to t k and you ended up with t 1 up to t k minus 2 two of them let us say the last two you join  this is t k minus 1 and t k this is what the algorithm   refer time  29  37    you know by induction  that there is an optimum which at these as sub trees there is an optimum tree  which at these as sub tree so  let us look at this optimum so  by the inductive hypothesis  there is an optimum tree with t 1 up to t k as sub trees now  we would like to argue that there is an optimum  which as all these as sub trees  voice are true well the reason is this let us look at this optimum tree  there is an optimum tree let us say call it t let us look at t now  among these ? so  how this look like so  it is something like this  then there is a sub tree maybe goes as a sub tree somewhere around this is a sub tree  then the branches of line this maybe there is a sub tree here and so on so  there are these sub trees t 1 t 2 up to t k sitting here so  look at the lowest sub trees i clean that i can put t k here  the lowest sub tree must contain t k now  why is that is so supposing it is not supposing something else it is here what i do is some t i ? what i do is exchange  supposing let us say t k for here and you know t i for here then  i exchange t i and t k i do this sub tree exchange and by the previous argument that we did  we saw that the cost decreased when we did the exchange trick and we did the calculation  we saw that the cost actually decreased so  this should not be possible  in which means in the optimum i should always have t k sitting at the bottom similarly  t k minus 1 must also these sitting at the bottom  which means the height must be again maximum so  what i can do is take the bottom to nodes  take the bottom to nodes and i can always exchange whatever sits here  i can exchange it with this should be t k and this as t k minus 1 remember  that these frequencies are the smallest t k was the smallest and this was the second smallest so  i can always exchange any of those trees with these two  these are the bottom most and i will always get cost which is less so  i can assume that there is an optimum solution with t k and t k minus 1 and the bottom well  now i am done because this structure i just pull out from here this is t k that t k minus 1 and here is the other nodes so  this structure i just pull out from here good so  there is an optimum tree which as t 1 t k minus 1 and this as sub trees and we are we are actually done so  finish the proof that this algorithm is optimum so  each time you look at you merge to trees  which i have the least weight  the minimum weights of tree and the second minimum and you create the new tree by merging these two into a binary tree so  that is the algorithm and networks  this was done by huffman and it is called this coding is called huffman coding so  let us summarize our discussion of greedy algorithms  refer slide time  33  42  well  the main thing was we build the solution output  solution piece by piece each time we somehow get the right piece two imagine it is a kickshaw and you know pulling out pieces to fit and each time put the right piece and place  that is the crucial thing to help us we used what is called exchange trick essentially start with any solution and change solution to obtain better one this is just something that you do to figure out  what your algorithm should be doing this is not what exactly the algorithm does  this is just you help you figure out what the algorithm does  which is you start with any solution now  we start of   refer time  34  50   the solution a bit and see what happens and typically it is exchanging part of the solution for something else you takes something out of the solution puts something in  you do a better exchange then  you see what happens to the profit or the objective function if it is increases  if it is gets better than some tells you gives your   refer time  35  14   how to proceed so  this is about greedy algorithms i would recommend  that you study metroids  theory of methorids and linear programming  especially the so called primary dual method  which is available in most text books and linear programming and these two may give you a better sort of feeling for greedy algorithms  in how you know the strategies work good luck design and analysis of algorithms prof sunder vishwanathan department of computer science engineering indian institute of technology  bombay lecture  14 pattern matching ? i we next look at the problem called pattern matching this is something i am sure you used before  refer slide time  01  02  so  the problem is this you given as input  text and pattern and you want to find  if you going to determine if this pattern occurs in the text and where output is find an occurrence of the pattern in the text you could changes to say find all occurrences  it really does not matter for so  the time being we concentrate on this  we want to find and occurrence of this pattern in the given text grep is something that does it so  i guess you are used uniques  we use uniques then  the command that does is it grep  you can grep for a string in a file so  intake of file and the string as input and look for this string in this file and i am sure every other operating system provides this facility  it is a very basic sort of facility  which is provided and which is used quite extensively so  what we going to do is design and algorithm for this problem  which is given a text and pattern  find an occurrence of this pattern in the given text so  what is the first thing  first algorithm that you would write well  take the pattern and you sort of look for the pattern at every possible position in the text so  the text is say n characters long  you started the ith character in check whether  the pattern there is an occurrence of pattern starting of the ith character do this for every i so  for every possible position  you check whether the pattern does in fact  occur that position you could do this in a loop so  you can sort of scan the text  character by character starting at you know from left to right say and for each character from this character onwards is there is a pattern exist so  you check for this  refer slide time  03  53  so  symbolically let us say this is the text  well that is the pattern i start matching character by character here if there is a complete match  then i know that the pattern occurs here if it does not  let us say there is a mismatch somewhere here  then i shift the pattern by one  one character to the right and now i start comparing from this second character onwards i check whether the second character  that text equals the first character the pattern etcetera so  i check whether the patters occurs here  if it does output if it is does not i again shift this pattern by one more and so on so  each time i sort of shift the pattern  along the text and well when the pattern  when the sort of pattern is at some place let us say here so  this is the pattern in that is the text  i just check whether the match character to character i just check that these two character as same if all of them are the same  then i actually found the pattern here of the text and i can output this index this is where the pattern occurs so  this is sort of a   refer time  05  19   and the sort of the first thing that you would do  first algorithm that you would come up with to search for a pattern in text how much time i guess is the question  that we asking throughout this course and we will ask again so  how much time does it takes  first you this can be sort of return in a i mean i hope all of you can write code for this   refer time  0547   algorithm is putted in the loop and you sort of you know move the pattern across the text so  this is should be able to do and if you doubtful i would suggest you  you just try this before we proceed further then  we come back to this question how much time does it take well  you compare let us say that of the size of the text  refer slide time  06  19  let us say the text as n characters and the pattern as m characters  size of the pattern is m  size of the text is n then  for each starting point in the text we search for the pattern so  in the loop there are i have to check for starting with n characters and each time and the worst case i may make m comparisons so  the total sort of time is o m n  this is the total time this should be clear  because for each of these n characters   refer time  07  11    so  i when i start comparing from here i could go you know most of the pattern  i could sort of go down most of the way and then pattern discover a mismatch in which case  you know the total time taken is o m for each of these indexes and there are o n indexes so  well the total time is order m n or objective is to can we do faster so  this is the question that we would like to address and in fact  we will do faster so  by the time this lecture ends you see that we can do much faster than o m n  that is you have written to see how we can how do we go about doing this  the first thing do is to check out the few examples  which is what we will do so  we will take some patterns  some simple pattern we will see how would once search for this pattern in a given text  refer slide time  08  26  so  let us say so here sample 1 so  supposing my pattern is a b to the k  this means it is a b b b there are k occurrences of this you can take k to be 20 for instance  real exact value of k does not matter but  the pattern looks like this and your searching for this pattern in a text so  let us just see what happens here so  let us say this is the text and you started searching from this point onwards so  this is the pattern you sorted searching from here this is a let us say so  there is a match  then you sort of check maybe  this is also a match you go down you know of few characters their own matches and then there is a mismatch so  here i have a mismatch  before the pattern ends  which is the pattern ends here so  somewhere here is a mismatch  which means this is not a b i know that this is not a b now  what can we do well the   refer time  09  55   algorithm you know what   refer time  09  57   algorithm have done  it will have shifted this pattern by one character and then  started comparing  this a would have been compared again this b you would have immediately got a mismatch  then again go down one more you would have a again have a mismatch because  well all of these are b ? s up to here  because the pattern is match so  for all of these are b 's in fact so all these when i start shifting  all the initial shifts are useless in a sense because  when i shift by let us say this amount up to this i know this a is go to have a mismatch with the b so  i am going to start shifting again i am not going to find the pattern so  the pattern this pattern is suddenly not going to start anywhere here if at all it is going to start here so  if there is a mismatch i can for this pattern  i can move this all the way up to here so  i can start comparing here now so  i move the pattern all the way here and i start comparing i do not need to sort of shifted one by one i can shifted all the way here and i start comparing so  there was a mismatch at this position i move the pattern all the way and now i start comparing so  for this pattern this algorithm works so  i start comparing  you know the text in the pattern  here the text in pattern and if there is a mismatch in the ith position of the pattern  i just move the pattern all the way up to here and i start comparing now with the first character of the pattern so  essentially we have some more use the factor  you know the text as match the pattern up to here so  we know that these are all b ? s and my pattern must start with an a since  there is a mismatch here  there is a good chance i mean there is some chance that there is an a here so  we can not sort of move it even further  this is what we move the text of tool so  supposing we follow this algorithm for this pattern  if there is a mismatch i am move it all the way here and know again start comparing  i compare this with a i go down here suppose there is a mismatch i move the pattern all the way here so  how much time does existing well this is the crucial argument  because some such argument will use in general when  the time the total number of comparisons by time will just say the total number of comparisons is actually 2 n at most 2 n comparisons now why is that well we look at each position of the text i claimed  that for each positional the text i will make at most two comparisons why is that well  let us see we start here for these positions of the text i make only one comparison because  once there is a match here i never make anymore comparisons for this position in the text if there is a mismatch i may make one more comparison  which is the pattern shifted all the way now  i match a with this  if this does not match i shift the pattern and i move ahead so  for each position in the text i make at most two comparisons so  the time here the total number of comparisons that i make for this  that i need to make for this pattern this 2 n so  well there is a   refer time  13  49   that maybe we can do faster than m times n so  let us takes another example again any simple example  but this will again illustrate what we are trying to do  refer slide time  14  05  so  example 2  so earlier we took a b to the k that is now take a to the k times a to the k b which is nothing but a  a  a k times followed by b this is the pattern we are looking for in a text so  how much time does it take  how many comparisons do we have to make  if we sort of do things not just namely  but in an intelligent way so  again let us see what happens so  here is my text i start comparing  let us say from here i have done all the way up to here and i am sort of comparing from here and let us say that is the pattern and i have a mismatch here so  i have a  a  a here all the way up to b this is the pattern in the text i know that i have a is here here  there is a mismatch  this could be anything this is the mismatch well what can i do so  in the   refer time  15  28   sort of thing i would have shifted this by one  this would have shifted by one and i will was started comparing again and you know that again there will be a mismatch here there will be matches all over here and there will be a mismatch here this is known  because i know what the patterns so  if you want to do it intelligently  you know that starting the pattern anywhere here all the way up to here is useless because  i need you know string of size a is of size there should be k a ? s and they do not occur here this is not an a  so i can in fact  start the pattern right here so  once as a mismatch i can start comparing right here so  i can move the pattern all the way after the mismatch once  as a mismatch i just move the pattern up to here so  i know because the reason is i can not find pattern  anywhere here i have to start here because  pattern has to start with an a and then i need to see k a ? s and i do not see that here  this is not an a so  again i use some information about the pattern to move the string all the way there is one more case here  which is when the mismatch is at b but  before we do that let us maybe do that and then come back and see how much time takes  refer slide time  16  54  so  here is my text now the mismatch is here so  this is the mismatch  but text matches all the way up to here  except you know there is a mismatch now what do i do so  now there is a good possibility  that this is a there is a good possibility that the place  where there is a mismatch  in the text it actually is a in which case  if this is a b the next thing is a b  then you will miss i mean you can not move it by more than one so  i have to try this the pattern shifted by one this is quite possible a  a b if there is a mismatch at this position  it is quite possible that the pattern occurs here so  i can at most shift the pattern by one so  this looks like you know worst case i can not shift by more  the pattern by more this looks like a worst case and be a back to looking something looking at the   refer time  18  12   algorithm but  while we are shifting the pattern by one  there is something you are gaining what we are gaining is that  we know that there are up to this position there is a match while i shift the pattern by one i do not have to again match all of these as i did in the   refer time  18  31   case remember  then   refer time  18  35   algorithm i shifted by one  then i started my comparisons here do this match  do this match  do this match and so on here  i do not have to do that  i can start comparing from here i shift the pattern by one and then i start comparisons here so  this is an a if there is a match  then i move up again  this fellow moves up a again this moves now  i see whether this is b  if there is a mismatch i am moving the pattern by one  but the text pointer remains where it was so  the text pointer remains where it was so  how many  so i hope if the algorithms in this case is clear it is slightly more complicated than a b to the k but  i hope the algorithm is clear so  how much time does it take so  what is the time  which means how many comparisons do we make in the case of a to the   refer time  19  35    so  let me again let us over this   refer time  19  37    suppose  this a mismatch in one of these positions i move the pattern all the way here i the move the pattern here  if there is a mismatch at b well i am move the pattern by one by one unit but  i start comparisons here  this takes with this pattern now  of course there could be a mismatch  if there is an mismatch  then i move the pattern all the way up if there is a match  then i move this point by one  this point by one and i compare that is the   refer time  20  10   algorithm works how much time does it takes  well the claim is again for look at each position in the text by look at this position in the text  then the number of times is compared is at most 2 if there is a match  then the text pointer moves forward if there is a match text pointer moves forward and that position is never compared again so  it at most once a match occurred at a position with text at most once how many times can a mismatch occur  will again i claimed that the mismatch will occur at most once well  if the mismatch occurs at a position  which is not b at one of the a ? s  then the text moves forward so  this is the case  if the mismatch occurs at one of the a ? s  then the pattern is moved all the way up here and the text pointer  which was here now moves up by 1 if there is a mismatch text pointer moves by 1 i never compare this again  i never compare this character again if there is a mismatch at b  the text is again here  but the pattern has moved now  check what can happen  either there is a match in which case the text moves or there is a mismatch at an a again the text moves so  there can be at most two comparisons per text character so  for each character in the text  there can be at most two comparisons so  the total time is 2 n number of comparisons so  number of comparisons at most 2 n so  in both cases we see  that the number of comparisons is at most 2 n and this gives as scope  that maybe we can do it you know faster for instance  it is   refer time  22  47   asking can we do it o n time for all patterns  refer slide time  23  00  so  what is the scene  suppose it should be able to do this  what is the scene ? well  the question is so  if i have this is the text and i have  now this is the pattern and i match some of them and here i have a mismatch in each of the earlier cases  we some more used this matched information this information  that this portion of the string matches with this portion the text was used in both cases so  this is what we would like to use so  can we use the match information  which means the information that this portion of the string actually match this portion of the text can we use this and this is what we would like to do  somehow use this information what you would also like to do  like in the previous case is try and shift the pattern as much as possible  refer slide time  24  23  so  here is one goal  so shift pattern as much as possible so  what does this mean  i mean so  here is my text we going to see many of these two line highways throughout this  this stock so  here is my pattern in fact  if you see this figure this is a text in   refer time  24  54   pattern i am not going to keep writing this text pattern will miss now  let us say this is a mismatch here now  when can i say that moving it by one is not necessary when can i say that this is useless well  this is useless if this portion will not match this portion of the text if this portion does not match this same portion of the text  there is no point and moving it by one we in some ways no this portion with text  because it has exactly match this portion of the pattern all this i have a match  in the text up to here i have a match this is my first mismatch  which means if this portion of the pattern  does not match with this portion of the pattern forget the text for the time being  just look at the pattern this is the original pattern and here is the pattern shifted by one character if this portion of the pattern  does not match this portion of the pattern  there is no point and shifting it by one  because if there is a mismatch somewhere here  there is going to be a mismatch there also how about two characters  again it is a same thing only now i am concerned about this portion of the text  this portion and you see that portion of the pattern in general  so let me draw this again  well the crucial idea again is that once i have matched up to here  then whether to shifted by one or two or three or four it is only something do with the pattern  it has noting do with the text in some sense  because the text is already matched so  this decision i can make by looking at only the pattern what do i mean by this  refer slide time  27  20  so  here is my pattern so  thing of the text sitting up there  i have a mismatch here and let us say  you know the best way to do it is to shift by you know some i units essentially  i do not by shifting i do not want to miss and occurrence of the pattern  in the text so  that is what i do not want to miss  otherwise i would like to shift as much as possible so  here is the text  supposing this is shifted by i units here  supposing you know if i is the best you can do  what is this mean ? this means  that this portion must match this portion this portion which is a prefix of the pattern  must match the suffix of this portion of the pattern this prefix must match this suffix and this must be the largest prefix i can not have a larger prefix matching this  for instance if instead if i move the pattern by less let us say i minus 1  then i have a and there is a match  then i should not you know miss that opportunity so  what i am saying is this so  here is my text  so here is the pattern  let us say this is shifted by j  which is less than i  this is j which is less than i the mismatch is at the same position  this is the same position is that now  i can safely do away with this j 's  if this is not a prefix this prefix is not a suffix of this so  for all j less than i this prefix is not a suffix of this portion of the pattern so  this is this portion of the pattern is the same as this  these two are the same so  any prefix which as larger than this length  larger than this will not match suffix  then i can move it forward and essentially i want you know the largest prefix of the pattern  which matches the suffix at this point so  if there is a mismatch that is what i want then  i can move it all the way up to that portion and then start comparing so  this is what i really want to do so  this data is only pattern dependent  that is the first thing  refer slide time  31  32  so  this only pattern dependent this is the data  that tells you how much to shift and what is this data so  supposing you have a mismatch at the ith position how much to shift the pattern by that is what we want to know so  how much to shift the pattern by well  refer slide time  31  45  so  let us look so here is the pattern  this is the ith position i have p 1 through p i minus 1 and supposing i have an optimal shift  which looks like this so  p 1 so on and this is p i minus  now let us say t minus 1 so  supposing the optimum shift is by t characters  which means if i shift by less than something less than t character then  this is not going to work i mean in the sense there will be a mismatch so  what is that mean so  if it is less than t to here is my pattern p 1 to p i minus 1 if i shift with by less than t then there will be a mismatch somewhere here  mismatch if shift is less than t so  this means that there is no point and time out the shift  i am it as well shifted all the way up to t and if the optimum shift is t  this means that you know there is i have a match all the up to here  which means p 1 and so on up to p i minus t minus 1 is a suffix of p 1 to pi minus 1 so  this is the word and that this is the word with i minus 1 characters and this word is a suffix of this  that is what this shows that these two of the same  these two of the same and so on  and because it is a mismatch somewhere here we also know that p 1 through p i minus  let us say j is not a suffix of p 1 through p i minus 1 for small for the values is j which are smaller than t minus 1 so  for j less than t minus 1 so  the values of j is less than t minus 1 so  the shift is less than t which means  the shift is less than t then  this is not a suffix  when i shifted by t units  then i have a match all the way  which means this prefix of this pattern is also a suffix of p 1 through p i minus 1 so  given a pattern what we trying to do is for a mismatch we want to find out how much we can shift the pattern by  we would like to shifted by as much as possible  without missing and occurrence of this pattern in the text  that is what we are doing so  supposing the so  let us look at this figure so  supposing the mismatch is at the ith position in the pattern  you the text sort is to here there is mismatch at this position how much should i shift the pattern by and we saw that  i would shifted by t units  this is t units if the following thing holds  which is this prefix p 1 through p i minus t minus 1 i minus t plus 1 this prefix is a suffix of this portion and this is the largest such string if i shifted by less than this  then there will be a position where there is a mismatch  that is what it says for all smallest shift there will be a mismatch  here i get a perfect match  there is a match between these so  just says that this is the longest prefix of the string  which is also a suffix of the string i want actually then to be proper prefix of this  which is the suffix so  then the crucial thing  the thing that we want to find out this is  refer slide time  37  02  so  the first thing is this quantity is a function of the pattern and can be pre-computed so  i can pre-compute this value as to for each mismatch how much to shifted by so  what is this function of the pattern  that i want to compute is for each i  for each position let us say find the largest prefix which is also a suffix so  i have actually not defined prefix and suffix i hope you know what it is ? so  given a string a prefix is all the initial portions  the initial the first i portion is a prefix and suffix is just a last portion  this is suffix and this is the prefix so  given any string this is the prefix and that is the suffix so  given any string i want to find the largest prefix  let us say largest proper prefix which is also a suffix clearly  if i take a string  the string is a prefix of itself and suffix of itself but  i want the largest proper prefix of a string  which is also a suffix so  this is what i want to compute supposing  i have computed this  refer slide time  39  05  so  i have pattern p 1 through p n is the pattern let us say f i is length of the longest prefix of p 1 up to p i minus 2  which is also a suffix of p 1 up to p i minus 1 this p i minus 2 is just to make it proper prefix so  any prefix of this so  length of the longest prefix of this  which is also suffix of this i could have written p i minus 1 but  then i will mention the longest proper prefix so  supposing i have computed this for each i  for each position in the pattern of computer this f i  which is the length of the longest prefix of p 1 through p i minus 2  which is also a suffix of this  which means if i take any other prefix which is longer  than f i then it will not be a suffix of this pattern now  on mismatch at position i what can i say ? i say  that you can shift the pattern by i minus 1 minus f i i can shifted by this much  given f i which is defined this way then  on mismatch at position i  i can shift the pattern by i minus 1 minus f i so  let us see this  refer slide time  41  31  so  here is my text  this is the pattern and this is the ith position in the pattern and there is a mismatch now i know  so let us see what this is   refer time  41  58   this says f i is the length of the longest prefix  which is also suffix so  let me if i put the term here so  this is the string that i am considering now i am only looking at   refer time  42  15   p 1 through p i minus 1 the mismatch is at i but i am looking at p 1 through p i minus 1 good so  this is the string i am looking at and i know that which is the longest prefix of this same string  which is also a suffix this i know as length l i  which means if i move the pattern all the way up to this position then  there is a match here if there is a match here i know that the text as match everywhere here so  there is a match in the text i also know  that this is a longest prefix which means if i shifted by any less  there will be a mismatch at some position  which means you have be a mismatch in the text so  the among time move is this the among the shift the pattern is this  this is i minus 1 so  among the shift the pattern is i minus 1 minus l i which is what we had f i this is f i so  i must shift this by i minus 1 minus f i so  this is what i have said in the previous step so  once i compute these f i 's if there is a mismatch of the ith position of the pattern  if p i mismatches the text then  i just shift the pattern by i minus 1 minus f i i start comparing now so  let us right this code   refer time  44  02   see what this terms out so  initially i am looking for a pattern in a text  refer slide time  44  12  so  the pointer to the pattern i will refer by x  the pointer to text i refer by y  the variable will be y initially my initialization is x is set to 1  y set to 1 and a start comparing now  what is the generic step look like so  if p of x is text at y  this is a text  this is the pattern if there is a match then what you do ? well  i need to increment x and y by 1 and then continue then  x is set to x plus 1  y is set to y plus 1 and i continue  i will have to go back in a loop what happens otherwise  if there is a mismatch  then i know that i am a shift the pattern by this much   refer time  43  35   i shift the pattern by i minus 1 minus f i so  this is where the mismatch is so  this is where my y this pointer on the string is y  the pointer in the text was i here now  what do i set it to  i set it to f i plus 1  if it your i initially i set it to f i plus 1 if it your x initially i set it to f i plus 1 because  i have moved it by i minus 1 minus f i or i minus 1 minus f x remember my point of   refer time  46  21   x here it does not i but  x if there is a mismatch  i shifted by x minus 1 minus f x so  the point next character i am going to check in the pattern is nothing but f x plus 1 so  let us do this so  else x is set to f x plus 1  this is what i want to do there just one case that we need to take care of which is  this actually is true when x is not equal to 1 so  this is when x else  if x note equal to 1 then you do this now  if x equals 1  what is this mean ? let me the first character of the pattern  there is a mismatch with the text and comparing the pattern with the text  if x equals to 1 is the first character so  the first character of the match pattern mismatches with the text then  i just move both pointers i move the pattern by one and i move the text pointer by one so  let me right this all it is inside else so  if x equals 1 then y is y plus 1 so  i have i just increment the text pointer by one so  all this it is inside the else so  this is inside the else and the whole thing is in a loop when do have success  when do i stop well  i stop when x becomes size of p plus 1 so  let me write that to success  if x is size of p plus 1 or m plus 1  which means i have match successfully the m positions of the pattern then  i have found the pattern in the text  this is the success and the whole thing is in a loop so  in a while loop you can check whether x is p plus 1 or not so  this is the procedure this is the algorithm given f x in this function f x for each f i for each position i then  the algorithm for searching  this pattern in the text is this and now let us see how much time this takes ?  refer slide time  49  31  so  what is the time  the time let me quickly write the main loop again  it is says if p x equals t y  then x is made x plus 1  y is made y plus 1 else the crucial step is if x not equal to 1  then x is f x plus 1 so  you shifted the pattern if x equal to 1 on the other hand  then y is y plus 1 in the new loop  this is algorithm and then  you check for success so  how much timed does it take  well it is a number of times these statement are executed so  how many times are these executed ? well  i can count number of comparisons  this will tell me the order of the algorithm now  the number of comparisons  that i make totally over is nothing but number of successful comparisons  which is number of matches plus number of mismatches so  now let us look at each of these in term  this is very similar to the merge sort so  i would  once you look at this analysis go back and check out merge sort so  now what is the number of successful comparisons now  each time there is a successful comparison  why increases by one  check out that y increase by 1 y never goes down the pointer on the texts never goes back always goes forward so  the number of comparisons each time y goes up by one it is start set 1 ends up at n so  this is at most n minus 1  because y this smallest value is 1  the largest value is n and each time there is a successful comparison it increases by 1 how number of mismatches  well in one case x drops by at least 1  in the other case y increases by 1 so  if i look at y minus x  if i look at this function y minus x this always increases  when there is a mismatch if there is a match y minus x remains the same both of them go up by 1 nothing happens to this if there is a mismatch  this increases initially this is 0  the largest this can be is is at most n minus 1 so  number of mismatches is also n minus 1 the other way to see that the number of mismatches at most n minus 1 is that each time there is a mismatch i shift the pattern by at least one remember  there is a mismatch i shift the pattern so  i shift the pattern by at least one  the number of time i can shift the pattern is at most n minus 1 so  the number of mismatches  number of unsuccessful comparisons is at most n minus 1 so  the total number of comparisons is twice and minus 1  which is what we had for those small examples but  this now shows that true for any example all we need to do now which we do next time is to compute this function f x once we compute that we are through design and analysis of algorithms prof sunder vishwanathan department of computer science engineering indian institution of technology  bombay lecture ? 15 pattern matching ? ii we were discussing pattern matching in the last lecture the problem was given a pattern and text you want to find let us say the first occurrence of this pattern in the text and we were on our way to finding linear time algorithm for this problem so  the first thing was that the notice that  what we would like to do on so  we start matching the pattern to the text and once this mismatch we would like to shift the pattern the worst case was algorithm the proof of the algorithm shifted the pattern by 1 and started comparing from the beginning again we would like to do much better than this  refer slide time  1  52  and we will let us quickly go away what we did last time so  you have the text and you have let us say the pattern and let us say you are comparing this position starting from this position now  you compare till you find a mismatch let us say that is mismatch and your matches is all over this place  what you like to use is the fact that the text in this portion exactly equally pattern in this portion this is the information that we would like to use and on a mismatch we would like to push the pattern as much as possible so  we would like to push the pattern the maximum possible without missing and occurrence of the pattern in the text that is the goal and for this  we set that what we want to know is for each position in the pattern so  what is the longest prefix of this  this portion of the pattern which is also suffix of this portion of the pattern so  this is something that we want to compute ; because if there is a mismatch and we shift the pattern across let us  say to this position then we notice that up to here this is the prefix of the pattern which is a suffix of this portion of the pattern there is a mismatch here so this portion  if this has to match prefix of the pattern has to be equal to the suffix of the pattern and since  we do not want to miss out and occurrence of the pattern in the text we find the maximum search  i mean in the maximum search which means a minimum shift which gives you this so  we find the maximum length of a prefix of the pattern  which is also a suffix of this portion of the pattern  the mismatch that occurred here so  this is what we want to find and the algorithm was once we determined this this is a function of the pattern and can be pre-computed so  once this is computed  then the algorithm was clear  i keep matching once i have a mismatch i move the pattern over as much as i can and i start comparing at the same position starting from the same position on the text the pattern is of course shifted  refer slide time  04  53  and we argue that this let us look at this algorithm once again this is from yesterdays notes so  pointer to the text is y the pointer to the pattern is x the generic step is this you check if these are equal if there is a match if there is a match then you go on to the next character you go on to the next character in the pattern and next character in the text that is what this portion does x is x plus 1 y is y plus 1 if there is a mismatch ignore the x equal to 1 part well if x equal to 1  then you are the first position in the pattern which means your pattern shift and the text also and the pointer on the text also shift the pattern shifting does not make any changes  because x remains 1 but  y increases by 1  because now we are going to start comparing the next character in the text now  if x is not equal to 1 that is the crucial stack that we set then  this is where you shift the pattern why remains the same so  you start comparing from the mismatch onwards this is what happens   refer time  1  52    this y remains the same  but now the pattern shifts so  initially x was pointing here now x will point here this is a new x this is the old x this a new x and how do you find this new x  new x is nothing but this length plus 1 and that length we have calculated already and which is f x so  f is an array then you want to write you know f of x if f is stored in an array  then you would write want to find a prefix ? so  this gives you the longest prefix of the pattern up to x minus 1 which is also suffix of the pattern up to x minus 1  only you need the prefix to be a proper prefix so  let us say the prefix of p 1 up to p x minus 2 prefix of this which is the suffix of p 1 up to p x minus 1 so that is what we wanted so  this moves the pattern up that is what this says and you start comparing again this is in a loop the question was what is the number of comparisons we make  because this dictate the order of the algorithm now  this i can write as number of successful comparisons in number of unsuccessful  which mean number of times i take the  if else number of times i take the else close well in the  if close we notice that each time  this is a match y moves up by 1 the pointer in the text moves by 1 increases we also notice that the pointer in the text never moves back it always moves forward so  the total number of comparisons can be at most n minus 1 total number of successful comparison can be at most n minus 1  because why increases each time now  what about unsuccessful comparisons here ? well again let us look at only x not equal to 1  when x is not equal to 1 then  the pattern shift the text pointer remain as it is with the pattern shift to the by at least 1 unit so  the total number of times a pattern  which shift is at most n minus 1 and well that is why this is also bounded by n minus 1 and y minus x plus 1 actually gives you the position of the head of the pattern so  if you look at y minus x it points to the position before the pattern so  x points here y points here  this position is actually y minus x so  this is giving me y minus x so  if i look at y minus x since the pattern shift this will strictly increase in this case in fact  in both this in that you can see that  this strictly increase why increase in the first case the second case i mean in the first case x decreases in the second case here when x equal to 1 y increases so  this thing always goes up it never goes down even when there is a match it does not go down the pattern remains as it is that is what that is what this is so  this follow initially starts out being 0 and the largest value it can be is n minus 1 so  maybe this is n so  the largest value is n actually  x will be 1 so  this is n minus 1 so  the total number of comparisons is order n  it is let us than equal to 2 n clearly which is what we wanted to show so once we compute this fx  which is a which is a property of this if the pattern we can do these pattern matching in linear timing ? so  we still left with the problem of computing this function f so that is something we are to do ?  refer slide time  10  48  and we will in fact  do this do it now so  what is it that we want ? so  we have given so input is the pattern p 1 p 2 p 3 p m it is m characters long now what we want as output is for each i  find the largest prefix largest or longest maybe longest prefix of p 1 p 2 up to p i minus 1  which is also a suffix p 1 p 2 up to pi so this length  we call say li and this is nothing but f of i plus 1 from the previous this is the f which was defined earlier so  for each i for each position i want to find the longest prefix of this string  which is the sub string of this  which is also a suffix of this  refer slide time  12  42  so  again pictorially let me draw this so  i have this is p 1 p 2 up to pi pi minus 1 so  i want the longest prefix which is suffix of p 1 up to pi which means the longest prefix will look like this p 1 this is pi minus 1 so that the minimum  i have to shift  so that there is a match up to this place this length is what we want and this length is called li it is also equal to f i plus 1 as of the previous so  let us focus on li will not look at fi as if not so  this is what i want shift the pattern a minimum so that  there is a match here suffix of this string is the prefix of that string this is a prefix and that this is the suffix this is what we want this is what we want to compute and this is a proof force way of doing this so  what the na ? ve way  well i just shifted by 1 and c if it matches shifted by 2 and c if it matches shifted by 3 and stop the first i am i get the complete match if it does not match at all the value is 0  this length is 0 i mean if at no stage you are find the match here ; that means  this value is 0 so  the na ? ve way is for each position up to i  i do these comparisons and it looks like i have to do about for position j i need do i minus j comparisons so  this will just be sigma i minus j  j going from this is the position j so  this is firstly for i going from 1 to m for each i  now i need to compare for each i i need to start this off from j  j going from 1 to i it is i minus j so  this is too much well i let you figure out what this sum is  but this is suddenly not linear in the size of the pattern this is too much time so  i guess you can write this it needed  but the time taken by this just too much it is suddenly not linear in the size of the pattern so  we want o m time so  this is not so well then  what we do ? so  we will put over algorithm design sort principles to use some extend so  we would like we again so  this is the problem on arrays roughly it is look like a problem on arrays or less so  we will use our favorite inductive kind of approach will use our inductive approach and assuming which means assuming that i have d1 it for you first i minus 1 places  how do i compute the ith value ? so  the value of l i have computed let us say up to the first i minus 1 places and i want to compute li  l1 l2 up to li minus 1 is d1 so  how do we do this ? the question also is can be use this l 1 l 2 up to li minus 1 so  let us see  what this is ? so  you thing is to keep working at the problem and trying and understanding what it really means sometime thing added some point and time thinks just start to click  refer slide time  17  30  so  here is my pattern this is the ith thing which i want to calculate li is what i want to calculate so  what i want is shifted by the least amount so that  it matches algorithm this matches up to here it all matches upwards  what is this is what i want so  shifted by least amount or the longest prefix whichever way you want to look at it so  the first thing to notice is that  when if this portion matches this portion exactly this portion up to the previous character also matches this portion if this big portion matches all the way up to i this portion up to the previous character also matches up to this portion which means if i want a prefix to be a suffix up to i i want to prefix also to be suffix up to i minus 1 so  i want that to happen  i know this length is at most l of i minus 1 so  i can start of by shifting so  i can start by shifting pattern by i minus 1 minus l of i minus 1 these have already computed by induction  i have already computed so  i do not need to start of here i can start of by shifting it by this much and now i start comparing i know that there is a match up to this place ; i only need to compare these two if there is a match here also  then i know what li is just li minus 1 plus 1 if there is no match here  then what we do to shift this pattern further but again  we use we make use of the fact that if calculate the l calculated l values previously  refer slide time  20  13  so  what happens here ? so  here is my pattern so  here is the same pattern shifted and this is the ith position  ith position in the top so  this is something else now  this i know is l of i minus 1  this is my first shift day shift this is my first shift and now i am comparing these two i know that shifting it by any less is useless so  shift by this and now i am comparing these two if there is a match or found li if this does not match now what should i do well i need to shift this further i need to shift this pattern further  why by how much how much do i shifted well again we notice that if you shifted more then this is the i minus 1 at position  if i look at this pattern and this i need a match from here forget this for the timing focus on these two focus on these two now  i know that these two patterns must match in this portion which means this prefix of the pattern must be a suffix of this part of the portion  this part of the pattern this index is l of i minus 1 so  this is nothing but l of l of i minus 1 so that is  what this length is so  let us this is just 1 level of because itself so let us see what is exactly going on so  i have a mismatch here now  i want to shift the pattern further this is the original pattern now i want to shift this pattern supposing this is the correct shift after this  this is the correct shift now  we observe that this portion of the pattern must match this portion which is what we want the middle part for the timing this portion must match this portion correct now  put the middle portion back this portion of the middle part match this portion of the text and so  must match this portion so  if i look at only the middle part and the bottom part  this portion of the pattern must match write above it so  if i omit the top part what is see is  this is a similar phenomenon  what i want  is the longest prefix of this portion which is also suffix so  this initially this length was i and i got l of i minus 1 now  this length is l of i minus 1 so  what i get is l of l of i minus 1 so  essentially i use the l values to shifted further that is what this length is so  i know what the shift is ? so  this shift will be nothing but l of i minus 1 minus 1 minus l of l of i minus 1 that is what i shift now  again if i find the mismatch again i do the shift only now i work with this rather than l of i minus 1 and so on so  you can see even though it is sounds bit complicated see roughly  how we use the previous values of previous l values  to shift the pattern forward so  let us write what happens well it could so happen that you keep shifting and you shift all the way to 0 so  it will happen that you shift it all the way across  there is no place to you get any you get complete match which case l of i could well turn out to be 0  refer slide time  24  39  so  let us write this algorithm down so  the position on the on input pattern let us say is i so  li is what i want to fill l 1 l 2 up to li minus 1  i have filled so  pointer on the prefix  this is the shifted  this i will call j so  this is my initial pattern that is i and this will be j so  initially j for instance here is li minus 1 plus 1  then it becomes l of li minus 1 plus 1 so that is what j the shifted pattern so  now what do you have  so again it is let us see l 1 is 0 let us do the initialization first l 1 is 0  i as set to 2 j i set to 1  this is my initialization so  i have to find l 2 l 3 and so on  refer slide time  26  10  now if so here is the procedure  if let us say note down separate things and need the space if pattern at i equals pattern at j then  well here there is a match so both pointers move forward so also  where was this  if there is a match at any time  then i know what the l value is if there is a match at any time i know what l value is its nothing  but j so  li equals j  this is the longest prefix that i can now increment i and j now  i want to find it for the next value of i this is inside if else this is the crucial part well again  if j is not equal to 1 which means you are not looking at the first character in the pattern in the shifted pattern then what can we say then  we just shift the pattern some more and what is the shifted value it is nothing but l of j minus 1 plus 1 so  this is what this is exactly what we had so  let me recall this portion it was l of we will looking at i  refer slide time  20  13  so  it is l of i minus 1 plus 1 similarly again here  it will be l of i minus 1 minus 1 l of that is what we want so  j will be l of j minus 1 plus 1  this is j is not equal to 1 if j equal to 1 this is an exceptional case then  which means we are looking at the first we shifted the pattern all the way so that  the first character match is looking is matches is below the ith character of the pattern and this does not match which means li must be 0 li equal to 0 and i can move forward so  i is i plus 1 so  i calculated li and i move forward this now goes into a loop this thing goes into a loop and that is it so  we just keep calculating this way so  what is the time taken ? well again  we break it into two parts 1 is the  if and else the successful match and unsuccessful comparison so  pi equals pj and pi is not equal to pj so  plus number of times you execute the l close so  how many times do you execute this well again we notice that here i goes up by 1 so  look at i  i goes up by 1 time which is the pattern you fill out this entry you filled out 1 entry in the pattern is li set to j and i goes up i never decreases anywhere so  the number of times  this will happen is m so  each time this happens i is incremented by 1 the largest value of i is n so  this is at most m  how about this else clause well either i is incremented or j is decremented which means pattern below its shifts again it is the same argument so  i look at i minus j the maximum value  this is always incremented here it remains the same here ; this is always incremented in this case so  the maximum time in the time taken here is m so  the total time taken by this procedure is twice m i just realize that i made will small mistake let me correct that so  everything was light except this so  just focus on this here i have i wrote this minus 1 which is not true what i said earlier was perfectly correct the reason the j value is actually n of i minus 1 plus 1 so  what i have here is l of j minus 1 when i take l of j minus 1  now it becomes l of l of i minus 1  this minus 1 now  the new j value will be this plus 1 and so on so  what i earlier said was correct  this length there is no minus 1  here this length is exactly l of i minus 1 i am sorry for this for this mistake so  let us go back to the algorithm  we just finished looking at   refer time  32  59   y the time is twice m so  in linear time we have actually computed the l values  well if you want to write l as an array i have written as subscripts i have this is the way i describe this algorithm you can of course  write l of j minus 1 if you are storing the l values and array and this is what this  how you write it i just written it as a subscript so  if there is a match then i have computed the l value for i and i go ahead and i am going to try and compute the l value for i plus 1 that is what this portion says ; this portion says if there is a mismatch then  i need to shift the pattern and how much do  i shifted by  what do i shifted by well  that is why this j comes in j i set to l of j minus 1 plus 1  refer slide time  33  06  the reasoning is exactly what we did earlier  because so here is my pattern the ith position is what i am trying to compare and i have shifted and this is the jth position on this bottom pattern this is i n that is j and there is a mismatch  how much you are shifted by well  if i shifted by some amount so  let us say i shifted by this amount and this is the value then up to here this portion of the pattern matches this portion of the pattern and i need to shift by minimum so  that this happens which means this length is nothing but l of j minus 1 this length is l of j minus 1 so  the new j will be here  i am going to compare start comparing this with i so  this is nothing but l of j minus 1 plus 1 which is exactly what we have so  if j is not equal to 1 j is l of j minus 1 plus 1 if j is 1 which means i have comparing the first character  if which means it looks like this  refer slide time  34  25  so  let us do 1 figure so  what happens when so this is i and the pattern actually looks like this j is 1.now  if there is a mismatch  then i know there is no prefix which is the suffix till of this portion so  li equal to 0 and i shift i now move everything over so  i move i 1 and i move the pattern by 1 and i start comparing from this same position that is what this is the whole thing is in a loop and you end when you finish scanning the entire pattern the total time is m  because the number of time this is executed is at most m  number of time this is executed is also at most m so  the total time is 2 m and in 2 m steps  we can compute this length l and once we have the length l  we can compute the f that we wanted by just shifting the l values 1 step to the right that is all the risk because  fi plus 1 is li and once we have computed f for the pattern now  we can scan any text for this pattern using the algorithm that we saw last time and that also works in linear time this algorithm is due to   refer time  36  57    this is not actually use in practice an algorithm  even though this is efficient and algorithm that is found to be more efficient in practice is due to boyer and moore and that what is used in   refer time  36  20   and boyer moore  the idea is very similar you want to shift by as much as possible and use the fact that has been a match in some portion of the pattern in text the difference between boyer moore and this growth moore is track is there boyer moore starts comparing from the last pattern i mean in the last character of the pattern downwards here  you start from the first character of the pattern upwards and boyer moore you look at the last character of the pattern downwards that is what they do but  the concepts are very very similar let us before  we windup this thing let us go back and look at those two examples that we looked at write at the beginning and see what happens to the f function and how this pattern matching algorithm works on that they were two strings that we worked at we looked at the beginning so  let us look at them and see what we have  refer slide time  37  30  so  the two strings first string was a b b b this is k times in other words it is a b to the k so  what happens in this case ? suppose this is the pattern what we do so  let what are the l values so of course  for a it is 0 if i look at a b  what is the longest prefix  which is also a suffix  while if i look at a b the only prefix proper prefix i have is a and certainly it is not a suffix so  this value is also 0  how about a b b ? well the two proper prefix is a and a b and meet the algorithm is the suffix here a is not a suffix ab is also not a suffix i will this matches this does not matches so  there is no proper prefix of a b b which is also a suffix of this is 0 and in fact  all of them as zero for n1 of them is that a proper prefix which is also a suffix  so that is the l value so  f values are also 0 now  what is so when there is a mismatch  what is what does 1 shift by let us recall this so the shift value  if there is a mismatch at the ith position is i minus 1 minus fi in this case it is i minus 1 if there is mismatch of the ith position i shift the pattern to the right by i minus 1 position this is exactly what we did earlier so  we basically shift the pattern all the way up to the position  where this a mismatch almost to the position  where there is a mismatch so that was for that pattern and this is exactly what we did earlier before we did all this you know analysis about prefix and then suffixes but  this is the idea that the sort of intuitively used use then  refer slide time  40  07  so  let us look at the second 1 which was a a a b this is k times the string of a  so this is the second pattern that we looked at so  what are the l values ? well it is 0 for 1 the first 1 if i look at a a the longest prefix which is also a suffix well the only proper prefix is a that is also a suffix so  this value is 1  how about a a a ? well the longest prefix which is also a suffix is a a longest proper prefix which is also a suffix is a so  this value is 2 and so on this value will be k minus 1 for this it will be 0 these are the l values what are the f values ? well the f values are just all of these shifted to the right so  i have 0 0 1 2 so on k minus 2 k minus 1 these are the f values and if there is a mismatch  then you shifted by i minus 1 minus fi and in this case and this case this will be i minus 1 and fi you can see its nothing but i minus 2 so  this is 1 so  you actually shift the pattern by just 1 unit which is also what we did there if there is a mismatch we only shifted it by 1 unit but  the crucial thing is we start compare now we start comparing where there was a mismatch we do not go back and compare everything again this was the other lesion be we had we are learned  we also done this earlier that once the pattern once you have matched up to some level of the pattern we know the previous things are all a ? s in this case some we do not have to again sort of match all over them we know that the shift that portion will always match with the previous thing we just need to look at the new position in the text so that is it for pattern matching from us i would recommend strangely that you take more examples and do this yourself take examples of text strings take examples of pattern take these patterns compute the l values compute the f values see  how these values you know sit in then  take text and see how the algorithms runs with this pattern and takes what happens to the pointers how the pointers move how do you shift  what exactly this 1 mean by shifting a pattern etcetera  etcetera so  i would like you to run this algorithm you let you hopefully understood on a few examples that you can cook up yourself so that you get a better hang of what exactly is going on thank you design and analysis of algorithms prof abhiram ranade department of computer science engineering indian institution of technology  bombay lecture  16 combinatorial search and optimization ? i welcome to the course on design and analysis of algorithms our topic for today is combinatorial search and optimization the general problem that we are going to address is something like the following  refer slide time  01  27  so  we want to find an object and this will usually be a discrete object say a set of numbers or a graph and since it is a discrete objects object when we talk about a set of number we will usually mean set of integers so  even perhaps set of zero and values and this object must satisfy certain given constraints find us find an object such that first it satisfies some given constraints and further more optionally then might be a second requirement that of all objects satisfying those constraints we want an object of the lease cost  where the cost is a function defined on the objects ? is a function from the space of objects to real numbers ?  refer slide time  03  24  optionally  we might also have an alternate definition in which instead of saying we want least cost so option 2  it could be stated differently instead of asking for the object of least cost we want an object of maximum benefit find object of maximum benefit of course in addition to satisfying the given constraints benefit is also a function from the objects to real numbers given note initiately that these are not really too distinct for simulations but  we can set benefit equal to negative of the cost and then instead of maximizing the benefit it is the same as minimizing the cost so  this is an abstract definition and let me give you a few examples initiately  refer slide time  05  09  so  let me first start with the familiar example in which there is no objective function given so  this cost function for example  or this benefit function is also often called an objective function and so in this first example  we are not given an objective function  but here only given constraints so  this example that i am going to talk about this is so called 8 queens problem probably  most of what this is so  we are given usual chess board and we want to place queens on it so  place 8 queens on usual which is an 8 by 8 chess board such that no 2 queens capture each other or no 2 queens are in the same row same column or same diagonal we remainder by this is a problem in the sense  we have described well  we can always modulate as problem on numbers an integer so  we can think of this as find q 1 q 2 to q 8 let us say q i represents where in column i the ith queen is going to be placed so  find these q such that q of i is not equal to q of j so  interferic q of i as equal to the row in which the queen in column i is placed so  this condition simply says that the green which get placed in the row in column i does not have as it is row number value  which is the same as the value in which value which the queen in the jth column is placed so  in other words the ith queen in the jth queen are not placed in the same row and we will assume that the ith queen is placed in the ith column and the jth queen is placed in the jth column but this is not all so  we have already taken care of the first condition that they are not placed in the same row well  they are not placed in the same column because  we are only defining 8 variables and we are saying that k of i is the position the queen which is going to a fixed in that which is going to be placed in the ith column and we are only giving the row now  we need to take care of the condition that they are not placed in the same diagonal and this condition is easily taken care of by asserting that q of i minus q of j this is simply the distance along the rows the vertical distance the distance between the rows and if i take the absolute value it will actually be an unsigned and this should not be equal to i minus j this is true for all i j distinct so  we have now formulated the 8 queens problem as the problem of finding a sequence of numbers q 1 through q 8 such that they satisfy these 2 conditions well actually these are not 2 conditions between these conditions which apply for all i and j so  this is say  whatever 8 times 8 upon 2 conditions this is also 8 times 8 upon 2 conditions  refer slide time  09  15  of course  this is really a toy or game problem  but we can do we can have more serious problems as well so  let me give a few of those the second problem that we are going to look at is so called knapsack problem in this case in fact  we are going to have an object function as well   refer time  05  09   so in this case  our question was to their exist such numbers that is satisfy these conditions but  it did not say that if there are several such 8 tuples of number which one we are interested in we said the just find any such set or any such sequence here  also going to have an objective function so here as  we are going to have an input and the input consist of say numbers v 1 v 2 all the way to v n and thing of v i as the value in rupees say of ith object so implicit is that  there is a set of n objects over here each object has a value and that value is known to us in addition each object also has a weight so  say w 1 w 2 all the w8 w n and say such that w i is the weight saying in kilos in kilo grams of the ith object and imagine that  we are given a knapsack which has capacity c  c is weight  capacity by weight the problem should be this manner ; we want to determine which of these objects we should place in the knapsack such that we do not exceed the capacity of the knapsack and such that we are picking up objects of maximum value so  we want to find a subset of the object such that value of objects the total value of selected objects is as large as possible  while capacity while total weight is less than or equal to 0 so although  this problem looks like it arises in the context of objects and values and weights it can also arise and other context so maybe  there are jobs and their profits associated with the job and we want to decide which job to select something like that so  again there is a constraint over here well first of all there is a combinatorial object that we need to take which is this  which is a subset of the given objects or crops call these objects items so  we want to pick a subset of these given items and there is a constraint on them that the weight of the selected items should not be bigger than c that the constraints and then there is also an objective function the objective function is that there value should be as large as possible so  this is a maximization problem and the objective function given to us can be thought of is a benefit function because after all  if we pick as many objects of large value we are maximizing the benefit to ourselves  refer slide time  13  32  let me give you one more example  this is another classic example this is so called travelling sales person problem the input over here is a weighted graph and the output is a tour through the graph that visits every vertex exactly once and objective we want to minimize the weight of the edges in the tour the names come from the association that we can think of this weighted graph as representing a map map of a city for example  i am sorry a map of a country or a map of the world vertices represent towns and edges represents roads and weight represent the length of the edges so  imagine that there is a sales person who wants to visit every city for his for selling his products however  he wants to make sure that he travels as little as possible so  this is a problem that he might he or she might want to solve she might so  he would like to find the tour that visits every city exactly once such that the total distance covered along all the edges is as small as possible again on one hand this may seem like a problem relating to sales persons and cities and roads but  this problem arises in many many contexts for example this problem arises in robotics ; this problem arises in computation of biology and many other fields before carrying on let me do an example of this problem just to clarify what is meant over here so  let us say that we have four cities these are our four cities and so this is our graph so typically  we will have a complete graph over here the graph i am going to draw is going to be a undirected graph but  this problem makes sense in the directed case as well  so this is the graph and let us say there are weights over here so for example  this weight could be 1 this could be 2 say 9 this could be 6 this could be 2 and say this could be 4 so  there are various ways in which the sales man could tour through this cities and so  for example  1 way is this outer most this outer tour  what is the cost of the ? well the cost of that is 1 plus 2 plus 9 plus 4 so that is 16  here is a better tour so for example  something like this so  this figure of 8 this tour  what is the cost of that ? that is 2 plus 6 plus 2 plus 4 so that is 14 so  this is in fact  a better to over than just the outer tour and in fact  you should be able to check that this is the best possible tour in any case the pointers we want to we want to make sure that we have found the tour which has the minimum such cost at the minimum such length if you like these are only three examples of the kinds of problems we have in mind and in fact  later on in the course will be considering more will be considering more problems of a similar kind  refer slide time  18  10  but let me now state the topic for today the idea today is to look at we want to start a study of strategies for solving combinatorial optimization problems combinatorial optimization and search problems  i may drop the search occasionally  but that is what that is imply today i am going to talk about a few basic strategies one is so called well one is based so called backtrack search  but it really a sort of a brute force search strategy so  under this we will be looking at what is called backtrack search then  we look at the strategy called branch and bound will also be looking at a strategy called dynamic programming and in fact  it is this strategy that we want to study the most because  this is the strategy which we can analyze and very often it gives us very fast algorithms as well but  it requires as considerable about among the diff level analysis but  i am wanted to start talking about this strategy today there is also a strategy which is often called greedy strategy which is also going to be discussed on this course because  this is also gives us very fast algorithms for solving combinatorial optimization problems let me  point out right away that this strategy the backtrack search strategy is inner most was general this can be apply to almost any problem this is the more restricted strategy  but it becomes little bit more efficient than backtrack search problem search strategies dynamic programming typically is more efficient than branch and bound but  it requires as to do a lot more analytical work we need to actually exploit the problem and it is not always the case the dynamic programming ideas will be applicable  refer slide time  21  02  so  let us come to brute force search strategies the name should suggest to you that we are going to do something fairly simple minded and in fact  let us just catch how you may do this simple minded ; we might do something simple minded to make sure however that we do get an answer correct we correctly so  the basic idea as you might guess or as in such as is to systematically somehow a numerate or generate all possible objects which can potentially satisfy the constraints for each object here is  what we do ? then  we first check whether in fact the constraints are satisfied if so  we evaluate the cost or benefit function if the cost is better than the cost so for that is if the cost of this object is less than the cost of the best object so for then we storage so  we record cost if necessary similarly  if there is a benefit function given to us instead of the cost function we evaluate the benefit function and check this benefit function with the best benefit function found so for if this new object has a benefit function which is better than the best of best benefit function founds so for then  we record this benefit function and also the current object record cost  if necessary and current object so clearly  if we generate all possible objects and we check whether each of them satisfies the conditions this procedure must certainly work this procedure must will be guaranteed to give us the correct answer just because  we are just going over we are doing everything that can possibly have to be done  refer slide time  24  04  a question over is how do we systematically generate all possible objects and that is where slight amount of cleverness comes in so  generating all possible objects are how to generate  the basic idea is think of the object that we want as a collection of parts or a collection of slots then  consider each possible way of generating each part or if you want to think of this object as a template in which there are slots to be filled we want to think of each possible way possible way of filling each slot  how do we go about actually doing this ? well we go over the parts or the slots one after another so  the basic idea is that initially the sort of have an empty template so  all ? s slots are empty then  we fill in the first slot of course ; this can be done in several possible ways so here  we have to consider all the alternatives then  we fill in the next slot and again we consider all the alternatives over here and so on  refer slide time  26  17  let us  take an example and that will make this idea very clear so  let us take the example of 8 queens  but sense this page is small  let us just make it 4 queens so  what we have well we have very wells q of 1 q of 2 q of 3 and q of 4 so  we can think of these four as the smaller parts comprising our big object that we need to find or we can think of this as a template with four slots and it which is given to us and we want to fill in values into the slots instead of looking at these numbers themselves i am going to think of this i am going to interpret these numbers and pictorially draw the board that corresponds to this so initially  we are going to start up with an empty template so that corresponds to a board which is completely empty then  we are going to consider the ways in which the first slot in this template can be filled so q 1  we said must have a value between 1 and n where n is 4 over here and so  there are four ways in which this can be done so for example  something like this so  we can placed the first queen in this first position over here but  this is not the only way in which this slot can be filled there are many other ways in which the slot can be filled as well and so  here is another so from this empty template  we can come to this way this possible way of filling the first slot that is that we placed the first queen in the second row of course  it is not necessary that we placed in the second row  but we might place it in the third row as well so  this is that corresponding position and of course  we can place it in the fourth row i am not drawing the entire board but  i am just telling you where the queen is going to be placed so  these are all possible ways in which the first queen is placed first queen can be placed and we started out with the empty board and at the end of one way of one considering how the first slot can be filled we have arrived at four possible positions four possible candidate objects each of edges only partially built so  we started up with one partially built object candidate object and now we have four partially built candidate objects but  they have been they have been extended so  we started up this object and these are object which are full arrange substance or more build up than this previous object but  we can keep going in this manner so from this point  we will again consider all possible ways of placing the second queen the first queen is placed over here  the second queen could also be placed in the same row or it could be placed in the next row or the row after that or finally  in the last row so  when we come down to this level we have our object even more fully constructed so  we have placed 2 queens  we are not placed all of them here but  this is at a higher level of construction higher level completion than this one which internal at higher level of completion and this one and of course  each of these objects will have to be constructed further so  there will be something over here and meet everything which represents the ways of placing the third queen and then and read  this there will be something which represents the ways of placing the fourth queen in this case  we can see what is going to happen so  we started with one node  then we have four nodes we have four bolts in this next level then  we have a 4 square bolts to be consider at this level 4 square partially constructed objects then  we have a 4 cube partially constructed objects at the next level 4 times as many as the when the previous level and finally  4 to the 4 objects at the leaf level  in fact  i have already told you what this looks like in some sense so  we have leaves at the bottom  we have root at the top and in fact  this is going to look like a tree in fact  in almost every in every exhaustive generation method we will be following something like this so  we start of in the empty object  we keep on extending it so  the extension will correspond to children of the parent node their extensions will correspond to object which are even fuller and until we get to a point at which we can not extend object any further that is the point at which we have got to leaves and that is the point at which we can terminate our generation step once  we get to a leaf in this case for example  we know that we have all the queens completely placed so  what is the next step ? well at this point we are going to check if the queen position satisfy our conditions if they do then in this case  we can just report we can just print out saying yes they do in fact satisfy all the conditions if they do not well  then we have to consider the other leaves this is as for as considering problems in which there is no objective function if there is an objective function then when we get down to this level  we need to not only consider whether the leaf the leaf objects satisfy the constraints but  we also need to evaluate the cost function so suppose in fact  we have a cost function rather than a benefit function this for each leaves  we can think of the cost function and we can evaluate the cost function and our problem is to find a leaf such that it is satisfies all the constraints and such that whose cost functions is as small as possible  refer slide time  33  07  so  let me  said as summarize what i said in terms of a programming idea so  first we need something so that we represent our objects properly so for example  in the case of the 8 queens problem  where the four queens problem ? we used a data structure of an array of four element array to represent the four queen positions in case of the travelling salesman problem of the travelling salesperson problem  we want to represent the tour again if the graph as n vertices we could use an array of length n  where the ith element of the array denotes the ith city in the tour in the case of the knapsack problem is simply need to say whether a certain object has been selected or not so  corresponding to each object we could have a bit which is equal to 0 meaning that object is not replaced in the knapsack or if that bit is 1  then the object is replaced in the knapsack so some of the other  we need to have a representation of the object that we are looking for after that  we need a procedure for filling in the next empty slot in the object of course  there will be many ways in which this can be done and so  this procedure should allow us to provide all possible ways at this point in the algorithm  we are simply going to recurse so then  we will recurse and we will recursively fill in the sub sequential loss  what happens next ? when we have recurse we will it is prevalent to starting at say the root and we filled in the object filled in the first slot and then when we recurse we filled in the next slot and so on eventually  we will get the leaves so viral you need to have a check over here  just to see whether we have in the leaves so for leaf objects  we need a procedure that checks constraints i am it written true if all constraints are met it written false if the constraints are not met we also need a procedure for evaluating cost that is not all however so this way  we keep going down and we go down to a leaf but  you also want to go back up  and go to the other leafs and that is why this whole procedure is called back track search so  we get to the leaf and then we go back up again in the tree and then  we go to the next leafs then from that leaf  we go back up again  refer slide time  37  10  so over here  we said that we need a procedure for filling in the next empty slot in the object but similarly  we need a procedure for removing the last value filled in the last slot this is how we will go back up into the tree and make sure that all the objects will get generated  refer slide time  37  50  so  let me try to write this as a very vaguely as a procedure just give you the idea of what is going on so  our search procedure is going to look something like this so it going to take this candidate objects as an argument and let me  just note that this while have a return while have return on object every here i really mean this template which could be partially filled so  what does this do ? so  if object has all slots filled which is to say that it is a leaf then  we are going to check constraints we are going to evaluate the objective function if constraints satisfied and in fact we will return the value  if the constraints are not satisfied we will return so  if the constraints are not satisfied  then will return infinity if we are given a cost function that is to say that this leaf found is to be disorder infinitive will just say that this is a useless leaf if you return a minus infinitive or a zero if this is a benefit function assuming that the benefit is positive we can even return zero so this is what we do at a leaf ? if object has unfiled slots then what we need to do ? well we need to do this branching business  refer slide time  40  09  so  this has an unfilled slot so  we need to filled for this slots  if object has unfilled slots we are going to consider all possible ways of filling slots filling next slot so  you will fill in ith possible manner we will recurse on modified object and this recursion will give as value so  let me call this v sub i and this will be in the end of the loop so  this think of this consider as being a loop and now the idea is something like this so  we are going to be go over all possible leaves underneath so  starting at this or starting at this we will be visiting this will be visiting this will be visiting this will be visiting this will be visiting this and we are going to associate with this node a cost function which is the value of the smallest cost function found anywhere over here that way  we can written that and eventually will make sure that the smallest cost gets written to the top  refer slide time  42  13  so  here we are going to be return in smallest v sub i calculated above so let me  do this again so  we start up with an empty object  then we consider all possible ways of doing it but really in the first instance  we just consider we just go along this path then  we go along this path then we go along this path and so on until we get to a leaf say let us say we get to a leaf over here then  we return the value of the cost as seen at this leaf then  at this point we consider other waste of generating it and we get to another leaf for example  then we return this cost then  we go back again then we return the cost over here and let us say there are no more ways of filing in that last slot then at that point what gets return over here is the smallest of these cost values so  in general in this manner  what is going to get return to the root  is going to be the value generated at along some path in this graph and that is going to be the least cost leaf the value of the least cost leaf so  this complete at description of this search procedure in fact  if you have studied this in the data structures scores  what you have done really is a depth first search over this search space search space is simply set of leafs we are looking at all the leaves somehow the other and so that is of called search space and what we are doing is ? we are putting a tree on top of that search space and we are just doing a depth first search of it  refer slide time  44  04  there are some improvements possible to procedure we do not need to do our condition with checks our constraints checks at the very end we can do them as early as possible so for example  in case of the 4 queens problem suppose  we placed the first queen in this first column in this first row then when we placed the second queen in the first row as well we could say look these 2 queens already capture in each other or the condition that q of 1 equals to q 2 to not equal to q 2 to is not being made and therefore  we can say we do not really need to go and further check where condition where queens three and four placed so  what happen to be here was that we started up with empty board then we placed the first wheel and then we placed the second wheel as well and then  we can safely ignore this part of the search tree immediately because  we have checking the condition right here and this condition which we check should be something which will be valid no matter what will be get over here and in fact  that is true  these 2 queens are not going to change their position at any of these leaves at these leaves only these queens will get placed and therefore  this condition   refer time  45  18    so  the possibility of early conditional checks condition checks can improve up on efficiency  refer slide time  45  36  i would also like to note that this procedure is ; obviously  correct and that really follows from the fact that we are considering all possible ways of filling each slot  refer slide time  45  56  let we go to our second example which is the travelling salesperson so  i am just going to give a quick description of what this search tree is going to look like so at the root  we are going to have the empty tour so  let us see this is our graph then  we can say that let us say just designates this vertex of the starting vertex it does not really matters since  we are going to come back to vertex anyway since  the tour is cyclic but suppose  this is the starting vertex  then we can ask how many ways are there in which the tour we can go out of this vertex and that gives us that is sort of closing in the first slot in the tour so for example  we can come to this point at which the tour has proceeded along this edge or we can come to this point at which the tour has proceeded down diagonally or we can come to this point at which the tour has gone now then this tour can further we extended over here so starting at this  we can go down directly or it can go back diagonally and so on so  eventually we will get to a leaf where will be having complete tour  refer slide time  47  29  so  let me say something about the running time of backtrack search so basically  the idea is that we are going to look at every possible leaf so  the time taken will be will essentially have to be at least in the worst case have to be the number of leaves so  let us consider this for the case of the n queens problem specialized generalized it 2 n queens rather than 8 queens or 4 queens so in this case  how many leaves to be have ? well the first queen can be placed in n ways the second can be placed in n ways and so on and so  there are n to the power n leaves and so time is at least n to the power n if you implement the early checks  then the early checks will improve the situation some part and in fact  a simple very simple early checks is to say let me make sure that this new queen is placed in a different row from all the previous queens so here  our output will simply be a permutation on the numbers 1 through n and so  here there will be n factorial leaves so  the time is going to be omega n factorial this is still to still quite large  what about the tsp  how many tours do you have  through n cities well starting at any city the first city you can be placed can be picked in n minus 1 factorial ways then  n minus 1 ways then the next n minus 2 ways and so on and therefore  we will have n minus 1 factorial different leaves in this search space so simply put  there are three ways of getting to the first the vertex of the first level the next there are two children and in this case there will be 1 child and so on but in general  if you start you can n city tour there will be n minus 1 children of the first level n minus 2 children at the second level for each of the vertices in the previous level so therefore  we will have n minus 1 differently used and again here the time will be omega n factorial  refer slide time  50  10  for the knapsack problem  it should be possible for you to improve that there will be 2 to the n leaves corresponding to all possible ways for mean subsets of n objects  refer slide time  52  00  so  let me summarize this co situation so  what have you done ? so for  we have looked at very general method it is brute force in that it generates every possible object and it does not try to be two efficient or two clever in saying look i do not i am not going to generate this class of objects  because this is not going to be useful to me and typically it takes long time in fact  typically takes time exp1ntial in the size of the object so either  due to again or n factorial or n to the n or something like that and we have defined terms likes search space and tree that set on it as a search tree and let me end this lecture by mentioning one more term which is combinatorial optimization combinatorial explosion this term is commonly used in connection the back track search the idea is that at the first level the tree might look manage level  because it has a small number of children and the second level the number of children the grand children of the root will grow at the third level the number of children will be a further and eventually even you get believes you will have many  many  many children or many many  many vertices and so  enzymes your tree will explore and this think and this phenomena is commonly called combinatorial explosion  refer slide time  52  40  so let me  just look at what we are going to do next we are going to study improvements to backtrack search as mention before we are going to study a technique called branch and bound and then we will look at dynamic programming and greedy strategies and this is really the focus of this course  but it is important to view these as part of a grand picture in which sort of the simplest backtrack search ideas are present as well as these more sophisticated strategies are present we should also note that the backtrack search is a very  very general strategy  whereas these are very  very sophisticated but very specialized strategies  these are likely to be fast as we will see in the next lectures this is likely to be slow  but it is likely to   refer time  53  52   it is going to very  very general and applicable in almost any combinatorial optimization problem design and analysis of algorithms prof abhiram ranade department of computer science engineering indian institute of technology  bombay lecture ? 17 combinatorial search and optimization ? ii welcome to the course on design & analysis of algorithms our topic today is combinatorial optimization and search and this is going to the second lecture  on this topic let me start by summarizing  what we did last time  refer slide time  01  09  in the last lecture  we gave examples of what we mean by combinatorial search and then we discussed  a fairly general technique called backtrack search this is sort of the obvious idea  just executed systematically so  let me just summarize by saying that it is a general technique it will work essentially for every combinatorial optimization problem or combinatorial search problem and the basic idea  over here is to systematically try out all possibilities the problem of course  with these techniques was so called combinatorial explosion as we explained last time  this just means that the number of all possibilities  which we are going to trial is really large and therefore  this technique typically takes a lot of time however for many problems  this is the only way to go our topic today is to consider cases  in which we can do better so  today ? s topic is going to be how to improve upon backtrack search  refer slide time  02  50  and specifically  we are going to see a variation called branch and bound this is going to be the main topic but  we will eventually also look at other ideas called dynamic programming and also greedy algorithms  which are also used to solve combinatorial optimization problems but this will come later  all this will come later and we will deal with dynamic programming  as well as greedy algorithms are quite some length and today  we will mostly focus on branch and bound so  the main question is  we know how to do backtrack search and we will review it  in a minute how do we make it more efficient that remains the main question so  let me start by reviewing what backtrack search is ?  refer slide time  03  57  so  the idea over here is that we are expected to find the combinatorial object let us call it some x star  which satisfy certain constraints so  we know all the constraints and we require that the combinatorial object x star must satisfy all those constraints and further there is cost function  which is also given for these objects and we are  we need to minimize the cost so  there is a set of possibilities for x star and somehow  we have to be sure that we have found the one with the least cost among them and furthermore of those  we first once which satisfy the constraints ; and from those we find the one which minimize the cost so  last time we said that a good way of organizing  this search is to start up with an object techniques called x x is really not an object in the sense that we have defined over here  x is sort of a template so  x think of x as consisting of slots  which need to be filled the first slot of x can be filled in several ways and corresponding to that will have branches going out of this node so  the first way of filling the first slot  will give me a partially constructed object  let me call it x 1 the second way of filling  this first slot will give me partially constructed object x 2 and so on so  the first slot can be filled in many  many ways and i will get  different partially constructed objects based on ; however  choose to fill that slot i can keep going in this manner say from x 1 or from any of these now  i take any of these partially constructed objects and from these  i am going to filled the second slot so  let me denote this by calling this new object x 1 2 well it is not a fully constructed object yet x 1 1 but two of its slots have been filled the first slot has been filled  in the first possible way the second here  has been filled in the first possible way as well but  the second slot could also be filled in another way and that another way  could be written down in this manner so  in this manner i keep going and eventually  i will get the complete object constructed over here of course  when i get the complete object constructed  i have to stop so  at this point  when i get to the leaves of this tree i have to stop and at each leaf i am going to make sure that may constraints are satisfy and then i will evaluate the cost function  given to me and then from that  i will pick the object with the minimum cost so  let us say the minimum cost object x star appear somewhere over here and that is how i am going to find it and that is the way that i am going to written we said  last time as well that this is a tree  which is often called the search tree  this tree which is the search tree is explored in a depth first manner so  from x we generate  we fill the first slot and we generate x 1  from x 1 we filled the second slot and which of the rate x 1 1 and so on  till we get to the end at this point  we check the constraints  we evaluate the cost if necessary and then we go back and then we consider the second way  of the next possible way of filling  the next possible slot or the same slot and then we go down in the tree  but in the different direction now and so  whenever we come to the leaf  we go back and in this manner  we go through the entire tree  going back and forth and this back  this going back gives a name backtrack search for this method so  we will have to explore  the entire tree and in order to be sure that we have found the minimum cost leaf extra so  this is the organization of backtrack search although  i have drawn the tree in parallel  i have drawn the entire tree for you remember that the way a program would execute this procedure it will start with the top node under it and will go along some particular path  then come back  then go down again so  at any point and time  it will only be having certain path in this entire search tree and it will either go down that path or it will roll back that path and then take a different path that is how the algorithm will execute so  this is the search tree that we mentioned and now the natural question is  do we need to explore the entire tree  refer slide time  09  36  this is an extremely important question we said that there is combinatorial explosion and that because the number of leaves is a huge number  the leaf coming back to this picture at every time as we go down the tree  the numbers of nodes doubles are get multiplied by a large number as we go down and therefore  the number of leaves is enormous so  suppose it was possible for as to say  that look anything that is underneath these x 2  really does not need to be explored and without even seeing  anything underneath it  if we could do that  then we will be saving ourselves a lot of work and so the idea that we are looking for over here is called pruning so  another way of expressing the same version is  can we prune the search tree so  if we could do that  then you reduce our work and then we would have  a more efficient way of searching  refer slide time  11  11  so  the branch and bound is just one possible heuristic of pruning  is a pruning heuristic so  let us come back to this picture here again  refer time  11  28  so  let us say that we have explored this whole thing  we found certain solution x over here and we evaluated its cost and this cost turned out to be some c of x so  we went down along the left branch in the tree and we found  a certain c of x now  suppose this cost function c can be used to evaluate the cost of x  which is a complete object or let me call this say some x bar  x bar is a completely defined object but  let us suppose that this cost function c is also  will also be able to tell us the cost of partial object so  this is an important idea so  we are given a cost function c  some c of say x bar  which gives the cost of a complete object x bar but  suppose c of say some xij is also defined suppose so as we construct our object little by little  we can still associate a cost with it this may not always be true  but in some cases  it will be and we are talking about those some cases and we are going to give an example in a minute and furthermore  suppose this extended cost function  now has the following interesting property so  in addition to this  this is the first condition that we need  in the second condition is that if say x bar is obtained by extending xij  then we know that cost of x bar is greater than or equal to cost of xij suppose the cost function satisfy these two properties what are the two properties that it must be defined  even for partially constructed objects and furthermore  we should only increase as we go towards completely building that object another way of saying the something is that  this is our tree  refer time  14  26  so  the cost function should be defined at every node in this tree and furthermore  it has to only increase  actually does not have to increase but it can not decrease  it should not decrease as we go down from the root towards the leaves so  if these two properties are met then we can have a branch and boundary    refer time  14  46    so  let me write that down and then i will explain why so  this implies that branch and bound is possible  so here is the idea so  suppose i have found some solution over here  a complete object and its cost  refer time  15  13  and now i am exploring this tree and i go back and i go back to this point x 2  at this point avail you evaluate the cost of x 2 now  if the cost of x 2 is itself larger than the cost of the best solution that i have found so far  then something interesting has happen i know because of the property  of this cost function that no matter how i complete this object  refer time  15  39   how i extend this object into a full object  the cost is only going to increase but  this cost itself is bigger than the cost of the best object found so far and therefore  there is no point in searching further below x 2  this is an important idea this is the idea of branch and bound so  let us take an example  refer slide time  16  06  so  my example is the travelling salesman problem so  i am going to use a very simple graph  to explain how branch and bound can be used with tsp so  here is my simple graph so  it is just a problem on four cities so  here are the edges and vertices in my graph  this is of course  ridiculously small problem  but it will do for explaining our ideas let me put down the weights along with this so  may be this edges weight 2  let us say this edges to be 1  let us say this edges be 15  this edge over here has be 10  say this edge has be 3 and this outer edge has be 1 so  this is the graph that we have given and our goal is to find a tour through this graph  a tour is simply a sequence of vertices  which contains all vertices and this is the complete graph and furthermore  the weight of the edges traverse according to the tour should be as simple as possible i am going to first explain  how backtrack search should work on this and then i will tell you how  a branch and bound heuristic will allow us to prove away a lot of the searching so  let us start with the empty tour  the empty tour is  so these are my four vertices and let us say this is my starting point  i could have anything as my starting point  does not really matter so  this is the starting point at this starting point  my idea is going to be that i am going to extend the tour  just i have been given a partially constructed object  an object which represents a tour which is not fully constructed here so  i am going to extend it edge by edge so  the first edge i can add in several ways so  for example  here is one possibility so  i can start over here and i can go down so  these are my other two cities so  i could have gone down over here of course  i could have something else what could i have done  i could have say taken the horizontal edge instead this i will explore little bit later  but right now this is what i am going to explore so  as we said earlier the algorithm is going to proceed in a depth first manner so  the algorithm will make a choice that the first edge is the downward edge over here next  it is going to extend the tour starting at this vertex so  how will you do that  well it will consider all possibilities out of that so  from here  from here there are two possibilities  now either it can either take the horizontal edge or it can take this diagonal edge going back so  let us say take this diagonal edge  the horizontal edge will come in somewhere over here then from here what are the possibilities so  from here the possibilities are that till valid it is gone down and it is gone up and this point really there is only one thing left to do  it has to go down here again  it has to go down and then finally  from here there is only one possibility in fact  so there is no  even here there was one possibility ; here as well there is one possibility so  this is what we had done so far  and then from here it will go back so  this is the tour that we would have got  by doing depth first search so  take the first alternative at each step and we would come to this tour at this point we would ask  what is the cost of the tour ? what is the cost of the tour here  so it is 2 for this 2 plus 1 3 3 plus 3 plus 1 so  the cost of this is 6 so  in backtrack search  we would just record this 6 and we would say this is the best object we have gone  so far what we do next then we would go back from here when we go back here  we say is there another way in which this tour could have extended no there is no other way because from here  we have to go back to the starting point so  there is no other way so  then we go back over here again so  at this point again we need to ask is there another way of  in which this tour could have been extended well even here  there is no other way because we can not go back to town which we have already seen  because we have to go through all the towns first so  the only way could have been to go down  which is what we did over here so  even here there was no other choice so  we come back over here and at this point  we could ask well instead of going in this direction could be go straight so  yes we could go straight in fact  and therefore  we would go and we would explore this part of the search page so  from here  instead of going back diagonally we could go down horizontally now  from this point onwards backtrack search would go down and search the rest of the tree   refer time  21  43    but  here is how branch and bound would make a different decision now  branch and bound could say  let us evaluate the cost of the tour that we have constructed so far so  what is the cost of this tour  the cost of these edges is 2 the cost of this edge is 15 so  the total cost of this partial object that we have constructed is 17 we know that as we add more and more edges into this  the edge length that the total distance covered is only going to increase and this is based on the important assumption  which is that all edge lengths are positive let me write the term because that is really an important assumption  all edge lengths are non-negative if this assumption when not true  there we would not have a branch and bound algorithm at least not the way  i have described so far but  this is the very natural assumption and so we can conclude at this point that in fact  any possible way in which this tour is to be extended  does not need to be considered  because its cost is going to be bigger than 17 and therefore  it can not beat this best cost tour that we have found so far so  branch and bound would not do  any of the work of exploring anything below this so  at this point branch and bound will say  oh i do not need to do anything and i would return back and it would keep the 6 as the only tour that we have found so far and that is the best and it would know that with assurance   refer time  23  34    we have essentially proved in fact  that noting underneath over here needs to be explored what would happen next  we come back over here    refer time  23  44    and then we say  instead of exploring in this manner  instead of taking the first edge itself going downwards  let us take it some other way so  what is another way  it is this way  let us take the horizontal edge again branch and bound will evaluate  what is the cost of this partial tour that we have constructed what is the cost over here  the cost of this partial tour is10  because this horizontal edge has cost 10 again  without exploring things below this vertex  we can safely conclude that this is large enough  that we do not need to explore anything underneath it because whatever comes underneath is  is going to be obtained by extending this tour  which already has cost 10 and therefore  we can say forget it  the cost of anything below this has to be bigger than at least  it has to be at least 10  but we already found that tour of cost 6 and therefore  we can return back over here and then we can take the next stage and so on so  in any case in backtrack search we would have explored everything underneath this  in backtrack search we would have also explored everything underneath this in branch and bound  we come to this vertex and we know we do not need to do this work also we come to this vertex and we do not need to do this work and therefore  we have saved on the total amount of work that we do so  let me just summarize this  refer slide time  25  16  so  let me just write this as a comparison of backtrack and branch and bound so  this is backtrack and this is branch and bound so  branch and bound backtrack is a general method  branch and bound needs suitable cost function and there might be some amount of cleverness that might be needed  in defining this cost function this may do more work  because more nodes of the tree are visited in this  there maybe pruning and there may be fewer notes of the tree  which might get explored in this but there is this small overhead of evaluating cost at every node in general branch and bound  if you have reasonable cost function will work substantially better than backtrack and i might go as far as saying  that whenever you can find a reasonable cost function  which has the property that i just mention you would invariably use it  because the gain from the pruning that you get  will typically be much more than the over head of maintaining  the cost function is concerned so  let us take one more example  before we go on to something else  refer slide time  27  26  so  we are going to look at branch and bound  but we are going to look at branch and bound this time  for the knapsack problem we introduce this problem last time  but let me defined it again so  the input r is the vector is r two vectors  say v 1 2 then v n so  we have been given n numbers  vi represents the value of the ith object and we are also given numbers w 1 w 2 till w n where  wi represents the width of the ith object and we are given one more input  which i will call c  which is the capacity so  we have been given think of it  as in this manner so  we have a back of capacity c where  c is also measured as a weight so  say we have been given a bag  which can carry at most 50 kg in front of us r n object  we know the weight of each object as well as we know the value of each object our goal is to select objects  such that we do not overflow our bag so  we do not put too much weight in our bag  which might break it but  subject to this constraint we should put in as much value as possible  that is the goal so  i want to explain how they will do branch and bound  how we can implement the branch and boundary heuristics for this but before that i would like to start with the backtrack search  how backtrack search will work on this and like the last problem where  there was a natural cost  the natural objective function over here is a different function so  the natural function over here is a benefit function so  our goal is naturally expressed as maximize total value so  this is ; however  object ; however  object is naturally expressed we want to pick up objects  such that we get maximum value  subject to the constraints that our weight  the total weight that we pick up is at most c so  how will backtrack search work on this  well we need to define the nation of  we need to develop this idea that when we construct solutions  candidate solutions for this we have to  can we do that as step by step process in which we start of within empty candidate object and we extended little by little so  that eventually we have a complete solution  so our nation so  what we are constructing over here is a selection  selection of objects  which is essentially a subset so  clearly we should think of this as  we start of by looking at the empty subset then the first decision point that we take and that point we need to  we simply decide  do we take the first object or do we not take the first object let us make a firm decision so  either we take the first object or we reject the first object so  say let us say we decide to take the first object  then the subset that we have selected so far is consist of 1 and the subset that we have not selected so far consist of 2 through n so  let me here there is an additional number  which is 1 to n so  the pointers that our search let me just write this separate  this is beginning fill up to small  refer slide time  32  28  so  initially our search can be characterized by writing down  what objects we have selected and what are the objects  which are yet to be selected so  initially we have selected no object what so ever this is the set of object  about which we have not made a decision so  this is the starting point  now the idea is that we are going to make decisions about objects we will either decide to take an object or either will reject an object in the first decision point  we will make the decision about the first object what could the decision be  well there are into two choices  we will either pick that object or not pick that object so  if we decide to not pick that object we will get this state so  we will not have picked any object whatsoever but the set of objects about which we have to yet to make a decision or going to be 2 to n this time so  here we have said that we are definitely  not interested in the first object why are we not interested  well there is no real reason for it  this is just one of the possibilities that we are considering remember the back track search involves  exploring all possible decisions so  this is just an exploration of this possibility in which  we do not take the first object  the other possibility is that we in fact  include the first object so  here will have 1 and those objects about which we have yet to make a decision  will appear over here of course  we will not be building this set immediately  we will first just make a decision about this and will come to this point then what will be do  then will we will make a decision about the second object again there are two possibilities and therefore  we could get something like this  again may be we decide no  we are not interested in the second object either so  we will get 3 to n over here or we could decide that we are  in fact interested in the second object this time so  we will get something like 3 and 3 to m and similarly over here so  in this manner  if we proceed we will get to two to the leaves  which will correspond to all possible ways of taking subsets of n objects and then at that point  we could evaluate our benefit function and then we would have to keep track up of the best possible benefit function at the best possible completely generated subset with what its benefit is going back to our definition ; however  we said that in order to apply a branch and bound  we need to write down a proper cost function   refer time  35  54   and further more the cost function must have this property  of course we could do the same reasoning with the benefit function as well that is indeed possible but i am going to take slightly different rule  i am going to keep our definition of branch and bound the same  that is we will vary about cost function rather than benefits function and now i am going to express the knapsack problem  in terms of a benefit functional rather than a cost function  refer slide time  36  26  so  our original function is maximize value of selected objects is there a natural cost function  which we can minimize i could simply do the negative of this  but that negative is not really very interesting  it does not really give us any insight into the problem instead of that  why not ask for minimizing the value of rejected objects i claim that these two are identical so  if you give you a subset  in which the value of the selected objects is as large as possible  i claim that the value of the rejected objects must also be as small as possible this is simply waste of the observation that the total value is fixed and therefore  if you select large value then you are rejecting the small value so  this is the key to applying branch and bound to this problem so  now we are going to think of this knapsack problem  not in terms of maximizing the value of the selected objects  but in terms of minimizing the values of rejected objects in fact  let us take a numerical example  which will make this idea completely clear  refer slide time  38  13  so  our numerical example is that our array  this is our array v  so v consists of value 20 3 7 and 5 let us say and w consists of say 5 2 4 and 1 so  the third object has value 7 and weight 4 and let us says our capacity is equal to 8 what will be the branch and bound tree for this looks like so  originally we will start with no selection so  the selected set is this  the set of out which we are to make a decision consists of all four objects 1 2 3 4 then we make a decision about the first object so  let us say that is n says that the first object is not to be taken so  now we still have to make a decision about 2 3 4  over here let us say we do make in fact  let me change my idea little bit  let us say here we make the decision that the first object is to be taken so  here the decision could be that the first object is not taken and 2 3 4 we have again really made up our mind yet so  if we take the first object  we will have used up v 5  so now we need to make a decision about the next object so  let us say all the left going paths are the once in which we are greedy  we are keep taking those objects so  what would happen over here  so we will take the second object as well so  our bag would contain or our knapsack would contain both 1 and 2 and we will not yet have made a decision about 3 and 4 and then at this point  we could either decide to take third object or we could check the constraints  remember do not have to be always we checked at the end  we could make the check earlier as well so  if we decide to check the constraints earlier  if we have already taken the first two objects then taking in the third object would make the weight be 11  which is bigger than 8 and therefore  there is no question of taking in the third object so  we could say we could definitely go down to 1 2 and only 4 over here so  we have rejected object three so  from this  we will go down to leaf which is 1 2 and 4  this is an acceptable leaf because 1 2 and 4 are these objects which have a weights 5 2 and 1  which adapt to eight  server capacity is not being violated and at this point  i am putting an empty set over here  indicating that all our decisions have been we made  we have made decisions about all the object so  we have come to leaf and we have found the solution and what its value  well this is an object which we took  this is an object that we took and this is an object that we took so  the value is 20 plus 3 plus 5  which is 28 so  here found a leaf  we have found a solution with value 28 so  now we can go back and we could explore this  i would like you to focus your attention on what happens  when the search reach us this point over here well this is the benefit  but we let me just remind you that we said that we are not going to worry about the benefit  we are going to worry about the cost so  what is the cost over here remember that the cost is nothing  but the value of the object  which were rejected so  what is the value of the object which is rejected  well the object which was rejected is the third object and so the cost over here is equal to 7  this is the benefit  which is interesting of course  because in the end we want to report the benefit but the cost of this solution is 7  in the 7 that we defined  the cost as the value of the object that was rejected so  if we want to execute branch and bound on this  what is going to happen we are going to evaluate the cost at each particular point  in this whole exploration so  when we come to this point  we will also be evaluating the cost so  now i would like you to tell me  what the cost of this partial solution is at this point  the object that we have firmly rejected is the first object  because that is the decision that we made  so what is the cost of this so  the cost of this is going to be the cost of the object which we have definitely rejected  all objects which we have definitely rejected  so far we may reject further objects in the future  but that we are not going to cover so  the cost of the object that we have definitely rejected  so far is 20 so  now we can conclude that no further exploration underneath this  are really interesting because as we go down  we will only reject more objects we will include more objects as well  but remember that right now we are focusing on the object which will reject so  in that sense if we think of the cost  the cost of this is only going to increase if at all  it is not going to decrease and therefore  whatever leaf we reach underneath this is going to have cost at least 20 if it cost at least 20  then what is the maximum benefit that we can ; however  here well the total value is going to be 20 plus 3 plus 7 plus 5 which is 35 so  the total benefit  the total value is 35  we have already rejected an object of cost 20 so  when we come down to over here  our benefit is going to be at most 15 the interesting thing is that we know that even without seeing all possible subsets and that is what makes the method powerful so  at this point itself  we can reject the entire sub tree and we can say forgive it  go back and this has to be the best cost subset whereas  backtrack search could  in fact has search everything because it is not keeping track of what is the best and it is not putting  the bound on how much  how good as a solution we can get so  again even in this case branch and bound will work quite well  refer slide time  45  41  i want to give an exercise so  basically if you want to use the branch and bound heuristic  you need to come up with good ways of constructing these cost functions so  we already saw two problems over which  we constructed cost functions in one case the cost function was fairly natural in another case we have to do a little bit  we have to be slightly cleaver in order to construct the cost function so  let me give you a variation of the tsp problem say let us call this the geographical tsp problem  the euclidean tsp problem in the euclidean tsp problem  what we know well think of this as a euclidean or the geographical tsp problem  which means that  in fact we think of vertices as towns and edges as roads correcting them so  suppose in addition to knowing the road distances  we also know the latitudes and longitudes or the x y coordinates of the town is themselves so  then there is a notion of the shortest distance or the straight line distance or which is often called the distance as the crow flies so  this distance is definitely going to be a lower bound  on the length of any road correcting  because a road code wind does much if it goes through other towns it might even reverse  but the road could wind and therefore  there is going to be this direct straight line distance which is going to prove a lower bound  which is going to give a lower bound on the distance possible so  in addition to the input that you already have  so the input in this case is this distance matrix plus all possible straight line distances so  using this i would like you to construct a cost function  which will be better than the cost function which we have constructed earlier what does better mean  that if i give it partially constructed tour  it should give me a value which is bigger than the value which i had earlier now  why could that mean so  that would have to be because  it is somehow has to take into account that i have committed to using these edges but  i still have to visit all these towns and if i want to visit these towns  i would at least have to have some amount of distance added to the distance that i have calculated so far  so for doing this  the straight line distance information will come   refer time  49  12    so  this should be an exercise i will like it to try so  let me summarize again  just i have summarize this so  let me write down in the final word  refer slide time  49  28  so  there is some cleverness needed in constructing cost functions that is an important idea  but once the cost functions are constructed  usually substantial pruning happens so  let me look ahead a little bit well  actually know let me look back little bit  refer slide time  50  23  so  there are number of issues  which you have not talked about in both branch and bound and backtrack so  the first issue that we have not talked about is  how to organize the search space  what i mean by this is let me come back to this example    refer time  50  51    here we said that will look at the first object first  that is what necessary  we could have used some other heuristic of deciding which object to look at first so  this initial that   refer time  51  05   we do might have great pay offs in the end so  maybe we could say that we want to look at the object  which has the highest value first or which has the highest value to weight ratio first so  at a given node  which edge to select first to explore first again coming back to this   refer time  51  46   at this given node  here we select at the edge in which we included the object here we did not include the object in this case  we explore this edge first at not this edge and you saw that was the more interesting thing to do because  we got a good object with good cost and as a result of which could prove in this if we have gone in different direction  then maybe we did not have what this pruning effect so  clearly how we which has to explore first is an interesting decision so  there are additional heuristics possible for all these questions  both these questions there are lots of works and there are internet articles as well  these days which will tell you about these things and which will tell you  how the different heuristics works for different kinds of problems for our next lecture  we will take a complete different viewpoint  refer slide time  53  12  and we look at dynamic programming  which is an even more interesting way of doing combinatorial search and optimization and this stop here design and analysis of algorithms prof abhiram ranade department of computer science engineering indian institute of technology  bombay lecture  18 dynamic programming welcome to the course on design and analysis of algorithms our topic today is dynamic programming dynamic programming is a powerful technique for designing algorithms  and it actually finds applications in many areas  refer slide time  01  10  the name itself is not really very  very enlightening so  i will not talk about it  there is an origin is somewhat obscure and so let us not worry about it but  as i said the techniques finds applications in many areas  including operations research  signal processing  computational biology  geometry and many more we are going to view  dynamic programming as a technique for combinatorial optimization and in some sense  will think of dynamic programming as an optimization of the backtrack search technique that we have seen so far  refer slide time  02  29  so  we will think of dynamic programming  as in some sense are optimization of backtrack search so  here is what i am going to do today i am going to talk about  i am going to take an example to illustrate the technique and this example is going to be our familiar knapsack problem so  i am going to review the backtrack search solution  then i am going to say how we can really   refer time  03  03   slightly differently and how we can optimize it and that is how we will lead into the dynamic programming idea and then i will describe some details and then summarize so  let me take an example of an knapsack problem so  let us say our knapsack problem  let me remind you that our knapsack problem involves filling a knapsack with objects of the maximum value the inputs to this problem are two vectors  refer slide time  03  55  let us say v  v is one of the vectors  which gives the value of each object so  let us say we have v value is which are 7 2 1 6and 12 this juts mean that the first object has value 7  the second object has value 2  the third object has value 1  fourth object is value 6 and so on second thing of the value has been given to us a rupees or something like that  then we are also given another vector so  this vector gives the weight of each object so  for example  we might have that the first object weights 3 kilograms  the second object based 1 kilogram  the third object weights 2 kilograms the fourth object weights 4 kilograms and say the last object is 6 kilograms and finally  we are given a knapsack and it is capacity so  this is our last parameter c and say this time c is given to be 10 so  let me remind you what the problem is  we are suppose to pick up objects from the set  such that the total weight is at most 10 but  we want to pick up the objects of maximum value so  this is the problem let me now remind you  how we did backtrack search on this so  we become the backtrack search  by saying that at present  we have all possible objects in front of this and we have not picked up any object so  that is corresponds to the first node in our search so  let me draw that node out here so  this first brace says that we have not really made any decision yet  on any object we have not picked up any object yet and the second set 1 2 3 4 5 gives us the list or this is the set of objects  which i still need to make a decision about so  this first vertex just says that i have to make a decision about all objects and backtrack search  systematically goes through the entire search space and determines the value  when different objects are picked or not picked the first decision point is where we decide  whether or not to pick up the first object on this side  i am going to consider the case that the first object  in fact is not picked so  for example  i will write here  nothing has been picked yet but to indicate the fact that i have made a decision about the first object  i will remove the first object from the set of objects  which about which i have to make a decision so  i will just write 2 3 4 5 over here on this side  i will make the other choice which is  i will pick up 1 and then on this side i will include 2 3 4 5 this means  that i have decided to pick up the first object and these are the objects about which  ith have to make a decision in a similar manner  i will keep on examining each of these nodes  i will make more and more decisions and i will systematically evaluate all the possibilities so  for example  on this side i could have said that  so next decision is about object 2 so  maybe i will on this side  i will not pick up object 2 and this side maybe i will pick up object 2 so  now the set of object that remains unpicked is 3 4 5 or the set of object  which remains undecided is 3 4 5 on this side  similarly let us say i go to the left and as i have said we have by convention decided that on the left side  i will not picking up the corresponding objects so  again the objects that i have selected are only one whereas  i just made a decision about object 2 and i said  i am not going to pick it up so  the objects that are remaining are 3 4 and 5 and so on i will explore the tree in this manner and we remark long ago that if we do this  we will generate a tree with height with total number of leaves 2 to the n if n denotes the number of objects that clearly is two large at time for even for moderately large n so  we clearly need a better algorithm than this now  i am going to take a different view of this backtrack search procedure that i just described let me first remind you  what the backtrack search view is ?  refer slide time  08  53  so  the backtrack search view of a vertex in this tree  corresponds to a partially constructed solution so  for example  i wrote 2 and 3  4  5  this just says that i have included 2 in a solution i have decided not to include object 1 and this is the part about which  i have not yet made any decisions but  here is an alternate view  suppose i have decided to include 2 so  let me go back to my old problem so  if i decide to include 2 and not 1   refer time  09  50   and i have capacity 10 then ; that means  that out of the capacity that i had of 10  i have used up capacity equal to 1 because the second object has weight 1 so that means  corresponding to this vertex so  this choice of mine really says that our new capacity after i have decided to include object 2 is 10 minus weight of the second object in this case  10 minus the weight of second object happens to be 1 and therefore  the new capacity happens to be 9 and then  i have not yet made decisions about objects 3  4  5 so  rather than thinking about this remaining part  as an extension of the old solution  this new view encourages us to think let us forget our original capacity let us say deal with what really is remaining what really is remaining is 9 so  we have capacity of 9  which is remaining and we have to select the 3  4  5  these objects 3  4  5 so  we are allowed  now to take objects from this set 3  4  5 our capacity is 9 ; that means  the weight of objects  which is select from this must not go beyond 9 and within that  we want to maximize our value clearly solving this problem  extending the solution over here is exactly the same as this  as solving this independent problem that is because even here  i will have used up 1 unit of capacity  out of the 10 units that i have already had and clearly  it makes sense for meet to fill up the remaining capacity with the best  the most valuable objects from this set whose total weight is less than 9 and that is exactly what happens here as well so  this is ; however  a substantial improvement because  now our view is that  we are not really thinking of about extending the solution but we are thinking of solving and independent problem and further that problem is of the same kind as the original problem so  rather than saying that let us extend the solution  we will just say that look in order solves this original problem let us solve this alternate problem  but this problem is not an extension  of any extension of any solution but  it is a simple new problem of the same kind so  this first of all introduce as simplification in our outlook and in some sense it makes programming easier  so the benefits of this new view  refer slide time  13  15  the first benefit is that  this new view is simpler or i would state that as the outlook  our outlook overall is simpler this in term such as our programming is going to be easier i will talk more about this in a minute and you will see that indeed  the programming gets simplified however these two are not really  the main important benefits there is an additional benefit  which is much more important  but about which i am not going to tell you right now let us wait  until i tell you of how to do the programming and then i will tell you  about this surprise benefit and you will agree with me then  that this is really a very exiting benefit so  let us start  let us worry about  how we going to program this so  the idea was that in order to solve the knapsack problem instead of thinking about extending the solution  we said let us construct a new problem of the same kind and then solve that maybe  one or as you have seen maybe  more problems of the same kind and solve that so  this naturally means that we should be thinking of recursion so  let me write down a recursive procedure  to solve this knapsack problem so  let me first define that procedure  let me write down this specifications of the procedure first  refer slide time  15  18  so  i will call my procedure ks and it will take two parameters c and i c is going to be the capacity  remaining or actually i should say rather  than remaining that the capacity given for solving the problem so  let me just strike this off  capacity for containing solution i is a parameter  which is the index of the first undecided object or in fact  instead of thinking about the first thinking of it as a first undecided object  i could think of it as the first object  which is allowed so  the object which are allowed for me ri  i plus 1  i plus 2 all the way till n so  my original problem that i needed to solve was say something like ks of 10  1 what does it indicate that i want to solve the knapsack problem  on knapsack of capacity 10 and i want all the objects  in the first object onward until n to be considered i have told you what n is and let us assume that n is a global variable so  there are some global variables over here so  n is a global variable  the total number of objects  similarly the value array is a global variable and the weight array is also a global variable and let me just remind you  that there are n components in each of these arrays so  these are the specifications of the procedure that i am going to write so  let me now write the procedure so  let us deal with the base case first that is fairly easy  refer slide time  17  49  ks of c i  so first the base case  so if i is greater than n so  that means that i have been asked to solve the problem  starting from the n plus first object or some object beyond n that means i really have made up my mind about all the previous object   refer time  18  19   or in essentially what is am saying is that  i am not allow to choose any objects at all in that case we should be return in 0  as the value of the solution i should say before i get into this  that in this problem that this specification that i have wrote down   refer time  18  42   i should also say that we will only we worried about the value of the optimal solution so  the idea is that case of 10  1 will not actually return  the set of object which are chosen but  in fact it will return the best possible value that i can get out of it now   refer time  19  14  i think this is a  refer time  19  15  simplification of the problem that is given to us and in some sense it is  but later on you will see that in fact  once we know what the value is  it is very easy to go back over our calculations and actually figure out what that set was so  we will leave that for the future  later in this lecture right now we will just concentrate on determining  what is the maximum value i can get by filling my knapsack  so that it does not overflow  it is in overloaded this is the base case  if i have already gone past n then ; that means  we have considered all the possible objects and then we should returning 0 because there is nothing to add to our knapsack  so you have already return otherwise  the first object that we are going to be considering is the ith object so  what can you do about it  well we need to consider it only if the capacity is in fact  able to take that object so  if c  if the capacity happens to be less than the weight of the object  then we can not do anything right so  if the capacity happens to be is less than the weight of the object then that object can not be considered  which means  we should just go on to the next object and return whatever we can with the remaining object so  in that case  we are going to return ks of c and i plus 1 the interesting case appears  if c is in fact  bigger than w of i  so else what happens so  else we have the possibility that we can in fact  include this object in our knapsack of course  it might not always be wise to include that object simply because  if we include that object we have to reduce  we have to use of some of your capacity and maybe the benefit that we get because of using of that capacity is not adequate enough or because we might be losing some better options later on so  therefore  to cover both possibilities  here is what we should do we should return the best of those towards so  the first is we will ignore this current object  in which case we should be returning ks of c  i plus 1 and then we need to consider the other possibility  which is that we include this ith object but  if we do include the ith object there are two effects first of we gets it value so  we should be returning in whatever we return  we should improve that value as well so  we are going to return v of i  but will also get some value form the remaining objects however  when we go for the remaining objects we would not have the full capacity c to consider but  instead will have a capacity  which is a little bit reduced so  we should we should be returning  v i plus ks of c minus width of i  because this is the amount by which the capacity series is going to be reduced and whatever choice we can make from i plus 1 from the remaining objects so  this is what we should return  the best of this quantity and this quantity so  this finishes the recursive implementation of the backtrack search that i have just mention just make sure  that we understand this let us actually execute this  on a sample on our own example  in fact  refer slide time  23  49  so  let me write down what our vs and ws are so  our vs and ws were v was 7 2 1 6 12 w was 3 1 2 4 6 and our capacity was 10 we started of by making the first call  which was ks of 10  1 ks of 10  1  now let us go back to our code and our value of n was equal to 5 so  if we go back to our code and ask how this code could execute  the first step we have to check is i bigger than n n is 5 and i is 1 so  clearly it does not happen and so  we come to situation so  the weight of the first object is 3  refer time  24  46  and the capacity is certainly bigger than that so  this is not case that we execute  but in state we execute this case in that case  we are going to consider two possibilities one is searching the knapsack possibilities  with the same capacity and i plus 1 so  let me draw this as a recursion tree so  here i am going to get ks of 10  2 and of this side i am going to get whatever is whatever corresponds to this   refer time  25  19   but  corresponds to this we are going to use up  out of the c capacity we are going to use a w i capacity so  w of 1 capacity is going to get used up the w of 1 capacity is 3 units so  on this side i am to i am going to make a call ks with the reaming capacity  which is 7  2 because  now i am going to only consider objects starting at the second object in this manner  we are going to execute so  let me do a few an executions  just to make sure that the idea is understood what happens  when we try ks of 10  2 again even here  we will see that   refer time  25  57   the weight of the second object is 1 and that is still smaller than the capacity and therefore  we will use this part of the statement so  the first thing will execute is ks of c  i plus 1  which is with the same capacity will try to go for the remaining object so  will be executing ks of 10  3 and on this side  we will be executing recursive call corresponding to this call over here   refer time  26  33   w of 2 now is this so  will have to decrement that 1 from that and therefore  the call that we execute over here will be ks of 9  3 let us do one more of these calls so  let us see what happens when case of 9  3 is executed further so  again if you go back to this procedure  you will see that even here the capacity is larger than this w i  w of 3 w of 3 is 2 and it is still larger and therefore  this will get executed  refer time  27  09  so  again there will be two children  two recursive calls so  the first one will simply be ks of 9  4 and on this side  the call would be ks of c minus w i so  c minus w i this time is this so  it is going to be 7  4 let us do something on this side and so  what happens over here so  case of 7  2 if we execute we are going to get something like this so  first we will execute ks of 7  3 this corresponds to the possibility that we do not choose the third object they will of course  be corresponding exploration on this side where  we do choose the third object but  even here there will be two possibilities ks of 7  4 and on this side the possibility that we choose the third object so  whatever that is at this point i would like you to do two things  first of all i like to make sure that this picture is understood what we have drawn in this picture is  what is popularly called recursion tree so  this is the first recursive called we need  that call give raise to this recursive call and this recursive call  that in terms give raise to these two recursive calls just will give raise to this call and some other call over here and so on but that is of course  quite routine  the most interesting parts of this picture are these two calls this should really write as ks so  there is something very interesting about these two calls they are identical what is it mean  in this part of the tree i am going to make a procedure call with parameter 7 and 4 and i am also going to make procedure call this side  also with parameters 7 and 4 so  this is the beauty so  this is where the optimization can come in once i explore this search tree underneath this  i won not need to explore this search tree again if i store the value  that i get from this call  then when i come to this portion if i just need to look it up  i will just look it up and i will get it in fact  that is going to be my default idea whenever i calculate the value of a certain recursive call  i will actually stored it in a table before embarking on any recursive call  i will first check if the table already contains are value  if it does i will just use that value if it does not then i will calculate that value but at the end of it i will stored it in the table that is basically the idea that is basically the optimization that i was talking about and you can see that by viewing  what remains to be done rather than thinking of this search tree as an exploration  in which you are extending solutions it is in this new view of things ; it is possible to determine that the work over here is the same as the work over here and thereby  we can do this optimization that we that i just mention so  let me let me now flush out this optimization for you so  we will go back to the same code and now i will write down  what is the corresponding code with the optimization so  let me write that down in different color so  if i greater than n then return 0 is the idea over here so  we said that before returning any value  we are going to store it in some table  refer slide time  31  29  so  instead of that instead of just returning 0  this part of the code will be replaced by this part so  we are going to still check  if i is greater than n but if it is then we are going to we are going to set table of c  i equal to 0 and then we are going to return table of c  i so  this is what  this statement is going to be replaced by in our optimized version let me just remind you that  that the idea is to remember value is before we calculate them  refer slide time  32  21  so  we are going to have another global array called table and table is going to have  this is going to be a two dimensional array so  we are going to have  the first index correspond to all possible capacities and the second index correspond to all i ? s  all the possible values of i so  this is going to be  i is going to be from 1 to well n and as we saw our base case actually takes at beyond n so  it is going to be n plus 1 so  the second dimension is going to be n plus 1  c is going to be in the range 0 through whatever value  so the capacity given so  in fact  let us say this is little c and so  little c 0 is less than or equal to little c  less than or equal to capital c so  the first index of the first dimension will have this range  this is the two dimensional array in which  we are going to store our values what about this if c is less than w i return ks of c i plus1 we just define the rule and we are just going to follow it so  instead of returning case of c i plus 1  we are going to first check so  the way we check  i will just write down the code corresponding to this so  we are going to check  if table of c  i plus 1 is not calculated and that i will denote by null then we are going to actually calculated so  will calculate table of c  i plus 1 is equal to ks of c  i plus 1 and then we will return table of c  i plus 1 so  this is going to be  what this code is will replace this code over here similarly  we will have code  which will replace this as well so  corresponding to this we will just make a check in c i plus 1 here so  this part will really with the same as this and instead of this  will make a check in a slightly different position so  let me write that down as well so  let me write that down separately over here so  return   refer time  35  19   of c i plus 1 is going to be done by first checking whether case of c i plus 1 has been calculated  refer slide time  35  28  so  that is as good as saying if table of so  return this expression is going to be done by checking this so  we are going to check if table of c i plus 1 is equal to null so  in that case we are going to calculated so  we are going to set table of c i plus 1 equal to ks of c i plus 1 then we are going to check whether this has been calculated as well    refer time  36  15   so  that is as could as saying  if table of c minus w i  i plus 1 is equal to null then we actually calculated  table of c minus w i  i plus 1 is equal to ks of c i plus 1 and then finally  we will just return the maximum of these two quantities   refer time  36  54   and that is done simply by saying  return max of table of c  i plus 1.and v of i plus table of c minus w i  i plus 1 so  this code will replace this last code in your program so  should be clear to you  that this new code that we have return because  it is going to do less work because some parts of the search tree  which were explored several times in the original code will now be explored exactly once but  now you would like to prove by how much precisely the work gets reduced  refer slide time  37  56  so  we come to the analysis normally when you write recursive algorithms  the idea is that will write to recurrence for the time taken and we will saw  refer time  38  07   in the case of dynamic programming and in particular  where we stored these values in a table and use them as needed  this recursive estimate is not going to be very good it is going to be an over estimate so  we need something much more precise  we can produce an estimate ; we can produce a way of doing that estimate  which gives us much sharper bounds so  here is the idea so  think of this way as the computer executes each line of the code suppose  it writes the diary  in the diary it is going to write the following things so  i am going to call this diary  it is customary to call this diary a transcript so  this is going to be a transcript of the execution so  the computer is going to write down  the line number in the program then it is going to write down  the value of c that is the correctly it has and then the value of i that it currently it has ? it is going to write down a triple  like this so  it will write down once such triple every time it executes a line in the program  where we did in this this will become clear in just a minute  if the idea is clear so  let me take an example so  here is my code so  let us say we have number the lines in this code  some going to for simplicity i am going to think of this has been my code or actually i could do this as well   refer time  40  06   so  this is my this is line number 1  this is line number 2  this is line number 2 and so on so  give numbers to every line in this program and i have added some lines over here so  i will give numbers these lines as well  maybe this is line number 14   refer time  40  22  this is line number 15  this is line number 16 and so on so  i have given numbers to everything whatever  this is line number 20 so  when the programs starts executing  i am going to be making the call ks of c i so  what will the computer write the first time around so  this is going to be my line number 1 whatever  this is going to be my number 1  this is going to be line number 2 and so on  refer slide time  40  54  so  my transcript of my program is going to be the line number  which is 1  what will be the value of cb  the first time the value of c is 10 and the value of i is 1 so  this will be the first entry in my transcript the next entry in my transcript is  i am going to execute the next line of the program 2  10 and 1 as i keep writing at some points  things will change  of course the line number will change almost every time but  at some points this 10 will also change  the 10 will change according to my execution tree  which i drawn over here so  in this execution tree as i started up with ks 10 1  but then i went on to ks 10 2 so  at some point during this execution  i am going to get again the 1  10  2 so  this corresponds to the first recursive call and similarly  the different recursive calls will come out over here so  this is going to be my transcript how will you lines will there be in this transcript how many triples will there be in this transcript  clearly number of triples is exactly equal to the number of time steps because at every step  the every step the computer takes  it is going to make one entry into its diary or it is transcript and that is going to be triples so  the number of triples is exactly equal to the number of time steps so  if i want to estimate the time taken by this computer on this particular problem  all i need to do is to count how many triples  i have in my diary or in this transcript you might now wonder  what is the big deal we want to estimate time  now we want to estimate triples there is actually rather interesting property to do with these triples  refer slide time  43  08  and this idea is  that every triple in this transcript must be different transcript will have to be different so  what i am cleaning is that if this is the transcript that my computer wrote  then subsequently i am not ever going to see the entry 1  10  2 again so  this is never going to happen  why is that the answer to that comes in the changes that we made so  if some entry reappears  then that means the computer is executing that same statement  whatever the statement is with the same two previous parameters  same two values of c and i but then ; that means  the ks must have been called with exactly those two values c and i again twice at least but  that was precisely the point of the changes that we made so  we said that before we make a call to ks  the computer actually checks did i already execute this call  if i did execute the call  i am just going to pick up that value from my table and therefore  we know that the computer never makes a called to ks  twice with the same values so  that from that it follows  the every triple in the transcript will have to be different that gives us a good way to estimate the total length of the transcript we just count  how many different triples can there be so  let us go back to the place where  we wrote down whatever triples were so  this is our definition of triples so  since every triple has to be different    refer time  45  12     refer slide time  45  13  the total length of the transcript will be  at most the number of different triples that are possible so  the number of different triples that are possible is simply  the number of lines in the program  times the maximum value of the capacity times  the different values that i can take the different values  that i can take n the number of objects so  this is the number of entries that can there be in the transcript so  let us complete our example  refer slide time  46  16  so  in our case the number of lines in the program  we just said was say something like 20 times the capacity we said was in 10 and n was 5  well actually we shall said n plus 1 because  we allowed the program  the procedure to be called not with just the number of objects  but one beyond the number of objects as well  so in that case is 6 so  then we can estimate that this has to me 60 times 2 equals 1200 so  our program will require 1200 steps of execution in general  the number of lines in the program  does not depend on the capacity that is given to you nor to the number of objects and therefore  i can write this as o of 1 times c times n or the total time taken is simply o of n times c  which is much  much less than 2 to the n  which was what we would get with backtrack search and this is what  we get with dynamic programming so  let me so  let me now summarize the main ideas in all this  the main ideas in all this are 2 3  refer slide time  47  44  the first idea is so  let me just review this this is the review ; the main ideas are first think about whether the optimization problem can be expressed as a sequence of decisions this is something that we need even in order to do backtrack search but  beyond that in dynamic programming we do something more  refer slide time  48  46  what we do more in dynamic programming is that rather than think of extending solution so  do not extending solution  but solving smaller problem of the same kind  that is the important dynamic programming step  dynamic programming idea let me see  this is the first dynamic programming idea and in the second dynamic programming idea is  check if the same problem is being solved again if so  keep a table and save time so  you keep a table and we save time by not repeating that calculation so  these are three ideas in dynamic programming  well the first idea was really similar to was really common to backtrack search  but these are the two new important ideas  refer slide time  50  24  and then there is also a fourth idea which is for the analysis the fourth idea says that do not use the recurrences  recurrence relation to estimate time but  instead use this transcript idea so  these are the important ideas so  let me say a little bit about what we are going to talk about in the next lecture in the next lecture  we are going to use a slightly different formulation of what we have seen so far and that will actually  end up eliminating the recursion and in some ways  it will simplify further our view of this whole procedure and then we will also use dynamic programming  on some other problems on these other problems  the expression and our program will become a little bit more complicated but the basic idea as you will see will remain more or less the same thank you design & analysis of algorithms prof abhiram ranade department of computer science engineering indian institute technology  bombay lecture  19 longest common subsequence welcome to another lecture on design and analysis of algorithms today ? s lecture will also be on dynamic programming the topic for today is the longest common subsequence let me  start by defining the problem  refer slide time  01  09  so  the longest common subsequent will abbreviated as lcs the input to this problem consists of two strings so  there is one string a of length m  and another string b of length n i might string occasionally  but i do means sequence  it does not really matter the sequence will be a sequence of characters typically so  i will use string and sequence interchange again the output is also going to be a string  let me call it c and say it will have some length p  which is as it unknown and we required the c have the following property so  c should be a subsequence of a as well as b that is what a common subsequences i will define subsequence  more formally in a minute so  c should be a subsequence and we want c to be the longest possible subsequence so  this is your problem let me define what a subsequences  refer slide time  02  34  so  we will see that x is a subsequence of y  if x is obtained by dropping elements of y and of course  the remaining elements  which hidden drop should be kept in the same origin so  for example  a d is a subsequence of say a b c d e so  these are the elements that we dropped  and as we said common subsequences or just sequences of both so  let us take an example of lcs  refer slide time  03  42  so  let us say a is a sequence of characters a b c d e and let us say b is a sequence of characters say b c d or let me say  let me write it as b c e a d and some q they do not have to give the same length so  clearly b c d is a subsequence of both and therefore  it is a common subsequence a d also is a common subsequence so  a d for example  appears over here and over here b c d for example  appears over here and over here so  now we know what common subsequences are we would like to define  but the longest common subsequences so  lcs of a and b for example  is b c d you can look through the sequences and you will figure out  the only common sequences of length 3 are very few and in fact  there is no common sequence of length 4 and so therefore  this is a long common subsequence in fact  you can see that b c e is also a common subsequences so  let me just underlying that  so say b c e appears over here and over here it appears over here so  the goal now is to find one of these common subsequence so  goal in this lcs problem find any longest common subsequence of a and b we do not care which one  but we want one of those when we speak of lcs  we will mean either every lcs or one of the lcs ? s and what we mean should generally  we clear by context otherwise will explicitly   refer time  06  21     refer slide time  06  26  let me give you a   refer time  06  26   motivation about  why we care about this lcs problem the length of the lcs of say a and b can be thought of as representing the similarity between a and b clearly  the longer this lcs ? s the more similar a and b   refer time  06  58    you might ask  why do we care about strings similarity the answers to that are fairly well known and obvious probably so  say we have two dna sequences and we want to know are they very similar if they are very similar maybe there is a common evaluation in those sequences and so therefore  some measure of similarity is useful  suppose we have a miss pelt word we would like to know the word  which is most similar to   refer time  07  38   from our given dictionary again  we need some measure of what similarity is so even here  the nation of an longest common subsequence is useful as it terms out  lcs is not the only measure of similarity  there are others as well but  interestingly enough the algorithms we develop a lcs will we somewhat similar and you can probably extend them for other measures of similarity as well let me  start by considering a brute force algorithm for lcs  refer slide time  08  14  this is probably a good idea before embarking on trying to discover a complicated algorithm it at least tells you something if  not you have something to fall back up on so  what could be the brute force algorithm so  the idea could be to say let us generate all possible subsequences of a this will be the first step then we will check  which are also subsequences of b and then  we retain the longest as we said  there could be several longest  but we do not care we will just keep one of those and that could be the answer that you could print is this algorithm good enough ? well  this is algorithm good  well certainly will written an answer but the question is will it be fast now  it is not going to be very fast  because a number of all possible sequences of a is quite large in fact  you know that if sequence as length m then there are 2 to the m possible subsequences so  therefore  the time for this algorithm is going to be at least 2 to the m in fact  if you want to write down the sequences the subsequences  you will also a multiply this by the by m itself but  any in case that is at least exponential m so  that is not what you want  we would like something faster and in fact  we have said earlier  that we are going to apply dynamic programming for this and our hope is the dynamic programming will give a something better so  let me review  what the dynamic programming ideas are  refer slide time  10  16  so  the first idea was that we should try to express the problem as a search problem by that i mean  that you should clearly identify what the search space is so  what is it  what is a possible candidate that you are looking for ? so  in our case  the search space consists of common subsequences of a and b so  we are going to search through the common subsequences and so  in that sense is first step is over well not quite so  there is also one more thing  which is we want to determine the objective function so  what is the objective function ? so  let us says of all the elements in the sub space  which is the one that you are going to pick so  in our case  that easily stated it is a longest one so  we have a space consisting of all subsequences of a and b and we want from it that string which is the longest of course  we are not going to generate this space explicitly we are not going to ever write down anywhere all the common subsequences that will take too much time this is just for the purposes of thinking so  let me write the term so  this is only help in thinking we are not actually going to generate the subspace any of these spaces then  what we are going to do is we are going to write the recursive procedure listed explicitly what a search space is having defined explicitly  what a search spaces  refer slide time  12  28  the next step is to write a recursive procedure  which looks something like this so  it is going to be a search and it will take as argument the space has which we want to search and now  the procedure typically will divide s into subspaces say  let me call those s 1  s 2 say some s k so  there are k subspaces and the idea is that will find the best in each s i so  by best we mean in the one in the element  which is which maximize as our objective function  which is which optimizes our object function so  in our case  it is going to be the longest subsequent belonging to each of these s i ? s and then  we will return the best of the best by this best i mean  the best computed above here simple enough  now the important point is going to be  how do we find the best in each s i ? this is the recursion is going to come in so  this if we can express recursively will at least have a recursive procedure of course  this is not your dynamic programming stops  refer slide time  14  13  so  the next step is that we are going to characterize the calls the recursive calls so  we are going to ask what are the arguments to the recursive calls ? in fact  to all the recursive calls  which are made during the life time of that recursive procedure so  we are going to define a table then so  let me call it t  which will store the results evaluated for each recursive calls now  here comes an importance which  now we are going to ask that we should write non recursive procedure and this  i am going to call fill  which fills entries of t  assuming other entries are already filled in particular  it fills say entry i j of t assuming all other entries are filled the key time is going to be that  the key point is going to be that as a result of this the total time will be small in fact  right now i can list what the total time is going to be so  total time is going to be size of the table times filling time so  if we can do this with a small enough table and if we can show  that each entry requires will be a small amount of time to be filled then will have a fast algorithm and your hope is that this strategy will give as a fast algorithm for the longest common subsequent problem so  let us see  how to do this in the case of lcs the first step is to define the search space  which we already did the next step   refer time  17  04   is to write this procedure and this requires just divide s into subspaces s 1 to s k so  just do that one  refer slide time  17  17  remind you  s is the space of the common sequences of a and b so  this is what we are going to search and now  we go to partitioning so  let me write down one strategy there can be several strategies and i am just going to write down 1 so  one idea might be  that i will defines i will partition s as s sub a union s sub b union s sub c all the way till s sub z the idea is that s sub a is subspace of s consisting of common subsequences starting with letter a s sub b will be elements of s  which start to the letter b s sub c will be the elements of s  which start to the letter c and so on so  here is one partition now  dynamic programming will require as the idea of dynamic programming will require as to find to write a recursive procedure  which recursively search as s a  s b  sc and so on and then  pick the best of the best of those answers this is the 1 strategy this will give us  better than exponential time solution but  this is not the only possible strategy so  let me give you another strategy as well  refer slide time  19  13  in this strategy  we are going to take s and we are going to partitioning it  only in two subspaces so  let me tell you  what s 1 is going to be so  s 1 is going to be all subsequences  which start with the first letter of a so  remember a is already known to us a and b are known to us and so we can ask questions about what a 1 is so  s 1 is going to be all subsequences  which start with a 1 s 2 is the rest  that is start with something else  some other character now  this is the strategy we are going to develop further but will see that the idea of this development can also be used for the previous strategy so now we need to define a recursive strategy  recursive procedure for searching s 1  refer slide time  20  21  so  our question is how to search s 1 so  let us let me remind you  what is s 1  s 1 consists of all subsequences  which start with a 1 so  when we are looking for something it is usually a good idea to write down explicitly what is it  that we are looking for so  let us say  r equal to longest element of s so  let us say r as p as length p of course  p is not known to us  but let me just define notation p what is to be know about r well we know that r of 1 the first letter of r is the same as the first letter of a not comes from the definition of s 1 we just said that  s 1 only consists of subsequences  which start with a 1 now  r is a subsequence of a as well as a subsequence of p so  which means that r 1 must appear in b as well so  without loss of generality we can assume let us  say that k is the smallest  such that b of k is equal to r 1 or actually it is equal to a 1 now  it may term out  that even does not at all appear in b that is possible  in that case this search this space s 1 will be null so  that is going to be a better of a degenerate case so  for now  for a minute i am going to assume that this does not happen and that k is well defined so  there is some k so  now once we know what this k is  what can we say about  can we give something more  can we say something more about r 1 well  here is what we can say so  we know now that r of 2 to p is a subsequence of a well not of a  because a 1 is already taken up by r 1 so  the rest of a so  2 to m and also of b from k plus 1 onwards so  it is going to be b of k plus 1 to n so  this just comes from the definition of r so  r has to be a common subsequence of a as well as b the first element of r appears at a 1 by definition of s 1 it appears at k  at b k in b and the rest of it also must be a subsequence otherwise  it would not be a common subsequence and therefore  r must be a subsequent of this as well as the subsequence of this so  now having characterize what r is our question is how do we find r usually  terms out that if you characterize something adequately it helps some finding and so  in fact  we are almost ready to find and in fact  we can claim that sense r has to be a subsequence of both perhaps it is a longest such subsequence so  let me write it down as a formal plain  refer slide time  24  13  so  r my claim is r 2 through p is an lcs of a 2 through m and b k plus 1 through n where  k is as we defined a minute ago k is the index of the first occurrence of a 1 inside p terms out that this claim is true and the proof is not terribly hard either the proof if by contradiction so  let us assume that r 2 p is not an lcs of this so  let us say  there is some s is some lcs of a 2 to m and b k plus 1 to n and in fact  for the sake of contradiction we have to argue  we have to assume that s is longer than r 2 to p so  that is length of s is greater than the length of this  the length of this is p minus 1 so  let us say  it is greater than r equal to p now  the interesting thing is that suppose i consider the sequence a 1 concatenated with s can we say anything interesting about this ? well  s is a subsequence of a to m so  this must be a subsequence of a 1 concatenated with a 2 to m but  this is simply a 1 to m similarly  this is also a subsequence of a 1 concatenated with b k plus 1 to n but what is this ? well  this is nothing but even is also b k and therefore  this is b k forward by b k plus 1 to n and therefore  this whole thing is in fact  b k to n so  what have we found then ?  refer slide time  27  07  so  we have found  we have discovered that this s is a common subsequence of a 1 to m and b k to n but  this is itself is a subsequence of b 1 to n so  i could just the right this as 1 to n and also  s starts with let me put it here a starts with a 1 now  what is the length of s s has length at least p so  this really up to be a 1 concatenated with s is a common subsequence of this and in fact  a 1 concatenate with s starts with a 1 so  a 1 concatenated with s has length at least p plus 1 but now  we have a contradiction because  we said that r of length p was an lcs so  then that means  there can not be longer sequence  which is common to both but  which we have just proved has to be there and therefore  our original claim must have been true so  what was our original claim  that r 2 p is an lcs of a to m and b k plus 1 to n so  what is the conclusion from this ? so  let me write it that down so  the conclusion is that we know now  how to search s 1  refer slide time  29  09  so  longest element in s 1 can be found as follows so  1 determine smallest k such that b of k equal to a of 1 second find capital l equals lcs of a of 2 to m  b of k plus 1 to n why did we do this well this ? well this   refer time  30  04   immediately from our claim so    refer time  30  09   r 2 p we claimed is an lcs of this so  we do not know what this is so  we just find it so  we just use recursion for it and finally  return r equal to a of 1 concatenated with this l so  we claim that this we showed  that this has to be along a common sequence of both a and b such that it lies in s 1  and so that is what we have constructed so  we have constructed a procedure for searching s 1  refer slide time  30  53  so  the next thing is to find a procedure for searching s 2.well  as before let us write down what we are searching for so  let us say  we are searching for some r again so  say again of lengths p  which is longest on s 2 can we characterize are further well  we know that r is a subsequence of a 2 to m why 2 to m because we know that a 1 does not appear in it so  there is no question of r being a subsequence of a whole thing so  it has to be 2 to m and r is also subsequence of b 1 to n so  note that over here  we have use a defining property of s 2 so  this is how you have characterized r so  at this point it might be tempt into claim or at least conjecture that perhaps r should be the longest common subsequence of both of these that is the nice conjecture but  unfortunately it is not true i am not going to explain it into much detail but  let me just give you  a hint of why it is not true and then you can prove it for yourself so  just consider a as a sequence of letters a a a a and b say as also the same then  work out what x 1 and x 2 are going to be  and the you see why this is not true so  r is not the longest common subsequence of both of these however  something equally useful is true so  let me write that down so  what is true is the following  refer slide time  32  54  so  i claim that r as defined here is no longer then lcs of a 2 through m and b 1 through n i will tell you  why this is useful in a minute let me  prove it first the proof is actually a single line we know that r is a common sequence of both of these so well  so it can not be longer than lcs of these two sequences  so done so  let me now compare to this conclusion that we have written earlier so  we said that we want to here is a procedure for looking for the longest element in s 1  refer slide time  33  56  so  we also want to procedure for looking for the longest element in s 2 so  let me write that down over here so  what will that procedure look like ? so  i claim  that i am going to define r prime equal to lcs of a of 2 through m  b of 1 through n so  this will be generated by recursive call and this is no smaller  no shorter than longest of s 2 so  what we have  we have a procedure to determine the longest element and s 1  and an element  which is no shorter than the longest of s 2 so  is this useful  so what was our goal so  let us go back to our goal  refer slide time  35  04  our goal was to determine the longest of the longest the longest of s 1 and the longest of s 2 and of course  so long as and we would happy so  long as it is longer itself in s so  this we know  this is our  this is what we just said or and this is r what if i put r prime over here so  this is no shorter and so if i take the larger of this it will certainly be no shorter than all of this and it will also lie in our original space and therefore  this is also  so the longest of s and so if i substitute r prime over here that is fine and therefore  this is nothing but longer of the longest of s 1 and no shorter than longest of s 2 provided this also lies in s so  if i do this i will still be happy and therefore  this conclusion that we have written   refer time  36  27   is good in the sense that  if i compute r prime in this way i just have to return the longer of r prime and r  and that will give may the best answer for the original space itself so  now we are ready to write down our general search procedure so  let me write that down  refer slide time  37  08  so  recursive procedure  which i am going to call lcs it will take as arguments a of 1 through m  and b of 1 through n and now  since we are going to write it as a procedure  let us we careful about boundary conditions so  if a equal to null or b equal to null that is as equal to saying  say these are strings of 0 length so  say first index is bigger than the second index return null so  clearly the longest common sequence of 2 null sequence or 1 null and something else is null sequence now  now we have to worry about the main case so  let us start by doing that so  we are first going to see how s 1 is going to be searched so  k is equal to smallest such that b of k is equal to a of 1 remember  we are searching s 1 and that is what we did if k equal to null as here or k is undefined that is my short form for that is k equal to null is a short for saying that then  again will return null otherwise this will   refer time  38  37    so  we are going to do all this so  we are going to write or i will just write it jointly so  we are going to compute r equals all of this so  let me write that down otherwise  will compute r equals a of 1 concatenated with lcs of a of 2 through m and b of k plus 1 through n  then they will compute r prime now  this time we are searching s 2 so  that as lcs of a of 2 through m and b of 1 through n and then we will return the longer of r and r prime so  we will get r and r prime explicitly will compute there lengths and will return the longer of the two now  i should point out some technicalities so  in this call we wrote down a of 2 through m the moment we write down a of 2 through m  we are implicitly assuming that m is bigger than or equal to 2  which may not be true m might be equal to 1 in this case  i am going to assume that an expression like this will be evaluated as null so  instead of passing  so this expression will be passed as null if m is in fact  smaller than 2 so  what will happen then  when this comes over here in the recursive call and null will be return but  which is exactly what we want over here and so therefore  this interpretation is fine so  i do not have to write that is special case  whether m is greater than or equal to 2 it is just handle by saying that  this just represents null strings or null sequence so  this is our basic recursive procedure now  one possibility might be to write down the recurrence to estimate the time taken for this and i will let you do that and you will see  that the time taken is actually quite large it will be exponential however  that is not where we stop dynamic programming require first go further and that is that next step is in fact  the one which is going to improve the time for us so  what this dynamic programming require so  dynamic programming as such to characterize what recursive calls happen in this so  let us look at that so  first the recursive call is this after that  there are two recursive calls so  there is a recursive call over here and then there is a recursive call over here so  is there any nice property  which we know for this recursive calls well  if i look at a the argument pass for first string it is always going to be some part of a and in fact  the second index is always going to be m similarly  if i look at the second argument to the recursive call it is always going to be b the second argument is always going to be n the terminal index in that array is always going to be n in the first index could be anything  it could be k plus 1 and if you recurs again  this property is going to stay they say so  we are  so when we recurs the key idea is that when we recurs our calls are going to be something like this  refer slide time  42  39  so  all calls  which are going to be made are going to have the form lcs of a of i to m  b of j to n where  i and j could be anything so  this is an important observation and this is an important part of the whole dynamic programming strategy so  once we know that all calls are of these kinds so  let me actually note that down  refer time  43  10   going to be a this kind so  let me write this term say  where 1 is less than or equal to i is less than or equal to m and 1 is less than or equal to j is less than or equal to n dynamic programming says let us define a table let us called t such that it is entry is will save the solution answers to these calls so  t i j will store lcs of a i to m  b j to n so  actually i am going to use not i ranging over just 1 through m  but over 1 through m plus 1 and j will arrange over 1 through n plus 1 and the reason for that will become clear in a minute it will serve as a certain kind of   refer time  44  18   so  this will be two dimensional table with m plus 1 rows  and m plus 1 columns and now our goal is simply to fill in the entries of this table so  once we filling the entries of the table t of 1,1 will be lcs of a 1 through m and b 1 through m and that is the solution we are looking for so  if we can generate the procedure for filling these entries and it has to be a non recursive procedure then we will be done so  long as we say  in which order the entries are going to be filled so  i claim that from this procedure of ours  there is a fairly nice way of filling in the entries so  why is that ? so  let me define fill i j as follows  refer slide time  45  20  so  what is will i j going to do it is going to fill the entry t i j but  what is t i j t i j is going to be the lcs let me write that term so  it is going to be lcs of a i through m  b j through n so  let me comment it so  this is what the entries suppose to be the fill is suppose to written this entry now  if we look at our original procedure then this is after all a string and this is another string and we could just as will pass this string or these two subsequences so  all i have to do is to mimic what happens in this procedure and see  how fill should really behave so  the first step is  if a is equal to null or b is equal to null  then return null so  in this case  what does being null mean so  being null is simply if you want to saying  when does this expression not having any meaning at all so  i will interpret that as saying if i is greater than n or j is greater than n  then it does not make sense and then so we will return null so  that is that take care of that in the next step is i have to find out  what this smallest k is such that b k equal to a 1 now  here the string was starting from the first index a here  we are starting from i so  naturally instead of 1 over here  i should be looking at the ith element over here  because this is nothing but a starting element of this string and therefore  this should be the element that i look at so  our corresponding step over here is going to be k equals smallest  such that b of k equals a of i and of course  k is greater than or equal to j why this because we want to search to happen from starting from j over here within this string we do not want this search to go before b the next step over here   refer time  48  03   was if k equal to null then return null so  we could put that in but  we do not as a terms out we do not need to so  the next step is going to be r is equal to a of 1 concatenated with this lcs but  remember that we do not want to recursive calls so  we can not write filled of all of this however  we are allow to assume that the table is already full so  in which case what can we do we can write over here t of a of 2 through m but    refer time  48  59   a of 2 through m has to be interpreted in light of this a of 1 through m  which in our cases i through m so  this 2 through m really is nothing but i plus 1 through m so in fact  we will be writing instead of writing a of 2 through m we will be writing t of i plus 1 comma k plus 1 the next thing is r prime and again over here instead of lcs of 2 through m since  we started with a which is going from i through m this will be i plus 1 and this is not going to be 1 through n  but this will be j through n so  this will mean that r prime will be slightly different so  r prime will have to be table entry not lcs again  but the table entry so  i plus 1 comma j  so this is what it was and now we are going to return the longer of r and r prime so  now here essentially done ? so  we have return the fill procedure so  let us see  how the fill procedure is working out  refer slide time  50  36  so  let us look at our table so  this is our table so  in our table  say i is going in this direction  j is going in this direction in the first step is    refer time  50  48   if i greater than m or j greater than n return null so  this step is essentially says  that if this is the m plus 1th entry so  this part of the table and this part of the table  so this is the n plus 1 entry over here or going to be all nulls  in this entire thing now  if we look at i j  how to fill i j it depends on i plus 1 k plus 1 and i plus 1 and j  these are the two table and   refer time  51  20   depends upon so  let me write that out so  if i want to fill  so this is some index i and if i want to fill this so  this is this j over here then this entry is going to depend upon i plus 1 j so  it is going to depend upon this entry so  let me write that is an arrow over here and it will also depend upon i plus 1 k plus 1 just going back to this   refer time  51  46   it depends upon i plus 1 k plus 1 as well so  i plus 1 k plus 1 could be something in this so  it could be some entry of over here so  it will also depend upon this so  if i want to fill this entry i need to have these entries already filled but  that is very easy how do i do it well  if i start filling the entries from the bottom then i am essentially  then i will ensure that when it comes time to fill these entries  these entries are already filled and then the fill procedure will work correctly so  that is all so  that is what i need to write down next so  that is very simply done so  i just want to tell you what the code is going to look like so  first i want to make sure that these two things these two are properly set  refer slide time  52  43  so  that will be done by saying for i equal to 1 to m plus 1 what happens  t of i comma n plus 1 is equal to null and for j equal to 1 to m plus 1 t of m plus 1 comma j is equal to null that just set the bottom row and the right hand side column to null now  comes the main point so  for i equal to m to 1  for j equals n to 1 will just fill t i j is equal to fill of i j  and because our rows are going m to 1 backwards will make sure that fill of i j will find the table entries already filled properly so  that takes care of the whole thing so  at the end of this is simply return t of 1  1 that is going to be your final answer so  what remains now  is to estimate the time taken so  this time is going to be simply o of m this is going to be o of n and this is going to be o of m n because  of these loops over here and the time taken for fill i j  refer slide time  54  18  so  how long this fill i j take well the only place  where time is taken there are two places where time is taken so  one is this so  this could take time proportional to the length of b so  this could take time proportional to n what about this so  we are going to do the concatenation now  if you store the subsequences  which have been computed as arrays  then this could take time o of m because  we are going to take the subsequence and we have to concatenated or in fact  it would take time  it could be that the subsequences the entire things so  it could be max of m and n and maybe the longer computing the longer will also take time this much so  over all the time taken for this is going to be o of max of m n so  coming back this last part fill is going to take time o of   refer time  55  24   max of m n and so the total time is going to be the product of these two things  refer slide time  55  39  or in other words  total time is equal to o of m n times max of m n now  let me just to summarize  let me just say that i will given you dynamic programming algorithm  which runs in so much time there is a simple modification that you can make to this  using which we can reduce the times further so  it could be it can be reduce to time o of m n so  i am just going to write down the code for fill and we will not prove it correctness but  the correctness will be include in the manner analog is it will be only slightly more complicated and so i will just do that and i will stop this lecture  refer slide time  56  24  so  this is the code  which will give you faster result it will require that you store the result being calculated the table entry should be stored as a linked list rather than storing the entire subsequence in each entry in the table  store it as a linked list if you do that  with the little slight trick you should be able to reduce the time taken just down to o of m n   refer time  57  58    so  with that i will stop this lecture thank you design and analysis of algorithms prof abhiram ranade department of computer science engineering indian institute of technology  bombay lecture  20 matric chain multiplication welcome to another lecture on design and analysis of algorithms our topic for today is matric chain multiplication let me begin with an informal statement of the problem that we have  refer slide time  01  02  so  our problem is something like this we are given n matrices let us call them m 1  m 2  all the way till m sub n and we would like to multiply them and form this product so  this is the usual matrix product we would like to do this as fast as possible here are some of the issues involved in doing this if the first issue is what algorithm should be use  when we just multiply to matrices together  because we are going to use this algorithm repeatedly that is how we are going to solve this problem today the second issue is since matrix multiplication is associative how do we parenthesize this product because  that will  it will be clear zone that will also affect the time required to do the entire to perform the entire product so  let us look at the first issue  refer slide time  01  55  so  let us say we want to compute c equal to a times b where  a is a p by q matrix b is q by r matrix a and b are given to us and c is a p by r matrix  which is to be completed the simplest algorithm for doing this is to use a definition of matrix multiplication matrix multiplication is defined as c i j is equal to summation over k  k going from 1 to q of a i k times b k j computing the single term such as c i j will require o of q time c has p r elements overall c is a p by r matrix so  it has p r elements so  the total time will be o of p q r now  terms out that there are faster algorithm possible but  we are going to strict to this algorithm for simplicity so  we will assume for the rest of the lecture that if i want to just multiply 2 matrices together my time is going to be o of p q r or to wake it even simpler i am going to just say p q r is the time required for multiplication of 2 matrices p by q matrix by and q by r matrix  refer slide time  03  13  the second issue concerns associativity let me explain this with specific example involving only 3 matrices so  say n is equal to 3 over here now  we need to compute the product m 1 times  m 2 times  m 3 there are two ways to do this one possibility is multiply m 2 and m 3 first and then multiply that with m 1 or multiply m 1 and m 2 first and then multiply that with m 3 let us  take a specific example  refer slide time  03  55  so  let us assume that m 1 m 1 is a 3 by 2 matrix m 2 is a 2 by 4 matrix and m 3 is a 4 by 1 matrix so  now if you want to multiplied according to this manner   refer time  04  23    then here is what will happen so  we will first compute matrix m 4  which is m 2 times m 3 so  m 2 was 2 by 4 matrix and m 3 was a 4 by 1 matrix so  the product will be following over formula p q r  it will be 2 times 4 times 1  or it will be 8 now  given m 4 our final answer can be computed just by multiplying m 1 with in m 1 is a 3 by 2 matrix and m 4 we calculated   refer time  04  58   2 by 1 matrix so  this will require time 3 by 2 by 1 or 6 so  this will require 6 matrix multiplication  6 scalar multiplication this will require 8s scalar multiplications then the total number of multiplication required will be 14 another possibility  which i have indicated over here  is to first form m 1 times m 2 so  we compute m 5 equals m 1 time m 2 remember  that m 1 was in 3 by 2 matrix and m 2 was a 2 by 4 matrix so  the total time over here  to calculate m 5 is equal to 24 the answer now  can be obtained by multiplying this m 5 which is just this  which we calculated by m 3 now  m 5 in the product was 3 by 4 so  it is p and q are 3 and 4 m 3 is 4 by 1 so  q by r is 4 and 1 so  this product requires time 3 times 4 times 1 or 12 the total time for this way of doing things is therefore  24 plus 12 which is equal to 36 so  clearly this first way is better  refer slide time  06  18  the problem now is that  if we are that we can take our product and we can parenthesize it in this way or we can parenthesize it in this way we could speak of parenthesization but  i would like to point out that these parenthesization can be you can also be thought of it rooted trees so  let me explain that  refer slide time  06  43  so  here are our metrics to be multiplied m 1  m 2  m 3 so  if i want to first form the product m 2 times m 3 let me represent it in our usual fashion by drawing vertex over here and this vertex represents there product the next thing i do is  i take the product of these two and this is my final answer so  this corresponds to the parenthesization m 1 times m 2 times m 3 alternatively  i could also have done something like this so  i could take product of m 1 and m 2 first so  this is the first way of doing things this is this is what i do first and then i take the product of those so  the parenthesization corresponding to this is m 1 times m 2 whole times m 3 so  this is this we have doing things which we had early indicated earlier in this particular example  we found that this   refer time  07  49   superior but  of course  with other choices of the matrix to mentions  other way could be superior this could be superior we want multiply m matrices   refer time  08  04   just 3 so  let us do one more example  what if we have 4 matrices  refer slide time  08  07  so  in this case  we have addresses m 1  m 2  m 3  m 4 so  the parenthesizations here  will corresponds to 4 trees with 4 leaves so  one possible parenthesization  which i might indicate is something like this  refer slide time  08  32  so  say for example  i could compute say the product of m 2 and m 3 first then  maybe i compute product of m 3 and m 4 next and then finally  i compute this so  this is my  this is the tree of course  there are many such trees possible and in fact  in this case let me just write down all the values all those so  say for example  here is another tree so  here is another tree  in which we first multiply and 3 and 4 then multiply that with m 2 and then with m 1 but  that is not only one here is another and then  we also have things like and also so  in this case  it terms out that there are 5 trees in which m 1  m 2  m 3  m 4 can be the leaves and therefore  5 ways of parenthesize in this product as you saw to go from 3 to 4 we went from 2 to 5 if you keep doing this  you will see that the number of tree is grow is very fast in fact  you can prove you should be able to prove that it grows exponential so  in the pointers that we want to pick a tree from one of these possible trees  and we want to pick the 1  which minimize this the total time taken so  we are now ready to give a formal statement of the problem  refer slide time  10  31  so  our input consist of an array d zero through d n so  this is going to represent the dimensions of the matrix so  d i minus 1 and d i respectively will give the number of rows and columns of matrix m i so  this means that the columns of the number of rows of matrix m i plus 1  will be d i so  which will be equal to the number of columns of matrix m i which is as i should be  because only then can be think of the product m i times m i plus 1 so  although we have m matrices  which are being multiplied and this is supposed to represent the dimensions of n matrices we only have a  have n plus 1 numbers over here our output is suppose to be a rooted binary tree with m 1 to m n as leaves and this whole thing should give us the least time for computing the product let me now  state as state again that if you give me a tree  i can estimate the time required for the tree and the algorithm here is actually fairly straight forward so  if we give me a tree  then the time required for the left sub tree can be estimated recursively the time required for the right sub tree can be estimated recursively and then  i just add up the two times but  that does not finish the time the total time i have to take the product represented by the left tree the product represented by the right tree and i have to multiply them together so  i need an additional term over here so  just to clarify suppose  we have we consider a tree  in which the left sub tree has i leaves  refer slide time  12  28  so  say m 1 through m i belong to the left sub tree and m i plus 1 through m n are leaves for the right sub tree and so we take the product over here so  this has some cost let me call it c l and let me call the cost of this c r so  what is the cost of this entire tree ? well  so  the cost is going to be c l plus c r plus computing the product of these matrices with the product of these matrices but  what is the product of these matrices  what is the dimension of the product of these matrices so  that is simply the number of rows in this matrix  which is d 0  times the number of columns in this matrix so  let me write over here so  this are here  this product is going to be d 0 times  the number of columns over here so  times d i  this product is going to be d i times d n so  as a result  what we are going to get is the total time or the time to computer this lost product  which is the only term remaining over here is d 0 times d i times d n so  stated here  the time taken is  the time for the left sub tree  time for the right sub tree plus d 0 d i d n so  here i was using subscripts here  i will just use array accesses  but you know there will be in the same thing i mean the same thing by them  refer slide time  14  40  so  how do we solve this problem so  here is a simple brute force algorithm so  we generate all possible n leaf trees and will use  will calculate the cost for each or cost is really the same as time in optimization   refer time  15  00   more customer is to think of cost so  i might use or i will use cost to represent time in the rest of the lecture so  we calculate the cost of each of these trees and then  will be return the 1 with the minimum cost as remarked earlier  this is going to be too slow  because a number of trees is going to be exponential in n  refer slide time  15  22  we would like to solve this problem or we would like to see  if dynamic programming can be used to design an efficient algorithm for solving this problem so  let me give a quick overview of what dynamic programming the whole dynamic programming ideas so  in dynamic programming the first step is to cost the problem as a search for an object over a certain search space so  this certain search space  we have to define a   refer time  15  50   clearly  just to clarify our thoughts then  we also need to define clearly an objective function  which is to be minimized or maximized in our problem  we have really done this step because  the search space is simply the search space is simply the space of all possible trees and the cost function is the time and that is to be minimized  the objective function or the cost function the next step is to design an algorithm  which partitions search space and each partition is searched to find the best  the best element in it or in our case the best tree in now  here is the key idea  each partition is to be searched  but we can do this recursively so  we find the best element in each partition and since  we want the best element in the entire space  which is staying the best of these best dynamic programming does not stop at this step 2 the last and most important step perhaps or the most unusual steps perhaps is that we are going to characterize  the recursive calls which over made over here so  in this we will making several recursive calls  we will ask the question is there anything interesting about those calls  is there any property that we can identify for those calls or somehow  can be say that the calls are only made on these arguments so  once we get a characterization  we are going to build the table  in which the results of all these calls are going to be stored so  we have identified  which are the calls which are going to be made over the lifetime of this algorithm and will have an entry in this table corresponding to each such call then will define a procedure  which i will call fill  which will be able to fill the table entries using other table entries the key requirement is that fill should not be recursive function if we can do this  we will have a dynamic programming algorithm and if everything works right this will the a fast algorithm so  let us see how we can make this work so  the first thing is to look at the search space so  let us quickly take a look at that so  the search space in our case is all possible trees with m 1  m 2 the matrices at the leaves note  that we have not actually been given the matrices and we do not need them  we do not we are not actually going to calculate the matrix product we are just going to design scheme for calculating the products  refer slide time  18  50  so  even though i say  that m 1  m 2  m n are the leaves i really mean  so i really mean there are some place holders m 1  m 2  m n where the matrices can eventually be placed so  there are these place holders m 1  m 2  m n which are going to be the leaves and we are going to consider all possible trees and that going to be our space s our object function is going to be a cost we call it a cost  because that is something that we want to minimize so  our cost is going to be the time required for a tree and we have already seen  how we calculate this time so  our goal is to find the minimum cost tree from this s the next step is to partition this s now  there are several ways in which you could partition if you have so this is the place  where we require some creativity however  i am going to point out a very natural way of doing this partitioning so  s is the space of all trees i am going to define s sub i is that sub space so  that sub space in which consists of all trees  in which left sub tree of root has m 1 through m i at the leaves and right sub tree of root has m i plus 1 to m n at the leaves so in fact  it is going to be a tree which looks like this so    refer time  20  22   in this picture  i have m 1 through m i at the leaves of the left sub tree so in fact  let me call this sub tree l and let me call this r now  i have told you  what structure l is going to have  it could have any structure what so ever i have told you what structure r is going to have  that could also have any structure so  all such trees  which have this form r going to be in this sub space s i so  i am going to consider as a sub space for each value of i  i going from 1 to n minus 1 because  i want at least 1 leaf in this side and 1 leaf in this side so  if i take the union of these subspaces s 1  s 2 all the way till s n  s n minus 1 i will get my original space so  every tree in this original space is place  somewhere in one of these subspaces the next question is how do i search s i so  if we can design an algorithm for searching s i then will be able to find the best in s i and then  will take the best of the best as we has said earlier so  whenever we look for something it is usually a good idea to characterize to design some property to define some property of it so  as i am looking for a tiger in a forest well  it will be useful  if i say that a tiger has strips or it is yellow or it is a large animal so  if we can define some properties for the best tree that we are looking at so  let us say t is the least cost tree in this s i and if we can define some properties for it  then it will help us in actually finding t  refer slide time  22  18  now  here is the main property of t that we are going to use and it is actually a fairly natural property so  the main lemma that we are going to prove is this so  left l be the left sub tree of t  which is a least cost tree in the side i am saying a  and not b least   refer time  22  41   cost tree  because there could be several least cost trees so  t is just a least cost tree in our subspace s i and l is it is that sub tree the interesting observation or the interesting   refer time  22  57   perhaps right now is that  if t is a least cost tree then somehow  we expect that l also should be a least cost tree but  l is only over the l only is computing the product m 1 through m i and so  our claim is that l must be a least cost tree for computing the product m 1 through m i and the proof of this is very follows in the standard dynamic programming   refer time  22  28   argument so  we are going to assume   refer time  22  30    so  we will assume that l prime be the optimal or the best tree for this product m 1 through m i and it is cost sorry and the cost of l is actually  greater than the cost of l prime  which has to be the case  if you want to claim that l is not optimal so  l has cost strictly bigger the l prime so  let me draw picture over here another picture  refer slide time  24  02  so  this is l  this is r and say this is my tree t now  what we are just claimed is that there is t l prime and this has leaves m 1 through m i and the cost of this is smaller in the cost of this of course  l must also have leaves m 1 through m i and let me write down the leaves over here as well this should be m i plus 1 through m n here  is what we are going to do here is sort of the standard dynamic programming idea i am going to construct another tree t but  this time i am going to place l prime instead of l so  my tree t is tree t prime is going to looks something like this so  i am going to have l prime over here and i am going to retain r as it was and i am going to connect these two together so  this is my tree t prime so  i start  so t is known to be optimal so  this t is optimal and i do not know anything about this but  i would like to estimate the cost of this so  let us look at the cost of t first so  cost of t is equal to cost of l plus   refer time  25  35   cost of r plus d zero d i d n so  this just comes from this observation  which we had made earlier now  what we know about cost of l  cost of l is bigger than cost of l prime so  if i substitute l cost of l prime over here i get that cost of t is bigger than cost of l prime plus cost of r plus all of this now  this entire expression really  if you look at thus picture over here  you can see is nothing but cost of t prime so  this entire expression   refer time  26  16   is equal to cost of t prime so  what have you proved  we have proved the cost of t is bigger than cost of t prime or   refer time  26  27   t prime has a smaller cost than t but we had made a claim about t so  we had claim the t is optimal so  t is optimal for this space s i so  t is the best possible tree of this form with i least on the left side it is a least cost tree but  we have just found   refer time  26  52    that t prime has even smaller cost than that so  we have a contradiction so  we have a contradiction and therefore  our basic statement must be true that l must be   refer time  27  07   l must also be a least cost tree so  if t is a least cost tree  then its left sub tree l must also be a least cost tree but    refer time  27  18   we can apply the same argument to the right side as well so  we can argue that the right sub tree of t must also be the optimal sub tree over it is leaves so  we are now  ready to design a recursive algorithm  refer slide time  27  39  so  we will call our recursive algorithm mcm stands for metrics chain multiplication it will take as argument then array  the array which gives the dimension of the matrices and it will return the optimal tree we do not have the leaves the matrices themselves so  the tree will just p the tree will just have an adequate number of leaves and the leaves will serve as place holders for the metrics we are only interested in the structure of the tree anyway so  we are not actually going to perform in the matrix multiplications themselves but  we are just going to indicate the order in which the matrix multiplication should be performed the basics for writing this algorithm is going to be the lemma that  which has proved but we need to first take care of some base cases so  suppose our array only represents a single matrix  in which case this n could be 1 we could be given just two number  the number of rows and the number of columns in this case of course  there is no multiplication to be done but  this is the base case and here we will written a tree but  this tree will just consist of a single vertex now  we are ready to generate the algorithm that we use the lemma generate our algorithm the basic idea is that we are going to explore each subspace s i so  here is the code for doing that so  i here is going to be the subspace that we are going to be exploring and i will take value is 1 to n modulation one so  for each subspace we will first calculate  the best left sub tree that must be made that must form  the that must be a part of the optimal sub tree optimal tree for that so  as for the previous lemma the left sub tree of the optimal sub tree optimal tree for this subspace must itself the an optimal tree so  to do that  will recursively call mcm but  this time we only want the optimal tree over the first i leaves so  likewise we will only we passing the i plus 1 elements of this array so  i plus 1 elements will define the rows and columns of the sizes of the number of rows and columns of the first i matrices and that is what we will pass likewise  will construct r i  which will be the optimal right sub tree  which will be the part of the optimal tree for s i so  this will be done by this call mcm of d i through n and this will simply be the dimensions of the remaining n minus i metrics c sub i is going to be the cost of this this is going to be computed using this observation so  it is going to be the cost of l plus the cost of r   refer time  30  50   and the cost of multiplying the two the results together so  this is what we have over here so  it is going to be cost of this tree  cost of this tree that we get and the time required or the cost required to multiply the results corresponding to these two note over here  that i am using this  this expression cost of l i and you may think of it either as a function  but more accurately you should think of it as a field selector so  when we return the tree itself  the tree will also contain its cost so  we are just extracting the cost out of it so in fact  as we designed the algorithm we should consider that we are not only written in the tree but  we are also written in the cost of the tree next  we will have all these c i ? s and we just want to find the minimum c i and let us call that i for which c i is minimum and let us call that i  lets us use i to denote time and then  we are simply going to take the corresponding tree in the corresponding left and right sub trees and make a tree out of those and written that so  this will typically we something like l i will be a pointer to the left sub tree or i will be a pointer to the right sub tree so  we will construct the new node  which will be the new root and we will make the root point to align r i and since we also want to written a cost  we will attach an additional field  which is c i over here so  this is the end of the procedure so  this is the end of the recursive algorithm that we wanted  refer slide time  32  39  now  we could work with this  but to simplify a matrix we will only consider will consider an algorithm  in which we only compute we only compute the cost and not the tree itself so  the idea is going to be very similar to the previous algorithm in that it going to be simpler so  earlier we said that  if n is equal to 1 then be written single element in this case  we are not going to written a element  but we are going to written its cost so  in this case  we are going to written 0 in the previous algorithm  we l i equal to mcm of this but here  now we are going to use mcmc so  mcmc is the cost the cost of the optimal tree so  will l i will now become the cost  arrive will also become the cost and so the total cost is going to be not cost of l i  but just l i plus r i plus whatever this was and now  we are going to find the i for which this is the minimum and then we will write this c of i so  we will work with this and the rest of the dynamic programming procedure will work will use this  refer slide time  33  51  actually  we would like to simplify  we can simplify the whole thing and here is however  simplify it so  let me go back to this and let me try doing this one paper   refer time  34  01    so  you can see both the things at the same time so  the idea over here was that we are going to calculate this expression and take the minimum so in fact  this whole part i can replace by the following expression  refer slide time  34  25  so  i will just write this as return the minimum over all possible values of i  of what are say in fact  i going from 1 to n minus 1 and what is it that we are going to take the minimum of it is just going to be this c of i over here so  instead of l i  i will just substitute this mcmc so  i will write that as mcmc of d 0 through i plus mcmc of d i through n plus d of 0 times d of i times d of n so  i just taken this expression   refer time  35  25   and i have substituted mcmc over it here and this mcmc over here and i get this expression so  that entire part could be represented as this so  we are going to take this expression for all values of i and we are going to find the minimum fit and that is what we are going to return so  this is the resulting expression   refer time  35  47    so  if n is equal to 1 again will return 0 otherwise will return this  which is what i have return over here the final step in dynamic programming is to characterize the recursive calls  refer slide time  36  01  so  our recursive calls  which we made   refer time  36  04  for this procedure mcmc of d 0 through n and we called d 0 over i over here and d i n over here so  we mcmc   refer time  36  14   was called these arguments so  if we look at how mcmc would progress further  you will notice that each time we are taking our original range and then we are splitting get in some way so  here we are taking a prefix of the range here we are taking the suffix of the range if we take a prefix of the suffix then we will get some range for not necessarily a suffix or prefix of this but  never there is always some sub range of this so  our characterization could be something like this that mcmc is always called is a sub range d j k  where j k is the sub range of 0 through n so  our entire array was d 0 through n but  will call it with some d j k so  this allows just define a table since  we know what all the recursive calls are going to be  we are going to allocate one entry for each possible result so  here is the possible result so  d j k  where 0 less than  j less than  k less than n so  this result of the call mcmc on this is going to be store in table entry t j k our next step is to define a non recursive procedure fill  which will be used to fill this so  if we call fill with j k  we should get exactly what is suppose to be filled over here or it should return mcmc of d j k so  mcmc of d j k  so you need to figure out what exactly is mcmc of d j k so  let us try and execute mcmc with the arguments d j k so  for that we need to go back to the definition of mcmc so  here is what we need to do  we want to execute the call mcmc of d j k  refer slide time  38  09  so  here is our basic procedure   refer time  38  25   mcmc so  this is being called the parameters over here are the entire sub range 0 through n however  we want to call it with this sub range so  assume of course  that the compiler will rename and make this appear like 0 through n but  when it does that  what exactly will this return what exactly how exactly will this execute that is what we need to understand so  the condition n equal to 1 over here just say  whether this last index is 1 plus this first index so  this condition really should be thought of us if the last index k is equal to j plus 1 in that case return 0 otherwise  it is going to return this expression so  we are still going to return this expression  but wherever the first index 0 appears will just put in j and wherever the last index n appears we just put in a k so  that exactly what we are going to what that procedure should be doing so  mcmc even called on d j k will work something like this so  it will return say min our i going from not 1 to n minus 1 but  instead it will go from j plus 1 to k minus 1 and it will compute mcmc of d of j i plus mcmc of d of i k plus instead of this we are going to have the corresponding right bounds   refer time  40  29    so  we are going to get d of instead of 0  we will get j times d of i times d of k  refer slide time  40  45  so  this is the value that should be return by fill so  here is what fill in fact  as so  fill is going to do exactly this   refer time  40  56    however  fill is not really allow to reference mcmc and in fact  filled does not need to reference mcmc so  when it wants to compute mcmc of d j i it fill knows that this is going to be stored in d j i so in fact  instead of mcmc fill is just going to return d of j i instead of mcmc of d i k fill is just going to return t i k and then this part is going to be the same as before in fact  that is what fill is going to be returning so  this procedure fill that we have defined over here is the right procedure it is non recursive it does not think talk about mcmc does not call anyone else  but it is look at the table entries so  this is what we needed  we want it to have a procedure  which tells us how to fill the j k th entry assuming that other entries are full so  this is what it is doing so  now we are really pretty match ready to right the dynamic programming algorithm so  what we need to do next is to characterize how fill works so  let me write down over table first  refer slide time  42  22  so  our table t is going to look something like this so  here is the first index  index which is i here is the first index  which is the second index which is j  sorry our first index is j and the second index is k so  let us now look at the first step the first statement and fill so  it is says that fill   refer time  42  56   the table must contain as 0  if k equals j plus 1 so  this is 1 1 so  this will be the entry 1 1 so  this is not of interest so  if k is equal to j plus 1 so  this entry is going to be a 0 this is where k equals j plus 1 but  that is not the only entry in fact  another entry  which will be 0 because  of this will be this entry and so on so  these are simply entries above the mean diagonal so  this could be the mean diagonal and all the entries above the mean diagonal will be 0 ? s and in fact  the whole thing is going to be define for j less than or equal to k so  in fact it is going to be defined on this side this side is not going to be use  this side and the diagonal are not going to be used at all now  let us examine how does fill   refer time  44  01   a particular entry j k so  let us say this is my j over here and so this is the entry that i want to fill and this is the index k so  the jth row kth column this is the entry that i want to fill how does filled   refer time  44  21   fill this entry so  it considers the values of t j i  where j ranges sorry t j i where i ranges from j plus 1 to k so  these are simply table entries in this region so  let me put them in a different color  so it so these are the table entries so  these are the table entries  which are needed for updating this so  let me put an arrow going from here to here but  it also uses table entries i k   refer time  44  53   again i going from j plus 1 to k minus 1 so  which are these entries well  these are the entries  which are below over here so  the entry is above the main diagonal but  below this element are also going to be used so  if these entries have been have already been defined and these entries have already been defined then fill can fill in this table entry so  then the entire procedure is extremely simple what is that we have to do we must ensure that when it comes time to fill this entry all these entries must already have been filled so    refer time  45  41   that such as is a very simple order in which we can fill these entries  refer slide time  45  47  so  here is my table so  here is the main diagonal so  this diagonal i am not going to do anything with but  the entry is above that i am going to mark as 0 ? s then  i am going to start filling entries but  i should fill entries such that if i want to fill this entry then  everything on this side and everything on this side is already filled so  next i can fill this entry maybe so  i will put a 1 over here after that  i can fill this entry i can fill this entry then i can fill this entry  this entry  this entry  7  8  9  10 in general  i can let me use another picture i can take my matrix  refer slide time  46  39  so  this is my matrix  this is my table i will filled the second diagonal with 0 ? s then  i will fill the entries  the rest of the entries in this order first  i will fill these  then i will fill these  then i will fill these  then i will fill these and so on the point is that  if i have fill entries in this order whenever  it comes time to fill say an intersome where over here i would already have filled these entries and these entries on which this entry depends so  that is the key idea so  if these entries are already filled then you can fill this entries so  the only thing that remains to be done now is to figure out  how much time the whole thing   refer time  47  28   will take  refer slide time  47  32  so  the total time taken is clearly equal to number of entries in the table times time to fill 1 entry so  this is already telling us  everything that we want to know how many entries do you have in the table well  t has all this entire range j k  where j k lie between 0 and n so  we have a two dimensional table with n rows and n columns we are only using half of it but  clearly the number of entries is therefore  o of n squared what about the time to fill a single entry so  t j i and t i k are already known  when we   refer time  48  29   are going to have to compute this product and we are going to do these additions but  and this has to be done these many times  so far every value between j plus 1 and k minus 1 this range can be as large as n and therefore  the time to fill a single entry is going to be o of n again so  the whole thing is going to be o of n cube so  that really   refer time  48  58   finishes the entire algorithm  refer slide time  49  02  so  the total time is n square times n which is equal to o of n cube now  i just want to make one more comment in the comment is about this simplification that we tell so  we said that  we are not going to compute the exact tree that we wanted but  we are just going to compute  the cost of the optimal tree so  i want to leave you with an exercise  which is how can you modify this algorithm so  that you actually  calculate the tree and not just the cost  refer slide time  49  44  so  this is exercise so in fact  the idea is something like this so  i am giving your hint over here in each table entry  store the root node of the tree along with the cost of that the root node will contains pointers to the left and right sub trees now  if you go and ask the question  how do i update a single element of the table you will have to construct the root node as well as these pointers and as well as these cost but  you will see that each filling now will still be possible in o of n time and since the number of entries are o n squared the total time should still be o of n cube so  that concludes this lecture design and analysis of algorithms prof abhiram ranade department of computer science engineering indian institute of technology  bombay lecture  21 scheduling with startup and holding costs welcome to another lecture on design and analysis of algorithms we will see 1 more problem which can be solved nicely using dynamic programming today the problem we are going to solve is scheduling with startup and holding costs let me define this problem for you  refer slide time  01  10  so  in this problem we are given a machine which has the capacity of producing 1 unit of something whatever it is per day however  if i want to start a machine a machine on some day which is so  what i mean by that is that if the machine was not on today and i want to get it working today then i have to pay a startup cost and that startup cost is some s units which is defined as the part of the input clarifying again if the machine was on yesterday then i do not pay a startup cost today if i want to keep the machine working if the machine was off yesterday  but i want to start it today then i will have to pay a startup cost we also have a warehouse to hold units which have been produced so  if i produce something today and it is not to be delivered today itself then it goes to a warehouse but of course  if i place something in the warehouse i have to pay some rent so  let us say that we will call that the holding cost and that holding cost will be some h rupees per night the main input to the problem consists of a daily demand for n days so  we are given the vector d of 1 through n in which d of i represents the demand for the ith day we are also given the startup cost s the value of s and the value of h the holding cost and what we are supposed to produce is a schedule so  we are supposed to produce a vector p of 1 through n in which p of i denotes whether or not the machine is to be kept on the ith day or in other words whether the machine should be producing anything on the ith day so  the final requirement which is which might be obvious perhaps  but let me state it nevertheless is at the end of n days all the units that have been manufactured must have been delivered so  we can not be left with any inventory at the end of n days and of course  it is possible that the demand that is being given to us is just not satisfiable because after all our machine can produce only 1 unit per day so  in n days the machine can produce n units so  certainly if the total demand is more than n we will not be able to meet there will be other conditions under which the demand could not be met  but whatever these conditions are our algorithm must report if the given demand can not be met  refer slide time  04  01  so  let me now take an example to illustrate this problem better so  in this example this is the demand vector that is given to us so  what this means is on day 0  day 1 nothing gets to be delivered nothing needs to be delivered  day 2 nothing needs to be delivered on day 3 there is a order of 2 units which has to be delivered day 4  day 5  day 6 nothing has to be delivered and on day 7 there is an order of 3 units the holding cost that is the cost of storing 1 unit in our warehouse is 1 so  it is 1 per unit and the startup cost is five so  here is 1 possible schedule or 1 possible production plan which will meet this so  i have shown this in a tabular form over here so  we have days on the x axis so  1 through 7 since there are 7 days the demand is given as 7 days on the third day there is a demand of 2 as specified over here and on the seventh day also there is demand and in this case the demand is 3 here is a production plan which will meet this demand so  notice that on this day 2 units have to be delivered and on any day we can deliver only 1 unit so  which means we have to start producing earlier so  say in this case we have to start producing on day 2 so  we will produce on these 2 days and then for to meet for this demand we will produce on these 3 days machine will be idle on this day and not this day since the machine is going to start having the idle on the previous day there will be a startup cost on day 2 so  this will be a cost of 5 similarly on this day the machine will have to start again been idle on this day so  there will be another startup cost on this day  day 2 whatever has been produced is not going to be delivered it is only going to be delivered the next day so  it will have to be held in the warehouse and so  there will be a inventory cost of 1 similarly  on this day whatever has been produced will have to be held in the warehouse whatever is been produced in the warehouse on this day will also be held so  at this point there will be 2 units in the warehouse so  holding cost of 2 will be incurred here this is of course  not the only plan here is another so  in this case what we have done is instead of keeping a gap on day 4 we have begun the production earlier  but notice if we begin the production earlier then we have our holding cost here as well so  infact  the 4 dates we have to hold in our warehouse whatever unit has been produced on the fifth day whatever has been produced has also to be held in addition to whatever have produced yesterday the day before that and finally  whatever is produced in the sixth day there have to be held and all these 3 units will have to be delivered on the seventh day which of these is better well  let us calculate the time taken and the cost of each the cost of the first plan that is simply the startup cost which is 5 plus 5 here plus 1 plus 2 so  this cost for the first plan is 14 the cost for this plan is 5 startup 5 for the startups and then 1 plus 1 plus 2 plus 3 which is written over here and this adds up to 12 so  eventually this plan is better our question in general is going to be to consider all such possible plans and figure out which is the best 1  refer slide time  07  56  so  this naturally suggests a brute force algorithm so  we will generate all possible plan for each will evaluate the costs and then will pick the best unfortunately the number of such schedules or such plans is going to be exponentially n where n is the number of days so  this is going to be just slow and we would like to have a faster algorithm this is where the dynamic programming comes so  here is a quick view of the dynamic programming so  the first idea is to find some kind of a repulsive solution how do we do that ? well  we cast our problem as a search for some object over certain search space so  it is useful to define the search space quite clearly as well as it is important to define clearly an objective function which is to be minimized or sometimes it has to be maximized whatever it is it has to be defined very clearly then we design a algorithm which searches the space typically the algorithm is going to partition the search space so  it is going to say let us divide the search space into spaces and each space or each part of the space each subspace is going to be searched separately so  i search the first subspace and i get the best solution and i get the optimal solution in it what i mean by that is the solution which minimizes the objective function or if we want maximization the solution which maximizes that objective function so  we calculate the best optimal in each search space and then return the best of the best so  that is the general idea of getting a recursive solution dynamic programming ; however  does not stop at this point it proceeds further in the following sense the idea is that will characterize what the recursive calls are in this part so  essentially we are going to ask the question what is that we are what are the problems that we are solving ? so  we essentially make a table of all those problems and in each cell of the table we would like to store the results of those so  we will define such a table then will define a procedure for filling table entries and this procedure will be a direct procedure it would not very particular and it will assume that if i want to fill a certain table entry i can use entries which have been filled earlier so  this will fill entries in the table given that other entries have been already filled so  in this recursive procedure the recursive procedure might be slower  because it might calculate the same quantity several times whereas  in this case when we make the table we will carefully think about what it is that needs to be calculated and will calculate that exactly once so  that is going to give us our efficiency so  what we would like to do now is apply this whole idea to our given problem the first step in this is to find the recursive solution so  we want to cast our problem as a problem in the recursive fashion our algorithm in a recursive fashion  refer slide time  11  19  so  here is how you might think of a recursive algorithm for our problem so  we would like to solve a n day problem so  typical steps in that might be that we somehow solve the first day and then we recurse on the remaining n minus 1 days now  unfortunately recursing on n minus 1 days will depend on what happens on the first day so  say for example  if the machine has been switched on on the first day then when i recurse this information is very useful to me so  if the machine is on then on the first day of recursion i do not have to pay the startup cost so  which means our recursion must somehow include additional history information the most natural way of doing this is to generalize our problem  refer slide time  12  18  so  let us see that so  our generalized scheduling problem looks something like this it has the same inputs as before except there are some more so  it has inputs the demand as inputs the demands for n days the startup cost the holding cost and then there are 2 additional inputs and input i which gives the initial inventory what i mean by inventory is that how many units have already in stock on day 1 ? so  when i begin the whole thing it is possible that i already have some units in stock ? so  that number is specified as a part of the input and that is this number i then on the day which i begin the machine might be already on or off and that is specified by this variable m so  m can take values on or off i can take values any integer the rest is similar well i should point out that our old problem which we had defined earlier corresponds to having i is equal to 0 that is no inventory when we start and m equal to off so  this is the generalization in the sense that now we allow i and m to take on a wider range of values our output is as before we are supposed to produce a production schedule and the requirements are same that at the end of n days everything that has produced must be consumed  refer slide time  13  54  so  let us now try to define a recursive algorithm for this problem so  the first step was to design or think about a search space for this problem and the objective function so  let me call s the search space so  what is the search space for the input problem as given well the search space will contain all possible schedules for this instance under instances characterized by these inputs so  a instance is defined as d s h i m where d is a vector of n elements if it helps us to think about will think of each schedule as being as n bit vector so  each bit specifies whether the machine is to be on or off on the corresponding day the objective function is the cost and it just consists of the sum of the holding and the startup costs and our goal is to get the minimum cost schedule from this space s next comes how we are going to partition this here is a natural way in which the space can be partitioned ? so  we will ask what does what happens on the first day so  in sub space s sub p we will put all schedules in which machine is producing on day 1 in space sub space s sub i we will put all schedules in which machine is idle on day 1 now  this union this is this in the sense that in every schedule in every element of s either the machine is producing on the first day or it is idle on the first day and therefore  this is equal to this union this so  we have partition s and so the next question is how do you search each of these sub spaces  refer slide time  16  13  so  let us look at s sub p first and we will try to think of how to search s sub p here is the key idea in devising recursive algorithms for optimization problems so  if p is the least cost element of s sub p then the question we should be asking is will parts of p themselves be least cost solutions some smaller instance if they are then we can use recursion so  this is called the so called optimal sub structure idea so  if p is an optimal solution in this subspace s sub p the question is will parts of p will also be optimal for a smaller sub problem and in fact ; it turns out in our case that that is true  refer slide time  17  00  so  let p 1 through n be a least cost schedule in s sub p will show the parts of it will have to be optimal for a sub problem for a smaller instance so  if p is the least cost schedule what do we know about it well we know that p of 1 is equal to true  why because p is the least cost schedule in s sub p and s sub p is a set of schedules in which the machine is on in the first day so  p of 1 must be true p of 2 n is the rest of the schedule  but now i can think of p of 2 as a schedule for the residual instance from d 2 so  let me clarify this so  our original instance was this d s h i m when n integer vector the residual instance is just a part of this which begins in the second day so  in the residual instance we have only demands for days 2 through n then s is the same the startup cost does not change the holding cost does not change ; however  the inventory on day 1 is going to be different on day 2 is going to be different on day 1 the inventory is 1 and on the first day we produced of the inventory that was present on day 1 we are going to deliver d of 1 which was what the problem require has to deliver and then it will add whatever we produced on day 1 so  this is the inventory for this instance beginning on the second day the final component is the machine status so  over here the machine status could have been whatever now we know since we did produce on the first day the machine status had better be true so  this i the residual instance and clearly p 2 n must be a schedule which satisfies this residual influence here is the key lemma this lemma says that not only does it satisfy the residual instance  but in fact it has to be a optimal schedule for this instance or in other words it has to be a least cost schedule for this instance the argument for this is fairly straight forward and it is of the typical argument in dynamic programming arguments so  well do the argument by contradiction so  we will assume that p 2 n is not an optimal schedule which means there are better be a schedule q 2 n which is optimal for this instance and it is cost be better smaller than the cost of p 2 n so  cost of q 2 n must be smaller than cost of p 2 n  but now let us consider this schedule r where r follows p on the first day or in other words r is true on the first day and on the remaining days it follows q clearly this is a valid solution for the original instance right because well on the first day we produced and then we were left with this instance and that is what q took care of now  what is the cost of r ? clearly cost of r must be smaller than the cost of p why because cost of r is nothing but the cost for remaining days plus the cost of p the cost of the first day this is the cost of r cost of p is cost with the first day plus cost of the remaining days which is not q 2 through n  but p 2 through n but in p we are adding this whereas ; in r we are adding this ; this is smaller than this and therefore  the cost of r must be smaller than cost of p  but remember we assume that p is a least cost schedule in s sub p but here we are showing there exists a schedule r which has even less cost than p and also the first element of r is true  because it is p 1 itself so  r also belongs to s sub p and therefore  we have a contradiction and therefore  our basic assumption must have been false or in other words our lemma must have been true then this is also optimal for instance the residual instance so  this tells us how to search the space s sub p and a similar idea works for the s sub r  refer slide time  22  00  so  let us say i 1 through n be a least cost schedule in s sub i then i 2 through n is a least cost schedule or is optimal for the instance d 2 n s h i minus d 1 false or this is the residual instance so  see that the last component the machine status is false  because s i consists of schedules in which the machine is off on day 1 therefore this is false and the original elementary was i but on day 1 we delivered something without producing so  the new entry is i minus d 1 so  this is the residual instance and this remark claims that i 2 through n must be optimal for this residual instance so  we have accomplished the goals that we set so  we have shown how to search s sub p and how to search s sub i and this has been done by using recursion  refer slide time  23  14  so  we are essentially ready to build a recursive algorithm for sorting this problem here is a outline so  we will call our algorithm opts schedule arguments the entire demands and when i write d 1 through n over here i simply mean that d is going to be a vector with n elements it will take additional argument which is the startup cost the holding cost the current inventory at the beginning and the machine status at the beginning there will have to some best case that will have to take care of  but this is sort of the main core of the recursive algorithm so  this part in this part we are going to search the space s sub p so  we are going to find the optimal schedule for the recursive residual problem assuming that on the first day we do produce and that will be our schedule p 1 through n i 1 through n will likewise be the optimized schedule for s sub i and in this case on the first day we are not going to produce anything and this is just the residual problem given that we do not produce anything on the first day so  these are the problems which we exactly looked at in the 2 lemmas that we just saw and then we are going to look at the cost of the first schedule the cost of the second schedule and we are just going to return the best of these so  this is the best of best idea so  this is the best solution in the first sub space this is the best solution in the second sub space what we are returning is the best of the bests and number of details have to be filled up over here however  you should be able to argue that this algorithm will take time o of 2 to the n see just try to recurrence and this will just come over so  what we are going to do now is not to fill up the details the remaining details and there are a few important details which are missing over here but what we have done is we have essentially identified what kind of recursion we are going to have so  we will just proceed to the next step of our agenda  refer slide time  25  34  so  the next step is the dynamic programming idea so  this step is the key step of dynamic programming which is characterizing the recursive calls so  the first observation is that opts schedule is always called using arguments d j through n and s h i m where j can be any number so  let me just observe let me just point out why this is so  initially opts schedule is going to be called with the entire input but later on it is going to be called with the residual problem in which we are only going to pass the sub array beginning with the second index now  what happens in this call itself in this call we are again going to throw out the first element of the demand array and then we are going to call it with the rest of it so  when we are doing the recursion on it we will be calling it with d 3 n and then when we recurse on that we will be calling d 4 n and so on so  in other words the argument is going to be of this form that we will be that the first argument that is going to be passed will be some sub range well  it will always be a terminal sub range of our input d so  suffix of that d array and then these could be pretty much anything so  j has to be some number between 1 and n so  the largest index is n so  j could be anything until n s and h get passed without any change what so ever the inventory can be at most l why is that ? well  we have n days during which the machine produces 1 unit per day and therefore  the inventory can not ever build up to more than n so  this argument will be an integer at most l and the last argument could either be true or false so  we have a reasonably good characterization of all the recursive calls that could get made in our recursive algorithm so  the characterization says that these are the arguments that are going to be passed and each argument can vary in this manner or not vary at all the next step of dynamic programming says well now that you have identified what the recursive calls are going to be make a table in which we are going to store the table so  we will construct a table t in which t will have 3 indices j i and m so  t of j i m will store the result of optschedule of d j through n s h i m the point of making the table really is that we just want to focus we just want to make a list of what are the sub problems that we are ever going to solve so  this is what we will do now  we can work with this table t ; however  dealing with schedules is a little bit cumbersome because we are going to simplify the problem just for a few minutes so  instead of working with entire schedules we will just work with the cost of optschedule in the table t we were planning to store in each cell the entire optimal schedule instead of that we will build a different table let us call it c for cost it will have same very similar entries similar number of entries with similar indices and in this we will just store the cost of the optimal schedule so  now  we are now going to work with this table and what dynamic programming requires to do is to figure out how entries in this table depend upon each other so  in another words if the entry c j i m has to be filled assuming the remaining entries are full what is the exact computation that needs to be done  refer slide time  30  13  so  we need to derive some kind of recurrence for c j i m let me just remind you that c j i m is the cost of this optimal schedule for the sub problem or for the residual problem actually d j n s h i m let me just remind you that you did not have s h in the table  because they are fixed throughout the execution anywhere here is what the optimal schedule for d j n is going to look like so  i claim that the optimal schedule for d j n s h i m is going to be the schedule with the lower cost from this schedule and this schedule this really comes from the procedure that we wrote a minute ago so  let us just see that  refer slide time  31  16  so  let me just write this term so  my claim is or what i want to examine is the optimal schedule for d of j through n s h i m this is what i want to understand and i want to figure out how what will the optimal schedule to this be ? so  let us go back to the code that we wrote or the recursive algorithm that we wrote and we said that the optimal schedule for d 1 through n sh i m is going to be one of these 2 now  one of these 2  when d is passed as the entire array is this  but only when a small smaller range is passed so  the first argument is 1 what will this be ? so  the first schedule will simply be true concatenated with optschedule and this time instead of passing 2 through n i should really pass j plus 1 through n  because this was just dropping the first element so  if i drop the first element from this this will become d of j plus 1 through n s h i m what about this ? so  this will become false and optsched d of j plus through n s h and in the inventory i should really remove what was present on day 1 so  or not on day 1  but on day j and i should have m this is what i should have here instead of inventory being i i should really have i minus d of j plus 1  because since the machine was on the first day i had to have a plus 1 over here so  you can see that this is a this is exactly what i have written over here so  true concatenated with optimal schedule for the demands d j plus 1 through n s h i minus d j plus 1 and true and the other schedule which is optimal schedule false concatenated with optimal schedule of d j plus 1 through s h i minus d j and false so  now we understand what optimal schedule is so  we just now have to figure out what the cost relationship amongst the cost is going to be ?  refer slide time  34  20  so  c j i m is simply the cost of this so  what is that going to be well this is the least cost of this ? so  c j i m better be the minimum cost of the cost of these 2 schedules so  what is the cost of these 2 schedules ? well  the first schedule starts with keeping the machine on day 1 so  here there is some potential for incurring a startup cost whether the startup cost is incurred depends upon whether this n was on or off or true or false to begin with so  if m is true then there is no startup cost or the startup cost is 0 if n is false then the startup cost is s so  this expression which i have written down over here is to be understood as the c expression that is the c expression mark true value colon false value so  if m is true then this bracket evaluates to 0 otherwise it evaluates to false in other words this bracket over here represents the startup cost it is either 0 or it is depending upon m then on day 1 there is going to be some inventory cost so  this inventory cost is given over here so  the inventory for day 1 is going to be this whatever the inventory subtract whatever was delivered on d j add whatever was produced so  times h will be the inventory cost and then there is a residual cost so  it is just the cost of this part of the schedule but this part of the schedule the cost of this is simply c j plus 1 i minus d j plus 1 true so  we have related the j i mth entry to the j plus 1 i minus d j plus 1 2 entry so  we have to take the minimum of this quantity and this quantity and in this case the computation of the cost is actually simple because there is since the machine is off on the first day this is false there is no startup cost  but there is some holding cost so  what is the holding cost on day j something gets deleted the inventory gets reduced by i minus d j so  the inventory is just this and the inventory cost is just this and this is the cost of the residual problem so  now  we have a recurrence connecting c j i m to other entries in the table so  this is the one other entry and this is 1 other entry there are some problems though so  when we wrote down these numbers  we did not we have not so far considered the possibility that i minus d j plus 1 or i minus d j might become negative that does not make sense so  i minus d j plus 1 or i minus d j is the inventory so  it does not make sense for the inventory to become negative so  we somehow have to take care of that essentially what is going to happen is what should happen is that we should check that if we want to consider the schedule and if we are requiring that i minus d j plus 1 be negative then we should not really be considering it at all and we should just use this here is a very nice fix to this a simple fix so  we are going to define c j c of j i m to be infinity if i is less than 0 so  let us see what this does ? so  in this example in this alternative for example  if in this table entry this is some negative number so  we are asking for c of j plus 1 some negative number true now this entry would be defined as infinity which would mean that this number would be infinite but if this number is infinite then this entire cost would be infinite but since we are taking the min over here  that would force us to look at this cost so  essentially we would be ignoring this cost  refer slide time 39  00  so  by defining c j i m equal to infinity we are saving ourselves the trouble of checking the indices are less than 0 in this expression and automatically getting the same effect now  of course  you might ask if we want to do this for all i less than 0  do we need to consider a infinite all the infinite negative numbers for i ? well  we do not and there is a simple reason for it  remember that the demand on any day can not be bigger than l why if the demand is bigger than n then we can not fulfill that instance anyway and so  we can check at the beginning whether the demand on any day is bigger than n in which case we just reject we just say that this problem is unsolvable so  this algorithm or whatever algorithm we are going to design will only be called if d of j is less than or equal to n but if d of j is less than or equal to n then we are only subtracting n from whatever has to be a positive number earlier so  in which case it suffices i is bigger than or equal to minus n so  we only need to consider i only a few negative values of n that is values of i that is values going from minus l to minus n so  in other words our table is not going to become too large the final problem that we need to consider is the base keys so  we relate entries with the table with other entries  but this can not go on forever so  some entries we have to set ourselves  refer slide time  40  39  so  these are the entries for the last day schedule so  remember that c j i m were the indices for the table entries if you look at the last day then the first index over here should be n so  in this case we are asking for a single day schedule so  we should be able to set this without much doubt so  here is the idea if we want to figure out what c i and m are to be and we need to figure whether there is a legal schedule so  the main condition over here is that whatever inventory we come in with should not be too large or should not be too small if it is too large then even after satisfying the demand we will be left with inventory and that is illegal that will not constitute a valid schedule if it is too little we may not be able to satisfy d of n at all so  the inventory should be just enough to satisfy the last day ? s demand so  very simply if i is the last day ? s demand if i takes the value last demand then we should run with the machine we should not produce anymore units and so  in that the cost for the last day is simply 0 so  c of n i m must be 0 if i is equal to d of n if on the other hand the inventory is just 1 less as we come into the nth day then what the demand is then we would better produce to make up the inventory that we want to deliver whatever is needed to be delivered so  we will have the machine be on and now for the last day we need to run the machine so  our cost is going to be 0 if the machine was already off was already on and s if the machine was off so  this is again the c style expression which evaluates to either 0 or s for all other values of i other than d n and d n minus 1 this means that we have too much inventory or too little inventory in which case we should set the cost to be infinite so  that takes care of the best case as well  refer slide time  43  23  so  let me pictorially show what has happened here or let me pictorially show whatever table is going to look like so  our table is going to look like something like this so  on this axis i am going to have i am going to have the days on this axis or this is where how j is going to vary on this axis i am going to have inventory  so  this is inventory greater than 0 on this side i have i less than 0 and on this axis into the page if you will i will have m  so  this corresponds to say on and this corresponds to off so  i will draw the off in a different color so  it will look something like this so  there will sort of 2 sheets so  even the second sheet so  our table is going to look something like this now how did we fill the table ? well  in this case we looked at the last column on the days side so  this is what we filled out all right so  this part we filled out so  this is nth day and this in the second part in the off side also we filled it out so  these values are known then  because of this we divided c j i m equal to infinity where ever i was 0 we have also filled this out this part so  these parts are filled out to begin with by very simple ideas so  the recursion part comes only for this part and this part over here so  how is that filled ? well that is filled using these expressions so  essentially if i want to fill this entry what am i going to use well  i am going to use some entry in the j plus 1 th column some value from this column and also the corresponding column of the off side on the  of the machine being off  because i am getting c j plus 1 true and c j plus 1 and false as well so  this entry depends upon the next column so  now our calculation is quite straight forward so  this part is already been filled up easily by the last 2 days calculation and by this calculation and then to fill each entry we will just use this expression and this entry can be filled in constant time as i have seen over here as i have shown over here so  basically the idea is that we fill the entire table in decreasing order of j so  we start from here and go backwards and we fill in the table so  the number of entries here well let us calculate that carefully j ranges from 1 to n so  here it is 1 and here it is n i goes from minus n and it goes all the way to plus n  refer slide time  47  20  so  there are 2 n plus values for i and m can be on or off so  there are only 2 values of m so  that total number of entries in the table is just all possible choices for j all possible choices for i and all possible choices for n so  n times 2 n plus 1 times 2 or o of n square time to fill each entry our recurrence can be evaluated in constant time or each single step can be evaluated in constant time so  that is o of 1 and as a result the total time is o of n square so  we will fill the entire table in o of n square time we should ask well we have filled this table  but which part is the 1 that we want  refer slide time  48  19  so  coming back to this ; this is the entry which we want so  at the end this is our final answer or i am sorry not this it is this  this is the final answer or the optimal cost is found in entry 1 0 falls of the table so  1 is the j value 0 is the  i value so  it is this entry on this axis itself and it is in the off or the false side so  this just says what is the cost of generating a schedule for the entire demand ? given that there is no inventory to begin with and that the machine is off  refer slide time  49  12  the final topic is how do we find out  how do i find the schedule given the table ? so  remember that c 1 off was the cost of the schedule now  this cost is going to be a minimum of some costs which costs well  we note we noted that if i want to calculate this cost it is the minimum of some cost in this and some cost in the corresponding column over here so  c 1 off would be the minimum of some expression involving some column some element in the next column on the off side as well as in the next column on the on side so  we simply check whether c 1 is equal to the first term or the second term it is the minimum so  it should at least be equal to at least one of these if c 1 off is equal to the first term what do we know ? we know that this must be generated by keeping the by using the first term or by keeping the machine on during day 1 if this term is equal to the second term over here then the machine should be off on that first day so  in this way by knowing the optimal cost we were able to and knowing the table we were able to figure out whether the machine should be on or off on the first day and we are also able to figure out what the corresponding entry for the optimal schedule for the residual plan is is it this or is it this so  then we can apply this argument again and again and will get machine status for every day and will does generate the entire schedule so  let me now conclude so  in this problem in today ? s lecture we saw another problem for which dynamic programming could be used before using the dynamic programming there was a important step that we took which is quietly an important and interesting step very often we are given problems for which to device recursive algorithm a we need to generalize the problem formation itself so  here is 1 such example you must have seen similar example in another problem which was the problem of medium finding if we want to device a recursive algorithm for medium finding well  we can not do that very easily or very simply so  what we do instead is we generalize the problem and ask for recursive algorithm for finding the arc smallest so  some similar issue was applicable here as well  refer slide time  51  59  i would like to make a comment on dynamic programming and the comment is simply the dynamic programming can be thought of as recursion a basic idea the basic idea the first basic idea  basic idea is recursion and the next idea is make sure that you compute every value only once and our table essentially made as focus on what values we were computing and we were and we thereby we could only calculate we can make sure that we have calculated every value only once finally i would like you to i would like to draw your attention to recursion itself and point out that when we use recursion in search problem in any search problems it can be thought as a divide and conquer of the search space so  with that i will conclude this lecture design and analysis of algorithms prof abhiram ranade department of computer science engineering indian institute of technology  bombay lecture  22 average case analysis of quicksort welcome to another lecture on design and analysis of algorithms our topic for today is average case analysis of quick sort let us begin by discussing average case analysis  refer slide time  01  07  let suppose a be an algorithm q is the set of input instances of a and let us make it  make q be a function of n so  q of n is a set of instances of instances for algorithm a of length n t sub a of q is the time taken by a on instance q given this  you can define the average time taken by a on inputs of size n  which we might write down as m sub a of n m could be interpreted as mean for example and m sub a of n is defined as sum over all instances in the set q of n or sum over all instances of size n of the time taken by those instances  divided by the total number of instances so  this is the usual definition of what we mean by an average this is not the most popular definition of course the definition we usually use is the so called worst case measure here the worst case time of an algorithm a on problems of size n  is defined as the maximum over all q of this the maximum time taken by any input instance of size n is defined as  the usual measure or the worst case time there are several reasons for doing this worst case is usually easier to compute than this average when we talk about the average  in some ways we have to talk about all input instances whereas  often it is easier to deduce what the worst instance is going to be and  then we can just worry about that for many algorithms  most of the inputs behave like the worst input anyway so  in which case it does not really matter  it is not really necessary to take the average in any case very often may be perhaps  the average case is easy to compute if at all and it might still not be preferable  it might still not be very popular because  in practice we do not know which input instances are likely to appear more frequently if some instances appear more frequently  then in this mean expression we would have to wait those instances more heavily therefore  if we just take the mean  then that is not an indication of what might happen in practice and therefore  again we do not really focus so much on the average case analysis worst case on the other hand might be conservative  but at least we know that it is conservative and therefore  at least we can give some guarantees our topic for today is quick sort however  where average case analysis turns out to be quite useful and quite interesting  refer slide time  04  14  so  let me say a few things about quick sort quick sort is a popular sorting algorithm  perhaps the most popular and the most commonly used in practice it is very fast and as i said  it is often the method of choice the worse case time of quick sort is o of n square the average case time on the other hand is o of n log n and will see this quite soon so  in some sense the excellent performance in practice might be better explained by the fact  that the average case time is o of n log n rather than by focusing on the worse case time so  let us now take a look at this algorithm quick sort is based on divide and conquer strategy and the algorithm is something like this  refer slide time  05  08  so  as input we take an array x 1 through n by writing x 1 through n  i simply mean that x is an array whose length is n this is an array in which we have keys we can think of these keys for the minute as some integers perhaps and our idea and the goal of quick sort is to sort them that is let us say that the smallest keys have to come at the beginning and the largest ones have to go towards the end we begin quick sort by looking at the best case first so  the best case just checks whether  this is an element this array has only one element if it only has one element  then the array is sorted trivially and therefore  we just return that array otherwise  we pick something which we will call it splitter and that splitter is chosen to be the first element of this x the first key is the splitter then  we built three lists a list which will call small is going to be small  which contains all elements of x which are smaller than the splitter a list which we will call equal  which will contain all elements of x which are equal to x 1 and so  we begin by putting x 1 into equal i should perhaps said less over here but  will not we are not worrying  we are not very careful about this and we will not be very careful about this throughout the course so  we will i just tell you that  we have just made equal just single list  a list with a single element and we will also construct a list  which we will call large and large will contain all the elements of x  which are larger than x 1 so  right now it has been initialized to null and small has also been initialized to null this loop is simply going to build up the lists  as we just described so  first step if so for every element other than the first we check whether it is smaller than splitter in which case  we add that element to small if it is equal to splitter  then we add it to equal if it is greater than splitter  we add it to large so  at the end of this loop all the elements have been put into the proper lists so  now it is simply a matter of recursion so  small is a list which contained all small elements so  we call qsort or quick sort in this list so  as a result we will get these elements sorted now  these elements are all guaranteed to be smaller than the elements in the list equal and those in turn  are guaranteed to be smaller than the list in large but we do not append large immediately over here  we quick sort it so  as a result we have a long list which is made up by appending three lists but  which in turn is guaranteed to be sorted so  this is how quick sort works so  as i said it is divide and conquer strategy  the division part is where the interesting work happens and  then there is a conquer part and then the combined part is trivial correctness is quite obvious here you can do a induction on size  if you want to prove it formally and i will leave that as a easy exercise so  now you wanted to analyze this algorithm  refer slide time  08  36  let me use t of n to denote the time for quick sort  on size and input so  right now i have only written n over here but  i will actually have a specific input in mind just for the minute later on we will worry about average cases or the worst cases or whatever so  right now let us say this is for a particular instance so  t of n is the time taken by that particular instance so  how do we analyze this ? well usually if we write something like this  we try to establish the recurrence so  of course no matter what input instance we feed if it has length of only 1  then the time taken is constant so  that is what is you write down first then  we need to find out how the recursion happens so  let us just go back to the algorithm so  over here the recursion happens by calling quick sort on small and calling quick sort on large and before that  we have a loop which runs about n times so  the result we have o of n time for the loop and t of large or t of the cardinality of large is the time taken for that instance  for further for evoking quick sort on the list large and t of small  is the time taken for invoking quick sort in the list small now  we can analyze this using the recursion tree so  this was our basic recurrence so  let us draw a recursion tree corresponding to this  refer slide time  10  28  so  we start off with the problem of size n and  then as per this recurrence we break the problem into two pieces one is the small list and the other is the large list so  this is the small part and this is the large part this is a problem of size n this is the small part  this is the large part if this is the small part this is the large part  then we are going to call quick sort recursively on these so  furthermore this problem will get split  this problem will get split of course  if the problem is going to get split  may be one part one side could be smaller than other side could be larger and so on and may be one of the lists could be empty in which case of course  this whole thing terminates so  in general it is going to keep on splitting may be once in a while  a list terminates and things keep on going in this manner so  what do we know all about this  refer slide time  11  38  well  here is the first observation so  if this node has size n  then we know that in this node the number of keys which are going to be present is definitely going to be less than n  in this node or in fact  in this node as well  so what does that mean ? so that means that as i go down from here along any branch  the size of the instance has to decrease so  which means i can not go down too far so  i start the instance of size n it has to decrease therefore  that means this height has to be utmost n that is the first observation the second observation is that  if i look at any node its children have a certain size but  that size adds up to something strictly smaller than this so  if i look at the size over here  it is n the size over here has to be less than n the size over here in fact  has to be smaller than this for these two and for these two  it has to be smaller than this so  this also has to be less than n this also has to be less than n so  if i look at any level  the size of that the sum of the sizes of the problem at that level have to be at most n but now  if we go back to our problem our algorithm at each inside the body of each invocation  we do work or we do work proportional to the size of the problem so  which means corresponding to each node over here we are going to do work  which is proportional to its problem size so  now if i look at the total work done here  it is going to be o of n because  it is going to be proportional to this problem size this problem size added up  it is going to be proportional to o of n or it is going to be o of n similarly  here also it is going to be o of n at every level  it is going to be at most n at most proportional to n so  now we have an upper bound on the work because  there are n levels at most and at each level the work is o of n and therefore  the total work has to be o of n square so  this is the upper bound on quick sort now  i will leave it as an exercise for you to construct an input instance for which  quick sort actually takes time n square so  this is actually fairly easy let me give you a hint  think of a sorted list what if the input instance is already sorted ? but  the key question is that this is the worst time but  will it be the most time  will it take this long usually or is this some kind of an unusual case so  if you come back to the recursion tree to this tree  then we know that at every level the work is going to be at most n so  the real question that we want to ask is  will the tree be of a large height or will the tree have a small height because  if the tree has small height then our total work will be less so  in fact that is what we will see is  going to happen quite frequently so  we did the analysis of the worst case so  let us ask what the best case is going to be ? so  clearly the best cases are the one in which tree is as small as possible  refer slide time  15  41  if the elements that we are trying to sort are all distinct  then i will claim that the height can not be smaller than log n why is that ? well  i am going to leave this as an exercise but  again let me give a hint so  we said as we go down the tree height must decrease but  we also said that the sum of the nodes over here  the size over here plus the size over here must be smaller than this but  if everything is distinct it will only be one less than this so  if it is one less than this  then you should be able to argue that it would not decrease too fast either so in fact  you should be able to argue that it essentially halves at each step and therefore  the total height will be something like log n so  what happens in the best case ? so  in the best case it turns out that the total time taken will be o of log n  o of n log n and in fact  there is a very simple situation in which the best case will happen  which is this ? if the splitter is equal to the median  then the problem size halves and  then the height becomes o of log n  refer slide time  16  57  so  if the height is o of log n that i have taken as n log n  and that is the best so  we consider two cases one case in which the splitter goes  somewhere in the middle another case in which the splitter was extreme  the splitter was the smallest element well  that was supposed to be a homework exercise but  suppose we take splitter  the splitter happens to be the smallest element then  the list would be split very unevenly so  let us consider a case which is somewhere in between so  in this the splitter is say larger than n over 10 elements in the list and is also smaller than n over 10 elements so  it could be somewhere in the middle so  of course this is an artificial case but  you can imagine that this will happen frequently enough because  after all if i pick an element from a list  it is likely to be somewhere in the middle so  let us say this happens let us say that  every time i pick a splitter it satisfies a property like this one what happens then ? well  let us go back to the recursion tree so  let us redraw this recursion tree  refer slide time  18  18  so  i start with an n node problem an n key problem now  i am going to pick a splitter such that  it is larger than n over 10 elements so  if i consider  what is the most uneven distribution ? what is the size ? well on one side i could get something like a list of n over 10 elements on this side  i could get a list of say 9 n over 10 elements so  this is good because  this list is going to shrink and its going to terminate quickly the height is going to be small over here this on the other hand  might appear to be a problem  because here the height has not reduced that the size has not reduced if the size has not reduced  then it will keep on going in this manner and may be the height of the tree may be large but  what we argued was the work done in this algorithm is  the height of the tree is at most the height of the tree multiplied by n because  n is the work at each level  refer slide time  19  23  so  let us see what happens ? so  in the first level as we have pointed out  the largest problem size will be 9 n by 10 it could be smaller than that so  it could be say half half but  that is actually not so bad that means  the third tree height will be actually small so  this is trying to force the tree height to be large and therefore  it is trying to force quick sort to take large  to take long time so  we are sort of looking at we said that we are looking at neither the best nor the worst cases but  we are sort of erring on the side of the worst within this region so  in the first level problem  largest problem level size is 9 n by 10 what happens next ? again  we assume that the problem will split in the ratio 1 is to 9 so  this will become say something like n by 100 and 9 n by 100 this will become something like 90 n by 100 and 81 n by 100 so  as you can see this rightmost branch will keep on having the largest size so  what will happen ? at each in each step  the size of the largest problem drops down by a factor 9 by 10 and therefore  we can conclude that log of n to the base 10 by 9  the problem size will even on this right most branch will drop down to 1 and even to do that  i will take log of n to the base 10 by 9 levels so  this is good news in the sense that  even when i am looking at a split which is lopsided  refer slide time  21  25  the number of levels  the height of the tree is still going to be about log well  it is going to be log not to the base 2 but  to the base 10 by 9 and let me just remind you that  log n to the base 10 by 9 is simply equal to log of n to the base 2 divided by log of 10 by 9 to the base 2 so  this is still only a constant and therefore  this is still o of log n so  the height given in this case is o of log n  the height of the tree the tree height is of log n and therefore  the total work is n log n  refer slide time  22  08  so  even in this middle case we have seen that the total work is about n log n so  that is the sort of the first intuition as to why quick sort should work ? quick sort may be works while in practice because  unless the splitter comes from too large or too small  the two sub problems that we create will be reasonably balanced and not too lopsided and if they are not too lopsided  then the height of the tree height of the recursion tree will not be too large next we are going to actually do sort of a very systematic analysis  of the average time taken by the quick sort we are going to do this in two ways in one way  we are going to derive the recurrence and we will not really solve the recurrence  but i will indicate to you how that recurrence could be solved and it will turn out that  the solution of the recurrence is n log n and  then i will indicate somewhat more elegant way using  which we can also derive n log n  refer slide time  23  22  so  when i talk about average case  i have to define what are the possible inputs ? so  in this case i am going to assume that  for this particular analysis i am going to assume first of all that all the inputs are distinct all the inputs  the numbers the elements the keys which are given to us are all distinct and if they are all distinct  i might as well assume that they are integers 1 through n for each of the n keys but of course  they will not be given to me as 1 through n  but they will be given to me as some permutation of 1 through n so  now i will state exactly what my allowed inputs are so  my allowed inputs are any possible permutation of the integers 1 through n so  there are n factorial possible permutations there are that many input instances for my algorithm so  my question will be  what is the average time taken by my algorithm over all these input instances or over all these permutations ? and of course  i would like you to express it as a function of n so  now i am going to express i am going to look at our analysis and i am going to figure out  how we can estimate this so  although i have been talking about different  about taking averages i can also think of this in terms of probabilities so  i can think of this as follows so  i have been given a set of input instances i have constructed a set of input instances and i am picking one of those instances at random and i am doing this  giving equal probability to every input instance so  there are n factorial instances possible each one has equal probability or in other words each one has probability whenever we assign so i am picking one of those and i could also be asking under this choice  what is the expected time for that for the instance that i pick ? which is of course  the same thing asking what is the time taken  what is the average of all the times ? so  now this average can be estimated by grouping the instances into separate groups and  then calculating the average within each group and then multiplying by  essentially by the size of the group or by the probability of picking that group so  here is how you are going to do it so  in the first step of the algorithm  we pick a splitter there are n keys and the keys are going to be numbers in the range 1 through n so  there is going to be some probability that  the splitter is going to be one of these keys it is to be even any one of those keys so in fact  let us assume that we always pick a splitter at the first element which is in fact  what the algorithm did so  in that case the question is so  we are splitting all our input instances into those permutations first in which the splitter in which i appears in the first place and within that group  we are taking the average time so  let me draw this picture out here  refer slide time  26  59  so  here is our set of input instances so  i am breaking it into pieces so  these are input instances which begin with 1 that is  they have 1 in the first place these are input instances which begin with 2 these are input instances which begin with 3 and somewhere over here are input instances which begin with i and of course  at the end there are instances which begin with n so  i am going to pick a group and  then i am going to pick an element from it or i can ask  what is the average time taken for this group ? and if all this groups are identical  then i can just take this average or i will have to wait with the size of this group so  that is exactly what i have done over here so  i have taken the average time for this group which is what is written over here ? average time given that splitter is equal to i but  given that splitter is equal to i is the same thing as saying  that the first element of the list is i so  i am in this region of my input space and since  i want the average over the entire space  i just want to i just multiply by the probability that the splitter is equal to i or the fraction which indicates how many instances are there in this group as compared to the entire group so  this is what i get  refer slide time  28  42  now  what is the average time given that the splitter is i ? well  if we go back to our algorithm here so  i pick a splitter over here then  i am going to have this loop anyway so  if i am solving a problem of size n  i will do n work in any case and  then i will have my inputs split into two lists or three lists but  only two of which will be interesting so  average time given splitter i is going to be o of n for that loop to take  loop to do its work plus the average time for sorting the small set but  what is the small set ? it is the permutation of the elements of integers 1 to i minus 1 and the average time for sorting permutation of elements i plus 1 through n because  that is what quick sort does it splits into groups  it sorts the first group  takes the equal elements in which case in this case there is only one equal element which is i sorts the last group and then concatenates them together so  in addition to sorting the time will require is o of n so  you might require o of n time also for concatenation but  in any case we have written o without actually mentioning the constant and therefore  this is fine or we might have a clever data structure in which case  we do not need this o of n time but  in any case we need the o of n time for the loop so  this is perfectly fine so  now you have the average time for sorting permutation of 1 through i minus 1 and then the average time for sorting permutation of i plus 1 through n here is the important part so  the first time we picked the splitter to be i and then we constructed this group but  the key observation has to be  that the numbers the order in which these numbers will appear is not going to be particularly biased so  we know that within the group that we selected  i is going to appear as the first element since we are dealing with all possible permutations  the other elements would appear equally likely in the first space in this group or in the second space in this group or in the third space in this group so  this group will have all possible permutations of 1 through i minus 1 as well so  if it has all possible permutations of 1 through i minus 1  then the time average time for sorting it will be t of i or other t of i minus 1 it does not really matter  t of i over here the time over here is going to be i plus 1 through n or it is going to be t of n minus i so  i think so  what do we get from this ? well this expression has to be put in over here and as a result we get something like this  refer slide time  32  03  t of n is equal to sum over i of this probability that  the splitter is i there are n choices for i and since we are considering all possible permutations everyone is equally likely to appear in the first place and therefore  the probability that i appears in the first place is just 1 over n so  this is 1 over n and this we just established is this and that is what i have written over here i just remarked that this should have been i minus 1 and that is what i have put in over here now  this recurrence can actually be solved it is a little bit tedious algebraically  but you can certainly solve it by recursion induction since i am telling you that  the solution is n log n so  that will establish that the average case of quick sort is n log is o of log n  refer slide time  33  02  now  we are going to do we are going to consider an alternate method for solving this so  this is going to be much more direct we are not going to write recurrences we are just going to do some interesting counting so  here we will focus on the comparisons performed by the algorithm so  after all the important operation in all of this  is comparison so  if you go back to the loop let us just take a look at that we did other work as well say we added elements into lists but  corresponding to every such operation there is a comparison operation going on as well so  certainly if we bound the number of comparisons  then that will give us a good indication of the time taken by the entire algorithm so  that is exactly what we are going to do so  we are going to estimate what is the number of comparisons performed by the algorithm on the average and we will show  then that is going to be something like o of n log n well  let us first determine what is the maximum number of comparisons possible ? so  the maximum number clearly is n into n minus 1 upon 2 this is  if every key is compared with every other key and of course  if the input is the worst case input  then something like this actually happens but  this will not this will  but if the input is some permutation  then every key will not get compared with other key so  just to see clearly what is going on  i am just going to describe a table which shows what happens for different input instances so  a table this table will have rows and there will be a row corresponding to every possible comparison so  our keys are integers in the range 1 through n and for every i and j  we will have a row so  i compare j that will be the label of that row and in that row  we will have information about whether i and j are compared in every possible input instance and in fact  the columns will be the input instances so  the entries are going to be indexed by two indices  one is i colon j well  this itself is a complicated index and the other is this permutation p so  here for example  is a table of course  i have just made up the entries  just to tell you what this table might look like so  the rows are labeled i colon j so  starting with 1 column 2  1 compare 2  1 compare 3 and so on to n minus 1 compare n so  during the execution whether it is or not 1 is compared to 2  when permutation 1 is input is going to be written out here so  you have left a blank over here and that just says that node  that node will not be compare it is just an example on the other hand  1 and 3 will be compared  when permutation 1 is the input similarly  if permutation 2 is the input then 1 and 2 will get compared  1 and 3 will get compared and may be some other things will also get compared similarly  there will be other permutations for which this will be the pattern of comparison  this will be and so on so  there are n factorial possible input permutation so  we have n factorial possible columns and for each possible comparison  we have a row and their intersection says that  whether that comparison actually happens in the corresponding execution  refer slide time  37  00  the key question is  are there many t cells in this or are most of the cells blank ? what we really want to know is  what fraction of the cells in the column are marked ? or what is the average number of cells which are marked in a given column ? we are not going to answer this question directly we will begin by asking  what is the fraction of cells which are marked in any row ? and interestingly  that will tell us something about what happens in columns as well say  if i go to a particular row of this table or the row which has labeled i colon j  the question that i am asking is  is i going to be compared with j in the first permutation or in the first input instance or in the second input instance in the third input instance and so on  refer slide time  38  12  so  here is the key observation for i to be compared with j either i or j must be chosen as a splitter before  one of the elements between that is elements i plus j or j minus 1 gets splitter so  let me explain this a little bit  refer slide time  38  33  so  here is i here is j and there are some elements in between well  i know i plus 1 i plus 2 all the way till j minus 1 so  these are the elements that i am considering of course  they will not appear in my input instance in this order they will be in my input instance  they will be scrambled up but  i am just thinking of them as sitting in a line now  suppose some element over here gets picked up as a splitter  what happens ? if this element is picked as a splitter  then this element is compared with everything else if everything else is compared with it  then this i will get input in the small list so  i will go into the small list j on the other hand will get input in the large list but  remember that once an element goes into this list and another element goes into another list  there is no question of comparing them subsequently so  if any of the elements in between over here get picked as splitters  before any of these two elements get picked then  we know for sure that these elements will go into separate lists and therefore  they will not be compared on the other hand  before these elements have been picked suppose i gets chosen  what happens then ? well  then i is going to be compared with everything larger than it  or certainly everything which has not been found which is in the current list but  if nothing in this has been selected as a splitter  then this had been better been in the current list and therefore  j will get compared with i and vice versa if j gets picked first  then i will get compared because j will be compared with everything over here so  which means that  these two elements must get split as splitters before these  inner elements are picked so  what is the probability of that happening ?  refer slide time  40  59  so  i claim that probability of i or j being chosen before i plus 1 or before elements i plus 1 through j minus 1 is in fact  2 minus 2 upon j minus i plus 1  refer slide time  41  22  so  here is i here is j so  how many elements are these in total ? these are j minus i plus 1 elements and out of these  the comparison happens only if this is picked or this is picked so  there are two cases which are good out of j minus i plus 1 cases and therefore  that is the probability so  now actually things are very  very simple  refer slide time  41  59  so  the fact that i or j is probability that i or j is chosen  before i plus 1 through j minus 1 is this just tells us something very simple it tells us that the fraction of t s in this row is just this because  that is what the probability is we are going to pick a row at random and we know that  2 upon j minus i plus 1 fraction of the time we get a t or the comparison happens so that means  in other words the number of columns the fraction of the number of columns in which t s appear  is just going to be this much so  what does that tell us ? so  it tells us that the total number of t s in the entire table is going to be sum over all the rows of this multiplied by n factorial let me explain that a bit slowly so  from this what can i conclude ?  refer slide time  43  03  i can conclude that  in row i colon j contains n factorial times 2 upon j minus i plus 1 t s  what t s represent ? where comparisons happen  whether comparisons happen or not but  if i want over the entire table i just have to sum over all possible rows so  this is what i have written out here  refer slide time  43  33  except that the n factorial has taken outside because  it does not depend on what row i am looking at well  this expression can be written out slightly differently so  all possible labels i j  i can now classify as all possible levels in which j is a second element and  then the first element has to be smaller and therefore  it is sum over i is less than j of this expression but  what is that  so summation over i of i less than j of this expression  well what is the first term ? so  i begin from 1 and so  first term is simply 2 upon j and next term is 2 upon j minus 1 and so up on until 2 but  what is this ? so  this is let me write it down again  refer slide time  44  37  it is 2 upon j plus 2 upon j minus 1 plus all the way till 2 upon 2 or written differently  it is 2 times 1 plus half plus one third all the way upon till 1 upon j and this we know simply l n n by treating this sum to an integral  converting it to an integral so  this is a good estimate or in fact  this is an upper bound  refer slide time  45  13  so  finally we have this whole thing as n factorial times sum over j of o of l n j but  if you are going to take the sum over j  what do we get ? well  we get n l n and n so  we get n l and n over here  what is n l and n ? so  we have total number of t s in the entire table list n factorial times n l n n so  what then is the number of t s per column or what is the average number of t s per column ? well  how many columns are there are there ? there are n factorial columns and therefore  we divide this total number by n factorial and then we get o of n l n n so  average running time is o of n l n n and why is that ? because t s represent the number of comparisons  and we said that the average that the time is in fact proportional to the number of comparisons so  the average running time is going to be o of n l n n but  o of n l n n is simply o of n log n as well so  here the base was the natural base was e or this was the natural logarithm here the base is 2  but that does not matter log of n and l n of n are within a constant factor of each other so  let me conclude so  i would just like to say that  a similar idea works for selection as well so  suppose we want to select the rth smallest element  then something like this will also be fine thank you design and analysis of algorithms prof abhiram ranade department of computer science engineering indian institute of technology  bombay lecture  23 bipartite maximum matching bipartite graph matching in the course on design and analysis of algorithms let me begin with a motivating example  refer slide time  01  01  so  in this example we have some j jobs and some c candidates the idea is that the each candidate we have given we have been given the list of jobs that they candidate can do and we also have been given a constraint the constraint says that  each candidate must be given at most one job and also each job must be assigned to at most one candidate the goal is our obvious goal assign candidates to jobs such that  maximum number of jobs are filled the bipartite maximum matching problem is exactly this  in more mathematical terms so  i am going to first define this problem and then  i will relate it to this job and candidate problem  refer slide time  01  59  in this problem  our input is a bipartite graph so  let us call it g g is composed of two vertex sets u and v and the cardinality of u says n 1 and the cardinality of v is n 2 and e is the set of edges let me remind you  what a bipartite graph is a bipartite graph is simply the graph  in which the vertex set is in two parts u and v and the edges only go between u and v  refer slide time  02  31  here for example  is a bipartite graph so  this forms this set of vertices forms u this set of vertices forms v and as you can see the edges  only go from some vertex in v to some vertex in u to some vertex in v specifically  there are no edges which connect our text in u to another vertex in u or our text is v to another vertex in v  refer slide time  03  00  so  let me now define what a matching is a matching is a subset of the edges such that  at most one edge is incident on any vertex either in u or in v  refer slide time  03  15  so  in this graph for example  the set of blue edges would be a matching if i add one more blue edge as here  this would not be a matching  because we would have a conflict at this vertex 7 so  here we would have been having two edges of the matching  of the so called matching being incident and that is not allowed  refer slide time  03  38  so  now let me tell you  what the goal in this problem is or what we require to output so  we are required to output a matching of maximum possible size a size of the matching is defined as the number of edges in it  refer slide time  03  54  in this graph for example  we have this blue matching which consists of three edges so  its size is 3 this however  is not the maximum sized matching  refer slide time  04  06  this red matching for example is the maximum size matching and in fact  it contains 1 2 and 3 and 4 edges maximum matchings are not unique  in this graph itself for example  this is another maximum matching  refer slide time  04  25  let me now relate  this matching problem to that of our candidates and jobs problem so  this first set of vertices which we call u  can be thought of as the candidates so  think of 1 2 3 4 5 as being the candidates and the jobs are represented by this second set  which we call v we draw an edge from a vertex over here to a vertex over here  if the corresponding candidate over here can do this job so  these two edges for example  represent the fact that candidate 1 can do job 6 as well as job 7 so  in fact the list ally that i have mentioned will contain 6 and 7 in the list of candidate 1  refer slide time  05  21  a matching now is simply  an assignment of jobs to candidates so  for example here is a matching which says let us assign to candidate 1 job 6 to candidate 3 job 7 to candidate 5 job 9 of course  we would like to maximize the number of jobs assigned so in fact  instead of this we should really be taking one of the red matchings that  we saw earlier  refer slide time  05  51  here is the outline of my lecture so  i am going to begin with an algorithm design idea and we will see few more ideas and refine them then  we will come to the notion of augmenting paths this turns out to be a very important concept  in this whole matching area that will understanding augmenting paths  will lead us to reasonably clean simple high level algorithm it is correctness depends upon something called berge ? s theorem so  we will state and prove that theorem next and then  finally we talk about how to efficiently implement this algorithm and we will estimate its time  refer slide time  06  35  so  let us start with a really simple winded idea how would you find the largest size matching ? the moment you talk about larger size perhaps  the most natural thing to consider is the greedy idea so  here is the strategy so  we look at all the edges and maintain a set called m and we keep on adding edges into m  till no more edges can be added of course  as we add edges into m you have to make sure that there are no conflicts or there are no two edges which ever are incident on the same vertex  which are placed in our set m  refer slide time  07  19  in this case for example  if we tried this idea what would happen ? well  we would start with this edge and add it no problem then maybe we will try to add an edge going out of 2  but this edge can not be added because it would produce a conflict at 6 so  we take the next edge so  we add that one then  we try that this edge over here this edge also can not be added because  it will produce a conflict over here then  we try adding this edge that also can not be added  because it would produce conflict over here and then  we try to add an edge out of 5 and say for example  we end up adding this edge after this  you can see that no more additions are possible have you reached the best possible matching ? no  because we know that there exists a matching in fact  any one of those red matchings which we saw earlier which have four edges in them rather than just the three edges  which we have found over here  refer slide time  08  24  so  we clearly need something better than the greedy idea so  here is what i call the kho kho idea so  if you have played this game of kho kho  you might remember that in that game there is a chaser who keeps running but  who occasionally goes and knocks on the back of a person  who is sitting down and then  that the person who was originally sitting down starts running and starts chasing the members of the opposing team until he or she again goes behind and knocks in the back of another person  who is sitting and who takes over so  this is the idea that we are going to explore so  i need a definition for that the definition is that of a free vertex so  given a matching m i will say that the vertex is free  if it is not an end point of any edge in that matching so  we will take an example on this shortly but  let me state that idea first so  the idea is something like this so  suppose we have a free edge then  that indicates that maybe we can augment or we can increase the size of our matching may be by throwing an edge by considering an edge out of that vertex  and may be trying to include that edge include that matching that we have found so far this may succeed if it succeeds great then  we have a bigger matching if it does not succeed  well how will it not succeed ? it will not succeed because  may be it conflicts with another edge which is already present so  this is where the kho kho idea comes so  this new edge that is present the old edge that is present will get knocked up and the new edge will sort of set in our matching but  now once we knock out an old edge  what will happen ? well  the other end point of that will become free and now we will try to match that and we will try to do on this  until we find that there is no conflict and if we succeed then  maybe we have increased the size of our matching a little bit so  let us try it out  refer slide time  10  33  so  the first thing to check is which are the free vertices ? so  in this case vertex 2 vertex 4 and vertex 8 are free because  they do not contain incident on the any edge in the matching  edges in the matching are shown in blue over here so  let us start with vertex 4 and let us try to see  what happens if we try to match it somewhere well  4 can only be matched to 7 so  i have indicated that this is our candidate for inclusion into our old matching if we try to include this  what will happen ? well  that will have there will be two vertices two edges incident at 7 so  essentially we will have a conflict conflict just means that  there are two edges incident in a matching we just want one edge to be incident or at most one edge to be incident well  if there is a conflict what do we do ? well  we are going to remove this edge  refer slide time  11  35  so  let us do that so  i am going to use the color code green  the color green to show edges  which were in the matching before we started this entire procedure  but which we have just removed remember that yellow edges are the ones which we just added so  they were not in the matching before  we started this procedure but  they have just come into the matching green edges are the edges  which were in the matching earlier but  now they have gone out of the matching if this edge from 3 to 7 goes out  what happens ? well  vertex 3 becomes free vertex 3 does not have an edge in the matching attached to it in longer remember this edge is just gone out of the matching so  now we try to match 3 well  we could try and match it back again to 7  but that seems foolish we just removed this edge so  we should not try to put it back again so  let us try to do something else so  the only else the only other thing that we can do is add an edge 3 to 9  refer slide time  12  38  so  let us try doing that so  now this yellow edge this yellow color says that  this edge has entered our matching and it was not there originally well  what does this do ? it produces a conflict of time how do we eliminate that conflict ? well  we remove this edge so  this edge goes out of the matching and therefore  we color it green now  it happens now  vertex 5 is free so  we try to match it  refer slide time  13  15  when we do match it  we match it to 8 and interestingly there is no conflict so  this is the basic step that i mentioned this is the kho kho sort that  i mentioned let us see  what it is accomplished  refer slide time  13  28  so  we ended adding three edges which are the three edges we added ? well  these three this yellow edge  this yellow edge and this yellow edge so  these three edges we added so  now you know why i color those edges very  very carefully so  that i can keep track of what exactly happen ? what happened to the edges earlier in the matching ? well  some of them stayed this edge was there in the matching earlier it state but  this edge which was there in the matching earlier went away this edge which was there earlier in the matching  that also went away but  two edges went away and three edges came in so  now we have a better matching which consists of this edge  this edge  this edge and this edge so  earlier we had three edges in the matching now  we have four edges in the matching now  the colors are going to tell us something more interesting  about what exactly happened and in fact  it makes sense to ask exactly how we traverse this graph in this entire process so  as we executed this process we really traversed a path in the graph so  we started at vertex 7 then  we added edge 4 7 then  we removed edge 7 3 then  we added edge 3 9 then  we added then  we removed edge 9 5 and added edge 5 8 so  that is exactly what happened ? this  this  this  this  and this in fact  there is more interesting pattern out here so  let me show what that pattern is  refer slide time  15  14  let me take this vertex 3 and let me drag it over here so  now you should be able to see  what that pattern is even more clearly so  if i look at the first edge on this graph  it is yellow the second edge on this path is green  the next edge on this path is yellow again its green again it is yellow so  in fact the colors of the edges alternate along this path and that is of course  to be expected why  because we alternately added and removed edges in fact  there is one more interesting thing so  whenever we were adding edges we were going forward in the graph and we were following edges which were originally not in the matching when we removed edges  we were following edges which were originally in the matching but  we were going backwards in the graph and similarly  again we go forward backward and finally forward again so  you see that what we did  the path we traced at this entire procedure has lots of interest in properties and in fact  this path that we traced is so important that it is given a name  it is called an augmenting path  refer slide time  16  28  so  given a matching there is a notion of an augmenting path an augmenting path is a sequence of vertices v 1 v 2 v k such that the first vertex is in the set u in the graph  the left the set of vertices on the left corresponding to the vertex 4 that we started off with and it is a free vertex that is exactly  how we started out the construction the last vertex is also free because  that is what enabled us to discover an extra edge and then  the intermediate vertices well the vertex from v 1 to v 2 was originally in the graph  but not in the matching and that is exactly  what this set e minus m supposed to denote e is the set of edges from that  i remove the set of edges which were in the matching earlier so  this edge v 1 v 2 is now was supposed to be in this set e minus m the next edge on the path however  v 2 v 3 is a backward edge and it belongs to the matching and this edge is the forward edge so  we go forward then  we go backward then we go forward again potentially again we follow an edge  which is not in the matching and we go forward and maybe we go backward again and we do this several times  until we end with this set v that is what a augmenting path is the operation of taking an augmenting path p and a matching m and generating a new bigger matching which we just did  we will abbreviate as m symmetric difference p why is this called symmetric difference ? well  here is the definition of this circle plus operator we will define q circle plus r or q symmetric difference r  as the set of elements in q or in r  but not in both so  in that sense it is the symmetric difference so  it is the sense in which q and r are different from each other  not their similarity  but there is difference so  in fact you can see that the new matching is the symmetric matching is the symmetric difference of the old matching and the path so  let us take a look at that quickly  refer slide time  19  04  so  the old matching was this matching 1 6 3 7 and 5 9 so  the old matching is the blue edges and the green edges the path is this so  the path is the yellow edges and the green edges so  as you can see the green edges form the intersection between the path and the old matching and therefore  they have been removed and what has included in the new matching is just the difference  the symmetric difference between the path and the old matching so  these are edges which were in the path  but not in the matching this is also an edge this is an edge which is in the matching  but not in the path and so  the new matching will just contain this and these  refer slide time  20  01  so  augmenting paths play a big role seem to play a big role when we want to increase the size of our matchings in fact  edmonds who was a eminent computer scientist from  ii guess from the previous century who is one of the who could even be called as one of the founders in some sense of analysis of algorithms so  i am going to describe an algorithm which is credited to edmond ? s so  the algorithm is essentially based on this idea of augmenting paths so  we start off with m which is an empty matching which is an empty set so  there is nothing in it and then  we keep on finding the augmenting path so  we check if there is an augmenting path p for this m  defined in this sense if there is then  we perform this operation and set m to the result we keep on doing this  until we discover that we can not augment our current matching the moment that happens  we stop and we output m so  this is the algorithm does it seem reasonable ? well  yes it seems reasonable we seem to be adding edges if possible  but there is a old problem which we had with our greedy idea as well what if we discover that there is no augmenting path can we stop then really or it is possible that there is some bigger matching which we will find using this idea so  this possibility can be ruled out and this is done by a theorem attributed to berge  refer slide time  21  55  so here is what berge ? s theorem says berge ? s theorem says that a matching m in a bipartite graph is maximum  if and only if there does not exist a augmenting path for m we will prove this in a minute  but i just want to persuade you that  this is exactly the theorem that we wanted so  this theorem says that if ever we come to a point at which we can not find an augmenting path then  we must have in our hands a maximum sized matching so  this theorem justifies or proves the correctness of edmond ? s algorithm so  let us now prove this theorem the proof is in two parts since  this is a if and only if theorem the first part is the only if so  what are we required to prove over here ? if a matching in a bipartite graph is maximum then  there can not exist an augmenting path this should be quite obvious if there existed an augmenting path  what would happen ? well  we could add we could augment that path with our matching we could compute m circle plus p and we would get a bigger matching is if that possible ? no  because we said that m is already maximum and therefore  the only if case is obvious the interesting case is the  if case we are going to prove this case by contradiction so  how does this work ? well so  let us assume the contrary so  let us suppose that m has no augmenting path so  m has no augmenting path that of course  by itself does not make up the contradiction so  we are going to assume that m has no augmenting path and there exists a matching m such that  n is larger than m so  this is the case that we really wanted to watch out with we can not find an augmenting path  neither have we got the best matching so  this is the if case and this is the case that we really want to rule out so  we are worried about such m we have in our hands matching m and we know that there is  we are assuming that there is a bigger matching n somewhere out there well  at this point it is natural to ask  what is the difference between m and n ? so  that is exactly what we do so  we will ask will say  let r be the symmetric difference of m and n and then  let us investigate what this  what the properties of the symmetric difference are i claim that  so we are going to look at this symmetric difference r my first claim is that this r must be composed of paths and cycles why is that ? well  before saying why that is  let me just say that the claim is equivalent to stating that the degree of every vertex in r is at most 2 remember m and n are sets of edges r is also a set of edges and it is over the same vertex set u and v so  i can talk about the degree so  i claim that if r is made up of paths and cycles  it is the same thing that saying the degree of every vertex in r is at most 2 obviously the degree of any vertex in a path is 2  the degree of any vertex in a cycle is also 2 so  if i can prove this i would have proved this well  why is this true ? well  just examine how we constructed r ? we constructed r by taking some edges of m and some edges of n but  note that the degree of every vertex in m and n is at most 1 the edges of m and n are such that  on every vertex at most 1 edge from either m or n is an incident so  even if i take the union forget the symmetric difference  even if i take the union the degree of every vertex in m and n will be at most since the degree of m and n in every vertex is at most 1  the degree of the union will be at most 2 so  from this it follow that this is true therefore  this is same as this here is an interesting fact about r so  it consists of paths and cycles  but the edges in r alternate between m and n why is that ? well  could there be two consecutive edges in r coming from m no  because m has been defined to consist of edges which are incident on vertices but  at most one edge is incident on any vertex so  in m itself two edges are never incident on the same vertex and therefore  when those edges going to r this property will stay around and similarly for n and therefore  we know that the paths and cycles inside r consist of alternate edges of m and n some more properties  what is r ? well r is the symmetric difference so  that is as good as saying that  we take the union and then we remove the intersection so  the size of r is size of n plus the size of m minus the size of the intersection but note that  n has bigger size than m so  we subtract something  but in the end we subtract something we just common to both m and n so  in the end r must have more edges in it from m rather than from n so  we just how to examine the implications of this  refer slide time  28  28  let us look at cycles first each cycle consists of an equal number of edges from both m and n so  if we just look at the cycles this fact can not be explained so  what first happen ? there has to exist a path  which must contain more edges from n if it contains more edges from n  what do we know about such a path ? well  let us take a picture  refer slide time  29  03  let us draw a picture over here so  here is this path it contains more edges from n so  let us say n edges are in black so  if it contains more edges from n clearly  the first edge has to be from n then  from m then  from n then from m and finally  the last edge must be from n so  now what do we know about these vertices ? so  will these vertices have an additional edge either from m or n going out well  if there was a red edge going out then  this could be a path which have been continued we could have considered that itself  but this is the maximal path that we are considering so  there can not be a red edge going out of here there can not be a black edge going out over here  similarly over here so  that means that the first and the last end points of p must also be free in m and what is this this is simply the definition of an augmenting path so  this path p is an augmenting path for this matching m so  we have proved that an augmenting path exists and in other words  this you can improve your path and we started off by saying that  m has no augmenting path so  we have got to a contradiction so  this proves berge ? s theorem so  we have proved berge ? s theorem  refer slide time  30  35  so  we know that augmenting paths are useful and not only useful  but they are sufficient so  the only question that  remains is can we find augmenting paths quickly ? so  what are the properties that we know about augmenting paths ? well  an augmenting path starts and ends at a free vertex it starts at a vertex  say u and ends in a vertex v so  without loss of generality or vice versa  so without loss of generality we can assume that the starting point is u so  we are given a graph and we want to start an augmenting path we want to check whether there exists a augmenting path  starting at some vertex in u if we knew where the path started  we could just try growing the path out of that vertex so  we could try something like depth first search or something like that and try going out from that vertex into the rest of the graph but  we do not know where it starts so  here is an interesting idea so  since we do not know where it starts  we are going to start growing from all the free vertices in u now  we know something more about augmenting paths  which is that the paths have to grow forward using only edges in e minus m e difference the edges which are in e  but not in the matching e minus m and the path must grow backwards using edges  which are in the matching you may do this several times it may go forward and go backward and then  forward again several times  but every time it goes forward it must use an edge which is not in the matching and every time it goes backwards  it must use an edge which is in the matching finally  if we reach a free node or a free vertex in the set v  which is the set on the right side through any paths  whatever we are done so  that is how we are going to grow these parts and if we reach a free vertex  great we are done we would have discovered a path now  it turns out that you can package this set of ideas very nicely as a breadth first search on a new graph well  on a slightly a graph which has been derived from this m and g this graph is going to be very similar to g  but it is going to be crucially and slightly different so  let us take a look at this but let us first take an example first of  how we can do this ?  refer slide time  33  28  so  here is our old graph and here is our old matching so  let me try to see how this idea will work out  on this graph so  the first point was to look for a free vertex  actually a free vertex on the u side we say here two vertices are free  vertex.2 and vertex 4 these are the free vertices so  these are the vertices from which we can start the paths so  we could say for example  that let us grow paths from 2 and 4 going in the forward direction and backward direction as we just described  refer slide time  34  08  instead of that  just to make our description nice and compact we are going to throw in a new vertex  we will call s we will also throw in two edges  going out of s to both the free vertices or to all the free vertices  whatever free vertices there are and in fact  we are going to direct these edges so  earlier graph was an undirected graph this new graph is going to be a directed graph we can do breadth first search or any kind of search on an undirected graph  just as well as on a directed graph so  here we are going to do it in a directed graph so  now instead of saying that we go upon paths 2 and 4  notice that we can just say grow paths out of s  just a single vertex s so  we want to grow out of s and once we do and if we do that  we will naturally hit 2 and 4 which is where you want to go anyway what do we next ? well  we want to grow the path itself and for growing the path  we need to use edges which do not belong to the matching and furthermore  we know that these edges will be used only in the forward direction  refer slide time  35  24  so  a natural way to enforce this constraint is to say  this is the constraint that we want to enforce so  the natural way of doing that is to direct these edges in the forward direction so  every time we grow the paths we know that the edges which are not in the matching will be used  only in the forward direction so  we put a direction on them so  that forces the search to use them  only in the forward direction  refer slide time  35  53  now  what happens when the path the augmented path goes backwards ? well  it uses edges  which are in the matching if it uses edges which are in the matching  it only uses them going backwards so  we put backward arrows on this so  the idea now is  we start over here we keep we go forward we are allowed to go forward then  we are allowed to go forward using edges  is not in the matching we can come back  we can go forward we can come back  every time we come back we need to use edges in the matching every time we go forward we are supposed to use edges  which are not in the matching and finally  we would like to end up with a free vertex in this set v again there could be many free vertices over here and instead of saying  let us end up saying free vertex in this set v here is what we will do ?  refer slide time  36  44  instead of saying that  the path must end at any free vertex we will put a vertex t out here and all the free vertices we will connect it to t in this case  there is one free vertex so  this is what we will connect to t so  now what is the problem that we want to solve ? while we want to ask the question  does there exist a path from s to t in this directed graph notice that by putting directions  we have essentially enforced all the constraints that we wanted on that augmenting path  refer slide time  37  18  so  let us formalize this let us state this algebraically so  we will define this directed graph and we will call it the auxiliary graph  for g and m so  what does this graph look like ? first of all  we will get symbol g prime it is going to consist of vertex set v prime  which i will define in a minute and edge set so  these edges are actually directed so  this edge set is e prime so  what is the vertex set ? v prime is the union of set u in the original graph  the set v in the original graph and these two vertices  which we added the s vertex and the t vertex let us define the edge set e prime consists of these edges  which goes out of s so  just to remind you so  these are the edges which are e s edges e f is the edges  which are the forward edges so  let us go over them step by step e f are the set of edges in the forward direction that is  what this f is supposed to end up so  they consist of the arcs of the form u v  where u belongs to the left hand set of vertices  v belongs to little v belongs to the right hand side vertices of capital v and u v is not in the matching  but it is in the edge set e sub b are the backward edges  the edges which we directed backwards and these are simply all the edges in the matching so  these are all the edges which were not in the matching which we directed forward these are all the edges which in the matching  which we directed backwards so  notice that this is u to v whereas  this is v to u and finally  we are going to put edges out of every vertex  every free vertex in v to the vertex t so  this defines our auxiliary graph so  now comes our main claim our main claim says that  g has an augmenting path for this matching m if and only if  g prime has a directed path from s to t we will prove this  we just point out why this claim is important notice that  we did not have any specific way of finding augmenting paths however  given a graph g prime does it have a directed path from s to t that we know how to find it completely ? that is just simple  breadth first search  depth first search or whatever you like so  this claim would enable us to find augmented paths very quickly and that is why  it is a very significant claim so  let us prove it the proof really goes along pretty much along the lines of the construction  as i explained it in a minute ago  but i will try to explain it formally right now  without reference to a specific graph so  let us look at the only if part first so  the only if part says that suppose g has an augmenting path then  g prime must have a directed path from s to t so  the only part says that let a be an augmenting path for g m then  we will show we must show that s v 1 v k the same path  but in the new in the graph g prime  must be a directed s t path so  i hope there is no confusion because  we are using the same set of vertices  but we have carefully defined everything in this definition and also in the previous graph in the previous picture  we actually use the same set of vertices and we just transformed our original g to g prime by context you should know  when i refer to over vertex whether i am referring to it inside g prime or inside g so  we had given this augmenting path v 1 to v k and we want to show that  this must be a directed path in g prime  refer slide time  42  04  what do we know about augmenting paths ? it starts at a free vertex in u  goes forward and backward several times and terminates at a free vertex in v so  v 1 is a free vertex and the path goes forward from v 1 when it goes forward  we must have an analogous edge in g prime so  all that you need to do is that  analogous edge is present in g prime note that in the forward direction  the path users edges in e minus m this is inside g an augmenting path in the forward direction  uses edges which are not in the matching the path is moving forward  but it is using edges which are not in the matching now  these edges are in fact present in g prime these are exactly the edges they are present in g prime and in fact  they are directed in the forward direction so  if we look at the forward going edges in this  they are all present in g prime and they are all oriented properly they are oriented as per the movement of the direction  movement of the path direction what does the path do ? well  the path can go backward when it goes backward  it uses the edges of m but  these edges are also present in g prime and they are directed backwards so  again this is exactly what we wanted well  there is no coincidence over here  because we arranged it to be this so  in some sense if you followed that example then  this should not be surprise to you at all so  again going back to this  v u is an edge going from v to u and that is how we have oriented it if for all such edges which belong to the matching so  what have we proved then ? we have proved that  this portion which is an augmenting path which is also present in g prime and it has proper orientation so  all that we need to argue is the s to v 1 connection and v k to t connection so  what do we know about s to v 1 so  s has a directed edge to every free vertex in u and of course  v 1 must be a free vertex so  this since it is an augmenting path so  s to v 1 must be an edge in g prime similarly  v k to t also is an edge  because every free vertex has an edge s to t so  that is also present in g prime and thus we have a path in g prime also so  this entire thing is a path in g prime exactly as we wanted  refer slide time  45  14  the if part  the if part says that if g prime has a directed path from s to t then  g must have an augmenting path so in fact  you will see that exactly the reverse of this reasoning will accomplish the  if part also so  let us summarize what we have done so  we have defined this auxiliary graph and using the auxiliary graph  all we have to do is just find the path in it and we get an augmenting path so  we find a path from s to t and we get an augmenting path  refer slide time  46  03  so  now that brings us back to our algorithm so  that brings us back to the algorithm so  we just have to build up on this step of finding an augmenting path  but we know how to do that so  here is our augmenting path procedure so  we construct g prime just as we defined a minute ago then  we find p the path from s to t we can use breadth first search for it or we can use depth first search  it does not matter but  somehow we do it and then  if p is not null then  we delete that as t and return the augmenting path this return path will be used over here to augment the matching so  let us now analyze this so  let us say n denotes the cardinality of u plus the cardinality of v or the total number of vertices let m denote the cardinality of the edge set of the original graph and let us assume that  the graphs are represented in the adjacency list representation in fact  we will keep m also in some adjacency list representation just for the purpose of simplicity of thinking about this whole thing so  what do we know now ? well  we have to analyze what the time for constructing g prime ? and g prime is and then  doing the breadth first search and so on so  here we construct this graph g prime so  how did we do that ? we took g and we took the matching and we took its union  we oriented the edges so  in any case that can be done in time proportional to the sizes of two graphs  which is o of m plus n the next step over here is to find the path from s to t using bfs bfs breadth first search takes time again o of m plus n so  this step also takes time o of m plus 1 so  both of these steps take time m plus n time for this entire procedure  i claim this is going to be o of m plus n  this entire procedure augmenting path that is  because this part deleting s and t can be done in constant time and therefore  this entire thing is just the sum of this plus this  which is o of m plus n so  this takes o of m plus n time so  the only question that remains is  how many times do we augment ? well and how do we do this augmentation itself ? in fact  in m plus n time you can compute m augmented with p also because  that is just going over the graph completely once even that  will take time o of m plus n how many augmentations do we do ?  refer slide time  48  54  well  what do you know about matching size ? the matching size is at most n by 2  since the number of vertices is n and so  the number of augmentations is going to be at most n by 2 so  now we know what the total time is  the total time is n by 2 multiplied by this or in other words  it is o f n times m plus n m is typically larger than n and so  we can write it as o of m n so  that completes the analysis of the algorithm  the description and the analysis of this algorithm  refer slide time  49  29  so  let me make a few concluding remarks we can actually think of this algorithm  as an iterative refinement what does that mean ? well  we have a matching currently and then  can we improve it by making the small change so  an augmenting path essentially allowed us to determine if a small change can be made this is not the fastest algorithm in fact  an m root n algorithm is known not just m n so  m root n algorithm is known  refer slide time  50  01  and in fact  we can define this problem for non bipartite graphs so  that also turns out to be useful very often and as it turns out that  similar bounds can be found for the non bipartite case so  maximum matching can also be found in non bipartite graphs  refer slide time  50  26  in the same time as above  but the algorithm is much  much more complicated and i will stop here design and analysis of algorithms prof abhiram ranade department of computer science engineering indian institute of technology  bombay lecture  24 lower bounds for sorting welcome to another lecture on design and analysis of algorithms the topic for today is lower bounds for sorting let me begin with a very fundamental  very basic question  refer slide time  01  03  suppose  we just designed a algorithm  that takes time f of n to solve some problem  where n is the size of the problem what would we like to know then naturally the question  that we would like to ask is  is this the best possible algorithm or can we do better than this here is the possible result  we will desire we would like it  if we can prove that every algorithm for solving the problem  must take time at least omega of f of n remember  f of n is the time taken by the algorithm that we just designed supposed  we proved a result like this  what could it mean ? would it be a valuable result  well first of all this result if we can prove  it is called a problem lower bound it is called the problem lower bound because  it says something about the problem  it is not really saying anything about a specific algorithm it says every algorithm for solving the problem  must take time at least omega of f of n so  it is a lower bound  then the time taken for any algorithm  which solves the problem now  if we could prove something like this that is  if we can prove that the time taken by the algorithm  is equal to the problem lower bound or is equal to within this omega  or within some proportionality constant what do we know  well if we could prove something like this  we know that we have the best possible algorithm if this equality is exact and we know  we have the absolute best possible algorithm if this equality is approximate  well if it is of the form omega so  we know that this bound  that we have the best possible algorithm to within a constant factor so  this is the main motivation for studying  what are called problem lower bounds  refer slide time  03  05   so  here is what we are going to do today so  we will be having a general discussion  of problem lower bounds then  we will consider the question of lower bounds for sorting so  problem lower bounds  but the problem being sorting regarding sorting i will introduce a model of computation  called the decision tree and then  we will prove problem lower bounds on decision trees  for the problem we are sorting now  you may wonder why do we care about decision trees  is it not the random access machine or the ram model  which we have been defined and which we have been using so far in the course ; is it not  it a good enough model well  it turns out that whatever we do on decision trees  is actually quite relevant to the ram model so  we will see this relevance and then  we will come to average case lower bounds again we will prove average case lower bounds  on decision trees but  again all that will be relevant to the ram as well  refer slide time  04  12  so  let us start with something  which we have already studied  at least a little bit so  we have already defined the notion of putting the lower bound on the time taken by an algorithm so  here is what we said  we said that if we have an algorithm a for a problem p then  we will say it has a lower bound f sub a where f sub a is a function if the time for algorithm a on instances of size n  is greater than or equal to f a of n and of course  we are not worried about every instance  but we are worried about the worst case instance  the worst instance  the instance for which the time is a largest so  always in this course  we have been stressing the worst case bounds or the worst case times  when we measure the performance of algorithms and even here we are doing that so  we are asking  what is lower bound on this time  the time of the worst instance i of size n of course so  it will be f a of n  this is what this means this is what it means to have f a as a lower bound on a problem p  on a algorithm a for a problem p now  of course  it is customary to say that  this inequality holds  only for large enough n so  there is actually a clause over here  which says for all n greater than n naught  where n naught is some number but  let us not worry about too many technicalities   refer time  05  47    now  the typical way we prove lower bounds  is by constructing instances i which take a large time so  if we have an algorithm  we look at where the algorithm is weak so to say and we construct instances  which show that the algorithm must take a long time on that particular instance so  that gives us good value of f sub a  f sub a of n and we have to do this for every instance  every size instance size if we can do this  then we can construct this lower bound function next  we will look at problem lower bounds a problem p is said to have a lower bound f  if every possible algorithm has lower bound f so  this statement has to apply  but it has to apply for all possible algorithms  for this problem p here is the more algebraic statement so  notice that this part  is just similar to this part ; and now we have an additional quantification we are saying that this must hold  even for the best algorithm so  the algorithm which takes the minimum worst case time  must have time bigger than f of n of course  with the best algorithm has time bigger than f of n then  clearly every algorithm will have time bigger than f of n so  this is what it means for f to be a problem lower bound  for this problem p how do we prove such bounds  now we have to construct bad instances instances which show  that the time taken is large but  we have to construct them  not only for all problem sizes  but for all algorithms as well and of course  the instance it need not be the same instance for all algorithms it could be that for one algorithm  it is one instance which is bad for another algorithm  it is another instance which is bad  that is ok but  somehow we must give a construction  which shows that no matter what algorithm you give here is an instance on which it will take a long time we may not do this directly  but at least indirectly something like this has to be proved  in order to prove a lower bound over here  refer slide time  08  21  here are some trivial lower bounds  that we can prove most problems have omega of n lower bound  where n is the instance size i say this is trivial because  no matter what algorithm is used  the algorithm at least has to read all the input the input has length n and therefore  omega of n time has to be taken so  in that sense  there is nothing clever about this and it by and large applies to every problem there could be some problems where it may not apply  but those are not probably very interesting problems  refer slide time  09  06  what about non-trivial bounds ? it turns out that on random access machines ram ? s  proving non trivial bounds is actually very difficult and here are some of the reasons basically when we assert a problem lower bound  we are saying something about every possible algorithm now  on ram ? s  the space of all possible algorithms is really huge ; and it is tricky to analyze so  a single problem may have lots and lots of algorithms enumerating those algorithms or analyzing them in some structured fashion  is often a very tricky business it is tricky  because of some of these reasons  a ram has many instructions ram has many instructions to do all arithmetic  they can be composed to do more complicated things you can take logs  you can exponentiate  you can take trigonometric functions you can do all such things with ram instructions there are also many control flow pattern  there is looping  there is recursion and so if i give you a ram program  analyzing it is pretty difficult and saying something about all possible ram programs is even more difficult so  here is what is typically done so  we define a simpler computational model  which say does not have that many instructions and which does not have that many control flow patterns so  we define such computational models and then  we analyze that it will turn now  that on such models the space of programs or space of algorithms is actually fairly small well  it is never really small  but it is much easier to visualize so  that is what we are going to do next  refer slide time  11  13  so  here is one such model  this model has been introduced in the context of algorithms such as sorting so  it will instructions which are relevant for sorting  but which are perhaps not very useful for other problems  but since typically we will consider this model in the context of sorting this model will actually be a reasonably good model to look at so  here is what the model looks like so  the input to every algorithm is always going to be a sequence of numbers and we are not going to be worried worry about inputting those numbers we will assume that those numbers have already been read and they are already stored somewhere in the model a program for this model is a labeled tree so  every non-leaf node has labels of the form i colon j  where i and j are integers i will tell you exactly what a label means in a minute  but let me just describe the model first each edge also has a label and the label could be any of these relational operators leaf node labels are the values  that are going to be printed so  that is what the structure of a program is  refer slide time  12  40  so  here for example  is a program which will be used for sorting 3 numbers  which can sort 3 numbers i have not told exactly how it sorts 3 numbers  i will tell you that but  i just wanted to give you a picture for the program tree model  that i just mentioned so  as you can see  the label over here is 1 colon 2 this edge is labeled with greater than and so on and the leaf is labeled with 3  1  2 which is what it is going to output  as per the execution model  which i will describe next   refer time  13  17   the execution in this model begins at the root  at any node labeled i colon j input x i  which came over here is compared with input x j the result of this comparison is some relational label  so it is either say actually its either less than equal to or greater than and based on that result  the execution follows the appropriate branch so  say for example  the result is less than then  we will look for branches having either the label less than or the label less than or equal to branch on that we will require that only one out of less than or equal to and less than be a label present  on the outgoing edge this is because  we want our algorithms to be deterministic so  once we make the comparison  we want a unique path to follow outwards when the execution arise at any leaf  the label of the leaf is output so  let us try this out for this program  that we have drawn over here so  as i said this program is going to be used for sorting 3 numbers so  let us say the input instance is x 1 equal to 20  x 2 equal to 30  x 3 equal to 10 as we said the execution starts by looking at the root by starting at the root at the root we compare x 1 with x 2 so  in general if the label is i and j  we compare x i with x j so  when we compare x 1 which is 20  with x 2 which is 30  we discover that x 1 is smaller so  if it is smaller  then we follow this branch so  we follow this branch and we arrive at this node and when we arrive at this node  we have to act according to the instruction  represented by this node  which is to compare x 2 and x 3  x 2 is 30  x 3 is 10 so  therefore  we find that x 2 is bigger than x 3 and therefore  we follow this branch then  we perform the instruction represented by this node  which requires to compare x 1 and x 3 so  x 1 is 20  x 3 is 10  x 1 is bigger so  we arrive at this node and at this node  we just output so  we output 3  1  2 which is representing our conclusion  that x 3 is the smallest  x 1 is the next smallest x 2 is the largest and indeed you will see  that x 3 is 10 which is the smallest of 10  20  30 x 1 is the next largest and x 2 is the largest so  at least for this instance  we have checked that this decision tree  which represents a sorting program has in fact  correctly sorted this input instance notice that if i increase the size of my input    refer time  16  44   i will have a different program i will have to have a different program this is unlike what we usually do in a ram model but  that is we are going to allow that in this model  to complete the discussion of the model  let me just mention that the time taken by the program is simply the number of comparisons performed for each instance the worst case time is equal to the length of the longest path  in the program tree because  if in any execution you follow that path  that is the time you will end up setting likewise  we can also define average case time so  here the input instances will be chosen randomly with equal probability  from all possible input instances or we are just asking for an average time over all possible input instances and therefore  we should consider all possible root to leaf paths and it is simply the time for each path is  it is length and so the average case time  is just the average root to the leaf path length so  notice that  the time taken has a very nice graphical interpretation in this model so  now here are the main claims  that i am going to make  refer slide time  18  03  the first claim is that sorting will take time n log n in the decision tree model this claim has to be understood properly this claim says  that no matter what algorithm you use and by algorithm we mean no matter what tree you use because  that is the space of all possible algorithms  no matter what we use that time taken will be n log n the length of the longest path in the tree  will be at least n log n so notice that  this is already a non trivial bound  because a trivial bound will be omega of n we will prove this in a minute  but i first want to relate all this to a ram model because  ram is after all what we use every day so  a definition first  ram sorting algorithm is said to be comparison based  if the only operations it performs on keys are comparisons and copying so  if you go back and think about the sorting algorithms  that you have seen there is a good chance that most of them are in fact  comparison based take heap sort for example  the only operations you do on keys  are compare to keys and maybe you copy that  similarly for merge sort  similarly for insertion sort similarly  for shell sort  if you have studied that so  in fact  most of the algorithms  that you have studied probably are comparison based algorithms and it turns out  that they are actually quite nicely connected to decision tree algorithms so  here is the claim  let a be a comparison based ram sorting algorithm then  there exists a decision tree sorting algorithm c that performs the same key comparisons  as a on every input instance in fact  in the same sequence  this means that the time on this decision tree model  is intimately related to the time on the ram model also for this class of algorithms the final claim says  every comparison based sorting algorithm on the ram  must take time n log n so  this is where we have finally  come back to the ram so  here is a non trivial result not about all sorting algorithms  but for comparison based sorting algorithms for comparison based sorting algorithms  we have proved that the time taken must be at least n log n well  we have claimed and we will have proved this in a minute so  let us try proving this first this this in fact  turns out to be the main  the crux of the matter so  we want to prove that sorting takes time  omega of n log n in the decision tree model  refer slide time  21  04  so  we are going to start with a certain input x remember x is that sequence x 1  x 2  x 3 and let us suppose  we have a sequence x which is the permutation of the sequence 1 through n permutation is just the rearrangement of that sequence so  x is pi of this so  what can we conclude from that  well then 1 to n must be pi inverse of x so  if i apply the inverse permutation on both sides  i will get pi inverse over here  and 1 to n over here so  notice that when we do the sorting  we actually are supposed to compute this inverse permutation so  every leaf the answer  essentially must be pi inverse 1  must be pi inverse so  essentially pi inverse must appear as a label of at least one leaf  essentially  because that leaf must have somehow identified this permutation and only then  can it name how this permutation can be unscrambled to get 1 to n so  pi inverse must appear as a label of at least one leaf now  this holds good for all possible permutations not just a particular pi  how many permutations are there  well there are n factorial permutations and therefore  there are n factorial leaves so  this is the key insight we have proved that this tree  the algorithm tree must have at least n factorial leaves each node of the tree can only have at most 3 outgoing edges well there are so many relational operators which we mentioned but  we can not use them simultaneously  as i mentioned earlier   refer time  23  15   we can only have  so here is a node if we have less than here  then we can not have less than or equal to over here we must have something which is disjoint from this so  the only possibilities are we have greater than or equal to  in which we can have only 2 labels only 2 outgoing edges or we could have 3 outgoing edges less than  equal to and greater than so  at most 3 outgoing edges are possible now  each tree node can only have 3 outgoing  if it can only have 3 outgoing edges then  the height must be log of the number of the leaves to the base 3 the height of a ternary tree  is log of the number of leaves to the base 3 at least and therefore  height must be log of n factorial to the base 3  because n factorial is the lower bound on the number of keys number of leaves  turns out however that if you are using only permutations of this  then we will never pass through this equality edge so  which means the n factorial leaves must be accessible only passing along the strict inequality branches so  which is as good as saying that  if our input is permutation of 1 to n then  we are only considering a sub tree whose degree is at most 2 and therefore  the height is at most log of n factorial to the base 2 but  log of n factorial to the base 2  can be now thought as to expand out the product you can see that it is this and this is actually easy to see  so let us do that so  n factorial is n times n minus 1 times n minus 2 and so on somewhere in this  we are going to have n upon 2 and then  there are going to be some more terms over here but  all these numbers you can see  are going to be at least n upon 2 and how many such numbers are there well  there are n over 2 numbers each of which is at least n over 2 so  therefore  n factorial is bigger than n over 2 to the power n over 2 so  what we have now is log of n factorial to the base 2  is at least log n over 2 to the power n over 2 well  we can simplify this and we get n over 2  this is equal to n over 2 log of n over 2 and therefore  this is omega of n log n   refer time  26  04    so  what you have done  so far is that we have proved this now  we want to look at the next claim the next claim is the one which relates comparison based sorting algorithms in the ram  to decision tree sorting algorithms so  the claim says that let a be a comparison based sorting algorithm then  there exists a decision tree sorting algorithm c  that performs exactly the same key comparisons as a on every input instance so  we are going to prove this and the proof is actually going to be constructive so  what we will do is  we will assume that we have been given this algorithm a which is a comparison based ram sorting algorithm and then using that we will construct a decision tree sorting algorithm c and we have to make sure  that c performs exactly the same key comparisons as a does on every input instance  and also in the same sequence  refer slide time  27  22  so  this is what we are going to do  we are going to construct c from ram algorithm a so  let me remind you what c looks like so  c is going to be some root node   refer time  27  37    may be some branches may be nodes over here and so on so  if i tell you how these things are going to be labeled  then i am done so  let me tell you the edge labels right away the edge labels are going to be less than over here  equal to over here greater than over here so  let us keep it really simple and similarly subsequently  but i have to tell you what the label here is going to be so  the question is how do i do that  so it is going to some i colon j but  i have to figure out what i colon j  it is going to be and here is the key insight in figuring this out the main idea is that which keys are compared first by a  is our ram algorithm does not depend upon the key values  why is that ? well  a is going to read things and then  it is going to compare it is not going to look at the keys and base it is decision on which keys to compare on the value of any key because in fact  value of any key  does not really come into the control structure of a  other than in the comparisons that a does so  if a does a comparison  that is the only time when a can actually peep inside a key value so  clearly the first time that a peeps inside a key value  is the first time a does a comparison and in fact  that which keys it compares is going to be the same no matter what the values of keys are so  i j can be determined by examining the algorithm a we just look at the code of a and will know which key is going to examine first so  whatever they are we can write that now over here  i colon j we find actual numbers i and j and we put them down over here so  i claim now that whatever numbers i and j be put down over here they will be the same  no matter what the values of key are and that is what i just explained so  next we want to find the labels of say this node over here  to do that here is what we consider so  suppose i is an instance  in which x i is less than x j now  we are going to examine the program of a to determine  which keys are compared in the second comparison for this instance i suppose  x k and x l are compared so  those values  those k and l  we will put down over here so  now what is the idea ? so  for instance i what will happen  the first comparison will be this then of course  in the decision tree  this will be the branch taken and the second comparison will be this so  instance i  in fact in instance i  the first two comparisons have been the same in the decision tree as they have been in our algorithm a  so far so good so far we have built this part of our tree and indeed in this tree the decision  the comparisons made on this instance  are the same as comparisons made on the instance  in our ram model also but  let us consider another instance i prime  what will happen in that so  if we consider another instance  i prime in which x i is less than x j what will happen  which keys will be compared next i claim that even in that case  the keys to be compared will be just be these x k and x l  why is that ? so  let us examine this    refer time  32  06   so here is an execution of i and here is an execution of i prime so  somewhere we start execution so  we compare x i versus x and we know that the first comparison is the same in both and so we compare x i versus x j then  we do some instruction over here and we compare x k versus x l so  when we do this comparisons  the question is what will happen over here i claim that if in these instructions  no key is even touched then  exactly the same instructions will be executed over here why is that  well the control flow can change  only if there is a comparison  and if the comparison has a different result so  in this entire part  there has been a comparison with keys  but the result over here is the same as the result over here because  in both cases we said that x i is less than x j so  during this entire portion and this entire portion  wherever there have been comparisons the comparison outcomes have matched and therefore  the instruction which is going to be executed over here  is going to be exactly the same and therefore  we will also compare x k and x l over here so  which means even for this instance i  i prime k and l  x k and x l will be compared and therefore  we can put down this label of the node as k compared to l now  we can generalize this  refer slide time  33  46  so  the general pattern is  we have constructed some super tree of c  how do extend it so  we have constructed something like this  and so in general we might have had constructed some path and then  we want to extend this path outwards  how do we do that so  here is the observation  the label of a node can be determined by considering an instance  consistent with the labels on the path till the node so  i want to determine the label of this node  how do i do it well  i look at instances on the path from the root  i look at the labels on the path from the root to this node and i construct a instance  which is consistent with these labels so  which means that x i must be equal to x j and then  what ever the label is over here the corresponding keys here must satisfy this label and so on and then  we simply look at the algorithm for a and ask what is the next comparison  that is going to be made and we put down those labels over here so  we have constructed an instance  which is consistent with the labels along this path and we look at the algorithm and decide  what is the comparison that is going to be done by the algorithm in that instance so  if the comparison is between some x p and x q we put down p colon q over here   refer time  35  21    so  exactly like what we did in this case  but just more general so in fact  in all such instances  which are consistent with the labels along this you can argue exactly in the same way  that x p will get compared with x q and that will fix the label of this if it turns  that the information which has been gleaned along this path  is enough to determine the final sorted order and because of which the algorithm does not do any comparisons at all then  well we simply take that sorted order and put back the label and name and make that leaf now  the important point is  that we have constructed our decision tree algorithm in this manner and by construction  because of the care that we took in the construction you can see that c performs  exactly the same key comparisons as a does and exactly the same sequence on every instance   refer time  36  31   so  this finishes this claim so  now i want to argue that every comparison based sorting algorithm on the ram  must take time n log n but  this is actually fairly straight forward  refer slide time  36  41  so  suppose some comparison based ram algorithm  takes time t of n  what do we know about comparison based ram algorithms  by the claim that we just proved we know that there must exist a decision tree algorithm  taking time at most t of n but  the first claim tells us something about decision tree algorithm the first claim says that the time taken by the decision tree algorithms  must be at least n log n so  t of n had better be at least n log n  but then this t is the same as this t and therefore  comparison based ram algorithms must also take time n log n so  we have proved this claim   refer time  37  29    so  notice that in a single claim  we have proved a lower bound on a variety of sorting algorithms  heap sort  merge sort  shell sort  insertion sort  and many other algorithms  which we have not even though about but  which only if they are comparison based  then this lower bound will apply to them  refer slide time  37  57  next we turn to average case complexity so  basically we are going to ask the question  what is the average case  can we put a lower bound on the average case complexity so  on the average time to sort  not just the worst case time so  this actually first follows fairly easily  just based on structures  on some properties of trees so  here is the first claim  if a tree has k keys  then there exist at least k over 2 keys at a distance log k over 2 why  well there can be at most square root k leaves at a distance  just like worst case we can use similar arguments  to prove bounds on the average case complexity of sorting as well  on the decision tree model of course and of course  that will imply some lower bounds  on comparison based sorting algorithms as far as the average case complexity is concerned and these bounds follow very simply from some properties of trees  with respect to the relation between the height of the tree and the number of leaves so  here is the first claim  if a tree has k leaves  then there exist at least k by 2 leaves at a distance at least log k by 2 from the root well  here is the proof  there can be at most 2 to the   refer time  39  45   l leaves at a distance l  which means there can be at most 2 to the power log k  whole thing upon 2 leaves a distance l and this is nothing but k to the half  square root of k so  that is what is claimed over here the rest of the leaves have to be at larger distance and in fact  this says that as many as k minus square root k  have to be at a larger distance  whereas what is claimed over here is just k by 2 so  the claim is very easily proved in fact  we can prove something stronger but  once we have that  we can prove the following  we can prove that the average path length average is taken over all leaves  is at least log k upon 4  why is that ? let us count what the total path length is the total path length  we are going to calculate first for these leaves which we talked about so  there are k over 2 of them and each has a path of length log k upon 2 so  the total path length just for these k over 2 leaves is at least this much so  now the average is going to be a k th fraction of this and therefore  it is simply going to be this k factorial drop out  this 2 and this 2 will give us 4 so  it is going to be log k over 4 from this it immediately follows that  the average time for sorting on decision trees  is n log n and here is why  every algorithm tree has n factorial leaves thus average path length is bigger than this claim  log of the number of leaves or the number of or log of n factorial upon 4 but  log of n factorial upon 4 is n log n and therefore  average time for sorting on decision tree is this much and this means that the average case time  for all comparison based sorting algorithms say heap sort  merge sort  shell sort all of these is n log n so  when we did  when we studied average case complexity of quick sort we proved that although the worse case complexity for quick sort was n square the average case was n log n so  in a similar manner  it may be conceivable  that heap sort may have worse case complexity of n log n but  it is average case complexity is just n  it might have been conceivable but this result shows that the average case time for heap sort  is also going to at least n log n and similarly for merge sort  shell sort and any other comparison based sorting algorithm  refer slide time  42  51  so  now i want to conclude and here are some remarks first some implications of the lower bound  on comparison based sorting algorithms in the ram so  here is an important application so  if i want to sort faster than n log n  what do i know  well i know  that i must use some other operations besides just comparisons if i limit myself to just comparisons what happens well  there is a decision tree algorithm  which mimics the ram algorithm and therefore  i am stuck to a lower bound of at least n log n so  i must use some other operations besides comparisons  are there such operations there actually are ?  refer slide time  43  46  so  in fact a popular sorting algorithm called bucket sort  does exactly this the main operation it performs  is that uses the key values  as an index into an array  which was not allowed in comparison based sorting algorithms so  here is the main claim  if the keys are integers are 1 to r  in the range 1 through r then  sorting time is at most n plus r so  i will just give the algorithm as the proof so  first we are going to build an array of buckets b 1 through r so  these buckets will be initialized to null so  that will take about r time then  we take key x i and we insert it into this bucket b of x i so  b of x i can be compute in constant time so  in constant time  we keep a list over there and we insert this key into that list and we do this for all the keys  for all i so  now all keys are now into their buckets now  for i equal to 1 to r  we go visit each bucket in turn and we print out the contents  clearly we will print out the contents in increasing order what is the time taken  well the insertion time is proportional to the number of keys so  because of this statement  the time taken to initialize the array of this b array  is going to be r and printing out the answer  well we have to visit every element of the array and we also have to visit every key  which has inserted in some bucket so  the time taken is going to be o of n plus r so  the total time is going to be o of n plus r and this is going to be faster than n log n  if r is small in fact  this is not the only range  if you want some bigger ranges say 1 through r square is slight modification of this will in fact  do the job   refer time 45  56   the second concluding remark that i want to make  is that not only sorting but  you can use the decision tree model to study several other problems so  the other problems are say merging and this will figure in the exercises  which i will show later and also things like finding duplicates and this is going to be the topic in the next lecture in fact  let me point out  let me say in conclusion that in fact  we can extend this decision tree model so  i want to go to my second concluding remark  which is to say that the decision tree model  can be used to study other problems as well  not just sorting so  in particular it can be used to study merging problems  which i guess you might think as very similar to sorting it can also be used to study problems  like finding duplicates or it can also be used to study problems  such as putting lower bounds on the time to calculate the intersections  between two given sets now  this problem  might seem like rather different from sorting and it is perhaps somewhat surprising  the decision trees can be used for this nevertheless  they can be used and in fact  we will see the finding duplicates example in the next lecture i will also state that the idea of decision tree  is the general model of decision tree is can be extended somewhat so that  rather than just allowing comparison  you can also allow some arithmetic incident and so the lower bound model can be made somewhat stronger in any case  we have seen some lower bound results and we will continue this with the next lecture thank you design and analysis of algorithms prof abhiram ranade department of computer science engineering indian institute of technology  bombay lecture  25 element distinctness lower bounds welcome to the course on design and analysis of algorithms our topic today is element distinctness lower bound this will be the continuation of the previous lecture  which was on sorting lower bounds let me start by defining the problem so  the problem is as follows  refer slide time  01  11  you are given as a input  a sequence of numbers let me call the entire sequence x and the individual numbers in the sequence are x 1  x 2  x n we are supposed to output a yes answer  if all the x are distinct and otherwise  if some x i equal to x j some two numbers x i and x j  distinct two numbers x i and x j are identical then  we are supposed to output and no answer a no answer means  that no all the elements are not distinct what we are going to prove today is that  in the decision tree model  which we talked about last time and which i will quickly define this time also just for continuity we will prove that in this decision tree model  the time is going to be n log n so  this is a non trivial lower bound  in the sense that it says that you need to do a little bit more  than just examine the numbers  which would just take omega of n time it should be quite obvious to you  that if you are allowed to use n log n time then  element distinctness can easily be solved how ? well  in n log n time we can sort all the numbers and now  if two numbers are identical  they are guaranteed to come next to each other in which case  we simply have to compare adjacent numbers after sorting and that will enable us to find out  whether all the numbers are distinct are not so  in summary in n log n time  we will be able to actually compute  whether the numbers are distinct however  the subject of today ? s lecture is not that the subject of today ? s lecture is to prove  that at least n log n time is needed so  we want to prove a lower bound  refer slide time  03  03  here is what i am going to do today ? so  today i am going to talk about  the decision tree model i am going to go through this rather quickly because  it is going to be very similar to what we did last time last time we looked at lower bounds on sorting and we introduced a lower bound technique so  i am going to explain  why that lower bounding technique does not work  that seem like a very nice technique but  it will turn out that  that does not really work for the problem  which we are looking at today then  i will talk about a new lower bound technique  which works for this problem of element distinctness and then  we will prove the lower bound that i mentioned finally  i will extend the model so  instead of the decision tree model  i will define a more powerful model i will call the algebraic decision tree model and i will talk a little bit about it  refer slide time 04  04  so  let me start with the decision tree model so  in the decision tree model  the input is always going to be a sequence of numbers say  the same sequence x 1 through x n and we will assume that these numbers have already been read into the model so  there are no input instructions as such  the inputs are already read a program in this model is a label tree and all non leaf nodes are labeled i colon j  where i and j are integers and these integers have to be fixed as a part of the program leaf node  each leaf node has a label  which just says what is the value to the output ? edge labels are relational operators so  less than equal to greater than or not equal to less than less than or equal to greater than or equal to let me quickly take an example of a decision tree program  refer slide time  05  12  so  here is a decision tree program for sorting three numbers as you can see  each non leaf node is labeled i colon j in this case 1 colon 2 and it is each leaf node is labeled by the answer  that is to be output and furthermore  each edge is labeled with a relational operator   refer time  05  34    execution begins at the root  at each node labeled i colon j  x i from this set of inputs is compared with x j whatever the outcome of that comparison is  the corresponding branch is found with the corresponding label the corresponding outgoing edges are found and execution follows that branch  that outgoing edge so  you go down to the node  which is at the other end point of that edge and you repeat this whole thing  until you get to a leaf when you get to a leaf it is label is the output so  that is the answer that you are going to compute  that you wanted to compute so  two things to be noted so  as we saw in the example  there is going to be a separate program for each input size so  there is going to be a separate program for n equal to 1  for n equal to 2  for n equal to 3 and so on and we saw a program for size n equal to 3  not for element distinctness  but for sorting and then  the other point we noted is  that although we are looking at a decision tree model and a decision tree model is not exactly like our computer  like any of our computers it does in fact  have a connection to the ram model or the random access machine model which in fact  resembles our computers and this relevance has been discussed in the previous lecture and let me remind you what that relevance was so  we said in the previous lecture  that if we have a lower bound in the decision tree model  when it applies to the ram model  not completely but  it applies to comparison based algorithms in the ram model comparison based algorithms are simply algorithms  which compare the keys  they do not perform arithmetic on the keys or they do not use  the keys to induct into the arrays they simply compare the keys and of course  they make copy  refer slide time 07  48  so  here is a quick overview of the sorting lower bound the input to the sorting problem was the sequence x the same sequence which we have mentioned  consisting of components x 1  x 2  x n so  these are all numbers and we want to sort these numbers the key thing to observe in this problem or in the lower bound argument is that there are n factorial possible answers so  either i could say that  this is the sorted sequence or i could take a permutation of this and say that is a sorted sequence i could take every possible permutation of this and that could be my answer every possible permutation could be an answer and there are n factorial permutations and therefore  there are n factorial possible answers this is a very important point  in this argument now  the answer is going to be printed at the leaf of these trees of the program tree  which means that  if you want to print out n factorial different answers then  you must have n factorial leaves  you have no choice because  a single leaf can just print a single answer  it will print out that entire permutation but  that entire permutation constitutes a single answer so  if execution arrives to that leaf  then that is the only answer it can print out so  if your program tree is going to be even capable of printing n factorial different answers then  it had better have n factorial leaves but  if it does have n factorial leaves then  the height of the tree has to be at least log of n factorial  which is n log n and in fact  the height of the tree is the worst case time and therefore  the worst case time taken is at least n log n so  this is a rough sketch of the argument  that we saw last time this idea  that if the answer is n factorial possibilities or if there are n factorial different answers  in the language of information theory  can be expressed as saying that this answer has high information so  whatever we are going to print out as the answer  if you think of it as a variable  it has a very high information content the answer can take n factorial different values so  it has the information quantity in it is high is something like n factorial in fact  information theory measures information as the number of ways  the log of the number of ways in fact  it is going to be log of n factorial  very roughly in any case  bounds of this kind  where we say that there are so many ways in which this variable can take a value are therefore  called information theoretical lower bounds so in fact  you will see the sorting lower bound often refer to as the information theoretic lower bound let us now turn to the element distinctness problem how many answers do we have ? well  the answers are two really  the answer could be yes  which means all elements are distinct or the answer could be no no means some duplicate exist so  we only have two answers  so our answer has only two possibilities so  log of that is not going to be very large it is going to be in fact  exactly one so  if we just say that there are two possibilities and therefore  there are two leaves that does not help us much because  log of 2 is 1 and it just says that  the tree must have height of value  which is really a silly bound so  we need to do something better so  we need to have a new strategy  refer slide time 12  01  so  here is a rough sketch of the strategy so  we will show that there must be at least n factorial leaves again the n factorial is going to turn out be somewhat significant but  we will prove that in fact  there must be n factorial leaves all of them giving yes answers so  this is going to be a interesting argument and we will be making some rather clever use of n dimensional geometry do not be worried about n dimensional geometry  most of the time for the purposes of getting intuition you can visualize  what is going on in two or three dimensions and that usually tends to be enough which is in fact  going to be the case in our proof however  if we want it algebraically write down things  the arguments can get a little bit complicated but  fortunately even the algebraic argument that i am going to show you is going to be rather simple  refer slide time 13  12  so  here is our main claim again  we are going to prove that the time required for the element distinctness problem is going to be at least n log n on the decision tree model let me remind you what this claim means so  this claim asserts a problem lower bound  it says that the time required for this problem  irrespective of any algorithm is this so  it is not a bound on an algorithm well  if you want to think about algorithms  it is a bound on all possible algorithms  in this model of course here is a quick overview of the proof  the proof is a little bit long  but not terribly so the first idea is going to be to interpret our input sequence so  we have our input consists of n numbers x 1  x 2 all the way till x n so  we are going to think of this n tuple as representing  the coordinates of a n dimensional point so  x 1 represents the first coordinate  x 2 represents the second coordinate  x n represents the nth coordinate and the entire thing represents a point and will call that point x we are going to restrict these instances to the unit n cube what do i mean by that ? well  we are going to insist that all the x i 's lie between 0 and 1 we will prove the lower bound under this restriction so  that should not be a cause of any worry if we prove the lower bound under this restriction of course  it works if we do not have this restriction so in fact  if we do not have this restriction  who knows things can get even much worse but  we are not worried about that  we just want to argue that things can certainly get at least as bad as this and so it is to have this restriction here is the first claim  that we are going to make we will use the notation r of l  which stands for the region for l so  the region of l  where l is a leaf is a set of all input instances for which leaf l is reached at the end of execution so  this is the definition of what r of l means  what we are asserting is that this r of l is connected i will explain to you what connected means in just a minute but  it is the usual notion so  a region is connected  if it looks together or if there are two points in it and they can be joined by a path  within that region we will do this a little bit more formally and we will write it in a minute the second claim goes something like this suppose we have a point x whose coordinates are distinct so  the sequence consist of distinct elements or alternately these coordinates are distinct now  we take y which is the non trivial permutation of these coordinates so  here is this coordinates  pre reorder than such that this sequence and this sequence look different that is what x and y are so  the main claim is and this is the key claim in this entire proof x and y must reach distinct yes leaves so  the point is this  that this will allow us to argue  that there will be at least two distinct leaves or who knows more and then  we will argue in fact  based on this essentially  that the number of yes leaves is bigger than n factorial the time is simply going to be the height of such a tree and it is going to be log of n factorial or n log n so  this will immediately follow from this  we will see that later so  let me go over each of these items and let me explain each of these items so  let me start with this  we are going to interpret  this x as a point in n dimensional space and of course  we are restricting the points to unit n cube as i said  it is hard to visualize n dimensional space so  we are just going to leave it two  we will visualize two dimensional space and we will say what happens ? and you must note that  even if we work with two dimensions  we will get enough of the insight  refer slide time  18  16  so  here is our visualization so  our instance is this x that first coordinate is x 1  second coordinate x 2 so  this is our x 1 axis  this is our x 2 axis our instances come from the unit cube so  here is the unit cube in two dimension  this entire interior is a unit cube so  this is one  this is one  this is where our instances come from  what i mean by that is i pick a point over here it is first coordinate is the x 1 value  it is second coordinate is the y 1 value so  let me continue the analogy of little bit further to show you  what we make sure we understand at least this case of two dimensions very well i claim that all the instances which lie on this diagonal are no instances for this problem of size two well  what are the instances which lie on this diagonal so  the instances on the diagonal simply are those points whose x 1 coordinate is exactly equal to the x 2 coordinate but  if the x 1 coordinate is equal to the x 2 coordinate  then we know that they may n dimension distinct and therefore  these points on the diagonal are in fact  the no instances the interior of this triangle and the interior of this triangle are the yes instances why  because if i pick a point in it  i know that the x and y coordinates can not be equal  the x and y well the x 1 and x 2 coordinates can not be equal similarly  over here this is the line which divides the square into two parts and this is the line on which the x 1 and x 2 coordinates are in fact  equal so  what we have done over here is  we have taken our problem and we are viewing it geometrically and you will see  that this geometric view point gives some interesting insights   refer time  20  25    so  we have finished interpreting these instances in a two dimensional space or in particular in an n dimensional space so  we have finished this part we interpreted our input instance  as a point in n dimensional space well  in this case it was just two dimensional space but  you should get the idea and we also restricted the instance to be in n cube  which in this case was just the unit square now  you want to prove this claim so  this and this are the two main claims this claim says  that if we look at the region of this cube from which instances will reach l then  this region is going to be a connected means so  let me start by defining what connected means  refer slide time 21  21  a connected region in n dimensional space so is as follows a region r is said to be connected if for any points x or y in r  there exists a path from x to y passing entirely through r so  we have any points x and y in r and there exists a path from x to y  which goes only through the points to r so  what is a connected region ? so  the cube that we mentioned for example  is a connected region the interior of it well the surface of it is also connected region so  let me now define a convex region so  this is going to be needed in the proof a region r is said to be convex  if for any points x and y in that region the straight line path from x to y passes entirely through r so  here we just said  that the any path that some path passing from x to y  must pass entirely through r now  we are making the stronger requirement so  we are now saying  that in particular the straight line path from x to y must pass through r notice  that convexity is only a special case of connectivity so  in other words  if we know that a certain region is convex  then it has to be connected of course so  the reason why we worry about convexity is that  if we look at only straight line paths  they are very easy to reason with and therefore  its often much easier to argue  that is a certain region is convex so  convexity of a region is easier to prove and therefore  we are worried about convex regions but  notice that if we prove that something is convex  we are also proving that it is connected so  some examples of convex objects are the cube  the fill the whole cube or the whole sphere examples of objects which are connected  but not convex say something like a torus so  let me draw a picture  refer slide time 23  53  so  if you have a torus  so it has a hole in it so  if i pick a point x over here and a point y over here then  the line joining the straight line joining them would pass through this region  which is not in r so  this torus is itself in r  but this region is not is r  refer slide time  24  17  a kidney shaped region or a cashew shaped region is also not convex because  i can take a point x over here  a point y over here and this line passes outside the region  refer slide time  24  33  if i look at a cube and if i just look at it is surface not the interior but  if i just look at it is surface  if i take a point over here and a point over here and this phase then  the line joining them has the straight line joining them  will pass through the interior  which is not in this region r and therefore  this shell is not convex either so  that describes what connected means   refer time  25  14    formally and also what convex means  refer slide time  25  17  so  here is our claim so  we claim that r of l is the set of instances for which leaf l is reached on execution and then  we want to argue that r of l is convex well  actually we wanted to argue  that it is connected but  we will in fact  argue that r of l is convex  which will assure that it is also connected let me pictorially remind you  what this r of l is  refer slide time  25  53  so  here is our decision tree and this is leaf l so  what is this region that i am talking about ? well  if i start with any instance x and suppose i follow this path and i reach l then  i will say that this instance belongs to r  if execution arrives at l so  then it belongs to r of l in the region of l so  let me first begin by giving some intuition so  i said that x is a set of points  such that if i start from here and i get eventually i get to l so  what do i know about x ? well  each node that is visited has some condition associated with it so  may be here the comparison is between i and j and say this is the less than path  then these two together say that  this x must satisfy x i less than x j may be this label over here is k l and say this is the equal to path then  this says that the condition satisfied must be x k equal to x l so  if any instance gets to this level l  i know that all of these conditions along this path must be satisfied so  this is one way to characterize the region  the set of points which reach l during execution but  notice that this characterization is geometric so  this just says that the ith coordinate is smaller than the jth coordinate so  this is naturally putting our region r of l into some parts in our unit cube so  let us start with this root itself so  which are the instances  which can visit the root  well at the root any instance will arrive so in fact  any x in the entire unit cube  will arrive at the root  will start at the root so  instances visiting the root constitute the entire cube what about instances visiting this node ? so  this node is visited by those instances  for which x i is less than x j now  here is the key insight  refer slide time  28  44  so  asserting that x i is less than x j is equivalent to saying that  if this is our unit cube now  i am looking at three dimensions and may be this is some x 1  this is x 2  this is x 3 and if i say  that x 3 is say less than x 1 what do i do ? well  i will look at this x 3 and i look at this x 1 and then i look at first  that portion where x 3 is equal to x 1 so  it is this  it is this plane let me just shade it so  it is this slice through the centre of that cub that is  where x 3 is equal to x 1 and if i want x 3 to be smaller then  which side should i take then x 1 so  i want x 1 to be larger and x 3 top be smaller so in fact  this is the entire region so in fact  it is this region  the wedge shaped region that is facing us so  the idea is this  the moment i assert a condition  i am going to slice my current set and i am going to take one part of it  if my assertion was something like x 3 equal to x 1 then i won not take one part  but i will take that slicing region itself so  notice that i started off with the entire cube which is convex  refer slide time  30  18  and the important point is that  whenever i go to a child as in this i am going to shrink the set of instances  which visit this   refer time  30  28    and when i shrink them  i will be shrinking them in a convex manner  refer slide time  30  36  so  it is sort of i will take my region i will take a region  which is convex then i will slice of a part of it but  this slicing operation maintains convexity so  that is roughly the idea so  even if i do it several times  the region that i am left with at the end is going to be a convex region and therefore  also connected region  that is roughly the argument so  now we are going to see it more formally   refer time  31  05    so  here is the proof  suppose x and y are two points in r in this region so  i am going to consider three execution in the first execution the instance is going to be x  what do i know about this ? i know that l is reached by definition  by our assumption that x and y are points in r of l so  when i finish this execution i know that l is reached in execution 2 i am going to start with y what do i know about this ? well  i know that for this point as well l is reached so  even for this point l is reached right  again that is because i said that y belongs to r of l which is nothing but saying if i do an execution with instance my instance equal to y i will reach that same leaf my third execution is the interesting execution here  i am going to start off with an instance which i will call z z let me remind you has n coordinates  just like x and y so  i will call those z 1  z 2 all the way till z n and i am going to set z i in a curious looking manner z i is going to be lambda times x i  where lambda is some positive number between 0 and 1 i will tell you more i will write that down in a minute  and lambda times x i plus 1 minus lambda times y i so  this is how each z i is going to be set so  it will be some kind of an average of x i and y i  where the weights for x i and lambda and the remaining weights come from y so  if i take the case lambda equal to 0 what does it mean so  if i take lambda is equal to 0  then this part goes away and i get y if i get lambda equal to 1  this part goes away and i get x and if lambda is somewhere in between  what do i get ? if lambda is equal to half  then i get half of this and half of this and in fact  i get the midpoint of line segment x  y if i take other values of lambda i will likewise get  points on the line segment joining x and y  the straight line segment line joining x and y so  this is the key behind  this is the key part of the definition set i am going to i have defined z  so that it happens to be on this straight line x  y and so long as i restrict lambda between 0 and 1  it will be in the interior of this line segment x  y so  what do i have to prove in order to i give that  this r of l is convex well  the definition says that straight line path must lie inside of r of l if i prove that then i am done  that is exactly what i am going to do so  i am going to analyze this execution 3 and figure out what happens during execution so  let us start with root  the root label is i colon j and suppose  the less than branch is taken in execution 1 i am just taking this as an example  the argument will really work for every possible branch so  the less than branch is taken in this execution in this execution  what do i know about this execution ? well  clearly the same branch will be taken in execution 2 why ? well  in execution 2 we reach final finally  the same leaf and there is only one way to get at that leaf and therefore  it had better be along the same path so  even in this second execution  we are going to follow the same branch what can we conclude from that ? so  from the fact that in the first execution  this branch was taken it clearly means  that x i and x j got compared and x i turned out to be less than x j in the second execution y i and y j got compared and y i turned out to be less than y j so  this is what we know  if we assume that the less than branch was taken in this execution now  i am going to multiply this by lambda and this by 1 minus lambda and i am going to add these two things so  let us see what happens so  i claim that i get this inequality  let us check that out from the left hand side i am going to get from this inequality lambda times x i  which i have got over here from this i will get 1 minus lambda times y i  this is what i have got over here on the right side i got lambda times x j from this inequality and from this inequality i got 1 minus lambda times y j so  what we have now is this inequality so  this is the inequality that we got  let me just complete this argument so  to complete this argument what we have is  that this part is simply z i and this part is simply z j  so this is z i this is z j so  we have concluded  that z i must be less than z j but  that is what we wanted  why ? because  if z i is less than z j  we know that the less than branch will also be taken in execution 3 so  for this first root we have proved that execution 3 will follow the same path  as that followed by execution 1 and execution 2 so  this argument can be made if instead of less   refer time  37  38   than we took some of the other and we can make it at every node along that path and so we can argue that finally  l is going to be reached so  what we have argued is that  z will reach l z is any point on this line segment and so if i start with any point on this line segment i reach l and therefore  i have concluded that r of l is convex so  i have proved this claim so  let us now turn to the next claim so  this claim says that   refer time  38  20   if i have a point or an instance x  whose all coordinates are distinct and y is another non is a permutation on this  which is not exactly equal then  x and y must reach distinct leaves of course  they must reach yes leaves  because their coordinates are all different but in fact the claim asserts that they must reach distinct yes leaves  refer slide time 38  50  so  you are going to prove this so  x consists of distinct values  means the answer of x must be yes  y is some permutation sigma so  the answer to y is also yes we are going to prove this result by contradiction so  we are going to assume that say x and y reach the same leaf l then  we know that every the region corresponding to every leaf l is a connected region if it is a connected region  then there has to exist a path p from x to y  which passes entirely through l this is what we know from the previous claim so  what we will show  that if you tell me that it is this path i will show  that there exists a point z on this path p  such that the answer to z is no now  this will be a contradiction  because z lies on p  p supposed to lie inside r of l and what we have argued is that  the answer is no whereas  the answer for l is supposed to be yes so  this would be the contradiction so  this is what we are going to do ? let me start with the sub claim the sub claim says that there have to exist i and j such that  the ith coordinate of x is strictly less than the jth coordinate of x  whereas the ith coordinate of y is bigger than the jth coordinate of y i will prove this in a minute but  let me just examine it is implications so in fact  this is going to give us the proof almost immediately so  let me define a function f in this space  where f for a point w is simply the difference between the ith and the jth coordinates so  what is f of x  f of x is x i minus x j  but x i is smaller than x j so  f of x is less than 0  what is f of y ? well  y i is bigger than y j so  y i minus y j is bigger than 0 and f y is bigger than 0 so  f of x is less than 0  f of y is bigger than 0  they are joined by this continuous path p so  what happens is we move along this path from x to y the path is continuous  this is a continuous function and so  by the mean value theorem  there has to exist a point z on p such that f of z is 0 if f of z is equal to 0  what does it mean ? this means that z i equal to z j  but z i equal to z j  then two coordinates are equal that means  the answer to z is no the coordinates are not distinct so  the answer is a no  but this supposed to be a point inside r of l so  the answer had better been yes  so there is a contradiction so  we have proved our basic claim so  all that remains now is to prove this sub claim so  let us prove that  refer slide time 42  32  so  the claim is that there exist i  j such that x i is less than x j and y i is less than y j so  i am going to do this with an example to help you understand what is going on so  here is my example so  i am looking at say five dimensional space  this is my x  the coordinates are all points in the unit cube this is why remember that y is just a permutation of this permutation sigma and y is such that  it is not identical so  the same numbers as x is repeated over here but  it is not repeated they are not repeated identically well  they can be identical at one place that is now  here is the key step  i am going to define a permutation pi which sorts x so  pi of x then is going to simply take these and rearrange them in increasing order so  it is going to 0.2  0.5  0.7 and so on it may not be clear to you  why i am not defining this permutation pi but  please bear with me  the answer will hit you in a minute so  i know what pi is i know how to take x and generate pi of x from it i apply the same permutation on y as well so  what happens well i look at the column  the column of x which moved over here and i take the corresponding value for y and move it down over here so  it is both are 0.2 over here  so  it is both are 0.2 over here then  0.5 came from here so  the value over here must also come over here 0.7 came from here so  below it is 0.9  so over here also there is 0.9  then we have 0.8  0.7 from this and 0.8  0.9  0.5 from here so  we have rearranged x and we have rearranged y now  be patient with me just for a minute  i am going to prove this claim but  i am going to prove it for pi x and pi y  where it is easy to see and you will see that  they can just trace it backwards to x and y so  what is it mean to prove the claim for pi x and pi y ? well  we are supposed to find i j of this property so  because we sorted and here is now the reason for sorting it  if i is less than j we know that pi of x i has to be less than pi of x j  because this is sorted order so  pi of x sub i the ith component of pi of x must be smaller than the jth component of pi of x so  this is smaller than this  this is smaller than this and so on so  long as i choose i smaller than j  this first property is guaranteed to me what do i know about y ? so  here is one sequence i change that and then permuted so  i know now that somewhere  this new sequence has to be non increasing this was in increasing sequence  this is a permutation of it so  somewhere this sequence has to be non increasing so  let us say that there exist i j  such that i less than j and if it is non increasing then ; that means  pi y must be greater than pi y j but  notice that then we have found these two so  we have found i j such that  pi y is bigger than pi y j  whereas pi x i is bigger than pi x j so  just to illustrate in this example here is the case  where this is smaller than this  but this is larger than this now  we have proved it for this pi x and pi y how do we go back ? well  the idea is simply we take we figure out  where these columns came from our original example this column came from here and this column came from here and sure enough  this number is less than this whereas  this number is greater than this so  i will skip the algebra but  this is exactly what is happening ? we just have to follow back and then this property will hold nevertheless   refer time  46  40    so  what have we proved  well we have proved this claim and this was all that was needed to prove our original claim  which was this   refer time  46  48    so  once we prove this claim what do we know that if we have two distinct permutations of this if we distinct permutation of this  then that must reach a different region but  now i know that the number of distinct permutation can be n factorial and therefore  the number of yes leaves has to be bigger than n factorial and the time has to be bigger than the height of the tree  which is log of n factorial or at least n log n so  this finishes the claim that the time for element distinctness is at least n log n on decision trees  refer slide time 47  30  here is the quick summary of the argument so  what we did here was  that we said that the instances visiting any yes leaf  former connected region in the instance space no instances partition the instance space  such that distinct permutations are not in the same connected region and therefore  we conclude that the number of yes leaves must be large what is the implication for the ram model ? well  for the ram model we can conclude exactly using ideas similar to last time  that the comparison based algorithms for element distinctness  must take time n log n  refer slide time 48  13  i want to quickly extend this to the case of algebraic decision trees so  here is a quick definition so  again the program in this model consists of trees with outgoing branches  less than equal to or greater than but  this time we will have these three relational operators  just for simplicity  although you can have other operators too the node labels are no longer i colon j  but they are algebraic expressions  over the input the components of the input so  x 1 square plus x 2 square minus 25 the action is we are going to evaluate the label expression and we are going to compare it to 0 so  this expression is equal to 0  then we will choose the equal to branch if this is less than 0  then we will choose the less than branch so  let me just take a quick example so  if our expression is x 1 square plus x 2 square is 25 or minus 25  refer slide time  49  18  then  the condition x 1 square plus x 2 square minus 25 is less than 0 simply means  that our point lies inside this so  we are restricting our point to be inside this region if the expression is linear instead of this  which is a quadratic then  our previous results actually hold unfortunately  if the expression is non-linear   refer time  49  50    then the intersection of constraints can produce disconnected regions  which can not happen if things are linear  refer slide time  49  57  the main result is something like this  this is a deep result actually from algebraic geometry so  we have a decision problem over inputs x 1 to x n  by a decision problem we simply mean  that we mean a problem whose answer is yes or no  just like element distinctness suppose a is a algebraic decision tree algorithm for the problem such that  the degree of each algebraic expression is some fixed constant d so  x 1 square plus x 2 square would be degree 2 suppose  the no answers partition the instance space into w connected regions  within each of which the answer is yes so  in our case for simple decision trees  this w for the element distinctness was n log n but  in general the time required is going to be by this algorithm is going to be omega of log of w minus n now  you might wonder what happened to that d so  this d actually appears inside this omega so  there is a constant of proportionality  which depends upon d for decision trees the time is log of w  not even omega it is actually just log of w  a log of w to the base 2 so  what this theorem says  that the complicated algebraic model does not really help all that much so  the lower bound that we get is almost as good well  it is a little bit smaller  because of this minus n and may be the proportionality constant is a little bit different  but it is essentially the same lower bound for element distinctness in fact  there is no change  the algebraic tree for any fixed degree will give us n log n as before the proof idea is actually pretty difficult now  as we said a single leaf can correspond to a small number of connected regions  not just exactly one connected region so  now we have to get some heavy duty machinery from algebraic geometry to count the number of connected regions when that you get when you take intersection of several constraints if the constraints are linear  then it is very simple if the constraints are high degree algebraic expressions  then this becomes rather complicated  refer slide time 52  30  so  quickly summarize lower bound theory  that we have been looking at in the last two lectures  tells us when to start searching for better algorithms this is very good  because it is good to know that you are done another interesting point of this theory is that it has some connections  with some really deep mathematics algebraic geometry is supposed to be rather deep area of mathematics and it has connections to many other fields also here is a very simple context  in which this idea can be used so  i will leave it as a problem for you suppose  you have 27 coins  such that 26 have equal weight and one is heavier  find the heavier using 3 weighings this is probably we have puzzle that we have solved now  i want you to use the ideas expressed in this lecture to formulate a decision tree model using which  you should be able to argue  that you can not do this in fewer than 3 weighings thank you design and analysis of algorithms prof sunder vishwanathan department of computer science engineering indian institute of technology  bombay lecture  26 np  completeness  i motivation introduction to reductions  refer slide time  00  51  deals with the notion of np completeness  we will study this notion and we will see how to use this notion effectively the first thing i will sort of talk about is motivation hopefully at the end of this i will motivate you sufficiently that  you will be interested enough to study about the subject so  imagine that you finished your b tech you have the all new subjects well  and then you gone for a interview  you land your dream job the company pays well  you are really happy  you settled in software job you very happy  because it is you have to deal with designing things and your boss is also happy with you and he wants to give you challenging projects the first project you get is the following it scheduling jobs on computers so  the input is set of jobs and each job as a size associated with it so  let us say the jobs are j 1  j 2 up to j n and the sizes are s 1  s 2 and so on to s n these sizes could vary  they could some of the jobs could have the same size  some could have different you can think of them as positive integers now  there are two processors so  there are two processors p 1 and p 2 both are identical and your job is to schedule  these jobs on these two processors so  you want to automate your boss wants you to automate this process you have to write a program  which takes input these sizes s 1  s 2 up to s n so  let us say a array of size n or reads it of a screen and you have to schedule these jobs on processors p 1 and p 2 now  the scheduling should be such  that the last job finished is fastest so  what do we mean by this ? supposing  i schedule all of them in p 1 and the time taken will be s 1 plus s 2 plus s 3 and so on up to s n let us say that job of size s i takes the same time  time s i to finish if i schedule it  it takes time s i so  also assume that there is preemption once  you start a job of it has to run for completion so  if i run all of them on one processor  the time taken is s 1 plus s 2 plus s 3 up to s n so  at time s 1 plus s 2 up to s 3 and so on up to s n  all jobs must finish now  supposing i schedule  the even jobs on p 2 and odd jobs on p 1 let us say s 1  s 3 and so on here it is s 2  s 4 let us say there are seven jobs s 5  s 7  s 6 so  this is how the jobs are scheduled now  the time taken on p 1 is s 1 plus s 3 plus s 5 plus s 7  time on p 2 is s 2 plus s 4 plus s 6 now  it may so happen that these are very large jobs let us say  these sizes are 1 and these are 100 then  the time at which the last job completes is 300 this process finishes of in four units of time  while this takes 300 units of time so  the last job finishes after 300 units of time  this is not what we want so  you want to sort of distribute the jobs evenly among these two processors so  you want to distribute the jobs in such a way  that these large jobs say one of the s 4 should be here and may be s 5 should be here and so on  we want to distribute as evenly as possible so  you think about this for some time and here is the first idea  we come up with  refer slide time  06  07  so  you take the jobs in order so  the jobs are of sizes s 1  s 2 and so on with the first job on the first process it is p 1 i put s 1 here the second job goes to p 2 now  you put when i take the third one  you put it on the processor  which is least lightly loaded for instance  if s 1 is greater than s 2  then i put s 3 here and you keep doing this  i look at s 4 and i see which one is greater assume that s 2 and s 3 are on p 2  check whether s 1 is larger than s 2 plus s 3 or is s 2 plus s 3 larger than s 1 if s 2 plus s 3 is larger than s 1 then i put s 4 on p 1 so  s 4 will go here  if s 2 plus s 3 is larger than s 1  you can break ties as you want this is the first algorithm you come up with you code this and it is starts working your boss is happy but  the next day the boss shows up and tells you  that this does not work so  he gives you an input on which you produce the schedule  which takes some time t so  you take some time t while the boss produces the schedule which is less than t so  the optimal schedule on the input is smaller  the total time the optimal takes is smaller than the time that your algorithm produced and the boss says fix this so  at this point i would like to give you an exercise  what give an example to show that  this algorithm does not produce a optimal schedule so  give an example to show that this algorithm does not produce the optimal schedule well  you go back and figure out  there is something wrong with the algorithm in the sense  that if these jobs are mixed up if the sizes are mixed up arbitrarily  may be that is the reason the algorithm does not work  refer slide time 09  10  so  here is your next attempt  the first thing you do is you sort the jobs by size now  you know that s 1 is less than s 2 less than s 3 and so on you know that  the jobs are increasing order of size and now you do the same algorithm  you put s 1 on the first processor  s 2 on the second and so on well  this seems to work  but two days later again your boss shows up with an example  which does not work so  let us look at this example the example has five jobs and the sizes are as follows 2  2  2  3 and 3  let us see what your algorithm does so  here are my two processes p 1 and p 2  these are the job sizes so  the first job goes on to p 1  the second job is on p 2  the third job you could put either on p 1 or p 2  let us say we pick p 1 the next one goes here and well 4 is smaller than 5 so  you put the last one here now  the total time that your schedule takes is 3 plus 2 plus 2 which is 7 on this processor of course  you take 5 units on this you take 7 units so  the total time to finish all jobs is 7 units well  you can see that this is not optimum  the best way to do this is i guess you have seen them by now  you put the 2s on the one processor and the 3s on the other now  there are only 2 3s so  this takes 6 units and that takes 6 units every job has finished in 6 units of time so  this   refer time  11  19   that you will come up with also does not work let me again repeat the algorithm  you sort the jobs by size now  you look at these jobs one by one and assign these processes greedily  in the sense that  you put the first one in the first process  the second one in the second and when i reach the ith job  you put it on the least lightly loaded processor you sum up the previous sizes  which are on each processor  choose the smaller one and put this new job on that processor this was the algorithm we came up with and this does not work  here is an example that it does not work so  now what do you do ? well  you can go back and try fix this example when  you come up with another heuristic  which beats this example but  i can tell you that it is extremely difficult possibly impossible to come up with a smart heuristic like this  which does well for all inputs  which always does well so  you after trying various heuristics most of them do not work you are now at the end of your patience  you are really worried about what your job  because boss is now threatening you with dismissal so  then what you do is this so  you now sort of want an algorithm that works always so  what you do is a brute force algorithm  refer slide time 13  11  so  you take every subset of the jobs so  i have jobs  i have sizes s 1  s 2 up to s n so  take all subsets   refer time  13  30    so  compute the size of s and size of s bar s bar is the complement of s so  all jobs not is s which is s bar so  essentially we want to put s on process on p 1 and the complement on the processor p 2 so  you do this and look at the maximum of these two and choose the maximum of these two so  this will tell you the time  the total time that this schedule took so  do this for every possible subset and pick the minimum take all subsets compute these choose the maximum and of these maximum pick the minimum of these you can easily see that this will always give you the right answer because  you will look at all possible ways of scheduling  these jobs on two processors and one of them has to be the minimum now  this is fine  this algorithm works  your boss tries out a few inputs and it is all it works for all most all for in fact  for all inputs till your boss feeds in an input with let us say 1000 numbers there are 1000 jobs  which have to be read to these processors these two processors so  we give the input as a list of size 1000 now  you start running the algorithm 5 minutes pass an hour passes  2 hours  10 hours  1 day the algorithm does not stop  2 days  one week and now your boss is starting to get worried here is an algorithm which is got for all small inputs and now it is weak and the algorithm does not stop for an input of size 1000 well i can tell you that this is not going to stop even after many years you can take years and years and this is not going to stop  in my lifetime not your lifetime may be many lifetimes that is a problem with such brute force approaches let us do a quick calculation to see how much time your algorithm is going to take on inputs of size 1000  refer slide time 16  30  if you have 1000 jobs  the number of subsets of jobs that you look at is to be 1000 so  the number of subsets of jobs is 2 to the 1000 now  let us assume that in one instruction a computer can process one of these subsets let us see  how much time computers will take to process all of these subsets now  the fastest computer runs  let me make an estimate let us say 10 to the 20 instructions per second this is the fastest computer on the fastest  this is a big over estimate it is much smaller than this  but we will assume this  how many computers you think there are in world 10 to the 20 let us say 10 to the 20  it is again an overestimate computers in the world what we are going to do is put all the computers in the world  at your disposal to solve you know to run the your brute force algorithm so  putting these two together  then i have 10 to the 40 instructions i can do instructions per second  using all computers so  i am using all the computers in the world and i can do 10 to the 40 instructions per second so  how many instructions do you think  one can do in a year  let us do this calculation so  it is 10 to the 40 instructions per second so  in an hour i can do this time 60 in a minute i can do this time 60 another 60 for an hour then  i have 24 hours a day then  i have 365 days in a year so  these many instructions per year let me do a overestimate here  this i am going to say 10 to the 2  this let us say this is 10 to the 2 well  10 to the 2 since we are generous  this is 10 to the 3 so  number of instructions per year is certainly not more than 2  4  6  10  10 to the 50 good so  i have 2 to the 1000 subsets to look at i can look at at most 10 to the 50 subsets per year let us see how many years this takes so  let me quickly  refer slide time  19  53  so  you have 2 to the 1000 instructions and using all the computers in the world the fastest speeds you have 10 to the 50 instructions  that you can do per year now  this 10 to the 50 is roughly 2 to the 10 is   refer time  20  21   let us say is certainly less than equal to 2 to the 4 times 50 that is 2 to the 50 for the 200 so  now you can i just divide this by that to check  how many years i am going to take so  the number of years you going to take is 2 to the 1000 minus 200  this is roughly 800 years you can check  how many particles there are in the universe this is not very far away from that number of atoms in the universe per year so  you are going to take a lot of years for this algorithm to finish this is certainly not what you wanted to do if the boss figures this out  you certainly fired in fact  even if he does not figure this out you are fired so  what do we do ? so  this is a motivation for studying np completeness i will give you a recipe  using which you can hopefully save your job if your boss is going to be a little bit intelligent so  the first part of the course  which you did design of algorithms  was to help you get a job the second part that we do theory of np completeness will help you quickly so  our final objective will be to show that  this problem is among hard problems in this world  what do i mean by hard ? it means that  so far nobody else has been able to find the solution to this problem well  the boss can say i mean this problem  just come up with our company you know other companies handle other problems  why are you saying that  this is not been solved before well you can in fact  say that if somebody manages to solve this problem then  a lot of other problems  which will lot of other people have not been able to solve can also be solved so  this is the kind of statement we would like to make so  before we formally look at this notion of np completeness we will lead a few other fonda 's  few other notions that we need to imbibe the first one is the concept of reduction  refer slide time  22  59  so  we will first look at an example and we will see what reduction really means so  reduction really means this  supposing you are having a library so  you have a library where large number of problems have been very well coded by very good programmer now  given a new thing that you want to code  it would be very nice if we can use sub routines from the library in your code rather than  re invent the wheel and try and do all this by yourself this is all that is to reduction how to efficiently use code for other algorithms to generate new algorithms so  let us look at an example this is not very easy example  it is not very hard either so  before we start a few definitions  which we will recall hopefully you have seen this before the first thing is notion of a matching  in a graph g equals v e  v is the vertex set  e is the edge set a matching in a graph is a subset of the edge set is a subset of e so  let me call this m  the subset m of e  such that no two edges share the same vertex such that  let me write this down no two edges in m have the same end point  let us look at an example  refer slide time  25  21  so  let us take a graph  let us say this is a graph then  here is a matching  the ticked edges form a matching if i look at these two edges  they do not share a end point the end points of these edges are these two  the end point of this edge is these two they do not share any end point for instance  let me label this a  b  c  d  e if i take a  b and a  d these do not form a matching  this is not a matching essentially one set of edges so  that they look like this in a graph  they do not share an environment  this is what matching is the next definition we need is that of a perfect matching  refer slide time 26  40  we will say that a matching is perfect  if all vertices in the graph are end points of at least one edge this is a matching such that  all end points a perfect matching is a matching  such that all vertices are end points of at least exactly one edge in m so  it must be matching m such that all vertices are end points of exactly one edge so  the size of a perfect matching is just half the size of the vertex set so  then note that size of m is size of v by 2 so  this is a perfect matching  let us go back to this old graph of ours  do you think that this graph has a perfect matching   refer time  28  30    the answer is no  well there are five vertices so  if i take a matching with two edges i can cover four of these vertices  i can not cover five and i can not take three edges  because then two of these edges will share an end point so  this graph does not have a perfect matching  but for instance if i change this graph slightly  refer slide time  29  04  let us see i have a  b  d  e  c let me add one more  let us say f now  this graph has a perfect matching in fact  it has many at least two perfect matching 's  may be two perfect matching 's  here is i take these three edges a  b  e  d and c  f is a perfect matching a d  e b and c f is also a perfect matching it ticks for one perfect matching and the crosses form another perfect matching  this graph   refer time  29  55    so  now that we have defined these two problems here is a problem that we would like to solve so  there are two problems  refer slide time 30  20  so  here is let me say one is problem perfect match pm is for perfect matching the input is a graph g and the question you ask  does g have a perfect matching this is your first problem  the input is a graph g does g has a perfect matching the other problem we are going to look at this  it is very closely related to the first one so  this is maximum matching  the input is the same  input is a graph g the output i need is a matching of maximum size so  let us look at these two problems for this problem i only need to say yes or no ii just want to say whether the graph is perfect matching or not we actually need an output  which is i need to know the i need edges  which will represent in some matching of maximum size we have seen that a matching a graph could have more than one matching 's of maximum size  any one of them will do now  supposing in your sub routine library  you know you have sub routine coded for this for problem maximum matching which so  given you have an algorithm  so that when you feed in a graph g  it will output a matching of maximum size can we use this to solve all this problem  the answer is yes just feed in the same graph g  look at the matching if it has v by 2 edges  v is the number of vertices if this has v by 2 edges  then the graph has a perfect matching if it is less than v by 2  then the graph does not have a perfect matching so  given a algorithm for this second problem  very easy to construct one for the first problem how about the other way around which is given supposing  this sub routine library has an algorithm for this  which is given in graph g  it will tell you whether it has a perfect matching or not and now  you want to output matching of maximum size your job is to construct an algorithm  which does this and you would like to use this algorithm  effectively because  this is a very good implementation and you would like to use this effectively let us see how this is done so  i hope the objective is clear  let me write this down  refer slide time  34  00  so  we are given an algorithm for a perfect matching and we want to design one for max matching this is what we want to do ? well  how do we really use this algorithm  that is the crucial question ? in fact  let us take something which is sort of intermediate between these two  how about time to find the size of the maximum matching we will look at this problem maximum matching   refer time  34  54    the output should be edges  set of edges in a matching of maximum set of size that is what the output should be now  supposing we only want to know the size of the maximum matching in the graph  can we solve this let me write this problem down and will try and do this so  this is objective 1 and intermediate objective is this design and algorithm to find the size of a maximum matching in g so  we are given an algorithm for perfect matching  you want to design an algorithm to find the size of the maximum matching in g  this is how you do it so  you take this graph g  refer slide time 36  14  so  here is your algorithm that does perfect matching now  you first feed in g and ask whether it has perfect matching ? if g has a perfect matching  then you know the size of the maximum matching is hopefully it is if g has a perfect matching  then the size is v by 2  where v is the number of vertices in the graph so  let me write this down  if g has a perfect matching  then the size of maximum matching i is a v by 2 so  supposing  it says that g does not have a perfect matching now what you do ? well  what you do is this you take here is g so  step 2  so you take g add a vertex  this is a new vertex connected to all vertices in g now  call this graph g 1  now you feed g 1 into this algorithm for perfect matching you have taken g  you added vertex  which is adjacent to every other vertex in g and now you feed this into this perfect matching again two things can happen  either it can come back and say it has perfect matching or it can say no let us see what happens in both cases  refer slide time 38  48  so  g 1 is g plus a vertex connected to all vertices in g  that is what you want now  supposing g 1 has a perfect matching then  what can you say about g well  if you look at a perfect matching in g 1  this vertex this extra vertex is will in the matching  it will be connected to something in g this is the picture here is g  here is the extra vertex and i just going to draw the edges in the matching so  this fellow will be connected to something here in this matching the other vertices in g are all match somewhere over the other this is if g 1 has a perfect matching  then this is the figure  this is how it looks like  which means except for this vertex if i remove this vertex out  the rest of the graph is a perfect matching  which means there is a matching of size  this implies that g has a matching of size v minus 1 by 2 and this is the size of the maximum matching really we know that  because v did not have a perfect matching  it does not have a matching of size v by 2 on the other hand  it has a matching of size v minus 1 by 2 this is if g 1 has a perfect matching  what if g 1 does not have a perfect matching if g 1 does not have a perfect matching  then we know that g does not have a matching of this type so  then we create graph g 2  refer slide time  41  05  so  how do we create g 2 well  similar you take g 1 plus a vertex connected to all vertices in g 1 now  you ask g 2 has appropriate matching and so on then  you form g 3 and ask whether g 3 has a perfect match and you stop as soon as the perfect matching fellows says yes so  let me write the generic step so  you get g i the graph g i minus 1 plus a vertex connected to all vertices in g i minus 1 well  this is how you get g i so  now you look at g 1  g 2  g 3 and so on and ask whether g 1 has a perfect matching  g 2 has a perfect matching and so on so  let us say k is the first index  where the perfect matching algorithm says yes so  which means  refer slide time  42  32  so  assume that g k minus 1 does not have a perfect matching and g k has a perfect matching now  what is g k minus 1 and g k look like so  here is my original graph g for g 1 i added one vertex with this connected every single region this is g 1  g 2 i added two vertices now  this is connected to this and also connected to everything and so on so  g k minus 1 i have k minus vertices here  they are connected through each other and they are also connected to everything in g similarly  this is g k minus 1 and g k looks like this i have the graph g and k vertices here these are connected to each other and they are also connected to each other k vertices which are connected to everything each other now  this does not have a perfect matching and this has a perfect matching  what does this mean now  let us look at the perfect matching here now  actually i must say that g 1  g 2 all the way up to g k minus 1 does not have a perfect matching so  k was the first time that we encountered a graph with perfect matching g 1  g 2  g 3 up to g k minus 1 and does not have one and g k has one so  let us look at g k so  what can a perfect matching look like  can it have a edge like this  the answer is no why is that  supposing it has an edge like this in the matching  you remove these two and this graph now looks like g k minus 2 and this as a perfect matching but  we know that g k minus 2 does not have so  all edges in the matching must be from vertices here to vertices inside g so  here is the picture here is g  refer slide time 45  07  so  this is g k these are k vertices  these k vertices are matched with some k vertices in g there are n minus k other vertices in g  which are matched amongst themselves so  there is a matching inside g of size n minus k by 2 so  there is matching in g of size n minus k by 2 and we would like to observe that  there is no smaller  there is no matching of size larger than n minus k by 2 this is the largest matching that you can have  now why is that so supposing  first this has to be even m minus k has to be even  because this is an integer right so  if there is a matching of size larger than this that means  so suppose there is matching of size  let us say n minus k plus 2 by 2 which is nothing but  n minus k by 2 plus 1 one more edge so  supposing there is a matching of this size so  the claim is that one of the previous graph in fact  then g of k minus 2 has a perfect matching so  this will be a contradiction supposing there is matching of size larger than this  then g k minus 2 will have a perfect matching so  we will see this  refer slide time 47  26  so  here is g and here is a subset which is of size n minus k by 2 plus 1 so  here i have n minus these many which is so  n minus n by 2 plus k by 2  that is this size  which is n by 2 minus and minus plus k by 2 minus 1  refer slide time 48  20  so  let us look at this let me just refresh   refer time  48  24   we are here supposing there is a matching of size n minus k plus 2 by 2 and we have to show that g k minus 2 has a perfect matching so  this is g  so it has a matching of size n minus k by 2 plus 1 so  the number of vertices in the matching is twice this  so the size  so number of vertices here is twice this  which is n minus k plus 2 so  the number of vertices here is n minus  so which is k minus 2 this portion has a perfect matching  which was our assumption so  let us look at g k minus 2 so  g k minus 2 had g  this is g this in fact  is g and i had k minus 2 other vertices  which were connected to everything in g  and also amongst themselves  if this was g k minus 1 what i do is this  i take this matching in this portion as if and each of these k minus 2 vertices i match to the other size you can see that  this gives perfect matching matching on this portion remains the same as in this portion here and these k minus 2 vertices i match to the other side so  this shows that g k minus 2 in fact  has a perfect matching   refer time  50  20    so  let us recap we were given a algorithm for perfect matching and the objective the intermediate objective was to design a algorithm to find the size of the maximum matching in g and what we do is this ?   refer time  50  38    we define graphs g i  where g i 's g i minus 1 plus the vertex connected to all vertices in g i minus 1   refer time  50  49    we find the first graph we look at g 1  g 2 and so on and we find the first graph g k which has a perfect matching  which means g 1  g 2  g 3 and so on up to g k minus 1  they do not have a perfect matching g k has a perfect matching   refer time  51  05    then  we claim that there is a matching in g of size n minus k by 2 and no larger so  the size is the largest matching is n minus k by 2 this shows that given an algorithm for finding the perfect matching i can design an algorithm for finding  the matching of maximum size in a graph by calling the other algorithm repeatedly i do not call it too many times i call it at most k times here you can actually do it much faster  you can call it faster than k  so in the following sense this k can it can go down all the way up to 4  5 up to a constant  which means you can call it about n times so  we have shown that given an algorithm to find the perfect matching in a graph i can design an algorithm that finds the size of the maximum matching  refer slide time  52  19  the way you do it is you look at graphs g  g 1  g 2  and so on find the first graph g k which has a perfect matching and the size of the maximum matching  then is n minus k by 2 if k is 0  it is g and the size of the maximum i think n by 2 and so on if g k is the first time you get a perfect matching  the size is n minus k by 2 in the worst case  you would call the perfect matching algorithm n times i would call g 1  g 2  g 3 and i can go all the way up to n now  there is a smarter way to do this using binary search  which i will let you do  which let you figure out this k  using only log n calls to the algorithm for perfect matching it resembles binary search and is an exercise for you   refer time  53  17   essentially it shows that by calling this algorithm at most n times i am able to determine at least the size of the maximum matching in the graph but  let us go back now  the objective we had was to find this perfect matching so  here now i have full filled this intermediate objective   refer time  53  39    i can design an algorithm to find the size of the maximum matching  i want to actually find the edges in the maximum matching how do i do this now  here is the trick  you take this graph g  refer slide time  54  01  so  i take g i find the size of the perfect  size of the maximum matching so  let us say the size is l  the size of a maximum matching now  what i do is  pick any edge in g and remove it  remove a edge from g and now ask the same question  what the size of the maximum matching ? if it remains l  then i throw away this edge and i concentrate on the graph at remains if it is not  if it decreases then i put this edge back this edge is part of the maximum matching so  i leave this edge i look at the edges e 1  e 2 let us say up to e m 1 by 1 when  i look at a edge i remove it and i ask  if the resultant graph has a matching of size l i find the size of the maximum matching the resultant has if it is l  then i throw away this edge otherwise  i put this edge back by the time i am through with all these edges but  what i will be left with is a graph which will be just a matching it will just be a matching of size l let us do an example so  let us take a simple graph  refer slide time  55  43  so  here so let us take a simple graph e 1  e 2  e 3 the size of maximum matching here is 2 i remove e 1 and i ask what is the size ? now  the size is fallen by one the size is one so  i put e 1 back i remove e 2  i ask what is the size ? size is 2 so  i throw away it  at this point the graph looks like this  because i have thrown away e 2 then  i ask for e three i remove e 3 and the size falls by one so  i put e 3 back so  i am left with e 1 and e 3 and when i look at the graph that remains is the matching in fact  it will be a matching of size l  apart from the matching of size l if there is any other edge  you know when you remove this edge  the graph will still have a matching of size l so  you will throw this edge away so  at the end of this procedure  what you will be left with is the matching of size l  which is what you are looking for so  this completes our objective which we started out with we were given a algorithm for perfect matching and we wanted to design an algorithm for maximum matching what we did first was we designed algorithm   refer time  57  04   to find the size of maximum matching and using this we were able to design one for maximum matching so  we had this in our sub routine library  we added this to the library by using this algorithm and once we had this in the library it was easy to design a algorithm here we started with this and we ended design and analysis of algorithms prof sunder vishwanathan department of computer science engineering indian institution of technology  bombay lecture  27 np-completeness ? 2  refer slide time  01  48  last time we looked at the concept of reduction so let me just quickly review  what we mean by reduction ? so  reduction means this given an efficient algorithm for a problem pi 1 so  you are given a efficient algorithm for problem pi 1 then using this  we design an efficient algorithm for a different problem so  we essentially showed that this implies that there exists an efficient algorithm for problem pi 2 so  given that there exists an efficient algorithm for problem pi 1 so  one problem we show that there is an efficient algorithm for problem pi 2 the way we do it is imagine that this pi 1 somebody has coded for pi 1 an algorithm for pi 1 and it exists in some library then  we use this to design an algorithm for problem pi 2 so  this algorithm that we design it takes as input for problem pi 2 it perhaps does something to the input and then calls this sub routine for an algorithm pi 1 may be once may be twice repeatedly and at the end of this it outputs an answer and this answer is for problem pi 2 so  essentially we have solved problem pi 2 assuming that somebody has given a solution for problem pi 1 the two problems  we looked at last time was pi 1 was given a graph does  there exist a perfect matching in the graph that was pi 1 and the algorithm  we designed was given a graph output a matching of maximum size that was pi 2 and we did in fact  repeatedly call this pi 1 after modifying the inputs likely so  why is this called the reduction ? it is called the reduction in the following sense that you actually want to call  you actually want to solve a problem pi 2 and you have somehow reduced this problem solving the problem pi 1 in the sense that you can solve the problem pi 1  you can solve problem pi 2 so  you have reduced the problem pi 2 to a problem pi 1 and that is how the word reduction comes but  the essential method is this given an efficient problem algorithm for one problem find an efficient algorithm for another problem and you have to use the algorithm for the problem given to construct this efficient algorithm so  this was a concept of reduction we will in fact  see one more example of this very shortly before  we see an example here is a word which seems a bit new which is efficient problems have been defined  we have seen many problems  algorithms have been defined we have seen many algorithms well what does this efficient means so  let me define what i mean by efficient ? we had seen earlier a solution to one of these scheduling problems  where the time taken was just too much ? that was a inefficient algorithm an efficient algorithm  we would like to be something that on reasonable size inputs finishes in reasonable amount of time  refer slide time  05  03  so  let me tell you algorithms which are not efficient to start with so  these are usually brute force algorithms are not efficient so  what do you mean by brute force algorithms ? brute force algorithms are algorithms that look at all possible solution sets there may be many possible solutions given your input there could be many possible solutions and you want to pick one which is good or the best so  the brute force algorithm would look at all possible ways of doing this and pick the best one for instance  if you want to look for a matching of maximum size a brute force algorithm would look at all possible collection of edges all possible subsets of edges  then check whether each subset is a matching and also the size and from this you can certainly find out a matching of maximum size but this algorithm takes too much time so  if there are m edges then you take 2 to the m  you have to look at 2 to the m subsets and we have seen that this is just too much so  these usually search exhaustively through the entire solution space and if the input is of length n that is the other sort of characteristics of this brute force algorithm the input size is n typical times taken by these brute force algorithms is 2 to the n  because we have just looked at all subsets typical times are 2 to the n and this is just too much and we saw that you design such algorithms you may get fired so  these are not the algorithms that we are looking for these are not efficient  what do i mean by that so  by efficient i mean that the running time of the algorithm is bounded by the polynomial in the input size  refer slide time  07  38  let me write this down for us this would mean this  the running time of the algorithm is bounded by a polynomial in the input size so let me restate this again  what do i mean by polynomial ? the running time which is bounded by let us say n to the constant so let me  just state this again which means i am just going to restate this there exists a constant c such that the running time t of n is big o of n to the c  where n is the input size so this is  what efficient for us will mean ? most of the algorithms  we studied so far are efficient in this way for instance sorting  we can do in n log n times that certainly bounded by a polynomial n square bounded by n square the other algorithms which you have studied in your divide and conquer or dynamic programming they are all bounded by a polynomial in the input size n square n cube shortest path finding minimum spanning trees all most any algorithm that you have studied so far they are all their running times are all bounded by a polynomial in the input size and these are the algorithms we have studied so far are efficient  why polynomial  why is the notion of efficiency  why not some other function of n  refer slide time  10  19  so  the reason is this  if you compare n to the c if you compare n to the c and let us say 2 to the n so  these are 2 to the n is what brute force algorithm would take and n to the c is what our notion of efficiency is will be if you look at these two functions then 2 to the n grows just much faster than n to the c so  even for inputs of size 1000 square and 2 to the 1000 are just two different things if you have something which is let us say n square then if you program an algorithm  which runs in time 1000 square that will take a few minutes to complete if your algorithm takes 2 to the 1000 well  it is not going to stop at least in our life times may be much more so that is why  this motion of efficiency has been is prevalent the other sort of reason is the in practice usually when the problem has an algorithm which runs an polynomial in input size so usually in practice  if there is a constant  so that you can bound the running time by n to the c this c happens to be very small 2 or 3  there are very few algorithms  whose running times are more than n cube if there is a polynomial time algorithm for this problem which means the running time is bounded by a polynomial then  using this polynomial is small like n n square n cube log n so in practice  if there is such a c  usually less than equal to 3  so that is why one uses this notion of efficiency and this is the notion that we will use for the rest of this course   refer time 1  48    we said that if there is an efficient algorithm for problem p i 1  there is an efficient algorithm for problem pi 2  refer slide time 12  54  so  let us again look at the matching example that we did pi 1 so  we said that lets assume that there is an efficient algorithm for perfect matching so this  we said implied an algorithm for finding the size of the maximum matching and this implied an algorithm for finding a maximum matching this is what we saw last time ? now even though this algorithm  it is efficient it is not necessary that when you do this transformation the new algorithm is efficient in this case  it actually is true the reason is this algorithm is called at most n times so  the running time were let us say n to the c this was called at most n times running time here is roughly n to the c plus 1 and this called this at most n times  so this turns out to be n to the c plus 2 so  if there is a c if there is a constant c so that this running time is bounded by n to the c there is some other c possibly may be c plus 2 2 c so  that the new algorithm runs in time n to the c so that is what i mean by saying that if there is an efficient algorithm for problem pi 1 there is a efficient algorithm for problem pi 2  refer slide time  14  54  for our next problem i need to define a couple of terms these are concepts of hamiltonian path in hamiltonian cycle some of you have seen this before  but let me define this anyway so  a hamiltonian path in a graph g is a path of length n minus 1  where n is the number of vertices so  path of length n minus 1  where n is the number of vertices in other words it is a path which spans all vertices other words  it is a path that contains all vertices so  it must be a path in the graph and all vertices in the graph must be present clearly  the length is n minus 1  the number of edges which is length n minus 1  this is hamiltonian path the hamiltonian cycle is very similar ; it is a cycle which spans all vertices one single cycle which spans all vertices  refer slide time  16  30  let me write this down to a hamiltonian cycle in a graph is a cycle which spans all vertices so  let us look at an example  so supposing this is a graph now  here is a hamiltonian cycle in the graph  this edge  this edge  that edge this edge so  this is a cycle which has all the vertices in the graph and if i remove any one of these edges it gives a hamiltonian path in the graph any one of these edges in the hamiltonian cycle  this gives a hamiltonian path in the graph so  if a graph has a hamiltonian cycle  it has hamiltonian path the reverse of course may not be true so for instance  the graph is just a path if the graph is this path then it has a hamiltonian path and it has no cycle  refer slide time  18  43  so now we have defined these two terms  let us put them to use we are talking of reductions so we are going to see  how closely are these two problems or two problems that i am going to define right now related one is the problem of finding the hamiltonian paths and the other is the problem of finding the hamiltonian cycle so  the first problem is called hamiltonian cycle let us see  it is this input is a graph g the questions you ask does g have a hamiltonian cycle ? this is problem hc similarly  i have problem hp not to be confused with hewlett packard and the input to the graph g  the question does g have hamiltonian path ? so for this problem  you want an algorithm that says you fit in a graph g and this algorithm should say either yes the hamiltonian path or not the graph does not have that is for this problem for this problem  it should say yes put the graph as hamiltonian cycle otherwise no the graph does not have hamiltonian cycle so  the input is a graph  the output is yes or no  yes if it has this structure either the path either the hamiltonian path or cycle no  it have so  these are the two problems and clearly they seem to be related i mean hamiltonian cycle is a cycle that spans all vertices hamiltonian path is a path that spans all vertices so  one would expect these two problems to be similar in some way in fact similar now  we have a clear notion of what similar must be ; we want to say something like this if we can solve hc which means if there is an efficient algorithm for hc  there is a efficient algorithm for hp and there is a efficient algorithm for hp  there is a efficient algorithm for hc  refer slide time  21  10  so  these are the questions that we are going to address now an efficient algorithm for hc which is hamiltonian cycle design a efficient algorithm for hp  this is hamiltonian path so  how do we do this ? so here is my here is the algorithm hc if i feed in a graph to this algorithm it will say whether or not it has a hamiltonian cycle now  what i want to design is one for hp i want this feed in a graph this would say yes if it has a hamiltonian so  supposing the input is some graph g i want to determine if this graph has a hamiltonian path or not and i have to use this algorithm somehow supposing i feed the same graph into hc  two things can happen hc can say yes or it can say no so  let us try this let us see what happens in each of these two phases  refer slide time  22  42  i want to determine if g has a hamiltonian path  does g have a hp ? this is the question so  i feed g into hc there are two cases this fellow can say yes and what do you think you can conclude if it has a hamiltonian cycle does it have a hamiltonian path ? well absolutely it must have a hamiltonian path so  if g says this hc says yes for g you are done great it has hp so  you can say with surety that if it has a hamiltonian circuit it has hamiltonian path what if it says no ? now  we really do not know may be it does not have an hp in which case it does not have a hc and so we are all fine the problem is the graph could have a hamiltonian path  but it need not have a hamiltonian cycle so that case  we are not just able to distinguish using just a blind way of using this sub routine so  we have to be a little bit smarter in using this sub routines and that is the duty of the subject you have to figure out how exactly do i use ? this sub routine hc effectively to get a hamiltonian path this is what you do ? you should may be try it you see the solution i am going to give right now in fact  these transformations to the graph that i am going to do you have seen earlier it is a hint so  what we do is this  refer slide time  24  52  so here is g  i take g as it is i added this is g  i added new vertex u connect u to everything in g  u is connected to every vertex in g now  this graph i feed into hc i have an original graph where i want to determine if it has a hamiltonian path or not i add a vertex i connect it to every other vertex in g and feed it into hc now  if hc says it could say yes or no so  what i am going to do is if hc says yes then i will also say yes  the graph g has a hamiltonian path if it says no  i say g does not if hc says yes then the output of the algorithm for hp will say yes if hc says no i will say no on this new graph ; however  it is on the new graph so  does this work does not this work well it does work we will see why there are 2 things we have to show we have to show that if it answers yes this answer must be correct which means this graph g must have a hamiltonian path and if it answers no the graph should not have  refer slide time  26  27  so  the two statements that we need to prove is the following are the following here is g vertex u is adjacent to every other vertex in g so  call this new graph g prime new graph is g prime so  i want to show that g has a hp if and only if g prime has hc this is an if and only if statement  so if there are two things the two things correspond to yes in output for instance  we have to show that g has an hp this implies that g prime has a hp and the no output g does not have a hp this implies that g prime does not have hc so  these are the two things so  assume that g has a hp  we need to show that g prime has an hp and i think this should be fairly obvious  there is g here is my hp i do not know how this looks ; it is a path which runs between  which has all the vertices in g now  i need to construct an hc which is a hamiltonian cycle in g prime and that is easy i just add these two edges so  this path starts at some vertex and it ends at some vertex it starts at x and ends at y  the hamiltonian cycle  i get by just appending u to x and y i add the edges u x and u y and i get a cycle in g prime and you can see that i can go the other way also so  given the hamiltonian cycle in g prime i can construct a path a hamiltonian path in g and the construction is similar so here  supposing this is graph g prime and i have a hamiltonian cycle it goes around like this in g prime now  this vertex u sits somewhere  this is my vertex u if i remove this vertex u from the graph what i am left with is the hamiltonian path in g this path will start from this vertex x and will end at this vertex y so  the path starts at x it will go around and end at y and this path is in the graph g so  we have proved statement both ways that if the original graph had a hamiltonian path the new graph have a hamiltonian cycle if the original graph did not have a hamiltonian path the new graph will not have a hamiltonian cycle what about the other way round ? we said given an algorithm for hamiltonian cycle we could construct one for hamiltonian path  refer slide time  29  47  what about the other way round ? the other way round would be this given an algorithm for hamiltonian path constructs one for hc and from now on it  if you are given a algorithm it is efficient and you want to construct one that better be a hp so  these are all well i just said algorithm  we want efficient algorithm the previous one was clearly efficient i had to call the hamiltonian cycle routine exactly once i do a small modification to the graph the input size does not go up by much and i call the hamiltonian cycle routine exactly once so that running time is roughly this the old algorithm was sufficient the new one was so  how about this ? so  you are given a algorithm for hp which means if you are given a graph g it says yes or no and what i want is for hc ? the given let me call this g prime to distinguish from this g so  this is what we want  this is what we want this is given to us so  somehow again i have to use this routine for hp efficiently to get this routine for hamiltonian cycle so  how do i do ? i could let us try the usual trick feed this graph into hp and it says yes if it says yes then well it could have a hc or it may not have an hamiltonian cycle on the other hand if it said no then i am here that the graph does not have the hamiltonian cycle in fact  does not have a hamiltonian part so  if it says no i have no we are all fine  but if it says yes  again we run into a similar problem we can not decide whether it has a hamiltonian circuit or not  refer slide time  32  20  so  what do we do ? so  here is there is an attempt take the graph g so  i want to use let me just put this on the side so this is  what i want to use and i want to create one for hp so  what do i feed into hp so  take this graph there is a input graph g so  take g now remove edges from g one by one so  let us say i remove edge e now i feed into g prime and i will see whether it says yes or no now  if it says no for any edge  then clearly the graph will not have a hamiltonian cycle  why is this ? so  supposing g has a hamiltonian cycle then if i remove any edge from g  the graph still has a hamiltonian path so  if i take g and remove any edge the hamiltonian paths must keep saying yes is this a good algorithm i mean can i sort of say that remove every edge from g feed into hp ? if it says yes all the time then yes g has a hamiltonian cycle if at least once it says no then g does not have hamiltonian cycle let me write this algorithm for each edge e call hp with g minus e if hp says yes for all edges then g has an hc so  you output yes otherwise output no so  if it says if hp says no for any input then you say no so  this is the algorithm well the question is does this work what do we mean by this question what does it mean for this to work or not it means that whenever you output yes so  whenever you output yes the graph better have a hamiltonian cycle and whenever you output no the graph should not have a hamiltonian cycle let us take both of these in turn and see whether we can prove this  refer slide time  35  29  so  let us take no first  so that is easier so  you output no if for some edge e prime g minus e prime does not have a hamiltonian path so  if i remove this edge e prime then the graph does not have a hamiltonian path we have seen that this implies that g does not have hamiltonian cycle if it had a hamiltonian cycle then if i removed any edge out i would still have a hamiltonian path in the resultant graph so whenever we output no  we are in the clears we are correct the algorithm is always correct  what about the yes case ? it seems very reasonable that for every edge if i remove it and ask there is a hamiltonian path and it says yes there should be reasonable that there should be a hamiltonian cycle so  if there is a hamiltonian path after removal of any edge is a good chance that one feels that graph should have a hamiltonian cycle  but this is false so  this statement is false  refer slide time 37  11  let us see an example  so here is an example so  this is an example of a graph such that removal of after removal of any edge graph has a hamiltonian path  but the graph does not have hamiltonian cycle so  there are two things to be checked with this graph that it does not have a hamiltonian cycle  but if i remove any edge from this graph any edge at all then this graph must have a hamiltonian path let us check both of them now  this graph does not have a hamiltonian cycle and to see that we focus on this vertex in the middle if i remove this vertex  then this graph becomes disconnected there is a vertex in this graph so that if i remove this middle vertex and the graph becomes disconnected this can not happen if the graph has a hamiltonian cycle if the graph has a hamiltonian cycle  if i remove any vertex the resultant graph will have a hamiltonian path and will in fact be connected so  if a graph has a hamiltonian cycle if i remove any vertex it must remain connected that is not true for this graph that is the reason why this does not have a hamiltonian cycle  we still have to prove one more thing which is that if i remove any of these edges in this graph then it should not have it must have a hamiltonian path  refer slide time  39  26  so  let us look at these edges one by one let me draw this graph again so  let us focus on this edge supposing i remove this edge does there exist a hamiltonian path ? well the answer is yes choose this edge choose that edge choose this edge you come down this way then you choose this edge then  you go down this way to this edge that edge there are other ways of doing this so  you go up and down like this and this you can see the hamiltonian path so  removal of this edge here causes no problem so  let us look at some other edges so  let us remove this edge what if i remove this edge ? so in this case  i can start here come down here go up here there it says this and this so  this will give a hamiltonian path so  you start here and you go up and down like this go up go down up to this and this you can see gives a hamiltonian path so  removal of this edge is also not a problem let us look at one more lets remove this edge if i remove this edge  the graph still has the hamiltonian path this this this this so  you go down this way go back keep going down all the way up here go down and back here so  if i remove this edge too there is a hamiltonian path now we just see that all edges in the graph are similar to one of these three for instance this edge is similar to that right this edge at the bottom is similar to this so  all edges on this side is taken care of these two edges are taken care of  this is the middle edge these two are similar and the graph is symmetric about this vertex so  all edges on this side on the left hand side are also taken care of so  these three cases are enough to enumerate to sort of hopefully convince you that removal of any edge in this graph leads the graph with a hamiltonian path  refer slide time  42  18  so  this type the algorithm that we described is wrong this fails so  it looked like a reasonable thing to try  but it fails  refer slide time  42  35  so  what you do is this ? so  here is my graph so  here is let me take some edge u v now  what i will do is remove the edge u v remove this edge and i will attach to other vertices this is u prime that is v prime so  i have a original graph g i remove the edge u v so that is g1 from here and then i attach two additional vertices u prime and v prime now  i ask does this graph have a hp  does this graph has a hamiltonian path the question we wanted to answer was does this have a hamiltonian circuit now let us see  supposing this graph did have a hamiltonian circuit supposing this had a hamiltonian circuit not only that the hamiltonian circuit pass through the vertices u and v in order which means the edge u v was present in some hamiltonian cycle in the graph then  let us notice that this new graph has a hamiltonian path so  what does it mean for this graph to have a hamiltonian cycle passing through u v ? it means there must be a cycle this way it goes through all other vertices like this and also u v so  let us look at the same cycle  where i take the same cycle and i add these two edges and i get a hamiltonian path in the new graph so  if the old graph had a hamiltonian circuit then  if i picked an edge which was present in the hamiltonian cycle and ask if this has a hamiltonian path then  it will have a hamiltonian path that is the first thing to notice now here is the second thing to notice supposing i took a graph like this so  i took a graph and here is g i took a graph g edge u v i removed u v attached u prime and v prime and i ask if this has a hamiltonian path supposing this says yes what does the hamiltonian path look like ? now  the path has to these two vertices have degree 1 so  they have to be the end points of the path u prime and v prime have to be the end points of hamiltonian path which means the hamiltonian path has to look like this it has to start at u prime and then go through all vertices in g and then go back up to v prime this is how the hamiltonian path should look like then  what we do is this ? we remove these two edges and then put this edge u v which you have removed this will give you a hamiltonian circuit in the original graph now  our algorithm is almost complete  what we do is ? we do this for all edges in the graph we remove that edge attach this u prime v prime and ask if it has a hamiltonian path if any edge it says yes then we say yes it has a hamiltonian cycle if for all edges it says no then we say no  it does not have a hamiltonian cycle so  let me write this algorithm  then we will argue that this algorithm is in fact correct  refer slide time  46  19  so  the algorithm is this for every edge e  do the following so  remove e from g  let us say e is equals is u v then  add vertices u prime and v prime and connect u prime to u and v prime to v this lets say graph g lets say e  for an edge e i get this graph g e now feed g e ; that means  input g e to an algorithm for hp if this algorithm says yes for any e output yes  if it says no for all e you output no so  otherwise output no so you output no  if it says no for all edges you do this you remove this edge add these two extra vertices and ask whether it has a hamiltonian path if it says no for all these edges then you output no so  this is the algorithm and we have to show that this is correct which means if the original graph had a hamiltonian circuit then  this algorithm will always say yes if the original graph did not have an hamiltonian circuit  it will say no and it is efficient if the hamiltonian path algorithm runs time polynomial in the input size  so does the new algorithm so  these are the three things to check  let us just make sure that the algorithm is efficient first you call it once per edge the hamiltonian path routine is called once per edge and you do not change the input size by much you just remove one edge and you add two more edges so  the number of edges goes up by one so  the input size does not go up by much and you are calling it at most m times so  the original running time was bounded by some n to the constant it is still bounded by some n to the constant again there are two cases case 1 is when the algorithm outputs yes then it has to be correct second case is the algorithm outputs no this no answer also has to be correct  refer slide time  49  54  let us take both these cases so  case 1  so the algorithm outputs yes so  this implies ; that means  for some edge hp must output yes this means there is there exists an edge e in g such that g e has hamiltonian path so  here was g and here is e i get g e by removing and adding these two so  this had a hamiltonian path and now we have seen this argument that if this has a hamiltonian path then this has a hamiltonian cycle essentially you take the hamiltonian path here remove these two edges and add this edge back together hamiltonian cycle here so this case is done  refer slide time  50  59  the algorithm outputs no this means for every edge e g e does not have hamiltonian path so  we need to see that this implies that g does not have hamiltonian circuit so  this is what we need to show ? so  we need to show that for every edge e if g does not have hamiltonian path  then g does not have a hamiltonian circuit now  it is easier to prove the contra positive  which means this statement  which i have written is equivalent to saying the following that this let me write the equivalent statement on the right hand side so  i am going to say that g has a hamiltonian cycle this implies if g has a hamiltonian cycle what should happen ? this means this statement can not happen it is not the case that for every edge e g does not have a hamiltonian cycle it means there exist an edge e such that g e does not have hp so  these two things are the same so  these two are equivalent so  let me just sort of say what i am doing so  you want to prove that a implies b you are proving that not b implies not a i want to prove that this implies this is a and this is b so  i want to prove that a implies b all i am doing is not b is this implies not a which is this because i am going to prove that if g has a hamiltonian cycle there exists a edge such that g does not have a hamiltonian path and you can see this also we have proved  refer slide time  53  37  if g has a hamiltonian cycle  so that is g so that is my hamiltonian cycle  if i remove any edge in the hamiltonian cycle so  if i remove this edge and attach these two vertices this resultant graph has a hamiltonian path which is what we wanted to prove so  we want to prove that g has a hamiltonian cycle there exists an edge such that g does not have sorry g does have a hamiltonian path i am sorry about this g does have a hamiltonian path and this we have just proved in fact  this edge e can be any edge in the hamiltonian cycle so  remove any edge in the hamiltonian cycle for that edge removal of that edge g e will have to remove that edge attach those two vertices this new graph g e will have a hamiltonian path  refer slide time  54  37  so  when we looked at these problems there were two kinds of problems we looked at for some problems we said that we wrote input output for certain other problems we said input question  now when i looked at problems like with finding a matching of maximum weight of finding a matching with maximum number of edges they were of the form input and output because  the output was edges which were there in a matching of maximum size now  for instance does the graph has a hamiltonian cycle that is the question the input is a graph does the graph has a hamiltonian cycle i ask a question the answer should be yes or no now  the difference was for these kinds of problems the answer was a single bit  it was yes or no for these problems the output spend many bits so  these had a single bit as the output these had many bits as output for instance edges in a matching in fact  let us look at the hamiltonian cycle problem we said input is a graph g  does g have a hamiltonian cycle if yes or no so  the answer is yes or no on the other hand i could have asked for a output for saying output the hamiltonian cycle input is a graph g output the hamiltonian cycle now  this problem seems to be somewhat similar to the previous problem only here the output has many bits we have to output every edge in a hamiltonian cycle we have to choose one hamiltonian cycle and output the edges from the hamiltonian cycle so  we would like to distinguish between problems  where we require the output to be 1 bit and problems  where we require the output to be many bits these we call decision problems and these we call search problems  refer slide time  56  58  so  typically here for decision problem let me write decision and search so here  output is 1 bit here is many bits so  here you want to decide whether true or false here you want to search for a solution and output the solution that is why this i guess that is why these are called decision and these are called search problems so example  does g have a hamiltonian cycle so  this is an example of a decision problem example of a search problem output a hamiltonian cycle if one exists the input is same to be both which is a graph g in one case you just want to know g has a hamiltonian cycle or not the other case you want hamiltonian cycle output you want actually the edges we have now  how do these two things  how do these two problems relate to it is one easier than the other is one harder than the other ? now it turns out people have just observed it that they are related to each other quite closely in the sense that one is easy the other one also turns out to be hard  refer slide time  59  08  so  let us take this hamiltonian cycle problem and see this so  i have so the input is a graph g now there are two problems  one is does g have hamiltonian cycle  this is 1 and the second thing output a hamiltonian cycle if one exists this is problem 2 that is problem 1 now supposing there is a algorithm for problem 2 so  which means given so if you feed in a input graph the sub routine outputs the hamiltonian cycle can we find a algorithm for problem 1  yes i hope all of you answered yes so  the answer has to be yes so  you just feed the graph in so this sub routine and look at the edges which have been output so  if it forms a hamiltonian cycle  then you just output saying that yes it does in fact have a hamiltonian cycle and if it says no it does not the original routine says no it does not have hamiltonian cycle  i just then you just output things  so that is trivial our job is to look at the other way which means i have an algorithm for problem 1 i have an algorithm for this now i want a algorithm for this so given let me add efficient just to remind you that we are talking of efficient algorithms given an efficient algorithm for problem 1 construct one for problem 2 so  this is what we really want to do and how do we do this we have actually done something like this before which was with respect to matching ? s when we wanted to find the edges in the matching of maximum weight we used this sub routine which answered something else and we somehow got these edges out and we used absolutely the same trick here so  what we do is this ? we will look at the edges one by one let us say the edges e 1 e 2 e 3 e 4 and so on so  we look at the edges one by one i remove e 1 now i ask if the graph has a hamiltonian cycle or not if the graph has a hamiltonian cycle it does have a hamiltonian cycle which does not use this edge e 1  so i just throw away e 1 similarly  i look at e 2 i remove e 2 and ask if the graph has a hamiltonian cycle if it says if the sub routine says yes i just throw away this edge and i keep doing this supposing i remove an edge and it says the graph does not have a hamiltonian cycle then you also know that this hamiltonian cycle uses this edge so  these edges you put back in and you do not want to throw these edges so  you just do this look at all edges one by one for those edges  where the algorithm says that it does not have a hamiltonian cycle you throw away those edges  but edges for which the algorithm says that there is if you remove there is a hamiltonian cycle you throw away those edges for edges when you remove those edges and ask the question it says no there is no hamiltonian cycle ; that means  this edge must be present you put it back here so  you do this for all edges and at the end of it you throw away some of edges and some edges remaining the claim is that the edges which remain must form a hamiltonian cycle firstly  the graph must have a hamiltonian cycle  because whatever remains whenever we ask whether it has a hamiltonian cycle it says  yes so  the graph that remains must have a hamiltonian cycle  why should not there be other spurious edges floating around  why should it not have apart from the hamiltonian cycle other edge supposing it had some other edge when you remove this edge and ask whether the graph is hamiltonian cycle the answer should have been yes so  the answer is yes you would have thrown this edge away so  there are no spurious edges the only edges which are present are hamiltonian  refer slide time  1  04  29  so  let me write this so  just to recap we are given a algorithm that says whether a graph is a hamiltonian cycle or not we are constructing using this as a sub routine  we are trying to find out edges in some hamiltonian cycle so  consider edges one by one for edge e i remove e i and ask hc if the resultant graph has a hamiltonian cycle if i says yes throw away e i else retain e i that is it you just do it for all edges finally  what remains is a hamiltonian cycle which means what remains are edges from a hamiltonian cycle so  you can see that both problems are quite related i just need to call the other algorithm n times able to actually extract the hamiltonian cycle so  it is been observed for most problems there are of course  problems which do not fall into this which do not fall into this property but for most problems if i can sort of decide then i can also search for a solution so  if the decision version of a problem is easy then even the search version is easy on the other hand  if the search version is hard then social decision word  so this is the reason they are going to focus on decision versions of problem from known so  we are just going to look at the problems which have whose output is 1 bit and say yes or no  refer slide time  1  06  53  we are now ready to define the class np so  before i formally define this let me  sort of give you a informal definition let us take our favorite hamiltonian problem so  hc the input is a graph g and the question does g have hamiltonian cycle so  let us look at this one just consider now we are going to do something slightly different from what we have been doing we are not going to design algorithms for the time being what we are going to do is have a game between two players there are two players now one i will call a prover and one i will call a verifier these are two people prover and verifier  prover is all powerful ; he just knows everything the verifier has limited resources and it does not trust anybody sort of does not trust especially he does not trust the prover he has limited resource now  the prover and verifier sort of lets sitting in a room and there is a huge graph in front of just imagine that somebody has drawn a huge graph in front of them the prover looks at the graph and he says this graph has a hamiltonian cycle he is powerful he can figure these things out and he says this graph has a hamiltonian cycle now  the verifier is skeptical he does not believe the prover so  he says the why should the graph have hamiltonian cycle and what is the deal  then what should the prover do the prover wants to convince the verifier that he is all powerful and he has made the right statement he said the graph is hamiltonian cycle and the graph has a hamiltonian cycle so  he wants to convince the verifier that the graph has hamiltonian cycle so  what could he do ? well if you think about this for a minute what the prover could do was pick out the edges from the graph which form some edges which form some hamiltonian cycle so  pick out some hamiltonian cycle from the graph and tell the verifier that here are a set of edges which form a hamiltonian cycle now all the verifier needs to do is look at these edges make sure they form a make sure that they form a cycle you just have to verify that these set of edges form a hamiltonian cycle which is easy to verify even though he has limited resources and he is not too intelligent this much he can do so  the prover has somehow convinced the verifier that this graph has a hamiltonian cycle so  the prover says that there is a hamiltonian cycle in this graph the verifier asks why the prover then picks out the edges these are the edges look at these edges they form a hamiltonian cycle in a graph the verifier then verifies that yes indeed the graph did have a hamiltonian cycle and we have just proved that hamiltonian cycle is in the graph is in the class np i have not defined the class np so  you really do not know what it is but  this is all there is to proving things proving the problems are in the class np so  let us do this prover verifier game for some other problem just to get sort of just to get used to it let us say the input is an integer t so  you want so the output factors let us say two non trivial integers t 1 and t 2 such that t equals t 1 times t 2 so  you want to find factors of these  refer slide time  1  12  08  now  this is a search question it is not a decision question so  the corresponding decision question is the following so  here is the question is t composite which means can t be written as t 1 times t 2 where t 1 is not 1 and t 2 is not 1 so  these two are non trivial factors of t so  let us play the same prover verifier game so  here is the prover whom i will say p here is the verifier v so  p says t is composite  now v asks why  now t has to convince v that in fact  it is composite so  prover sends t 1 and t 2 across so once  he sends t 1 and t 2 across vmultiplies t 1 and t 2 and checks the result with t if its equal to t of course  he is convinced that t is composite and in fact  t 1 and t 2 are two factors go back and look at this hamiltonian problem again so  let us look at this again  now if the answer was yes it does have a hamiltonian cycle which means the prover says it has a hamiltonian cycle then the prover could convince the verifier that the graph does have a hamiltonian cycle  what if the prover says no it does not have a hamiltonian cycle so  usually he says yes this guy asks why  so he sends edges in hc and this guy verifies now supposing  he says no  it does not have a hamiltonian cycle now the verifier asks why and well what does the prover do could he convince  how could he convince the verifier the given graph does not have a hamiltonian cycle now  think about this for a little  while you see that it is a difficult question to answer  how could he convince somebody that this graph does not have a hamiltonian cycle well the only way seems to be you just try all possible subsets of edges whether they form a hamiltonian cycle or not if known of them form then it does not have  but this is a brute force method and the verifier does not have so much time he does not have so  much time to check all possible subsets of edges so  there is no easy way for the prover to sort of convince the verifier that g does not have a hamiltonian cycle so  it looks like the answer to the question  whether it is yes or no if it is yes the prover can convince the verifier if it is not the prover is not able to convince the verifier so  they behave sort of differently design and analysis of algorithms prof sunder vishwanathan department of computer science engineering indian institute of technology  bombay lecture  28 np completeness ? 111 we are ready to define the class np we will first do this using an example so  the example we will pick a problem  the problem we pick is hc  refer slide time  00  57  the input is a graph g and the question is does g have the hamiltonian cycle so  this is the question now  forget algorithms for the time being so  we are not going to discuss algorithms  what we are going to do is discuss a game between two players there are two players we will call them prover and the verifier  we will see  why they are called prover and verifier very shortly this prover think of them as all powerful  he knows all answers that he supposed to and even answers is not supposed to and the verifier has limited resources in particular  he does not have too much time  limited resources or this includes time now  the prover and the verifier meet so  they are sitting in a room and they see a graph in front of it now  the prover looks at the graph and says this graph is a hamiltonian cycle the verifier  who is bid trustful of the prover  asks him why  why does the graph have this now  the prover can convince the verifier that the graph does have a hamiltonian cycle very easy  how does he do it he just gives the edges in the graph  in the form of a hamiltonian cycle so  he picks any hamiltonian cycle in this cycle and he tells he tells the verifier  look at these edges  these edges in the graph form a hamiltonian cycle the verifier  even though is sort of limited in resources this much he can do  given a hamiltonian cycle in a graph you can verify that these edges in fact  do form a hamiltonian cycle so  let me just write this conversation between the prover and the verifier so  the prover says yes so  this yes is an answer to this question above  yes here is an answer to the question does g have a hamiltonian cycle the prover says yes  the graph has hamiltonian cycle or there is an input graph that both of these are looking at both the prover and the verifier are looking at the prover says yes  the graph has a hamiltonian cycle the verifier then asks why and the prover supplies edges in some hc in g and the verifier verifies that these edges indeed  form hamiltonian cycle so  this is the conversation let us go over it again  the prover says yes to this question and to supplement this statement  he supplies the edges in some hamiltonian cycle in g and the verifier looks at these edges in the hamiltonian cycle and it is easy for him to verify  now that these edges indeed do form a cycle this is a game that we will sort of consider and in some ways  we have already proved i have not defined what a np is but  we have actually proved that the hamiltonian cycle problem is in the class np so  let us formally define the class np and then we will see  why this constitutes a proof that hamiltonian cycle is in it so  hamiltonian cycle problem is of course  a decision problem the question we ask is does there exist a hamiltonian cycle or not it is to be answered yes or no so  it is a decision problem  refer slide time  06  15  so  a decision problem here is the definition  a decision problem is said to be in the class np  if for all yes inputs let me write this down then i will explain what this means if for all yes inputs  there is a proof  you can think of this as an advice or a string so  this is  think of this as a string the prover gives to the verifier so  this advice is what the prover will give to the verifier so  there is a proof using which  it can be verified quickly where  the input is indeed a yes input so  let us now  look at this so  there are some terms that i need to describe so  first thing is what is this yes input  what is quickly and then what does the statement mean  let us focus on the yes input see this problem is a decision problem  since it is a decision problem for each input the answer is either yes or no right so  yes input are those inputs  for which the answer is yes so  we look at only those inputs  for which the answer is yes and we say that for these inputs  i do not worry about what happened to the no inputs for the yes inputs there is a proof  this is a proof that the prover gives the verifier  so  this is proof so  yes input  yes inputs are inputs for which  the answer is yes  this is 1 the proof is what the prover sends to the verifier and quickly in time polynomial in the input size so  i could have said efficiently  i guess that is what i have been using but  quickly efficiently they all mean the same thing  which is that  it can be verified in time polynomial in the input side so  let me just  let go again this definition again  since it is sort of important so  let us move over definition again a decision problem is said to be in np if for all yes inputs  there is a proof  there is a proof using  which it can be verified that the input is indeed a yes so  let us look at our previous example for hamiltonian circuit  refer time  10  57  if for all yes inputs for a yes input  this means  if the graph does have a hamiltonian cycle there is a proof that the prover can give the verifier using  which this proof consists of edges in some hamiltonian cycle and using this verifier can easily verify  in polynomial time whether the graph has hamiltonian circuit or not so  this actually proves that hamiltonian circuit the problem hamiltonian circuit is in the class np  refer slide time  11  41  so  let me highlight the main sort of points in the definition then we will go over this again so  first it has to be a decision problem so  answer has to be yes or no we  look at only yes inputs and on yes inputs supply a proof this proof will help the verifier verify that it is a yes input  which helps verification and this verification has to be in polynomial time so  these are the initial things  it has to be a decision problem  we only look at yes inputs it is only for yes inputs that the prover has to convince the verifier and to convince he can give a proof  which can help the verification so  here is let me sort of draw this diagrammatically so  here is the yes input so  you pick any one of the yes inputs the prover is looking at this  shows the verifier and the prover sends the proof or an advice  sometimes called an advice  refer time  13  28  some string think of zeroes and ones  whichever way the verifier looks at the string and sort of figures out  what the prover means so on yes inputs  the prover can send some proof to the verifier using  which the verifier in polynomial time can verify that in fact  this is a yes input so  let us do one more example  refer slide time  14  06  this example is a composites  the input is a positive integer l  the question is l composite  it is the question so  we want to know whether composites this problem composites is np or not first thing  it is a decision problem so  we can it is a problem that we can look at  which could be in the first thing it has to satisfy that the first property it has to satisfy is that it must be a decision problem which it is the question asked is l is composite and the answer is either yes or no so  what is the next thing for all yes inputs  there must be some way for the prover to convince the verifier that in fact  it is composite so  let us just do this so  we are looking at yes inputs which means  l is composite so  what is the proof that the prover can give verifier so  well i guess  it must be sort of obvious by now  that if you  now if you know that something is composite and you want to convince somebody that it is composite  you can just give the two factors so  you give so  the proof that the prover supplies to the verifier is a factorization so  he gives let us say l 1 and l 2 so  this is the factorization of l so  the verifier checks that indeed l equals l 1 times l 2 and neither of these is 1 okay l 1 and l 2 can not be 1 so  this proves that this is the proof that the prover can give to the verifier  to convince him at some number is composite so  one question i guess which most people sort of encounter when this definition is made you know  you focus only on the yes inputs what about the no input so  take composites for the time being input is a positive integer l  question is l composite let us say the answer is no  the prover says no it is not composite now  how can he convince the verifier that it is not composite  which means you have to somehow convince the verifier that it is prime  you know the only sort of numbers between 1 and the number that divides it are 1 and the number itself  nothing else in between well  to think about it  this is not this does not seem to be as easy actually  in this case there is a way to do it but  ways to do it is certainly not easy  there is a recent research  which says even without an advice  one can figure out whether a number is prime but  that is way beyond the syllabus  for this course so  let us again do this for the hamiltonian cycle problem also  just to get the hang of this here  now  refer time  18  26  when the answer was yes yes the graph has a hamiltonian circuit ; this proof can be supplied right edges having hamiltonian circuit in this supposing the prover had said no  if we look at the no inputs how can a prover  how can you convince somebody that graph does not have a hamiltonian circuit so  if you think about it for a while  you see that this is actually a very difficult thing supposing  you say that this graph does not have a hamiltonian circuit by some way or method whatever  you know this how  do you convince somebody else that it does not have a hamiltonian circuit  seems to be extremely difficult thing so  only way it looks like is the other person with or without your help  looks like all possible collections of edges and sees whether these selection let us say  all edges of it take n edges where  n is the number of vertices the graph takes all possible subsets of n edges and check whether they form a hamiltonian circuit  now this could take a long time the time taken to go through all this exhaustive technique is certainly not may  certainly not be polynomial in the input side so  and up to date nobody knows anything of better  so for at least hc and many of the problems that we look at it looks like the yes input behaves differently from the no input when the answer is yes  there is an easy proof by which you can convince the other person that it is the yes input the no input seems to behave very differently for the class np  all that we require is that when the answer is yes  there should be an easy way to convince somebody else that in fact  it is a yes input so  how do you prove that something is in np  we have seen two examples but  let us just sort of note this down to prove something is in np  you need to do two things  refer slide time  20  59  you have to say the proof to prove that a problem is in np  you have to do the following things you have to say  the proof that the prover sends to the verifier and then how does the verifier check so  these are the two things that you need to send  this proves by the way for only yes inputs for yes inputs  you must say what is the proof given  any given yes input  what is the proof that the prover sends to the verifier and once the verifier receives this and he looks at the input  the verifier should you should say how the verifier checks  how the verifier is convinced by looking at this proof and the input that the input is  in fact yes inputs and for the hamiltonian circuit case and for composites  for the problem composites we specified both in the composites case for instance  the prover sends the factorization and the verifier just multiplies two of them and checks whether equal to so  we have  in fact now defined a class np we will see many other examples in fact  i would encourage you to look at problems you have previously come across  at least the decision problems and try and prove whether  some of these fit into the class np so  one importance of this class np is why i define this class np at all well a large number of problems  which occur in real life fall into this class np in the following sense i mean  if you look at problems in real life they  most of them you do not want problems with a yes no answer  the answer is usually many bits but  these problems can be transformed into a problem  which is not much easier in fact  as hard as the original problem where  the answer is one bit and when one looks at these versions of real life  very often they can not be that is why this class is very important because  a large number of problems that we will encounter and that people have been encountering in real life  they all fall into this class defined yet another problem but  this is sort of central problem that will work around with so  this will be a problem in np and it will be the central problem  you would have seen something like  this in many of your previous courses in computer science if you have done logic  you would have seen something like this ; you would have seen this boolean circuit etc  etc so  many of these concepts that we will deal with and will define should not be new to you anyway  i will define the problem the problem is this i need to make few definitions  before i get on to the problem  refer slide time  24  40  so  a boolean variable is a variable  which takes value  which takes values in the set yes  no it takes two values  yes or no now  if x is a boolean variable then x bar is a negation  is called the negation  is called its negation and if x is we would in fact  may be yes or no will also sort of alternate between true or false  instead of yes and no essentially there are two values yes no  true false or 1  0 and if let us say x is true then x bar is false and if x is false then x bar is true and x bar is called the negation some people also use  x is also used instead of x bar a literal is either a boolean variable or negation so  both x and bar are literals a clause is an or that will define what an or is of literals so  or is actually the usual logical or for example  here is an example a clause x 1 or x 2 bar or x 3 bar or x t so  this symbol or so  clause is just an or of literals and this clause evaluates to either true or false  depending on values that these variables here if any one of them evaluates to true  then the clause evaluates to true otherwise  it evaluates to false for instance if x 1 is true the clause is true if x 2 bar is true then it is true and so  on any one of them  evaluates to true it is true it is false if all of them are false  which means x 1 should be false x 2 bar should be false  which means x 2 should be true x 3 bar should be false x 3 should be true and x 3 should be false all of them are false then it is false otherwise it is true  refer slide time  28  39  let us let me write this so  it is false if all literals are false otherwise true you could have said it is false if and only  if all literals are false this is the usual or  and is coming up right now  refer slide time  29  19  so  a boolean formula in cnf is an and of clauses so  i have c 1 and c 2 and c 3 and c k and each of these is a clause  this symbol is and this is again the usual symbol that we use in logic so  this is another definition so  boolean formula in cnf is an and of clauses  each of them is a clause c 1 c 2 and c k and c 1 and c 2 and c 3  so on up to c k  this i will call a boolean formula in cnf  cnf stands for conjunctive normal form so  each of these clause remember is an or of literals so  this is an and of or ? s and it is called the conjunctive normal form so  i just have to tell you the logical behavior of and  which is again the usual thing this formula is true evaluates to true if  every clause evaluates to true otherwise it evaluates to false so  maybe i can just write this formula  evaluates to true if and only if every clause evaluates to true if any one of those clauses is false and this formula evaluates to false so  what do i mean by evaluates to false it means  when you give values to these variables then if it is a variable then it evaluates to true if it is the negation evaluates to false then you check each clause whether  it is value is true or not that you check a clause evaluates to true if  any one of those literals evaluates to true and then you look at the formula  you look at the collection of these clauses and this evaluates to true  if all clauses evaluate to true of course these are special boolean formulae  which we call cnf  which is you have and ? s outside and or ? s inside of course you can form general boolean formula  which and ? s or ? s and not ? s any kind of way you like but  these are the only boolean formula that we are interested in and one can show  but we will not do it one can show that every boolean formula  take any boolean formula that can be written equivalently in cnf i need one more definition then we are ready to roll so  boolean formula  the boolean formula in cnf is said to be satisfiable  if there exists an assignment to the variables  an assignment assigns a value either true or false to each variable there is an assignment to the variables such that the formula evaluates to true there is some way of assigning values to these variables each of these variables there is a way to assign values either true or false that when you evaluate the boolean formula  it evaluates to true in this case  you call the boolean formula satisfied  if not you say that it is not satisfied so  clearly there are boolean formulae  which are in cnf  which are satisfiable and which are not satisfiable for instance x and x bar if i take one clause to be just x and another clause to be x bar and there is no way that you can satisfy this formula whatever  value you get to x the formula will always evaluates to false  refer slide time  35  30  we are ready to define our next problem that we will study  which sat not to confuse with exams  sat stands for satisfiability so  the input is a boolean formula in cnf  the question is let me call this boolean formula something f  is f satisfiable  this is the question so  now let us observe that sat is now empty so  sat is empty or is this  well it is a decision problem and suppose the answer is yes  we have to only focus on yes input  so the prover says yes it is satisfied then  how can the verifier be convinced that a given formula is indeed satisfied so  what is the prover give the verifier  so that the verifier so  the verifier can look at this advice that the prover has given him  look at the input formula and you know we completely convince that yes indeed the formula is yet satisfied well i hope most of you have answered this question  by now the proof that the prover gives is just the satisfying assignment so  the proof is an assignment to the variables the verifier uses this assignment and checks that the formula evaluates to true the verifier uses this assignment and he checks the formula evaluates to true so  given an assignment you should be able to convince yourself that it is easy to check  what the value of your formula is so  you just plug in the values into for each variables  now check whether each of these clauses are true all clauses are true then you are true the formula is satisfiable so  if it is a yes input that is the proof that the prover can give the verifier using which the verifier can easily check that the formula is in fact  true so  what is the big deal yet another problem in np that is yet another problem in np  but this as we shall see in a minute is a very special problem in np so  thing that makes sat very special is the following theorem of cook  refer slide time  39  01  cook is steven cook  it is a name of a person steven cook and there is no relationship to  refer time  39  22   so  this theorem says if sat has an efficient algorithm then so does every other problem in np let us tire at this time for a minute so  this is among the most important theorems in computer science let us make sure that we understand this by efficient i mean polynomial time  efficient means polynomial time this says that if  there is a polynomial time algorithm for sat then there is a polynomial time algorithm for every other problem in np so  to solve any other problem in np  it suffices to solve just sat i mean  remember that np includes factor  factoring it also includes hamiltonian cycle  matching etc  etc  etc large number of problems that we have seen  so far  in fact is in this class empty and this theorem says that if sat has an efficient algorithm so  does every other problem in np we have proved statements of which are sort of similar we have proved i mean this is the notion of reduction so  we showed that if hp has an efficient algorithm we showed this means that hc has an efficient algorithm  we have done this now  and this was not easy we were  we took a little bit of time and effort to do something  which is as sort of well specified at this both these problems are very well specified  we knew where we were going look at this statement here this says that sat as an efficient algorithm implies every problem in np has an efficient algorithm this is not just one problem somewhere  it is not hamiltonian cycle  hamiltonian path what have you which means every problem in np has efficient algorithm so  i hope you appreciate the power of the statement and this theorem has changed the shape of computer science so  there are two ways of reading this statement  one is to say that if you can solve sat  then you can solve everything else so  concentrate all your attention on sat that is one way to do it lot of people have tried and you know  they have not been able to find an efficient solution for sat so  the other way of to do this look at the statement  sat is among the hardest problems here is a class np and sat is amongst the hardest problems in the sense that you can solve sat everything else follows so  it is among the hardest problems so  sat is one of the hardest problems are there other problems in np  which are as hard of which i can make a statement  which is similar that if this problem is as hard is the you know  i can solve this problem in np and i can solve every other problem in np and i make this statement of other problem and how easy and difficult is it to prove these statements fortunately with cook ? s theorem in hand there are in fact  let me further say there are such problems in np and with cook ? s theorem in hand proving that these problems are among the hardest problems in np becomes a bit simpler i mean how does it go about doing  so here is one so  i take a problem say some problem pi so  what i do is this  refer slide time  44  23  if there is an efficient algorithm for pi then there is an efficient algorithm for sat supposing  i can prove something like  now this is something we have been doing and it looks like we are capable of doing such things we have done it for instead of pi and sat  we have done it for we have taken hp hc and well matching we have seen some matching where  we have taken two algorithms  two problems like this and we have shown if  there is one algorithm for a problem and there is another algorithm essentially we use the sub routine for pi and we construct an algorithm for sat something like this  we can do but  now let us see what this gives us  now here is the other sort of implication that i would like to use cook ? s theorem says that if there is an efficient algorithm for sat there is an efficient algorithm for every problem in it  there is an efficient for all of np which means  all problems in np so  let us just put these two things together  if there is an efficient algorithm for pi there is one for sat and there is one for sat then there is a efficient algorithm for all of np this follows from cook and this result we will have to hook up so  this is up to us to prove so  to prove that pi is amongst the hardest problems in np  all i need to do is prove that if there is a efficient algorithm for pi  there is an efficient algorithm for   refer time  46  36   and once i do this using this implication  i am down to prove that there is an efficient algorithm for all of np so  this would sort of identify pi among the problems in np so  let us identify our goal for the next few lectures  few hours will be to identify more and more problems in np  which are among the hardest  in the sense that if you solve this then you can solve every other problem in it so  here is the first one  i need to make a definition  refer slide time  47  36  so  a clique in a graph g is a subset of v such that lets say u of v such that  when i look at any two vertices in u  there must be an edge between them so  you look at a graph you look at a small subset of the vertex set and in this subset when  i look at the vertices if between any pair of them there is an edge  in the graph then this i will call a clique the subset u such that for every u 1  u 2  till u u 1  u 2 must be an edge in g  must be an edge in g which means  if i restrict myself to u  i must get the complete graph every edge must be present or let us take an example let us take this example a b c d e f let me add this in this example  a  b  c  d is a clique so  i look at these four vertices every possible edge is present i can not replace c with e a  b  d and e is not a clique because e is not adjacent to a and d  if i just take a  b  d that is a clique if  i take just d  c and g that is also a clique so  this is a clique of size 3  this is a clique of size 4 it is a subset of the vertex such that if i take any two vertices  it is in edge between any pair of them  problem is this  refer slide time  50  10  so  this is a search version so  the input is a graph g  output find a clique of maximum size  find a subset find a largest subset you can so  that when i look at these set of vertices it forms a clique so  in this graph for instance  the largest clique that you can find is of size 4  which is a  b  c that is the largest so  the search version is this remembers we are trying to identify problems in np this is not even a decision problem so  you want the vertices in a largest clique so  we need to look at  what we will do is we will define a problem  which is very similar to this  which will be a decision problem and i will focus on that so  the decision version is this clique  input is a graph g and a positive integer k  you see how k creeps in or y creeps in a minute the question is does g have a clique of size k so  this is the question that we look at so  let us focus on these two problems now  the second problem which is a decision problem  in the sense the question the answer is either yes or no so  that is fine  now if you can solve the search problem if there is an algorithm to solve the search problem  clearly there is an algorithm to solve the decision problem just find the clique and find the size if the size is k or larger then you are done the graph does have a clique of size k on the other hand  suppose you have an algorithm for this how do you find  can you solve the search problem well the answer is yes and the construction is very similar to what we have done earlier so  what you do is first find the size of the largest clique you can do a binary search on k  feeding the graph with various values of k and essentially want to find  the largest value i mean if the graph has an edge when you place two  the answer is yes so  find the largest k for which  this is true  this we can do by binary search you can even go sequentially and do this k equals 1 2 3 4 5 6 stop as soon as the answer is no the previous one was the largest clique size of the largest clique in the graph  once you found this size  refer slide time  53  57  let me write this  first the hint i am going to give you more hint  the hint is first design algorithm to find largest cliques size or size of the largest clique once you do this  now you use this to solve this what you do is look at the vertices one by one  throw away vertex and ask does this graph  what is the size of the largest clique in this graph if it is k  i mean if it is you know same as that of g so  let me just backtrack a little bit so  first you feed this graph g and find the size of the largest clique  let us say k 0 so  k 0 is size of the largest clique in g now  i remove a vertex and ask whether  there is a clique of size k 0 in this graph if yes  i throw the vertex away  if no i retain the vertex i go through all vertices and at the end  i would have thrown away every vertex except  k 0 vertices and these will form a clique this is very similar to finding the edges in the hamiltonian cycle  if you did earlier only here we throw away vertices  instead of edges so  i would encourage you to  i strongly encourage you to follow through the steps  which i have said and make sure that you can do this given an algorithm that solves the decision version of the clique problem  describe an algorithm that solves the search version of the clique so  now  the decision version seems to be as hard as the search version of the clique and from now on  we will focus on the decision version can we solve the decision version or not in fact  our goal is to show that the decision version is among the hardest problems in np  which means if there is an efficient algorithm to solve the decision version of this problem  there is an efficient algorithm to solve all problems in it design and analysis of algorithms prof sunder vishwanathan department of computer science engineering indian institute of technology  bombay lecture  29 np completeness ? iv  refer slide time  00  55  let us do a quick recap  we have defined problem called sat the input was sat was clauses c 1  c 2 so on up to c m each clause was an or of literals so  it could be something like x i 1 or x i 2 or so on up to i k so  we had clauses c 1 through c n  each clause was of this form and the formula we consider is the think of the formula c 1 and c 2 and c 3 so on up to and c m and the question we ask is  is this formula satisfiable in other words  is there an assignment of values true or false to the variables such that the formula evaluates to true so  this is the question so  let me just go over this again  you have clauses c 1  c 2 and c m the formula we are interested in is c 1 and c 2 and c 3 and so on up to c m each of these c i ? s looks like this these are you have x i 1  x i 2 and so on up to x i k you could also have negations also for instance if you put x i 1 or x i 2 bar and so on so  each clause is an or of literals and the formula is an and of clauses and the question you ask is  is this formula satisfiable in other words  is there a assignment of values true or false to each variable such that  when you evaluate this formula  the formula evaluates to true  which means  in each clause when i look at each clause there must be some literal  which must evaluate to true either there should be a x and x should be set to true or there should be a x bar  where x is said to be false if x is set to false x bar is true so  in each clause there must be at least one literal  which is  which evaluates to true then we say the formula evaluates to true and such an assignment an assignment to these variables that gives you the value true is called the satisfying assignment so  the question we ask given a formula is does this formula have a satisfying assignment  the formula of which are of this kind  where you have and of clauses where each clause is or of literals these formula ? s are called formula ? s in cnf conjunctive normal form this all this means all are same so  this is the problem sat and we also had a theorem  refer slide time  04  53  so  this theorem is due to cook and it is called cook ? s theorem it states that  if there is a polynomial time algorithm for sat then there is one for every problem in n p so  if there exists a polynomial time algorithm for sat there is one for all problems in n p so  in other words satisfiability  the problem sat is among the hardest problems in n p you can solve this efficiently  which is a efficient algorithm to solve sat there is one for every other problem in n p so  this somehow identified one problem  which is among the hardest problems in sat in fact  there is a name for these problems so  we call these problems n p hard  these are hard problems in n p so  let me define this so  a problem is called n p hard if it has the following property if it has the above property which means  if what is the property  the property is this if there exists a polynomial time algorithm for this problem there is one for every problem in it so  the properties we are looking for is this  if it has a polynomial time algorithm then there is one for every problem in n p to be   refer time  07  32   more correct we say that  if there is a polynomial time algorithm that solves this problem then there is one for every problem in it so  these are the hardest among the problems and hence  they are called n p hard there is one more definition i need  which is this a problem is called n p complete  refer slide time  07  56  a problem is n p complete if it has to satisfy two things let us say  this problem is pi pi must belong to n p and pi is n p hard then it is called n p complete so  apart from being among the hardest problems of n p   refer time  08  28   it should be see when you say a problem is n p hard we do not require the problem to in n p the problem could be outside n p as long as it has the property that if you can solve this problem you can solve everything else in n p it is called n p hard but  for n p complete we require the problem to be in it the problem should be in n p and it should be also be n p that is the only difference so  now we can state cook ? s theorem in a different way the theorem due to cook  it states that sat is n p complete we saw that sat was in n p so  sat belonged to n p and cook ? s theorem essentially says it is also n p hard so  it means  that putting these two things together  we know that sat is n p complete essentially  n p complete problems are just to state it again they are among the hardest problems in n p they have to be in n p and they are among the hardest problems so  our goal is to add many problems to this set this set of n p complete problems and will begin with clique  which also we defined last time so  let me just define problem clique  refer slide time  10  11  so  problem clique is as follows  the input is a graph g and the positive integer k and the question we ask is this does g have a clique of size k so  this is clique  we saw last time that this problem is in n p we gave there was a proof that  the prover could give the verifier so  that the verifier by looking at the proof and the input would check that a given graph  in fact  did have a clique of size k our objective now is to prove the other part  which is the clique is n p hard  which means  if there is a polynomial time algorithm to solve clique  there is one to solve everything else in n p again  we saw i think i sort of entire how will do this and this is how we do it so  what we do is this we have to use cooks theorem to do this so  we show the following  if there is a polynomial time algorithm for clique we will show that this implies then there is one for let me remove this clique so  there is a polynomial time for clique will imply that there is a polynomial time algorithm for sat so  this is what we will prove there is a polynomial given there is a polynomial time algorithm for clique will show that  we will construct a polynomial time algorithm for sat now  cook ? s theorem says that  if there is a polynomial time algorithm for sat there is one for everything in n p and we will use this so  just following these two implications we can see that  we prove what we want so  all that remains now is i will assume that there is a polynomial time algorithm for clique and i will construct one for sat so  here is how we do it so  we have a algorithm for clique  refer slide time  13  01  so  this is given  we are given an algorithm for clique now  what does this state input it takes input as graph g  it takes k and it says yes if the graph is a clique of size k and it says no otherwise now  what we want is this we want a black box  which will take so  this is beyond a algorithm for sat so  the input to the sat is a collection of clause  so c 1 and c 2  c m now  you want to know whether it is satisfiable or not now  what do these two answers  so somehow  we have this sub routine we have to use this as a sub routine to solve this problem so  here is an example  where actually the two things actually look quite different this is a problem on you know it is something to do with logic you have all this variables and you have clause this is a problem on graph so  this is available in a sub routine and we want to construct something so  how do we sort of use something  sub routine which   refer time  14  27   graphs to construct to determine whether a formula is satifiable or not so  here is a trick so  what we do is this so  we take these clause so  we take c 1 through c n now  somehow i will tell you how  to construct a graph g and some k feed this to feed into  so it is called this algorithm clique into clique and if this says yes  we will say that the formula is satisfiable if this says no  we will say not satisfiable so  let us look at this we take these formulas ? s c 1 through c m we construct some graph g and some k and we feed this into sub routine feed now  this will say yes or no  if it says yes means we will we want to say that  the formula is satisfiable it says no then we will say no now  how do we do this transformation from these clauses we have to construct a graph and from a clique  what we get as output is graph has a clique of size k somehow clique of size k should indicate that the formula is satisfiable so  let us see how this is done so  you take the formula c 1 through c m and here is how you construct the graph  refer slide time 16  32  so  here are the clauses c 1  c 2 so on up to c m now  for each clause i will have a number of vertices in the graph so  let us say  c 1 x 5 i will tell you what  this x 5 are x 7 bar  c 1  x t bar if the clause c 1 were x 5  x 7 bar or  or  or up to or x t bar then i have a vertex for which each of them in other words  i have a vertex for each literal clause literal pair so  a literal appears in a clause i have a vertex for that similarly  i have listed them for c 2 so on so  c m will have it is own set of vertices so  for each clause literal pair i have a vertex i have divided the vertex set into these end parts one for clause now  what do we want is that  from a clique of size k we still have not specified what k is from a clique of size k i should be able to figure out that there is a satisfying assignment so  what does the satisfying assignment look like  the satisfying assignment essentially says  for each clause which of them i can set to true  which literal i could set to true may be i can set x 7 bar to true  which means x 7 to false so  may be here i have c 2 and x 30 may be x 30 i can set to true somewhere  down the line let us say  c 45 i have x 30 bar i could have x 30 here and x 30 bar elsewhere and so on now  somehow each of these clauses i want to set one of them to be true i want to make that one of them is true and this i should be able to pick out with a clique so  it stands to reason that i would have a clique of size m and this clique i should pick by picking one vertex from each of these sets i should be able to pick one vertex from each of these sets  if i am picking only one vertex from each of these sets then each of these sets may be an independent set i do not want edges between any of these all the edges in the graph   refer time  19  21   may only go between vertices   refer time  19  23    now  let us look at these two c 2 30 and c 45 x 30 bar now  clearly i can not if i say this is true  i can not set this to true  this will immediately become false  which means  i should not be able to pick these two in my clique remember  the way we are going to get from a clique to a satisfying assignment is  when i pick the vertices in the clique they will decide  which literal in a clause is satisfied this is roughly the intuition that i want so  from each vertex the clique i will pick one vertex from a clause and you can tell me which i can  which is the literal which is satisfied so  if i pick x 30 in clique 2  i can not pick x 30 bar in clique 30 or 37 so  these two i should not be able to pick  which means there should not be any edge between them otherwise  i can add all the vertices so  this roughly actually is the concept so  let me formally tell you what the construction is construction of the graph is  refer slide time  20  38  so  the vertex set so  vertex set v is consists of c and l  where literal let me  make this c i and l  literal l belongs to clause c i so  there is one for each clause literal pair  if the literal belongs to that clause so  what are the edges  this is the vertices edge set is c i  x and c j y is an edge if and only if first of all i is not equal to j  because edges flow only between vertices corresponding to different clauses and x is not y bar because  if x is equal to y bar then i do not i can not pick both of them in my clique so  i can pick both of them in the clique  if they have different variables all together one is x 15 another is x 30  it does not matter what value i set to it but  i can not pick x and x bar together so  this is the graph and k is m so  given my clauses  set of clauses i construct this instance of clique i feed it into the sub routine for clique if it says yes  it has a clique of size m when i say yes  the formula is satisfiable  if say no i say the formula is not satisfiable so  let us actually  formally prove this so  if we actually  informally argued this  but let us quickly sort of do a recap so  supposing the formula is satisfiable  refer slide time  23  00  so  if formula is satisfiable  what does this mean in each clause so  look at any satisfying assignments if there is a satisfying assignment look at any satisfying assignment  in each clause some literal will be true so  pick all this  so pick these vertices in the clique so  we have picked m vertices so  we have picked one vertex per clause though we have m vertices and clearly  there will be a edge between any two of them because  firstly they are in different clauses and if they are the same sort of they used the same variable then they can not be negations of each other so  these in fact  form a clique in the graph and in fact  the other sort of other direction is also similar so  take a clique of size m  which means there exists one vertex per clause so  i am going to set these literals to true so  these literals which corresponds to these vertices set them to true and you can check that this can be a satisfying assignment  in this clique with a variable x appears x bar will not appear so  this will not be a satisfying assignment   refer time  25  04   we will just prove that clique is n p hard we saw last time that clique is empty so  put these two things together  we have just shown that clique is n p complete so  the thing to notice is that once we have cook ? s theorem in hand it is much easier to prove some other problem is n p complete all you need to do is prove that  if there is a polynomial time algorithm for this new problem there is one for any of the old problem  which you have already proved n p complete for instance sat but  now you can use clique so  our next problem is called independent set so  let us define what a independent set is  refer slide time 25  51  so  given a graph g  this is the vertex set this is the edge set a subset u of v is called an independent set if there are no edges between any pair of vertices in u if all u 1  u 2 belonging to u u 1  u 2 is not an edge there are several subsets  so that if i take any two of them  there should not be an edge between them this i mean for clique he wanted an edge to be in every pair here  you do not want a edge between any pair so  our next objective is to prove the version of the independent set decision version of independent set problem to be n p complete  but the search was an independent set essentially states that given a graph find an independent set of maximum size the decision so we convert this to a decision version in a fashion  which is very similar to clique so  it looks like this so  independent set problem independent set has input graph g  the positive integer k the question you ask is does g  have an independent set of size k this is the question we ask so  the first thing to see is this problem in n p the well i hope most of you can now show that this problem is in n p anyway  let us do it so  if the answer to the question is yes  we are looking at only yes inputs  inputs for which the answer is yes and for these inputs  the prover should be able to convince the verifier that in fact  it is a yes input so  we need to figure out  what is the hint or proof or advice that the prover gives the verifier in this case  again it is just the independent set then he just picks out k vertices from the graph and tells the verifier here  these vertices form independent set the verifier looks at these verifiers he checks that there is no edge between any of them so  that is what he verifies and that is very easy to verify and we convince that  in fact  it is a yes input the advice that the prover gives the verifier in all most all cases is supposing the question is yes  why is this question yes you just answer the question so  here for instance if it is yes if g has an independent set of size k so  focus on the independent set of size if this is given to the verifier and the verifier can check that it is in fact  good so  this problem is in n p  our job is my job for the time being is to prove that it is n p complete so  let us do it so  what we do is this  refer slide time 30  01  so  we will the strategies is we want to show  where independent set is n p hard how do we do this we will this so  assume there is a polynomial time algorithm for independent set so  assuming this will construct a polynomial time algorithm or clique what does this give us ? we know that clique is n p complete  which means if there is a polynomial time algorithm for clique there is one for everything in use good so  that is what we will use and you can see that we have done  if we can do this step assuming that  there is a polynomial time algorithm for independent set we construct polynomial time algorithm for clique and because  there is one for clique  there is one for everything in n p so  to prove essentially that  if there is a polynomial time algorithm for dependent set  there is one for everything this is not as difficult as the previous one and independent set and clique they are both sort of problems in graphs and we also look feel similar in one case we have an edge  in the other you have subset where there are no edges and in fact  this is exactly what we do so  supposing i have a polynomial time algorithm for independent set now  how do i find for a graph as a clique of size k so  i take this graph  where ever there is an edge  if there is an edge between two vertices i remove it if there is no edge between two vertices i add an edge so  i sort of complement the edge set and in this new graph i feed into the independent sub routine if it says there is a independent set of size k clearly  these k vertices must form a clique in the original value because  i complemented the graph  well this is all there is so  let us just write this down  refer slide time 32  38  so  here is the transformation so  take g and k so  complement g to get g c so  what is the complement essentially  the edge set is the same i have complemented  the vertex set is the same the edge set i have complemented so  the edge set of g c is  so u  v such that  u v is not e of g if it is not there i put it in if it is there i do not put it in so  this is the complement so  essentially the complement of e g now  keep this into so feed this into the sub routine or the independent set and if  this says yes then you output yes if this says no then you output no so  we recall that what we want is to find or decide to decide if g has a clique of size k this is what we are doing  we are going to decide whether g has a clique of size k we complement the graph feed this into the sub routine for independent set and if  this independent set sub routine says the yes then we say yes  when we say no  the answer is no and you can check that this box i mean in the sense that  if g has a clique of size k g complement also will have a size of k if g does not have a clique of size k  i am sorry if g has a clique of size k  then g complement will have a independent set of size k if g does not have a clique of size k and g complement will not have a independent set of size k so  the answer in both case is correct our next problem is called vertex cover  refer slide time 35  20  vertex cover  so what is a vertex cover so  a vertex cover in a graph g is a set of vertices such that  every edge has at least one end point in the set is a set let us say  u of vertices  so that every edge has at least one end point in u now  let us take an example this graph let us see  suppose i pick this  then i take this this let us say that  now  i need to cover this edge so  hopefully i have covered all edges so  this edge  this end point is in  for this edge this end point is in  this edge this is in  this edge this is in for these both are in  but that is fine similarly  for this edge both are in and that is fine this is in the vertex   refer time  36  57    so  if i pick these vertices  which are tick marked  that forms a vertex cover in the graph it is a subset of the vertex set so  that every edge has at least one end point for instance  supposing you have a computer network you have a network and you want to monitor each link you want to monitor each link what do you want to do is pick a set of these nodes pick a set of these computers and to each computer you can assign a link to each computer you can assign a link so  that it monitors traffic on the links both ways from the computer and from going into the computer and you want a subset of the computers   refer time  37  45   every link is covered  which means for every link at least one of the end points must be chosen to monitor it so  this is the vertex cover ideally  we would like to pick very small number of computers to do this job and in fact  we will look for vertex covers of minimum size given a graph find the vertex cover of minimum size is the optimization or the search problem that one looks for and our objective is to show that this problem is hard  in the sense that you can solve this problem then you can solve every problem in n p in polynomial time so  this is among the hardest problems in n p if you want to pick up minimum number of computers to  so that traffic on every link can be monitored this problem is n p hard unlike that you will come up with a algorithm so  that is what we are going to do good so  i just mentioned the search version of vertex cover  what is the decision version remember  if you want to prove something is n p complete  we look at the decision version they are easier to handle so  here is how we do it the trip is the same we introduce this extra positive integer k and we ask if the graph has a vertex cover of size k so  if it has a vertex cover of smaller size  clearly it has one of size k so  here is the problem  refer slide time 39  34  so  vertex cover input is a graph g  the positive integer k  the question is does g have a vertex cover of size k so  i will leave it as an exercise for you to do the following as a exercise for you to do  which is supposing there is a algorithm for this problem and argue that there is an algorithm for the problem of finding minimum vertex cover it is very similar to the things that we have done in the past the other direction is simple if you can find vertex cover of small size i mean smaller size and clearly given this question you can determine whether  there is a vertex cover of size k if that size is smaller than or equal to k the vertex cover you found and there is a vertex cover of size k so  adding vertices to a vertex cover  it still remains a vertex cover so  you can form as large vertex cover that is not a problem you put all the vertices in the vertex cover if you take all the vertices in the graph it will give you a vertex cover so  here is the problem and we would like to prove that this is n p complete the first thing we need to do is to prove that it is in n p so  the first step is v c belongs to n p why is this true ? so  what we do is  so what is we go to our usual game so  here is the prover  here is the verifier and here is the graph g and k now  what does the prover give the verifier ? the verifier is looking at this graph g and k what does the prover give the verifier ? so  that the verifier can verify that there is a vertex cover of size k well  the prover just gives the subset subset of k vertices  so this is the prover gives the verifier now  the verifier looks at the subset and when he looks at every edge and checks that  this for every edge one of the end points is in the subset so  that is what the verifier checks he takes the subset of size k from the prover now  he goes through the edges one by one and for each edge checks that at least one of these edge these two end points is in the subset and the prover is correct so  this proves that vertex cover is in n p now we need to show that vertex cover is n p hard so  given we want to show that  if there is a polynomial time algorithm for vertex cover  there is one for everything in n p  refer slide time 43  04  so  we will show we will assume that  there is a polynomial time algorithm for vertex cover so  what we will do is we will construct a polynomial time algorithm for dependent set instead of independent set i could have chosen clique or i could have chosen satisfiability either of these one of these three problems i could have chosen in fact  as we go along  we could choose any of these already proved n p complete problems here but  trick is to choose the right one so that  we choose the right sort of problem  then this proving becomes easier so  we assume that there is a polynomial time algorithm for vertex cover we will construct the polynomial time algorithm for independent set now  how does one do this  what is the relationship between vertex cover and independent set so  here is my graph g and k i have a graph g and k and i want to find out if this graph g does g have an independent set of size k ? this is what we want to sort to prove that  we want to construct this is the algorithm we want to construct one for independent set so  supposing this is my graph g now  supposing there is a independent set of size k let us say  this is the independent set supposing this portion of the graph is independent set let us  look at what can we say  we know that there are no edges here there are no edges between any pair of vertices here  where are the edges in the graph well  the edge can be on this side or the edges can be of this form edges are either completely in this side or one end point is here and one end point is here  which means that this portion is a vertex cover if this portion is a independent set  this portion is a vertex cover and in fact  this goes this implication goes the other way if this portion is a vertex cover and the rest of the graph better be an independent set because  there can not be a edge here every edge must have one end point in the vertex cover so  if a subset of the vertex set is a independent set the complement of this set is subset will be a vertex cover and if it is a vertex cover the other will be a independent set so  we will use this to prove so  essentially if you want to find independent set of size k we will look for a vertex cover of size n minus k  where n is the number of vertices so  if there is a vertex cover of size n minus k  then clearly there is a independent set of size k and if there is no vertex cover of size n minus k  there is no independent set of size either so  that is all there is so  what we do is we just take the same graph  instead of k we take n minus k and then we call the algorithm for vertex cover so  given a algorithm for vertex cover  we are considering one for independent set  refer slide time 47  08  so  we take g and k so  we want to find well we want to determine  if g has an independent set of size k what we do is we take g prime and k  where the vertex set of g prime is the same as the vertex set of g the edge set of g prime is also the same as edge set of g this would be k prime  k prime is nothing but  n minus k  where n is the size of the vertex set so  this is the same graph as the original so  we just copy the graph down and k prime is nothing but  n minus k and now  you ask is this pair for this pair i feed this into sub routine for vertex cover so  you ask does this is spread into sub routine v c if it says yes  then you output yes if it says no  then you output no  that is it so  g and k are input so  this is an algorithm for independent set g and k are input  i take the same graph and i take n minus k as k prime g prime is the same as g and k prime is n minus k this i feed into sub routine for vertex cover  which we have assumed remember  we have assumed that there is a polynomial time algorithm for vertex cover and if this says yes  then our output is yes  if this says no our output is no and it is easy to see we have argued that  if g has a vertex cover of size n minus k it has a independent set of size k and if it does not have a vertex cover of n minus k  it does not have a independent set of size k so  we have just added new problem to our list of n p complete problems we have just proved that vertex cover is n p complete so  we started out cook ? s theorem gave us sat sat was n p complete and we then showed that clique is n p complete then we showed that independent set is n p complete and now you have showed that vertex cover is n p so  our next problem is very similar to vertex cover but  it is a restricted form of vertex cover  refer slide time 50  28  so  this is called less than equal to 3 v c so  the input is a graph g  such that the maximum vertex degree is at most 3 and the positive integer k the question is same does g have a vertex cover of size k without this  if i omit this just graph g and integer positive k  this is just vertex cover i add this additional constraint to the input the only inputs i can look for is graphs where the maximum vertex degree is 3 delta of g is the maximum vertex degree in g delta g is max degree of a vertex in g so  look at the degrees of vertices in the graph and delta is the maximum so  the maximum degree i want is 3 so  this problem is like vertex cover  the only thing is that we focus on graphs on only some graphs  not all graphs only graphs which have maximum vertex degree at most 3 now  is this as hard we know that vertex cover in general is the hard problem is this problem as hard  is it as hard is it much easier to find vertex covers in graphs  where the maximum degree is just 3 and not more than 3 the answer is no in fact  even in this case the problem turns out to be hard so  even for graphs where the degree of a vertex does not exceed 3 vertex cover problem vertex cover is hard we will show that this problem is n p hard or n p complete to prove that  this is in n p is very similar to the vertex cover thing and i will not do it so  just do it i hope most of you can do it all of you can do it good we will just prove that  less than equal to 3 v c is n p hard now  which of the problems  which have already been proved n p complete should be used for this  i guess vertex cover is a closest so  once you sort of once you go for vertex cover i mean it is correct so  vertex cover we will do so  what we will do is given an efficient algorithm polynomial time algorithm or less than equal to 3 v c we would like to construct efficient algorithm polynomial time algorithm for v c general v c so  let me write this statement now  refer slide time 53  59  so  given polynomial time algorithm for less than equal to 3 v c we want to prove that there is we want to construct a polynomial time algorithm for v c this is what we want to do and once we do this  we know that from the previous construction that there is one for everything in n p so  let me just sort of say what this means  see going the other direction is easy supposing  you have polynomial time algorithm for v c clearly  if you restrict the input in any way  it does not sort of i mean you can still feed the same input to the algorithm but  here you can not just do that your input is a graph and k now  this graph could have vertex degrees  which are much higher than 3 what do we do with these ? that is the   refer time  55  05    if all vertex degrees were 3 then there is no problem  i just feed this into algorithm for less than equal to 3 v c   refer time  55  13    so  what happens when that of vertices with degree greater than 3  4  5  6 whatever up to n minus k largest n minus 1  when the trick is to somehow decrease that decreasing the vertex degrees by creating new so  given this original graph i construct u graph  where the vertex degrees are smaller and somehow  you know you must have some control over the vertex cover the vertex cover in the new graph must be related very closely related to the vertex cover in the older so  here is the trick so  i will tell you what the trick is to reduce the degree of the vertex so  the trick  main trick is degree reduction while keeping the size of the v c  while controlling the size of v c if there is some controlled over how it changes  when we are here so  let us take  refer slide time  56  46  so  here is the vertex of degree 5 somehow i need to that is here is rest of the graph so  what do we do is this  if you change the degree i break it up into three parts  i break this vertex into three parts so  here is the rest of the graph   refer time  57  08   three of them here so  let us see  what i have done see there are 5 of them i have just broken it up into two parts let us say  1  2  3 this is a  b  c  d and e a b c so  this one vertex i have broken up into three vertices i have added two new vertices and you can see that  vertex degrees are decreasing initially i had one of degree 5 and now  if i look at these vertices  there is one of degree 2  one of 3 and one of degree 4 so  these vertex degrees i have decreased  if this were 4 i would not have one of these edges let us say  that a was not there then i am done so  i have three vertices one of degree 2 and two of degree 3 i am done i have decreased vertex degree of degree 4 vertex if it is 5 i will have one with 4 if i have larger again i just split it into two parts of course  i have to do this repeatedly i take this graph i split this vertex into two so  i decrease the degree and i keep doing this  till every vertex has degree 3 or less that is the rough idea let us see  what happens to the vertex cover  when i do this split so  just to recap i look at this graph look at if there is a vertex of high degree i split it into i split this degree up i split this degree into two parts and add a new add another vertex so  instead of vertex and i have a three vertices and the degree has been reduced initially  they were 4   refer time  59  03    they have two of them are degree 3 and 1 is degree 2 and so on so  i keep doing this now  i have to somehow say  that the vertex cover does not change much even if it changes there must be i must say that it changes clearly by this much only  then can i make the connection between the vertex cover in the final graph and the vertex cover in the original graph final graph has degree of vertex at most three  that is fine i should also be able to say that  if there is a vertex cover of this size in the final graph if and only if there is a vertex cover of size k the original graph  and that we will see right now so  supposing the claim is this so  here is the graph g and here is g prime  which i get this way now  the claim is that g has vertex cover of size k  if and only if g prime has a vertex cover of size k plus 1  which means the size of the minimum vertex cover goes up by 1 so  this is my each time i use this split the vertex cover size goes up by 1   refer time  1  00  28    so  why is this true ? so  let us prove both ways of this inequality so  let us prove this supposing g has a v c of size k why should g prime should have a vertex v c of k plus 1 so  take the v c  which g i there are two cases so  case 1 is let us call this vertex u and here let me call these u 1  u 2  u 3 case 1 is u belongs to the v c now  u is in the v c then put u 1 and u 2 the rest of the vertices are in the vertex cover remain the same there are some these vertices in the vertex cover in this portion of the graph and i have used i put u 1  u 2 and the same set of vertices in the vertex cover and now  let us observe that this is also a vertex cover so  the vertex cover size has gone up by 1 if there were l plus 1  l vertices here and 1 vertex here  then i have l vertices here and 2 vertices so  it is l plus 1 why is this the vertex cover now  if the edges completely in this portion it is covered now  since both u 1 and u 2 are in these edges are covered and these two edges are also covered so  every edge is covered so  this is a vertex cover now  case 2  u is not in v c then put u 3 in the v c now again let us  check this is the vertex cover now  all of these vertices all of this edges a u  b u and so on have to be covered  which means a b c d e must be in the vertex cover  u is not in the vertex cover so  all these edges are covered by the vertex cover in this portion since  a b c d e is in the vertex cover so  the only edges we need to worry about is u 1  u 3 and u 2  u 3 and because we put u 3 in the vertex cover these four also covers so  this is also   refer time  1  02  54    just to recap  if you is in the vertex cover remove u from the vertex cover and put u 1 and u 2 to get the new vertex cover in g prime this is the vertex cover if u is not in the vertex cover  take the old vertex cover and put u 3 additionally in this vertex cover and this will be a vertex cover in the graph this proves one direction so  we need to prove  the other direction in which that is also very  very simple now  just to observe let us look at u 1  u 2 and u 3 now  there are two cases again so  only look at the vertex cover here i will show that the vertex cover of 1 size 1 less on this side whatever  vertex cover is there on this side i will show there is 1 of size 1 less so  the case 1 is u 3 belongs to the vertex cover we remember this vertex cover is in g prime then i just remove u 3 and you can check the rest of the vertices form a vertex cover for this input i must say that  only u 3 is in v c  but not u 1 and u 2 so  only u 3 among u 1  u 2  u 3 ; so that is case 1 u 3 is in the vertex cover  but u 1 and u 2 are not in the vertex cover now ; that means  that a b c d e all of these edges are present in the vertex cover so  these edges are taken care of and clearly if there is a edge only in this portion it is taken care of here also because  this portion of the vertex cover remains the same i have just removed them so  if u 3 is in the vertex cover  u 1 and u 2 are not in the vertex cover  then you are done you just remove u 3  refer slide time  1  05  27  so  the next case case 2  if it is not this case so let us  look at u 3  u 1  u 2 then i claim that at least two of u 1  u 2  u 3 must be ? two of these  why 2 of these well  i can not have only u 1 in the vertex cover i can not have only u 1 in the vertex cover  because if both u 2 and u 3 are not there  then this edge is not covered the case only u 3 is taken care of so  we must have at least two of these in this case  i remove this and put remove and put u in the v c instead then  i get vertex cover of 1 less then  well what if all three are present well this can not be a minimum vertex if there is a vertex cover of size there is one of smaller size on the right hand side so  that does not impede our progress so  when i do this split  the vertex cover may rise by at most one the vertex cover will rise by at most one by so then the procedure is clear i start with g  then i do this sequence of these transformations g 1  g 2 each time i split a vertex into 3 so  i keep doing this as long as there is a vertex of degree 4 or greater so  finally  i end up with g l i have done this l times and i get a graph so  delta of g l is less than or equal to 3 so  i want to find out  if g has a vertex cover of size k what i do is i ask if this has a vertex cover of size k plus l if it says yes  i say yes g has a vertex cover of size k  which says no i say g does not have a vertex cover of size k and the proof that it works essentially is we have actually done it reason is each time we do a split vertex cover rises by 1 so  this actually  proves that less than equal to 3 v c is n p complete we just one small thing we need to worry about well  you start with a initial graph start splitting  what if there are too many splits we have got the polynomial time algorithm for this we started out with the graph and supposing you construct a new graph you take lot of time then this will not work we will show next time that this is not true and l is bounded so  l will not be too large if you show that l is not too large  then the final graph is not so big and so our entire procedure will run in polynomial time design and analysis of algorithms prof sunder vishwanathan computer science engineering department indian institute of technology  bombay lecture  30 np-completeness  5  refer slide time  01  00  we were looking at the problem less than equal to 3 v c so  this problem the input is a graph g such that delta g  which has the maximum vertex degree is at most 3 and the positive integer k the question we ask is does g have a vertex cover of size k ? our objective was to prove that this is n p complete ; it is easy to see that this problem is an n c ; the proof is very similar as for the vertex cover the fact that delta g is less than or equal to 3 does not change anything so  now we need to prove that if there is a polynomial time algorithm for less than or equal to 3 v c there is one for everything in it in n p ; we have seen that it suffices to prove that if there is a polynomial time algorithm for less than equal to 3 v c there is one for b c that is what we were doing the trick was this  so we are given a algorithm for 3 v c so  we are given an algorithm for less than equal to 3 v c so we want to construct one for v c  so we take the input graph let us say g now  this could have vertices of varying degrees especially vertices with degree greater than 3 if the vertex degrees are less than equal to 3 then we have no problem now  what do we do for large vertices with large degree  we use the split operation so  supposing i have a vertex with large degree  what you do is you sort of split this vertex into 3 parts now  some of the neighbors are attached here and some of the neighbors are attached there essentially this is broken up into 2 parts some of them are attached here some of them are attached here now  we saw that if this has a vertex cover of size let us say l this has a vertex cover of size less than equal to l plus 1 and if this has a vertex cover of size p  this has a vertex cover of size less than or equal to sorry p minus 1 so  this is the effect of splitting a vertex into these 2 parts so  the vertex cover goes up by only 1 while we have reduced the degree of this vertex  refer slide time  04  01  so  the algorithm is as follows  so you take g and then construct keep splitting vertices as long as there are vertices with you know degree at least 4 g 1 g 2 so on so  maybe i will do this l times so this is done l times so  and in g l i know that there is no vertex with degree greater than 3 so  g l is applied on this algorithm for 3 v c so  this is now  input to an algorithm for 3 v c and the k  so initially k had a k here so  now i feed in k plus l and if it says yes then i output yes  if it says no then i output no  so this is the entire algorithm so  i take g i want to find out whether it has a vertex cover of size k i do this sort of transformation i get final graph g l i feed this into 3 v c with my new k as k plus l if it says yes then i say yes if it says no i say no now  if this runs in polynomial time i want to show that this entire thing runs in polynomial time the first thing is that there should not be too many of these steps right if there are large number of these steps the overall algorithm may not be polynomial may not run in polynomial time so  let us first bound l  so how many times can a vertex get split into 2 parts so  supposing i have a vertex with let us see this  refer slide time  05  55  so  how many times can a vertex be split into this is the question we want to answer so  supposing i have a vertex with degree d so  i could split this once twice  i mean how many times we do this well  note that each time the maximum degree will keep decreasing by at least in fact  by 2 so  the maximum number of times i am going to do this is d by 2 you can do it in fact  better by splitting it in half etcetera so  the maximum times you will do this is actually d by 2 i hope this argument is clear let me repeat this so  each time i split right the maximum  so if i just look at these graphs the degree of this vertex now  is at least at most d minus 2 rights and the next time i split one of those vertices again i will decrease by 2 and so the maximum number of times i do this is d by 2 the other way to see it is this so  let us say i have 1 2 up to d first time i split it like this i have 1 2 and the rest 3 on up to d and now  for this vertex i again split it into 2 parts so  this portion remains as it is so  this portion now i split as let us say this is 3 4 and then and so on and you can see that this is done at most d by 2 times so  the number of times the vertex of degrees d is split is d by 2 so  the total number of times  so the total number of times a split occurs is summation over all vertices d v by 2 and that we know that is nothing but number of edges in the graph right so  this half comes out and sum of the degrees of the vertices is twice the number of edges right so  the total number of times the split occurs is at most if the vertex has degree 3  of course it does not split  refer slide time  09  07  so  here let us go back to this  i know that l is at most m and each time you split you add 2 new vertices to the graph so  the size of g l v of g l is less than equal to size of v of g and each time you split you add 2 new vertices plus twice l so  it is at most n plus twice m  the degree of each vertex in g l is 3 so  the number of edges is also some constant times n plus 2 m so  the size of g l  so the number of edges in g l is polynomial in the size of g it is not too big you do not do this too many times and final graph that results size of that graph is not too big so  any running time which is polynomial in that size is also running time is polynomial in the input size so  this thing runs in time  so we look at 3 v c this runs in time  which is polynomial in this input size of g l  but that is also polynomial in size of g so  the whole thing runs in time  which is polynomial in size of g and now so we need to know show that this works so  i hope that you are all convinced that this whole thing runs on polynomial time if 3 v c runs on polynomial time now  we need to convince ourselves that if this says yes then answer there is yes it says no then the answer is no now supposing g has a vertex cover of size k  so let us prove both ways supposing g has a vertex cover of size k if g has a v c of size k  so this implies that g l has a v c of size less than equal to k plus l right so  this has a vertex cover of size at most k plus l so  it will say yes  so this answer will be yes  so this is fine what we need to do is also the other way round that it does not have a vertex of size k it will answer no  but it is better to do the other way round so  if this says yes  if 3 v c says yes  this means that g l  refer slide time  12  26  so  3 v c says yes  which means g 2 has vertex cover of size k plus l and we know that this implies that g has a vertex cover of size at most k right so  g has a size of at most k  so this is also same which means g has a vertex cover of size k so  both ways we have proved that the algorithm worked out if it has a vertex cover of size k well  here its k plus l g has a vertex cover of size k this implies that g l has a vertex cover of size k plus l we have done  which means g has a vertex cover of size k then it answers yes and if it answers yes then g must have a vertex cover of size k so  this proves that 3 v c is  refer slide time  13  55  so  our next problem is exact cover so  called exact cover xc exact cover here  the input is set s and the collection of subsets s a 1  a 2 up to a m there are m subsets of s the question we ask is the following is there a sub collection of these sets so  that the union is s and the sets in the sub collection are disjoined is there a sub collection so  let us say a i a i 1 a i 2 up to a i p is that 2 things the union must be yes the union for all j a i j is yes second thing is that a i j intersect a i x i s null set so  you need to pick some sub sets of some of these sub sets in these collections so  that each element is present in one of them and exactly one of them so  each element is covered and covered exactly once so  that is where you get the name exact cover is there a sub collection  which exactly covers the set s  which covers set s in the sense that union is s union of i j is s and it is covered exactly in the sense that each element is covered exactly once so  these must be disjoint so  is this problem n p complete the answer is yes as you must have guessed  because those are the problems we have discussing so  let us prove that this problem exact cover is indeed n p complete so  the first thing to note first thing is to prove that exact cover is in n p to prove that this is in n p so  what is a s input a s input is just collection of i need a collection so  that there is some sub collection so  that a union is s and the sets in the sub collection are disjoined so  i guess it must be clear what prove the provers must supply the verifier so  that the verifier can verify that there exists such a sub collection in fact  the prover just gives such a sub collection the prover tells the verifier what are the sets in the sub collection the verifier takes these sets he verifies that they are disjoint and he also verifies that the union is s  which means every element of s does appear in one of the sets in the sub collection so  this proves that this problem is in n p the blame will prove it is n p hard is to show that if there is a polynomial time algorithm for exact cover there should be a polynomial time algorithm for less than equal to 3 v c  refer slide time  18  26  so  let us do this so  given a polynomial time for x c i want to construct one for less than equal to 3 v c this is our goal  so how do we do this so  we want to construct for one less than equal to 3 v c so  i guess the algorithm will take a graph as a input  there is also this k  we know that delta g is at most 3 and we want to know we want to somehow figure out whether this graph has a vertex cover of size k or not that is the problem we really want to solve and what we can use is the sub routine is this problem so  somehow we need to use sub routine to x c to solve this problem on graphs so  let us write down what a solution to x c and solution to vertex cover look like the input to x c is set s and a 1 through a m this is the input this is input to v c now ; here what i want to know is vertices to cover all edges here i want sets to cover all elements by sets i mean from this collection a 1 to a m right so  somehow vertices there must be some relationship between the vertices and the sub sets  so let me say sub sets there must be some correspondence here and the edges must correspond to elements of s if there is such a correspondence then it looks like  you know that there is some similarity between these 2 problems this is what we would like to we would like to exploit  so let us take to attempt at this problem  refer slide time  21  43  so  we know that the elements of s are edges and blind thing and then we take g is the input so  now  i look at g and i need to convert this input into a input for exact cover so  i say the set s is nothing but the edge set of g and now  the subset must correspond to the vertices each subset here must correspond to a vertex in g and what is the natural sort of correspondence for vertex i  i add a set a i for vertex i this is nothing but the edges incident on vertex i  if i take a i to be the edges incident on vertex i now  i have this correspondence right so  i have the graph g here and i have s a 1 through a m picking vertices to cover edges corresponds to picking subsets  which cover elements of s elements of edges are covering edges and each subset is like picking a vertex in g picking a sub set here is like picking a vertex in g now  there is a small problem first problem is i need to pick  you know there is this k floating here so  i should somehow make sure that make sure that only k vertices are picked we need this if i just do this there is no sort you know i could pick as many of them i want right so  a vertex could be i could pick as many of these subsets so  this fact that in the original problem  i should only be allowed to pick k vertices is the does not reflect in this transformation so  how do we fix this problem ? this problem is actually fixed easily so  what we do is this  so we take k extra elements  refer slide time  24  26  so  take k extra elements we need to what we are shooting for is pick only k subsets this is what we want to do ? so  take s now apart from g there will be some k extra elements x 1 x 2 so on till x k and supposing the sets k 1 through what i do is i take the  i change the collection of sets i take a 1 union x 1 a 1 union x 2 so on a 1 union x k similarly a 2 union x 1 so on up to a 2 union x k and finally  a n union x 1 up to a n union x k well  n is the number of vertices in the graph that is why it is n here and not m  m is the number of vertices well  the number of collections has now  grown to a factor of k initially  we had n sets and now  we have k times n but now  let us see supposing i pick up an exact cover here supposing i pick up an exact cover from this collection from these i can i can pick any one of them right but if i look across row  i can pick only one of them from this i can pick only one from this  i can pick only one and so on and from this i can pick only one  so totally i can pick only k sets so  let me say this argument again now  when i look at this collection then in each set one of x 1 x 2 x 3 x k occurs each set one of these k elements must occur and if x 1 occurs in many states  i can only pick one of them  because x 1 has to be covered exactly once right in fact  exactly one of these sets  so i have to pick a set  which contains s 1 as to pick a set  which contains s 2 i have to pick a set  which contains x 3 and so on up to x k these are k sets and i can not pick anymore  because any other set has to contain one of these elements x 1 x 2 up to x n so  this trick force  you to pick when you pick an exact cover this forces  you to pick exactly k of these sets so  that is taken care  so is there any other problem well unfortunately there is the problem is that in vertex cover  refer slide time  27  59  so  let me sort of tell you what the problem is  so your problem is this  when you look at a graph i look at vertex cover you know it may  so happen that there may be a edge between 2 vertices in the vertex cover so  this is the vertex cover  it may be  it may happen that there is an edge between 2 things in a vertex cover so  when i pick the corresponding sets a i 1 and so on this is a i k  when i pick the corresponding sets  then you know this edge gets covered twice it gets covered when i pick this vertex at this set ; it also gets picked when i pick this set so  this is the problem so  what we would like to do is when you get to this vertex not pick this edge  but may be pick the other edges  which are not yet been covered so  i would like to pick these sets 1 by 1 when i get to a vertex i would like to pick those sets those edges  which are not covered and the solution is actually simple so  supposing i had recalled that the degree of each vertex is 3 supposing i had a degree of vertex 3 right so  there are 3 edges  which are adjacent let us say e f and g now  it could  so happen by the time i get to this vertex i would like to pick this vertex  these edges sum of these edges are already been covered by other vertices then i would like to leap at g let us say if e and f are already covered i would like to leap at g for this vertex if only e is covered i would like to pick only f and g and similarly if none of them are covered then i would like to you know have a set  which contains all of them so  the trick is to just take all possible subsets so  take all possible subsets so  initially i had with the vertex  i had the set e f g now  i will have many of them instead of this  i will have now e f g and then all pairs e f so on and finally  e f g so  there are 7 of these with each vertex well actually this would have x 1 x 2  so on up to x k also so  that also has to be put in so  this set would have e f g in one of the exercise remember our previous construction that x i would also go into each of them so  it us not 7 subsets  it is 7 k times it is 7 k with each vertex i had k subsets 1 for x 1 1 for x 2 and so on now  i have 7 k subsets per vertex i have 7 k subsets per vertex  if the vertex has degree 2 then it would be less then it would be 3 k  because there are if i take a size 2 the number of distinct subsets  which are not empty is 3 if the vertex has degree one then i would just have one subset  which would translate k other subsets so  when the vertex has degree k degree 3  i have 7 k subsets per vertex  refer slide time  32  23  so  this is translation i take this graph g and for every vertex for vertex u i must say that the set is nothing but e of g x 1 through x k for a vertex u i would have either 7 k or 3 k or k say depending upon the degree of the vertex so  what is the typical  what does the typical set look like ? so  it is a subset of edges adjacent incident on u and one of the exercises so  i take a subset and x 1 subset x 2 subset and x 3 and so on up to x k so  this is how each element corresponding to a vertex look like there could be either 7 k or 3 k depending on the degree of the vertex and this is now  i take this instance of s and this collection and i feed this into sub routine for x c if this says yes then i output yes if this says no then i output no so  this is this is my transformation now  to see that this works clearly  if this has a vertex cover of size k i have a exact cover of i have exact cover of this  why is that true ?  refer slide time  34  35  so  let the vertex cover consist of vertices v 1 v 2 so on up to v k now  for v 1 i put the set x 1 comma all edges incident on v 1 i pick the set with v i i pick the set x i comma all edges incident on v i  but not on v i so on up to v i minus 1 this is my set which i picked and you can check that this is this is an exact cover x 1 is covered by this set this set corresponding to v 1 x i is covered by v i and so on x k x 1 through x k is covered and every edge is also covered since this is a vertex cover if i take any edge  it must occur between there must be an end point in this there must be at least one end point choose the smaller end point supposing it is v i then this edge must have the other end point is either one of v i plus 1 through v k or something else completely different in either case that edge will be part of the set so  every edge is covered by by this collection edge is covered here so  every element of s is covered by this collection similarly  if i have a collection which covers let us say b 1 through b k is the sub collection  which covers every element of s let us say that b i contains x i x 1 through x k must be covered so  and they are covered exactly once  so they occur there must be k sets and each of them must contain one of x 1 through x k let us say b 1 contains x 1 and so on and b i contains x i b k contains x k now  every edge must also be covered and must occur in one of these sets  which means if it occurs in set b i  it must be adjacent to the vertex i right so  what i do is the vertex cover just consists of the vertices  which corresponds to each of this each of these sets  which corresponds to b 1 the vertex  which corresponds to b i and so on  so those vertices will form a vertex cover  so this shoes that exact cover is n p complete  why did we pick 3 v c  why not i mean less than equal to 3 v c  why not vertex cover ? we have done this with vertex cover well the answer is no at least this reduction does not work  refer slide time  38  06  the reason is that supposing i have a vertex of large degree some degree d the number of subsets  i have corresponding to this is 2 to the d minus 1 times k this is the number of subsets  remember how we took ? how we got the subsets for the vertex ? we took all possible subsets here non empty subsets and we added for each subset i added x 1 x 2 x 3 so on up to x k so  these many non empty subsets times k is the total number of subsets we get and this can be just very large right if i have a graph with degree for instance the complete graph as degree n minus 1 this size can be 2 to the n minus 1  which is just too much  which is too much and by the time you write this down  it is much more than polynomial type so  that will it will not work in polynomial time if you take just about extra cover so  we want this degree to be bounded by a constant  if we use 3 v c and then this is 7 times k which is not 2 bit so  the total number of subsets we have is actually 7 k times the number of vertices  this is the total number of subsets is at most this much if its instance if the degree is bounded by 3 that is the reason we needed less than equal to 3 v c we have shown that exact cover is n p complete the main trick was to first show that 3 v c is n p complete less than equal to 3 v c is n p complete  which is a restriction of vertex cover then show that exact cover is n p complete one can actually do it in other ways without going through 3 v c and i will let you try this on your own the reductions become slightly more involved  but you can still do them after this we will look at problems where sizes come into play so  that is the next installment design & analysis of algorithms prof sunder vishwanathan department of computer science engineering indian institute of technology  bombay lecture  31 np-completeness ? vi  refer slide time  01  07  the next problem we consider is called subset sum in this problem the input is a set s with m elements each element has a size so  you have sizes s 1 s 2 and so on to s m each element has a size also you are given a positive integer b we will assume that these sizes are also positive integer you have a set s with m elements each element has the size m and also part of the input is a positive integer b the question that we want to answer is this is there a subset subset t of s whose size is b so  this is a question that we want to answer now  what is the size of a subset ? the size of a subset is just the sum of the sizes of the elements in the subset let me define this  the size of t is nothing but the sum of the sizes of the elements e measure in t so  take all the elements in in the subset t and sum of their sizes this will give you the size of a subset now  if you look at size if you look at subsets in s each of them will have some size right now  we would like to pick up the subset of s whose size is exactly b this is the problem this is the algorithmic problem that you want to solve and this is the next problem we show is this np complete as before let us first prove that this problem is in np this should not take as too much time what put the prover give that verifier so  that the verifier can look at an input to to the subset sum and conclude and convince himself that in fact  it is a s input now  for a s input there there must exist a subset whose set whose size is exactly b right so  the prover just gives this subset to the verifier the verifier takes this subset sums up the sizes verifies that this in fact  is b and this verification happens very fast he just has to sum up the sizes of the subset so  subsets are measured np our next job is to show that subset sum is np hard and to do this we will use the exact cover  what does it mean that we going to use exact cover what we are going to do is this so  we will assume that there is a polynomial time algorithm for subset sum if there is 1 such we will construct polynomial time algorithm for exact cover so  this polynomial time algorithm for exact cover will take input for exact cover then if we convert it your input for subset sum somehow feed it into the subroutine for subset sum get an output back and then you know given answer for exact cover  refer slide time  04  55  so  let us do this so  we are given a polynomial time algorithm for subset sum what you want to do is construct a polynomial time algorithm construct 1 for exact cover frame model construct a polynomial time algorithm for exact cover so  since we are constructing 1 for exact cover somehow we should convert input for exact cover into input for subset sum so  what are the inputs for exact cover ? so  let us see we have exact cover on this side the input is the set t and subsets a 1 a 2 so on up to am what a subset sum ? subset sum has a set s that is this positive integer b and then we have sizes s 1  s 2 up to sm that these 2 m ? s are the same is not a coincidence mean i have chosen this anyway let us just proceed now  what do we want to do here in xc ? we want to pick subsets to exactly cover elements of the set t you want to pick some of these so  that this set p is covered exactly what do we want to do here ? we want to pick elements of s of size of total size equal to b now  here we have to pick subsets it says picks up sets here it says pick elements so  there must be a correspondence there must be a correspondence between subsets in the exact cover input and elements in the subset sum input so  somehow subsets on 1 side and elements on the other side must correspond so  that is the reason why m is i have chosen m to be the same because they will be the same so  for each subset in the exact cover input there will be an element in the subsets sum input right so  you want to pick subsets here and you want to pick elements here so  subsets will correspond to elements so  let us write these subsets  it correspond to elements what is the other thing ? the other thing is here i want to exactly cover elements of t here i want to pick 1 of size b somehow exact cover on this side must translate to the subset of size b this side i want to pick exact cover this side this side i want to pick a subset of size b the crucial thing of course  is what is b  what is b and what are the sizes s 1 s 2 s 3 up to sm ? i know that with each element of s what it is a subset of exact cover that corresponds ? how do i pick these sizes to make sure that an exact cover on 1 side corresponds to a subset of size b ? so  this is what this is a problem that we would like to solve now so  to solve this let us do a small switch so  we will solve a different we look at a problem which is very related and then we show at a solution to this problem can be transformed i mean is exactly what we are looking for a problem is this  refer slide time  09  36  so  you have given a multi set m what is a multi set multi set is a is like set except that elements can be repeated you have multi set m the size k distinct elements let us say x 1 and so on up to xk each element occurs at most l times each element each of this can have at most l other copies of itself and we are given a positive integer l now  what do we do with this ? what we want to do is assign so  the problem is this so  assign sizes to each distinct elements to x 1 up to xk such that the only way to pick a subset of size l from m is to pick 1 copy of each element let us see what this means so  i you have a multi set there are k elements and there are l copies there could be l copies or perhaps less there is at least 1 copy and at most l copies of each element and there is a positive integer l this positive integer not given to you so  let me clarify this is a positive integer you will have to fix also now  what you want to do is give sizes to these elements x 1 to xk i want to give some sizes i also want to fix this positive integer l so that when from this multi set if i pick any subset this subset can also be a multi set so  if i remove any subset so  that the size is exactly l then it must be the case that i have picked exactly 1 copy of each element 1 copy of x 1 1 copy of x 2 1 copy of x 3 and so on up to 1 copy of xk then we at least know that what l should be right l then should be size of x 1 plus size of x 2 dot dot dot up to size of xk this we know  because when i pick a subset of size l i must get 1 copy of each element nothing else now  these sizes are to be picked in such a way that any other subset of size l if i pick any other subset that must also contain 1 copy of each element if i put 2 copies of any element the size of such subset can not be l so  this is the goal so  how do we do this ?  refer slide time  13  34  so  let us see so  we have elements x 1 x 2 let us say x 3 and so on now let us just focus on the first 2 elements let us look at x 1 and x 2 so  without loss of general generality i can say that x 1 has size 1 so  size 1 we have at most l copies of each let us say this is the size 1 in fact  what i want is the sizes to let us say increasing x 1 is the smallest size x 2 next 1 x three so  i am going to assign them in increasing order x 1 is of size 1 what can be the size of x 2 so  let us look at x 2 so  size of x 1 is 1 what is size of x 2 can it be 1 really not because if its size is 1 i can always replace a copy of x 2 with the copy of x 1 can it be 2 well not really  because if i could replace x 2 with 2 copies of x 1 then i am again failing you know so  somehow the size should be such that i should not be able to replace x 2 with some copies of x 1 right so  i guess the smallest we can take is let us say l minus 1 or l if i take l the way to do it the way i could replace x 2 is with all of x 1 let just look at the 2 element case supposing i just had 2 elements so  i just had x 1 and x 2 supposing x 1 i said had size 1 x 2 has size l and my capital l has to be 1 plus l now  what can we do ? well  if i pick a subset of this size i can not do it only with copies of x 1 right there are l of them so  the total size is is l so  i can not pick something of this this size this is my set s using only x one so  i will have to pick at least 1 copy of x 2 so  if i r pick 1 copy of x 2 then this l goes out of the picture and now well  i have 1 and the only way to fill this is is to pick 1 copy of x 1 so  if my sizes are 1 and l and capital l is 1 plus small l then the only way to pick a subset from this set so  that the size is 1 plus l is to pick 1 copy of x 1 and 1 copy of x 2 there is no other way we can do this so  let us see if we can generalize this right so  supposing we have 3 elements x 1 x 2 and x 3 this well we will still retain it as l what do we think ? what do you think we should put here well  there are l elements here each of size l that gives l squared so  may be l squared is a good  good  good bound here of course  anything larger than l also will work here you can check that again anything larger than l will also work similarly something larger than l squared may also work so  let us check this so  l is nothing but 1 plus l plus l squared these are this is when the set has s 3 elements now  if i just took x x 1 and x 2 the total size i will get is l times l which is l squared plus 1 times l which is l right i can only get l plus l squared which means there must exist at least 1 copy of x 3 in in capital l so  there must be exit exist 1 copy of x 3 there can not exist 2 copies of x 3 that i can make sure by stating that 1 plus l plus l squared would be less less than 2 l squared and for reasonable l ? s this is this is satisfied so  i can just pick i i can do this otherwise you pick l prime to be maximum of this l and some other constants so  that this is satisfied we will just assume that this is satisfied so  i have to pick 1 copy of x 3 now we are down to 2 elements 1 of size 1 the other of size l and it is the same argument the only way i can i can do this is to pick 1 element of size l and the other element of size one so  to pick an to pick a subset of size capital l the only the only way to do it is to pick 1 element of size l squared 1 of size l and 1 of size 1 which is 1 copy of each element and in fact  a straightforward generalization of this actually works  refer slide time  19  36  so  i have i have k elements i pick the size of xi to be l to the i and capital l to be 1 plus l and so on up to l to the k minus 1 actually this is l to the i minus 1 x 1 was 1 x 2 was l and so on so  size of xi is l to the i minus 1 and capital l is this then the only way to pick something of this size is to pick 1 element of each kind either you can not do anyway so  what is this got to do ? what is this got to do with our set cover ? so  before that it is just easier to prove that exactly 1 copy of the last element has to be for that we will assume that l is less than twice l to the k minus 1 so  for large enough l this is satisfied so  this l we can take to be the maximum of the initial l that was given and an l which satisfies this so  what is this got to do with our with exact cover ? so  let us let us come back and do this so  exact cover the input is a set t and i have subsets a 1 a 2 so on up to an these are the subsets now  i will choose l to be the maximum number of times an element occurs in this collection so  is the maximum number of times that an element occurs in this collection the number of the number of ele the number of elements is nothing but size of size of t which is n which was k for us there were k distinct elements here there are n distinct elements right and with each collection there are a few elements and l is just the number of maximum number of times an element occurs in this collection good so  then how do we fix the sizes ? so  the question is how do we fix sizes of each of these these each subsets so  what we do is we fix the size of these elements first i mean fix the size of elements like this size of each element is l to the i minus 1 so  we think of each element of t and for each element of t for the ith element i pick the size as this what is the size of a subset this is what we really want which is going to be our input the size of the subset is just the sum of the sizes of these elements right so  subset sum so we want to transform this to subset sum so  we need a set s and an element ei or each subset ki for each of these subsets there is an element ki now  the size of ei is nothing but the size sum of the sizes so  we will think of first fixing sizes of elements of t that is fixed like this the size of the ith element is just l to the i minus 1 so  now  i can fix size of ei for each x in ai remember that x is an element of t i just add the size of x so  if it is the let us say jth element of t its size is l to the j minus 1 right if it is jth element in t in t it is nothing the size is l to the j minus 1 l then is the other thing or b is nothing but our l which is which is 1 plus l plus l squared and so on up to l to the n minus 1 now  if this set p has an exact cover supposing this set t has an exact cover then if i look at those subsets corresponding to those subsets i will have elements in the subset sum if i sum sum up these sizes it will be nothing but 1 plus l plus l squared plus so on up to l to the n minus 1 and for the reverse direction pick any subset of size 1 plus l plus and so on up to l to n minus 1 the only way to get something of this size is to pick 1 copy of each element this this ends a discussion of of sub set sum  refer slide time  26  24  so  let me just do 1 small calculation just in case there was a confusion so  what is 1 plus l and so on up to l to the k minus 1 this is nothing l to the k minus 1 up on l minus 1 so  instead of summing this up like this you can you can sort of you can calculate it fairly easily so  this proves that so  calculating the input is easy and so this proves that subset sum is np complete i think i mention that we need to choose l so  that so  that this is is less than twice l to the k minus 1 so  that can also be done in fact  you can check that for more most ones this will be true so  just check that this is true because we just multiply it out you get that l to the k minus 1 is less than twice i just multiply these 2 right so  twice l to the k minus twice l to the k minus 1 so i will need l thrice so  i have 3 sorry i have 2 l to the k minus 1 minus 1 should be less than l to the k so  you can check that for reasonable values of k this is true for l 3 or 4 it is true i can just take some l which is larger than that so  that was 1 thing which was which i am not completely cover it whose that subset sum is is np complete  refer slide time  28  55  and the last problem we have in this series is called partition it somewhat similar to to subset sum so  the input so it is called partition the input is a set t sizes t 1 t 2 up to tn the question we would like to ask is is there a partition of t into subsets let us say s and t minus s so  partition of t into 2 parts which is s and t minus s such that size of s equal size of a t minus s so  this should be exactly half size of t so  can you partition it in 2 parts both of which have exactly equal size again the size of the subset is nothing but sum of the sizes of the elements in this subset so  can you partition these into 2 parts so that size of both are equal firstly  the total size must be even if let us say all sizes are integers positive integers then the sizes for instance it must be so  let us assume that these things are true now  this problem is also in np clearly because if the answer is yes then the proof that the prover gives the verifier is just this partition for each element he tells which part it belongs right now  a verifier takes this partition he verifies that the sum of the sizes of elements in the first partition exactly equals sum of the sizes of the elements in the second partition so  this problem is in np we would like to prove that this is this problem is np hard and for most problems where sizes are involved we would like to use subset sum so  what we show is if there is a polynomial time algorithm for partition there is 1 for subset sum that is what we do so  let us see so  there is an algorithm for partition for subset sum the input is are some sizes s 1 to sn and i have b i want to know if there is a subset of size b so  here is partition here i want to know if there is a partition into 2 equal parts what is this partition say this means that there must be one of size b and 1 of size let us say w minus b where w is nothing but sum of the sizes so  this asks if there is a partition of this form this asks if there is a partition where they both equal so  how do we sort of use this to solve subset sum ? well  here is b what i am looking for is b and let us say w minus b what i do is i add say 2 extra elements 1 of size l 1 1 of size l 2 so  that l 1 plus b equals l 2 plus w minus b supposing i do this so  i take this input  input for subset sum i add 2 more elements 1 of size l 1 and 1 of size l 2 right now  i ask is there a part and l 1 and l 2 satisfy this l 1 plus b exactly equals l 2 plus w minus b now  i ask is there a partition into 2 equal parts so  i look at this supposing there is a partition into 2 equal parts supposing so  supposing l 1 and l 2 land up in different partitions you partition into 2 eq 2 equal parts if l 1 and l 2 are in different partitions then there is a yes there is an answer to the subset sum problem right there is a yes answer to the subset sum problem so  if there is a yes answer to the subset sum problem then there is clearly an answer to the partition right i take an answer to the subset sum problem put l 1 along with b and w minus l 2 in the sorry an l 2 along with w minus b then i know there is an answer now  if i look at an answer for partition can i construct an answer for subset sum well i can if l 1 and l 2 land up in different partitions can i make can i force l 1 and l 2 to land up in different partitions the answer is yes just take l 1 and l 2 large enough for instance if i take l 1 let us say equal to twice w if i take l 1 to be twice w then you can check that l 1 and l 2 will land up in different partitions because if l 1 and l 2 both of them land up in the same partition on the other side the maximum size that can happen is w right which is the sum of all sizes the sum of the sizes of all elements in the set set s so  if i take l 1 to be twice w then l 1 and l 2 this is elements which have sizes l 1 and l 2 must land land up in 2 different 2 different partitions and the rest of the elements when i look at the rest of the elements the ones with l 1 must sum to be b the ones in l 2 must sum to w minus b and this is the answer that we are looking for so  this is then a reduction that shows that partition is is np complete so  this let me quickly revise what we have done so  far i have not written a formal proof that partition is np complete  but i hope that you can fill in the detail so  let us revise what we have done so far  refer slide time  37  05  so  cook told us that sat is np complete and then we use this to prove that vc is np complete then we showed that 3 vc is np complete getting from sat to vc we first showed that clique is np complete and then independent set and then vc so  it it is not a direct sort of reduction we first showed that clique was np complete then independence set then vertex cover then 3 three vc then we showed that xc is np complete then we show subset sum finally  partition so  let us now look at a problem that we sorted out with so  remember that you are joined a company and your boss had given a problem where you are given jobs with with time execution time for each job and he wanted to schedule them on let us say 2 processes so  that the time that last job finishes was smallest supposing you could have supposing just was possible mean you actually had an efficient algorithm let us say polynomial time algorithm for this problem then you know observe that you can solve partition the reason is you just take take the input for partition the same input you feed into your your algorithm which did the split in to 2 processors the sizes are just given as running time now  you look at the partition that your algorithm proof is gives you back if they are both of the same size then there exist a partition of the original set into 2 equal halves right where the sizes are equal if not there is not so  the problem that you solve is more general than partition so  if you could have solve that all that problem that your boss gave you then you could have solve partition and by this sequence of reductions that we that we did you could have you would actually have constructed an algorithm for all of np that seems highly unlikely so  hopefully when you give this the  this set of arguments or this argument to your boss will be both reasonable and intelligent with intelligent enough to understand and may be give you a raise instead of firing thank you this finishes this module on np complete design and analysis of algorithms prof abhiram ranade department of computer science & engineering indian institute of technology  bombay lecture  32 approximation algorithms for np complete problems ? i welcome to the course on design and analysis of algorithms the topic for today is approximation algorithms for np complete problems so  let me start with a question  suppose we have an np complete problem  which we need to solve so  you wanted to solve a certain problem  which arose in some real life situation and it turned out  that it was np complete what you do ? this is going to be the subject of the next two three lectures how do we cope with the problem  which is known to be np complete ? usually  np complete problems arise  when we talk about optimization problems  refer slide time  01  36  so  if finding an optimal solution is np complete one wonders whether  we can at least get nearly optimal solution in polynomial time this approach is actually  quite promising and it is the approach of finding fast approximation algorithms it will not be interested in this two three lectures in finding the optimal solution but  we will be interested in finding an approximately optimal solution and therefore  we will be devising algorithms  which are called approximation algorithms and our hope is that for the real life application that we are worrying about  the approximately optimal solution that we find is also fairly useful another possibility is to examine  whether the real life problem that we want to solve has additional features that make it a special case of an np complete problem if this is true  then sometimes the special cases can have efficient algorithms  can have polynomial time algorithms so  for example  vertex cover is np complete but  if you are finding about x cover on a tree or on the bipartite graph we do have fast algorithms  polynomial time algorithms for solving such problems so  it is useful to think about  whether the real life problem that we are solving has any special features sometimes those special features can be exploited to get fast algorithms another possibility is to find what are called pseudo polynomial time algorithms so  let me explain this a little bit more  refer slide time  03  31  an algorithm is said to run in pseudo polynomial time if it is run time is polynomial in the size of the input instance so far so good so  far like the usual definition here is a difference the run time is polynomial in the size of the input instance when  the numbers in the input are represented in unary in the normal definition or in the definition of polynomial time  we require that the numbers be represented in binary or in some radix  which is larger than 1 what happens  if you represent them in unary ? so  just to clarify  if we have a number 13 that we represent  that we want to represent in the unary number in the unary representation system it will be represented by a string of 13 ones so  note for one thing  that this representation is going to be much longer  than this representation if you look at binary  this is going to be the representation but  this is still substantially smaller over here  over than over here in fact  the number of bits needed over here is log of this so  there is going to be a bit difference between the length of your input instance  when measure it in unary or in binary so  naturally if you are only interested in devising algorithms  which run in time polynomial in the length of the unary representation you have got a lot more freedom to work with your algorithms can take somewhat longer then if they were to be running in time polynomial  in by in their binary representation  when the numbers are represented in binary we have in fact see in pseudo polynomial time algorithm in this course so  this was the napes the dynamic programming algorithm  that we saw for the knapsack problem let me remind you  what the problem was you are given n items specified by their weights and values and you are given an integer capacity for a knapsack all these where integers  this description applies only 20   refer time  05  51    now  it was assumed implicitly  that the weight value and capacity are in d bit numbers so  since there are 2 n weights and values and one capacity the input size is d times 2 n plus 1 and if you remember  we showed that the time taken for the knapsack problem was o of c times n  where c is the value of the capacity so  this is crucial it is not necessarily the length of the bit string needed to represent the capacity but  it is actually the value of the capacity now  if these numbers are represented in unary  then the time taken is o of d n because  then c would be d bits long so  this c itself would be d as would be smaller than d and therefore  there is no problem  the time taken would be c times n but c times n is also d times n is at most d times n as well and this is certainly polynomial in the input size  because the input size is just this in fact  it is linear in the input size however  if the numbers are represented in binary what happens well  if the capacity is represented as a d bit binary number then c can be as large as 2 to the d so  then this time o of c times n really could be as large as o of 2 to the d times n now  notice that this expression 2 to the d times n is not polynomial in this expression whatever  power you take of this  whatever constant power you take of this you can not beat this and therefore  this is not polynomial so  clearly polynomial time  if you can get polynomial time it is better than pseudo polynomial time so  pseudo polynomial time is not the best possible or it is really different from our notion of good algorithms  which are polynomial time algorithms however  pseudo polynomial time is better than exponential time so  that is also worth noting  because the length is o of n d and exponential would be something like 2 to the power n d so  here we are getting d in the exponent  but we are getting n not in the exponent n just as a multiplier so  this is certainly  still much better than this so  as a compromise between polynomial time and exponential time it is useful to think about whether  there are pseudo polynomial time algorithms  possible for the problem that you want to solve  refer slide time  08  52  then people do look at algorithms  which are difficult to analyze  but instead of analyzing them they try out lots of instances and check whether the algorithms run fast enough this is what i mean by saying we try to discover algorithms  which work well in practice  such algorithms are called heuristics and they do tend to be useful  when solving problems which are known to be very hard so  often it may turn out that you may have a good heuristic  which you really can not analyze but  it seems to do the job  if it seems to do the job why not yourself the last idea  which is also used is to use the exponential time algorithm so  if a problem is np complete  we know that it can be solved by an exponential time algorithm so  we use that exponential time algorithm if the problem size is small or small enough  then the time taken may be acceptable or  if the problem is just to be solved once  then even if the problem takes a day does not matter we will run a computer for a day and get a solution so  this also works sometimes  sometimes for solving real life problems the real life problems tend to be reasonably small and today computers are getting really fast so  exponential time algorithms can work  it is not that they are entirely useless our focus at these lectures however  is going to be on approximation algorithms we would like to device algorithms  which are provably fast which are running in polynomial time  that is all that we mean in this  in these two three lectures when we say provably fast  that there are in polynomial time  and while they may not give the optimal solutions we will prove that they will give somewhat close to optimal solutions  refer slide time  10  49  so  here is the outline for today so  i am going to define the notion of approximation algorithms i will also define a term called the approximation ratio of an algorithm or an approximation factor of an algorithm and then  i will describe approximation algorithms for two problems one is the metric traveling salesman problem and another is the precedence constrained scheduling problem so  let us begin with the definition of approximation algorithms  refer slide time  11  22  so  let us say p is denotes an optimization problem p is an optimization problem and it look something like minimize this objective function subject to these constraints of course  it could be maximize  but for definiteness let us consider minimization first let a of i denote the cost of the solution found by an algorithm a on instance i so  we are not worrying about the time right now we are worrying about the objective function cost so  we want this objective function cost to be as small as possible but  this algorithm a  when run on instance i produces this objective function value suppose  opt of i denotes the cost of an optimal solution to this instance i for technical reasons will assume that opt of i is greater than 0  we will see why in a minute now  we define the approximation factor or the approximation ratio rho on instance i as a of i upon opt i what is the factor by which a is worse than opt i that is what this approximation ratio is all about so  it is a natural definition clearly  a of i the cost found by the algorithm can at best be as small as the optimal cost in general  it could be larger and therefore  this rho sub i is going to be larger and we would like it to be as close to 1 as possible in general  the approximation factor of this algorithm is just the maximum value of rho sub i over all possible instances of size n so  it is customary to use the worst case by enlarge and so here  as well we are going to look at the worst case ratio and of course  it is going to be parameterized by the size n so  we will write this as rho of n so  for different n we will have a different ratio so in fact  we are looking for  looking to evaluate this and we are looking to keep this small the goal clearly  is to design approximation algorithms or algorithms such that  rho of n is small as close to 1 as possible for large n and of course  the time for this algorithm is polynomial the algorithm must run in polynomial time sometimes  you want to maximize the objective function in which case  we will define rho sub i  the approximation factor as the reciprocal so  now we know  that a of i can at most be as large as this and therefore  it will turn out but  this is still going to be bigger than 1 so  again our goal is going to be similar so  rho of n is going to be the same and the goal is also going to be similar we want algorithms  which keep rho of n as close to 1 as possible  which get rho of n as small as small as possible  refer slide time  15  09  so  now we will use these ideas  to device an approximation algorithm for the metric tsp problem so  let me define this problem first the input to this problem is a graph g and this is going to be specified as an n by n metrics d  in which d of i  j denotes the distance between vertex i and vertex j in this graph now  the metric in the title  says that d has to have certain additional structure specifically  d has to form a metric and by that we mean  first of all for all i the distance of a node to itself is has to be 0 the distances have to be symmetric  the distance from i to j has to be the same as the distance from j to i and the final thing is that for all i  j  k the distance of going from i to j directly has to be no larger than the distance of going from i to k first and from k to j next this is often called a triangle inequality constraint so  imagine that i  j  k are the vertices of a triangle and this just says  that the straight distance going from i to j is smaller than the indirect distance so  let me first take an example of what a metric problem is going to be so  i am not going to draw the metric  the metrics d  but i am just going to take the problem and i am going to draw the graph corresponding to the problem so  one way to use such a graph is to imagine that the vertices are embedded in the euclidean plane  refer slide time  17  10  so  for example  here is vertex 1  here is vertex 2  here says vertex 3  here says vertex 4 i could draw out all the edges  but even without drawing all the edges let me tell you  that the distance from i to j is simply the straight line distance in the plane so  d of i to j is straight distance  straight euclidean distance in the plane now  all of us know that the distance from here to here  plus the distance from here to here can never be smaller than the shorter distances from the straight distance and so clearly  our third constraint the triangle inequality constraint is obviously  applicable over here so  for completeness i could write down this is the graph that here looking at for example  and if you do the arithmetic you could say for example  that you could calculate the distances so  this is the graph and if you look at d i  j to be the euclidean distance then clearly  it will satisfy all these metric property  the properties mentioned over here so  this would be a traveling salesman problem instance what is suppose to be output   refer time  18  41    what is suppose to be output is a cycle in the graph  passing through all vertices exactly once such that  the sum of the distances associated with the edges in the cycle is small as  is as small as possible so  this is the same thing as in the tsp problem you want to tour in the graph passing through every vertex such that  the tour length is as small as possible the first claim is that metric tsp is np complete well  i think we have studied earlier  the tsp is np complete but  it turns out that even with these restrictions so  this is the special instance of a tsp but  even with these special restrictions tsp remains np complete here is the claim  that we are interested in and which we are going to prove so  the claim is there exist a two approximation algorithm for metric tsp here is a quick overview of the proof in fact  the proof is actually quite simple the idea is actually quite interesting and but short so  the general idea is this and this scheme appears in other places as well so  first we are going to find the lower bound l  on the length opt of the optimal tour so  whatever graph we are given  it has some optimal tour  will try to figure out a lower bound on it then  we will construct a tour of length c  which is at most twice this so  notice that  it is very hard to figure out the length of the optimal tour so  we really want a tour  which has say twice the length of the optimal tour but  rather than that we will find a lower bound  which will be easily computable and we will show that we can construct a tour  which is at most twice the length but since  this is a lower bound we know that this is that c must also be less than twice opt because  l is less than opt therefore  c is less than twice opt so  this is going to be what we are going to do so  we will look at each step in turn so  this is the first step so  we want to find a lower bound on the length of the optimal tour  refer slide time  21  00  so  here is the main claim the claim says that the weight of a minimum weight spanning tree of g  with d as the weight matrix is a lower bound l on the length of an optimal tour of g so  this is the lower bound that we wanted so  i just have to prove this so  let us imagine that we are given any optimal tour we take that optimal tour and we remove an edge in it  edge from it what do we get ? well  we will get a path  which passes through all the vertices of the graph once it starts at some vertex and it passes through all the other vertex and returns to some other vertex but  is there anything interesting that we can say about this path well  this path is also a special case of a spanning tree this is a spanning path  it passes through every vertex and therefore  this also is a spanning tree of g so  it is length  it is total length is certainly no smaller than the weight of the minimum spanning tree because  the minimum spanning tree is by definition  that spanning tree whose weight is the least and therefore  the length of this  which is the weight of the corresponding the spanning tree  by the way  in this case weight and length are to be use synonymously weight is the terminology used in connection with minimum spanning trees and length in connection with tours so  i am sticking to those  stick into that  but really length and weight are the same so  the length of the path has to be greater than or equal to the weight of the minimum spanning tree it will be equal  if the path itself happens to be the minimum spanning tree  a minimum spanning tree but  the length of the tour is bigger than the length of the path because  the tour in fact  contained an extra edge and therefore  the length of the tour is also bigger than the weight of the minimum spanning tree but  this minimum spanning tree has beat l and therefore  we are done so  the length of the optimal tour is bigger than l so    refer time  23  32   we have proved this we have established a lower bound l on the length opt of the optimal tour now  we want to argue  we want to construct a tour and argue that it is length is at most 2 times l and once we are done  we will have proved our result  refer slide time  23  56  so  here is have you construct a tour with length less than 2 times l so  i am going to give you the algorithm so  first we find a minimum spanning tree  which allows us to determine the l so  the weight of the tree is l we can actually  write this down we can find the minimum spanning tree and we can find it is weight and that is going to be l  the lower bound next  we do a dfs  a dfs traversal depth first traversal of t or do a depth first search of t and we look at  the sequence of vertices that get visited and that sequences return out as e so  let us take our graph and let us look at that sequence so  here is our graph   refer time  24  49    now  we start at 1 and if we are doing the depth first search well there could be many ways  in which we do the depth first search so  first of all i have to identify  what this tree t is going to be so  clearly this is going to be the tree t so  this is going to be the minimum spanning tree in this graph the red edges now  if i want to do a depth first traversal of this tree  say starting at vertex 1  what would i get ? so  from here  let me just use red again so  from here i would visit 2 then from here i would visit 3 then i would go back  then i would go forward then i would go back  then i would go forward and so the e that i get is going to be something like this so  i start with 1  then i go to 2  then i go to 3  then go to 2  then i go to 4  then i go to 2 and then i go to 1 so  this is going to be my sequence e  so 1  2  3  2  4  2  1 so  this is how i have constructed e now  the idea is that if d appears  more than once in e so  there are several vertices  which appear more than once we are going to delete it is first appearance so  1 appears more than once  but this 1 is really the   refer time  26  28   same as this 1  because the tour is just closing so  we do not worry about this so  the first that appears more than once is this 2 so  now we are going to delete it and we are going to replace it by the direct edge so  we are going to delete 1 to 2 and 2 to 3 and we are going to replace it by a direct edge or maybe i will use black this fine  this will be perfectly understandable and then  we are going to repeat the previous step while possible so  if a vertex appears several times  we are going to short circuit it  we are going short cut it so  our current e now is going to be this  we have just removed this   refer time  27  14   so  the next vertex that appears twice is 2 it already appeared  but it is going to be 2 it could be another vertex  but in this case it just happens to be 2 so  we are again going to delete it is first occurrence so  if we delete it is first occurrence what does it mean  instead of going from 3 to 2 and 2 to 4 we are going to remove these edges and we are going to replace it with this direct edge so  we are going to keep on doing this step as many times as needed and at the end we are going to return e so  let me draw another picture to show you what this e  that was that is to be return is ?  refer slide time  28  02  so  this is our vertex 1  vertex 2  vertex 3  vertex 4 so  in our so original tour was 1  2  3  2  4  2  1 so  we removed this 2 and then we removed this 2 and we were left with this so  our e that remains at the end is going to be this so  going directly from 1 to 3  then going directly from 3 to 4 then 4 to 2 and 2 to 1 so  this is the e that we would be returning this is the claimed final answer so  let us go over each step and we will very about how exactly it is done so    refer time  29  10   the first step is done as is finding the minimum spanning tree so  how long does it take ? well  if we use prims algorithm it will take something like e plus v log v  so clearly polynomial time how long does this take  this is just depth first search so  it takes time linear in fact  so this time is less than this time so  now let us worry about so  these steps are again going to be fairly straightforward so  let us not worry about the time but  let us worry about the correctness so  once we find this e  we eventually we modify it and eventually return e so  let us try to figure out some properties of e so  my first claim is that weight of e after the step 2 is going to be twice l so  that actually  is obvious from this picture but i will just draw it again  refer slide time  30  19  so  our graph was this and our tour was this so  notice that our tour used every edge our tour e  the original value of tour e used every edge twice this is going to work in general  yes on every tree it will work  because no matter what do you have when you do the tour starting from any edge  you go down an edge and then eventually  you come back up and you have to do it exactly once so  clearly every edge will appear twice   refer time  31  01   in this e and therefore  the weight is going to be twice the length of the tree  twice the total weight of the tree but  the weight of the tree itself is l and so the weight of e after the step 2 is going to be twice l the other property about e that is important is that e must contain all the vertices in t e has to contain all the vertices in t so  it is a tour the only problem is that it contains some vertices more than once and that is why  it can not be it is not a good tour for us so  if v appears more than once in e  we remove the first appearance so  this is good because  we are going towards making sure that every vertex appears only once but  what is this due to e in particular does it do anything bad to the weight of e so  here is the important claim the claim says  that after step 3 weight of e is at most twice l it can only decrease now  this is the main part of the argument and the proof is actually  quite simple so  what is the new weight ? the new weight is the old weight and suppose v was the edge we deleted so  let me take a picture to explain this  refer slide time  32  25  so  this was a portion of e and this is the vertex v  which we are deleting how do we deleted  we take the previous vertex  which we call u let us call it u  we take the next vertex  we call it w and we removed these two edges and we put down this edge so  what happens to the total weight ? well  the total weight now becomes the old weight plus what we put in or minus what we removed so  minus what we removed is so minus of d of u  v and we also removed this plus d of v  w and we put in u  w so  this is the new weight that is what we have written down over here but  notice that these are two sides of a triangle and this is the third side essentially so  this is the straight path and this is the cross path so  which of these two is bigger ? so  clearly this one is going to be bigger  if at all and therefore  we know that this entire thing has to be less than or equal to 0 or therefore  the whole thing is at most old so  we have proved that the new weight is at most the   refer time  34  01   old weight the old weight was twice l and so the new weight is also twice l so  if we keep on repeating this step as many times as we can what happens  the weight keeps on reducing so  it will always be bigger than  it will always be smaller than twice l so  we have proved that the weight of e is always going to be at most twice l so  the final claim is that just before we return of course  the weight is going to be at most twice l but  e is also going to have every vertex exactly once why is that ? well  we repeated until no vertex appeared more than once so  clearly no vertex appears more than once but  initially every vertex did appear at least once and therefore  finally  every vertex appears exactly once and so therefore  e is a tour every vertex appears once and it is weight is twice l l is lower bound and therefore  we are done the final issue is there might be some question about the time required for this part so  here is a very nice simple observation  which says that this entire thing can be done in linear time what does this loop do ? so  it says  if v appears more than once  delete the first appearance but  what if v appears several times  then we will delete all  but the last appearance and this is going to be true for every vertex so  we are going to keep only the last appearance of every vertex in this traversal e but  what is that  we know that when you do graph search  we should do a post order traversal that is exactly what this is and therefore  the time for this steps 3 and 4 together is at most the time for a breadth for a depth first search and therefore  it is just o of the number of edges plus the number of vertices so  the total time is just simply is dominated by the time for finding the minimum spanning tree and therefore  it is e plus v log v  say using prims algorithm  refer slide time  36  41  let us now consider the next problem the next problem is precedence constraint scheduling  which is also an np complete problem and we are going to find a polynomial time approximation algorithm for this the input to this problem has two parts the first part is a directed acyclic graph g vertices in this graph  represent unit time tasks and there is an arc directed edge going from u to v corresponding to the restriction that vertex u must execute before vertex v so  there is a precedence constraint from u to v and therefore the name of this problem you are also given as a part of the input  and integer p  where p denotes the number of available processors so  p is the number of tasks that you can perform at each step you may not be able to find that many task but certainly  you can not perform more than p tasks at each step for the output we require to specify an integer time of execution t of u for each vertex u such that  first of all t of u is greater than 0 and at most p vertices has the same time of execution furthermore  if there is an arc from u to v then the time of u is must be strictly less than the time of v remember that  these are integers  so this really means less than or equal to so there is a difference of at least one and finally  we want to minimize the length of the schedule so  the maximum over all times is as small as possible this problem is known to be np complete for variable p so  if p  p changes p is allowed to change as a part of the input then this is known to be np complete here is one lower bound i claim that the length of the longest path in this graph is a lower bound so  let us do that band let us see that and for that let us take an example as well so  let us take a simple graph  refer slide time  39  32  so  say the graph g looks something like this so  here is vertex 1  which is one task here is vertex 2  which is another task then maybe there is vertex 3 over here and there is an edge from 1 to 3  there is vertex 4  there is an edge from 1 to 4 as well  may be there is an edge from 2 to 4 also may be there is a vertex 5 and there is an edge and say there is an vertex 6 with these edges so  this for example is g so  this is one part of the input and let us say p is equal to 2 so we want to find a schedule so  i claim the first lower bound  which and that is claimed in the first lower bound that no matter what you do  the length of the longest path is lower bound at the time required   refer time  40  35   so  the idea is actually fairly simple so  let us identify a longest path over here so  in this case  the longest path is quite simple so  say for example  this is the longest path there are several longest paths  but this is the longest path what is this length ? well  we are suppose to measure the length  in terms of the vertices  the vertex length so  this has length 3 and the claim is that the length of the schedule must be at least 3 why is that ? well the precedence constraints says that  if this is executed at step 1 whatever step it is executed  at this can not be executed at the same step so  it has to be executed one step later this has to be executed one step further than that and so on so  whatever the length of the graph is that many steps are needed for this execution so  that is the first lower bound   refer time  41  36   second lower bound is based on how much load can be consumed at each step so  if n is the number of vertices in g how many time steps  how many how many vertices can be consumed can be worked on by the p processors at each step well  at most p and therefore  n over p steps are at least needed so  that lower bound in this case is 6 upon 2 which is also equal to 3 so  the first lower bound l is equal to 3 the second lower bound is also equal to 3 so  let us now consider  let us now examine whether in fact  the upper bound for this matches so  is it match ? well  here is one possible schedule so  we will schedule this at step 1 we will schedule this also at step 1 and in fact  that is our only choice next  we have these 3 so  we can pick say this we will schedule at step 2 this we will schedule at step 2 this is ready to be scheduled  but we can not schedule it  because we only have two processors so  this has to be schedule at step 3 this we have a processor available but  this can not be scheduled at step 3  because this has to be scheduled only after this so  this has to be scheduled at step 4 so  t of 1 and t of 2 are both 1 ? s t of 3 and t of 4 are both 2 ? s t of 5 is 3 and t of 6 is 4 so  in this case the upper bound in fact  is 4 and it is bigger than the lower bounds so  now i am going to describe the algorithm  which will get   refer time  43  36   within twice the best possible schedule and it will use these lower bounds and it will also use the notion of a ready vertex so  vertex is set to be ready or ready to be scheduled  if it has no predecessors or all it is predecessors have already be in scheduled so  now i will describe the scheduling algorithm  refer slide time  44  04  so  this is a procedure sched  which takes g and p and it is a 2 approximation algorithm it produces a schedule  whose length is twice the optimal schedule as we will prove in a minute so  here is the algorithm actually  it is quite simple  while the entire graph has not been scheduled we select as many ready vertices as possible  but at most p for each selected vertex u  we will set t of u equal to i so  we will schedule it at step i and then  we will increment the time and then we will repeat how long does this whole thing take ? well  the algorithm will take time the time required will be the time to identify this ready vertices so  the ready vertices will be found by looking at by looking at vertices  which have already been scheduled i will just say that this can be done very efficiently by doing a topological sort and in fact  you can do the whole thing in time linear in the size of the graph so  this in fact  will run in polynomial time so  it is easily shown that a topological sort will suffice let us now  consider whether this is correct so  is this correct well we are following the restriction about the number of processors  because we are only picking at most p vertices we are following the restriction about precedence  we are because we are picking only ready vertices so  this is going to produce a correct schedule a valid schedule and it is going to run in polynomial time  the only thing that we need to prove that it is a two approximation algorithm so  let g sub i denote the graph induced by the unscheduled vertices after iteration i l sub i is the length of the longest path in g sub i so  remember that that is a lower bound on g sub i let n sub i denote the number of vertices in g sub i the first claim is either n sub i upon p  which is the lower bound on the ith graph is equal to n sub i minus p  n sub i minus 1 upon p minus 1 so  either this lower bound decreases or l sub i is equal to l sub i minus 1 minus 1 so  either this lower bound decreases or this lower bound decreases so  after first iteration we have l sub 1 then we have l sub 2 so  l sub 2 will be either 1 less or this lower bound for the second iteration will be 1 less and this will be enough to prove the 2 approximation so  let us prove this  refer slide time  47  00  the proof is actually quite simple so  the basic step in the algorithm is to find p vertices in that step3 of iteration i so  suppose it does find those p vertices in iteration i so  what happens ? so  if it finds p vertices  then the number of vertices that remain is going to be p less so  n sub i is going to be equal to n sub i minus 1 upon p but now  if you simply divide by p then we will get part a so  this happens then part a will hold the other case is suppose that the algorithm does not find p vertices if the algorithm does not find p vertices  then there are at most p minus 1 ready vertices so  what are the ready vertices  the ready vertices are the vertices in the graph  refer slide time  48  06  such that their predecessors have already being scheduled or they do not have any predecessors whatsoever what do we know about such vertices ? well  what do we know about paths here is the key idea every longest path   refer time  48  22   must originate on one of these ready vertices suppose  it does not  suppose here is a longest path well  we go back this is not ready vertex so  there must be vertex behind it if there is a vertex behind it  then we are getting a path even longer therefore  by contradiction the longest path must originate over here so    refer time  48  47   the algorithm on the other hand schedules all these ready vertices but  if it does schedule all these ready vertices  then the lengths of all the paths starting at these ready vertices  including the longest path must decrease by 1 but  that is essentially saying that l sub i equal to l sub i   refer time  49  04   minus 1 minus 1 thus   refer time  49  07   we have proved this  either this holds or this holds the next claim is that this algorithm gives a 2 approximation so  here is a proof  refer slide time  49  17  so  remember l was a lower bound the length of the longest path in the entire graph so  i am going to call it l sub 0 n was the number of vertices in the entire graph i am going to call it n sub 0 the initial lower bounds thus are l sub 0 and n sub 0 upon p after iteration i the bounds are l sub i and n sub i upon p  and what else to be known claim 2  which we just proved says that either the first bound or the second bound drops by 1 in each iteration so  starting from l 0 and this n 0 upon p  we go to l 1 and n 1 upon p l 2 and n 2 upon p and so on claim 2 says that either the first one drops or the second one drops eventually  until we get to the last iteration no bound can drop below 0 .because it does not make sense to say that the length of the path is negative or that the number of vertices is negative so  which means that if more than l plus n over p steps are taken then one of these bounds must become negative starting from here because  this l 0 can only drop by l  this can only drop by n over p so  one of these has to go below 0 but  that is not possible and therefore  it means that l plus n over p steps must suffice our schedule must have length l plus n over p at most but  what do we know about l plus n over p well  this is certainly less than 2 times max of l and n over p so  we will just replace the smaller of the 2 with the max so  this is going to be less than 2 times max of l and n over p but  what is max of l and n over p so  this is a lower bound  this is a lower bound so  the larger of the 2 is also lower bound so  but if it is a lower bound  then opt is even bigger than this so  this is less than twice opt so  this max is a lower bound so  opt the length of the optimal schedule can not be smaller than the max and therefore  we have that l plus n over p is less than 2 times opt but  this is the length of the schedule  which we produced and this length is less than 2 times opt  so l done so  we have proved that this algorithm gives a 2 approximation  refer slide time  51  58  so  now i am going to conclude so  today we discussed various strategies for coping with np complete problems the strategy which we are going to study is the strategy of devising approximation algorithms so  these are defined as giving nearly good solutions rather than the best possible solutions but  the good thing about them is that the time is polynomial there are various techniques for designing approximation algorithms and the techniques that we studied today can be summarized as follows so  basically we try to find lower bounds on the objective function  which we want to minimize and then we try to get close to this of course  if the objective function had to be maximize and we will try to find upper bounds and we will try to get close to those so  device we will algorithms in this case  which will get close to this lower bounds so  the lower bounds are easily should be easily identifiable and therefore  we can actually compute them and then  we can may be try to target an algorithm  which tries to meet them but  of course  it will not succeed in meeting them but  it will try to it will succeed in hopefully getting close to them we will see more such techniques in the next lectures thank you design & analysis of algorithms prof abhiram ranade department of computer science & engineering indian institute of technology  bombay lecture  33 approximation algorithms for np  complete problems ? ii welcome to the course on design and analysis of algorithms this is the second lecture on approximation algorithms we already saw in the previous lecture  what are the approximation algorithms were ? we will go over that very quickly today and we will take two more examples so  let me quickly remind you what an approximation algorithm is  refer slide time  01  20  so  an approximation algorithm is a polynomial time algorithm  first of all and it solves  an np complete optimization problem and it gives near optimal solutions the solutions it gives are near optimal of course  if it gave optimal solutions in polynomial time then  that would prove p equals np and of course  we are not here to prove that and most people in fact believe that  that is not correct so  the most people believe that p is not equal to np and therefore  polynomial time algorithms which give optimal solutions are unlikely to be there for np complete problems so  last time we looked at this and we define some notation for this so  let me just quickly go over that notation  refer slide time  02  29  so  let us say p is a problem and a is an approximation algorithm for solving it we said last time that  if i is an instance then a of i is the cost of the solution found by a on instance i so  here we are going to assume that our problem is a cost minimization problem so  there are some constraints which are specified as a part of the problem and a cost function is given and our goal is to minimize this cost as we said  this cost in general will not be the optimum a i  will not be the optimal for instance i but in fact  there is some other cost  which i will call opt of i which is presumably better than this so  this would be the cost found by the optimal algorithm so  clearly we know that a of i is at least as big as optimal as opt of i so  we said that rho are the approximation factor on instance i  is simply going to be a of i upon opt of i and we also said that  rho of n is going to be defined was defined as max over all instances of size n of rho sub i so  this is the approximation factor of the algorithm and this is what we want to keep small so  this is the general framework in which we are going to work so  we are going to devise algorithms for np complete problems  which produce a small approximation factor in other words the cost that they return  the cost of the solution that they return a of i is going to be reason reasonably close to opt of i and of course  they are going to be running in polynomial time in path of i so  today i am going to look at two problems  refer slide time  05  22  so  one problem is going to be the set cover problem and another is the  is going to be so called the k center problem i will describe these problems in a minute and i will also give real life examples  corresponding to these problems in addition to that  we are also going to look at some techniques or some themes first of all  we are going to use some kind of a greedy idea for solving these problems see you have already seen greedy strategies earlier so  a greedy strategy is  in greedy strategies procedure for solving is to be thought of as a sequence of decisions that you make every decision is going to change the cost a little bit and the goal  the idea of the strategy is that the ith decision that you take  tries to make the ith change in cost the ith increase in cost  as small as possible greedy is also short sighted in the following sense that  we will try to minimize the ith increase in cost using the ith decision but in doing so  we will not worry about what happens subsequently in general of course  greedy algorithms will not work but  in many cases they seem to work and today ? s problems are somewhat in that same framework in addition to that  we are also going to see an interesting proof strategy which i will call compete with the optimal so  we are going to imagine that  there is an optimal algorithm running along with the algorithm that we design and we are going to try and do at least reasonably well  as compared to that we can not do better than or even as well as the optimal algorithm but  we will try to see that we do not do too badly so  let me start off with the set cover problem so  here is the set cover problem  refer slide time  08  00  the input consists of sets  s 1 s 2 all the way till s sub n let me use  u to denote union of s 1 s 2 s n so  let me call this collection this entire collection i am going to call c so  this is the collection of sets and our goal  the output that we want is a sub collection let me call it c prime such that the union of s sub i  where s sub i belongs to c prime has to be exactly equal to u or the union of all the sets so  we want a small collection c prime a small set of sets such that  they contain the same elements as the original connection so  what is to be there is something to be minimized over here and we want to minimize the cardinality of c prime so  this is the set cover problem  refer slide time  09  53  in the decision version of the problem  so the decision version as usual we are given an additional target t so  this is the number of sets allowed in c prime and what happens over here is that this and we are asking does there exist a c prime with cardinality t such that its union  the union of the sets in it is u and this problem is np complete in fact  the decision there is a reasonably simple reduction from the vertex cover problem we are not going to prove this but  we are just going to directly try and find an approximation algorithm for the set cover problem now  before going to the algorithm let me give you a brief application  a small application of this problem to tell you that  to tell you how it might arise in real life  refer slide time  11  07  suppose u denotes a set of villages suppose we are also given a set l of locations  here hospitals can be built for each location say little l  we are given the set s l and this is the set of villages that will be served  if a hospital is built at l the natural problem now is to determine the smallest number of locations such that  all villages can be served let me write the term we want to pick as few locations as possible and build hospitals there  such that all villages will be served do you see now  how this corresponds to our set cover problem ? well  the correspondence is actually exact our sets s l constitute the collection these are the sets in the collection as defined in the problem  in the set cover problem and what we want is to pick a sub collection of this collection such that  all villages will be served or in other words all villages will appear in at least 1 s l so  the correspondence is exact so  now we turn to the algorithm  refer slide time  13  45  so  the basic idea of the algorithm is greedy what do i mean by that ? well  we are going to think of picking sets when at a time and every time we pick a set  we will try to cover or to collect together as many elements  which have not yet been collected so  here is the idea so  here is basically the algorithm so  i am going to start by defining my c prime to be null so  c prime is going to be the collection our answer eventually i am also going to say that  all elements of u which is the union of all the sets are initially uncovered by covering  i mean they are inside a set which is included in this c prime and now  here is the basic loop so  while some uncovered elements exist  what do we do ? we pick set s i that contains maximum uncovered elements so  we will have to maintain some data structure which says  which keeps track of which elements are covered and which elements are not covered and as soon as we pick elements and we put that  pick a set and we put that set into c prime we will have to cover those elements so  we pick the set and then  we set c prime is equal to c prime union s i and then  we uncover or recover elements in s i so  our universe or the ground set  has some elements has all elements initially uncovered then  we pick sets as a result of which some elements get covered and we keep on covering elements and eventually  we get to a point where all elements are covered and that is a time  we stop so  this is where we end the loop and we return c prime so  we are making the decision at each stage which said to include into our sub collection and at each stage we are saying so  let us include a set which gives us which tries to cover maximum elements so  the analysis of it is actually quite nice and simple the analysis goes by iterations of this basic algorithm so  i am going to start by defining some notation so  let say n sub 0 is the number of elements in u  the total number of elements which i want to cover or this is also equal to n there is a reason why i want to call it n sub 0 as well n sub i is going to be the number of elements  uncovered after iteration i so  number of elements in u originally  these are all uncovered so  this is uncovered before the first iteration n 1 would be elements  which are uncovered after the first iteration after the second iteration  all the way till after the ith iteration i am going to use opt to denote  number of sets needed by optimal algorithm to cover all elements so  this is my notation and now i want to state my main claim so  the main claim is the following the main claim says that  n sub i plus 1 is less than or equal to n sub i times 1 minus 1 over opt  very simple claim and the proof actually is also fairly simple so  how does the proof work  refer slide time  19  30  so  our algorithm has executed for i iterations and this is our set u of which some elements have been covered and these are the n sub i elements  which are not covered after the i  after the ith iteration now  what do we know about the optimal algorithm as we said our strategy is going to be  we will try to do we try to compete with the optimal algorithm so  what we know about the optimal algorithm ? well  we know that this entire set u is covered by opt sets  by a number of sets equal to opt so  it covers all these elements so  therefore know that even this region is also covered by opt sets so  now if i look at how these sets are covering this region ? i must claim that  there has to exist at least one set which covers n sub i upon opt elements at least so  at least one set covering at least because  if no set covered at least these many then this group of n i elements would not be  it would not be possible to cover this so  there has to exist at least one set which covers at least n sub i upon opt elements here is where the greedy property now comes in so  at this point we choose a subset  which covers the maximum number of elements from this so  which ? so  what do we know about the set ? so  this subset that we choose is this then  we know that this region has to contain at least n sub i upon opt elements  refer slide time  21  53  so  what does that mean ? that means that number of elements remaining after i plus 1 th iteration has to be less than the n sub i  which were present before the i plus 1 th iteration minus whatever got covered and these are n sub i upon opt so  in other words n sub i times 1 minus 1 over opt this is precisely what we claimed a few minutes ago  refer slide time  22  36  so  this claim has been proved so  let me write down our second claim just here our second claim is that cardinality of c prime  number of sets returned by our algorithm is less than or equal to opt times l n n so  it is opt by log n log of log n factor  log taken to the natural the base e so  we will return c prime elements whereas  opt will return the optimal algorithm  will return opt and we will not be returning too many  too much worse we will not be doing too much worse we will be doing only an l n and n factor worse this is our second claim  this is what we want to prove next  refer slide time  23  41  so  how many elements to be returned ? well  if the algorithm runs for t iterations then  we return cardinality of c prime is equal to t so  what we need to evaluate is how many iterations  does the algorithm run for so  suppose it runs t iterations then  what do we know about the number of elements that are uncovered after t iterations well  we know that n sub t is that number and we know from our first claim that  n sub t is less than n sub t minus 1 times 1 minus 1 over opt but  we can keep on repeating this and so  therefore we get this is less than or equal to n sub 0 times 1 minus 1 over opt whole to the power t because  this factor will keep on repeating and note of course  that this is just n now  nice little inequality comes to our rescue and that inequality is 1 minus x is less than e to the power minus x for x not equal to 0 so  what does that allow us to conclude ? so  therefore we can conclude n sub t is strictly less than now n times so  this is 1 minus 1 over opt so  1 over opt is going to be my x so  i am going to have e to the power minus t upon opt now  let us check what we what happens if t is equal to opt times l n n what happens then ? so  this is nothing but n times e to the power minus l n n times opt upon opt or this is n times e to the power minus l n n and therefore  this is strictly less than 1 so  we have proved that n t is strictly less than 1 but  if n t is strictly less than 1 what does it mean ? that means  that n t is exactly equal to 0 what is that mean ? that means  after t iterations all the elements of u have been covered nothing has been left uncovered but  this happens for t equals opt times l n n so  in other words cardinality of c prime is opt times l n n and that is exactly what we claimed  refer slide time  26  40  so  this finishes the analysis of the set cover algorithm so  what we have proved over here is that  we can also find a set cover the size of that set cover is going to be worse by a factor l n n we now come to our second problem the second problem is the so called k center problem i do not want to use the letter k  because k comes in handy for indices and things like that so  i am going to call it as the t center problem  refer slide time  27  19  this time  i am going to describe the problem informally first and then  i will formalize it so  let us say let us take an example the example is that  we are given some statistics say about students in a class so  perhaps we are given a plot so  on one axis we say have the height on the other axis may be we have say the weight so  may be the class has some number of students and may be the plot looks something like this so  this is the plot of the heights of different students now  an important question in analyzing this data statistically  is to see if there are any clusters so  say for example you could think  that this region is consists of students who are somehow similar in their build this region consists of students  which are who are somehow similar in their build so  it would be so  it is interesting statistically it is see  if this entire distribution can be described just by using say two representatives and the question then is  is there a good way of choosing those representatives for example  we could say that this could be one of the representatives and say  we could say that this is another representative how do we choose a representative ? well  a representative is chosen  so that its distance from the rest of the elements in the cluster is as small as possible and furthermore  we decide to put an element say this one into this cluster because  it is closest to its representative so  the two the thing to the important thing to note over here is the so called radius of each cluster so  this cluster has this is the farthest element in this and therefore  this is the radius of this cluster in this say for example  this might be the farthest distance and so  this is the radius of this cluster and so  the maximum of these two is the so called radius of this clustering so  what is our goal ? well  we are given how many clusters are needed and we want to pick the cluster centers and divide the data into clusters so  in this case this is one cluster and this is going to be another cluster here the clusters are very obvious but  in general they will be mixed and finding representatives and defining the boundary between the clusters  is going to be a little bit hard but in any case  we want to define the boundary and find a center the representatives and the radius  the radia of the clusters such that  the largest radius is as small as possible so  that is how we measure the goodness of the clustering problem so  we are going to do this but  before that i would like to give you the formal definition of this problem  refer slide time  30  48  so  the input consists of a set p  which is a point set let us say  this point set is in some d dimensions and it has say some n points and we are also given an input parameter t this is the number of clusters desired before i describe the output  i need to have good notion of distances and things like that so  the distance between two points is going to be simply there  euclidean distance so  it is a d dimensional space it is going to be square root of the sum of the squares of the coordinate differences as usual but here  we are also going to be talking about the distances between a set of points and a point so  let me define that or a distance between one set of points and another set of points so  let us say s is one set of points and s prime is another set of points so  i am going to define the distance between two point sets as the minimum  the minimum distance between p p prime  where p is in s and p prime is in s prime so  these could be in general arbitrary point sets but  we want to see are there two points at least which are too close or very close and that is the distance that if we are going to pay attention to so  let me now tell you what the output is  refer slide time  32  49  so  we had the set p of points so  the output is a set c which is a subset of p such that  cardinality of c is equal to t remember t was number of clusters and what we want is  that d of c to p should be as small as possible so  i am going to take the maximum over little p belonging to capital p of this distance so  i am going to ask which is the point which is farthest from these centers  that is the distance which i want to minimize ? so  max p in p is smallest possible so  what is the intuition over here ? so  the c is the c that we want to select are the cluster centers every point is at some distance so  when we ask d of c comma p  we are asking which is the closest center for this point ? so  of the closest centers i am going to take that distance but  now i am going to ask  which is the point which is farthest from its closest distance ? so  which is exactly  what we talked about earlier ?  refer slide time  35  00  so  what has happened over here is that  this is our set of points so  we pick some centers and for each point we are asking  which is the closest center ? and we are taking its distance and we are minimizing this maximum over all distances so  let me also define the radius of each cluster so  first of all let me define each cluster so  if a point is closest to this center then  i am going to say that this point belongs to this cluster so  that is how clusters are defined and of all the points which are belonging to a certain cluster  i just look at the maximum distance and that is the radius of that cluster let us now describe  let me now describe an algorithm for this problem so  this is the clustering algorithm  refer slide time  36  00  so  what are we suppose to do ? we are supposed to select the centers we are supposed to select t centers such that  no point is too far away from any of the centers and we are going to use a greedy strategy for solving this problem so  how do i select the first center ? well  for that we do not really have really good idea right now so  let us just say that it is picked arbitrary but note that  if i pick some k clusters so  i have picked c 1 c 2 c k already picked then  if i want to pick c k plus 1 there is an interesting greedy idea that can come into play so  what is the greedy idea ? well we ask  what is the farthest point from these first k clusters ? we picked first k centers and we want to know  which is the point which is farthest from this k centers ? so  this point which is farthest is at some distance r then that in fact  is the radius of this clustering that we have produced so  now we want to reduce this radius of this clustering how do we do that ? well  here is a simple idea we pick c k plus 1 to be exactly that point this is the point  farthest from c 1 all the way till c k so  let me now have some so  that is why that is basically the algorithm so  i will pick c 1 arbitrarily then  i will pick c 2 to be the farthest point from c 1 then  i will pick c 3 to be the farthest point from c 1 as well as c 2 then  given k points i will pick the k plus 1th point  to be the farthest from all these k points let me now define some notation so  my notation is as follows i am going to use r sub k to be the clustering radius  which is what we want to keep small after k points are picked what do we know about this clustering radius ? so  we know that r k plus 1 is less than or equal to r k why ? because  that is what we exactly did we pick the point  which is farthest and we made it into a cluster so  very likely now the farthest point the distance of the farthest point has reduced and therefore  we expect the k plus 1th clustering radius to be smaller than r k well  what do we know about r k itself ? what is what do we know about the clustering radius r k ? this in fact  is the distance from c 1 c 2 c k to c k plus 1 because that is exactly  what we did we looked at which was the farthest point after picking  having picked the first k centers we picked at we looked at the farthest point so  this was the point p and we said  what was that distance so  this is exactly that distance and therefore  that must have been the radius after having picked  the first k points and what we have argued earlier is that  the distance is going to keep on decreasing so  these two are the important facts about how the radius of clustering change is related to each other and to the distances and to the centers so  we are going to use these facts in a minute  refer slide time  40  55  so  let us now analyze this algorithm in more detail as we said earlier  this algorithm the analysis of both these algorithms is going to be based on this compete with the optimal strategy so  we are going to try and see what the algorithm  what the optimal algorithm would do so  let me just remind you that  c 1 c 2 c t are centers selected by our algorithm a let us say  o 1 o 2 o t are centers selected by the optimal algorithm so  we would like to see how these centers are related to each other the interesting thing is that  we do not know what o 1 o 2 o t are but  nevertheless for the purposes of the proof we will assume that  we know them so  we are not actually going to compute them any time but  for the purposes of the proof we know them and then  based on that we will be able to argue  what the relationship between the two clustering is so  we have those two we have the center selected by c 1 by the algorithm and the center selected by the optimal by our algorithm and then by the optimal algorithm so  let us focus on the centers selected by the optimal algorithm  refer slide time  42  23  so  say this is o 1 this here is o 2 this here is o 3 this may be here is o 4 may be this is o 5 and let us also look at the corresponding clusters so  say these are the corresponding clusters i am not going to draw all the points inside but  i am just drawing the corresponding clusters so  now let us ask how do the centers that we selected relate to these clusters a natural question to ask is  is there exactly one center we select from each of the optimal clusters or do we perhaps select two centers from each cluster natural guesses to say that perhaps  we just select our clusters our centers are also selected such that  that one comes from that each one comes from the other  from the optimal cluster of course  it could be this or it could be the case that we select two centers two of our centers are selected from the same cluster  as the algorithm selects as the optimal algorithm selects so  basically there are two cases  refer slide time  43  46  case 1  each optimal cluster contains 1 c sub i or case 2  each optimal cluster or some optimal cluster contains say c sub i and c sub k so  in fact let us call this optimal cluster o sub j you might think that  we also need to consider a third case some optimal cluster contains no centers picked by our algorithm but  note that in this case some other optimal cluster must contain at least two of our centers and so in fact  we will be having our case two in this possibility as well so  let us now analyze case one in detail in fact  each optimal cluster contains 1 c i and we are free to give the optimal clusters whatever  names we want so  we can say that each optimal cluster o sub i contains  exactly 1 c i and in fact  we will name it correspondingly  refer slide time  45  36  so  let us come to this picture over here so  in this picture may be what we are looking at is  this is c 4 this is c 3 this is c 5 c 2 c 1 so  the algorithm happen to choose center such that  exactly one point is picked from the clusters that the optimal algorithm would have chosen so  this is the first case that we are looking at so  basically the idea now is that we will prove that in both cases we will not do much worse than  what the optimal algorithm did so  now let us pick point p in one of these clusters what do i know about the distance of this p from c 3 i know that  this distance is at most the sum of this distance plus the sum of this distance but  what is this distance itself ? this distance is at most  the radius of this cluster this distance is also at most the radius of this cluster so  that means that this distance is at most twice the radius of this cluster  refer slide time  47  17  so  what is the important observation that we have made we have established that  if p is a point in optimal cluster  c cluster o i capital o i then  the distance of p from c i is at most two times the radius of o i but  this is nothing but  this is certainly at most 2 times the max radius of the optimal clustering so  we are going to call this r sub opt so  this is 2 times r sub opt so  we have really proved what we wanted surprising as it may seem  we have proved that this point is close to some center not necessarily the center with which it is associated  in the final clustering that our algorithm produces it might be associated with some other center but  its distance to this center c i is itself less than 2 times r opt and this is true for every point and therefore  in this case we have in fact established the theorem which said that  which says that  refer slide time  48  49  so  we have proved that the distance of the quality of the clustering  our quality of clustering produces clusters of radius at most twice the radius produced by the optimal cluster so  now we come to case 2 so  case 2 says some optimal cluster o j contains c i and c k  refer slide time  49  16  so  let us draw picture so  here is my optimal cluster let me call it capital o j here is little o j  its center and suppose that it contains cluster centers c i and c k which our algorithm picked well  what do we know about distance from c i to c k so  i claim that this is greater than or equal to the distance of c 1 to c i and i am going to assume here without loss of generality  that i is less than k so  the distance of this set c 1 to c i or rather c 1 to c k minus 1 with center c k so  here i am only taking the distance from c i to c k here  i am allowing a large number of other points to come in so  this distance can only be bigger than this distance but  what is this distance so  this distance from what we argued earlier is something quite is related to our clustering so  we argued that r k plus 1 has to be less than r k  which is this so  comparing these two we can argue that  this distance has to be bigger than so  this distance is nothing but  r k minus 1 but  this r k minus 1 is certainly bigger than or equal to the final radius  that the algorithm got so  what we have argued is that the distance between these two clusters  if they happen to fall within the same optimal cluster so  distance between any two clusters  any two cluster centers is going to be the optimal is going to be bigger than the optimal radius produced by our algorithm here we have not yet use the property that these two centers actually lie in this single optimal cluster so  we will use that property now so  what do we know about the two  these distances so  we know that this distance  again by the triangle inequality is at most this distance plus this distance so  what do we know ? we know that  d c i c k is less than or equal to the distance from c i to o j and the distance from o j to c k but  what is this tell us so  this lies inside this cluster and therefore  this is at most the radius of this cluster this is at most the radius of this cluster so  this is just 2 times the radius of this cluster so  what have we proved ?  refer slide time  52  57  we have proved that  the distance from c i to c k is at most 2 times the radius of cluster o j but  this is less than 2 times the optimal radius so  together what have we argued ? so  we have argued between these two things that twice the optimal is greater than or equal to distance from c i to c k  which in turn is greater than or equal to the radius produced by the algorithm so  even in this case we have argued that  the radius that is produced by the algorithm is at most twice the optimal radius and that is what we argued in the first case as well  refer slide time  53  56  so  in conclusion we can say that greedy algorithm produces clustering of radius at most 2 times radius produced by optimal algorithm this is the main theorem of this entire exercise so  let me now summarize and have some concluding remarks  refer slide time  54  56  first  in the clustering example the distances i mentioned had to be euclidean but  we did not really use the euclidean property we just use the property that  the distances we just use the triangle inequality so  essentially this means that any metric is enough so  this clustering algorithm works not only for euclidean distances but  anything which satisfies the metric properties that is  it satisfies essentially the triangle inequality that will work so  instead of clustering the same t center problem or the k center  can be related not only to clustering but  it can also be related to some kind of a facility location problem so  let you think about this i would also like to observe that  set cover like problems also appear quite frequently  just as this clustering problem many scheduling problems and important problem amongst them is so called crew scheduling  is a variation on set cover and in general  the greedy algorithms and a style of analysis used over here which is which i would like to call  compete with the optimal also appears quite frequently so  we have seen two styles of analysis  compete with the optimal and compete with and compare to the lower bounds so  between these two styles we should be able to design a fair number of approximation algorithms thank you design & analysis of algorithms prof abhiram ranade department of computer science & engineering indian institute of technology  bombay lecture  34 approximation algorithms for np  complete problems ? iii this is the third lecture in the series on approximation algorithms a short review of what we have seen so far  refer slide time  00  55  so  one of the approximation algorithms we saw gave us  a two factor approximation for metric tsp we also saw a two approximation algorithm for metric clustering having seen these algorithms  a natural question might be that instead of just getting a two approximation is it perhaps possible that  we can get a 1.5 approximation say either for metric tsp or for metric clustering here are some possible answers and these happen to be true  as a matter of fact so  for example for metric clustering you can not get better approximation than a factor two  unless p is equal to np so  this is the other interesting kind of a result that  it is not only is it hard to find fast clustering algorithms  which accurately cluster but  even getting approximate clustering algorithms seems to be difficult  because we believe that p is not equal to np most people believe that however  here is some good news if you look at the euclidean tsp  so which is a special case of the metric tsp it turns out that  you can get any approximation factor 1 plus epsilon however  there is a catch and the catches that your running time will also depend upon epsilon so  essentially there is going to be a trade of between the running time and the approximation factor obviously  the closer you want the approximation factor to one  the higher is your running time going to be  so the smaller the epsilon the larger the running time so  this dependence will be captured somehow we can evaluate that dependence and in fact  we can work with a wide range of epsilons and that is  what this result says such results are called approximation schemes and this is  what we are going to study today  refer slide time  03  13  here is a quick definition an algorithm a for a problem p  is said to be a polynomial time approximation scheme abbreviated as ptas  if the following conditions hold first of all  a must now take two arguments of course  it has to take the problem instance and it will return the problem answer but  it will also take in addition a single number epsilon  which is greater than 0 which is going to tell the algorithm  how close an answer we want  how good an answer we want so  a is going to return a solution with approximation ratio one1 plus epsilon the smaller we name this number  the better is our solution going to be but  as mentioned earlier the time taken by a is going to be polynomial and now  it is going to depend on epsilon so  this polynomial we change with epsilon that is the new addition so  the time is going to be polynomial we are going to be able to do this for any epsilon but  the time will be a function of epsilon as well so  it is a scheme in the sense that  you can operate anywhere it is not just one algorithm but in fact  you can think of it as a family of algorithms  defined by different values of epsilon there is also a notion of fully polynomial approximation scheme  which is often abbreviated as fptas and in this case  we require that the time should also be polynomial in 1 over epsilon so  yes the time will increase as you reduce epsilon but  it will only increase polynomial here however  the time can increase and in fact  it can increase much faster than any polynomial so  this is going to be something harder to do or this is going to be something  which is going to be faster for the same epsilon here is  what we are going to today we will describe an fptas or a fully polynomial approximation scheme for the knapsack problem  which we studied earlier the time taken by the scheme is going to be o of n cube upon epsilon so  notice that this is polynomial in the size of the instance n  as always we will use n to denote the instance  the size of the instance it is also polynomial in 1 over epsilon so  it is n cube read it as n cube times 1 over epsilon so  it is dependence on 1 over epsilon is linear so  it satisfies this condition as well so  the time taken is polynomial in n as well as it is polynomial in 1 over epsilon and therefore  we will what we will get is going to be an fptas and so we will be designing an algorithm which will take  which will be an approximation scheme so  it will take an instance as one argument and it will take the number epsilon it will return a solution with approximation ratio 1 plus epsilon and its time taken will be this  refer slide time  06  49  let me quickly remind you  what the knapsack problem is we have actually  we have of course studied it earlier the input consists of an array v 1 though n  v stands for value and v of i is going to be the value of the ith item the second part of the argument is going to be an array w 1 through n so  these are all going to be integers v and w are both going to be integers and w of i again is going to be the weight of the nth item that is going to be a third argument  which is c again an integer and c is going to denote the capacity of a knapsack the output  for the output we have to select a subset of these items 1 through n such that  the total weight is at most c so  think of filling this knapsack but  when we fill the knapsack we should not exceed its weight capacity otherwise  a knapsack will tear or something like that and we are only allowed to pick a subset of these items and furthermore  we want to pick the most valuable subset so  whatever a subset we pick  we add up the values of the elements which we pick and that is the value that we get and we want that value to be as large as possible so  it should be maximum over all possible subsets you have of course  seen this problem earlier we have devised an algorithm for this the algorithm was pseudo polynomial time  as we discuss some time ago and in fact  knapsack is np complete what we are going to do today is  we are going to discuss a new algorithm it is again going to be based on dynamic programming so  we will have a new dynamic programming algorithm it will also be pseudo polynomial time so  that would not really work  for work as an approximation algorithm for one thing  it is not an approximation algorithm it is going to be an exact algorithm but  the reason it would not work is that  it will take pseudo polynomial time whereas  we wanted to take polynomial time so  after that we will describe how we can modify the input instance to this algorithm such that  we get good approximate answers but  we get them fast so  that is going to be the interesting idea the sort of new idea of today ? s lecture is going to be this that will take a pseudo polynomial time algorithm and we will use that but  instead of feeding to it  the exact instance that is given to us  we will feed it a different instance perhaps sort of an approximated instance so that we will get the answers fast although  we will get approximate answers  refer slide time  10  05  so  let me quickly go over the dynamic programming formulation that we have studied earlier  very quickly and  then i will tell you however  new dynamic programming formulation is going to be different here is the old formulation so  what we asked over there was  what is the best value we can get for each knapsack capacity c little c  where c is an integer somewhere between anywhere between 1 and c 1 and capital c so  capital c is the integer given to us as the part of the problem instance so  if you remember the dynamic programming algorithm which we studied long ago  it was doing exactly this for each value of c little c between 1 and capital c  it calculated what is the best value we can get so  basically the question was for a fixed capacity and we take different capacities but  in each individual question the capacity is fixed what is the best possible value or what is the largest possible value  we can get ? the new problem or the new way of looking at this problem  is to ask a different question the question which we are going to ask is  what is the lightest knapsack ? and by the time in  what is the smallest capacity knapsack  which we can use for getting some value v and we will do this for all values of this little v  in the range 1 through v all what is v all ? the v all is the sum of the values of all the items we are looking at this v all  because at most we will be interested in filling all items beyond that  there is nothing of interest  because there are no more items to fill if you figure out  what is the capacity needed to fill all the items  we should be we should really be happy so  this is going to be the kind of question that  we are going to study today and as you can see  it is a sort of the complementary question so  here we ask the largest value for fixed capacity here  we are going to ask the least capacity for the fixed value or the lightest knapsack for the fixed value i want to point out that  if we answer these new questions we will still get a solution to the original problem  which we started off with so  these new questions essentially compute s of v  where s of v is the lightest knapsack for value v that is exactly  what is being computed over here for different values of v and notice that  s of v is an increasing sequence or a non decreasing sequence now  the question that we started off with was  to get the best value for capacity c so  on the phase of it  it might seen that this is the natural question to ask and indeed  it is somewhat more natural but  the point is that even if we answer these questions  we will be able to find this why because  we simply ask what is the largest value v such that  the size required for it is less than or equal to c so  clearly this is going to be the value which we are going to get  had we started out with a knapsack of capacity c in this question so  the idea over here is that  even if we solve all such questions we will be able to solve this question  which we originally started off with you might think that  here we are mobilized to solve many many questions  even though our single question over here is just  there is the single question over here but  if you remember if you go back to the dynamic programming algorithm we looked at  even to answer the single question we really answered all this question so  it is not that we are going to answer more questions  this time well  we are so  we may not necessarily answer more questions over here  we are just going to answer different kind of questions and from those answers as mentioned over here  we will still be able to get the answer to the real question that  we are asking so  from on for the rest of the lecture  i am going to think of these new questions and once we have the answers to these new questions  we will use this idea to return the best value of capacity c  for capacity c so  basically my new instances for each of these questions are going to be characterized  by these three arguments so  we are going to have an argument w the weight  the value v and the target value the target total value that we need and we are going to have such questions  for all such different v ? s and our objective is going to be minimize  the knapsack size so  it is going to be  this kind of questions where we want the lightest knapsack for this fixed value v  refer slide time  15  26  so  how do we solve this problem ? basically  we see that the algorithm is actually the logic behind designing the algorithm is very similar to  what we did for the other for the more natural looking algorithm basically  we need to decide whether it will include item 1  whether it will include item 2  whether it will include item three and so on so  we have a series of decisions to make so  this is what we want we want the optimal solution to the instance w 1 through n v 1 through n and v and let us look at it  as this search space idea so  we consider the search space for this big instance for this instance  which is w of 1 through n v of 1 through n and little v so  this is our search space so  what is contained in it ?  refer slide time  16  25  it contains  knapsack capacities or it contains solutions to such problems so  it contains feasible solutions to such problems now  the feasible solutions to such problems can be of two types so  for example we can have solutions which have value v so  everything over here must have value v but  may be the solution contains item 1 or not so  there could be one set of solutions  which contain item 1 and another set of solutions which do not contain item 1 so  this consists of sets with value v  not containing item 1 and this must contain item 1  sets containing item 1 so  we really want the lightest capacity set  the lightest weight set from this but  we set that this search space has been decomposed into two parts so  we could ask what is the lightest capacity  the lightest set in this ? what is the lightest set in this and we could take the lighter of the two this is precisely  what has been written down over here so  we wanted the optimal solution to the instance w 1 through n  v 1 through n and little v and we can get that  by picking the lighter of the solutions or the solution with the smaller weight  of these two solutions so  the first one is we look at  this set and we pick the lightest solution in that and that is over here we will pick the lightest solution in that and that is over here so  if you remember this is pretty much idea we used  in our original dynamic programming algorithm as well what can we say about these two things ? we can say something rather interesting  refer slide time  18  54  so  this first term over here consist of all solutions to this instance  which do not have item 1 but  what does that mean that just means  that we might as well we asking for solutions to w 2 through n and v 2 through n and v  because we are not using item 1 any way so  this term is in fact  exactly the optimal solution to this instance so  notice that this is interesting this is useful  because we are heading towards a recursive solution so  a solution to this is being expressed in terms of a solution to a smaller problem that is always good news what about this ? we want the lightest solution of value v  having item 1 so  we are looking at this set we know that at this set of sets  this part of the solution space  refer slide time  19  57  we know that  every set over here contains item 1 so  we take that away and what is left so  what is left are sets which do not contain item 1 but  we also know that there value had better be this  v minus the value of 1 why ? because  every set in this part of the space  originally had value little v if they contain an item 1 and if we remove that item 1  then the new value must be exactly v minus v 1 so  what remains in this part of the search space are sets  which do not contain 1 but  whose value is v minus v 1  refer slide time  20  46  so  we take the best amongst those and act to that  the item1 but  what is the best amongst those so  the best amongst those is again an optimal solution to some instance in fact  it is an optimal solution to this instance so  it is an optimal solution to w of 2 through n  v of 2 through n and v minus v 1 this is the knapsack capacity that  this is the value that we are seeking why we seeking this smaller value ? because  we know that we are  at the end we are going to add item 1 to it so  if we add item 1 into it  the total value will become v and which is the value that we want so  therefore we take an optimal solution to this problem instance we take an optimal solution to this problem instance add 1 to it and take add the item 1 to it and take the lighter of these two things so  that is basically the algorithm so  now all that we need to do is  express this as a recurrence well  there is a slight catch so  when we do this  this v minus v 1 could become negative what is that mean i want an optimal solution in which the value is negative  that does not mean anything so  we had better be careful about that so  we must generate this part only if v minus v 1 is greater than or equal to 0  because otherwise this problem instance is undefined so  when we write down our occurrence  we will have to put an explicit check whether v minus v 1 is greater than or equal to 0 or whatever  greater than or less than 0 so  here is our expression in terms of which we are going to define our recurrence so  s i v i am going to define as the least capacity knapsack  which can give the value v using items i through n so  this i defines this i this is the same i and this v defines  the value that we want so  s i v is going to denote the least capacity knapsack  which gives me value v using items i through n so  now i am just going to take this expression that we derived and express it in terms of this kind so  what is this optimal solution to w v v well  this is simply this left hand side is simply s 1 v because  we are starting with i equal to 1  we are allowing all items and we are asking for value v then  we are going to take the lighter of the solutions so  correspondingly we are going to use the minimum over here  the minimum of two solutions what is the first solution ? the first solution is to the instance w 2 n v 2 n little v so  it is going to be the least capacity solution to this instance so  that is as good as saying  it is s 2 v so  that is what we have written down over here the second part is this however  when we write this down  we have to make sure that v minus v 1 is not less than 0 so  let us check that so  i am going to write down a c style expression so  this says that  let us check whether v minus v 1 is greater than or equal to 0 if it is  then the value that we want here is w of 1 plus s of 2 v minus v 1 why is that ? because  we are going to have item 1 always and therefore  we are going to have the weight corresponding to item  always present over here and we are going to add this item 1  into the optimal solution to this problem but  the weight of that optimal solution  the capacity needed for that optimal solution is simply s of 2 we start with items 2 through n and therefore  this is a 2 over here and the value that we are expecting is v minus v 1  exactly this so  if this expression is greater than or equal to 0  then this is the value that we want if this expression is less than 0  this problem is undefined but  what does how do we represent that ? so  that we represent by putting in an infinity so  we are taking the min  this infinity will never be taken and if this second expression is in infinite  then that is as good as saying just give me the first expression so  this is what we have distilled out of this well  it is actually the same thing but  it is now return out more compactly in terms of variables of this kind  subscripted expressions of this kind you must of course  generalize it in order to use it to design an algorithm  we need to generalize it so  here is a generalization so  here we were looking at all the items 1 through n and you can note  you can see that internally we got we needed to have solutions to problems in which  we were having items 2 through n if we did recursion on that  then we would get 3 through to n 4 to n and so on so  therefore we now consider the more general case so  we are going to ask what is the least capacity solution of value v using items i through n so  that is s of i v  that is denoted by s of i v so  analogously we will write down the expression  the more general expression so  here we skip the first item and so we started off with 2 so  similarly here we are going to start off with i plus 1 so  that is the first solution corresponding to this and instead of checking  whether v minus v 1 is greater than or equal to 0 we are simply going to be checking  whether v minus v of i is greater than or equal to 0 if so here we took w 1 plus something here we will take w i plus something again  this something is going to contain items 2 through n over here it will contain items i plus 1 through n over here and so we will have i plus 1 over here this was the value required  was v minus v 1 here the value require is v minus v i  because we are adding an item i later on anyway in case we want v minus v i that is the value of this part and then to it  we add item 1 so  the weight solution value of this whole thing together will be w of i plus s of i plus 1 v of v minus i  as before and of course  if v minus v i is less than 0  then we do not want this entire term to be taken into account at all and therefore  otherwise we are going to put down infinity so  this is a defining recurrence that we are going to use so  let us see how this recurrence can be solved  refer slide time  28  41  so  as usual we are going to be keeping a table so  i have just written out that recurrence again so  let us see what kind of table  we can use for this so  here is the table this is the i axis going down vertically this is the v axis so  earlier i said that v really needs to start from 1 but  it is useful to have a 0 here  as well so  have started it off from 0 and as before it goes to v all so  since we are going to have  since i want to show you what is recurrence means in this table ? i have put down some specific  i have marked out from specific entries so  let us ask how this recurrence will work out in this table so  i want to compute s of i v  which is this entry in this table ith column vth row  ith row vth column now  this entry depends upon which entries well  it depends upon this entry and it depends upon this entry it does not  we just do not have to look at this entry we have to do something to it but  this other thing that we have to do  we know what the value of w i is so  it really depends upon this entry and this entry so  which are these two entries ? so  this entry is this entry and this entry is this entry of course  if v minus v i was less than 0  then this entry would fall outside the table and so we would only have a single entry over here but  this is the more common more interesting case so  to fill this entry  it is sufficient if we have this entry and this entry filled that is  all coming out of this recurrence which we have written down over here well  that suggests a way of filling in this table so  we sort of fill in going bottom up or bottom right from we start of at the bottom and go upwards  but we also need this and so therefore  we also have to start from the left side so  to do that we would need to have these entries filled how do we fill  this entries ? well  let us interpret what these entries are so  this entry in general is going to be s i 0  for different values of i and let me remind you  what s i 0 is s i 0 is the capacity  the minimum capacity needed to get a value of 0 using items i through n that does not seem to difficult does it  we just want to get a value of 0 so  what is the minimum capacity  we need well trivially  the answer is 0 capacity so  if we get capacity  if we get a knapsack of capacity 0 we can certainly get value of 0 by filling nothing into it and clearly there is no smaller knapsack that  we can use  because this is the smallest possible knapsack so  this entire yellow column just needs to be filled with 0s and that something  we can do without any computation so  that leaves open the question of  how do we fill this row so  let us try to interpret what this row is so  this row in general is going to be s n v  which is denoted  which is denoting the capacity needed to get value v  the least capacity needed to get value v using item n alone so  you are just allowed to pick item n nothing else so  what kind of capacities can you get ? what kind of values can you get ? well clearly  if this v happens to be v of n  then you can do that using just a single item so  that would been that if v happens to be v of n  then the least capacity knapsack that you would need  would have to have capacity of w of n so  one of these entries can be filled using this what about the rest of the entries ? suppose we want to do this for a different vale of v so  let us say we want to get a value larger than v of n  can we do it we are only use to  we are only allowed to use the item n so  clearly we can not do it we can not get a value either bigger or larger than v of n  by using only item n so  then what we do so  that can be represented quite nicely  by saying that by putting in infinity  in these entries so  v of v n  so if v is not equal to v n  then we can not get that value and so we will say that a capacity knapsack of  capacity infinite is needed let me explain  why this works basically later on  we are going to take things like min of this or min of this or something like that so  if there are infinities over here  then that value is essentially going to be ignored or if both of these values are infinites and if you take min of those infinities  then an infinity will crop up over here but  says that even this value is impossible to accomplish so  that is sort of a nice thing that is a kind of a nice coding that  infinity allows us to accomplish so  basically now we have express the algorithm entirely so  we have entries to fill and there are sort of three kinds of entries to fill we have these yellow entries to fill we have these blue entries to fill and  then there are the rest of the entries which we fill according to this recurrence  refer slide time  34  46  so  we can write done our algorithm so  i am going to call my algorithm ks as knapsack it is going to take as argument so  that brings us to our algorithm we will call this ks for knapsack it takes as arguments  the value  the weight of all the n item and the capacity so  we are going to solve  we are going to find an answer to this but  we are going to find an answer using our new formulation so  as mentioned earlier so  there were those yellow entries to be filled they were all going to be filled with 0 so  we will do that so  these entries using this equation then  there were the blue entries to be filled the blue entry is where that  for if v is equal to v of n  then the knapsack capacity needed was w of n otherwise  the knapsack capacity needed was infinity so  this is how you fill the bottom row so  this is how we filled and this was the equation used finally  the rest of the entries were filled using the recurrence which we derived so  this is that recurrence at the end of it  to get the value for the capacity c  we needed to find as we described earlier the largest v such that  s v 1 is less than c so  what is s v 1 so  we look at all possible v ? s so  this is column 1 and we know that  as we go as we go down this is going to be non decreasing so  we can easily find the largest v such that  s v 1 is less than c and that is the value  which we will call v star so  that is the value that we are going to get if v in fact  had a capacity of c so  if you are just interested in the value  then we would be done at this point however  if we wanted the items to be returned as well  then we can do that also you remember  how we did that for the older algorithm we just had to keep track of some additional pointer  some additional data structures basically  we can do that as well so  corresponding to every entry of the table  we can determine what the corresponding set is going to be so  we can also return the set of items so  how long does it take just to compute everything from here  until v star ? well these are all  this loop will take time o of n this will take time o of v all and here  we have nested loops and therefore  it will take time o of n times v all in fact  if you compute the set itself  we will have to keep some additional data structures but  as explained in the previous algorithm we can use exactly the same ideas to do the  to compute the set as well in exactly the same amount of time well  to within constant factors so  in o of n times v all time  we can not only compute v star but  we can also compute the corresponding set so  this entire problem can be solved n time o of n times v all so  that finishes the first task that we undertook so  we now have a dynamic programming algorithm  which finishes in time n times v all so  now we come to the approximate algorithm here is the important point  refer slide time  38  54  so  here we are going to be allowed an error in the answer we are only required to get within 1 plus epsilon so  epsilon is sort of the error we are allowed the point is that  if we are allowed an error  then it means that we can calculate using low precision so  basically we are going to calculate using low precision and that is going to allow  us to reduce the time so  we are going to write a new procedure  a new algorithm which we are going to call an approximate case approximate knapsack or aks it is going to take the same arguments as before but  it is also going to take an argument called delta  which is somehow going to reflect the precision and we will tight up to this epsilon later on so  delta and epsilon will be related how they will be related  we will specify pretty soon here is the algorithm so  we are going to define a new array v prime so  v prime is simply going to be v divided by delta corresponding elements so  every element here is going to be divided by delta and we get the corresponding element of here but  we want v to be integer as well so  if we get a fraction which we will in general do we will take the floor we will take the largest integer less than or the floor and  then so essentially we are scaling down the v values by a factor delta we will call ks using those scale down values but  the scale down values are not really going to be important for the final answer so  for the final answer we want to return delta times whatever  scale down values we got so  this is expected to do roughly the same job we scale the values down  we got a good solution and  then we return we scale the values up  the only catch is that here we took the floor so  this will produce some error at the same time sense  the answer the time taken for this case is proportional to the second argument the time over here is going to be v prime rather than v so  that is where we are going to say on time  as well so  we will say on time at a cost of some error so  what remains now is to analyze all these let us say  that s denotes the set returned by our original ks call so  where we were using the actual values of v as given ? so  this is truly the optimal set which was return and its value x is the actual optimal value and i can think of this as the full precision problem or the full precision answer as well s prime is the set returned for this problem this by  this case without multiplication by delta for the minute let us say its value is x prime and this is a low precision answer so  what we now need to do is to relate this x prime  that we so x prime and x and so on so  let us just do that so  i observe first  that x prime is the value of the sets over here  value of the set over here and what is that so  it is the values of all the elements in the set so  v prime of i  because this time we had passed v prime  where i belongs to this s now  this s prime  now here is the important point this x prime  this value is bigger than this expression as well notice that the only difference between these two things is that  instead of choosing s prime i am choosing s so  this was the optimal set which was returned this is not the optimal set but  it is some difference set so  what happens if it is a difference set ? so  its value need not be as big as the value of this so  in other words this value could be has to be at least as big as this value but  notice that this s was an acceptable solution to this and in going from here to here  we have not changed the weights so  this set is also an acceptable solution to this so  therefore we can clearly say that this value had better be no smaller than this value because  this is also feasible solution to the v prime problem as well so  s is also a feasible solution and therefore  we get this now finally  we observe that if i take the floor of any number  it is bigger than that number minus 1 and therefore  this is the floor of this and therefore  this is bigger than this minus 1 so  v prime of i is the floor of this and therefore  v prime of i is bigger than v of i upon delta minus 1 because of this so  we have been able to relate x prime to this quantity over here so  the point is that we are getting towards the optimal set somehow we want to relate it to the value of the optimal solution so  let us now ask what is the value that aks will return ? aks returns  delta times this value correct  delta times this value this is what we do over here and what is that so  delta times that value well  for x prime i am going to substitute this so  i am going to get this  multiplied by delta so  if i multiply by delta  this delta is going to get cancelled out so  i get a v minus i and instead of this one  i am going to get a delta  because i multiplied by delta so  that explains this part now  what is summation over s of v i that simply x so  i get an x over here and there was a minus delta but  it was also in the sum so  my delta is a constant delta does not vary depending upon  which element of the set i am considering so  i will get minus delta as many times as the cardinality of s so  i am going to get x minus cardinality of s times delta so  here is an important observation we have proved  that the value returned approximately is at least the actual value minus cardinality of s times delta  refer slide time  45  55  so  now we are ready to evaluate the approximation ratio what is the approximation ratio ? so  it is on the value of the returned value  upon the value of the value returned by the approximate solution remember  that this problem is a maximization problem therefore  the optimal solution has the largest possible value and in that case  we define our approximation ratio as the optimal solution upon the approximate solution so  that is exactly what we are doing over here so  we want the approximation ratio of x upon y  refer slide time  46  46  so  let us collect the equality inequalities which we had over there so  we had one inequality which is y is greater than or equal to x minus cardinality s times delta so  this is one inequality and i can write this as x is less than or equal to y plus cardinality of s times delta if i divide the whole thing by y  i am going to get x upon y is less than or equal to 1 plus cardinality of s times delta upon y this is what we are going to get so  this is what we finally have over here  refer slide time  47  34  what is the time taken by aks ? so  it is n times v all v prime all or v prime all is essentially v all by delta so  it is this time so  notice that the time over the exact evaluation has reduced by a factor delta so  all that remains now is to choose delta carefully now  here is the clever choice we are going to choose delta equal to epsilon times v all upon n square so  let us just do that  refer slide time  48  15  so  we choose delta equal to epsilon times v all upon n squared so  what does that do for us ? so  first of all this was our approximation ratio so  if we substitute into that  what do we get ?  refer slide time  48  29  so  we substitute delta upon y over here so  we get epsilon v all and  then this v of the y remains as it is from here and we get an n squared over here so  this is the approximation ratio so  what becomes to this ? what becomes of this  i claim that this becomes at most 1 plus epsilon why ? so  let us see that the first observation is that  the cardinality of s which appears over here has to be less than n after all  what is s ? it is a subset of n element so  its cardinality has of course  to be less than n so  that is one important observation then  second v all is the sum of all the elements so  clearly it has to be less than v max times n so  if v max denotes the maximum value if i multiplied by n  i certainly should get something which is bigger than just the mere some of values so  this is what i get and i claim that  v max can not be bigger than y so  the maximum item had better be accommodatable in my knapsack otherwise  i would not have considered it in my list in the first place and therefore  the optimal solution had better include this largest item and therefore  v max times n has to be at most y times n so  it follows from this  that s times v all is less than y times n squared so  this s times this v all is less than y times n squared and therefore  our approximation ratio is 1 plus epsilon i also claim that the time taken is n cube upon epsilon and this is much easier to see so  time taken is n times v all upon delta so  i just substitute and i get instead of delta i substitute this so  this v all cancels and  then i get an n squared here so  i get n cube upon epsilon so  what has happened ? we have shown that  our approximation ratio is 1 plus epsilon so  no matter what epsilon you give me i will be able to get this time  if i choose delta equal to this in my procedure and furthermore  my time taken is going to be n cube upon epsilon so  which is exactly what we have promised ? we had promised to get an approximation scheme  in which the approximation ratio is 1 plus epsilon for every epsilon and that we would prove that  the time taken is polynomial in n the input instance length and 1 over epsilon that is what we have done so  that concludes the main part of the lecture i just want to make a few remarks  refer slide time  51  18  our fptas had time of n cube upon epsilon it is possible to device another fptas  with a different expression n log 1 over epsilon plus 1 over epsilon to the 4 so  it is very likely that in practice  this approximation in many practical situations  this might be a better algorithm than this but  this is of course  more complicated as well and we are not going to look at it now  here is an interesting fact that such as  that somehow that knapsack problem is actually among the easier np complete problems  in the following sense knapsack problems  in which weights and values are drawn uniformly at random from the interval 0 through 1  can be shown to take polynomial time on the average so  in some sense the fptas result says that  it is easier to approximate and this says that  it is also easier on the average in fact  its polynomial in the average finally  i just want to say that many problems having pseudo polynomial time algorithms even if there np complete can be shown to have a ptas or even an fptas one example is the sub another example is the subset sub problem thank you 