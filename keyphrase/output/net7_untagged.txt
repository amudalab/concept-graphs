computer networks prof s ghosh dept of computer science & engineering iit kharagpur lecture -31 tcp  start time 00.42  good day  our topic today is tcp tcp is the second most important transport protocol  slide time 01  13  01  44  tcp is very widely used by many applications we have already discussed udp actually this is a little more complex then udp but it also has some advantages we will see what they are so the transport layer responsibilities are   slide time 01  13  01  44  transport layer creates packets from byte steam received from the application layer and in order to multiplex and de-multiplex amongst various applications it uses port numbers to create process-to-process communication it uses a sliding window protocol to achieve flow control and uses acknowledgement packets  time out and retransmission to achieve error control so  unlike udp which is unreliable this seeks to provide a reliable communication that means it is error free  it is a connection oriented protocol and it also has some kind of congestion control mechanism and it does the basic thing of connecting between processors amongst two distant nodes possible  slide time  02  54-03  02  tcp is called connection-oriented reliable transport protocol it adds connection oriented and reliability features to the services of ip ip as such is best effort kind of service so it does not give you any reliability secondly ip is connectionless whereas tcp tries to give some kind of connection-oriented flavor to the connection and proposal is implemented entirely at the two end nodes that means the intermediate routers etc do no have any role to play  slide time  03  35-04  07  the communication abstraction is that  it is reliable and ordered ordered means the packets are received in order in ip it is not guaranteed that the packets will arrive in order because what might happen is that one packet may be routed in one direction and the other packet may be routed in other direction so the packet which was sent first may land up at the destination earlier and vice versa but tcp makes them ordered tcp brings an order amongst them  slide time  04  08 ? 04  22  tcp is point-to-point and unicast all these features namely it is ordered  point-to-point  reliable and unicast is what gives the connection-oriented flavor to tcp it takes the byte-stream as an input and gives that as the output it is a full loop-less connection that means both the nodes a and b are connected by tcp then a can communicate to b and b can communicate to a and it has flow and congestion control  slide time  04  22-04  32  tcp  transmission control protocol  is a connection-oriented protocol  reliable  unicast  end-to-end  byte-stream over an unreliable network  slide time  04  58-05  41  so  before any data transfer tcp establishes a connection just like in a connection-oriented network like classical telephones  first there is a connection which is set up by all kinds of control signals similarly  like in atm  first a connection is set up and then the actual data transmission begins similarly  in tcp a connection is set up between the two end points before any transmission takes place  so it sets up a connection  slide time  05  42 ? 06  09  one tcp entity is waiting for a connection that is the server the other tcp entity-client contacts the server the actual procedure for setting up connection is more complex the client first makes a request for a connection and then the server acknowledges the request and accepts it  then the data transfer begins so there is a connection set up before actual data transfer can take place and at the end of the data transfer there is a disconnect phase also  slide time  07  08 ? 07  34  it is reliable the byte-stream is broken up into chunks which are called segments therefore at the tcp level they are called segments the receiver sends acknowledgements for segments so this is the basic mechanism by which reliability is brought in that each and every segment is acknowledged tcp maintains a timer if an ack is not received in time the segment is retransmitted so this is the basic mechanism for reliability the sender sends a packet and it waits for some acknowledgement and if the acknowledgment does not come through from receiver then the sender assumes that the packet has been lost and sends the packet again hoping that this packet will eventually reach the receiver and the receiver will send the acknowledgement this might also lead to duplicate packets at the destination for example  the sender has sent the packet and the receiver has received it and it has sent an acknowledgement but then the acknowledgement got lost so naturally the original sender did not receive any acknowledgement and after sometime it will send it again so a duplicate segment would be a received by the receiver but then it knows that it is a duplicate and will eliminate that so  by this way it achieves reliability  slide time  07  34 ? 07  59  tcp can detect errors tcp has checksum for header and data like udp segment with invalid checksums are discarded each byte that is transmitted also has a sequence number so  if some intermediate sequence number is missing the receiver knows that something has been lost  slide time  07  59 ? 08  32  to the lower layers tcp handles data in blocks  the segments this is where the packets actually originate but to the higher layer tcp handles data as a sequence of bytes and does not identify boundaries between bytes so  the higher layers do not know about the beginning and end of segments hence to the higher layer it is just a stream of bytes  slide time  08  33 ? 09  02  for example  the application writes 100 bytes then it writes 20 bytes so all these go into the queue so the queues are bytes to be transmitted and then tcp transmits them at the other end also there is a queue of bytes and at the other end also it reads 40 bytes at each go so  to this application this is the byte-stream and for this application also this is just a stream of bytes coming in such units  slide time  09  03 ? 09  37  the unit of data transfer between devices using tcp is a segment it is 20 to 60 bytes header followed by data it is a 20 byte header if there are no options and up to 60 bytes if it contains some options so there can be up to 40 bytes of option so the header naturally contains some field which allows the tcp protocol to run now let us look into the details of this header  slide time  09  37 ? 09  02  the header has 16 bit source port address and 16 bit destination port address it also has a sequence number  acknowledgement number  header length  hlen   reserved bits  some flags and then there are some options like window size  checksum  urgent pointer  options and padding  slide time  10  21 10  30  so there is a source port address in the client-server processes actually communicate through ports and the port and the ip address together form the socket which uniquely identifies every session when a tcp session is going on  on both the sides two port numbers are assigned for standard applications the first communication will start on well-known port number then they will switch to ephemeral port numbers  slide time  10  31 ? 10  51  then there is a source port address which is a 16 bit address that defines the port number of the application program on the host that is sending the segment  slide time  10  51 ? 11  15  similarly there is destination port address which is again 16 bits the port number is from 1000 to 65000 and those are the ephemeral port numbers destination port address defines the port number of the application program on the host that is receiving the segment  slide time  12  50 -12  55  there is the sequence number which is a 32 bit number it defines the number assigned to the first byte of data contained in the segment during connection establishment a random number generator is used to create an initial sequence number  isn   the field for the sequence number has been kept quite big  it is a 32 bit and is kept with a reason if the number of bits for the sequence number was small then what could happen is that  when a particular session starts it sends the number and then it would go back to the beginning for any finite length sequence number after some point it is going to go back now  if it goes back and starts these numbers once again for a very small segment size what might happen is that first of all the two packets may get the same segment number which are on the network at the same time and similarly other kinds of confusion might arise so what is done is that  actually a large sequence number is generated so that even if it comes back to a low number we can know that which one came earlier and which one came later if after receiving very high numbers if you start getting some low sequence numbers you know that it has looped back actually it may not be a strict loop  slide time  13  27 -14  28  therefore this is the range of sequence numbers so 232 is about 4  ? lian  which is a large number and random number is used to generate the initial sequence number this initial sequence is expected in a large range so it is not going to clash and this isn is exchanged between the two nodes  slide time  14  29 ? 14  44  there is an acknowledgment number which is 32 bit just as the segments which have been sent they have a sequence number similarly these segments as they arrive on the other side will get an acknowledgement so it is one acknowledgment after the other so a stream of acknowledgements will come actually in the best of circumstances there will be as many acknowledged numbers as there are segments which are sent so this acknowledged number will start somewhere and that would be communicated by the receiver to the sender in the connection set up phase which is also a 32 bit number that is large it defines the byte that the receiver of the segment is expecting to receive form the other party this is standard norm for the sliding window protocol  slide time  14  53-14  57  the next field in the header is the header length which is of 4 bit this indicated the number of 4 byte word in the tcp header this is required because the header is of variable length the header could be anything from 20 bytes to 60 bytes depending on the options so the header length is to be specified and then there are some reserved bits for future use  slide time  14  58 -15  51   slide time  15  51 ? 16  17  followed by some flags it defines 6 controls flags urg defines the urgent pointer which determines whether the urgent pointer is valid or not if the urgent pointer is not valid the urgent pointer field itself may contain some garbage ack tells whether acknowledgement is valid or not psh is for request for push rst is for resetting the connection syn is to synchronize sequence number and fin is to terminate these connections these flags are used for setting up and termination of connections  slide time  16  17 -16  35  there is a window size of 16 bit which defines the size of the window in bytes that the other party must maintain it is sort of controlled by a receiver the receiver gives the window size which is of the sliding window protocol and then there is a checksum which is of 16 bits the checksum is like ubp  slide time  16  36 ? 16  58  the urgent pointer is 16 bits and is valid only if the urgent flag is set it defines the number to be added to the sequence number to determine the last urgent byte in the data section of the segment  slide time  16  55 17  02  options are up to 40 bytes options could be a single-byte or multiple-byte multiple-byte in options may contain maximum segment size  windows scale factor and timestamp etc in single-byte there are end of options and no operation for padding purpose  slide time  17  03 -17  24  end of option is used for padding at the end of the option field and no operation is used as filler between the options  slide time  17  24 17  51  if there are options on the other hand then one option is the maximum segment size which defines the size of the biggest chunk of data that can be received by the destination of the tcp segment this is not the segment but the size of the data which is taking the header field out  slide time  18  53 -19  00  the window scale factor defines the size of the slicing window and how it is changed timestamp is filled by source when segment leaves and destination returns it in the echo reply field this allows the source to determine the round trip time some estimation about the round trip time is very essential because suppose a particular segment has been sent then the sender is expecting an acknowledgement if the acknowledgement does not arrive on time  then how long would the sender wait for the acknowledgement ? so you have to make an estimate and that estimate is based on the round trip delay  the maximum segment life so it has to wait that much or may be some more and only then the sender would sort of come to the conclusion that may be the original packet is lost or may be the acknowledgement is lost but in either case the packet or the segment that was send it has to be retransmitted  slide time  19  01 ? 19  24  there is a checksum  it has the same calculation as udp and inclusion is mandatory with tcp  slide time  20  21 ? 21  10  setting up connections and resetting of connections  tcp is a connection-oriented protocol so  a virtual path between the source and the destination has to be established this is only a virtual path because remember that tcp is working on ip that means an ip is a connectionless service so the underlying actual service is actually connectionless but tcp gives a feeling to the upper layer  this application layer that  as if it is a connection-oriented thing that means all good things about connection orientation like segments  arriving in order and reliability etc is present but tcp has to work on a connectionless ip network so this path established from source to the destination is only a virtual path unlike traditional strict connection-oriented service where the connection may be physical  slide time  21  10 -21  21  for connection establishment four actions are required before sending data and here two of them may be combined so it can even be three actions first host a sends a segment  if host a is the client and it wants to establish a connection to the server may be a host b  so host a sends a segment to announce its wish for connection and includes its initialization information and then host b sends a segment to acknowledge the request of a here host b sends back an acknowledgement  slide time  21  21-21  24  then host b sends a segment that includes the initialization information so here the second and third steps can be combined  slide time  21  24 21  47  that means the host b can acknowledge the request of host a as well as in the same acknowledgement it might send the initialization information meaning the sequence number etc have to be exchanged  slide time  21  47 24  09  then host a sends a segment to acknowledge the request from b second and third can be combined which is called as the three-way handshaking  slide time  24  09-24  30  for the first syn  that is  for synchronization the sender sends a synchronization which is a request to set up a connection to the host and it also gives initial sequence number the receiver then sends a syn  its own syn the sequence number which is something like 4800 and an acknowledgement of 1200 this segment which is the segment 1 already consumes the first sequence number 1200 so  while acknowledging it sends the value 1201 meaning that 1201 is the next segment or next segment number that the receiver is expecting what might happen is that  this packet may get lost because once again the tcp is sitting on a best effort kind of ip service so it may get lost therefore what would happen to the sender is that  the sender will not get any acknowledgement so after some time it will send the syn pack again if it is persistent and then finally it will get the syn and the acknowledgement similarly  if this is lost once again this is sent after sometime and finally this is achieved and then the sender sends the sequence number 1201 and acknowledgement 4801 so this sequence number is always the next number that is expected from the other side when it sends sequence number1200 it replies back saying that next it is expecting 1201 and its starting number is 4800 so he replies in his acknowledgement that the next acknowledgment he is expecting is 4801 so this is the three-way handshake where the syn and the ack has been combined and then the data transmission can start  slide time  24  53-24  58  on the other side we have to think about the connection termination to terminate the connection either party can close the connection if connection is terminated in one direction data can continue to be sent in the other direction remember that this is the full loop-less communication which means a is communicating to b and b is communicating to a at the same time even if only one side is sending data to the other side the acknowledgement is coming from the other side anyway now  the connection can be terminated from the both sides but then somebody has to initialize the termination  slide time  24  58 ? 25  22  so  if connection is terminated in one direction data can be continued to be sent in the other direction  slide time  25  27 ? 25  34  four actions are required to close the connection in both directions first  host a sends a segment announcing connection termination this means it sends the segments containing fin  slide time  25  34-25  44  host b sends a segment acknowledging the request from a and after this the connection is closed in one direction  slide time  25  44-25  50  when host b has finished sending data it sends a segment indicating connection closure  slide time  25  51-25  55  here  the second step and the third step can not be combined together  slide time  25.55 ? 26  10   slide time  26  11-26  24  although we are sort of allowing the termination of connection for one side but on the other side it may have acknowledgements or other things to send to this side so it will not terminate the connection therefore these two can not be combined together  slide time  26  24 29  32  the third step can be taken only when host b has finished sending data from its side and it sends a segment indicating a connection closure host a acknowledges the request from b so this is called a four-way handshaking  slide time  29  32-29  36  this is the diagram showing four-way handshaking for example  assume that the sender has sent a fin in a segment 2500 and when it receives the fin if everything is all right he will send back an ack and he say he will say that the next one he is expecting from the other side is sequence 2501 and sequence 7000 which is the acknowledgement for this fin after sometime when b has finished sending all its acknowledgements and other things it might want to send to the sender then it is sends a fin from this side so  this is for closing the connection from the other side the acknowledgement that is expected once again is 2501 and this is still 2501 because nothing else has arrived from the other side and then it takes the next segment number 7001 from this side and he acknowledges this second fin when the acknowledgement reaches here so everything is closed gracefully of course things may not run so well because one or more of these packets may get lost on the way because we know that the underlying network is unreliable if the first fin is missed he does not get any acknowledgement and after sometime he will send the fin again similarly  if this ack is not received then he will send this fin once again anyway so this fin may be lost since he does not get any acknowledgement he will send the fin after sometime the trouble is over here because at this point of time after sending this acknowledgement the sender will assume that everything is fine so he will close but this last acknowledgement may get lost as all the fins have been sent and received and the acknowledgement has been received and sent he will say that it is the end of the story but for him it is not end of the story because this ack is now lost but if this fellow has already completed he will not send any ack any longer so what he will have to do is that after sometime he will have to close the connection  slide time  29  36-29  42  actually the tcp goes through a state diagram  slide time  29  42-31  11  this is the state diagram and these are the different states  slide time  31  11-32  21  one is of course closed that means no connection is active or pending the other is listening may be the server is waiting for an incoming call syn rcvd means somebody has made a request  a connection request has arrived and is waiting for acknowledgement syn sent  the client has started to open a connection established means a connection has been established and that is the normal data transfer state fin wait 1 means client said it is finished fin wait 2 means server has agreed to release timed wait means wait for pending packets this is the 2 msl wait state so for the last pending packet to come in you have to wait this much this is the maximum segment and this is sort of estimated from the round trip time closing means both sides have tried to close simultaneously close wait means server has initiated a release and a last ack is waiting for the pending packets  slide time  32  21-33  05  when it is closed nothing is happening now some syn or syn plus ack it receives and once it receives that then it goes to the syn rcvd state as explained earlier this is the connection set up phase and once the connection is set up then it reaches the established phase and then the two way communication is going on then  after this  it goes through a closure either through the closing and then fin wait 1 and fin wait 2  time wait etc and here it is the close width and the last ack  slide time  33  05-33  17  the flow control is implemented by tcp this defines the amount of data a source can send before receiving an acknowledgment from the destination tcp uses a window imposed on the buffer of data to limit the amount of data sent before an acknowledgement must be received this is the traditional sliding window protocol  slide time  33  18 -34  21  in the sliding window protocol a sliding window is used to make transmission more efficient as well as to control the flow of data so that the destination does not become overwhelmed with data  slide time  34  23-35  01  in sender buffer there is the occupied part of the buffer and out of this some number or packets or segments have been sent but they have not been acknowledged and these are the next byte to be sent which is available in the buffer now  depending on the windows size it can go on sending these bytes or if the window size is already over then it has to wait here let us assume that the size of the window is three units  in that case the sender has to wait here till some acknowledgment comes and if the acknowledgment comes suppose 200 is acknowledged then the window will automatically slide and this now can be sent  slide time  35  02-35  22  at the receiver side it has received 194 to 199 this is the occupied part of the buffer and from this buffer the destination process will keep on consuming the data from these segments as q stream of bytes going out of this segment here  this is the empty part of the buffer where new segments can come in  slide time  37  48-39  55  suppose the size of the receiver window is 7 then after having sent this the sender can go on sending up to the receiver window size the point is that  if a window size is not fixed what the sender might do is that  the sender may send in a lot of packets or may be the channel is absolutely bad  in that case none of the packets have gone to the other side so you will have to send all these packets once again  and that is one point and secondly the sender may be very fast but the network may be congested so the sender will push in a lot of data but that will only make the congestion worse so whatever would have been received on the other side does not reach that side because the intervening network has been congested  or may be some buffer has overflowed or some router has dropped some packets etc so there is a limit on the window size the window size also has a bearing on the speed at which effective data transfer is taking place if the window size is very small  suppose 2  then after sending two segments the sender has to wait till the acknowledgment comes back so the efficiency of the channel goes down because for each acknowledgement the packet will reach the other side and then the acknowledgment will travel all the way back so it is a round trip delay now suppose the window size is only 1 so after sending each segment or each byte you wait for the round trip time and then you send the next byte so now the overhead has become very high and now this is the minimum possible rate at witch the sender is sending on the other hand if the window is very large a large amount can be sent even without waiting for acknowledgement  slide time  39  55 ? 39  58  now when everything is fine the stream of acknowledgements will also start coming in after this round trip delay and the overall efficiency would be high therefore between congestion and the efficiency at which the data can be transferred that is the effective data transfer speed there is a trade-off suppose if these are the sequence numbers and now the receiver side may be 205 has dropped somewhere and 206  207  208  209 etc has come so after acknowledging 204 where the acknowledgement packet will tell that the next byte its waiting for is 205 although 206  207  208  209 etc have been received but he will not send any acknowledgement so after sometime the sender will realize that this acknowledgment for 205 has not come so he will start pumping 205  206  207  208 and 209 again now as soon as 205 arrive what happens is that this gap is closed and the window will slide all the way here and he will say next byte that is expected is 210 then what will happen is  206  207  208  209 now becomes duplicated segments on the receiver side so they are simply dropped using the segment numbers not only we can put them in order  suppose 206 comes earlier and 205 has come but has come later but then they can be put in proper sequence because sequence numbers are there and secondly even this 205 was lost it can still be recovered by this retransmission etc so that is how this protocol is reliable and when this is acknowledged the window slides  slide time  39  59-40  11  so  this is the size of the receiving window  slide time  41  17-41  31  sometimes what happens is that  if there is actual congestion in the network  you will find that only after sending some packets and seeing that some acknowledgements have not come and the packets have got lost so  the sender thinks  one thing is  this packet may have got lost and secondly  may be the network is congested so what will happen is that  there is a mechanism for automatically reducing the window size so that now you are going to transfer data at a much lower rate but then you are sort of trying to control the congestion so this is the congestion control that is in built into tcp there are some variations as to how this window size is going to be changed over time etc so we will come to later on we when will talk in more details about congestion control  slide time  41  32-41  51  in tcp the sender window size is totally controlled by the receiver window value  the number of empty locations in the receiver buffer however  the actual window size can be smaller if there is congestion in the network  slide time  41  51 42  15  the source does not have to send a full window ? s worth of data when you have limited data then you will send only that data which is available the size of the window can be increased or decreased by the destination the destination can send an acknowledgment at any time  slide time  42  18 -43  50  error control  the tcp is a reliable transport protocol tcp delivers the entire stream to the application program on the other end in order without error and without any part lost or duplicated so tcp provides reliability using error control  slide time  43  51 ? 44  01  for error control there are the different things that tcp would do it detects corrupt segments  by detecting lost segments  by detecting out of order segments  by detecting duplicated segments and by correcting errors after they are detected detecting corrupt segments is  when checksum would be wrong in that case you can drop it or if you can correct the error using the checksum then you may correct it also detecting lost segments is achieved by getting the help of the sequence number when you have the sequence numbers and when one particular segment in between is lost you know that the segment number is lost because that particular segment number would be missing detecting segments that are out of order is  when the segments are in out of order we have got the sequence number and so we put them in order you can detect and correct it at the same time detecting duplicated segments  if a particular segment has come twice  may be its acknowledgement is lost or delayed  in that case once again by the serial number we can see that it is duplicated and we can drop it  slide time  44  22 ? 44  39  error detection is achieved by three simple tools  checksum  acknowledgement and time-out suppose you had sent the first segment and the acknowledgement never came then after sometime you have to give a time-out and that is how the sender detects that this segment has been lost and it has to be re-transmitted  slide time  44  38 -44  52  checksum  acknowledgement and time-out are the three things together offer reliability and error detection and correction capability of tcp  slide time  44  52-48  05  for error correction when time-out counter expires segment is considered to be corrupted or lost and the segment is retransmitted  slide time  48  05 ? 49  35  tcp timers  tcp has to maintain a number of timers inside tcp has to do a lot of work to give that reliability which is necessary on this unreliable ip service that is why this protocol is a little more complex than others but then at the same time this is necessary for example  when you are sending a file transfer protocol for example  you are transferring a file from one machine to another then even the misplacing of few bits will make the whole file useless so there are applications and specifically protocols like ftp and actp etc use the tcp protocol because there is reliability of the connection and absolute error control  it is error free  and the nature of communication is very high then there may be applications where a loss of few bits or bytes here and there does not really matter for example  suppose we are carrying a voice communication  now if you are carrying voice communication what would happen is that even if a few bits here and there are lost the quality is not impaired that much of course voice or other multimedia kind of communication is very sensitive to other kinds of network parameters but the point is that  even if few bits are lost here and there then the other person will also be able to make out so in this case  on the other hand if there is a delay and differing rate of transmission because if the window size is large the rate at which you can transmit is high and if the window size is small the rate at which you can transmit is small so  if the rate at which transmission is taking place keeps on varying or takes a lot of time etc is not acceptable for voice communication so those are the cases where we will not use tcp there are cases where we definitely want to use tcp because of its reliability and then there are cases where we do not want to use tcp so ftp and http etc are examples where tcp is necessary and we use them tcp uses four timers  retransmission timer  persistence timer  keep-alive timer and time-waited timer  slide time  49  35 -51  56  retransmission timer  used to control a lost or discarded segment suppose a segment is lost  now  after what time will it be retransmitted ? so in this retransmission timer using the timer stamp you can get an idea of the round trip delay and this you will use to set your retransmission timer and there will be a buffer because it is not constant by this way you can set up your retransmission timer as soon as the segment is sent immediately the retransmission timer starts when the next segment is also sent because your window size happened to be more than 1 then you will need another retransmission timer for this segment so  for each segment we are maintaining this retransmission timer and as soon as one of these timers is timed-out it means that the packet was sent and the acknowledgement was not received therefore  immediately you have to retransmit that packet so this is the retransmission timer tcp creates a retransmission timer when it sends a segment if an acknowledgement is received before the timer goes off the timer is destroyed because this timer is not required for each new segment you start a new retransmission timer  slide time  51  57-52  03  if the timer goes off before the acknowledgement arrives  that means the acknowledgement has not been received before the estimated time then the segment is retransmitted and the timer is reset now this timer will be reset because even though you retransmit the retransmitted segment may also get lost so you have to start the retransmission timer once again and so tcp uses a dynamic retransmission time which is different for each connection and may be changed during the connection tcp is a protocol and there may be a lot of connections going through this tcp protocol just think a simple case of a web server now there may be a number of people for very active websites and a number of people may be hitting it at almost the same time so if you have a concurrent server and you have spawn so many processes on the application side and each of the server is sort of serving one particular client and a number of them so a number of tcp services are going on at the same time but of course they are using different port numbers so whenever any acknowledgment or anything comes you will see the port number to determine to which process this belongs to secondly  now assume that two clients using the same server at the same time now the two clients may be connected in two different networks and the round trip time for each of these clients may be different since the roundtrip time is different you can not use the same kind of retransmission time for each of these connections this has to be dynamically assessed from the time stamp so it may be different for each connection and may be changed during the connection  slide time  52  03-52  26   slide time  52  40-52  50  there is a persistence timer when tcp receives an acknowledgment with the window size of 0 it starts a persistence timer window size of 0 means  it does not accept anything at the moment because either it is overwhelmed or something is sort of wrong in between so anyway it is very congested and is not willing to receive anything but now what would the sender do ?  slide time  53  15 53  23  he will start a persistence timer and when the persistence timer goes off the tcp sends a special segment called a probe what would have happened is that the buffer may have become full so now  before sending a packet you have to find out whether it has recovered so you have to send a probe segment  slide time  53  23-53  34  this prevents a deadlock from occurring in case an acknowledgment is lost  slide time  54  29-54  51  there is a keep-alive timer that prevents long idle connections between two tcp ? s suppose two sides the connection has been setup and nobody has closed the connection but both sides are idle for a long time now  as soon as the tcp connection is setup and after the last communication this keep-alive timer is automatically started when the keep-alive timer goes off  that means there has not been any communication between the two for a long time what might have happened is that the other side might have got switched off therefore at times there can be disruption in connection hence it is not a graceful connection  slide time  55  22 55  38  each time the server hears from a client it resets the timer and starts the countdown from the beginning if it does not hear from the client after the timer times out it once again sends a probe segment to see where the other side is alive or not if the other side is indeed alive and wants to keep the connection on it will reply to the probe segment and then it will know that it still wants to do something but then it is much occupied at the moment and that is why it is not sending so this is the keep-alive timer if on the other hand it has been switched off then this probe will go unanswered so after sometime this side will take some action to close everything  slide time  55  22 55  38  time-waited timer is used during connection termination when tcp closes a connection it is not really closed until this timer times out it allows for fin segments to be received in the previous case the last acknowledgment has not been received because it was lost so everything was fine but the other side has actually gracefully terminated the condition and stopped it so it has to wait for sometime which is more than twice the round trip delay therefore it waits for the time-waited timer and once that is over then the connection can be terminated with this we close our discussion on tcp and there is some more variance of tcp regarding how you control the window sizes etc we will discuss that when we discuss congestion control lecture no # 32 ip multicasting  slide time  56  51 ? 57  01  good day  today ? s topic is ip multicasting there are three modes of operation one is unicast that means one sender is sending something to one receiver the other is broadcast which means one sender is sending it to all the receivers that means to all the nodes in the networks multicasting is  when you want to send it to just a group of hosts and not all the hosts  slide time  57  26 ? 57  46  ip multicasting  multicast communication refers to one-to-many or many-to-many communication unicast refers to the source is one and the destination is one broadcast is  when source is one and destination is to all and in multicast destinations are a few  slide time  57  46 ? 58  08  ip multicasting refers to the implementation of multicast communication in the internet individual hosts are configured as members of different multicast groups multicasting is not connection-oriented an ip multicast group is identified by a class d address so these are the general parameters one particular user may be a member of different multicast groups for a one particular multicast group there will be a few members in the network and it has to reach those and not others and the other thing to be understood is that  multicasting is not connection-oriented that means all the packets are sent that means it is packet by packet and it is not from the source  multiple channels or a prairie set up like that computer networks prof s ghosh dept of computer science & engineering iit kharagpur lecture no # 32 ip multicasting good day today ? s topic is ip multicasting  refer slide time  00  56 01  08  slide time  00  56 01  08 till now  we have seen the three modes of operation  one is unicast where one sender is sending to one receiver one is broadcast where one sender is sending to all the receivers that means all the nodes in the network and multicasting is when you want to send it to a group of hosts but not all the hosts so  ip multicasting is the topic for today  refer slide time  01  31 01  53  slide time  01  31 01  53 multicast communications refers to one-to-many and many-to-many communications for example  this is unicast when the source is one and the destination is one  broadcast is when source is one and destination is all and multicast is when destination are a few  refer slide time  01  53 02  14  slide time  01  53 02  14  ip multicasting refers to the implementation of multicast communication in the internet  individual hosts are configured as members of different multicast groups  multicasting is not connection-oriented  an ip multicast group is identified by class d address these are the general parameters one particular user may be member of different multicast groups but for one particular multicast group there will be few members in the network and it has to reach those and not others other thing to understand is that multicasting is not connection-oriented that means all the packets are sent it is packet by packet it is not that from the source multiple channels are prairie setup or anything like that  refer slide time  02  46 03  15  slide time  02  46 03  15 they are identified by class d address which is for multicast the class d address is seen in the fashion that the beginning four bits are 1110 so the initial value is 224 something and these 28 bits this is how by looking at an address you can see that this is the multicast address  refer slide time  03  15 03  26  slide time  03  15 03  26 there are many applications like news  sports  stock and weather updates let us take the example of stock updates now  not everybody would be interested in stock updates  only some group of people would be interested in stock updates again  it may be such that one group of stock is of interest to one group of people  another group of stock to another group of people and so these would be different multicast groups and the news feed should reach these people then multicasting may be applied in distance learning  refer slide time  03  49  04  17  slide time  03  49  04  17 when some learning material is distributed to distance learners and just specific group of learners  configuration  routing updates  service location may be the areas where multicast may be applied pointcast-type push applications where push means when the actual source of information finally on its own sends it to the group of people for example  stock quotes may be  refer slide time  04  28  04  52  slide time  04  28  04  52 pushed teleconferencing  audio  video  shared whiteboard  text editor etc is an interesting and important application of multicasting that means you may like to have a video conferencing amongst a group of people so this same video stream should reach the entire group of people  refer slide time  04  52 ? 05  18  slide time  04  52 ? 05  18  distributed interactive gaming or simulations  some people are participating in some game in a distributed fashion  email distribution lists  content distribution ; software distribution  web cache updates  database replication multicasting has very large number of applications but the trouble is  multicasting is a little complex the technology is not so simple and the fact is there are most of the routers which are in operation today are not configured for multicasting in a proper manner because it takes a toll on the routers capabilities so we will come to see what a multicast routing is all about  refer slide time  05  42  05  59  slide time  05  42  05  59 there are three essential components of ip multicast service   ip multicast addressing is  how you address  ip group management  multicast routing  refer slide time ? 06  00 07  13  slide time ? 06  00 07  13 if you look at this diagram  suppose you have these three routers and three networks connected to these three routers respectively suppose  you take any of these routers and that is connected to its own group of machines and some of these machines would be the members of a multicast group of course  new machines can come in and new machine can actually decide to join the multicast group and some of the old group members may choose to leave a particular multicast group so there is a group membership protocol which goes on between the router and the different machines connected to the network then  amongst the routers there is question of multicast routing so this internet group management protocol  igmp  runs between host and their immediately neighboring multicast routers and within the routers we have the multicast routing protocol running  refer slide time  07  13  07  41  slide time  07  13  07  41 this is another picture showing the same thing this is the service model  suppose  these are the hosts and host to router protocol which is known as the internet group management protocol  igmp   and then amongst the routers there is multicast routing protocol and there are various types of multicast routing protocol  we will just discuss a few  refer slide time  07  41 ? 08  07  slide time  07  41 ? 08  07  ip multicasting only supports udp as higher layer  there is no multicast tcp udp is a connectionless datagram oriented protocol and tcp is connection-oriented since  ip multicasting is essentially connectionless that is why it chooses udp as the transport layer protocol  refer slide time  08  07  09  13  slide time  08  07  09  13 if you look at the details of this part of the protocol stack here we have the network interface  ip and ip multicast layers the ip part of this network layer takes part in the normal routing protocol whereas ip multicast part takes care of the multicasting routing protocol then above this in the tcp ip stack we have the tcp protocol and the udp protocol so  for tcp we use the stream sockets whereas for udp we use the datagram sockets as well as multicast sockets so  multicast sockets also use udp this is the socket layer and above this we have the user layer so that is the application layer which uses these multicast sockets and uses udp to send multicast messages which are routed by the multicast supporting routers  refer slide time  09  13 ? 09  51  slide time  09  13 ? 09  51 ip multicast works as follows   multicast groups are identified by ip addresses in the range 224.0.0.0 to 239.255.255.255 these are the class d addresses  every host  more precisely every network interface card  can join or leave multicast group dynamically  at present the way it has been done has no access control so  if there is a multicast group which requires access control then this has to be implemented in the application layer as the protocol stands today this has not been included one of the reasons is that ip multicasting in actual practice constitutes a very small amount of traffic compared to its potential true multicasting later on let us see what is true multicasting and simulated multicasting but actual true multicasting traffic is really small compared to its potential mainly because most of the routers may not support  they are not more precisely configured to support multicasting because of the cost involved  refer slide time  10  46 ? 11  06  slide time  10  46 ? 11  06 since there is no access control every ip datagram sent to a multicast group is transmitted to all members of the group there is no security  no floor control moreover since it uses udp  ip multicast service is essentially unreliable  refer slide time  11  06 ? 11  46  slide time  11  06 ? 11  46 more detail about the multicast addresses   the range of addresses between 224.0.0.0 and 224.0.0.255 means the last byte for the first range inclusive is reserved for the use of routing protocols and other low level topology discovery or maintenance protocols  multicast routers should not forward any multicast datagram with destination address in this range so they are reserved addresses and other addresses can be distributed to different multicast routes  refer slide time  11  46  12  21  slide time  11  46  12  21 examples of special reserved class d address   224.0.0.1 really means all systems on this subnet  224.0.0.2 means all routers on this subnet  224.0.1.1 is for ntp  network time protocol  used for synchronizing machines  224.0.0.9 is for rip-2  a routing protocol   so these are some special addresses and there are others  refer slide time  12  21  12  28  slide time  12  21  12  28 now there is a question of multicast address translation you remember what happens in the case of unicast is  from the ip address  for transmitting packets in the local network we need to go down to the data link layer and we need to find out the hardware address let us say if we are using ethernet then we need to find out the ethernet or mac address and then data is actually sent as an ethernet frame so the ethernet address is put over there now  that is the case when we are handling unicast now  what happens in multicast ? for this particular ip address it is actually representing a particular multicast group and there will be a number of machines in that group how do you handle it in the ethernet level ?  refer slide time  13  23  14  19  slide time  13  23  14  19  in ethernet mac addresses a multicast address is identified by setting the lowest bit of the most left byte that is this byte suppose you want 1  2  3  4  5  6  if you remember  ethernet address is 6 bytes long  now  of the first byte the most significant byte if you want and the last bit of that is set to 1 in ethernet mac addresses to indicate that this is the multicast address  unfortunately not all ethernet cards can filter multicast addresses in hardware so  if it can not be done in hardware then filtering is to be done in software by the device driver so  you accept the packet and then do the filtering if it is multicast and if you are a member of the group and then accept it  refer slide time  14  19 ? 15  49  slide time  14  19 ? 15  49 this is how the mapping is done suppose in this 1110  the first 4 bits and suppose this is the class d address and we are looking at the first byte of that address and the first four bits 1110 identifies that this is a class d address then this bit is actually ignored and then we have a 23 bit address this 23 bit address comes straight to the ethernet address so these 7 bits  these 8 bits and these 8 bits are matched straight to the last 3 bytes of the ethernet address for the first three bytes of the ethernet address we have a one here showing that this is multicast actually the ethernet address with 01  00  5e in the first 3 bytes are reserved for ip multicast so 01  00 and this is 101 is 5e and 1110 is e so  this is 01  00  5e and this is first 3 bytes  this is reserved for multicast and this part comes straight away  refer slide time  15  49 ? 16  12  slide time  15  49 ? 16  12  now let us move on to igmp which is the internet group management protocol this is a very simple protocol for the support of ip multicast  igmp is defined in rfc 1112  igmp operates on a physical network that is the single ethernet segment  refer slide time  16  12 ? 17  08  slide time  16  12 ? 17  08 if you remember  in the previous diagram we saw that one particular router is connected to one ethernet segment so igmp is between this router and the host which are there so igmp is used by multicast router to keep track of the membership now  who all amongst these members  who has ceased to be a member  who is the new person who wants to join as a member  so this has to be kept track by the local multicasting router and that is what igmp supports so  it supports joining a multicast group  query membership and send membership reports so the multicasting router will send queries from time to time and the host will respond or not respond depending on whether or not they are members of the group  refer slide time  17  08 ? 17  27  slide time  17  08 ? 17  27 so  we have this multicasting router over here and one single ethernet segment and number of machines connected there so igmp query comes from this multicasting router and igmp report goes to the router from the host  refer slide time  17  27  18  30  slide time  17  27  18  30 there may be an igmp general query that igmp group address is set to be equal to 0 it means that this query is for all the hosts  for all the groups and destination ip address is broadcast in this subnet you remember that 224.0.0.1 is broadcast in this subnet and source ip address is the routers ip address there may also be groups specific query in which case the igmp group address is the group address  destination ip address is again the group address because now i want to give this query only to the members of one particular group and source address is the routers ip address and this individual host sends the reports so it is the igmp membership report therefore igmp group address is the group address  destination ip address is also the group address and source ip address is equal to host ? s ip address  refer slide time  18  30  19  02  slide time  18  30  19  02 in the igmp message format there is a version and type type may be 1 or 2  version is usually 1 and then a 16-bit checksum and 32-bit group address type  1 for the query sent by multicast router and 2 is a response sent by a host group address is a class d ip address on query it is 0 and on response it is the group address being reported  refer slide time  19  02  20  27  slide time  19  02  20  27  a host sends an igmp report when it joins a multicast group  note  multiple processes on a host can join a report is sent only for the first process   this means that when a host wants to join one particular multicast group then it sends an igmp report to the router that it wants to join  on the other side when a particular host wants to leave a group  it does not want this multicast traffic any longer so when it wants to leave that group it does not do anything at all only thing is that when the next query comes for this particular group  for which it was a member then it will not respond so this means that there is some kind of aging in the group membership list that the router would maintain so  no report is sent when a process leaves a group  a multicast router regularly multicasts an igmp query to all the hosts  group address is set to 0    a host responds to an igmp query with an igmp report if somebody fails to respond then it is taken that he has left the group  refer slide time  20  27  20  51  slide time  20  27  20  51 what does the igmp host reports look like ?  host sends a report when it joins a group  it does not report when it leaves the group but does not respond to the next query so this is the igmp report  the time to leave is 1  the igmp group address is group address  destination ip address is group address and source ip address is host ip address  refer slide time  20  51 ? 21  17  slide time  20  51 ? 21  17 for general query  group address is 0  destination ip address is naturally broadcast and source is the routers ip address  so routers sends query at regular intervals to see if anyone still belongs to any group queries send out each interface  host responds by sending one response for each group to which it belongs  refer slide time  21  17  21  36  slide time  21  17  21  36 igmp messages are only 8 bytes long we have ethernet header  ip header and the igmp message which is version  type and some part is unused and the checksum and a 32-bit class d address  refer slide time  21  36 ? 22  04  slide time  21  36 ? 22  04 suppose you have a network with multiple multicast routers that means the same network but it has got multiple multicast routers only one router responds to igmp queries so this is the query so the router with the smallest ip address becomes the querier on a network one router forwards multicast packets to the network  so it is the forwarder if a network happens to be so constituted that there are two routers connected to it and both of them support multicasting in that case  only one router will actually do the querying so out of these two routers whichever has the smaller ip address will be the one which does the querying  refer slide time  22  27 ? 22  38  slide time  22  27 ? 22  38 now we come to the topic of multicast routing so  what is special in multicast routing ?  refer slide time  22  38 ? 22  52  slide time  22  38 ? 22  52 let us see this diagram suppose there is no support for multicast at the network layer which is the case in many practical situations  even then you could sort of simulate multicasting by doing repeated unicast so your original source has the list of all the members meaning all their ip addresses so it sends the message to all of them one by one  one by one  one after the other so this is just a successive unicast done therefore this is as if a multicast obviously you are more packets are packed into hops if you take some measure like that and of course you are doing much more work than what is strictly necessary but this is all you can do this has an advantage that the source can closely control that who could be a member of the group and who would not be a member of the group therefore you can impose some kind of access control by having an access list  refer slide time  24  05 ? 26  37  slide time  24  05 ? 26  37 now  if there is support for multicasting then what would happen is that  let us say there is one packet being sent to all three members of the group and say this is the source therefore the source will send only one packet to the next router this router is going to duplicate the packet as to one on this link and the other on this link this packet is ultimately destined for this host whereas this packet again gets multiplied and now this packet goes to this host and this packet goes to this host so  the number of packets traveling down is minimized a lot  of course the final number of packets is the number of users but if you just consider how much each packet travels  that means if you take the number of packets into the hop count kind of a weighted measure then that could be much lower overhead in some sense for packets that are traveling so it is much lower overhead on the links but may be more overhead on the routers now  if you have to have this multicasting capability for the routers this specifically requires two things one is  packet forwarding that can send multiple copies of the same packet for example  consider this router  this router is receiving only one packet but it has in its list that there are two sort of users for which the packet is supposed to go through this router so it has to duplicate the packet one for this link and other for this link so it forwards multiple copies of the packet this capability has to be there in the router secondly  multicast routing algorithm builds a spanning tree dynamically but how you found this tree ? this looks somewhat similar to forming a routing tree for unicast cases but there are some differences this tree has to be built up dynamically and in a distributed fashion by the routers for that they have to run ip multicasting protocol between themselves that is what we mean when we say routers are supporting multicasting  refer slide time  26  37  26  52  slide time  26  37  26  52 goal  the main goal of multicast routing protocol is to build a spanning tree between all members of a multicast group so these are the members of the multicast group and we have to somehow get this tree  refer slide time  26  52  27  57  slide time  26  52  27  57 this can be looked upon as a graph theoretical problem in whatever graph you have you have to embed the tree such that all multicast group members are connected by the tree suppose these are the three members of the multicast group seen in the previous graph and you want to form a tree like this then the only solution is to have a shortest path tree or source-based tree that is  build a tree that minimizes the path cost from the source to each receiver so this is the so called source-based tree and you can form this hence this is one kind of a solution  refer slide time  27  57 ? 30  24  slide time  27  57 ? 30  24 this is called a source-based tree  this is a good tree if there is single sender so sometimes multicast group is such that there is a single sender take the previous example we were talking about  a central news service or some kind of financial advisory service has some members and these are the members of this multicast group and there are some stock codes so this particular group may be interested in the quotation of a particular group of stocks and for this particular group of stocks this company collects all the information  the stock value  etc in a regular fashion and keeps on pushing it to the members of this group now  this is multicasting where there is a single sender and in such cases making a source-based tree makes quite lot of sense but of course  if there are multiple senders you need one tree per sender now that becomes really difficult where it becomes more democratic where all the group members are interacting and any of them can send messages to any member in the group therefore you have to have a tree for each sender having a single sender is easy to compute for this multicasting router we will always assume that whatever unicast routing is happening through osp etc is always present here but this multicast is sitting as an additional service by the router which essentially means that the unicast routing table is available for building up your source-based tree so in such a case this is easy to compute the tree is built from receiver to the sender this is called reverse shortest path or reverse path forwarding  rpf    refer slide time  30  24 ? 31  20  slide time  30  24 ? 31  20 a second solution to the same problem is that  if you remember the other graph  this graph looks different because this is the graph where it minimizes the total cost of the edges if you do not know who your sender is going to be  that means if any member of the group can be a sender then it makes sense to make the tree in such a manner  suppose  if you assume that all of them send packets frequently then in that case having the tree with the minimum total cost of edges would be the optimum solution so this is the second kind of solution  refer slide time  31  20 ? 32  18  slide time  31  20 ? 32  18 this is very difficult to compute  this is called the core-based tree  this is a good solution if there are multiple senders instead of keeping one source-based tree for each potential source we keep one core-based tree  very expensive to compute  not practical for more than 30 nodes for a very good solution  selects one router as core  also called ? rendezvous point ?    all receivers build a shortest path to the core using the reverse shortest path or reverse path forwarding but who would be the core of the rendezvous point depends on how good your core based tree is thereby depending on how you choose the core if you have chosen the core towards the center of the potential graph then that is good  refer slide time  32  19-34  26  slide time  32  19-34  26 let us see the details of reverse path forwarding  rpf   this is the way to build the tree  rpf builds a shortest path tree in a distributed fashion by taking advantage of unicast routing tables  main idea  given the address of the root of the tree  you know the source this tree is being formed from the sources each of the destinations  that means each of these potential recipients are trying to reach to the source and for that they use the unicast routing table which is already there in the router so given the address of the root of the tree  a router selects its upstream neighbor in the tree  the router which is the next-hop neighbor for forwarding unicast packets to the root so  what you do is  for each of the potential recipient you are trying to minimize the path cost from this recipient to the one single source right now for the source-based tree you have one single source whatever you do for unicasting  while sending a message from this node to that node is what you have to follow what the routers have to do is that  on the way suppose two different potential routes from two different recipients go through the same router then from this point onwards it is expected that this router to the final destination is actually the source of multicast communication and there is only one path and the tree would be automatically formed this is the basic idea of reverse path forwarding  refer slide time  34  27-34  48   slide time  34  27-34  48  how can this be used to build a tree ?  rpf forwarding  forward a packet only if it is received from an rpf neighbor  set up multicast routing table in accordance from receiver to sender along the reverse shortest path tree  refer slide time  34  49-35  51  slide time  34  49  35  51 this is an example suppose h1 is the source and rpf neighbor of r3  this is r3 and this is r1  r4 and r5 from these when they try to reach h1 they go from r3 to r2 to some path to h1 so r2 is the rpf neighbor of r3 the destination is h1 and the next hop is r2 this is the unicast routing table so r3 knows that r2 is the rpf neighbor of itself  refer slide time  35  52  36  09  slide time  35  52  36  09  routing table entries for source-based trees and for core-based trees are different  source-based tree  for source-based tree it is  source  group  or  s  g  entry  core-based tree  naturally anybody can be communicating so it is  *  g  entry  refer slide time  36  10  36  49  slide time  36  10  36  49 the source ip address  multicast group  incoming interface  rpf interface  and outgoing interface are the l2  l3 etc  this is a list and finally when a packet arrives the router has to forward one copy of the packet along each of these outgoing links which eventually reach some members of this particular multicast group  refer slide time  36  50  37  08  slide time  36  50  37  08 for building a source-based tree in a network like this set routing tables according to rpf forwarding and then use flood-and-prune  refer slide time  37  09  39  43  slide time  37  09  39  43  set routing tables according to rpf as we have already discussed  flood-and-prune what is flood ? forward packets that arrive on rpf interface on all non-rpf interfaces receiver drops packets not received on rpf interfaces these routers require the capability of forwarding multiple copies of the same packet so  if a packet has come from its rpf neighbor   rpf neighbor is with respect to a particular group  bearing this address means that it is actually coming from the source now it has to be forwarded to each of the outgoing interfaces of course it is a non-rpf this means  when i say rpf interface it means that the rpf neighbor is coming from the source side to all others which lead to different members of this multicast group what happens if you happened to get a packet from a link for this particular group who is not your rpf member ? first of all  how did it happen ? you must remember that we are doing this in a distributed and dynamic system so things can come up and go down so that way a packet can come in but obviously so far as this particular router is concerned if a packet comes from non-rpf link for this group then this is not coming from the source so that packet is dropped and naturally it also does pruning pruning is sending a prune message when a packet is received on a non rpf interface this is one case when you prune or when there are no group members in its local network and no connection to other routers suppose  it so happens that this particular router  you remember that this router is also connected to its local network and with the host in the local network it is running igmp always finding out who are the members of group etc it could happen that this local member has retired it no longer wants to remain in the group that means it is no longer sending your igmp reports so it has nobody to send it to nor is it connected to neither any router nor a part of a link from a distant source to a distant destination  nor a transit link like that so it is not connected in that case also now as the local contributor member of the prune has retired then this of course may not be known to others so it may still get a packet but then what it will do is that  it will send a prune message stating not to send anymore packets to it any longer so it will send the prune message along the route because the neighboring router has sent in a packet and it has nobody to distribute it to nor is it a transit router so  naturally it drops this packet because it has no use for this packet and along this link whoever has sent this packet to it the local member sends a prune message it means do not send the local member any further packets because it has nothing to do with this group anymore so that is one case   refer slide time  41  23  42  24  slide time  41  23  42  24 or  has received a prune message on all non rpf interfaces that means  it was a member earlier with all the non rpf interfaces and just like the group member may have retired similarly this may also have been a transit router on a link from some distant source to distant destination this may be an intermediate router on the way but then it has got a prune message on all its non rpf interfaces that means it was a member of a transit link earlier but now it is no longer a member so once again whoever had sent it a packet  it will send back a prune message to that destination that prunes this link as well so this is no longer interested  refer slide time  42  25  42  51  slide time  42  25  42  51  prune message temporarily disables a routing table entry  effect  removes a link from the multicast tree  no multicast messages are sent on a pruned link  prune messages is sent in response to a multicast packet  which has come and which satisfies any of these conditions  refer slide time  42  52  43  07  slide time  42  52  43  07 once again that it has received on the non-rpf interface or when there are no group members in its local network and no connection to other routers or it has received a prune message on all non rpf interfaces so in such cases the prune message is sent  refer slide time  43  08-43  19  slide time  43  08-43  19 the prune message has the effect of temporarily disabling a routing table entry the question is  why temporary ?  refer slide time  43  20  44  16  slide time  43  20  44  16  why the routing table is only temporarily disabled ?  what happens is that  you may have a receiver who may again like to join so one needs to reactivate a pruned routing table entry in that case so what happens is that  this group member may have gone away somewhere and now has come back and wants to be a member of the group once again so  it gets the igmp report saying that it is a member of this group now  it is aware of who knows the source  this multicasting router so what it will do is that it will try to reactivate this link and then the rest of it will work  so this is called grafting  sending a graft message disables prune and reactivates the routing table entry so this pruning and grafting are complimentary to each other you prune to disable and you graft to enable again  refer slide time  44  17  45  50  slide time  44  17  45  50 next is the core-based tree this was a source-based tree when you have one source and many receivers now you have many to many kind of situation that was one to many communications  not one to all  not broadcast   but now we have many to many communications that means there are many members of the group who might like to communicate with other members of that group in this case we would like to have what is known as a core-based tree  one router is the core  receiver sends a join message to rpf neighbor with respect to core now every receiver actually wants to join to the core  join messages creates a  *  g  routing table entry  source sends data to the core  core forwards data according to routing table entry now  since there is no source or that anybody could be a source we put a star in place of s for a particular router all the links get messages like this so a message may come in through any such link and it has to be forwarded to the other links  refer slide time  45  59  46  47  slide time  45  59  46  47 we just mentioned about multicast routing protocols which is actually implemented in most of the routers and many of the level 3 switches this is called dvmrp so you find them actually but unfortunately i have seen it very rarely being used but this is there in most of the routers of today as well as in many of the level 3 switches dvmrp is there  dvmrp is a distance vector multicast routing protocol  this is the first multicasting routing protocol  it implements flood and prune distance vector routing  if you remember  the distance vector routing uses the distributed bellman ford algorithm which is implemented by rip the centralized diesters algorithm  the link state algorithm is implemented in the routing protocol called ospf  refer slide time  46  48  47  58  slide time  46  48  47  58 open shortest path first  recall our discussion about routing protocols and this ospf is currently the most acceptable routing protocol there is a multicast extension of ospf which is known as mospf  multicast open shortest path first    multicast extensions to ospf  each router calculates a shortest path tree based on link state database  it is not very widely used  pim-sm builds core-based trees but they are not widely used  refer slide time  47  59  48  18  slide time  47  59  48  18  distance-vector multicasting routing  dvmrp  consists of two major components   a conventional distance-vector routing protocol  like rip    a protocol for determining how to forward multicast packets based on the unicast routing table  refer slide time  48  20  49  27  slide time  48  20  49  27  dvmrp  distance-vector multicasting routing  routers forward a packet if   the packet arrived from the link used to reach the source packet this is the reverse path forwarding that means if from this router i want to reach the source then i have to go to that next hop and that is my rpf neighbor so  if the packet arrives from this link from my rpf neighbor link  arrived from the link used to reach the source of the packet then this is the rpf chain  packet forwarded only to the child links not in the direction from which it came but other child links  if provided the downstream links have not sent a prune message  refer slide time  49  28  52  45  slide time  49  28  52  45 but dvmrp has limitations   like distance vector protocols  affected by count-to-infinity and transient looping in the count-to-infinity problem some link has failed but nobody could make out that the link has actually failed so it is going round and round known as the count-to-infinity problem where exactly the counting of potential distance to that link comes to infinity since we are using the same distance vector routing transient looping means sometimes a routing loop may form and the packet goes round and round  multicast trees are more vulnerable than unicast for these problems  this shares the scaling limitations of rip and this scaling limitation essentially comes from what i have written in the last  no hierarchy  flat routing domain one of the advantages of ospf over rip was that in ospf we break up the network into a hierarchy there are these autonomous regions or autonomous domains and then further down it can be broken up so that the routing problem remains simpler and you can scale to bigger and bigger networks since dvmrp is based on rip or essentially on the ideas of rip it is again a problem in dvmrp also here you can not scale and then you have further problem because of multicasting  you may have  s  g  state in routers  even in pruned parts  broadcast-and-prune has an initial broadcast when i say flood-and-prune  actually you are flooding the network so there is some kind of broadcast going on if the network size is small  this broadcast may be acceptable but when the network grows bigger and bigger broadcast becomes unacceptable so that is again another problem in scaling  this is limited to few senders many small groups also undesired since this essentially forms a source-based tree you can have only a few of them  just a few senders many small groups are also undesired if you have large number of groups  once again you have the same problem of maintaining so many trees and that also becomes a limitation for scaling  refer slide time  52  46  53  29  slide time  52  46  53  29 let us discuss about an effort to implement multicasting as i told you  most of the routers are not configured to use the multicast in every manner but still some people want to use multicasting so they built up this multicast backbone  mbone  which is essentially an overlay network of ip multicast-capable routers using dvmrp so it uses dvmrp and it is an overlay network of ip multicast-capable routers what does that mean ?  refer slide time  53  30 ? 55  05  slide time  53  30 ? 55  05 that means  some of the routers in the network in some places are multicast-capable and what happens is that they are going to support multicasting in its own locality that means it will support multicasting amongst the network to which they are directly connected these routers are going to run a multicasting protocol between themselves but then in between there are whole lot of other routers in between there is a cloud of routers which are not supporting multicasting so what it will do is that it will tunnel through this cloud to the next multicast supporting router so this is the picture of the mbone you have r which is the host or the router and this r and this h are the mbone routers they support and the part of the mbone is shown in light blue where the multicasting is directly supported whereas when they try to communicate to another multicast supporting node over a cloud which does not support multicasting they tunnel through it  refer slide time  55  08  55  18  slide time  55  08  55  18  mbone tunnel is a method for sending multicast packets through multicast-ignorant routers  ip multicast packet is encapsulated in a unicast ip packet  ip-in-ip  addressed to far end of the tunnel  refer slide time  55  21  56  07  slide time  55  21  56  07 you have the ip header destination which is unicast and then you have another ip header destination that is multicast and then the transport header what happens is that  the intervening routers which are not multicast-enabled are going to see this destination and this destination would then actually the next multicast router and here this part will be the pay load so the network nodes would not look into this when it reaches the next multicast supporting router it will get this and then discard this and then look at this multicasting header so this is the ip-in-ip encapsulation and tunneling  refer slide time  56  07  57  23  slide time  56  07  57  23  tunnels act like virtual point-to-point link  intermediate routers see only router header that means the unicast routing header  tunnel endpoint recognizes ip-in-ip  protocol type = 4  and de-capsulate the datagram for processing  each end of the tunnel is manually configured with unicast address of the other end so  this is what you have to do this was done to implement multicasting in an environment and try it out if there are problems about the one which is actually implemented in most of the routers namely dmrp that does not scale well and if there are many groups  in today ? s world when everybody in sort of networked and people have all their special interest etc it is quite considerable that the number of groups will explode if it could really do multicasting in a very easy fashion and that is very difficult for routers to handle that is why most of them do not use it at the moment but potentially this is a very useful kind of technology thank you  refer slide time  57  27 ? 57  28  slide time  57  27 ? 57  28 good day today we will talk about some protocols which are useful for controlling the network and making the machines connected to the network specifically  under the broadcast we will talk about dhcp and icmp there are some protocols associated with this and we will talk about this  refer slide time  57  59  58  02  slide time  57  59  58  02  refer slide time  58  03  58  20  slide time  58  03  58  20 dhcp is the dynamic host configuration protocol it is about configuring a host  configuring a machine  configuring a may be a pc or some computer which is connected to the network  refer slide time  58  20  58  44  slide time  58  20  58  44 its chief utility  there are other utility are dhcp we will be discussing later the chief motivation came from dynamic assignment of ip addresses now  dynamic assignment of ip addresses is desirable for several reasons computer networks prof s.ghosh dept of computer science & engineering iit kharagpur lecture 33 dhcp and icmp  refer slide time  00  47  good day today we shall talk about some protocols which are useful for controlling the network and also in keeping machines connected to the network specifically under the broadcast we will talk about dhcp and icmp there are some protocols associated with this so we shall discuss about it  refer slide time  1  14-1  16   refer slide time  1  18-1  38  dhcp is dynamic host configuration protocol it is about configuring a host  configuring a machine may be a pc or some computers connected to the network  refer slide time  1  35-3  29  the chief motivation came from the dynamic assignment of ip addresses the dynamic assignment of ip addresses is desirable for several reasons  ip addresses can be assigned on demand for example  when you have a scarcity for real ip addresses then you keep a central pool of ip addresses and then as some computer comes on line it assigns an ip address from the pool and when it goes out those ip addresses are withdrawn and are given to some other machines another place where it may be required is  suppose you have some kind of a ras or remote access server to which a number of machines should be connected via dial up connections then in that case you give them a temporary ip address for the connection now if somebody wants to visit some network with a laptop then they have to get a network address of that particular network therefore that network address has to be dynamically assigned ip addresses are assigned on demand it avoids manual ip configuration which is prone to errors it also supports mobility of laptops  refer slide time  3  30-3  59  dynamic assignment of ip addresses is done using three different protocols 1 rarp  it was widely used up to 1985 and even beyond this period people kept using it 2 bootp  bootstrap protocol was used until 1993 3 dhcp  is used after 1993 and currently this is in wide usage a bootp client can also use dhcp server  refer slide time  4  08-5  33  rarp is actually the reverse arp address resolution protocol the problem is  given an ip address what is the mac address this is for mapping between the ip addresses and the mac addresses finding the mac address for the ip address is useful when you want to communicate over a lan rarp is the reverse of this given a mac address  rarp finds ip address this would be necessary in case you have something like disclosed work station which boosts the signal over the network a disclosed work station has its own mac address and it wants to get an ip address assigned this is where rarp is typically used rarp is used to broadcast a request for ip address associated with a given mac address rap server responds with an ip address it only assigns ip address and not the default router  subnet masks etc that are required and they are not a part of this server refer slide time  5  34-5  46   refer slide time  5  34-5  46  so ip address to mac address is the arp and ethernet mac address to ip address is the rrap  refer slide time  5  45-6  39  let us see the improved version of rarp i.e bootp bootp not only assigns ip addresses dynamically but also has some more functions host can configure ip parameters at boot time basically there are three services  ip address assignment detection of the ip address for a serving machine the name of the file to be loaded and executed by the client machine i.e the boot file name this is the source from which it gets the name bootstrap protocol i.e when the machine is booting up it not only gets an ip address but also gets the name of the file which can be loaded and executed this is the bootstrap protocol or bootp  refer slide time  6  40-7  37  bootp not only assigns ip address but also default router  network mask  etc therefore whatever that particular machine requires for communication namely the addresses such as network  subnet mask  gateway etc are given by the bootp protocol this is sent as an udp message so udp port 67 is for server and udp port 68 is for the host port 68 for host is required when you may want to find out a machine from bootstrap protocol which is already available on the network and use limited broadcast address that is 255 255 255 255 if you recall from our discussion about addresses this is a broadcast address where the broadcast is limited to this particular subnet or network  refer slide time  7  51-8  14  bootp can be used for downloading memory image for diskless workstations so whatever was the motivation for rarp the same thing can be done through bootp also but assignment of ip address to hosts is static this is one sort of drawback of bootp  refer slide time  8  16-8  52  to make it dynamic we go to the dynamic host configuration protocol which is standard now and more versatile than rarp and bootp it can do a lot of things apart from just giving the ip address this was designed in 1993 as an extension of bootp with many similarities to bootp and same port numbers as bootp that is why dhcp server can handle a few bootp clients  refer slide time  8  52-10  05  extensions  there are lots of extensions especially with options but these extensions support temporary allocation or leases of ip addresses leasing of ip address  suppose when we have a remote access server and when people are dialing what would happen is that  it would be given a particular ip address for a fixed amount of time when its lease expires then that ip address may be withdrawn and half way down the lease period if there is no great demand for ip address then the lease may be automatically extended or if there is a great demand the lease may be withdrawn also this is for a temporary period of time and that is how it is dynamic dhcp client can acquire all ip configuration parameters not only subnet mask and gateway addresses which are there in bootp but also other kinds of parameters can be downloaded from a dhcp server  refer slide time  10  16-10  23  so dhcp is the preferred mechanism for dynamic assignment of ip addresses and dhcp can interoperate with bootp clients  refer slide time  10  24-12  17  dhcp has a number of options it is not possible to mention all the available options here other dhcp information is sent as an option so the number of options is actually greater than 100 which include things like subnet mask  name server  host name  domain name  forward on/off  default ip time to leave  broadcast address  static route  ethernet encapsulation  x window manager  x window font  dhcp message type  dhcp renewal time  dhcp rebinding  time server smtp server  client fqdn  printer name etc as the number of services given over a network grew it became important to give more information to the machines originally the machine was used just for communicating between two computers suppose there may be a centralized print service in the network and whenever you want to print something it can be done in the network similarly all other kinds of services became available in the local network as well as over wider networks so all these would require some kind of configuration on the host end therefore such information can be transferred via this dhcp  refer slide time  12  18-12  50   slide time  12  18-12  50  there are a number of dhcp operations let us discuss a few of them dhcp discover  at this time the dhcp client can start to use the ip address renewing a lease  it is sent when 50 % of the lease has expired if dhcp server sends dhcpnack then the address is released then you know your lease is not going to be renewed  slide time  12  49-12  55  dhcp release  at this time the dhcp client has released the ip address  so the client has given it up  refer slide time  12  56-13  40   slide time  12  56-13  40  dhcp message header fields  in some fields there is an opcode it may be a dhcp request from the client or it may be dhcp reply from the server the dhcp message type is sent as an option the hardware type of message is 1 for ethernet and hardware address length is 6 for ethernet hop count is set to 0 by client and transaction id is an integer used to match reply to response if there is more then one request  refer slide time  13  38-14  30   refer slide time  13  38-14  30  seconds  it is the number of seconds since the client started to boot client ip address  your ip address  server ip address  gateway ip address  client hardware address  server host name  boot file name  etc all these fields are available so when the client sends the request it would fill in whatever is known to it  maybe the mac address is known to it so it puts in the mac address and all other fields are left blank dhcp server will pick up the message that we broadcast and then fill up all the other necessary fields and then broadcast it back  refer slide time  14  29-14  50  the following are the dhcp message types sent as an option  dhcpdiscover dhcpoffer dhcprequest dhcpdecline dhcp acknowledge dhcp not acknowledge dhcprelease dhcpinform and so on  refer slide time  14  48-14  58 our next topic is icmp internet control message protocol  refer slide time  15  05-15  30  let us see ip protocol and its deficiencies before that  slide time  15  05-15  30  the internet is of course based on the internet protocol ip protocol has some drawbacks though it is a best effort delivery service it lacks error control and lack of assistance mechanisms since ip is a best effort delivery at some point of time the effort may not be enough and routers ore other nodes on the network may have to drop packets and packets may not reach its destination on time and in proper order first of all there is no error control and secondly if such errors do occur there is no message to the sources secondly  if you want to control the network for some reason  for example  may be the network is getting congested and so you want to do something about it  but ip does not have the mechanism so  for all these purposes icmp was brought in  slide time  16  36-16  55  therefore what happens if a router must discard a datagram because it can not find a route to the final destination ? what if the time to live field has zero value ? what if it has to discard all the fragments because not all were received in a predetermined time limit ? in all these cases ip has to discard a packet  slide time  17  05-17  24  and similarly there are other situations for example  may be it has reached the destination but the port is not available so ip protocol also lacks a mechanism for host and management queries so icmp was designed to compensate for these deficiencies  slide time  17  25-18  01  icmp is a type field that indicates the type of icmp message being sent and the type may be queries or errors code field gives further information specific to the icmp message for example  when an error occurs it tells what kind of error it is checksum field is used to verify the integrity of the icmp data so once again the checksum is included to control the error  refer slide time  18  00-18  24  there are two types of icmp messages one is error reporting and the other is query response if there is some error then the error reporting type of icmp message would be generated and if there is a query another type of icmp message would be generated  refer slide time  18  23-18  29  there is no effort in icmp to correct the errors this is the job of some other layer so it does not really try to correct the errors but nearly reports the errors the error messages are sent to the source suppose the datagram has been sent and something has happened to it and due to that there is an error  and now whoever drops that packet send an icmp message back to the source it may be a router on the way or may even be the final destination  refer slide time  19  23-19  50  these are the various types of errors in error reporting there may be a destination unreachable  there may be a source quench sending to first some of the important ones may be time exceeding  some may be parameters problem or redirection  etc  refer slide time  19  51-21  29  please note that  no icmp error message would be generated in response to a datagram carrying an icmp error message that means  somebody has generated an icmp error message and it is traveling back to the source  and that error message itself gets an error and may have to be dropped on the way  then in such cases we do not generate another icmp message a little bit of problem happens due to congestion of networks so if the network is very congested many packets may get dropped and then if in response to dropping many packets you generate more packets then the congestion is not going to go away so  icmp error messages do not trigger other icmp error messages for a fragment datagram that is not the first set of fragment for example  the datagram may have been fragmented into a number of parts  may be fifty parts  now for each of them you generate an icmp message then the icmp message would be too many so it is only generated for the first fragment for a datagram having a multicast address  once again we can not send an icmp messages to all members of the group and for a datagram with a special address such as 127 0000 or with some address like 0.0.0.0.0 also no icmp error messages are generated for these  refer slide time  21  30-23  15  all error messages contain a data section that includes the ip header of the original datagram plus the first 8 bytes of data in that datagram this information is required so that the source can inform the protocols about the error from the original packet that was dropped the ip header of that original packet is sent back first of all you need to know the source and know where you want to send back this icmp message secondly  even after this icmp message gets back to the specific machine from which the original packet was generated at this point it may have some error messages due to network intervening or this may have to do something with some process or application which is running on the source machine so  after getting the message the host must know to which process it relates to after the ip header what comes is the transport layer header so  a part of the transport layer header also goes back along with the icmp message .this information is required so that the source can inform the protocols about the error  refer slide time  23  16-23  50  destination unreachable  this is one type of an error message when a router can not route a datagram or a host can not deliver a datagram  then in that case the destination is unreachable a router can not detect all problems that are preventing the delivery of a packet so it is not always possible to exactly know why the destination is unreachable but at least this information that the destination is unreachable  goes back to the source  refer slide time  23  51-25  30  source quench  this is a crude attempt to implement some kind of flow control ip protocol has got no flow control routers and hosts have limited size queues so what happens is that  may be in an intermediate router and a number of packets have come up and certainly there is a flood of packets into one intermediate router from various directions so what would happen is that its buffer is going to overflow and it will not be able to process because there is a limit depending on the speed of the router etc  there is a limit as to how fast packets can be processed and forwarded by an intermediate router and if other packets keep coming in  within that time they are going to be stored in the buffer where in the buffer might overflow in that case the router can not do anything else but to drop those packets this router desperately wants to tell other people in the network to slow down on sending packets and that it can not handle it because of overload basically it tries to slow down the flow of packets into itself so it sends the source quench icmp message towards the sources if datagram is received faster than they can be processed the queue may overflow and in that case it asks the network to slow down  refer slide time  25  32-25  43  if a router or host discards a datagram due to congestion it sends a source quench message to the sender the source must slow down the sending of datagram until the congestion is relieved  refer slide time  25  44-26  03  this may be used when bottlenecks occur for example  on a wan link with too much congestion it is used to reduce the amount of data lost but a warning is  source quench message will in turn generate network congestion there were already too many packets in the network but you have sent a source quench packet towards the source which is just one hop towards the source was already getting packets from the source but will also get an icmp message from the router just one hop down so it is having more packets now so by this way congestion might travel towards the source but anyway finally it reaches the source and the source will hopefully slow down and all these will die  refer slide time  26  48-27  18  time exceeded  whenever a router receives a datagram with a time-to-live value of zero that means it has been going round the network it discards the datagram and sends a time exceeded message to the source when the final destination does not receive all of the fragments in a set time it discards the received fragments and sends a time exceeded message to the source these are two different cases  one is that  in the destination all the fragments did not reach so there was a specified time after which it has to drop all the fragments and send a time exceeded message to the original source the other thing is that  when a router receives a datagram with the time-to-leave field which is zero if you remember  keeping a time-to-leave field and decrementing it at every hop is quite important because suppose there were some packets which were floating around in the network and due to some trouble with the routing tables a loop has been formed  so  if you do not have this time-to-leave field it will go round and round at infinite term where they slowly burden the network so  the solution to that was  after a certain number of hops the packet is dropped and when a packet is dropped a time exceeded message is sent to the source there may be parameter problems  refer slide time  28  35-28  48  if an ambiguity is found in the header of a datagram the datagram is discarded and a parameter problem message is sent back to the source  refer slide time  28  47-28  59  redirection  a host usually starts with a small routing table that is gradually augmented and updated one of the tools to accomplish this is the redirection message  so  actually this helps in routing  refer slide time  29  00-29  18  now let us come to queries icmp can also diagnose some network problems for example  echo request and reply  time stamp  address mask  router solicitation and advertisements  these are example of queries we will just see a few of these also  refer slide time  29  22-30  01  echo request and reply  is used very often when you want to find out whether the network is up and running or not an echo request message can be sent by a host or router an echo reply message is sent by the host or router which receives an echo request message the echo request and echo reply message can be used by network managers to check the operation of the ip protocol echo request and echo reply message can test the reachability of a host this is usually done by invoking the ping command later on we will get into more details of ping because that is one kind of command which even users quite often require for example  if you are logged on and you find that you can not reach your destination anywhere in the network then you have to find where the problem lies  is it in your local network or in the local subnet therefore in the local subnet you might ping that gateway to see whether you can reach up to the gateway if your ping message goes up to the gateway and you get an echo reply then you know that up to that much the network is ok and if you are ok up to the router you may want to ping the router in the entire network now the problem may be somewhere in the link outside the problem may even be in the destination which you are trying to reach so one way is to go probing the network  even by users is to use ping  refer slide time  31  16-31  47   refer slide time  31  16-31  47  timestamp request and reply  timestamp request and timestamp reply messages can be used to calculate the round-trip time between a source and a destination machine even if their clocks are not synchronized so  sending time is equal to the value of receiving time stamp minus value of original time stamp so this way you can get some idea about the round-trip time there are other ways also  refer slide time  31  49-32  12  so receiving time = time the packet returned ? value of transmit timestamp round-trip time = sending time + receiving time so the timestamp request and timestamp reply message can be used to synchronize two clocks in two machines if the exact one-way time duration is known  refer slide time  32  14-32  21  address-mask request and reply  enables a host to request and receive the network or subnetwork mask it is useful for diskless stations at start up but we have seen the dhcp is another way of handling this  refer slide time  32  23-32  43  router solicitation and advertisement  allows request of routing information and the reply of this information routers can periodically send router advertisements without being solicited suppose a router has just been connected to the network  anyway the routers have to run the routing protocol like the rip or bgp  osp etc  this means it needs to communicate to the neighboring routers  but how do the other routers know there is a new router in the group so  one way is  as soon as the router gets connected it does some router solicitation and it advertises itself so that other routers get to know that and slowly the entire network becomes aware of this new router which is connected similarly  a link may go down and all kinds of other things may happen so  the exchange of router information has to happen through some mechanism  refer slide time  33  44-34  15  router discovery message  host can learn about available gateways to other networks host send the router solicitation message to begin the process using the multicast address of 224.0.0.2 as the destination it can also be a broadcast message in case a router does not accept multicast messages when a router receives the message it will advertise its available gateway  refer slide time  34  16-34  28   refer slide time  34  16-34  28  the checksum of the icmp message  in icmp the checksum is calculated over the entire message  that is the header and data combined this is just to keep some control over errors  refer slide time  34  26-34  48  clock synchronization  software may require time synchronization so icmp time stamp message combats this problem it allows local host to ask for the current time from a remote host using icmp timestamp request so it is type 13  refer slide time  34  47-35  04  remote host uses icmp timestamp reply which is type 14 so  the better way of synchronizing the clocks is to use the network time protocol the time is the ut universal time  refer slide time  35  05-35  08  ping and traceroute   refer side time  35  15-35  55  this is an overview this is part of the icmp messages ping sends an icmp message to a remote host and lets you determine if that host is responding actually ping uses echo and echo reply for the icmp message traceroute uses ttl fields to query all hosts enroute to a specific destination you can use traceroute to map a network that means  if you want to know which is the route you are tracing then this helps you  refer slide time  35  58-36  46  ping is named after sonar in sonar if you want to probe some place you send an ultra sound signal just like you do in radar and if it bounces of something you get a ping  so that is where the name comes from if you want to send an echo request you expect an echo reply and that is your ping so server normally implemented in kernel uses icmp echo and echo reply messages on unix the identifier field is set to unix pid or sending process sequence numbers starts at 0 incremented every time a new echo message is sent actually  when you ping a machine not just one request is sent the machine you are trying to ping or the channel may be noisy and if that happen then your echo request or the reply may get dropped in between so  sending one request is not sufficient and may be three times or five times etc you can configure it  it sends echo request and it expects all the three or all the five replies and if it receives none of them  then in that case it will say that hundred percent of packet loss or it may get two out of five so it will say sixty percent of packet loss or forty percent  refer slide time  36  37-39  46   refer slide time  36  37-39  46  let us see one example of ping suppose we ping a machine 144 16.182.1  we have pinged this machine and then give the ip address over here by the way if you have a name server on the network you could also put the name over there ping 144 16.182.1  56 data bytes is your data plus it will have some thing so you may get a result like this  64 bytes  this is what you are getting from the echo reply  64 bytes from 144 16 182 1 icmp sequence = 0 time-to-leave is 240 and time = 37 milliseconds so it gives you some idea about how much time it takes then another packet has came back as an echo reply 64 bytes from the same machine  sequence number 1 and time is so much for each packet it receives back as reply it is going to print a line like this and then finally it will give you a statistics as something like this  13 packets transmitted  11 packets received which means that it had originally sent 13 packets and it got only 11 packets back so 2 packets must have got lost therefore it is a 15 % packet loss and the round-trip time you may calculate the mean  average  max so  from the ping you can get an idea about the round-trip time  refer slide time  39  51-40  12  some details on the output sequence number are shown for each message in our example message returned in order but we lost some packets they may be returned due to out of order also ttl field of return message is displayed and round-trip time is calculated at the host based on the sequence number  refer slide time  40  14-41  45  we can estimate not only the round-trip time but also the bandwidth using ping but this works only for few hops if it is beyond a number of hops your ping will not work the ping packet can estimate the bandwidth in this way  20 byte ip header  8 byte icmp header  56 byte message this can be set by the user  so the total datagram size is that + 76 + 8 = 84 bytes so 84 bytes were sent now  if it was sent through ppp it will add about 8 bytes so the total size will be 92 bytes so this connection looks like 92/.180/2 that is about 1069 bytes per second what is this .180 ? it is 92 bytes so this is time this gives you some idea about what kind of bandwidth you have in this particular case the bandwidth is not that much but it is only about 1069 bytes per second this is a very crude estimate but you can get some kind of feel about your immediate locality  refer slide time  41  56-42  23  record route option  most ping implementations provide record route which is ? r option on linux  ? r option on windows each router stores its address in the ip options field  only 9 addresses are possible thus round-trip is only possible for 4 routing hops so you can take only 4 hops and within those 4 hops you can find out that how your message went and how it came back that is  may be it came back through different paths or it could have returned in the same path etc you can actually trace the route and because of the limitation on the size that is on the number of addresses you can store you can only route or map the network in your immediate locality but if you want to go beyond this then you have to use something else called traceroute  refer slide time  43  12-46  16  traceroute uses a sequence of icmp messages to determine the current route to a particular destination this is actually done in an iterative fashion suppose i want to traceroute to a distant machine whose ip address is known then i will send a message to that machine but with a very small number for the time-to ? live therefore what will happen is that my message will take so many hops but then it has not reached the destination so may be it must have just started and it will be somewhere in the beginning so its time-to-leave is going to become 0 as soon as the time-to-live becomes 0 the intermediate node may be that router will have to drop the packet and it sends an icmp message back to the source now my program gets this icmp message and now it sends the same dummy message to the destination after increasing the time-to-live by one unit now it is going to pass that router that had dropped the packet in the previous instant and so it will go one more hop and then the packet will get dropped so that router is now going to send an icmp message back to the source now we will know which router is on the way therefore by this way iteratively you keep on increasing the time-to-leave one by one and you trace the entire route  that is  you map it out but let us see what happens when it reaches the destination ? when it reaches the destination what happens is that this message is sent to a very unlikely port  a randomly selected port most probably the destination machine will not know about this port so it will say that the port is unreachable and then that icmp message will come back now we know that we have reached the destination hence this way we have traced the entire route one by one from the source to the destination traceroute uses a sequence of icmp messages to determine the current route to a particular destination the ttl specifies the number of hops a message can travel trace route sends udp datagrams while varying the ttl the router that drops the udp packet now replies with a time exceeded icmp message  refer slide time  46  20-46  40  the end point will not reply with that icmp message because it has already reached there so traceroute sends to an unlikely udp port eventually get a no such port icmp message it knows that it has reached the end  refer slide time  46  41-46  46  so this is the reference about icmp messages actually these are not the only internet control message protocols but there are a number of others which we did not discuss we just discussed a few of them there are other protocols like dhcp  bootp  rarp  arp for example  they help in running the network in a better fashion arp protocol is a low level protocol then we have this rarp  bootp and dhcp for assigning a network this icmp helps in controlling the network operation and giving error messages then there is another side protocol which we will discuss in the next class namely igmp which is internet group management protocol so  that is another part of routing that we have not discussed as yet preview of the next lecture lecture ? 34 dns & directory good day  so today we will take up two topics and they are dns and directory first let us talk about dns  refer slide time  48  29-48  33   refer slide time  48  35-50  00  the dns is a short form for domain name system until now we have seen two kinds of addresses one is mac address or the so called hardware address and in case of ethernet they are also called ethernet addresses which is used in the data link layer for direct communication then we have seen ip addresses which are used for communication between two end points and these two end points may be anywhere in the network so ip address includes information used for routing and there is a network part and post part etc but unfortunately these ip addresses are tough for humans to remember you can remember only a small part of address which you basically require for your own configuration like your own ip address  address of your gateway  address of your mail server etc but beyond that if you have to remember ip addresses of other people it becomes very difficult for human beings to remember and of course they are impossible to guess humans find it much easier to remember and use the domain names domain names are what people use in surfing the web and for a www site sometimes you do not know the name exactly but you make out some guesses by some combination of .com or .net etc we not only can guess but it is much easier to remember we remember so many site names but now  what is the name ? it is used to map some particular machine or site therefore this is also some kind of an address so we have a third layer which are the domain names today we will see how we use these domain names and of course just as in the local area network you require mapping from ip addresses to mac addresses which is done by the arp protocol and in the reverse mac address to ip address done by rarp here you need a mechanism for mapping the domain names to ip address  refer slide time  51  51-52  36  we have discussed this  why not centralized dns > ? single point of failure  traffic volume  distant centralized database would not work  maintenance would be a problem  does not scale so it is distributed so  no server has the entire name to ip address mapping local name servers  each isp company has local name or the default name server and host query first goes to the local name server authoritative name server  for a host  towards that hosts ip address names can perform name address translation for that host name  refer side time  52  37-53  10  there are some root name servers which are   ? .centric   and some of the biggest name servers are in usa but of course it depends on what root it is and they could be distributed also  refer slide time  53  10-53  36  nslookup is an interactive resolver that allows the user to communicate directly with a dns server so  from the os you can use this nslookup and give a name query this is actually a name server look up and that is how the term nslookup comes nslookup is usually available on unix workstations   refer slide time  53  38-53  55  servers handle requests for their domain directly servers handle request for other domains by contacting remote dns servers servers cache external mappings  refer slide time  53  56-54  21  if a server has no clue about where to find the address for a hostname it asks the root server the root server will tell you what name server to contact a request may get forwarded a few times for example  if the iitkgp has a name server now the iitkgp has a name server and request for a particular domain name translation has come to it and it does not know to whom to connect the name server so it can always transfer it to the next higher level namely ernet and if the ernet also does not know where this ernet is there then it can contact the in in will definitely know all the sub domains under it in has to know because it is administering that domain similarly if it is for some address which is from outside you can send it directly to the root of that particular domain  suppose from india you are trying to contact something for japan then you can send it to the jp root name server and then jp root name server would know which name sever to contact so that will come back so this way the dns queries will go back and forth few times and finally the name will be resolved  refer slide time  55  30-56  39  now we come to ldap which is the lightweight directory access protocol since this was designed by the same people who has designed osi this x.500 actually tends to be a little complex it is heavy and it uses the all the seven osi layer for the internet purpose actually it uses the tcp ip stack rather than the seven layer osi layer stack there was a lightweight directory access protocol which can interoperate at least on one side so the ldap can use that x.500 directory service but this is much simpler than x.500 and ldap is used in many places these are lightweight directory access protocol it supports x.500 interface  it does not require the osi protocol it uses the tcp ip protocol so this is x.500 for the internet crowd it is useful as a generic addressing interface like netscape  address book and so on  refer slide time  56  41-56  57  the ldap or lightweight directory access protocol is a networking protocol for querying and modifying directory services running over tcp ip an ldap directory usually follows the same x.500 model which we have discussed  refer slide time  56  58-57  24  now it is a tree of entries  each of which consists of a set of named attributes with values an ldap directory often reflects various political geographic and or organizational boundaries depending on the model chosen when you do that you can also define your security policies based on this directory and based on these boundaries these are especially authentication services  refer slide time  57  33-57  40  so directory is a tree of directory entries an entry consists of a set of attributes an attribute values the attributes are defined in the schema  refer slide time  57  43-59  00  so this is the protocol stack for ldap suppose you have a directory based application or some authorization service or may be access to some information which may be there for the organization which uses ldap may use tls this tls is transport level security you can also use ssl here we are talking a lot about security in future we will give one lecture to security because this has become so important now  in an organizational context a directory may be an important component of the entire security arrangement security is a complex issue but anyway for this ldap we need to communicate securely in many cases and many ldap implementations support this tls the transport level security or you can use ssl or sasl also computers networks prof sujoy ghosh indtan institute of technology iit kharagpur lecture  34 dns & directory  refer slide time  00  42  good day today we will take up two topics  dns and directory first let us talk about dns  refer slide time  00  54-00  56  slide time  00  54-00  56 dns is the short form for domain name system  refer slide time  00  57-4  19  slide time  00  57-4  19 we have seen two kinds of addresses one kind is mac address or the so called hardware address in the case of ethernet it is also called as ethernet addressed which is used in the data link layer for direct communication then we have seen ip addresses used for communication between two end points the end points can be anywhere in the network so  ip address includes information used for routing ip is used for routing where there is a network part and post part etc but unfortunately  this ip address is tough for humans to remember we can remember only a few addresses that we really require for our own configuration  like our own ip address  address of gateway  address of mail server etc beyond that it is very difficult for human beings to remember ip addresses of other people they are impossible to guess what human beings find is  it much easier to remember and use the domain names domain names are used most of time by the people for surfing the web  www site name used for surfing   while surfing the web for a www site  sometimes you may not know the name exactly but you can make it out in 3 or 4 guesses  may be make some combination of .com  .net etc the most important thing here is that it is much easier for humans to remember site names the name is used to some particular machine  site etc this is also some kind of an address and we have the third layer namely the domain names we will see how we use these domain names just as in the local area network  you require a mapping from ip addresses to mac addresses done by the arp protocol and the reverse mac addresses to ip addresses using rarp you need a mechanism for mapping the domain names to ip addresses that is what this whole scheme of dns is all about may be sometimes we will look into the reverse query also  refer slide time  4  20-5  58  slide time  4  20-5  58  the domain name system is usually used to translate a host name into an ip address  domain names comprise a hierarchy so that names are unique  yet easy to remember it is important to remember that  if it is an address it needs to be unique in ip v4 we ran out of ip addresses because we had limited length here  we can go easily with the length because these are not to be used in high speed computations but may be used just once or twice in a session this means you can allow longer names  that is  rather than 4 bytes you can use many bytes but then this also has to be unique what people thought was  if we make a hierarchy of names which is a logical hierarchy which corresponds with the external world then not only they will be remembered easily but can be easily administered and we can make them unique so  domain names comprise a hierarchy so that names are unique yet easy to remember  refer slide time  5  59-6  48  slide time  5  59-6  48 this is what the hierarchy looks like there is a root and from the root we have the top level domain here we have edu  com  org and so on and finally these are for some nations under edu  we have mit  albany and all other kinds of organizations under in  there may be ernet and all that under mit  also there may be cs or something so there is a tree and a hierarchy this is sort of a global hierarchy and is the dns hierarchy  refer slide time  6  49-8  34  slide time  6  49-8  34  each host name is made up of a sequence of labels separated by periods o each label can be up to 63 characters long o the total name can be at most 255 characters so  you have up to 255 bytes to code these names so you can go easy on the length  examples  whitehouse.gov  .gov is for government and since it started in usa  it is us government and whitehouse.gov is a domain similarly let us say csc.iitkgp.ernet.in  .in is a top level domain standing for india and ernet is an organization which comes in the next level in in under ernet  there is iitkgp which comes in the next level and under iitkgp there is csc which again comes in the next layer so  starting from here you can go from the leaf of the tree right up to the root of the tree that is the dns hierarchy tree and  after giving a label at each level we put a dot and then go up to the next level this is how we name hosts  refer slide time  8  35-9  04  slide time  8  35-9  04  the domain name for a host is the sequence of labels that lead from the host to the top of the worldwide naming tree  a domain is a subtree of the worldwide naming tree so  mit is a domain  ernet is a domain and so on there is a subtree under ernet and there will be many sub domains so any domain is a subtree of the worldwide naming tree  refer slide time  9  05-9  25  slide time  9  05-9  25  a host has a domain name specified using a sequence of names  each of which may be up to 63 characters long  separated by periods  names are case sensitive  a domain is an absolute domain name or a fully qualified domain name  fqdn   if it ends with a period  refer slide time  9  26-10  41  slide time  9  26-10  41  most generic domains  .com  .edu  etc  are international  but there are some like .gov for government and .mil for military are us-specifications so when you say .gov  it actually means us government and .mil is for us military if you remember  the history of internet started in usa from the arpa net and it evolved at that time  it was very us centric but now the whole world has embraced it  new top level domains recently been proposed  though they are not very popular yet  countries each have a top level domain  2 letter domain name   for example  in for india  jp for japan  uk for uk and so on  a system is required to map the domain names to ip addresses just like we have arp for ip address to mac or rarp for mac to ip address  we need a system to map domain name to ip address that is the chief one and may be the reverse also  refer slide time  10  42 12  45  slide time  10  42 12  45 implementing dns   distributed database implemented in hierarchy of many name servers this is distributed because this is widely used and you can not have a centralized database centralized database will be much more difficult to administer  contribute to a single point of failure and the network traffic at that node will be tremendous you will not be able to handle the network traffic if everybody tries to login to the same central name server that will not scale and it needs to be distributed later on we will study the way it is distributed and the way it is administered what are distributed are the name servers  servers with gives you the mapping from the domain name system to the ip address    there is an application-layer protocol host  routers  name servers  which sort of all combine and communicate to resolve names  provide this address/name translation   o note  core internet function is implemented as application-layer protocol o complexity at network  s  edge  so that at the network  s core where the traffic is very high and very heavy this is not present if you had to do the domain name servers at the core routers then it will be of much strain on the routers so they have been put to the edge of the network  refer slide time  12  46-14  29  slide time  12  46-14  29 full resolver  for resolving we use resolvers  and there are full resolvers and stub resolvers  the client for this naming system is called resolver this is transparent to the user and is called by an application to resolve names into real ip-addresses or vice versa when you type a domain name in your browser  the browser is a client application program for http and so its calls the resolver to translate these domain names to corresponding ip addresses the browser will send the http request to the web server using that ip address so it gets it from the resolver  a full resolver is a program distinct from the user program  which forwards all queries to a name server for processing it knows about the name server  responses are cached by the name server for future use  and often by the name server the local full resolver in your host may connect to the local name server if the local names server does not have the resolution  it is going to contact other name servers and finally will get the resolution we will look into the details later  refer slide time  14  30-15  19  slide time  14  30-15  19 in this diagram  the user program gives a user query to the full resolver the full resolver gives this query to the name server this name server has its own database  so it may look up in its own database if it is not there then it may send the request to foreign name servers and finally it will give a response the full resolver also maintains a cache and it will cache this response so that if it gets another request to resolve the same name then it can find from the cache and give the response to the user name server also maintains its own cache  refer slide time  15  20-16  16  slide time  15  20-16  16  a stub resolver is a routine linked with the user program which forwards the queries to a name server for processing responses are cached by the name server but not usually by the resolver there are two differences between a stub resolver and a full resolver one is that  full resolver has a cache but stub resolver usually does not  and secondly  stub resolver has to be linked with the user program where as the full resolver runs by itself  on unix  the stub resolver is implemented by two library routines  gethostbyname   and gethostbyaddr   for converting host names to ip addresses and vice versa  refer slide time  16  17-16  34  slide time  16  17-16  34 stub resolver is a part of the user program which gets linked and this routine sends the query to the local name server and gets the response and passes it on naturally the user program gets it  refer slide time  16  35-20  04  slide time  16  35-20  04 dns organization  is a distributed organization  distributed database o the organization that owns a domain name is responsible for running dns server that can provide the mapping between hostnames within the domain to ip addresses o so some machine run by say an organization like rpi is responsible for everything below within the rpi.edu domain o there is one primary server for a domain  and typically a number of secondary severs containing replicated databases so  it is important to understand how this is organized this is a global system  which means this is very big so any effort to control this centrally would become quite difficult let us say csc.iitkgp.ernet.in is a domain now .in is a domain  which is for whole of india there is one specific organization  in india  which looks after this domain so whatever sub domains are there under this domain  it is the responsibility of that organization to keep track of who all can be given sub domains that is  who all can be given names within these sub domains.in also  any name resolution query coming from anywhere else in the world will direct their query first to this organization who is maintaining the domain .in also there are so many organizations maintaining their own sub domains under the domain .in either that organization will have it in its cache or it will forward the query to the particular domain suppose if it gives sub domain ernet  ernet again gives sub domains to iitkgp and again under iitkgp there may be many sub domains but ernet does not really bother about whatever sub domains are there under iitkgp it only has to know that iitkgp is an organization under ernet which has got a name and a separate domain it is for iitkgp to decide how it is going to break this domain into further sub domains or it may not break at all this way the entire domain administration is decentralized and it is easy  and also these names are some kind of addresses that has been made unique since the names are broken into names which stand for actual organization at any level  naturally any particular organization administering a domain will not have two organizations having the same sub domain name under it this way all the names automatically become distinct which is another good advantage  refer slide time  20  05-20  54  slide time  20  05-20  54 why not centralize dns ?  single point of failure  traffic volume  distant centralized database should not work  maintenance should be a problem  does not scale so new server so its distributed server  no server has all names to ip address mappings local name servers   each isp company has local name server  default name server   host dns query first goes to the local name server authoritative name server   sometimes we may come across something that has been given by authoritative server  for a host stores that hosts ip address  name  can perform name/address translation for that host  s name  refer slide time  20  55-21  27  slide time  20  55-21  27 there are some root name servers the root name servers are actually still mostly us centric and have some of the biggest name servers so  root name servers are all in usa but of course that depends on what root it is and therefore they could also be distributed  refer slide time  21  28-21  54  slide time  21  28-21  54 nslookup   nslookup is an interactive resolver that allows the user to communicate directly with a dns server  from the os  you can use this nslookup and give a name query so this is actually name/server look up this is why it is called nslookup  nslookup is usually available on unix workstations  refer slide time  21  55-22  12  slide time  21  55-22  12  servers handle request for their domain directly  servers handle request for other domains by contacting remote dns server  s   servers cache external mappings  refer slide time  21  13-23  43  slide time  22  13-23  43 server operation   if a server has no clue about where to find the address for a hostname  it asks the root server  the root server will tell you what name server to contact  a request may get forwarded a few times let us say the iitkgp has a name server a request for a particular domain name translation has come to it it does not know to whom to connect the name server so it can always transfer it to the next higher level  namely  ernet if the ernet also does not know where in .in it is there  then it can contact in in will definitely know all the sub domains under it it has to know because it is administering that domain if it is for some address which is from outside you can send it directly to the root of that particular domain suppose from india you trying to contact something for japan  you can send it the jp root name server and then jp root name server would know which name server to contact dns queries will go back and forth few times and finally the name will be resolved  refer slide time  23  44-24  03  slide time  23  44-24  03 server  server communication   if a server is asked to provide the mapping for a host outside it  s domain  and the mapping is not in the server cache   o the server finds a name server for the target domain o the server asks the nameserver to provide the host name to ip translation  to find the right nameserver  once again it can use dns  refer slide time  24  04-25  20  slide time  24  04-25  20 recursion   a request can indicate that recursion is desired  this tells the server to find out the answer  possibly by contacting other servers   if recursion is not requested  the response may be a list of other name servers to contact so there are two versions of this dns server  the recursive server and iterative server if the recursion is requested and that is honored  suppose i am a name server  i will send the request to a name server whom i think may have a clue about how to do this request recursion now  he will make all the other contacts  the next level or may be another level contact as necessary and finally send me the answer so this is the recursive version in the iterative version what happens is that  he will simply give me a list of name servers to whom i can possibly contact directly to find out more then i make some more requests and finally get the name resolved  refer slide time  25  21-25  48  slide time  25  21-25  48 for example  host1 makes a request to the local names server then the local name server may send the request for recursion to another name server so  it will make the necessary request  get the response and then give the response  refer slide time  25  49-26  13  slide time  25  49-26  13 in the iterative case  it knew in one shot but it may not be in one shot when recursion is requested this may go deep down and finally find out and then send the request  refer slide time  26  14-26  37  slide time  26  14-26  37 whereas in the iterated queries  he gives a query and gets some list  he gives it to the next name server and gets some more response then finally he will get it and send it for you and then it will be resolved this is an iterated query  refer slide time  26  38-26  59  slide time  26  38-26  59 dns caching and updating of records   once  any  name server learns mapping  it caches mapping  cache entries timeout  disappear  after some time  update/notify mechanisms under design by ietf  how the cache is to be managed   refer slide time  27  00-27  46  slide time  27  00-27  46 we will not going into all the details  although they are given in the slide these are the so called resource records or rr the rr  resource records  have a format type = a where the name is hostname and value is ip address this is the most common one type = ns where name is domain and value is ip address of authoritative name server for this domain type = cname where name is an alias name for some canonical  the real  name and value is canonical type = mx is for mail server records  so these are the various resource records which are handled by the dns  refer slide time  27  27-28  02  slide time  27  27-28  02 this is the dns message format we have a header followed by some queries  followed by some resource records and some authority records if it is from alternative name server and may be some additional information this is the dns message format  refer slide time  28  03-28  18  slide time  28  03-28  18 we have in the message header  a 16-bit # for identification  16-bit # for query  and reply to query uses the same # and then we have a number of flags  refer slide time  28  19-28  44  slide time  28  19-28  44  qr flag identifying a query  0  or a response  1   op code is a 4-bit field specifying the kind of query  0 for standard query  query  ; 1 for inverse query  query   server status  status   other values are reserved for future use  refer slide time  28  45-29  10  slide time  28  45-29  10  aa  is authoritative answer that means this it is coming from an authoritative name server  tc  to see if the response has been truncated actually if it is truncated  it switches from udp to tcp  rd  recursion desired  ra  recursion available etc  rcode  is return code  refer slide time  29  11-30  19  slide time  29  11-30  19 both udp and tcp are used by name server   tcp for transfers of entire database to secondary servers  udp for lookups the lookups that we discussed about  usually use the udp protocol udp protocol is used as it is a very simple protocol with no extra over head but there is a limit to that udp and in response to udp you will just get one packet which is just 512 bytes  if the response requires more than 512 bytes  then the requester resubmits request using tcp if the message is truncated the flag is set and when the client sees the flag it opens a tcp connection with the corresponding name server and then resubmits the request so that it can get a longer request  refer slide time  30  20-30  46  slide time  30  20-30  46 we have already discussed the administration zones  a zone is a sub tree of the dns tree that is independently managed o second level domains  ernet.in  are usually an independent zone o most sub domains  iitkgp.ernet.in  are also independent independent means that what happens under this sub-domain is their business o eg  most universities have departmental domains that are then independently administered  refer slide time  30  47-32  24  slide time  30  47-32  24  a zone must provide multiple name servers this server records the members in the domain  you typically need a primary name server and one or more secondary name servers  secondary retrieves information from primary using a zone transfer  using a tcp connection the reason why the secondary name server is required is sometimes more than one secondary server is kept and the reason is that it is very vital for everybody suppose if you are administering a domain and you want to be independent then you can create extra sub domains under you where you have to maintain your own name server you can not maintain just one name sever but you have to maintain multiple name servers so that when the primary name server fails the secondary can immediately take over so  from time to time the primary will cache the data and from time to time or the database will be shifted to the secondary one so that they remain more or less in sync therefore as soon as primary goes down the secondary can start acting as a primary and be the authoritative name server for this particular domain there will be requests from other people to you for ip addresses in your domain and you are bound to give that that is the reason you need to have these different name servers and good network connection to handle all these requests  refer slide time  32  25-33  02  slide time  32  25-33  02 there is a reverse query which is sometimes used as a sort of weak protection against spoofing  set q = ptr i.e ptr is the reverse query for lookup so here what is done is that  an ip address is given and the domain name is found out just as we have arp and rarp similarly we have query and reverse query  refer slide time  33  03-33  42  slide time  33  03-33  42 reverse queries are used as a weak mechanism to avoid spoofing this is weak because those machines may have a name but if it connects to the network through a modem and is given a dynamic ip address then sometimes it may not get reflected so sendmail uses an identity message to identify the sender and receiver but this can easily be spoofed  refer slide time  33  43-35  18  slide time  33  43-35  18  some mail servers do not forward mail if you are not in the domain this is for anti-spam spams are mails which are spuriously generated mostly by programs/machines which are automatically generated and sent to millions of people it is estimated that the majority of the mails which go through are actually spam if the mail is not for my domain then i am not going to relay this to other mail servers sometimes you can do that but sometimes you have to relay because it is your responsibility to relay and does not accept mail if it can reverse query your ip address as soon as a mail comes it will find out from which ip address this mail has come from and send a reverse query by reverse query the name looks responsible may be it is coming from some known university etc then accept the mail if you can not reverse query it then maybe it is coming from some spurious source and it does not accept  this is not totally secure because hosts on same physical network can spoof ip headers of domains which are sort of respectable  but anyway this maybe of some use  refer slide time  35  19-35  35  slide time  35  19-35  35 who manages the in-addr arpa space ?  when you get a portion of the ip space you also become responsible for handling the in-addr.arpa queries for that space  this is why queries are in reverse order  refer slide time  35  36-35  57  slide time  35  36-35  57 dynamic dns   dns maps domain names to specific ip addresses o this requires that each domain name is statically assigned  since the zone table is typically stored on disk  this implies that a host using a dynamically assigned ip addresses means you connect to that host  refer slide time  35  58-36  34  slide time  35  58-36  34 this is what dynamic dns is for this is not widely implemented but there is an rfc for this and it uses a secure connection for doing a dynamic dns update you have to sort of update the name server database and anybody can not use this name server and so you have to use a secure connection to update the dns database  refer slide time  36  35-37  58  slide time  36  35-37  58 next  we take up the topic of directories this is similar to dns but more general actually the directories sometimes uses dns many of you have used mails and email clients where you have your own address book from where you can look up the email address of those who have sent you mails one example of a directory service could be some kind of a global address book where we can find the email address of anybody this was the original intention for creating the directory but of course people who produce spam mails have sort of killed this idea so nobody wants his email address to be in a directory which is universally available so that everybody can send junk mails anyway  directory can be used for many other purposes as well  refer slide time  37  59-38  48  slide time  37  59-38  48  what directories are ? o they are object repositories o typically read more than written o have explicit access protocols o support relatively complex queries dns queries are simple but it supports relatively complex queries for example  you can have something like give me the email addresses  assuming that email addresses are still available  of all people who live in delhi whose name contains ram etc you may be looking for somebody so you can have more complex queries than you have in a dns  refer slide time  38  49-39  03  slide time  38  49-39  03 but directories are not meant to be rdbmss  they are just for looking up so  lack notions of tabular views  join operations  stored procedures etc they are not regular rdbms but they are just for this particular service  refer slide time  39  04-39  41  slide time  39  04-39  41 x.500 was originally how directory was envisaged by the telecom industry  the goal was to have global white pages o lookup anyone anywhere o developed by telecom industry o iso standard for osi networks  idea was distributed directory o application uses distributed directory structure o application uses directory user agent to access a directory access point  refer slide time  39  42-39  53  slide time  39  42-39  53 the picture is something like this ; you have a directory user here who uses a directory user agent which in turn connects an access point to the directory  refer slide time  39  54-40  39  slide time  39  54-40  39  how is the name used ? o access resource given the name o build a name to find a resource o information about resource o these are the different uses of names  do only programs look at these names ? sometimes humans also need to use the names for constructing the names recalling names  is resource static ? o sometimes resource may move o change in location may change the name of a particular resource  performance requirements o human scale  refer slide time  40  40-41  12  slide time  40  40-41  12 directory information base which is defined in x.501 is given as a tree structure o root is the entire directory o levels are groups for ex  country  organization  individual  entry structure o unique name build from tree o attributes  type/value pairs o schema enforces type rules  there may be alias entries also  refer slide time  41  13-41  53  slide time  41  13-41  53 directory structure may look something like this you have these different levels  which starting from the top may represent some organizations and then some sub organizations and finally you have the objects now  in an object entry you will have some names and each of these names should have a series of type value pairs there may be more than one particular name ; it may have a number of attribute and each attribute will have a type/value pair this is the general structure of a x.501 tree structure  refer slide time  41  54-42  28  slide time  41  54-42  28  query is to this system defined in x.511 o query is a read  get selected attributes of an entry o compare does an entry match a set of attributes ? o list children of an entry o search  abandon request etc o these are all kinds of queries are possible  modification you can modify these records  add  remove  modify entry o modify distinguished names etc  refer slide time  42  29-43  14  slide time  42  29-43  14  there is a directory system agent o it may have some local data o can forward request to other system agents o can process requests from user agents and other system agents so  these are like the name server system with dissolvers and also this is the system using directory system agents which can do the query processing it may get the data locally or it may forward the request to other systems  referrals  o if dsa can not handle the request it can make request to other dsa just as you can make iterative and recursive queries in dns o or tell dua to ask other dsa  this is the iterative process  refer slide time  43  15-44  14  slide time  43  15-44  14  directory information can be protected actually they are usually protected  there are two issues  o authentication defined in x.509 o access control defined in x.501  this directory by itself does not give you security so you have to have other components in order to ensure security but a directory can be used for some authentication services  some security purposes  etc directory can very well be used they are actually used that way  standards specify basic access control and individual dsa  s can define their own access control they can specify to whom they are going to allow access to their local databases  refer slide time  44  15-44  59  slide time  44  15-44  59 replication  this is defined in x.525  single entries can be replicated to multiple dsas just like you have a primary name server and a number of secondary name servers  similarly you can have a directory in a primary or master and then you can have replication  two replication schemes  o cache copies  on demand o shadow copies  agreed in advance from time to time  there is a transfer  copy required to enforce access control o when entry sent  policy must be sent as well  modification is done at the master only  copy can be out of date o how to handle that is defined in x.525  refer slide time  45  00-45  30  slide time  45  00-45  30 there are a number of protocols which are defined in x.519  directory access protocol means the structure of the query and other things would be defined  directory system protocol o request/response between dsas  directory information shadowing protocol o dsa-dsa with shadowing agreement  directory operational binding management protocol there are a number of protocols  refer slide time  45  31-46  07  slide time  45  31-46  07  uses are of course for look-up o attributes  not just distinguished name o context  humans can construct likely names  browsing  yellow pages o aliases also may be given  search restriction/relaxation may be there  groups may be defined that means having a number of members who will be having they are own attributes  authentication information that may be contained in the directory and so on o directory may be used for various purposes  refer slide time  46  08-47  20  slide time  46  08-47  20 we will look at ldap  which is   the lightweight directory access protocol designed by the same people who designed osi x.500 it actually tends to be a little complex  it is heavy and intuits the osi all the seven layers now for the internet purpose which actually uses the tcp/ip stack rather than the seven layer osi stack there was a lightweight directory access protocol which can interoperate at least on one side  that ldap can use that x.500 directory service but this is much simpler than x.500 and ldap is used in many places  this is a lightweight directory access protocol o supports x.500 interface o doesn  t require the osi protocol this uses the tcp/ip protocol o so this is x.500 for the internet crowd  useful as generic addressing interface  like netscape address book  etc  refer slide time  47  21-47  35  slide time  47  21-47  35  the ldap or lightweight directory access protocol is a networking protocol for querying and modifying directory services running over tcp/ip  an ldap directory usually follows the x.500 model  refer slide time  47  36-48  11  slide time  47  36-48  11  it is a tree of entries  each of which consists of a set named attributes with values  an ldap directory often reflects various political  geographic  and/or organizational boundaries depending on the model chosen when you do that you can also define your security policies based on this directory and based on this boundary  especially authentication service  refer slide time  48  12-48  21  slide time  48  12-48  21  a directory is a tree of directory entries  an entry consists of a set of attributes  an attribute values pair  the attributes are defined in the schema  refer slide time  48  22-49  58  slide time  48  22-49  58 this would be the protocol stack for ldap you have a directory based application  some authorization service or access to some information which may be there for the organization which uses ldap ldap may use tls  transport level security   actually you could use ssl also in future we will give one lecture to security because it has become so important now  in an organizational context  a directory may be an important component of the entire security arrangement security is the complex issue but for ldap we require that we communicate securely in many cases and many ldap implementations support this tls transport level security we can also use ssl or sasl and these uses tcp tcp sits on the ip which sits on the other layer etc so it comes in between the directory based application and the tcp layer this is where it stands in the protocol stack  refer slide time  49  59-50  50  slide time  49  59-50  50 this mentions some of the operations which are in ldap    bind  authenticate and specify ldap protocol version this actually starts the process  start tls  protect the connection with transport layer security to have a more secure connection since you are giving some access to information  you need to put some security feature in that so that is the start tls  search  search for and/or retrieve directory entries if you go through the access controls then you can search for some records  refer slide time  50  51-51  08  slide time  50  51-51  08  compare  test if a named entry contains a given attribute value  you can add a new entry  you can delete an entry  you can modify an entry these are various ldap operations  refer slide time  51  09-51  37  slide time  51  09-51  37  modify dn  move or rename an entry  abandon  abort a previous request  extended operation  generic operation used to define other operations  unbind  close the connection  not the inverse of bind but anyway this is to close the connection  so these are roughly some operations in lightweight directory access protocol  refer slide time  51  38-53  18  slide time  51  38-53  18 we will just touch on this security issue   notion of security for a network protocol is comprised of at least these axes   identity and authentication  o who are you and who says so ? so identify yourself and then it should have some protection against spoofing that somebody is claiming to identity which is false  confidentiality  whatever information is being passed around  other people should not be able to snoop into it so confidentiality is important you might use some kind encryption for this confidentiality purpose  integrity  did anyone muck with this data ? this means  did any one change the data ? if there is a change which has done by some person who is authorized to do that you might want to keep a log or audit trail for such changes otherwise you want to be sure that on the way somebody has not changed this data it may be necessary for some applications to maintain some signature kind of a thing to see that the data has not been changed  authorization  yes  you can do that  but no  you can not do that other thing this means there some organizations in access control user these identity  confidentiality  nativity and authorization kind of accesses are definitely there  refer slide time  53  19-53  44  slide time  53  19-53  44  one needs to separately consider each of the four security axes in the context of anticipated threats  also need to consider security from the perspectives of o the information stored in the directory  and o attributes of the requesters  data security is not equal to access security  refer slide time  53  45-55  06  slide time  53  45-55  06  some typical security features of ldap implementations   simple password based authentication  ssl on a particular port 636  ssl is secure socket layer and tls is transport layer security this is on port 389  there is some access control  there is some configurability there are other things that you can do with the directory especially in the context of an organization therefore because of spams and other issues the idea of actually generating a very global white pages for everything under sun  such a grand idea did not really work out but in the context of specific organization with its own security boundary and its own needs  ldap is a well defined is a protocol which can be used good day the topic for today is congestion control  refer slide time 55  07-56  18  slide time  55  07-56  18 the performance of computer networks on a large extent depends on the kind of congestion present in the network so  actually this is a large topic and we will just touch upon some aspects of them one thing we know by now is that  in general network namely data network or internet in particular  although multimedia and other content are coming in  first of all this is a packet based network and a large data network where we make only best effort of delivering a packet what exactly do you mean by best effort ? most of these best efforts have to do with how we handle the congestion ?  refer slide time 56  19-56  52  slide time  56  19-56  52 what is congestion ? when too many packets are pumped into the system congestions occurs leading into degradation of performance here you can give selective acknowledgement that you have got something in particular but have not got that issue  reno and new reno retransmit at most one lost packet per round-trip time selective acknowledgement  the receiver can acknowledge non continuous blocks of data which means sack selective acknowledgement of 0 to 1023  1024 to 2047 and so on  refer slide time 56  53-57  17  slide time  56  53-57  17  multiple blocks can be sent in a single segment  tcp sack  o enters fast recovery upon three duplicate acks o sender keeps track of sacks and infers if segments are lost sender retransmits the next segment from the list of segments that are deemed to be lost  like fast retransmit  refer slide time 57  18-58  22  slide time  57  18-58  22 many people have tried various kinds of heuristics to improve the performance of tcp there are two competing demands one is that we have to maximize the throughput and if you can maximize the throughput  naturally the overall delay  congestion will be small and at the same time you will get your job done faster but in order to push this maximum throughput we should not get into congestion or absolutely no congestion collapse so we try to guard against that these are the various versions or various flavors of tcp for doing that so  we are going to look at some topics associated with this congestion control and one is traffic engineering this means  can you shape or handle your traffic in a particular way so that congestion is less likely to occur  refer slide time 58  23-58  53  slide time  58  23-58  53 all these build to give quality of service in a network at routers  these may depend on packet classification and packet scheduling at network entrance it may have to do with traffic conditioning at routers or somewhere in the network you may do admission control between hosts and routers you may do signaling therefore these are the different components of qos network computer networks prof.sujoy ghosh department of computer science and engineering iit  kharagpur lecture-35  refer start time 00  42-57  33  good day the topic for today is congestion control  refer slide time 00  48-02  00  slide 00  48-02  00 the performance of a computer network depends on a large extent on the kind of congestion that is there in the network once again this is a large topic we will just touch upon some aspects of them one thing we know by now is  in general network  may be data network or inter network in particular although multimedia and other contents are coming in  this is a packet based network and a large data network that is announced where we make only the best effort of delivering a packet now  what exactly do you mean by best effort ? most of these best efforts depend on how you handle congestion  refer slide time 02  01-02  38  slide 02  39-04  18 so  we know what congestion is when too many packets are pumped into the system  congestion occur leading into the degradation of the performance congestion tends to feed upon itself and backs up congestion shows lack of balance between various network components moreover it is a global issue because the congestion may happen in some intermediate router because of packets being pumped from various sources  so in that sense it is a global issue  refer slide time 02  39-04  18  slide 04  19-04  45 we have this intermediate node or channel etc and the demand is in the form of various sources pumping in packets at various rates namely ? 1 to ? n and this is being serviced by the channel capacity or the router capacity at the rate ? and going to the various destinations the problem is  the demand outstrips available capacity so this is basically the congestion problem and added dimension to this problem comes from the fact that although these ?  ? etc are coming from the general queuing theory and for simple queuing theory these are the arrival rate and service rates etc although queues are there  but in general the statistics which the data networks follow are rather complicated so traditionally telecom networks would follow some poisson distribution with exponential interval time but in data network it is seen that it follows something called as a self similar traffic or heavy tail distribution and that is a complex distribution one of its key features is bustiness that means data tends to come in busts and then there is comparatively long quotient period and again another burst comes so  that is the problem which has to be handled and when several bust arrives at a node at the same time that particular node may get overloaded  refer slide time 04  19-04  45  slide 04  46-05  43 so  if information about ? 1  ? ? etc is known in a central location where control of ? n and ? effected instantaneously with zero time delays then the congestion problem is solved unfortunately we can not do that because we have incomplete information and we require a distributed solution with time varying time delays this is what makes the problem a little difficult  refer slide time 04  46-05  43  slide 05  44-06  12 already we have seen this kind of throughput versus load curve we have seen already when we saw alocha networks  ccna cd etc but  in general this is what happens as the load increases the throughput also keeps on increasing at the same rate and then it starts sort of going down and it keeps going down because of the intermediate delays and other bottlenecks coming into the picture and then there is an area where the throughput does not increase any longer if you increase the load beyond that there is a catastrophic fall in the throughput so this part is known as the knee and this part is known as the cliff and this catastrophic fall is called congestion collapse  refer slide time 05  44-06  12  slide 06  13-06  42 so  knee is the point after which throughput increases very slowly and delay increases fast cliff  point after which throughput starts to decrease very fast to zero and this is congestion collapse and delay approaches infinity note in an m/m/1 queue delay = 1/  1 utilization   it does not follow this kind of a simple formulation  refer slide time 06  13-06  42  slide 06  43-07  03 if this was the previous curve and as you plot the delay  the delay is low in the beginning and starts increasing and in this area where there are lots of packet losses and there is a congestion collapse and there are throughput collapses the delay also becomes very high and it becomes a hyperbolic curve so obviously you have to take all precautions not to fall into this area  refer slide time 06  43-07  03  slide time 07  04-07  45 so  we talk about congestion control whose goal is to stay on the left of cliff that means not to go into the congestion collapse congestion avoidance  goal is to stay left of the knee and right of cliff is of course the congestion collapse region  refer slide time 07  04-07  45  slide time 07  46-09  15 so  the goal of congestion control is to guarantee stable operation of packet networks and a sub goal is to avoid congestion collapse to keep networks working in an efficient manner  for example  high throughput  low loss  low delay high utilization are the goals not always achievable especially because we have distributed systems with insufficient information about the global picture but anyway that is there  refer slide time 07  46-09  15   slide time 09  16-10  47 to provide fair allocation of network bandwidth among competing flows in steady state so  there has to be some kind of fairness  sometimes all are not taken as equal first of all you must see  if there is congestion at an intermediate node  what would happen is that there would be lot of packet loss over there so  various packets from various sources would be lost and the delay would become high many of them were running a tcp protocol therefore the question is who would again start retransmitting hence what would happen is that  more packets would be lost and more and more packets will keep on getting pumped this is just like a traffic jam  it starts at one place and then if the jam does not resolve soon then it becomes bigger and bigger and it starts getting pushed towards the source so the overall network throughput goes down and people tend to push more packets these are the kinds of scenario we would like to avoid  refer slide time 09  16-10  47  slide time 10  48-13  18 now  there are various policies at various levels that we can take for congestion control let us look at the data link layer  the open loop policies one is retransmission policy how would you retransmit ? one example of this re transmission policy is  suppose you have a ethernet network with csmacd going on and then you have detected a collision  the question is  are you becoming persistent or non persistent or you do a random back off or exponential back off or what is your retransmission policy hence you will try again similarly there are other things like out of order policy out of order policy is when you receive a packet when it is out of order acknowledgement policy is  do you acknowledge or do not acknowledge it for example  if you have an acknowledgement then the acknowledgement for each packet also takes up resources so  if you acknowledge every packet then there is going to be as many packets sent as many acknowledgements therefore this is a lot of acknowledgement for the network and there is a high overhead may be you take a policy of not acknowledging all the packets it could be some kind of a flow control policy therefore we have seen some kind of flow control in tcp we will look into more details and variations of it  refer slide time 10  48-13  18  slide time 13  19-14  25 in the network layer you can have the virtual circuit versus datagram this is an important issue and we will look at it in more detail when we discuss qos and multimedia communication suppose there is a very important communication going on between two hosts  mission critical or whatever it may be  now they will be exchanging a lot of packets let us say the packets are flowing in only one direction so lot of packets will be sent and we want some premium service for this if you want to give a premium service to this particular pair of nodes  may be they pay more or something then in that case you have to distinguish among the packets and assume that they form some kind of a flow so you need to have a kind of virtual circuit between these two points in order to distinguish them if all packets are on their own then that is a different kind of a situation where it will be more difficult to distinguish the flow between the two specific nodes virtual circuit versus datagram may be an important issue once again we will see more details of this when we look at rsvp  diffsservent in the qos when we discuss qos a little bit more in detail packet queuing and service policy means  in the router a number of packets may come and they are going to be serviced one by one and they are going to be put in a queue now the question is  do you put them in one queue or do you put them in several queues ? do you have the same priorities for all the queues or do different priorities for different queues and so on packet discard policy has to do with the buffer management of the router if the buffer becomes full  which packet do you drop ? routing algorithm  what kind of routing algorithm will you use ? packet lifetime management  this means the lifetime of the packet is over and you drop the packets these are important in the network layer so far we were discussing about open loop polices  refer slide time 13  19-14  25  slide time 14  26-15  26 now let us take a look at closed loop control this means monitoring the system to detect when and where congestion is occurring pass the information to places where actions can be taken adjust system operation to correct the problem this is more sophisticated and better the point is  if one router in between is congested and now if it can identify the chief sources of trouble where it can not handle a lot of packets  therefore if you could send a feedback back to the source so that he can control this behavior by sending less number of packets then the situation can be handled or you can adjust some system parameters  may be the window size in tcp etc these are the examples of the kind of thing you can do with close loop control  refer slide time 14  26-15  26  slide time 15  27-16  07 if you say that there is some congestion then we need to have some metrics for measuring congestion some examples are percentage of all packets discarded due to lack of buffer space this may be one measure average queue length in the buffer no of packets that time out and are retransmitted average and standard deviation of packet delay these may be metrics with which you measure congestion so  if these metrics go beyond a certain level then you might decide that some congestion is taking place and you need to take some action in order to prevent the performance degradation in a sharp manner  refer slide time 15  27-16  07  slide time 16  08-16  44 feedback mechanisms  it can be many as we have mentioned  router on sensing congestion sends a control packet to the source a bit in every packet can be reserved to announce congestion explicit probe packets can be sent to ask about congestion implicit algorithms make only local observations  refer slide time 16  08-16  44  slide time 16  45-17  56 then you can try adjusting system operations adjust time constants to a near optimal value decrease the load selectively if possible may be if one source somehow can decrease then all the others can be served very well because it goes below a threshold increase resources if possible  this is usually difficult   refer slide time 16  45-17  56  slide time 17  57-18  29 now  let us look at the one aspect of congestion control which is very important and which is done by tcp all the time tcp congestion control  if you remember uses a sliding window protocol we have a window and a sender can send right up to the window size to the other side and it will wait for acknowledgements and he will keep on acknowledging and once he gets the acknowledgement the window will slide this is about the basic tcp what we are going to see now is some variance of tcp since tcp is a very important protocol and application protocols like ftp  stp  etc use tcp  a lot of important traffic on the net is actually carried on tcp and that is why whatever we do at the tcp level is very much important and one of the chief tool for doing any congestion control by tcp is by adjusting the window size so  there are various variants  refer slide time 17  57-18  29  slide time 18  30-19  16 tcp has a mechanism for congestion control the mechanism is implemented at the sender the sender has two parameters  congestion window with a variable called cwnd and slow start threshold value with a variable called ssthresh so  initial value is the advertised window size so  with a tcp connection there is an advertisement of window size and this window size is taken as the initial ssthresh value  refer slide time 18  30-19  16  slide time 19  17-20  06 congestion control works in two modes one is slow start and the phase is slow at start when the cwnd value is less than ssthresh and congestion avoidance means that cwnd value is greater than equal to ssthresh so  basically we are trying to figure out whether we are on the left of knee or in the right of the knee so  if you are on the right of the knee but left of the cliff we are going to be careful if you are on the left of the knee and if things are going fine then we can try to increase the load stress to increase the overall throughput this is the basic idea  refer slide time 19  17-20  06  slide slide time 20  08-21  17 knowing initial values in a slow start i.e set cwnd = 1 naturally if the window size is small i.e one so one unit will go and the acknowledgement will come back and then only something else will go from this side that is why we are being very conservative and we are sending only a small bit of information note  the unit is a segment size i.e one of a second tcp is actually based on bytes and increments by 1 mss  maximum segment size   the receiver sends an acknowledgement  ack  for each packet so this is the slow start so  the receiver must acknowledge every packet  so the first packet it receives it can send an acknowledgement note  generally a tcp receiver sends an acknowledgement  ack  for every other segment  refer slide time 20  08-21  17  slide time 21  18-21  35 each time an ack is received by the sender  the congestion window is increased by 1 segment so what happens is that  the sender has sent one packet so it has got the acknowledgement  so actually the sender decides that things are fine and may do better that means it is the increase in congestion window size  cwnd   so we increase cwnd by 1 i.e cwnd = cwnd + 1 we make cwnd = 2 if an ack acknowledges two segments cwnd is still increased by only 1 segment that means for every ack it increases by 1 if it acknopwledges onlys one segment or two segments then cwnd is increased by one only actually the reason to acknowledge every other segment is to decrease the number of acknowledgements now  even if ack acknowledges a segment that is smaller than mss bytes long cwnd is still increased by one so  at anytime you get an ack you increase cwnd by one when you are in the slow start phase although it starts slowly does it increment slowly ? not really in fact  the increase of cwnd is exponential  refer slide time 21  18-21  35  slide time 21  36-22  24 the congestion window size grows very rapidly  cwnd rises very rapidly for every ack we increase cwnd by 1 irrespective of the number of segments ack ? ed the tcp slows down the increase of cwnd when cwnd > ssthresh   refer slide time 21  36-22  24  slide time 22  25-23  00 as you can see  suppose if it sends one segment it receives one acknowledgement  the cwnd is increased from one to two now you can send two segments  segment two and segment three it will get back the acknowledgement for segment two and acknowledgement for segment three now cwnd has become 4 it will send 4  5  6  etc  so three of them it has sent and the acknowledgement for 4  5  6 will come now cwnd has become 7 and it will send more you can see here  1  2  4  7 is increasing quiet fast because for each acknowledgement it is increased by one and when you are sending so many segments at a group you will get many acknowledgements therefore cwnd is increasing exponentially  refer slide time 22  25-23  00  slide time 23  01-23  21 congestion avoidance phase is started if cwnd has reached the slow start threshold value  ssthresh   if cwnd > = ssthresh then each time an ack is received  increment cwnd as follows  i.e cwnd = cwnd + 1/  cwnd  where  cwnd  is the minimum or the larger integer and is smaller than cwnd so this is only increased by a fraction while sending of course you will not send a fraction and whatever be the current cwnd value that is floured that many segments you can send  refer slide time 23  01-23  21  slide time 23  22-23  42 so  cwnd is increased by one only if all cwnd segments have been acknowledged that means  if all the cwnd have been sent or acknowledged  then cwnd increases only by one so we are very cautious while we move to the right of the knee  refer slide time 23  22-23  42  slide time 23  43-24  26 so  assume that ssthresh is 8 therefore what will happen is  round-trip time = 2  4  6  etc as time is going so cwnd is first increased exponentially it reaches the ssthresh value and then it increases slowly  refer slide time 23  43-24  26  slide time 24  27-25  02 tcp assumes there is congestion if it detects a packet loss now  what is the response to congestion ? tcp assumes that if congestion detects a packet loss a tcp sender can detect a lost packet via timeout of a retransmission timer or receipt of a duplicate ack duplicate ack has been received which means that previously may be some acknowledgement has been dropped and there is a duplicate ack so  when something is dropped it means that there may be congestion now there are different ways to respond to this congestion  refer slide time 24  27-25  02  slide slide time 25  03-25  20 one is  tcp interprets a time-out as a binary congestion signal which means there is congestion as soon as there is a timeout therefore when the sender performs cwnd is now reset to one i.e cwnd = 1 so once again it becomes very conservative ssthresh is set to half the current size of the congestion window ssthresh = cwnd/2 before sending it to one whatever be the cwnd value you divide it by two and make it the new threshold value and enter the slow start again  refer slide time 25  03-25  20  slide time 25  21-25  36 so initially  cwnd is equal to one i.e cwnd = 1 and ssthresh = advertised window size new acknowledgement  ack  is received  if  cwnd < ssthresh  / * slow start * / cwnd = cwnd + 1 ; else  refer slide time 25  21-25  36  slide time 25  37-25  58 congestion avoidance cwnd = 1/cwnd + 1/cwnd cwnd = cwnd + 1/cwnd if there is timeout it is multiplicative decrease i.e ssthresh = cwnd /2 and cwnd = 1  refer slide time 25  37-25  58  slide time 25  59-26  32 this is the typical plot of cwnd for a tcp connection  mss = 1500 bytes  with tcp tahoe  tcp tahoe is one flavor of tcp we have been discussing we will discuss about some other flavor also so  if cwnd goes on increasing  decreasing and then after sometime again increasing while things are good then this may be a typical plot  refer slide time 25  59-26  32  slide time 26  33-27  36 tcp tahoe uses one flavor  slowstart for every acknowledgement congestion avoidance that means only beyond the ssthresh it increases slowly fast retransmit in tcp reno there is also another version of tcp uses fast recovery and then there are some versions like new reno  sack,red  etc  refer slide time 26  33-27  36  slide time 27  38-28  42 acknowledgements in tcp  receiver sends acknowledgement  ack  to sender acknowledgement is used for flow control  error control and congestion control in error control if the acknowledgement is not received then you send a retransmit in congestion control we find that ack is used for controlling this ack number sent is the next sequence number expected delayed ack  tcp receiver normally delays transmissions of an ack for about 200 ms because it allows the packets to arrive thinking that it can send less number of acknowledgements this way  and acks are not delayed when packets are received out of sequence i.e a little out of ordinary  may be they came from two different paths  so you do not delay the ack but send it immediately  refer slide time 27  38-28  42  slide time 28  43-29  50 now fast retransmit  if you remember that the tcp reno uses fast retransmit if three or more duplicate acks are received in a row  the tcp sender believes that a segment has been lost this means acks have come meaning some earlier packets are gone this means  possibly the later packets or segments may be lost so what it does is  without waiting for the timeout to occur for this particular segment which has been sent it assumes that it has been lost and it sends one more again tcp performs a retransmission of what seems to be the missing segment without waiting for a timeout to happen and then it enters slow start that means it brings down the multiplicative decrease of ssthresh and sets the cwnd to one i.e ssthresh = cwnd/2 cwnd = 1 this is fast retransmit  refer slide time 28  43-29  50  slide time 29  51-30  18 in fast recovery the slow start is avoided after a fast retransmit that means after a fast retransmit intuition duplicate acks indicate that data is still getting through or at least the duplicate acks are through after three duplicate acks set retransmit lost packet  i.e decrease ssthresh to half so ssthresh = cwnd/2 but cwnd = cwnd + 3 and then you enter congestion avoidance so increment cwnd by one for each additional duplicate ack this is a fast recovery but then after this you enter the congestion avoidance that means  basically this is trying to tune the performance of tcp to get the maximum throughput without causing any congestion when ack arrives that acknowledges new data cwnd = ssthresh after that we enter the congestion avoidance so this is fast recovery  refer slide time 29  51-30  18  slide time 30  19-30  42 tcp reno  for duplicate acks it does fast retransmit and fast recovery fast recovery avoids slow start and if there is a time-out you retransmit and go to slow start tcp reno improves upon tcp tahoe when a single packet is dropped in a round-trip time but if multiple packets are dropped then of course the tcp reno can not handle that and for that we have a tcp new reno  refer slide time 30  19-30  42  slide time 30  43-30  58 when multiple packets are dropped reno has problems partial ack  occurs when multiple packets are lost a partial ack acknowledges some but not all packets that are outstanding at the start of a fast recovery  takes sender out of fast recovery the sender has to wait until time-out occurs  refer slide time 30  43-30  58  slide time 30  59-31  42 in new reno partial ack does not take sender out of fast recovery partial ack causes retransmission of the segment following the acknowledged segment new reno can deal with multiple lost segments without going to slow start  refer slide time 30  59-31  42  slide time 31  43-32  07 there is a selective acknowledgement  sack   here you can selectively acknowledge in an original tcp when you give an acknowledgement  that is the next segment you are expecting and all segments before that are acknowledged here you can give selective acknowledgement stating that you have got all these but not that particular one issue  reno and new reno retransmit at most one lost packet per round-trip time selective acknowledgement  the receiver can acknowledge non continuous blocks of data that means sack selective acknowledgement of 0 to 1023  1024-2047 and so on  refer slide time 31  43-32  07  ; slide time 32  08-33  12 multiple blocks can be sent in a single segment tcp sack enters fast recovery upon three duplicate acks sender keeps track of sacks and infers if segments are lost sender retransmits the next segment from the list of segment that is deemed to be lost like fast retransmit  refer slide time 32  08-33  12  slide time 33  13-33  43 to improve the performance of tcp  there are two competing demands here one is that we have to maximize the throughput and if you can maximize the throughput  naturally the overall delay  congestion etc will be small and at the same time you will get your job done faster but in order to push this maximum throughput we should not get into congestion  a collapse so we try to guard against that these are the versions or various flavors of tcp for doing that so  we have looked at tcp now we are going to look at some other topic once again associated with congestion control and the other one is traffic engineering that means  can you shape or can you handle your traffic in a particular way so that congestion is less likely to occur  refer slide time 33  13-33  43  slide time 33  44-34  00 all these build to give a quality of service in a network and at routers these may depend on packet classification and packet scheduling at network entrance it may depend on traffic conditioning at routers or somewhere in the network you may do admission control between hosts and routers you may do signaling so these are the different components of qos of a network  refer slide time 33  44-34  00  slide time 34  01-35  02 so  let us say you have a sender and receiver here  these are the intermediate routers then you can do the traffic conditioning at the edge of the network you can also do admission control here or somewhere else so  these are the different components  refer slide time 34  01-35  02  slide time 35  03-36  56 traffic conditioning mechanisms at the network boundary need to enforce that traffic from a flow does not exceed specification so  we will look later at what kind of specifications we are talking about  what kinds of things people may agree on  or negotiate about that what are the parameters but suppose from some source we had negotiated certain parameters and we find that the source is not sticking to that parameters and it is going out of that then we have to do some policing so  policing is a drop traffic that violates the specifications the specification as was agreed between the service provider and the sender shaping means the buffer traffic that violates specifications marking means mark packets with a lower priority or as best effort  if the traffic specification is violated  refer slide time 35  03-36  56  slide time 36  57-38  10 let us look at traffic shaping first regulating the average rate of data transmission allows control algorithms to work better so this is to be understood as i mentioned earlier  in computer network specifically data networks or internet traffic etc they are inherently very bursty in nature when it comes it comes in one big bunch and then for long periods there may be no traffic now the trouble is  if the burst peak to the average ratio may be as much as 1  1000  we have to accordingly design your buffer and other network provisioning so  we have to decide upon whether we are doing it for the peak or doing it for the average or may be doing something in between as designing for the peak if you design it for the peak everything works fine but that becomes very expensive and not practical in many cases you can not do it for the average also and that may be to lower so it may be somewhere in between therefore one inherent problem is the burstiness of the traffic now  if you could somehow make the burstiness smooth  then all your system will work much better one way of doing is to buffer it somewhere.the shape of the traffic is related to some statistics about data transfer rates as well as its sensitivity to error  delay jitter etc  refer slide time 36  57-38  10  slide time 38  11 one famous algorithm is the leaky bucket algorithm it is a single server queue with a constant service rate if you have a bucket which is leaking drop by drop that means water will come out at a constant rate therefore the same thing happens here if you have a single queue and then you service it at a constant rate this is the rate at which you are pumping the data into the network so  if there is a burst then it will get absorbed in your buffer at the edge so that in the core of the network the burst will not come and it will be more of a steady kind of a flow a steady average kind of flow is also something beyond the capacity of the intermediate nodes then of course the capacity of the intermediate nodes has to be increased that is the leaky bucket algorithm in short so the input buffer allows a bursty flow to be smoothed out to an even flow onto the network it may be implemented in hardware or the os operating system it may be implemented either in hardware or software  refer slide time 38  11  slide time 39  04-39  58 underutilized slots are written off by this what we mean is  the packets are being serviced by this network at a particular ray  let us say once every t unit of time now after another t unit of time it will try to service and finds that the buffer is empty so it will not send anything again after t unit of time and if something has arrived by that time it will send one packet the algorithm can work on the volume of the traffic rather than number of packets only problem here is  a somewhat slow response time for inherently bursty traffic which is quiet often in the node  refer slide time 39  04-39  58  slide time 39  59-41  19 one way to handle a little bit of burstiness is by a token bucket this again improves the throughput a little bit and it can accommodate burstiness to a certain degree we can not allow all kinds of burstiness because then the burstiness will flow into the core of the network where it will be more difficult to handle so this is the token bucket  it limits the input to specified burst size  b  and average rate  r   so traffic sent over any time t < = r * t + b  also know as linear bounded arrival process  lbap   so there is bound on an arrival process excess traffic may be queued  marked or simply dropped  refer slide time 39  59-41  19  slide time 41  20-41  44 so  tokens are generated for the buffer at a fixed rate which can be accumulated so this is the main point where is the token differs from the leaky bucket in the leaky bucket the underutilized slots were written off but here if your time comes you can get a token and you can collect and accumulate so many tokens and then when a burst comes up to that many tokens can be sent .the longer time average is helpless because there is a limit to the number of tokens you can really accumulate because after that you can not accumulate tokens anymore and at the same time a little bit of burstiness is allowed if your source is inherently bursty  and if you can allow some amount of burstiness that will improve the throughput so  for each token only one packet can be sent but tokens can be accumulated up to a certain maximum a variant is to allow k bytes per token essentially it allows bursts up to a regulated maximum length that is maximum number of tokens a leaky bucket may follow a token bucket also in order to make it absolutely smooth  refer slide time 41  20-41  44  slide 41  45-42  18 so this is the diagram  the bucket holds up to b tokens and there are so many tokens per second accumulating there and when a packet burst comes then it waits for the tokens if the tokens are not there then it can not send but if the tokens are there depending upon as many tokens that are available the tokens are removed and the packets are sent into the network  refer slide time 41  45-42  18  slide time 42  19-43  08 now having talked about this  let us just mention what are the kinds of traffic parameters that are important or that may be negotiated between the sender and the network service provider one could be maximum packet size that defines how big the packet is the token bucket rate  defines what the average rate is token bucket size defines how burst it will be maximum transmission rate tells us the exact maximum transmission rate  refer slide time 42  19-43  08  slide time 43  09-43  39 loss sensitivity  is this flow very sensitive to losses if you are just doing some file transfer it will be sensitive to losses  but if you are sending some voice it may not be that sensitive loss interval  at what interval it is a loss  if the loss is very bursty or if the loss has to be averaged out  etc burst loss sensitivity  packets   in terms of the number of packets minimum delay noticed  and maximum delay variation which are allowed these are again very important for multimedia traffic quality of the guarantee  is it just a best effort or better than the best effort is what it tells about these are the flow specifications of services  refer slide time 43  09-43  39  slide time 43  40-44  06 we will come to admission control and signaling in more detail when we discuss rsvp in the next lecture when we disscuss qos and multimedia traffic but just to mention it here  admission control is a function that decides if the network has enough resources admit new flow if enough resources are available reject the flow otherwise  refer slide time 43  40-44  06  slide time 44  07-44  20 you do some reservation of capacity through some protocol like rsvp which we will discuss later and if you find that you can reserve the capacity for this kind of flow that is the flow with these kinds of parameters then you admit it but otherwise you do not admit it this assumes that we have some kind of a virtual circuit  refer slide time 44  07-44  20  slide time 44  21-44  48 there may be distributed admission control instead of central admission control at the beginning for example  it may be end to end delay which must be less then than a delay bound d so calculate d1  d2  etc and you reserve resources  refer slide time 44  21-44  48  slide slide time 44  49-45  12 and what would you do is  the d is specified by the source and as it travels some reservation signal it calculates the delay d1  d2  d3 etc and if d < d1 + d2 + d3 then you reject the flow and if it is greater then you accept it send reject message to sender and release resources therefore if d > d1 + d2 + d3 accept flow  commit resource reservation and notify sender  refer slide time 44  49-45  12  slide time 45  13-45  39 some signaling protocol is used to reserve and release resources and to do admission control so you reserve one mbps that the request goes through  refer slide time 45  13-45  39  slide time 45  40-47  01 so  this is a congestion control in virtual circuits one approach is admission control  not allow new vc till congestion goes away or route new ones around problem areas other is  negotiate flow specification when new vcs are set up this requires resource like buffer space  bandwidth etc  and reservation along the way this may waste resources  refer slide time 45  40-47  01  slide time 47  02-47  59 one topic we mentioned earlier is the tcp ip source quench or sometimes called as choke packets this may be used as a crude mechanism for handling congestion each router monitors each output line and calculates the utilization as a weighted sum of current and past utilization above a certain threshold a choke packet with the destination is sent to the source and the original packet is tagged and sent along on receiving a source choke packet the source is supposed to reduce the traffic to that destination by some percentage if that happens and if it works then that is very fine when congestion is detected the source is sort of distributed and they are remote to each other so  if you could send this feedback instantaneously then you could control the congestion much better but that is not possible you have a distributed algorithm where you work only with some local information and something that might come along with some particular packet   refer slide time 47  02-47  59  slide time 48  00-48  16 the source waits for some time before acting again on the next choke packet because there may be multiple choke packets coming therefore for the same burst it has created ripples of congestion along the way and all the routers sending choke packets so multiple choke packets does not necessarily mean these are independent but they may have come because of the same source so it waits for sometime before acting on the next choke packet for high speed lines with a lot of hops  choke packets to the source is too slow so  choke packets may operate hop by hop thus by distributing the pressure on buffers these choke packets add to the network traffic and it operates hop by hop thus distributing the pressure on buffers  refer slide time 48  00-48  16  slide time 48  17-49  42 scheduling  this is another way to handle congestion  refer slide time 48  17-49  42  slide time 49  43-51  17 packet scheduling has to be done by deciding when and what packet to send on output link  usually implemented at the output interface suppose you have some switch or some router or network node and a number of packets are coming out so what you might want to do is to classify these packets in this context this could be the worst effort there are premium services and other kind of services etc and the rest are the best effort for with the rest what you may have is that you may have various classifications for these flows which may go into different queues and then there is a scheduler which schedules as to which queue to be serviced next a scheduler has a vital role to play on the kind of services on each of the packets at the micro level and each of the flows in general at a higher level they get  refer slide time 49  43-51  17  slide time 51  18-52  33 typical internet queuing  in internet queuing what we do is  we use fifo + drop tail what is fifo ? fifo is  first in first out it is a simplest choice and is used widely in internet this first in first out implies single class of traffic which is essentially means that we have a single queue so whoever comes in first  he is the one who would be attended first for servicing this fifo has to do with scheduling and there is a drop tail which means the arriving packets get dropped when queue is full regardless of which flow it belongs or regardless of its importance so  if the buffer is full whoever comes next will be dropped so fifo has to do with the scheduling discipline and drop tail that is the drop policy has to do with the buffer management this means how much of your buffer is kept empty or whether you allow the buffer to get full or whom you drop out when the buffer gets full etc are some of the buffer management policies now let us look at scheduling actually the scheduling policy and the buffer management policy always go hand with hand and the buffer management policy always come in pair  refer slide time 51  18-52  33  slide time 52  34-53  35 fifo issues  in a fifo discipline  the service seen by a flow is convoluted with the arrivals of packets from all other flows so there is no isolation between flows and no policing send more packets and get more services we have one single queue so whoever is pumping in more packets into it he is more likely to be serviced of course  you will lose some packets also but other people will also lose some packets but in some sense it favors somebody who is pumping data at a higher rate so he gets more service or may be he requires it or it is just some kind of a row node you do not differentiate between different flows at all there is no isolation between the flows if there is a flow which is very important but sends less number of packets he will get much lesser service compared to the one pumping lots of packets so that is the issue with fifo  refer slide time 52  34-53  35  slide time 53  36-54  02 drop-tail issues  routers are forced to have large queues to maintain high utilizations  that is a problem larger buffer implies larger steady state queues or delays so the delay is more synchronization  end hosts react to same events because packets tend to be lost in bursts so what happens is  when the buffer gets full  different packets coming from different sources would be dropped so  all the sources would know that the packets may be timed-out or something so they would act again in unition and this acting in unition is always bad  then you take another step which again is wrong in some way  this makes it become more bursty lock out  a side effect of burstiness and synchronization is that a few flows can monopolize the queue space so these are the drop-tail issues  refer slide time 53  36-54  02  slide time 54  03-54  11 priority queuing  classes have different priorities  and class may depend on explicit marking or other header info  for example ip source or destination  tcp port numbers  etc transmit a packet from the highest priority class with a non empty queue this has preemptive and non preemptive versions this is the kind of scheduling policies we have  refer slide time 54  03-54  11  slide time 54  12-54  27 so  routers must be able to classify arriving packets according to qos requirements this is known as packet classification and packets are transmitted in order to meet the qos requirements which are known as packet scheduling  refer slide time 54  12-54  27  slide time 54  28-54  41 you have class a service which is very premium class b services and class c services are also there you might attach different priorities to different queues and serve them that way  refer slide time 54  28-54  41  slide time 54  42-55  22 so  each router must implement some queuing discipline queuing allocates bandwidth and buffer space  so bandwidth tells which packet to serve next  scheduling  and buffer space tells which packet to drop next  buff management   queuing also affects latency  refer slide time 54  42-55  22  slide time 55  23-57  33 one thing which is very widely used is weighted fair queuing router maintains multiple queues for each output line one for each source the queues are serviced in a round robin fashion instead of packets the volume can also be examined and packets sent in order of their finishing some sources can be given a greater weight than others the point is  even if you do not give a greater weight  then the service which is premium  may be much less number of people might be there so  since the round robin is between the queues  automatically those which have the premium class get a better service since the population is low  refer slide time 55  23-57  33  slide time 57  43-57  51 use a few bits in header to indicate which queue  class  a packet goes into  also branded as cos lower delay and low likelihood of packet drop for high end users priority  round robin  classification  aggregation etc are the different mechanism which we use with this we come to a sort of the end of short handling of this congestion control issue  it is not a very easy issue because as we know this is a global problem but you have to take some local action so that it works fine in the next lecture we will take up quality of service  quality of service as we have already mentioned today  we have different kinds of quality requirements for different sets of people as i mentioned that if you are transferring a file  you do not want any bit to be lost because it may be a very vital bit so it may be a binary or a source or something that the whole thing may become junk if you lose one bit  it is very difficult and if it goes in a jerky fashion or takes longer time then you may not mind so that is one kind of quality you require another kind of quality you might require is  when you are doing some kind of multimedia transmission like audio  video  etc where i may sort of be insensitive to a few packets or a few bits being lost here and there but if the delay is too large or keeps on varying too much then i have a problem with the quality of reception so that is a different kind of quality so how to handle different kinds of quality and how multimedia transmission etc can take place in a network etc would be the content of our next lecture thank you refer start 57  34 good day  so our topic for today is qos and multimedia  that is  refer slide time 57  43-57  51  slide time 57  52-58  43 quality of service and multimedia  we will just look at these one by one  refer slide time 57  52-58  43  quality of service  what is quality of service ? qos refers to traffic control mechanisms that seek to either differentiate performance based on application or network operator requirements  or provide predictable or guaranteed performance to applications  sessions  or traffic aggregates it talks about a lot of things the basic notion is that there are some applications which require one kind of quality of service the quality of service may mean different things but the most important of them are the network delay and packet loss  so it is delay and various ways of delays 