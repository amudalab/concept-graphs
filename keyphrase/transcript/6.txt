COMPUTER NETWORKS
Prof. Sujoy Ghosh
Dept of Computer Science and Engineering:
IIT Kharagpur
Lecture # 07
Switches-1
(Reference Time: 00:45)

Good day. So today and in the next lecture we will be talking about switches. Switches are the (Refer slide time: 01:01 - 01:03)

most important part of a computer networks and we will look into some aspects of the different types of switches. In this lecture we will look at the kind of switches that are used in telecom networks. There is a special version of these, which we will cover in a later lecture and then in the next lecture we will be talking about packet switches,  which are closer to the heart of data networks. First thing is why do we need switches? (Slide Refer time: 01:38 ? 02:15)

The problem is that each user can potentially call any other user, as we have seen in telephone networks and we cannot have direct lines. We can not have an order n2 kind of lines. So every user is connected to essentially a switch in the switching office.  Switches establish temporary circuits; that means they establish a temporary connection between the caller and the callee at the switching station. Switching systems come in two parts: one is the switch and the other is the switch controller.  (Refer slide time 02:16 ? 03:41)

As you remember from our discussion about telecom networks, people want to talk to  other subscribers, who are in maybe a different location, under a different switching station. The switching stations have to connect among themselves. For making this connection through the signaling that we talked about, there is a control plane over here. Control planes are some kind of computers, which are capable of sending some kind of signals or instructions to the individual switches. In the control plane, we set up the path and there is a corresponding signal from the controller to the corresponding switch for setting up a particular circuit or a particular connection, so that an end-to-end connection from one user to the other is set up. This control plane is sort of an overlay network over the actual switch, and once the connection is set up naturally the call and all the signals, etc., flow in this plane. (Refer slide time 03:42 ? 04:10)



There is a network device called a router and there is routing, which is very close to switching. The only thing is that in routing, we decide about long-range paths. If you remember from our discussion about the seven-layer OSI protocols, you remember that there was a network layer and the router is a device in the network layer whose job it is to find out that in order to reach the ultimate destination station, what is the local link that I have to take here? For a switch it is slightly simpler; it is strictly a local decision but for the router there is a global implication ? what is the local link on the output side that I must take for a signal coming from input line to a particular destination, so that I will finally go through all these connections through various other routers to reach my destination, whereas for a switch the job is a little simpler in the sense that when the signal is coming from one incoming link and is going to go out to a particular outgoing link, which is known at the switch, how to set up the connection? Actually, a router is a switch with some added functionality. (Refer slide time 05:34 ? 06:22)










Many connections will need paths through more than one switch, so that is when we required routing. Static routing uses the same approach all the time; that means same destination is routed through the same path all the time. A dynamic routing may allow changes in routing depending on traffic; that means some part of the network may be congested, so it might want to avoid that path and go to the same destination through some other path. As we have seen at least in telephone networks, because of the way we distribute the telephone numbers to subscribers, it is easy to route. You can look at those incoming digits and just decide on the route. Switching is the local connectivity between input and output lines. Now we will discuss some basic concepts and terminology.   (Refer slide time: 06:31- 07:21)

One is the transfer medium ? transfer medium in a switch, could be buses, electronic buses, through which you can send a stream of cells. Queues are memory locations that temporarily store cells. Switches often require some queues; the queues may be on the input side or the queues may be on the output side; queues may be in various places. So we will look at all these different cases. But these are temporary memory locations, where we temporarily store some cells till we know what to do with it or forward it to its destination. A switching element is the building block for the switch. A switch may be either ON or OFF and if it is ON that connection is there or if it is OFF the connection is not there. Sometimes the switch is like this ? at a particular point a switch may connect either to one point or to another point. A switch is just a simple element and in a big switch, we have a lot of these switching elements. These switching elements form the so-called (Refer slide time: 07:58- 08:09)

switching fabric; that means the shape of the switch: like, how the switches are organized so that all the incoming calls can be handled. There is a term called blocking and non-blocking. (Refer slide time:08:18 - 08:35)

Blocking means a network is unable to connect stations because all paths are in use. A blocking network allows this. This is used in voice systems and the main reason is that we have short-duration calls. The point is that there are so many subscribers, may be thousands of subscribers; if we want to provide the switching capability to the maximum possible extent, that means, if everybody is busy talking to somebody or the other, that means, if every subscriber is using this, then if we try to provision our equipment based on this assumption that everybody may be talking to everybody at the same time, then the switch will become very big and very expensive. So what the telephone people have done is that they have made an extensive study of how many people are likely to talk during a particular time of the day, especially Bell Labs in USA. They were the pioneers and a giant telephone company, which was the largest at one point of time, and later was broken up etc. During the early part of may be 1940s and 1950s, they made a very extensive study about people?s calling habits. They found out that during the busy time or the peak hour when all the officers are open, how many people out of a subscriber base are likely to call. That is one kind of statistic they collected; and the other kind of statistic they collected is that if one person starts talking what is the distribution of the call duration; for how long is this particular subscriber likely to hold on to that line. They found the statistical distribution; and based on that they decided that if I provision this much in terms of our switching elements and switching capacity, then almost all the time everybody will be able to get the service, but exceptions may occur. This is based on some statistical study at some particular point of time. It may so happen that the number of subscribers who wanted to speak is just larger than the overall switching capacity that you have. In that case some of the calls will be blocked. I am just talking in terms of switching capacity. This kind of blocking may occur elsewhere in the network also.  We provision based on some statistical averages but at exceptional times, for short durations of time, we may have to block some of the calls because we do not have the capacity to handle so many calls at the same time. So that is a situation where it is blocking and what people try to do is that they try to calculate the blocking probability, which may be 1%, 2% or 5% ? 5% is quite high, as nowadays people do not even like that. That is the blocking probability that we wish to achieve ? we are provisioning this much for these many subscribers. (Refer slide time:12:08 ? 12:38)


Non-blocking permits all stations to connect in pairs at once. This may be used for some data connection where blocking is not allowed or blocking is not acceptable under any circumstances. If you want to make it fully non-blocking, that solution would be more expensive than if you allow a limited amount of blocking. Of course a high degree of blocking will not be acceptable to the users at all. (Refer slide time: 12:39 ? 13:21)


There are various kinds of switches. I am trying to present here a taxonomy of different types of switches ? not that all actual switches neatly fit into the boxes. Some of them take some portions of this and there is some kind of a hybrid or mixture. But this is one way in which the different types of switches can be classified.  We have space division switching and time division switching. Like in multiplexing we have a time division multiplexing and frequency division multiplexing, here we have space division switching and time division switching. Time division switching is very close to time division multiplexing as we will see. Now space division switches (Refer slide time: 13:37- 14:22)

maybe single stage or multistage. In single stage we have a switching matrix; then we have a knockout network, shuffle exchange networks. In multistage, it may be a single path or multipath like delta networks or Benes networks, sorting networks, etc. In time division switches we have shared memory and shared medium. Shared memory could be a central memory or shared medium could be a bus or a ring etc.; so these are different types of switches.  We will not have  the time to look at all these in detail. So we will just look at some of the very common types of switches that are used. (Refer slide time: 14:23 ? 15:25)

Let us first talk about space division switching. It was developed mainly for analog environment. Why do we call it is space division ? because the different calls go through separate physical paths. That is why they are called space division switches, unlike time division, where different calls may be going through the same physical path at different time slots; we will come to that when we discuss time division switching. But in space division switching, different calls go through separate physical paths. The simplest of space division switches is the cross bar switch. The number of cross points grows as the square of number of stations; loss of cross point  prevents connection; if a particular cross point becomes bad it is an inefficient use of cross points; all stations are connected but only a few cross points are actually in use; and this is fully non-blocking. Let us first have a quick look at a cross bar switch. A cross bar switch is very simple. (Refer slide time 15:32- 16:36)

What do we have in a switch? In a switch you have n input lines coming on from one side and n output lines ? you have to connect some of the line. Let us say I want to connect this particular line to let us say this particular output line, i.e., the fourth input line to be connected to the fifth output line. I want this particular switch to be ON and the other switch OFF. If this particular switch is ON, there is a connection between this line and this line. So I have a direct physical path between the fourth input line and the fifth output line. And each of these crosses that you see is a single switching element and as you can see, since there are n lines i.e., if there are n lines on the input side and n lines on the output side, we have n2 switching elements, which actually is  quite a lot.  (Refer slide time (16:37- 16:38)

So if you go back (Refer slide time: 16:38 -  17:49)

the number of cross points grows as the square of the number of stations. If a particular switch, which is at the crossing point between the fourth input line and the fifth output line, if that particular switching element, somehow has gone bad, then four and five cannot talk to each other ever. That is again another drawback of this kind of switches. Of course it is an inefficient use of cross point ? at every cross point we have a switch and all stations are connected but only a few cross points are actually in use. At most n/2 cross points can actually be in use, whereas we are using n2 cross points, which is a really a poor utilization of the switches. One good thing about this kind of switch is that it is totally non-blocking; that means, all pairs of subscribers can talk to each other at the same time. (Refer slide time: 17:50 ? 18:52)

The cross bar switch is the simplest form of n input lines and n output lines that feed to n2electronic switches. If all lines are full duplex, that means all lines allow communication in both the directions, then we require only half the number of switches. We possibly do not require the diagonal switches, the switches which are in the diagonal. I do not want to talk to myself;  the other thing is that only either the top half or the bottom half of the switches ? top half means  top half of the principal diagonal or the bottom half of the principal diagonal ? only those switches are required . In general n2/ 2 switches would be required if you have full duplex lines.   The  other problem about the  cross bar switches is that apart from the number of switches, the number of I/O pins is also a problem. If you want to a make  let us say  1000/1000  line switches for 1000 subscribers, you may have 1000 incoming lines, 1000 outgoing lines, but how do you put all these 2000 pins on the chip?  It also becomes a problem ? people have gone to more sophisticated design for switches. They are classified  in various ways: through input queuing, output queuing (Refer slide time: 19:32 ? 19:34)

or central queuing. We will look at these one by one later on. (Refer slide time: 19:35- 20:05)

Let us look at another kind of a space division switches known as knockout switches, which were introduced in 1987  by AT&T Bell Labs. Essentially we require  using output queuing over here. We will look at output queuing and the issues in detail later on.  If  the switch is trying to pump in more number of  bits or more number of bits to  a particular output line than that an output line can handle, you have to either drop them or you have to store them somewhere, maybe in a queue,  and that queuing  is associated with the output ports. That is what we mean by output queuing. So N inlets (Refer slide time: 20:26 ?  20:44)










and N outlets operate at the same speed. Transfer medium consists of  N broadcast buses, one for each inlet. N bus interface units, one for each outlet, takes input from all the broadcast buses. Each delivers an output to its own outlet. Let us take a look at the figure. (Refer slide time: 20:47- 21:52)

So we have the N input lines on one side. A call may come from anywhere, let us say   subscriber 1 is trying to call to subscriber 2. So what subscriber 1 will do is that it will broadcast the signal over this bus. There are N buses over here and all these N bus interface units are connected to all the buses, each of the bus interface units ? 1, 2, 3 up to N ? will receive that signal from N but this bus interface unit 2 knows that this is supposed be meant for me so he will allow this signal to transfer to the output line, whereas others will simply ignore it. This is again another fairly simple scheme. We have, of course, reduced the number of switching elements that are required. (Refer slide time: 21:53 - 22:01)

It is not using a bus, and you will not get a very fast switching, but this works fairly well. If you want to go beyond this, that means, you do not want so many lines, etc., as I said and even if you have 1000 subscribers you want to service from the same switch, even in knockout switch you have 1000 buses inside, which is not technically feasible. So we go to the other kind of switches which I showed earlier; namely, multistage switches and multistage switches have one of the most commonly used  switching architecture even today, so far as space division is concerned. In multistage switches we may actually provision less number of switching elements than are strictly required for the highest use, as we want to reduce the cost of the switch. All the input lines can go to all the output lines and it is broken up into several stages as we will see. (Refer slide time: 23:19 ? 24:04)

The advantage of a multistage switch is that it has reduced number of cross points, more than one path through the network; this increases reliability. As you remember, in our simple cross bar switch, if one particular switch is bad that particular pair of users cannot talk to each other; that is not really desirable. Since there are more than one path through the network, even if one particular switching element becomes bad they are still able to communicate. The control becomes more complex and it may be blocking. So, as I said, in most of the cases it will not block, but in some cases, it may be blocking. (Refer slide time: 24:05 ? 24:36)

They have a large number of inlets and outlets which are, sort of, grouped into small groups so that these problems with pins and input lines are not there. Often they have identical switching building blocks, forming a fabric. So there are many switching building blocks consisting of some number of switches and such modules are used all over the switch. As you can guess, in a multistage switch, we will have different stages. The input will come from one side and will go to one stage, then may be another stage, and a third stage, and then may be out. So there are multiple stages and each stage is broken up into small modules. These modules may or may not be identical, so it is sort of cheaper to manufacture and it also handles all the pins, etc., in this fashion. So it often has (Refer slide time: 25:11- 25:15)

identical switching building blocks forming a fabric, and a fabric has multiple stages. (Refer slide time: 25:16 ? 26:16)

This is just one example of a small multistage interconnection network. So as you see, this is one stage, this is another stage and this is the third stage. There are three stages over here. Each stage is broken up into small blocks; so this is an 8 ? 8  sort of switch. If you remember to have 8 ? 8 switches in our cross bar switch, we require 64, or even if we take half of it, maybe 30 or above 30 switches. Here each is a 2 ? 2 switch; so actually you can handle it with only one switch. The number of switches has come down drastically but there are problems, as we will see, that it may be blocking kind of system.  (Refer slide time: 26:17-28:01)

So how do you know that something, which is coming over from an input line, would reach the particular output line? Suppose the output lines are numbered and since there are eight output lines, we can code them using these three bits, from 000 to 111  in a binary fashion. This particular line, which really is the input line number 011,  wants to have a call to the output line number 101. If you know to which output line you want to go to, 1, 0, and 1 really become control signals for these switches on the way. So 1 means go to the lower output line; 0 means go to the upper output line. And you can see that those that are starting with 0 come at the top, those that are starting with 1 come over here and that is the same binary number order that we are maintaining. The switch is simply a rule that if it looks at bit, if it is 0 it connects to the upper line and if it is 1 it is connected to the lower line. So 1 will connect to the lower line, 0 it will connect it to the upper line. 1 over in this stage will be connected to the lower line and we will reach 101. This is almost self routing through the switching fabric and because of this, routing is easy. (Refer slide time: 28:02 ? 28:34)


Delta network is one kind of a multistage interconnection networks.  It is constructed of identical k ? k switching elements. It has regular interconnection patterns suitable for a large-scale chip integration. It is self routing as we have shown, requiring log_k N number of digits and a log-k N number of stages each, with N/k switching elements. So (Refer slide time: 28:34 - 29:53) 

this is the picture that we had seen earlier; this is the diagram of a bigger delta network, may be with 16 input lines and 16 output lines. As you can see, this is a four-stage switch, because in order to code for 16 lines, it will require four bits from 0000 up to 1111. Each bit decides whether that particular switch connects you to the upper output line or the lower output line and from any of these input lines there is a path to any of the other output line. This is the 16 ? 16 Banyan network. As you can see, by doing it in this multistage fashion in a regular cross bar switch, while we require some thing like 125 switching elements here, you require only, say, 32 switching elements. So the number of switching elements is much less. (Refer slide time: 29:54 ? 30:11)

But there may be a problem of contention in a delta network. Cells can be simultaneously switched through the network. However cells may have to contend for the same switching element. Let us go back to the earlier diagram (Refer slide time: 30:13 ? 30:39)

You see this, say, 0000 wants to talk to 0001, so he will simply go like this and similarly another line over here, may want to go straight down to somewhere over here. So many calls can go through together but this is not always the case. There may be a contention for the same switching element as we will see in this diagram. (Refer slide time: 30:42-31:58)

Let us say 0 1 1 0: so 0,  1, and then 1,0; 0 will come here, then 1 will go straight; 0 1 1 0; and then 0 1 0 0. So 0, upper 1,  1, and you see that we have potentially got a contention for the same switching element from two different points in the network; it is sort of converging to the same switching element before it can proceed. So there is going to be a contention over there. If there is a contention, obviously  both of them cannot be handled at the same time; either one of them has to wait. You have to make it wait by putting it in some buffer, etc., or one of them will have to be blocked or dropped. Although we save on the number of switching elements, sometimes we get a  contention. Some ways to reduce contention is (Refer slide time: 32:26 ? 32:37)

to provide buffers in every basic switching element, increase the internal link speed relative to the external speed, and use a backward feedback. These are different  sort of techniques to reduce contention. One is of course providing buffers as we have already discussed; you could increase the internal link speed relative to the external speed. That means  if the internal link is much faster than the external speed at which you are pumping in, then it may be able to handle both of them. So far as the input streams (Refer slide time :33:04 ? 33:31)

or the output streams are concerned, they will not notice the faster switching that is going on within the switch. So that they will think that there is no contention. You can use backward feedback; that means, at some later point in the main if you find that there is a contention then give one of them feedback so that it sort of re-circulates. (Refer slide time: 33:32 ? 34:07)

The other ways are to use multiple networks in parallel; provide multiple links internally between switching elements; and shuffle cells first so that they do not collide later. This is also another kind of switch  that is quite common; it is known as the Batcher Banyan switch. We have seen the Banyan switch part; in Batcher part there is some shuffling of the input cells so that in the Banyan part they do not collide by contending for the same switching element. (Refer slide time: 34:08 ? 34-57)

In the space division switch, the big switch is broken into multistage smaller chunks.  I will show a three-stage switch. Suppose I want an N/n switch; suppose it were a cross bar, then it would be an N/n switch. This is replaced by N/n number of n ? k switches in the first stage, followed by k (N/n) ? (N/n) switches in the second stage, followed by N/n number of k ? n switches in the third stage. So let us take a first look at a simple example. (Refer slide time: 34:58-35:57)

So this is a three-stage switch; in the first stage, we have 10 of them. So N is 10; and k is 2. So we have 5 ? 2 switches. We have 1 2 3 4 5 at the input stage, so N/n, that is, two of them or 5? 2 switches and then two 2 ? 2 switches followed by 2 ? 5 switches. Let us say 5 wants to speak to 2, so 5 comes here; gets switched in this fashion to 2; and 6 wants to talk to 1. So 6 goes to this switching fabric in this fashion in the third stage. (Refer slide time: 35:58 ? 37:59)

If you take the general case of N = 1000, n = 50, k = 10, this reduces the number of switches to 24,000 from about 500,000 because 1000 ? 1000/2  is 500,000; that is, the number of switching elements you would require for a simple cross bar kind of switch, whereas by breaking it up into three stages in this fashion taking this particular value of n = 50 and k = 10, you reduce the number of switches to 24,000. However, the switch can handle only N*k/n. So if N = 1000 and k = 10 and n = 50, so N * k/n would be 200. So only 200 simultaneous calls, so only 400 users can use it at the same time. As I mentioned, it may be fairly alright because you have a phone but you are not talking on that phone all the time; most of the time actually it is just lying idle. If the telephone company makes a good study and finds out that 400 is the maximum number that is sufficient for a very low blocking probability even during busy hour, then that is fine. (Refer slide time: 37:34 -38:24)



The general scheme of this is that you have N lines fitting into each of these N/n  modules at the first stage. So you have a total of N lines fitting into one module. So you have n ? k kind of switches so that this switch can be directed to any of the switches on this side. So there are k of them here there are N/n switch modules in this first stage, k modules in the second stage, and again N/n modules in the last stage,  third stage. So you have (N/n + k + N/n)  modules. (Refer slide time: 38:25 ? 39:28)

In the multistage interconnect network, you may either have a connection based  kind of system, that means, once a connection is made it is permanent till the call is held, as we have in a usual telephone network, or it may be a cell-by-cell, that means, although the user sort of goes on having the conversation, his stream of bits is divided into small cells and each cell is individually routed through the main. Many connections will need paths through more than one switch ? cell-based means routing tag versus network-based, which is routing tables. So if you have tags it is faster; if it is table it is slightly slower and as I said, the static routing uses the same approach all the time; dynamic routing allows for change in routing depending on the traffic. So what is a tag? (Refer slide time: 39:32 ? 39:35)

You remember that in a delta network we were putting 001 etc.; we were encoding the input or the output, specifically the output lines, and then using that address so that the cell can route itself through the network. That is an example of a tag, which means that within a switch each of the lines has got some kind of code in it; you might call it tag. This tag will be used for routing. Now if it is self-routing, like the one shown in delta network, which is very simple, it will be very fast. If it is some other kind of  tag we are using where such a simple scheme does not hold, then maybe we have to look up to some table to find out where I want to go. With tags, cells can be (Refer slide time: 40:27 ? 40:58)

augmented to include extra header information when they enter a switch. That means you look at which is the output line it wants to go to and then add the code for that output line on to this cell, which sort of becomes the tag. For example, with the Siemens ATM chip set, each 53-byte octet or byte cell is augmented to a 64-byte cell as it enters the switch so these extra bytes are for putting in this tags, etc. This extra information speeds up the routing process. (Refer slide time: 40:59 ? 42:09)

In a connection-based system ? we discussed this earlier, but once again ? the path is determined once for the duration of the connection. That means, once the connection is made, for the entire duration of the connection the path remains the same. All cells of the connection follow the same path. Now it may be a connection-based system with cells; that means, once the connection is set up, a particular path is designated that this is the path all the cells, which are originating for this particular call, will take. So, all cells of the connection follow the same path. This guarantees proper sequencing of  cells; of course, they will now go in an order. The connection, of course, may be refused: that means when you are trying to set up a connection you find that in an intermediate stage there is not enough resource. There is too much contention for that, so the connection may be refused. Paths can be determined by a combination of random selection to distribute the load. If you remember, in a multistage switch specially, there are various paths from the input to the output; there are various possible paths, so that if one path is not alright because of some defect or one path is not very good because at that particular point of time that path is loaded, you may route your  path through some other path of the switch. (Refer slide time 42:32 ? 42:49)

You might do a random selection amongst the paths to distribute the load, do a careful selection to arrive at the correct destination ? if this and this selection can be done either centrally or in a step-by-step fashion as is done in a delta network. (Refer slide time: 42:50 ? 43:49)

We have different types of switching ? we have connections plus tags, you have connectionless, which are tags. Of course we will look at connectionless switches in more detail when we discuss packet switches. So the same discussion that we have now, part of the discussion that we are having here, could also be useful in the next lecture. We have connectionless plus tables, that means, if tags are not possible you have to do some table lookup because this table lookup will always be slightly costly in terms of time. The kind of speed that you can achieve with a table lookup becomes limited because of that; but any way if this is again a possibility that you can have connections-oriented system by using tables, you can have connectionless systems using tables; this is also used in routers specially. (Refer slide time 43:50 ? 44:40)

We will look into buffers in more detail when we discuss packet switches. Buffers may be complete sharing, which is the best utilization of memory and best for bursts of equal loads. Complete sharing means everybody can share all, so there is a central memory kind of thing, which is shared. It may be a complete partitioning; naturally complete partitioning would be faster and best for unbalanced load. Partial sharing means some of it may be centralized, some of it may be distributed, etc. There may be various strategies regarding buffers. (Refer slide time 44:41 ? 44:43)

Next we come to time division switches ? time division switches follow almost the same principle as time division multiplexing, in which the same line was being used for communication by different channels; what we did was that we kept a specific time slot for one channel and these time slots go in a round robin fashion. So after a particular frame is over, that means, all the input lines have had their time slots then one particular channel will get back its second time slot, and so on. So it will go on in a round robin fashion. We use the same principle in time division switches; in time division switches the bits or bit streams may follow the same path, that means, different calls are following the same path but they are using different time slots. So a fixed (Refer slide time:  45:55 ? 46:17)


The number of bits from each line is read into a RAM in order ? this constitutes a single frame. That means a fixed number of bits from each frame, each line. A mapping table reads from the RAM in a different order for switching from line j to line k, the j th location is read in the k th step. (Refer slide time 46:18 ? 47:27)

This is shown here. Suppose these are the input lines 0 1 2 3 4 5 6 7. The output is read in a somewhat different fashion 4 7 6 3 0 5 2 1. That means actually whatever the 0, its line 0, puts in this slot would be read by line 4, because line 4 is reading in that time slot. So line 0 is, in effect, connected to line 4. Similarly, line 1 is connected to line 7, line 3 is not connected to anybody. It?s idle at the moment, so 3 3; similarly a 5 5; so it is idle. Similarly, just as we have 0 4 we 4 0; so this is a bidirectional communication, duplex communication going on. So in the first slot or, let?s say, the 0 th slot, the line number 4 reads it and in the 4 th time slot, line number 0 reads it. So line 0 is switched to line 4, 1 to 7, etc. Lines 3 and 5 are idle. (Refer slide time 47:28 ? 48:29)

Basic components of a TDM switch ?we will not have the time to go into the details of this. It has a dedicated internal buffer memory, distinct from the RAM memory used for program code and data. And this buffer will sort of contain the bits which are to be written into and read often on a completely separate physical module, that means, printed writing card. Usually it has at least two DMA bidirectional serial ports. If you have studied operating system, that is direct memory access, which is used for fast communication without the intervention of CPU, that is DMA. Of course we want this as fast as possible; so DMA bidirectional serial ports, input and output, are simultaneous on each port with dedicated hardware for each operation. So our memories are very special; that means, they have multiple port memories and serial I/O on each port. This is a very simplified block diagram. (Refer slide time 48:32 ? 49:25)

Usually in your line, the bit stream will come serially but on the bus internally we want to put them together, maybe 1 byte at a time. So we have a serial to parallel converter for getting 8 bits and converting it to a byte, which will be fed in parallel. So database goes into the buffer RAM memory ? there is a connection memory which is a mapping table that we have talked about. So when you are setting up a connection, you will make entries into the connection memory. We will just have a look at the connection memory, that is, what we have already seen and then it gets  consecutive address generated here and then from the buffer memory once again it  goes to the output side and then there is a parallel to serial converter for feeding into the individual lines. So this is the data bus and this is the address bus. (Refer slide time: 49:26 ? 49:47)

Connection memory contains a list or table for mapping input time slots to output time slots. When a call is set up, you make an increase in this connection memory. The pointer data values are set from the CPU on a per call basis. The data output of this memory is used as an address to access the buffer RAM; decimal representation is shown. (Refer slide time 49:48 ? 50:02)

This is another example. So in address 0, we have pointer or content is 14; that means line number 0 may be connected into line number 14 in this example. (Refer slide time: 50:03 ? 52:06)

Now just one more thing ? that assuming 1 byte is read from each line, each frame is to be processed in 125 micro seconds. You remember how this 125 micro second comes about?  Remember we talked about the voice channel, and each voice channel requires a capacity of 64 kbps as we mentioned. So 64 kbps means that what is the gap between every bit in 1 second ? there are 64,000 of them. So it would be 64 kbps. If we  handle 1 byte at a time, it will be 8 kbps and 1/8 kbps would be 125 micro seconds. So this is the basic time for frame. And in the entire time division hierarchy, as we will see in time division multiplexing, this 125 micro second is a very sacred figure; this is what everybody sort of sticks to so that a voice call can smoothly fit into this. Of course if you want to have a higher speed channel then what you might do is  in the same 125 micro seconds? slot, you may pack in a large number of bits. That is alright but we keep the frame size to 125 micro second. If the memory cycle time is T micro second and there are n lines we have 2nT, because each of them has two operations: one read and one write. So 2nT would be 125. So a 1000 line switch will require a cycle time of about 60 nanoseconds. That is the kind of memory speed, and as we will see, as the number of lines goes up or as the speed of switches goes up,  this memory speed becomes a constraint as with space to division switches, it is possible to device multistage switches. (Refer slide time: 52:07 ? 53:15)

Now just one last point about capacity allocation;  I mentioned that the service providers actually want to provision less than the maximum possible. But what would be the blocking probability if you provision it in a certain manner, let us say, in the C stage switch? So I will just give you the formula. If the average call arrival rate is, say, ? calls per second, which is an exponential distribution as people have seen for  a voice network ? for data network it is different; we will come to that in the next lecture ? the average holding time is m seconds per call, which is Poisson distribution. The call load in erlangs is A = ?m. This is called the load and given a load A, we try to allocate a channel capacity N such that the call blocking probability B is at an acceptable level, typically around 1% or less. This is the typical figure that we try to achieve. The difference is that within the busy hour, we get not more than 1% call blocking probability and the formula for calculating this is this (Refer slide time: 53:16 ? 53:39)

B is equal to A ** N / N ! *(?(A**i/i!) ) ? so you can use this formula or may be write a small program using this formula to calculate the call blocking probability giving values of A, N, i, etc. (Refer slide time: 53:40 ? 54:19)

So you can verify this. This is one last this thing ? verify using Erlang-B and Stirlings approximation for handling the factorial, that for large N, say 100, even with 70 % average channel utilization, that is A/N,  we get very small blocking probability. But of course, if the channel utilization or A/N becomes 80% and then 90%, what will happen is that you will find that your channel probability is going up and then you can do the same thing for a small n and try to see what happens. Thank you. (Refer slide time: 54:22 - 54:25)


good day so  in this lecture we will discuss about packet switches that means packet switches are  switches which as specifically designed for handling packets we have  seen the telecom switches like space division time division switches etc ok we have also discuss something about multistage switches which are used quit often in packet switches also but in this lecture we will specifically discuss about  a packet switches (Refer slide time: 54:59 - 55:01)

which is  at the heart of  all most  all your  computer networks ok  so (Refer slide time 55:11 - 55:26)

first  just to  give you some motivation  that why do we require switches the performance of a LAN can be considerably enhanced by moving over to switched LANs moving over from what  if you remember that some of the earliest   kinds of LANs that we have just seen a brief glance of  some of them used shared memory like a hub or a bus etc now in a if you are using a shared medium what is going to happen is that only  two  nodes connected to this LAN can communicate with each other at a particular point of time where as if you have a switch multiple pairs of  users or multiple pairs of nodes can communicate with each other at the same time so naturally you get a  very good boost in performance of the over all LAN so that is one reason why we require switches of course  we  require switches in other place also (Refer slide time: 56:18 - 56:23)

multiple distinct source destination pairs are able to communicate at the same time thorough a switch so that is why we require a switch in  (Refer slide time ? 56:24- 56:34)

packet networks a packet switch needs to look at the header of each packet to find the destination address of the packet so  you do not have to do a full scale  sort of  lookup of a very big table a  Refer slide time 56:43-57:52)




so you have a you lookup only a  very small table of  V C I and V P I in the V C table replace  but of course virtual connection just as their setup they may be  taken down  so replace old V C I V P I with new ones forward cell to outgoing interface transmit cell onto link so this is the  this is how an a t m  switch works so if you so you  see that the basic  concept of the cell   remains the same   that means  we have   we have some  we have to find out   the output port from the address and the here the address once it is setup after that the output  the address is simply a small table of V C I V P I which makes this lookup process much faster so this together with the fact that the  cells are of same size so you can develop your hardware in such a way so that  this a t m switches will work very fast otherwise the rest of the switch and the switching principle remains the same as we have discussed already (Refer slide time  57:53-58:08)





next comes  comes Ethernet switch which is perhaps the common most common type of switch that you  see  almost every where  as a matter of fact  this  number of Ethernet switches  which are being used and which hare being put to use the almost regularly  is so high that the cost of this switches have also come down you know because if  if you produce something in very large number you can  amountize the cost of is development etc over a large number of switches so this cost of the switch comes down  so Ethernet switches  have become  very cheap  these days ok   switch which are used to cost may be a lack or so may be ten  fifteen years back  cause only a few thousand rupees today so  this is the most common type of switch which we see every  where




COMPUTER NETWORKS
Prof. Sujoy Ghosh
Department of Computer Science and Engineering
I.I.T., Kharagpur
	Lecture Name #8	
Packet switches 
(Refer slide time: 00:40)
Good day. In this lecture, we will discuss about packet switches; packet switches are switches, which are specifically designed for handling packets. We have seen the telecom switches like space division, time division switches, etc. We have also discussed something about multistage switches, which are used in packet switches. In this lecture, we will specifically discuss about  packet switches, which are at the heart of almost all your  computer networks. (Refer slide time: 01:22 ? 01:37) 

First,  to give you some motivation, why do we require switches? The performance of a LAN can be considerably enhanced by moving over to switched LANs. If you remember, some of the earliest kinds of LANs that we have just seen, a brief glance of some of them, used shared memory like hub or a bus. If you are using a shared medium, what is going to happen is that only 2 nodes connected to this LAN can communicate with each other at a particular point of time, whereas if you have a switch, multiple pairs of users or multiple pairs of nodes can communicate with each other at the same time. Naturally, you get a very good boost in performance of the overall LAN; that is one reason we require switches. We require switches in other place also. (Refer slide time : 02:28 - 02:48)

Multiple distinct source destination pairs are able to communicate at the same time through a switch. That is why we require a switch in packet networks. A packet switch needs to look at the header of each packet to find the destination address of the packet. If you remember, when we discussed about connection-oriented and connectionless systems, telephone networks are predominantly connection-oriented, although connectionless technology is also getting there. Data networks are predominantly connectionless although some connection-oriented network technologies are also used in some parts of the network. Today when we are talking about packet switches, we are specifically talking about connectionless systems. In a connectionless system, since there is no fixed connection between the source and destination which is held during a session etc., such concepts are not there. So each packet is on its own and that packet is perishable and contains data from the source node, which is meant for the destination node. It will contain lots of others things like headers, etc., which we talked about earlier but now we are specifically interested in the packets ? that means, the data stream has been broken up into packets. So each packet must contain the destination address of where the packet will finally reach because since there is no space connection, each packet must carry (Refer slide time: 04:12 ? 04:38)

the destination address. And this destination address is sometimes also called the MAC address; MAC is for media access control. There are various types of MAC address. We will come to that later on. Depending on the address, the packet is forwarded to another port. What happens in a packet switch is that through one port, some data has come in; this data will contain a MAC address, this MAC address is the address of the node, the address of the destination where it is supposed to reach, the destination node. This destination node ? the switch has to know that this destination is connected to this particular outgoing link. There is a question of mapping from the MAC address to a particular output line and if that output line has a tag as we had seen in the earlier lecture on some of the switches that they use tags. If they have a tag, depending on this MAC address, this tag could be sort of maybe looked up from table or something and added to the packet so that packet can make its way to the switch and reach the final, correct output line, and from then on to the destination station.  (Refer slide time: 05:42 ? 06:57)

Depending on the address, the packet is forwarded to another port. Since more than one packet may arrive at different ports destined for the same output port, they may have to be internally queued. This was also discussed in the last lecture, that if we have more than one packet which is sort of destined for the same output port, obviously we cannot take both of them together. One of them has to wait in some memory or in some buffer or maybe it will be queued because there maybe other packets coming up for the same port so they will be all queued and this queue could be serviced by the output line 1/1, that means 1/1 will be popped out of the queue. You may queue it right at the input, if you see that the output port is busy, you may queue it at output port or somewhere in-between. Not dropping packets and being fair to all input lines are desirable criteria for packet switch design. That means we do not like to drop packets; we want to buffer them as far as possible. Of course if a switch is of small capacity, which is swamped by a huge number of incoming streams, etc., maybe for the same output port or something the queue, the switch may be forced to drop some packets. But usually we would not like to do that. We would not like to drop any packet and at the same time we do not want to starve any particular input line. In general, we like to serve all of them in a fair fashion. These are the criteria we use for evaluating a design for a particular packet switch. (Refer slide time: 07:37 ? 07:56)

The first-generation packet switch went through several generations, we are in the 4th generation or something. Anyway, the 1st generation packet switch is here, it simply uses a CPU, stores packets received online cards in its main memory.  These packets are routed to output queues, the packets in the same queue are scheduled by software for access to the output line card. Let us look at this, the simple kind of a situation. (Refer slide time : 07:56 ? 09:27)


We have a CPU with some memory. There are particular input and output queues; these are the line cards. What are the line cards? This is a 6 port switch as you see; there are 6 lines, and there are maybe so many ports. These ports, some of the functionalities are collected together and they are implemented in a small dotter card kind of thing, which is called a line card. A line card will handle just a few of the lines, and there will be several line cards on the same switch. If you go back these packets are routed through the CPU, which does the lookup ? it looks at the MAC address, the particular output port to which it must go. CPU will have all that information. The CPU looks up and then it directs that packet to the particular output port. There maybe a queue and this queue is scheduled by software for access to the output line card. This was the first-generation packet switch, which is fine ? excepting that because of this  CPU and memory storage, looking up, tables, etc., it takes time. These switches were  (Refer slide time: 09:27 ? 10:56) 

comparatively slow. Next we came to the second-generation switches. Here some of the routing functions are involved. Do you remember what the routing function is? The routing function in general means that we look at the destination address and decide which output line this packet must go to. And this routing function, if you are doing centrally, naturally it will be slower. You can distribute it somehow, like distributing some of the routing functions to the  line cards themselves. Line cards which can switch locally if the destination is on the same line card ?  there is another point you can do that ? and if a particular line cards sees that the output destination will go through the same line card through a particular line, then the line card need not send it to the CPU at all. This can switch locally. So that was one improvement over the first-generation switch. Secondly, there is contention for the shared bus; that is of course a problem: a contention for the shared bus, and packet switch cannot be routed and the same is handed over to a central processor. This cannot be the local line card, which does not have enough intelligence and knowledge to know how to route it. In that case that is sent to the central processor. (Refer slide time: 10:56 ? 11:32)

In third-generation switches, the shared bus is replaced by a switch fabric, which is an interconnection of buses and switching elements. The shared bus is replaced. If you remember, we can use a bus for broadcast, etc., like we did in knockout switch. That may become a problem because of the contention for this shared bus. So we replace that by the switch fabric, which is the interconnection of buses and switching elements. (Refer slide time: 11:32 -11:52)



When a packet arrives in this third-generation switch, which we will discuss in detail, when a packet arrives at an input port it is tagged with the output port ID by a control processor. The switch elements automatically routes it to the correct output port, as we have seen, this routing takes place in a MIN. The buffers in the switches can be associated (Refer slide time : 11:52 ? 12:21)

with the input or the output or with individual switching elements. The switch fabric may be cross bar, broadcast, Banyan or other configuration. We have seen some of these configurations; remember the cross bar switches? That means just a simple connection of switches; then we have some broadcast like the knockout switch, maybe a Banyan like a multistage switch or other kinds of configuration. (Refer slide time: 12:23 ? 12:54) 

There are three types of switching fabrics, one is that we are using through the memory, other is one which you switch through a shared bus or you have a fabric of switching elements like cross bar ? it is the simplest kind of fabric. Of course you can have more complex fabric over here as we have seen, like multistage interconnect network. These are the three different types of switching fabrics that we have. (Refer slide time: 12:54 ? 13:48 )

Switching via memory: this was the first-generation routers as we have discussed. Packets are copied by a system, a single CPU. Now, speed is limited by memory bandwidth at two bus crossings per datagram, what is happening is that from the input port, some packet is coming through the system; the bus sent to the memory where the CPU is looking into it and then decides where it must go, then sends it back to the data bus in that output port, which catches it and throws it out. This is the sort of part which gives a bottleneck to this entire scheme because this bus part tends to be slow. (Refer slide time: 13:48 ? 14:04)

In modern routers, as I mentioned input port processor performs lookup, copy into memory, etc. An example is ? this is short of commercial system ? like Cisco Catalyst 8,500, which would take this kind of approach. (Refer slide time: 14:04-14:40)


Switching via bus: datagram from input port memory to output port memory is via a shared bus. Bus contention: switching speed is limited by bus bandwidth. You have very fast buses like 1 Gbps for Cisco 1900, which is low and Cisco switch, which has now been discontinued. It has sufficient speed for access and enterprise routers, not regional or backbone. But this bus structure does not really do well for higher level switches. And one example of bus structure is a knockout switch. We have already discussed, just to recapitulate, we have n bus interface units, one for each outlet and each takes input from all the broadcast buses; each delivers output to its own outlet. This is, if you remember the diagram of the knockout switch, you have these buses on which the data is broadcast from the input side. In the output side, particularly bus interface unit for which it is destined, it catches the particular packet; others ignore that packet and so it is queued up over here ? like q1, q2, q3, q4, etc., and it is pumped out as per this queue: on a first come first serve basis; this basis could be something else. Also if you want to have some other pair, also in particular packets, etc., it?s possible. But in the simplest case, this is a first come first serve kind of queue. (Refer slide time: 15:47-16:16)


   


Switching via an interconnection network are just as in the other kind of switches we have seen. It overcame bus bandwidth limitations, that is the basic implication. As we want to go faster and faster, we have to migrate from buses to an interconnection network. We have already seen Banyan networks. Other interconnection was initially developed to connect processors and multiprocessors, etc. It became useful in this class of  packet switches. (Refer slide time: 16:17 ? 16:29)


Advanced design may involve fragmenting datagram into fixed length cells, switch cells through the fabric ? this was the idea. For example, this is done in, let us say ATM switches. In ATM switches what they do is that whatever the packets may come in the form of packets, it may come in the form of streams originally; it is finally broken into fixed length cells. The good thing about a fixed length cell is that you can, if you know length in advance, then you can design hardware so that it can be handled in faster a possible way. Although the ATM cell has 53 bytes inside the switch, it is something like 64 bytes, these 64-byte cells are fixed length cells and they are pumped as fast as possible. It is for designing very fast interconnection network, in which we fragment them into fixed length cells. Cisco 12,000 switches gbps through the interconnection network: when you go to this gigabit kind of switches, you require an interconnection network or maybe something more as we will see. (Refer slide time: 17:38 - 17:47) 


As was discussed earlier, we have three kinds of approaches to handling, contention and buffering, etc. One is on the input side, another is on the output side; the other one is somewhere in-between. We will discuss these one by one; first let us come to the input side but before we go forward, I will digress a little bit, I will just explain a little bit about what is involved in the input side, the input part of the switch. If you remember in our discussion about the several OSI models, in the bottom-most layer, we had the physical layer. Above that was the data link layer, above that we had the network work layer, above that we had transport and other layers, which do not occupy us at the moment. We will look at these later but we are having the physical layer, the data link layer and then network layer or the routing layer, these three are sort of very closely linked to switches operation. First is the physical layer, that means, physically the link is coming in some fashion. That means there will be certain kind of connecters, certain number of lines are coming per input line when we are saying one input line, but the one input line may consist of a number of wires when you are connecting. For example, if you are connecting through a cat -5 cable, there are a number of wires which will connect to that particular port; that is, a single port. Similarly, in other cases, you have a physical layer, you have an optical connection coming in, and you must have an optical detector over there, etc. This also means physical characteristics of the media have to be handled by the input port. If you look at this picture again, we have just the input part shown over here: this side is the switch fabric, that means, getting inside the switch so here it is sort of the  line card itself. First there is a line termination, which takes it specification, etc., from the particular physical layer that is being used. We have something like a data link layer functionality. An example of a data link layer kind of protocol is the Ethernet. We will, of course, look at details of Ethernet later on. What does the data link layer contain? The data link layer contains the address called the MAC address, which we have talked about in the last lecture ? the MAC address, the media access control address ? of the next machine to which it is destined. Please remember that these addresses are addresses of the machine, because the source node knows the destination, let us say. It may not know it also directly. Anyway let us forget about the large network, let us now focus on a particular switch and some nodes connected to this switch. This node A knows that I  want to communicate with the node B. Of course node A, B, etc., do not have any meaning at this level. So at this level, some particular address is given called the hardware address, which actually is the address of the networking card. If it is a computer that you have as a node, then the networking card of the computer, the network interface card, the NIC card, it will have some address built into it in the hardware. Similarly, the destination will also have another hardware address. This hardware address is put in this data link layer part and that means the data link layer puts this hardware destination hardware address into the packet. (Refer slide time: 21:53 - 22:22) 


This address has to be read and then somehow you have to make a mapping from this address. This particular address is connected to this particular line. For that you need some lookup, forwarding, and queuing ? if you are doing input queuing, you will do the input queuing also. What kind of lookup? This lookup will tell you that for this particular MAC address that is the output line card. Nowadays, what the switches do is that they  keep on sort of getting this information in a dynamic fashion. When a switch is connected to a number of computers, the switch will listen to the network traffic and find out all the hardware addresses, which are connected to these different lines. It will sort of slowly gather this information and store it inside; it will know that whenever some particular node is sending, it will give it the source address, also called the source MAC address. It will now get the source or MAC address and this particular MAC address must be connected to this line. This switch keeps on gathering kind of information, and storing inside; and part of it maybe stored locally in the line card. Some part of it maybe stored centrally and there are thousands ? nowadays, a switch can remember thousands of such MAC addresses, which are directly connected to the switch. There is another level of problem, which is finding about the addresses of remote machines, machines that are not connected to this switch at all. Machines, which are connected through others switches, routes, etc., in a different country, maybe. So that is a different problem; that is a routing problem, but in the data link problem we are concerned with all the MAC addresses of all the nodes which are connected to this particular network which is local. So that is stored in this switch; you will now look up the table (Refer slide time: 24:07 ? 24:21)

and decide that that is the final destination output line, which is the forwarding line and then you do the queuing. Then you send it to the switching fabric for its final destination. (Refer slide time: 24:22 - 26:04)

We will now talk about decentralized switching: given datagram destination, there is lookup of output port using routing table in input port memory, the goal is complete input port processing at ?line speed;? so this is what I want to do. We do not want to hold up this particular packet; you get the maximum throughput at the maximum speed. We do not want to hold up any packet in any buffer ? ideally that is what we would like to do. So we want to look it up locally at the input port itself, if that is possible. But please remember that even if you can do all this lookup business at the line speed at which the packet comes, even then you maybe forced to store the packet for some time. Because either the output port where this particular packet is headed is already occupied because it is sending some other packet through it at the moment to some intermediate switching element, there maybe a contention for intermediate switching elements, etc. For all this contention, etc., we maybe forced to put this packet on hold even if we do the lookup at line speed. So we have to queue this in the datagram and the other possibility is that if the datagram arrives faster, then forwarding it into switching fabric and forwarding maybe constrained by all this contention, etc. Then you have to put it in a queue. So what you can do is that you can put it in the input (Refer slide time: 26:05- 27:00) 


 
Side; you can buffer it at input called input buffering. Packets are buffered at the input. They are released when they gain access to both the fabric and the output trunk. That means both the internal switching elements as well as the output port must be free and then this packet maybe released from the buffer, which is held at the input. All elements except the arbiter, which is combinational, needs to run only as fast as the lines. So this is a good point about input buffering that elements do not need to be faster than the rate at which the lines operate. In some other cases, we will see that at faster speed, this is a big problem, this input buffering, which is known as the head of line blocking. Let us look at head of line blocking. (Refer slide time: 27:03 ? 28:33)



Look at this head of the line blocking ? you see what is happening is, here you see there are three input ports, and this is the switch fabric. There are three input ports; the destination is shown by the color, that means the following: red means it is destined for this particular port; blue one is destined for this particular port; green one is destined for this particular port. See what is happening, the first packet, red, comes here and goes to the output port; this one is also red, which means it is destined for the same output port although there is no contention in the switch fabric; it has to wait. That is accepted. The trouble is, this green one, which was sort of destined for something in the middle, and which could have gone through this fabric at the same time as this red packet, is flowing at the top. This has to wait because the queue is blocked by this particular red packet. This is called head of line blocking. At the head of the queue, there is somebody who is blocked and all the people who are behind, all the packets which are behind, have to wait, this is called head of line output port contention time t, when only one red packet can be transferred. So the green packet experiences head of line blocking. This is the head of line blocking, which is a problem with input buffering. Other reason you might require input port queuing is that the fabric is slower than input ports combined. Input ports combined means it is a special case when all the lines are trying to pump data at the same time. What you require is a sum total of all the speed that you get over there, which is not always possible, because then surely there will be contention inside for the switching element. So you will be forced to queue some of it at the input head of line blocking. We have already discussed that queue datagram in front of a queue prevents others in queue from moving forward. So there is queuing delay and loss due to input buffer overflow. This is the other thing, which might happen; what might happen is that this queue is given for each input port. At each input port since this buffer is not shared, this one is only for that particular input port. Naturally the size of the queue is limited; if it so happens that data is coming in at a fast rate and the fabric is very congested, it is not getting a chance to go through, this queue at the input port may overflow. If they overflow, which means that some packets have been lost, it is a very undesirable situation; but this may happen. This is the head of line blocking. So the other option is to buffer (Refer slide time: 30:33 -30:58)



them at the output; that means, you have a queuing buffer management, this part, then the output part is being fed from the switching fabric; queuing buffer management; data link processing, that means, protocol decapsulation for the next jump, etc., and finally the line termination. Once again you have to go to the physical layer over here. This is the output port queuing. (Refer slide time: 31:00 ? 31:13)

What are the implications of output port ? queuing buffering required. When datagrams arrive from fabric faster then the transmission rate, the fabric is pushing a datagram into a particular output line at a faster rate than the output line can pump it out. Naturally packets are getting accumulated near the output port, so packets are getting queued over there. So you develop a queue.  (Refer slide time: 3:30 -32:11) 
	


There is a scheduling discipline, which we might choose among the queued datagrams for   transmission ? which is the one that gets the higher priority, if you are putting some higher priority etc., and even if you are trying to give the same priority to everyone, it might so have happened that we might have kept one particular packet quite long in other places; we may have sort of fed it back; I did something with it as it was already late. We might like to give higher priority than other packet, etc. So there maybe a scheduling which is going on in this. Queued buffers are maintained for each output line. (Refer slide time : 32:11 ? 32:25)


This is a great advantage, because it does not suffer from head of line blocking. Naturally, because it is been buffering as soon as possible, but there (Refer slide time: 32:26 ? 3:42 )

are problems. Later stages of switch fabric and buffers have to be N times faster in the worst case to avoid dropping of packets within this space. This rule makes the switches more expensive. Another thing is that suppose, in the worst case, all the N inputs, say N ? 1 input lines, are trying to feed data into the same output port. What is going to happen is that these packets, which are coming at the line speed through N ? 1 lines, they sort of start converging to the Nth port. How can they converge? Obviously, if everything is operating at the same speed, then there is going to be contention in these switching elements or one way to avoid this contention is to make switching elements faster. That means suppose two of them have to go to the same switching element but suppose that the switching element operates at double the speed ? I am just giving a simple example ? at double the speed but at the same rate at which it is coming, it can push it forward. So as you go more towards output port, the speed requirement becomes higher and higher. 
Finally, near the output, at the very point of the output port, it has to be N times faster approximately, which is not done like N times faster. That is not possible but the point is this is a difficulty which we have with output buffering; although we do not have head of line blocking, they tend to be more expensive if you have a buffer fabric. I mean there is always a sort of trade-off between how expensive a particular switch is and what its  performance is. Naturally if you want to go for higher and higher performance where it is very important, then you need to sort of get switches that are more expensive. (Refer slide time : 34:47 ? 35:31 )




We have the buffer at the input and buffer at the output. The buffer maybe distributed in-between all over the fabric, the buffer is associated with each switching element, which is used when the next stage is not available. It needs a lot of memory and quality of connection is difficult to control. The point is that if, throughout switching fabric, if you have distributed our buffer, it is very difficult to have any kind of policy about priorities, etc. The switches will handle it any way they can, so we cannot have a policy which holds over all the buffer because we do not have processing capability over there; it is just a buffer and a switch. (Refer slide time : 35:32 ? 35:44)
	

This is an example of a switching fabric element. You look at this switch in the center. We have the switch and then with two input lines coming in, you have a buffer over here. If the next switching element is busy, then that particular thing can be put into the buffer, from the buffer it will come back to the switch and then get switched when the next element is available. (Refer slide time: 36:10-37:04)



Buffers need to be only as fast as an element?s fan-in ratio, and during overload, the resulting queue is distributed. There is one good thing about distributing the buffer ? which is that instead of putting all the buffer either on input side or the output side as the output side gets more and more congested in one particular area, what will happen is that the previous switching element will also get congested. They will buffer something and then the buffer will be pushed back. What is happening is that there is a distribution of this buffer all over the fabric. These later stages need not be all the way up to N times faster than the initial stages, which is not always possible. So that is one good thing about buffer fabric. (Refer slide time: 37:04 ? 37:35 )





We will look at some examples of packet switches, we will talk about three switches. That is, Ethernet switches, ATM switches and IP routers ? these are all packet switches. All of them hold very important positions in the world of networking; we will just look at these one by one. (Refer slide time: 37:35 - 37:51) 







Let us take up an ATM switch first. If you remember, in the ATM system, we have fixed sized cells ? in the ATM world, there called cells, then in the Ethernet world, etc., they are called cells packets, datagrams, etc. Anyway they are called cells; these are of fixed size, like 53 bytes etc. Each cell will contain a so-called destination address and the destination address is in two parts: one is called virtual path identifier and the other is virtual connection identifier ? VCI and VPI in the virtual connection table. What happens is that ATM is a set of virtual connections; what is a virtual connection? A virtual connection is somewhere in-between a physical connection on one side and completely connectionless, let?s say Ethernet packets, on other side. In a completely connectionless Ethernet packet, that packet contains the full destination and at each node in-between, this destination is interpreted, looked up into some table, etc., some algorithm is done, and then you decide what is the next step to go to. In a connection-orientated system, physical connection through the switches is set up from the source to the destination. In a virtual connection, virtual connection is also set up. So there is initial set-up time just as there is for connection-oriented, purely physical connection-oriented systems. Similarly for virtual connection also, we need a set-up phase and then you can start communicating. In the set-up phase, all the nodes in-between know that we are going to have a session, where the destination is going to be; what would be its next address, and so on ? it is computed, let?s say, as we do in Ethernet, etc. But once it is computed, it is coded in a very small VCI/VPI. The port one has to go to for this particular session is identified by the VCI and VPI. So this computation is done, recomputed at each of the intermediate nodes, and then the virtual connection is set up; that is all. There is no physical connection. So as soon as there is some cell, which is for this virtual connection, an intermediate node will immediately know that this is the next step and that is where it will push it. So you do not have to do a full-scale look-up, you have (Refer slide time: 40:29-41:40)




a very big table so we look up a very small table of VCI and VPI in the VC table. But, of course, just as the virtual connections are set up, they maybe taken down. So we replace the old VCI/VPI with new ones; forward cell to outgoing interface transmit cell onto link. This is how an ATM switch works. You will see that the basic concept of the cell   remains the same; that means, we have to find out the output port from the address. Once the output, the addresses are set up, it is simply a small table of VCI/VPI, which make this look-up process much faster. With this, together with the fact that the cells are of same size, you can develop your hardware in such a way, so the ATM switch will work very fast. Otherwise, the rest of the switch and switching principle remains the same as we have discussed already. (Refer slide time: 41:40- 41:55)

Next comes an Ethernet switch, which is perhaps the most common type of switch that you see almost everywhere. As a matter of fact, the number of Ethernet switches which are being used almost regularly, is so high that the cost of switches has also come down. It is because if you produce something in large numbers, you can amount the cost of its development, etc., over a large number of switches, so the cost of switches comes down. Ethernet switches have become very cheap these days (switches which used to cost maybe 1 lakh or so 10, 15 years back, cost only few thousands of rupees today). This is the most common type of switches, which we see everywhere. If it is an Ethernet packet, then it has to handle the Ethernet addresses which are in the packet. The Ethernet address is actually a 6-byte address, so we will have a 48-bit address. Usually we do not get to see it as users, because this is 6 byte; it is very difficult to remember such a long address, and these addresses come in a very random fashion. It is very difficult to remember Ethernet addresses, but human beings need not remember Ethernet addresses. Ethernet addresses are learnt by the system; when a system is put in a network where the system maybe a computer or a node or the system maybe a switch or a router, they learn about the Ethernet addresses, local Ethernet addresses, which are there. We will get into the details of Ethernet, etc., later on. But anyway this lookup is necessary. The first step in Ethernet switch is look up frame destination address in forwarding table; if known, forward to correct port; if unknown broadcast to all ports. So this is what an Ethernet switch does. Suppose a packet comes with a particular destination address, and if the destination address is known, the node having this particular address is connected to this particular port. Then the output port, for this packet?s output port, is forwarded to the correct port. If unknown, you broadcast it to all ports. Why is this? If you remember as I mentioned earlier, these switches automatically learn from whatever is going on; they automatically learn the MAC addresses of the local  machines. But suppose it is put in a network, in the beginning it does not know anything. What does it do? What it does is that, as soon as it comes from some port, it broadcasts it to all the other ports. It will listen and find out which is the one that really was successful and there are various protocols for finding these things out; it will slowly learn. It may so happen that this switch may get a packet for which the destination address is not known to the switch. In that case, it broadcasts to all the ports. Then we have, learn source address of incoming frame ? one thing of course is learned from this frame, although the destination address is unknown, it knows the source address; the source address is the particular incoming link it came in through. This particular address must be the node having a particular address, must be connected to this port so that later on if this particular node comes as a destination, it knows where to send it to. Forward frame to outgoing interface ? that is the other thing, transmit frame onto a link. These are the various functions of an Ethernet switch. (Refer slide time: 46:07-46:17)



We have something called an IP router; router as you know works at the network layer; it is not in the data link layer. That means a router has a different set of address called IP address; so IP address is a global kind of addressing scheme. There is some scheme for this IP address although not as good as the telephone numbering scheme, but anyway, there is some central control. The job of the IP router is to find out this global address that I have got. Maybe this particular node is in some other country; it has to understand that for going to that, reaching that final destination, I have to take this link locally. Then it will send it to the next level, that is, the data link level, for hopping that particular local layer. Once again, we have the same problem that we have got an address, some kind of destination address, which happens to be an IP address. We have to do that, look up, we have to decide this is the port it must go to and then send it to the port. So the same switch is used with some added processing capability for doing the process router ? usually it comes with all kind of other functionalities and also it needs some processor of its own and it needs some memory naturally for running programs, etc. There is some kind of an operating system which is there in a router, so a router is more complex but the hardware part, the major part of an hardware, is still the switch and the input and output line ports. Usually a router can handle different types of ports; for example, usually an Ethernet switch will handle only Ethernet port. So the physical characteristics or the physical layer  properties of the ports are constant for all the ports, but a router can cut different types of cards, which can handle different types of ports: maybe serial port,  high-speed serial port, and all kinds of things.  (Refer slide time: 48:27 ? 49:13)


But the basic thing remains the same look-up, the packet destination address in forwarding table ? if known, forward to correct port; if unknown, well, the router cannot broadcast it because if all routers start broadcasting then there will be broadcast all over the world through all the routers, which is not really acceptable at all. You can broadcast something only locally by an Ethernet switch; so if the destination is unknown, and it has no way of finding where to send it, it simply drops the packet. When it sends, it decrements the time to leave, update header, checksum ? so these are some of the side-effects which it has to go through ? let me not go into the details of this ? and finally forward the packet to outgoing interface, transmit packet on the link as usual. (Refer slide time: 49:14 ? 49:26)


This is a router ? once again we have a switching fabric, we have input port and we have output port, and we have a routing processor. (Refer slide time: 49:27-50:55)

Let me quickly look at some of the issues in developing fast packet switches. First thing you must know, understand, is that the demand for communication speed is growing much faster than that for computational speed. You know that in computers there is this Moore?s law, which tells that every one-and-half years maybe, the speed doubles; the memory capacity maybe doubled; etc. But if you look at the way growth has taken place in the communication field, especially with this speed of communication which is required, this is much faster than the similar developments in the computational area. There are two reasons for this: one is, of course, people are trying to communicate more. People previously were just satisfied with voice; now they want to send data, they want to upload or download large files, they want to send movies, etc. The individual demand for bandwidth is growing, that is one thing; and the other thing is that more and more people are getting connected to the network. As more and more people get connected to the network, the number of possible connections goes up as N2; if there are N people, as N goes up, it goes up in the squares so all this is going through the central backbone, etc. At the backbone level, the speed requirement is very high, so we require very fast packet switches and we will look at  (Refer slide time: 50:57 ? 51:08)

just some issues over here. The main bottleneck in an ultra fast switch is the speed of memory used in buffers. This is the main bottleneck. (Refer slide time: 51:09 ? 52:17)

For example, if you have a 5-nanosecond SRAM, that is, static RAM ? it is the faster kind of RAM rather than DRAM and you have 64-byte wide bus ?you have 5 ns per memory operation: that is your memory speed, two memory operations per packet. You have to write it there and you have to read it, so that makes it 10 ns and you have 64 bytes, which means 64 ? 8, that is, 512 bits ? so  you are handling 512 bits in 10 ns, which comes to 51.2 Gb/s. This looks like a very high figure but now, we get switches which have speeds of maybe 100 or even 200 Gb/s. They are very high speed, so the buffer memory is the maximum. This is not as simple as that; ultimately it comes to about 40 Gb/s or so. We use the fastest possible RAM but even that is not enough. So we can take some possibility: (Refer slide time: 52:18-52:33)

one is the ping pong buffering, which means the packet comes over here. There are two buffer memories: one is that either you send it or you put it in a buffer like this. (Refer slide time: 52:34 ? 52:59)

For example, when it comes, maybe one of them is being sent while the other is being buffered. Then this one will be sent and the next one that comes, will be buffered. So you are trying to double the speed at which it is operating. You require two buffer memories for it. That is one approach.  (Refer slide time: 53:00 ? 53:01) 


(Refer slide time: 53:02 ? 53:27) 

 The other approach is to use multiple parallel buffers; instead of two, you have many of them and you put them in buffer in parallel, so you can go about it maybe in a round robin fashion, put in an multiple buffer. And the other bottleneck is that, especially in a router, if that your processor, if that is centralized, then that becomes the bottleneck for speed. (Refer slide time: 53:27-53:35)

We have the fork, the so-called fork join router, where maybe you have put a number of routers in parallel. These lines are coming at a particular rate; they are distributed over all the routers so the routers individually do the look up, etc., at whatever speed they operate on. Then they keep on pumping this side as well as this side ? these are combinational so they can operate very fast ? the look up, etc., is paralyzed. These do not have any buffers in them, this part has the buffer. So this way, by going in for parallel operation, we try to make as fast switches and routers as possible. This speed is going on increasing as they have to increase in order to cater to the growing demand of packet switches, speed of packet switches, and capacity of packet switches, all over the world. Thank you. 



Preview of next lecture
Lecture No # 09
SONET/SDH
This lecture, we will discuss about SONET. The word SONET stands for Synchronous Optical NET work sonnet in the USA, Canada and Japan,synchronous digital hierarchy. elsewhere for example in India, We will be calling SDH, this is a time division multiplexing system that transmits a constant stream of information  (Refer the slide time: 55:14 ? 56:57)



Please look at this figure so we have some multiplexers. As the figure shows, we have a multiplexer in this side another which is an output fits to another multiplexer, this multiplexer is going in this direction and then after some time, the signal becomes weak. We want a repeater, that means what is a repeater? A repeater is something which boosts the signal strength, so there is a repeater then it travels some more distance and then there is a repeater again and then it travels some more distance and the other side. We have the corresponding demultiplexer and then it fits into the other  demultiplexer. From repeater to repeater, we call it as section so all from repeater to multiplexer this is also a section. Multiplexer to repeater, repeater to repeater, these are called sections and then from multiplexer to multiplexer we call it a line. At the repeater nothing happens except  the signal is cleaned up, signal maybe boosted or there maybe other cleaning operation synchronizing operation etc that maybe done at the repeater but as such the signals which are traveling here are the same set of signals are traveling here. At a multiplexer some of the signals may go off in another direction, some signals may go in some other direction etc. At the multiplexer there maybe convergence or divergence depending on which way the signals is flowing so that may happen at a multiplexer so from multiplexer to multiplexer, we call it a line and then from the end user part point to end user point, we called it a path. 
(Refer slide time: 56:57 ? 57:09)




The SPE does not necessarily start in column 4 which mean that the SPE that does not necessarily stay within 1 frame, these are 2 very important  points in. On it, the point is    that all through you have 87 columns but actually the data may start getting transmitted as some arbitrary  point  inside that 87 columns. What is the idea? Why do want to leave something and they only start from the middle?  The point is that if there are some kind of mismatches because of miss matches of rate etc, if everything in the world were absolutely synchronous, all activities an all equipments etc then would have started from the beginning but that is not the case. This is where we absorbed this kind of variation  and this gives a great flexibility to solve it which was not there earlier. Other interesting thing is that SPE does not necessarily stay within 1 frame which means, that the SPE start in 1  frame and then 1 in another, we will just look at a  diagram of this. Let us have a diagram of this, you see the SPE, this light green color that really starts from somewhere.   After leavening some of the root, starts hear in the path overheat is somewhere hear and there are 2 frame is here so the SPE is really spent in both  the frames. 
 

COMPUTER NETWORKS
Prof.Sujoy Ghosh
Department of Computer Science and Engineering
IIT, Kharagpur
Lecture#9
(Refer start time: 00:47)

Good day. In this lecture we will discuss about (Refer slide time: 00:54 - 01:16) 


SONET. The word SONET stands for Synchronous Optical Network, SONET in the USA, Canada, and Japan, Synchronous Digital Hierarchy elsewhere. For example in India we will be calling SDH. So this is a time division multiplexing system that transmits a constant stream of information. (Refer slide time: 01:17 - 02:10)


SDH is actually a successor of PDH. Few years back we used to have a PDH gear in our telecom infrastructure in the wide area network part that is plesiochronous multiplexing (nearly synchronous). This business of being a nearly synchronous introduces us a lot of problems and complications. So, when from this nearly synchronous we went to synchronous, that was a major achievement as well as improvement of services as we will see later on. In the PDH multiplexing in which two or more signals are transmitted and nominally in the same digital way and the significant instance occur at nominally the same time.  This was PDH. (Refer slide time: 02:10 - 2:34)


When SONET was introduced, it had a number of achievements to its credit. Firstly, it is a standard multiplexing using multiples of 51.84 mbps, that is an STS-1 rate and STS-N ? we will look at this rates. This was used as building blocks. This is something that is to be understood that why it is that particular rate value is so important. The point is when you are multiplexing the original source may come from various sources, and these signals will travel, will get together, will separate out and then mixed with others, etc. That is possible only when we have in international multiplexing standard and this international multiplexing standard was first achieved in SONET. What happened was previously of course the rate which people used were must less and they had all kinds of differences. As technology grew and different sort of countries on different  things came together, they came together at a certain rate of transmission and this is a basic building block of SONNET. There is a standard multiplexing using multiples of this particular day. That in itself was a big achievement. (Refer slide time: 03:50 - 04:23) 


Secondly, it also first stated the optical signal standard for interconnecting multiple vendor equipment. the point was previously of course at lower rates they were all electrical signal standards, SONET has both has electrical signal standard as well as optical signal standard and in this optical signal standard it was possible to bring together multiple vendors to agree on to some particular format. (Refer slide time: 04:23 - 05:23)

And the third achievement in SONET was extensive OAM & P capabilities. So what are OAM & P? O is for operation; a is for administration; and m is for maintenance; P is for protection. Maintaining the system, administrating the system, operation of the system etc., are much more flexible in SONET when compared to others. What kind of flexibility, etc., that we will see. Regarding protection also, as a matter of fact it is so strong in SONET, that we will specifically discuss this aspect in a separate lecture when we talk about protection. These are very strong points in SONET and that?s not all. (Refer slide time: 05:23 - 06:50)

The fourth one was multiplexing formats for existing digital signals. It?s not that such a development can take place in vacuum ? that means they had some history and the trouble was that different countries have different kinds of histories. It is not feasible for a technology to come and say throw away whatever you have been doing and put this. There will never be forklift upgrade; it is never possible because of cost, practical considerations, and all kinds of things. So an evolving technology, in order to be successful, has to bring together previous technologies so that they can merge into this new technology and that was another SONET achievement. These existing digital signals ? these DS1, DS2, etc., are different multiplexing standards at the low end. By the way there is also DS0 and DS0 rate is our venerable 64 kbps line rate. Do you remember once again that for voice channels, we require a 64 kbps because of PCM, etc. we have already discussed. So that is a DS0 rate and these several DS0 get together to form DS1  and so on, and that way, there is a hierarchy of rates. (Refer slide time: 06:50 - 08:25)



Then the fifth achievement of SONET was that it supports ITU hierarchy: E1, etc.  so this ITU hierarchy was more popular in Europe, India, etc. and they had rates like E1, E2, E3, etc. E1 was something like 2 mbps, and then E2 was 1mbps, and E3 was 34 mbps, whereas the development in USA was on a different track. They had a tone rate T1, T2, T3, etc. Their rates were as above: DS1, DS2, etc. What happened was that when SONET got introduced, these two sort of came together and although they are not perfectly identical ? these SONET and SDH ? for most of the part, they are identical; they interoperate with only very slight modification at the boundaries, which is not very important. That is a great thing and that means that the same standard is being adopted worldwide, so that any signal can be transported in any way. If there is an infrastructure, we can transport in anyway to another part of world; there is no problem. So bringing together of these, that means bridging the Atlantic Ocean of these two standards, that was another good achievement of SONET. (Refer slide time: 08:25 - 08:49)
 

The next is that it accommodates other applications. The other applications which were not a part of this kind of hierarchy, like BISDN, that is broadband ISDN, also can be accommodated in SONET and that way you see SONET was quite flexible; and how this flexibility is achieved we will see that later on. (Refer slide time: 08:49 - 09:32)

Finally it allows quick recovery from failure, talking about protection, etc. So if there is a failure like a line failure or if there is a terminal equipment failure you can deploy a SONET in a particular fashion and SONET can recover from this failure and this  retransmission, etc., can take place in a short period of time. That is very important when you want to give the so-called career great service, where arbitrary down time is absolutely not acceptable. As I said, we will discuss this separately in another lecture. (Refer slide time: 09:32 - 11:02)



Some of the broad features of SONET and SDH: it was first standardized by ANCI/ECSA, SDH by ITU?T. So SONET was by this ANCI/ECSA and SDH by this ITU?T. SONET is time division multiplexing, pure. We know what time division multiplexing is, and we will see later on how frames, etc. are made up. It is a pure time division multiplexing system. SONET encompasses optical and electrical specifications, so there are optical specifications as well as electrical specifications. You know that usually at the user end, quite often things start at the electrical level and the rates are low. But as you go more towards the backbone of the network, the rates that are needed at the backbone start becoming higher and higher and finally at the real backbone it has to be very high-speed network, and such high-speed networks are only possible through optical communication and optical networking. Once again, we will see about optical networking in the next couple of lectures. Our specification, the SONET specification, spans both the electrical side as well as the optical side, and that is a very good feature of SONET. (Refer slide time: 11:02 - 11:42)



SONET uses octet multiplexing, octet means the same thing as a byte that means 8 bits,  so sonet uses octet multiplexing. They are multiplexed byte by byte. SONET uses extremely precise timing, something like in 30 years, maybe; SONET has very precise timing and that is why things are synchronous. And if things become synchronous, then we derive a lot of advantages out of that. And SONET provides support for operation, maintenance, and administration (OAM) as we have already mentioned (Refer slide time: 11:42 - 14:24)
 

SONET is actually superior to T3 and T4, etc. with improvements over the T carriers; these T3, T4 are still in use but they feed into SONET nowadays. But earlier, they were used to feed into this PDH and these T3, T4 have particular rates which existed, and their specification left something to be desired. Because of this lack of synchronicity,  handling the signals from different sources is not easy. What could happen is that when things are not synchronous, but just almost synchronous, then to handle this ?almost? part, you have to do something; you have to incur some overhead; and you have to incur some complexity. That was the difficulty with PDH; in SDH or SONET, this is eliminated, and we get better transport performance. Then, we have the ability to identify sub streams. This was another advantage of SONET over PDH, which is that a particular user uses may be using a very small kind of bandwidth ? small in relative sense ? and then, as more and more users, as I said as all these data streams or communications streams come towards the backbone of the network, the pipes tend to get fatter. That means, we need faster and faster communication. So between say two points in the backbone, there may be a very fast communication going on and then after going to some other hops, this will again diverge. SONET has this ability that different streams can get together, travel for some time, and then again diverge. So the ability to identify sub streams is very important, and that is also allowed in SONET, which was more difficult in the RDR system. And of course international connectivity, as I said that it breached Atlantic and that was great. It enhanced control and administrative function, that was also very good from the point of view of service providers. (Refer slide time: 14:24 - 16:00) 


We have talked about this seven-layer OSI protocol; where does a SONET SDH really fit in? SONET SDH goes to the bottom of this. If you remember, starting from the application layer, we go  right up to the physical layer. There are several layers in OSI model, and there are other models. Anyway, usually the bottom-most layer is always the physical layer. So SONET really fits into the physical layer in some sense. So what would happen is that the layer just above the physical is the data link layer, may be, or layer two. So after all this encapsulation, etc. is over through all these six other layers including the data link layer, SONET takes it over for transporting it from one point to another. So SDH is placed at the bottom of the protocol stack in the physical layer along with the fiber. Any IP traffic even if it is the IP traffic of a packet oriented traffic ? and remember that SONET is a TDM system ? it can sort of travel within a sort of TDM transport as they quite often do. So any IP traffic that is destined to be transmitted across a fiber-based SDH network will be framed by a layer two protocol before being ready to take its orders from the SDH equipment. (Refer slide time: 16:00 - 17:59)


These are some of the multiplexing standards ? I have not given all of them I just indicate some of them. If you remember as I mentioned DS0 is a 64 kbps channel and 24 of them constitute a T1 line. So T1 rate is approximately about 1.5 mbps; 4 T1 gives T2 and 6 T2 gives T3 and so on. Similarly 30 DS0 ?this is a European system ? gives E1 line. So E1, if you remember, is about say 2 mbps: 4 E1 gives E2; E3 is a 34 mbps line. And then I suddenly jump right up to this thing called OC3;  this o is for optical. So this way, this 155 mbps is 3 of the basic STS 1 rates that I mentioned earlier; I will come to this later on. So these are some of the standards. There is a whole hierarchy of standards; for example, this name SDH is also synchronous digital hierarchy, this is a hierarchy. For the SONET, the basic rate is STS 1 that is synchronous transport signal level 1, and the speed is 51.84 mbps. This is designed to carry what was DS3 RDR or a combination of DS1 DS1 c, and DS2 etc. As I said a combination of different streams can flow through a SONET pipe or SONET infrastructure. So that is good and that means DS3 is a fat pipe or DS3 is almost the same as STS-1. So it is a fat pipe through which multiple pipes, say may be DS2 or DS1, etc. may travel. (Refer slide time: 18:03 - 18:25)

 
And this net goes up to STS-N, whereas synchronous transport signal level is N; so this has a speed of N ? 51.84 mbps designed to carry multiple STS -1. I mentioned that these are byte multiplexed STS-1 means 1 byte from one source and another byte from another source and so on. (Refer slide time: 18:25 - 19:49)

Fundamental SDH frame is STM -1;  SDH if you remember is the other standard, which came from Europe and they sort of came together and that is what we are talking about. SDH frame is STM -1 synchronous transport module and the SONET version is OC -3, that is, optical container, each providing 155 mbps. So when we come to this rate this 155 mbps OC 13, different rates etc. and different systems, are culminated here, at this 155 mbps, almost 155. STM 4 provides four times the STM -1 capacity, STM 16 provides a further fourfold increase, which means STM4 may be about 620 mbps, and then, if you go to STM16, which is four times that of about 2.5 giga bit/s, then you have STM 64, which is about 10giga bit/s. So all these rates are there; that means, from this point onwards, these two streams have converged and we are going to higher and higher rates in a sort of universal fashion, which makes things easy across the world. (Refer slide time: 19:50 - 20:17)

It is worth noting that the inter networking between SDH and SONET systems is possible at matched bit rates; for example STM4 and OC12; so they interoperate. A slight modification to the overhead is required as they are structured little differently so there will always be a little something; but anyway that is not very serious. So they do interoperate. (Refer slide time: 20:17 - 20:41)

We have seen the SONET electrical hierarchy; now we look at the SONET optical signal hierarchy: OC-1 is the optical career, level 1; it carries STS-1; OC 3 carries STS-3 or STM -1 at 155 mbps; OC-N optical career level N. (Refer slide time: 20:41 - 22:16)

OC - N as I mentioned is an optical carrier, which uses N ? 51.84 mbps, so  OC - 48 is about 2.4 gbps; overhead percentage is about 3.45%. OC signal is sent after scrambling to avoid a long string of 0s and 1s to enable clock recovery. This is a small technical point; that means in order to keep the whole thing synchronized,  the SDH  units use the transitions which happen when there is a 1. So the point is that if there is no 1 for a very long period in the data stream, then the clock on one side may drift relative to the clock on the other side; that is always possible. So we try to avoid long streams of 0s in this SONET or SDH, and we do that by scrambling the data from various streams, etc., or descrambling them. The idea is that even if one of them is sending a long stream of 0s, there will be quite a few 1s from the other streams and then the clock will be maintained. An STS -N is synchronous transport signal electronic equivalent of optical carriers. (Refer slide time: 22:16 - 23:29)

OC 3, OC12, OC 24 and OC 48 rates are common in telecom circuits ? if you remember OC 48 is 16 times of OC 3; that is, 16 times 155 mbps, which is about 10 gbps. Upto 10 gbps is very common these days. Actually right now, with DWDM systems, OC 192 rate is already in operation, and OC 768, which is 40 gbps, is being talked about. So that was another disadvantage earlier that this digital hierarchy of standard rates did not exist beyond a very small rate ? I mean small in today?s comparison. But now we have an extended and open system where, as technology improves, we can always go for higher and higher rates; so from OC 3, which is 155 mbps, we can go to maybe OC 192, which is 10 gbps or OC 768, which is 40 gbps that we are talking about now. (Refer slide time: 23:29 - 24:11)

How do you use these high-speed links? These high-speed links of course have to be on fiber ? we can look at details of fiber later on, but please note that in practical application, an SDH line system will have a multiplexer that takes its inputs from a variety of sources in different layer 2 data formats. So here we are talking about these different signals coming in the electronic domain, and they are coming from a variety of sources, may be coming with different layer 2 data formats. These are aggregated up to form frames at a line rate of system, for example up to STM 64 for a 10 gbps bit rate system. (Refer slide time: 24:11 - 25:08)

Now these frames at 10 gbps cannot be pumped anywhere. It is very difficult to pump it on a copper. So these frames are transmitted out onto optical fiber links. There is a possibility of multiple SDH multiplexers to each give out one wavelength of a WDM system. As we will see later on, this WDM stands for Wave Length Division multiplexing, which is some form of frequency division multiplexing. I mentioned about it when I talked about frequency division multiplexing. In fiber optics, we talk about wavelength multiplexing so it is possible that one multiplexer is feeding into one wavelength, another multiplexer is feeding into another wavelength, and all these different wavelengths are traveling together in the fiber. (Refer slide time: 25:08 - 25:44)

At the end of the system, there will be an SDH demultiplexer on the other end, just as we have a multiplexer on one side. Naturally, you have to have a demultiplexer on the other side that now accesses the individual data streams from the STM 64 frames as required. So STM 64 is carrying lots of frames in a very short time; they are sort of separated out and then fed into slower streams down the line. So there may also be an SDH add drop multiplexer with the ability to remove and insert lower bit rate streams from the signal. (Refer slide time: 25:44 - 26:33)

Alternatively a digital cross connect may be present with the ability to switch individual VC4s. Well, this is virtual container four, which is another concept, we will talk about later. So between different fiber links there is a digital cross connect; if you have the digital cross connect in the optical level, the advantage is that you need not go into the electronic domain at all. So the advantage of not going into electronic domain is that you are handling a huge, very fat, pipe; that means, a large number of channels, and you can just switch them from one fiber to another fiber simply in the optical domain without doing any kind of processing; and that is always an advantage. (Refer slide time: 26:33 - 27:50)

We will talk about some SONET terms now; for example, envelope. This envelope is the payload. Basically, after all encapsulation, etc., you remember that finally near the bottom we have this layer 2 and this layer 2 protocol will encapsulate it and then hand it over to SONET at the lower level, maybe at the physical level. So whatever this layer 2 hands over to SONET is the payload; the rest of it are kind of system overheads ? payload plus some end system overhead also goes into this payload. So these together form what is known as the envelope; this is a SONET term. Other bits and bytes which are used for management, that means OAM and P portion, goes as the overhead of SONET. Then there is the concept of concatenation; that means, unchannelized envelope can carry super rate data payload, for example, ATM, etc. So, the method of concatenation is different from that of T carrier hierarchy; we need not bother about it at the moment. (Refer slide time: 27:50 - 28:23)

Then there are some nonstandard functional names in SONET, like 
TM is for terminal multiplexer, also known as line terminating equipment or LTE. These are ends of point-to-point links. ADM is for add drop multiplexer; we have mentioned this. DCC is for digital cross connect wideband and broadband; MN is for matched nodes  and D + R means drop and repeat, etc. Anyway, these are just some terms. (Refer slide time: 28:23 - 28:40)

Now let us come to some important concepts in SONET namely: section, line, and path.  What is a section? I will just show you figure first and then come back to this. (Refer slide time: 28:40 - 30:24)

Please look at this figure: we have some multiplexers. So as the figure shows, we have a multiplexer in this side, another is an output that fits to another multiplexer. This multiplexer is going in this direction and after some time, the signal becomes weak. So we want a repeater; what is a repeater? A repeater is something which boosts the signal strength. So there is a repeater, then it travels some more distance then there is a repeater again and then it travels some more distance and then on other side we have the corresponding demultiplexer and then it fits into the other de-multiplexer. From repeater to repeater, we call it a section. So from repeater to multiplexer, this is also a section. So multiplexer to repeater, repeater to repeater, these are called sections. And then, from multiplexer to multiplexer, we call it a line. At the repeater, nothing happens excepting the signal is cleaned up. The signal may be boosted or there may be other cleaning operation, synchronizing operation, etc., that may be done at the repeater; but as such, the signals which are traveling here, the same set of signals are traveling here. At the multiplexer, of course, some of the signals may go off in another direction; some signals may go in some other direction, etc. So at the multiplexer, there may be a convergence or divergence, depending on which way the signal is flowing. That may happen at the multiplexer, so from multiplexer to multiplexer, we call it a line; and then from the end user point to end user point, we call it a path. (Refer slide time: 30:30 - 30:57) 

Look at this once; the portion from a multiplexer to a repeater is known as a section or it could be a repeater to a repeater also; the portion from a multiplexer to another multiplexer is a line. The portion from source to destination multiplexer is a path; below path line and section is the photonic sub layer; that means photonic sub layer is whatever is happening in the optical domain, and we are not discussing that at the moment. (Refer slide time: 30:57 - 31:14)

Sections are bounded by repeaters or multiplexers that terminate the line; lines may carry several tributary signals and are bounded by multiplexers, a path goes end to end between terminating multiplexers. (Refer slide time: 31:15 - 33:49)

Each STH frame lasts 125 microseconds. As I mentioned, this 125 microseconds time period, time epoch, is sort of sacred in this whole domain because 125 microseconds is what is required for a DS0 channel. Remember this is a time division multiplexing, which means that if you have a 125 microsecond kind of slot, then some of the DS0 bytes can  take these bytes. Actually if you have to take it as 8 kbps and if it is 8 kbps, inverse of that is 125 microsecond. So if you have a 125 microsecond slot, if 1 byte travels in this frame, then that is enough for 1 DS0 channel. In SONET we have very very sophisticated and very fast equipment; that means this is a time division multiplexing system; within this 125 microseconds, not only 1 byte can go but lot of other bytes can go. That means a lot of channels can travel together in this 125 microseconds frame. This is the idea. So each STH frame lasts 125 microseconds; how many bytes are going in there depends on whether it is STS -1 or STS -2 or STS ?N, etc. So 125 microsecond as I mentioned is 8000 frames/s. STS -1 frame has 6480 bits or 810 bytes. That means in one, 125  microsecond slot or frame, we are putting in 810 bytes. Theoretically, of course, that means it can carry 810 DS0 or voice signals; actually it is not 810, it is lesser than that because a number of these bytes are used for different types of overheads. We will talk about this. We have these 810 bytes, the octets are understood in terms of a table of 9 rows and 90 columns; so let us look at this figure. (Refer slide time: 33:49 - 36:16)

We have a SONET frame or an SDH frame, which has 9 rows, you can see the 9 rows on this side and then 90 columns, total 90 columns. Out of these 90 columns, 3 columns have been shown in yellow. These are sort of used for overhead and these 87 columns are  used for payload or for envelope. If you remember, the envelope contains the payload as well as little bit of overhead, which we will come to later on. This is how after every 90 bytes, we come back to again another 3 bytes of this overhead. This is how it is to be understood: the first 3 columns contain transport overhead and TOH has 9 rows by 3 columns, that means 27 bytes, which is subdivided into section overhead SOH (section overhead), 9 bytes, 3 rows of 3 columns; LOH, that is, line overhead, which is 18 bytes, that is, 6 rows of 3 columns. So we have section overhead and we have a line overhead ? remember we have these three concepts like section, line, and path. We have not talked about path overhead. There is some path overhead and it goes into the envelope; so there is some path and as far as these things as line and section are concerned, these are the overhead bytes. Just to clarify why do we require the over bytes ? the point is that the multiplexers or the repeaters have to have some communication between them in the control plain so as to give you this OAM  capability. For that some information needs to be sent or exchanged between the two points; anywhere there is a section, the section overhead would consider those things which are central to the section about the signal strength and other kind of things; line overhead maybe would contain something else and similarly path overhead would contain something else. But these are required for these OAM  capabilities that we have in SONET. (Refer slide time: 36:16 - 36:49)

Let us look at these overheads separately; first section overhead, which defines and identifies frames and monitors section errors and communication between section terminating equipment. So these are its functions: it identifies frames; monitors section errors ? if there are errors, it monitors section errors; and communication between section terminating equipment, maybe two repeaters or a repeater and multiplexer, and so on. (Refer slide time: 36:49 - 37:29)

Line overhead locates first octet of SPE and monitors line errors and communication between terminating equipment. We will come back to this locating of the first octet of SPE. This is a very interesting feature and we will talk about this separately. Previously we talking about section errors; so line errors and communication between terminating equipment, etc., is taken care of by the line overhead. Apart from that, line overhead contains this pointer, really, which points to the first byte of the SPE. (Refer slide time: 37:29 - 38:36)



And then there is a path overhead; and as I said path overhead is really inside the envelope and we will look at all these later. Path overhead verifies connection  path; you remember path means from end to end; that means from the end to end multiplexer is a path. Whether the connection has been established or not, it monitors path errors, receivers? status, communication between path termination equipment, and so on. This is the POH . We talked about the synchronous payload envelope or SPE that I was talking about. That is, the other 87 columns hold the SPE (synchronous payload envelope). So SPE has 9 waves by 87 columns, which are divided into path overhead and payload, which means the path overhead goes along with the envelope that is in the SPE,   whereas other overheads have separate bytes or separate columns associated with them as shown. (Refer slide time: 38:36 - 39:52)

Now this SPE does not necessarily start in the column 4, which means that the SPE does not necessarily stay within one frame; these are two very important points in SONET. The point is that although you have these 87 columns, actually the data may start getting transmitted at some arbitrary points inside those 87 columns. What is the idea? I mean why do you want to leave something and then only start from the middle? The point is that if there are some kind of mismatches of late, etc., if everything in the world were absolutely synchronous, all activities and all equipments, etc., then you could have started from the beginning. But that is not the case and this is where we absorb this kind of variation and this gives great flexibility to SONET, which was not there earlier. And the other interesting thing is that the SPE does not necessarily stay within one frame, which means that the SPE may start in one frame and then end in another. We will just look at a diagram of this; let us have a diagram of this. You see (Refer slide time: 40:00 - 40:21)

the SPE in light green color; it really starts from somewhere. I mean somewhere after leaving some of the rows: it starts here, and the path overhead is somewhere here, and there are two frames here. So SPE is really spanning both the frames. (Refer slide time: 40:25 - 40:57)

SPE is not frame aligned; it overlaps multiple frames; avoids buffer management complexity and artificial delays. Whenever there is something to send, you can just send it in the envelope; just put that pointer to that yellow edge, so that yellow edge will point to the first byte of the SPE. It allows direct access to byte synchronous lower level signals, for example, DS1, with just one frame recovery procedure. (Refer slide time: 40:5 7 - 41:33)

These are the advantages of the SONET frames. This is one frame coming in may be 125 microseconds; this is the next frame; and SPE, as I have already shown, can overlap. I mean it may start somewhere within the first frame and then continue in the second frame in this fashion and then be over here. Actually after this, some other envelope may come in over here. (Refer slide time: 41:35 - 42:34)

Now of course where is the path overhead? There are two fields, H1 and H2 in LOH; LOH means line overhead, which points to the beginning of the path overhead. Path overhead beginning floats within the frame; 9 bytes that is one column may span frame along with the SPE; it is originated and terminated by all path devices; and this gives you end-to-end support. These are the features of path overhead. The point is that if you remember the path is end to end, that means it is close to the end users; just as the end user may start somewhere arbitrarily in-between, a path overhead also goes along with the SPE and it starts over there and at LOH, we keep a pointer to this path overhead. (Refer slide time: 42:40 - 44:17)

Just as some of the equipment that we use in SONET, one of the most important of these is the add drop multiplexer. They are important because at certain point in the network, what might happen is that there are some sources which want to send into the network. They will sort of go so there is this SONET equipment, which is ADM let us say, and SONET stream is flowing let us say like this. There may be something that wants to upload and travel along with this thing. At the same time, this may be the destination location for some of the other signals which originated elsewhere; they have to be dropped here. So some signals have to be dropped, some signals have to be added. So this multiplexer can handle that and that is very important. That is why they are called add drop multiplexers. This stream is itself of course flowing at a tremendous rate, whatever that rate is. So SONET SDH is a synchronous system with the master clock accuracy of 1 in 109, which you will see is highly accurate. It shows when you come in some kind of   CCM clock somewhere and then there is a protocol for distributing and maintaining this clock over the entire network. Frames are sent byte by byte and ADMs can add drop smaller tributaries into the main SONET SDH stream and I have explained how that is done. Within that frame you can send lot of bytes; you can take out some of the bytes and add some of the bytes. That is how you take out some of the smaller tributaries and add some of the smaller tributaries. (Refer slide time: 44:22 - 44:58)

Digital cross connect, which is an optical layer equipment, is also very important. It cross connects thousands of streams and software control, so it replaces patch panel; that is a good thing about the digital cross connect and a software control is coming where the control is coming from the control plane of the switches. You can connect the streams from may be one fiber to another; it handles performance monitoring, PDH SONET streams, and also provides ADM functions; that means add drop multiplexing functions. (Refer slide time: 44:58 - 45:47)

Finally we have this concept of grooming in SONET. Grooming means, we group the traffic in some format. So you want to keep this group in one particular way; it could be that there is a one group of streams for whom you want to give higher priority or you want to give higher quality of service. So you have to group them together. Similarly there may be multiple groups; so it enables grouping traffic with similar destination, QoS, etc., which is a part of grooming. It enables multiplexing or extracting streams also ? that is also part of grooming. Narrow wider broadband and optical cross connects may be used for grooming. (Refer slide time: 45:47 - 46:40)

If you look at this figure, you have this narrow band, this SONET layer and optical layer. In the narrow band, we have this DS0 grooming and then in the DS1 grooming, there is a white band and then the broadband DS3 grooming ? so the rates are going up, starting from the 64 kbps, it is going up. When you are going up for the STS 48, you are in optical domain; that means STS 48 is STM 16, so that is a high rate. The point is that, at that rate, most probably, you are well in the optical domain. Then, finally, you can go to all optical domain; that means wavelength, waveband, and fiber grooming ? there are different levels of grooming, depending on what you want to do.  (Refer slide time: 46:42 - 48:00)

Lastly we will just talk a little bit about virtual tributaries or containers. We have already talked a little bit about it. This is the opposite of STM; actually in some sense this is called sub multiplexing; that is, different streams coming together to form one very fat or very fast stream. This is the other thing ? how do we, sort of, differentiate these sub streams within this, which has to do with sub multiplexing? STS -1 is divided into 7 virtual tributary groups, SDH uses the term virtual containers or VCs. We talked about VCs, we just mentioned what are called VTs or virtual tributaries in SONET lingo. So we have 7 virtual tributaries, 12 columns each, which can be subdivided further. You see that there are 12 columns each, with 7 virtual tributary groups ? we have got 84 columns and these 84 columns are out of the 87 you have in STS -1. (Refer slide time: 48:01 - 50:00)

VT groups are byte interleaved to create a basic SONET SPE. So this VT groups are byte interleaved. They may be again extracted from each other. VT 1.5 is the most popular, quickly accessed, T1 line within the STS-1 frame. So the idea is that you have a T1 line, which is approximately 1.5 mbps line, which is coming out of your small business, and you have a 1.5 mbps line. So that is your bandwidth requirement, you want to connect it to a distant location somewhere. And you do not want your thing to get mixed up with others. At the same time, as a small business you cannot have infrastructure of connecting to another location which is wide apart. So you will go with this public infrastructure or public switched tele PSDN network or whoever is maintaining this communication equipment. Usually telecom people maintain it in most of the places. Anyway, they have a sort of fiber going from one place to another, which contains very high-speed links. What you want is your T1 line should join them, sort of get transported over the distance and then go and feed into another T1 line at the destination. That is what you want. You want your T1 line to sort of have a separate sort of existence ? just like in a compartment, we have different passengers. Passengers have their own individual entity but together they are packed into one compartment and then they travel. Similarly your T1 line is going to ride onto to this very fast stream and travel to the destination. So VT 1.5 gives your T1 line. (Refer slide time: 50:00 - 51:16)

How do you find out about the difference? How do you separate them in the SP? The point is, you require one more level of pointer used to access it. You can access a T1 with just a 2-pointer operation, first from the LOH ? you remember, you go to the SP, just like that. Similarly, you go to the different tributaries or different containers using just one more level of pointer. This flexibility was not there earlier; so it was very complex to do the same function in DS3. For example, accessing DS0 within DS3 requires full demultiplexing, stacked multiplexing, etc. So you require full demultiplexing; that is not required in  SONET. The point is that the other streams may go; you know where in that frame your bytes really are traveling for the stream or for the container or for the tributary that you are interested; you just extract it, others keep on traveling as they are. So you do not demultiplex the whole thing and that gives a great advantage of add drop multiplexing. (Refer slide time: 51:16 - 52:02)

This is just a figure showing that you can have various types of lines, all feeding into the same infrastructure. You may have what we have put over here: DS1, which is 1.544 mbps, E1, 2.048 mbps, DSIC DS2, DS3, ATM .48.384, E4, which is 139.264 mbps, ATM is about 150 mbps, etc. They are sort of traveling; they are getting in different containers. From VT 1.5, different tributaries, that is 1. 5236 etc., form a VT group and  ride on a higher strength or higher speed stream. (Refer slide time: 52:02 - 53:55)

Just as I said, these are sort of identified through a pointer; so we have this transport overhead. We use some bytes for that out of those 87 columns we have. So we use some columns of that and then we put a pointer, which gives to the STS payload pointer. Then there is a VT pointer, virtual tributary pointer, and this much is the VT SPE within the overall STS-1 SPE, which is the payload. Even now SONET is the most widely used technology in wide area networking that is existing today. Of course, as you know, as technology grows, may be we will go out of SONET. People are already talking about going out of SONET because one disadvantage of SONET is that its equipment tends to be expensive. Well, expensive compared to what we think today. What is cheap today and what we think is cheap today may sound very expensive tomorrow; that is how the technology grows. So people are talking about direct transport over the optical layer, etc. May be we will touch those aspects later on. But all that is still in a sort of experimental stage and on the field, actually, SDH or SONET equipment is almost everywhere; all types of telephone companies are connected through that and major service providers use this as a means of transport. Thank you. (Refer slide time: 53:58 - 54:11)


Good day,so today we will be speaking about  fiber optic  components (Refer slide time: 54:11 - 55:22)

and fiber optic communication  as might of heard this lecture as well as the next couple of lectures, we will concentrate on fiber optic components. We have looked at some of the physical  layer components of  fiber optic systems before so we will sort of quickly review that, some of  the stuff we will be talking about today is going to be common and then from that point we will take out take it up into WDM systems, how wavelength division  multiplexing is done and how systems are  handled in fiber optic domain, this fiber optic domain happens to be very crucial because   a lot of  traffic in terms of volume   may be  as much as forty to fifty percent, actually  goes through the fiber  as days are going by and as more and more demand for bandwidth  is coming up fiber optics is becoming more and more important , we will be talking about fiber optic components today.   (Refer slide time: 55:22 - 55:56) 

In fiber optic component of course the basic fiber is there we have already talked about it, so we will talk little bit more about this then we have light source and receivers on two end  because we know that in fiber optic  cables light is the carrier of information then we require these different components like amplifiers, couplers, modulator, multiplexer and switches so we will look up at these components one by one and then we will start our discussion on wavelength division multiplexing.  (Refer slide time: 55:56 - 56:42)

The next set of components are multiplexers filters gratings, just talk little bit about it ,if you look at this wavelength , these are all; wavelength selective , devices multiplexers, filters , these are wavelength selective devices in a wavelength filter and what we want is suppose ?1, ?2 etc so many are coming, I want only ? 1 out   2 ?3 ? 4 etc are absorbed or something where as if you are a multiplexer I want the difference  this ? coming in different lines, I want all to be mixed together and use the same  line, these are wavelength multiplexer  (Refer slide time: 56:42 - 58:07)

so application could be particular wavelength or a particular wave band selection. Wave band is nothing but some contiguous  operating wavelengths which all are side by side,  if you remember  that in the operating window what ever be that  1550  what ever may be the window you are using there you can have a number of ? all side by side, there is a guard band between each of these operating ? so where the guard band that is given by the i q t has specified, how much guard band etc you have to have but so you can have large number of ? all group together in the same window. Aband out of that means a bunch of sequence is out of that you can short select instead of selecting only , that is wavelength band selection static wavelength cross connects and OAM is optical add drop multiplexers, you have come across this term optical add drop multiplexers in the context of  Sonet but optical domain we require optical add drop multiplexers, we will come to that. Equalization of gain so that is another application filtering of noise ideas used in laser operation and dispersion compensation modules etc, these are the different applications (Refer slide time: 58:08  -  58:15)

one of the standard wavelength selective component is arrayed waveguide gratings ,we have seen this before? 

COMPUTER NETWORKS
Prof: Sujoy Ghosh
Department of Computer Science and Engineering
IIT, Kharagpur
                  Lecture#10
(Refer Start Time 00:43)
Good day. so tToday, we will be speaking about [noise] fiber optic [noise] components (Refer slide time: 00:53 ? 00:56 min)

and fiber optic communication. as might of heard tThis lecture as well as the next couple of lectures will concentrate on fiber optic components. We have looked at some of the physical [noise]layer components of [noise]fiber optic systems before, so we will quickly review that. that so sSome of [noise]the stuff we will be talking about today is going to be common and then, from that point, we will take out take up WDM systems, details of how wavelength division multiplexing is done and how systems are [noise]handled in fiber optic domain. tThe fiber optic domain happens to be very crucial because [noise]a lot of [noise]traffic in terms of volume, okmay be as much as forty to fifty percent 40?50%, actually follows [noise] goes through the fiber. As days are going by and as more and more demand for bandwidth [noise]is coming up, fiber optics is becoming more and more important. [noisesSo we will be talking about fiber optic components today. [noise] (Refer slide time: 02:05 - 02:39 min)

sSo in fiber optic component, of course, the basic fiber is there; we have already talked about it, so we will talk little bit more about this. Then we have light sources and receivers on twotwo ends because we know that in fiber optic [noise] cables, light is the carrier of information. Then we require these different components like amplifiers, couplers, modulators, multiplexers and switches. sSo we will look at these components one by one. and tThen we will start our discussion on wavelength division multiplexing. [noise](Refer slide time: 02:40 ? 03:34 min)

sSo optical fiber as was mentioned earlier is very pure and very transparent silica glass is used. so aAt a moderate dimension, the light is restricted to the fiber because of total internal reflection for ordinary light; this is a multimode fiber. ok mmf MMF is used in lans LANs for low speeds or short distances. so mmf MMF may be used in lans LANs; that is another kind of fiber. bBut the multimode fiber happens to be the cheaper variety and this is used in lan LANs, but this is good only for low speed. wWhen iI say low speed iI mean comparatively low speed, ok for this say hundred mbps 100 mbps or hundred and fifty five mbps 155 mbps may be a low speed and by short distances may be couple of kilometers at [noise] the maximum. (Refer slide time: 03:34 - 03:51 min)

At a still smaller dimension, that part of the fiber that actually carries the signal ? if you remember our discussion about, the fibers in physical layer, we have shown some diagrams regarding the cross section of  a fiber, so at the very core ? is a very thin strand of glass fiber. tThis is surrounded by a cladding which is also made of glass; actually we will not be able to see the actual part of the fiber, which is carrying the signal. tThe surrounding is also made of glass and that is again coated or covered etc., by outer protection. So if that strand happens to be even smaller, say 8 to 10 nanometer range, tthen it acts like a wave guide and a single mode of operations. (Refer slide time: 04:39 - 05:10 min)

wWe will not go in details of eElectro mMagnetics and wave guide propagation of optics to this fiber, but any ways, the support of the single mode of propagation is called single Mode Fiber. SMF is used for higher speed, so all these speeds of 2.5 GB/sec or 10 GB/sec etc., are possible on Single Mode Fiber and it also goes over longer distance ok and nowadays, we have fibers which span across oceans. We have fiber from one continent to another, which is really a marvel of engineering and technology. (Refer slide time: 05:31 - 
05:50 min)

So these fibers are actually single mode fibers. There are a few transmission windows like 1310 and 1550 nm bandwidth etc. fifteen fifty nanometer 1550 nm window is preferred for long haul applications because it has less attenuation, wider window, and we can get very easily get good optical amplifiers in this range. (Refer slide time: 05:50 - 
06:30 min) 

so t,This is the diagram you have seen before, so you will see that at around fifteen fifty nanometer1550 nm range, we have some the [noise] kind of low attenuation. [noise] sSimilarly,thirteen ten 1310 nm  [noise] nanometeris another window, but this window is quite wide. Although it may not look very wide in this diagram, in actual practice, this is quite wide. So this is the wider window and we get good optical amplifiers. We will come back to this point when we discuss optical amplifiers. (Refer slide time: 06:38 - 07:37 min) 

Why do we require optical amplifiers, because there are losses in fiber,losses in fiber may be due to various reasons: One, of course, the main thing is absorption. That is what we showed in the previous diagram; it loses energy to the atom and absorbs some of the photons. So we get lower magnitude or lower strength of the light signal, as we go to longer and longer distances. Then there is scattering of photons by the medium. So there is Rayleigh scattering, due to slight changes in the refractive index of glass; then there is Mie scattering due to imperfection of the cylindrical structure, it?s made to rigorous specification, but it?s never exact in the engineering world ? it can never be exact. So we get all these different types of scattering, absorption, etc., leading to losses in fiber. (Refer slide time: 07:39 - 07:53 min)

Apart from these just lowering of signal strength, one problem is that the losses are in nonuniform for different wavelengths. ok that means that tThe [noise]loss may be more for one particular frequency and may be less for another particular another frequency. The trouble is, when you take a waveform, specially a digital waveform, which is a square waveform and if you analyse it, you will get a lot of spectral component. iIf you do a fuller transformation on that, you will get the different harmonics or components of that particular wave shape. And the and these for that for the perfect square shape to come up, these different harmonics have to be at specific strengths compared to each other. dDue to differential losses at different frequencies, what would happen is that their balance would get disturbed. What you will actually see is that your pulse shape has changed. (Refer slide time: 08:54 ? 09:23 min)

This is known as chromatic dispersion ? chromatic because it depends on the wavelength or frequency, or the color of the light that is why it?s called chromatic. Different spectral components of pulse travel at different velocities. [noise] tThis is another problem and there is something called group velocity dispersion. so wWe get some kind of velocity for all these different components and this leads to some kind of dispersion. [noise](Refer slide time: 09:24 ? 
09:46 min)

So this is an example of chromatic pulse ? we need not go into the details of this, but this is the input pulse whereas in the output pulse, it has a much more flattened shape because these different components have been attenuated differently as they went along the fiber. For compensating these dispersions, nowadays some special types of fibers have come up; we cannot go into details of these ? one is the reduced dispersion fiber. By dispersion shifted fibers we mean the natural dispersion is that is sort of acted on nonzero dispersion shifted fibers. Anyway, the point is that normal SMF is there in most of the places, say, more than ninety five percent95% of the deployed plant. Dispersion is measured in pico second per nanometer kilometer; so dispersion is much lower. For some of the interesting areas or the window, we get an almost zero dispersion for these fibers. So we can get very special fiber these days, which may be utilized for very long haul applications, where dispersion becomes a problem. By the way, for short haul  applications, when you are travel through a few kilometers, then the dispersion is not much of a problem and we need not bother about dispersion shifted characteristics of the fiber because in ordinary fiber, whatever the dispersion it gives in that small distance, will not matter so much. (Refer slide time: 11:20 - 11:25 min)

We have a bandwidth span product; that means, how much bandwidth for what kind of distance. This is very common in almost all kinds of transmission lines. ok [noise] sSo a transmission line, which operates at a particular speed quite well for some distance, will not operate that well if you either increase the rate at which you are pumping in the data (increase the frequency) or if you keep the data rate constant and increase the length. That would also not work very well. sSo for any kind of medium, we get the bandwidth distance product, which tends to be more or less sort of constant. so aA fiber which is good for at a particular speed for lets 2 kilometers may be just good enough for 1 km when you double the data rate. (Refer slide time: 12:25 -  13:26 min)

For older kind of SMF at 1310 nm, we get high speed ? mean these are some typical figures, they are not exact figures; these are some typical figures to get some ideas. sSo we can operate at two point five giga 
2.5 gbits/sec for six forty kilometers640 kms without amplification, or 10 gb/sec for, let?s say, 100 kms. A decent SMF can take up to two point five giga bits per second 2.5 gb/sec for four thousand four hundred kilometers 4400 kms,ok which spans from one continent to another or ten giga bits per second 10 gb/sec for five hundred kilometers 500 kms. Of course, you should multiply these figures for dwdm DWDM. We will talk about WDM presently, where it gives you a large number of channels, may be forty 40 channels. So you can see that you can realize really tremendous data rate, very very high data rate, using this fiber optic communication and that is the major advantage of fiber. tThere are, of course, other advantages like have i mean you are not susceptible to electromagnetic radiations; that is one good thing because nowadays with so many gadgets all around and so many things moving around, we get all kinds of very noisy electromagnetic ambience; but [noise] i mean fiber is immune to all that. that s That is the good point about fiber. (Refer slide ime: 13:59 - 4:46 min)

Next, from fiber, we come to light source that is the LED. Light source is of course of two types, depending on whether we are using multi mode fiber or single mode fiber. Usually, we would use ordinary light in a multi mode fiber and the source of the ordinary light would be LED. One good thing about the led LED is of course that it is very cheap. sSo the good thing about multi mode fiber is that it is cheap; the only thing is that it will not scale up very well with that bandwidth distance product. An led LED is just a forward biased 
pn-junction; what happens is that recombination of injected minority carriers by spontaneous emission produces light and it is a broad spectrum up to gain bandwidth of the medium; so that is an LED. (Refer slide time: 14:47 - 
15:42 min)

It is usually of a low power. ok power You remember that for  power you have to divide it by time. Although you might get a continuous source of light, it will be comparatively low power like twenty d b m 20 dbm, of low internal modulation. So you can modulate it if you internally modulate it. you can modulate it at best of with hundreds of mega bits/sec, per second which of course, for some applications, may be more than enough speed. But then again,, when you are talking about the core of wide area network, then it may not it may be it will is a very low speed. So obviously in the core of a wide area network, we will not be using multimode fibers or LEDs. LED slicing is LED plus a filter. iIt gives some power loss; we need not bother about that at the moment. (Refer slide time: 15:43 - 15:50 min)

The other kind of light source, which is very important, is the laser; ok of course, we use semiconductor lasers here almost always. It gives a high i mean much higher power output. So in a short duration of time we get quite intense pulses of light. That is very good; it has a high power output; it has got a sharp spectrum; that is it is coherent; that is, it is not at the wide range of spectrum. (Refer slide time: 16:12 - 16:42 min)

So it is sharp spectrum, that is the property of a laser that reduces the chromatic dispersion. It can be modulated either internally or externally, so that is also a good point. It is good for longer distances and larger bit rates compared to mlm MNF. (Refer Slide Time: 16:43 - 17:45 min)

mlm We need not bother about MLM. We have some special kind of devices, the tunable lasers. Tunable lasers means a laser, but we can change the color of light over a certain range. And as we will see later, the tunable laser may be quite important  in some cases. We will see what kind of time we require for tuning; it?s fairly rapid in the sense that it is in less than milliseconds? range. In the order of milliseconds, we can change the frequency emitted by the lasing system. It has a wide and continuous range of over hundred nanometer 100 nm; has a long lifetime and is stable over lifetime; and it is easily controllable and manufacturable. These are the good points about the tunable lasers. (Refer slide time: 17:46 - 18:21 min)

The methods could be eElectro optical; changing the refractive index by injecting the current or applying an electromagnetic field. sSo that is one way; it could be temperature tuning although it is not much preferred. First of all its range is narrow and then it may degrade the lifetime of a laser if you want to do it through temperature; or mechanical tuning using mems MEMS. This is compact but one problem of this tunable laser is that it is costly; it is quite costly and that is why this is not very common; also, it is slightly more complex to manage. But in some instances, it gives some advantages. We will mention this point later on when we discuss wdm WDM. (Refer slide time: 18:43 - 19:20 min)


then wWe come to receivers ?are of course the light pulse coming will have to be detected by something: whether a pulse has come or whether the pulse has not come, whether it?s one 1 or a zero 0. sSo if an photon comes, it will sort of push up an electron to a conduction band. sSo that is the standard photo detector, which you must have studied at school. So if it is sort of higher than this gap, this energy gap, then we get the electron in the conduction band, which will naturally show up by conducting; so that is the easiest thing. (Refer slide time: 19:21 - 21:03 min)

I was mentioning that nowadays we have fibers running for thousands of kilometers. sSo if we have such long haul fibers, what would happen is that naturally, after some distance, what you have to do is that your signal will become weak, so you have to amplify this. Previously this distance would be something of the order of four five kilometers 4?5kms. ok nNowadays twenty five kilometers 5 kms is very common, you can go hundreds, or in special cases, you can go even up to many hundreds of kilometers; you can go without amplification. But whatever it is, after some distance if, because of this loss, absorption, dispersion, etc., you will have to amplify the signal. For optical signals traveling long distance through fiber, it needs to be strengthened. This may be done through olt OLT, which was the older technology. In OLT, what was done was that, the optical signal was converted back to the electronic domain. And then you amplify the signal and then push it back to the optical domain. Obviously this has quite a number of disadvantages, main disadvantage being cost and the speed; that means cost is higher and speed is lower that has some advantages also. I will mention it later; this may be done in optical domain. The idea was the that, it could be done in optical domain through erbium doped fiber amplifiers. There are other kinds of dopers that I will mention; we will see that. 
(Refer slide time: 21:04 - 21:27 min)

So this is the scenario ? we have this light propagating and as it propagates with distance, the signal level comes down. When it comes down to minimum operating level, at that point, we will have an amplifier that will amplify the signal back; and then again, after sometime, it will sort of decay and then we again amplify. So these are some kind of repeaters. (Refer slide time: 21:28 - 23:33 min)

We come to one point where simple amplification is not always enough; sometimes we require regeneration. These are the so-called 3Rs: reamplify, reshape and retime. As far as absorption is concerned, if you simply amplify, that means increase the size, the strength of the signal is good enough. So the absorption loss or the loss of strength can be handled that way. But when you talk about very long distances, due to chromatic dispersion, etc., wave shape will become distorted as we have seen. In very long distance, on the other side there will be tremendous amount of errors that may not be acceptable. So we have to get a wave into shape. Now there are some special kinds of fibers; try to do it in optical domain. But more commonly they are deployed today to bring them to electronic domain and then give them the right square shape once again. So that is the second R; and the third R is the timing; that means how do you keep all the clocks synchronized. Because, after all, some TDM signals, etc., are traveling; so you will have to have a very strict control over the time. So these reamplify, reshape, and retime, are the 3 R kind of regeneration. Sometimes we only do with two r 2 Rs or simply one r 1 R, which is simply reamplify, which can be done simply with an EDFA, that is, the erbium doped fiber amplifiers; and at certain distances, we simply reamplify this. (Refer slide time: 23:34 - 24:32 min)

If you want to do some reshaping, suppose this is the input shape, which has come. This goes through the OE transformation, the optical domain, and we come to the electronic domain. We amplify this; shape this properly; and then again push it back from the electronic domain to the optical domain. So we again get a nicely shaped pulse and the output, which has been strengthened as well as reshaped. The problem of this is that, naturally, we can do the reshaping as well as amplification at the same time. But one problem that we face in this case is that the cost is high; and the other problem is that optics inherently can operate at a very high speed. But we can get fairly high-speed electronics also. But it becomes more and more difficult in the electronic domain as the speed becomes higher and higher. And when that happens we would like to do it in optical domain if we could and there are a number of schemes today for handling this in the optical domain. For example, we can have some very specially shaped optical waves, whose shape will not change over a distance. They are called solitons; they are sort of done in a way that different spectral components are mixed in such a manner that after dispersion, they cancel out each other. Similarly there are fibers which give the dispersion in the opposite direction to the standard fiber so that it can be brought back to shape. So these are all the atoms to handle this reshaping in the optical domain, but the most widely deployed system as of today is to take it to the electronic domain and do it there. (Refer slide time: 25:51 - 26:57 min)

Regenerators are specific bit rate, so that is another problem with the electronic domain. It is an opaque unit; that means. it has specific bit rate and modulation format that is used; whereas optical amplifiers, in optical domain what ever is coming is being amplified. So it is transparent to the bit rate, the modulation format, the protocol, etc.; it is transparent to all that. Whatever is coming, is simply amplifying. so that is the good thing about optical amplifiers. The system with optical amplifiers can be more easily upgraded to higher bit rate, without replacing the amplifiers. If you are going to use the same infrastructure for higher bit rate as service providers often want to do, in that case, if you are taking it to the electronic domain, you have to replace. Optical amplifiers have large gain bandwidth; they are also key enablers of dense wavelength division multiplexing. (Refer slide time: 26:58 - 27:31 min)

One of the standard kinds of fiber amplifiers, which is very widely deployed, is the erbium doped fiber amplification ? we have talked about this earlier. Erbium has a large number of excited states and from some of the excited states, it gives out this 1550 nm light, exactly the wavelength used in the third window. A few meters of optical fiber doped with a few parts per million of erbium is pumped with 1480 or 980 nm laser to give amplification. (Refer slide time: 27:32 - 27:46 min)

So EDFAs amplify all lambdas in the 1550 window simultaneously. So key performance parameters include saturation output power, noise figure, gain flatness, pass band, etc. (Refer slide time: 27:47 - 28:23 min)

We have input signal coming through regular fiber and this is the part which is erbium doped and you pump some laser. So these are combined using some kind of couplers; I will be talking about couplers later on. Because of the pumping laser, the erbium ions become excited and when the incoming signal hits these, they fall back to the ground state may be emitting more photons, so the signal is amplified. (Refer slide time: 28:24 - 29:11 min)

There is another kind of amplifier called Raman amplifier, which uses longer lengths of fiber. This has other advantages, basically using Raman scattering, so we are not going to the details of this once again. It has pumping through some kind of pump and signal. This pump may be in the same direction as the signal is going or it may be counter pumped in the other direction; the whole point is that, this pump keeps the atoms excited and, due to Raman scattering, so more and more photons come out, so we get an amplified signal finally. (Refer slide time: 29:12 - 29:45 min)

We have other kinds of dopants also: erbium doped for 1550 nm range, praseodymium doped fluoride fiber (PDFFA) for 1310 nm, thorium doped for 1350?1450 nm, thulium doped ? well, this is somewhat more academic, because thulium is considered as a rare material; it is not easily available. And even if it is available, it is quite costly. Anyway this is in the 1450?1530 nm range, (Refer slide time: 29:46 - 30:03 min)

tellerium erbium doped fibers in 1532?1608 nm range. Raman amplifiers address an extended spectrum using standard single mode fiber. That is the good thing about Raman amplifier. (Refer slide time: 30:03 - 30:09 min)

So EDFAs are popular in C-band, Raman are proposed for S-band, and gain shifted EDFA for L-band etc. (Refer slide time: 30:10 - 30:32 min)

We can have a look at this ? depending on the wavelengths, different kinds of doped and different kinds of fiber amplifiers become is more relevant, like EDFA for this range from 1550 etc.,  TDFA, PDFA, Raman amplification for this entire range, etc. Now we talk about some more components: the first component we talk about is 
(Refer slide time: 30:45 - 31:07 min)

some sort of passive device called a coupler. Optical coupler combines and splits signal wavelengths independent or selective; that means, the coupler can be wavelength independent as well as wavelength selective; fabricated using waveguides in integrated optics. Light couples from one waveguide to a closely placed waveguide because the propagation mode overlaps the two waveguides. (Refer slide time: 31:08 - 31:40 min)

So this is the picture: we have an input waveguide coming in and another input may come; and there are 2 outputs. So what might happen is that it may be used as a coupler; that means, 2 signals joining together, like we wanted to do when we tried to put in a pump in the EDFA amplifier. Or it may be used for splitting the signal coming from one and it is getting split into two directions. So this can be used in various ways. (Refer slide time: 31:41 - 32:3 min)

So if ? is the coupling ratio, power output 1 is ? times power of input 1. Whereas power output 2 is 1?? times power of input 1. So you see that together the input power is split into two parts if you want to have ? = ?. 
These are so-called 3-dB couplers. We put half the power in input 1 and half the power in output 2. If you want to broadcast the same signal to two different destinations, you can use a 3dB coupler. As I said, light couples from one waveguide to a closely placed waveguide, (Refer slide time: 32:31 - 32:38 min)

iIdentical waveguides complete coupling and back periodically. sSo this is the couple mode theory. (Refer slide time: 32:39 - 33:06 min)

Of course we have to follow the conservation of energy constraint so you cannot get more ? since this is a more passive device, we cannot get more out of it. Then when you put in actually you get less, so it is possible that electric fields at two outputs have the same magnitude. So they are exactly the same, but will be 90? out of phase and lossless combining is not possible, so nothing is really  100% efficient.
(Refer slide time: 33:07 - 33:50 min)

Passive star is a sort of generalization of this. It?s a broadcast device to more than one recipient. It divides the received signal to all output ports at original wavelength; of course, if you divide the same signal into so many different signals, the received strength of the signal will be proportionally less. You have to handle it by either amplification or some other thing, or maybe, weak signal is good enough for your application etc. So N ? N passive stars can route N simultaneous connections through. (Refer slide time: 33:51 - 34:12 min)

This is an example of an eight-port splitter. Whatever signal is coming, it is getting divided into two and again divided into two. So we get eight signals over here; we can actually use some 3-dB couplers to form this eight port splitter from 
Y-couplers. (Refer slide time: 34:13 - 34:35 min)

This is another example of an 8 ? 8 star coupler. There are 8 lines; any of these might communicate something which will be broadcast to all the other seven ports. So such things are used for broadcasting. (Refer slide time: 34:36 - 35:26 min)

We now come to optical modulation; that means how we modulate it. Of course the simple modulation scheme ? since we are talking only about digital systems ? is on/off keying, that means either on or off or 1 or 0. There are two types of modulation techniques, namely, direct modulation versus external modulation. In modulation, the extinction ratio of output power for bit = 1 to output power for bit = 0 is very important. We want this to be as high as possible. Some lasers cannot be directly modulated; that is one problem. Another problem with direct modulation is that you are modulating at the source, modulating at the same place, from where that light is being generated. Whatever diode or whatever you are using for generating it, we want to modulate through that only. So that is the direct modulation, whereas in indirect modulation, what we will do is that there is a continuous source of light and just as the light comes out, we will modulate it. We will put it on or off by making it go through something. So that is the external modulation. (Refer slide time: 36:00 - 36:22 min)

So the solution is ? naturally since direct modulation has the problem about the chirp, etc. ? external modulation for higher speeds, longer distance dispersion, limited regimes, etc. We prefer external modulation. The light source is continuously operated. External modulation turns light signal on or off; so this is the optical modulation. (Refer slide time: 36:23 - 36:49 min)

They can be integrated in the same package as laser; the laser source is there and the modulator is external, but they can be packaged. Electro absorption or EA modulator is one important kind of modulator; it applies an electric field, shrinks the band gap and photons are absorbed. (Refer slide time: 36:50 - 
37:03 min) 

So this is the picture, this is the continuous source of this light, which is being modulated, that is, being put on or off depending on whether you want to transmit 1 or 0. (Refer slide time:37:04 - 37:52 min)

The next set of components are multiplexers, filters, and gratings. We will talk a little bit about it. If you look at this, these are all wavelength selective devices: multiplexers, filters, and gratings. In a wavelength filter suppose ?1, ?2, etc. So many ?s are coming, but I want only ?1 out. ?2, ?3, ?4, etc., are absorbed; whereas if you are a multiplexer, I want the different ?s coming in different lines. I want all to be mixed together and use the same line. That is a wavelength multiplexer. (Refer slide time: 37:53 - 39:20 min)

So the application could be particular wavelength or a particular wave band selection. Wave bands are nothing but some contiguous, operating wavelengths, which all are side by side. In the operating window, whatever be the window you are using ? 1550 or whatever ? you can have a number of ?s, all side by side. There is a guard band between each of these operating ?s ?  the IQT has specified how much guard band etc., you will have to have. You can have large number of ?s, all grouped together in the same window. Now you can select a band out of that, a bunch of frequencies out of that, instead of selecting only one; that is wavelength band selection. Static wavelength cross connects and oadms OADMs (optical add drop multiplexers): you have come across this term, optical add drop multiplexers in the context of SONET, but in the optical domain we require optical add drop multiplexers; we will come to that. Equalization of gain: that is another application; filtering of noise; ideas used in laser operation and dispersion compensation modules etc. These are the different applications. (Refer slide time: 39:21 - 40:20 min)

One of the standard wavelength selective components is arrayed waveguide gratings. These are curved selection of silica acting as waveguides. Each waveguide is slightly different in length. The incoming signal is split. To be slightly different in length is like a running track bend ? in a running track you know that the outer track, in any athletic event, in an 800 m race or something, the outer track is longer than the inner track. That is why athletes are given proper handicap, because we want to make all the distances same. Here deliberately, we want the distances to be different. If they are different, they are going to sort of arrive out of phase at the output. The incoming signal is split; every wavelength then travels down each waveguide. (Refer slide time: 40:21 - 40:34 min)

Time delayed signals recombine to give each wavelength its own waveguide; can be reversed to act as a multiplexer rather than a demultiplexer; usable in optical integrated circuits; easily combined with other functions. (Refer slide time: 40:35 - 41:11 min)

This is the picture we had seen earlier: if your light is going in this direction, what you are doing is that you are demultiplexing. One bunch of frequencies are coming; we want all these different colors to get separated out. So that is the demultiplexing action going on. If different wavelengths or different colors of light are coming in the other direction, it?s just the direct opposite thing, and we will get a multiplexer, so it is an AWB acting either as a multiplexer or demultiplexer, depending on how you operate. (Refer slide time: 41:12 - 43:07 min)s

We come to optical switches; we have seen electrical switches. Now we will talk little bit about optical switches. What is optical switching? You remember electrical switching? In electrical switching, there are some one input lines coming in line i and I want to get that signal out through line j. So we want to operate the switch. Through the switch or something, want to connect the ith line and jth line. The basic idea was that the signal coming down from ith line has to go out of the jth output line. That was the simple switching element. It is the same thing here also; some wavelength is coming through some fiber, that is, one input port. We want to get it out of another fiber. For the time being, we do not have any wavelength conversion kind of thing, thus the same wavelength has to push it to another fiber. So that is my switching at the optical plane redirecting light from one optical fiber to another without electrical conversion. So we are always harping on that without electrical conversion, we can operate it at a much higher way and higher speed. Secondly it may be cheaper also, to operate at high speeds; it will be cheaper and then if you upgrade eventually, this is going to be transparent. This does not depend on the underlying protocol, etc., that is being used at the higher layer. These are the other advantages of doing at the optical plane. Now, the most advanced optical switching technology is MEMS, that is, tiny movable mirrors. (Refer slide time: 43:08 - 
43:13 min)

So this is the crossbar switch 4 ? 4 switch. (Refer slide time: 43:14 - 44:09 min)

We have seen this picture also. That is a MEMS optical cross connect. You see that these are all tiny mirrors; what I can do is that, to follow the red line which is coming, we are using these two mirrors to push it to line number 3 over here ? from line number 1 to line number 3. This way, by just adjusting the angles of the mirrors which we can do through the MEMS technology, we can have a simple and elegant kind of switch. The light goes and bumps off a couple of mirrors and goes out the other fiber. Whatever signals it is carrying, what protocol it is carrying is immaterial; similarly, the data rate also is immaterial. This is a MEMS optical cross connect. Now all this technology sort of enables what is known as the WDM technology, which is a wavelength division multiplexing. As I was mentioning when we were discussing multiplexing, wavelength division multiplexing is nothing but frequency division multiplexing; that means, you want different channels to come at different frequencies. It?s just the opposite of optical domain. In wavelength division multiplexing, different wavelengths and different ?s are getting together and light with different wavelengths can very well mix together and go to the other end. For example, sunlight has all the frequencies that are mixed up to appear as white light to us; if you send this through a prism, all the frequencies split up so we will get some kind of a demultiplexing action. So you want to use this property for wavelength division multiplexing for achieving very high data rates. There are two kinds of wavelength division multiplexing, or WDM, that people talk about. Mostly in the backbone, people use DWDM. DWDM means dense wavelength division multiplexing. By dense we mean that we put a lot of channels, lot of ?s side by side, so we get a lot of channels. Of course, DWDM would not usually be deployed in a LAN because DWDM is costly. But then, at the backbone, where you are talking about very high speed that cost is effective; whereas another kind of wavelength division multiplexing is coming into LANS, which is called CWDM or coarse wavelength division multiplexing. There the wavelengths are not so closely packed; they are sort of more sparsely placed, which is a good thing because then, the stability of these laser sources, detectors etc., are less of an issue. So CDWM tends to be cheaper than DWDM and CWDM is coming into use these days. We will start our discussion on wavelength division multiplexing in this lecture and then we will continue in the next lecture with the details of wavelength division multiplexing. (Refer slide time: 46:53 - 47:07 min)

WDM increases the capacity of optical fibers; different wavelength lasers, each transmit at same time down the same fiber; multiplexing is combining wavelengths; demultiplexing is splitting of wavelengths. (Refer slide time: 47:08 - 48:03 min)

Usually the number of wavelengths is in the power of 2, 4, 8, 16, 64, 128, etc., things like 32, 64 etc., are big. 16, 32, 64 are deployed; now people are talking about hundreds of wavelengths, may be even thousands. Wavelengths are separated by multiples of 0.8 nm guard band: I mentioned this is equivalent to 100 GHz. There is 100 GHz separation between two ?s; that is the minimum separation, which is mandated by the ITU standard. Coarse WDM has widely separated wavelengths so that the components can be little less sophisticated and much more cheaper. (Refer slide time: 48:04 - 48:25 min)

This is a WDM system; different lasers of different lights coming together to the multiplexer, flowing down the same fiber at the same time, being demultiplexed on the other end. The multiplexer, demultiplexer could be an AWG or some other. (Refer slide time: 48:26 - 48:59 min)

In point-to-point WDM system, one point is connected to another point through multiple wavelengths. WDM is the most cost-effective technology in point-to-point technology, where the distance is about greater than 50 kms. In shorter distances, multi fiber is cheaper because in DWDM naturally your end equipment tends to become quite costly. If you have some extra strands of fiber, then that may be a cheaper option. (Refer slide time: 49:00 - 49:48 min)

Wavelength add drop multiplexer is one thing we require, when we want to add on to a stream of WDM that is going through a fiber, or we want to add on some extra wavelengths on the wave or we want to take just one wavelength out and let the others pass through. So that is a wavelength add drop multiplexer needed for routing and wavelength assignment. It performs the same functions as the electronic counterpart at the level of wavelengths. We had come across its electronic counterparts like ADM in SONET; the same thing happens in the optical domain. One problem is that granularity is high, because of inherent capacity of wavelengths; so even if you take out one single wavelength out of a whole bunch of wavelengths, that one single wavelength can carry a large amount of traffic, let us say, 2.5 gbps ? that is a high amount of traffic. If there are small amounts of traffic which you want to add or drop, then this is not a very effective technology. (Refer slide time: 50:12 - 50:36 min)

WADM has multiplexers and a set of 2 ? 2 switches, one for each wavelength. They are managed electronically; that means, these switches, etc. are programmed electronically to control which incoming length flows through and which is dropped. (Refer slide time: 50:37 - 51:01 min)

So fiber and wavelength cross connects are important components of this WDM, needed in real networks. Point-to-point connection does not need a passive star, or a passive router or active switch, but in WDM, we may require all these for an entire communication network. We have discussed this. (Refer slide time: 51:02 - 51:51 min)

Passive router: It can route separately each wavelength, no wavelength conversion; it allows wavelength reuse, same wavelength can carry multiple connections through the router. For example, the same wavelength coming from fiber 1 going through fiber 3 and the same wavelength coming in from fiber 2 and going out through fiber 4 is perfectly possible. If enough wavelengths are there, N ? N router, can route N2 simultaneous connections; some routing issues are there. We will discuss the routing issues in the next lecture. (Refer slide time: 51:52 - 52:17 min) 

Finally, we may have some active switch, which has all the features that a passive router has. The difference is that active routing matrix, which has some functionality, has to be powered. This is 
of course an issue when you are talking about a very long haul. (Refer slide time: 52:18 - 52:20 min)

With this we conclude our initial portion of our discussion and in the next lecture we are going to discuss the details of WDM; that is, how different wavelengths are routed through the network how we can get an entire network output. Thank you.

PREVIEW OF NEXT LECTURE
COMPUTER NETWORKS
Prof. Sujoy Ghosh
Dept of Computer Science& engineering
IIT Kharagpur
Lecture 11
Routing and Wavelength Assignment
In WDM all optical networks
(Refer slide time:52:49 min)
Good day, so iIn this lecture, we are going to continue our discussion on wavelength deviation and multiplexing. specifically we are going to talk about routing and wavelength assignment. (Refer slide time:53:09-54:14 min)



rRouting and wavelength assignment to what? well routing and wavelength assignment means that we have some stream of packets or whatever data or whatever communication is going on from one source to one destination. now tThese sources and destination, first of all one thing is of course of point to point connection is if there is point to point connection is directly sends to the fiber that is [noise] simple but in general, they will not be directly connected. general  tThey will go through a network, [noise] they go through some intermediate notes to reach the destination. sSo this for this stream, we have to route this that is one problem and the other thing is that may be they are all the stream right some particular wavelength for the time being. lLet us assume that it is continued on the same wavelength. sSo we have to assign one wave length, so we have this problem of routing and wavelength assignment. (Refer slide time:54:15-
54:23 min)


in WDM all optical networks ok of course we can do[noise] routing etc., very easily in the electronic domain, [noise] that is known [noise] routing in the electronic domain how it is done etc., are we are not discussed is as easier we were discussed it, later on iIn the this these series of lectures but routing, I mean we are talking about a simple kinds of routing problem here. sSo we will talk about routing and wave length assignment. (Refer slide time: 54:43-56:53 min)

sSo this is an example of light path establishment, suppose you have these a b c d e  ABCDE these are connected. so iIn the first part, the left half of the figure what we have is the physical connection from aA to bB, there is a connection and bB to dD there is a connection and so on. iIn this one[noise] iI show some[noise]that means some[noise] RW routing and wavelength assignment has already been done and some of the wavelengths and some of the links have been used. sSo here only two 2 wavelengths are used, let us say aA to bB [noise] they are not connected  say  a to c A to C the a to c A to C light path has been established via bB so and aA to bB, there will be a switch which will switch the [noise] and suppose this dashed line is the lambda one  LAMBDA 1. sSo the lambed LAMBDA1 a one coming through this fiber from a to b A to B that is switched to lambda one LAMDA 1 [noise] using the same wavelength to the outgoing fiber from b to c B to C. so wWe have light a paths established  between b to c B to C. similarly  c to d C to D, there is a light path b to d B to D, there is a light path d to e D to E there is a light path e to f E to F there is a light path d to f D to F, there is a light path and so on, using the same lambda one LAMDA 1 then iI want to connect e to c E to C. now iI cannot go from e to c E to C after say lambda one LAMDA 1 has been [noise] assign iI cannot go, but from eE by lambda one LAMDA 1 to anywhere because all the outgoing fibers the lambda ones LAMDA 1s have been used up this side, for a this side for bB and this side for f F [noise] sorry sSo inorder to connect from e to c E to C, iI use the another wavelength. which say lambda two LAMDA 2 which connects me higher dD so and the dD cross connect will connect this particular lambda LAMDA from this fiber to this fiber. so wWe will get a direct [noise] light path from e to c E to C. sSimilarly we will get a light path from b to f  B to F using lambda  two LAMDA 2A to B a to b using  lambda  two LAMDA 2 and this way, [noise] they are all connected. (Refer slide time:56:55-57:40 min)

aA burst has a long and variable length payload, if it is long low amortized overhead no fragmentation. aA control packet is sent out of band that means using some other lambda lambda LAMDA control  and reserves bandwidth that is  lambda LAMDA data reserves a particular bandwidth along a particular path and configures the switches. sSo it is like a setting up temporary light path from the source to the destination ok aA burst is sent after an offset time, it arrives at a switch after it has been configured. sSo no buffering is needed, so our original problem is of not having optical buffer. sSo buffers in the optical domain that is avoided in this fashion. (Refer slide time:57:42-58:20 min)


sSo what will happen is that, this will now moving towards the other end to the next node and  [noise] here this will again [noise] do some [noise] do the go through the O to E o two e and then do the [noise] switch [noise] configuration and then again e to o E to O and go to the next half and this delay etc., is calculated in such a fashion that when the burst arrives, what happens is that we when the burst arrives at the intermediate node, the switch fabric is already configured. sSo you do not have to store it, you simply simply passes through in the optical domain. sSo that is (Refer slide time :58:21-58:44 min)

nice so offset of course is now t minus delta T-?, because it spent delta ? amount of time over here. sSo without any delay, the burst goes through the optical switch fabric, so depending on how many intervening notes are there, you have to have this original tT, so that finally when the tT is exhausted offset is exhausted but you have also reached your [noise] destination.
