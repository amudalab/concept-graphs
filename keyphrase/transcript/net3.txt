COMPUTER NETWORKS
Prof. Sujoy Ghosh
Dept of Computer Science& engineering
IIT Kharagpur
Lecture 11
Routing and Wavelength Assignment
In WDM all optical networks
(Refer slide time: 00:48 min)

Good day. In this lecture we are going to continue our discussion on wavelength deviation multiplexing. Specifically we are going to talk about routing and wavelength assignment ?  (Refer slide time: 01:02 -01:04)

routing and wavelength assignment to what? Well, routing and wavelength assignment means that we have some stream of packets or whatever data or whatever communication is going from one source to one destination. Now these sources and destination, first of all, if there is point-to-point connection they directly send it through the fiber. That is   simple; but in general, they will not be directly connected. They will go through a network; they go through some intermediate notes to reach the destination. So for this stream we have to route this; that is one problem and the other thing is that may be the stream rides on some particular wavelength for the time being. Let us assume that it is continued on the same wavelength; we have to assign one wavelength, so we have this problem of routing and wavelength assignment (Refer slide time: 02:09-02:19 min)

in WDM and all optical networks. Of course we can do routing etc. very easily in the electronic domain ? that is known ? but we have not yet discussed routing in the electronic domain, how it is done etc. We will discuss it later on in this series of lectures. But in routing, we are talking about simple kinds of routing problem here. So we will talk about routing and wavelength assignment. (Refer slide time: 02:39- 03:11 min)

The optical WDM networks are future backbone for wide area networks. The reason we want it is because the bandwidth demand from the users is going up at a very fast rate;   of course, we have to go for fiber, we have to go for higher speed and then we have to pack in a lot of these channels over the same fiber. So we require an optical WDM network. (Refer slide time: 03:18 ? 03:57 min)

The speed of electronics compared to optics is a major constraint at the backbone. So it is always preferable to handle the traffic at the optical layer, where we can achieve much higher speed and the whole thing is transferred. The physical topology that we will consider at the optical wavelength routers is connected by some fiber links. So these are the same kind of topology we had talked about; that means there are some nodes, which are connected by some communication links. In this particular case, the communication links happen to be optical fibers. (Refer slide time: 03:58-
05:10 min)

Most of the node pairs in the backbones are not physically connected. So we have 30 or 40, let?s say 40 nodes. Now for 40 nodes,  there are 780 possible pairs, that is, 40 C2, that is, 40 ? 39/2, that is,  780 pairs, which is a very large number. Of course, only few nodes are connected actually through physical fibers. So they can reach each other only through other intermediate nodes. A light path or connection: this is the concept that we will be talking about most in this part of the lecture. This is the path between two end nodes and a wavelength on that path, so we want to route, that means, we want to find this path between two end nodes and we want to select the wavelength on which to send all our communication from the source node to the destination node. (Refer slide time: 05:11-05:21 min)

The intermediate nodes can cross connect that particular wavelength from the incoming fiber to the outgoing fiber. Do you remember our discussion on digital cross connect? May be through MEMS, that means, a particular wavelength is coming through one particular fiber, we direct it through some mirror to another fiber on that same wavelength. It is as simple as that. Thus  with programmable optical switches, the traffic can be routed entirely in the optical plane.  (Refer slide time: 05:51-06:14 min)

So no wavelength conversion for the present, and any light path uses the same wavelength on all the links its path spans. So 16 or 32 wavelengths in each fiber are common these days and hundreds of ?s are being talked about. A single fiber could theoretically accommodate as many light paths. (Refer slide time: 06:13-06:39 min)

What are the basic concepts of the light path? End users use light paths to communicate, each light path passing along the same fiber occupies different wavelengths; that means on the same fiber there may be different light paths going through the same fiber, each of them would be writing on a different wavelength or different lambda two ?s. Two light paths with the same wavelength cannot share a fiber, that means two different light paths who are on the same wavelength they cannot have any fiber in common because then their signals will get mixed up with each other. Of course, we can have (Refer slide time: 06:52-07:36 min)

WDM with wavelength conversion also. As I mentioned in the previous lecture that wave length conversions are possible although they are quiet costly. So it is possible in different extents, for example none that means no wavelength conversion or we can have a partial wavelength conversion that means some of the nodes have WC capabilities while others do not have or it may be full. If you have wavelength conversion we get some kind of flexibility in routing and wavelength assignment. We will not concentrate on that but you can understand that if we could convert the wavelength midway that means, as the signal is going, we can convert the signal midway, then maybe we can have better possibility of accommodating many more light paths. For our purpose, we will be concentrating only without wavelength conversion. (Refer slide time: 08:00-08:21 min)

So how to establish light paths? Two parameters to be decided: path from source to destination ? this is the routing part ? and wavelength along the path, which is the wavelength assignment part. Traffic types subjected to optical networks may be static or dynamic; that means you know the specific source destination pairs, which have to be connected in advance and that is quiet stable. So now you may run some centralized algorithm kind of things and find the best way to assign the light paths. The other situation could be dynamic; dynamically there is a request from some node for a connection to some other node and you want to set up the light path on the fly and take it down. That is a slightly more complex situation. (Refer slide time: 08:59-
09:52 min)


So two versions of RWA are realized depending on the characteristics of the traffic applied: static light path establishment or dynamic light path establishment. Now routing and wavelength assignment. Several, signals can share a single fiber. All signals must have different wavelengths, so these are the constraints. Of course, technology sets the upper limit to the number of wavelengths. You cannot have  unlimited number of wavelengths, you can have a large number these days but even then this is not unlimited. So these are the constraints, which you will have to respect while doing the routing and wavelength assignment. The problem is how to choose a route and a wavelength to each connection, so that signals won?t block each other. How can signals block each other? Well, the signals can block each other in this way. For example, suppose you have routed two particular source destination pairs in a certain manner using a certain wavelength, let us say ? 1. Another pair of source destinations was, let us say, N1N2; then we want to put a connection between N3 N4,  for which the route happens to share some of the links. In these links now you cannot use ?1 for this, because ?1 on those links is already occupied by the link from N1 N2. So if N3 N4 share even one link, you cannot use the same ?1; you will have to use ?2, but then ?2 has to be blocked because of some other links and so on. (Refer slide time: 10:52-11:23 min)


So the objective of SLE, that is, static light path establishment, is to minimize the total number of used wavelengths connecting the maximum number of nodes. They are the same; if you can connect some particular set of nodes using the minimum number of wavelengths, using the number of wavelengths, which are given to you, you may connect the maximum number of nodes. Other objective functions we may also consider are the load in the most loaded link, the total number of optical switches, etc. (Refer slide time: 11:24-13:48 min)

So this is an example of light path establishment. Suppose you have these A B C D E connected. So in the first part the left half of the figure what we have is the physical connection from A to B there is a connection and B to D and so on. In this, I show some  RW routing and wavelength assignment has already been done and some of the wavelengths and some of the links have been used. So here only two wavelengths are used. Let us say A to B are not connected; say a 
light path has been established via B for A to C. At B, there will be a switch, which will switch this dashed line, which is ?1. So ?1 coming through this fiber from A to B is switched to ?1, using the same wavelength to the outgoing fiber from B to C. So we have light path established between B to C.  

Similarly, from C to D there is a light path;  there is a light path from B to D; there is a light path from D to E; there is a light path from E to F; there is a light path from D to F and so on, using the same ?1. Then I want to connect E to C. Now I cannot go from E to C. After say ?1 has been assigned, I cannot go from E by ?1 to anywhere because all the outgoing fibers of the ?1s have been used up this side for A, this side for B, and this side for F. So, in order to connect from E to C, I use another wavelength, say ?2, which connects me via D and the D cross connect will connect this particular ? from this fiber to this fiber. So we will get a direct light path from E to C. Similarly, we will get a light path from B to F using ?2, A to D using ?2 and this way they are all connected. As you can see, a large number of light paths have been established using just two ?s. (Refer slide time: 13:49-14:35 min)

Dynamic light path establishment, on the other hand, is appropriate for network having dynamic traffic, where the traffic pattern changes very dynamically and a communication request may arrive at any time. The goal is to maximize the number of the incoming communication requests accepted. Well, if it so happens that you have already made some  kind of reservation and some kind of allocation to an earlier request and when a new request comes, you find that there are no paths on which a ? is consistently free throughout. So in that case, you have to block that request or regret that request. So we want to minimize such regrets and maximize the number of incoming communication requests that can be accepted. That is the goal of DLE. (Refer slide time: 14:36-14:57 min) 


 In order to solve this problem, let us see what the complexity of the problem is. The RWA problem is difficult; it can be divided into two sub-problems ? routing and wavelength assignment. Both of these sub-problems are NP complete and tightly linked together. If you remember, by NP complete we mean that class of problems for which no  polynomial time algorithm is known and so, if you can solve any one of them in  polynomial time, all others can be solved in polynomial time. It is not known whether  any polynomial time algorithm exists, but since the number of such problems is so large  and people have thought about these problems in so many different guises, it is very unlikely that you would suddenly come up with a polynomial time solution to any of these. And of course, when an algorithm has an exponential complexity, it takes a lot of time. So that is a difficult problem in that sense; both its parts are NP complete. So there is no exact solution. So we cannot expect to get the so-called ideal solution or the optimal solution in all cases, but we can try good heuristics. (Refer slide time: 16:00- 16:31 min)

Now what we want from a good algorithm ? maximize the number of connections; use the shortest routes; and minimize the number of wavelengths. If you can maximize the number of connections, this is of course our basic requirement. If you use the shortest routes or the number of hops should be minimum, delays and all kinds of things should be minimum. So that is another good thing to have, and if you minimize the number of wavelengths that have been used, future requests can also be possibly accommodated.  (Refer slide time: 16:32- 17:06 min)


So for routing, there are different techniques like fixed routing; that means the path that is predefined is used; that means the for all pairs we have some fixed paths and fixed alternate routing, meaning multiple predefined paths are there and one of them is selected. So the criteria could be shortest path, least loaded path, least congested path, and so on. (Refer slide time: 17:07-17:27 min)

There could be adaptive routing techniques; the path is found on the fly. That means as the network is in operation dynamically, you find the least congested path or the most suitable path depending on the current situation in the network, and once again we may try to find the shortest path or least loaded path. (Refer slide time: 17:28-18:03 min)

Now how do the wavelengths affect routing? There cannot be two signals with same wavelength per fiber for two different light paths; that is not possible. So the shortest route may be blocked by other signal and in the worst case all the routes are blocked. Then, of course, either you have to reassign the wavelengths of other earlier assignments or you have to block this request. If the wavelength conversion is possible, the signal can use other free wavelengths but the conversion is not always supported. (Refer slide time: 18:04-18:15 min)

Wavelength assignment problem arises under the wavelength continuity constraint; that means the wavelength has to be continuously available in all the links down the line and there are various algorithms for that. One of the most commonly used one is the first fit,  which means, you have decided on the path by some means; that is the routing path. Once you have decided on the path you have a list of these wavelengths, ?1 to ?N, and you try them in order. The first wavelength, which is free on all the links in the route is the one which is assigned to this particular light path request. That is called first fit, and it has some good points in the sense that since you are always trying from the same end, you tend to use up these earlier ?s more and more, giving you very good utilization of the ?s. So you may be having a lot of other ?s in the network. You have them on reserve to accommodate the request. So that is a first fit algorithm. But then again you must remember this is just a heuristic, in the sense that there is no way you can prove ? as matter of fact you can disprove it. In some cases, this would not give you an optimal algorithm. In some cases it may give you an optimal algorithm. In many of the cases, it has been found that this gives good results, but in some cases, it may give very bad results. It may be random. You may want to distribute the load all over, so it may  be random or people have used the least used; that means the least used ?, which is most free, is the one that is picked up. When you try all the ?s one by one, you reach the ? that can fit this particular route. So this will have the opposite effect, in the sense that this will try to distribute the load evenly across all the ?s or the most used ones; it need not be the first fit or the least loaded, etc. So these are the different algorithms or different heuristics, which you might use for assigning your ?. Of course, what you could do also is that at some point you could go back and instead of asking about your routing path you can ask, give me the next alternative path. So from the shortest path, which it might have given in the first instance, it might give you the second shortest path on which you may try the same kind of wavelength assignments using that. So all kind of combinations of routing and the wavelengths, all these strategies are possible. (Refer the slide time: 21:18-21:34 min)

What are all the good points about wavelength routing? Setting up a light path is like setting up a circuit. You remember we have talked about circuit switch network and packet switch networks? So in circuit switch networks, specifically the telephone circuits, even today may be majority of their networks are circuit switches; that means, a call from one node to another, one caller to one callee, a continuous circuit is set up. All those things have become more complicated as we have seen because of the TDM and all kinds of packets and all these things are coming in between. Anyway, the essential circuit switching is that the circuit is switched. Just in the same manner, when we are assigning a particular light path to two nodes, which have to be connected, it is just like setting up a circuit. And once the light path is set up, the route is fixed and the wavelength is assigned and the light path is set up. Then these two nodes can communicate, pumping as many packets as they like through this, of course limited by the technology constraints. But this is quiet a high constraint. So the inherent advantages of setting up this circuit switching is that in this particular approach to the problem specifically the quality of service is good. What we mean by the quality of service is smooth traffic and QOS guarantee can be given due to fixed bandwidth reservation and SLE is easy to manage, that means static light path establishment is easy to manage. That is the advantage of wavelength routing. (Refer slide time: 23:22-23:39 min)   

The disadvantages are the following: long circuit set-up time. This circuit time will be in the order of tens of milliseconds. Actually the circuits are set up in such a way that the source sends some request and the request goes down all the nodes on the way. The local switches have to be set up in that particular fashion and then they will acknowledge and say that ok the light path has been set up. So all this communication takes the order of tens of milliseconds. Now tens of milliseconds may be a lot of time depending on  the situation. When a very high-speed communication is going on, then tens of milliseconds is a lot of time. But if you are going to set up a light path and then use it for one month or one year, then tens of milliseconds may not be much. So it depends on the situation; but if your traffic is changing dynamically then you want to adopt what light paths you want to assign etc., in a dynamic fashion, then tens of milliseconds may be a lot of time. The other problem is that the huge capacity of 1? can carry how much traffic, let us say of the order of 2.5 gbps or something like that or may be even more these days. Its capacity is very huge. The point is that for just two nodes communicating, it may be a gross underutilization if proper traffic grooming is not done at the edge. What you mean by traffic grooming? You remember that at the edge, first of all, when we are talking about the edge, we are talking about the backbone network, which is where usually all your WDM systems will be deployed. So we have this backbone network and these backbone networks have been fed by all different networks, are routed from different networks etc., so number of streams are coming into this backbone network. They are converging and then traveling to the backbone, and then going out on the other side. So, the routers which communicate with this backbone are the edge routers and the light path you have got from N1 to N2, as we have seen is the granularity is quite coarse; that means a quite high amount of traffic can be handled. So if the edge router get together a number of streams from different sources and then uses it, it?s good traffic roaming; it loads it quite well then that is good. So we are achieving high efficiency if it is two single users and the two end then it is a gross under utilization of this ?. Unfortunately in this circuit inherently you cannot have less than 1? for a circuit, if you are setting up a circuit. You have that capacity but you are underutilizing it. This is the same problem that we had seen in circuit switching versus packet switching, and as a matter of fact there is a development as we will see for the packet switching part also, which we will see in the next half of this lecture. (Refer slide time: 27:03-27:13 min)

So bandwidth is inefficient for bursty traffic. This is another term that you should know, if you remember I told you while discussing about the telephone traffic that the Bell telephones people had done a lot of study about what kind of traffic is there, and how long the people talk and how frequently they talk at different times of the day, etc. We know that this is the some kind of Poisson distribution with exponential inter arrival time etc., so  those are very well-behaved kind of systems. Unfortunately, in recent years first of all the voice traffic has shrunk to may be less than 5% of the overall traffic in the network, 95% of it is data traffic and this data traffic has seen to be very bursty. They also follow some kind of statistics, but it is a complicated kind of statistics, the whole thing is that the data traffic seems to come in bunch. When they come lot of packets are coming and then for a long time there is no traffic. It is also self similar, and it is complex kinds of statistics but in anyway the point is, for long time for which when it is not sending anything, the circuit is remaining unutilized; so this is a gross under utilization of the circuit. The busy time versus the lean time may be of the ratio of 1:500 or something like that. So that is why this becomes an inefficient way. Another disadvantage is wasted bandwidth during either off or low traffic periods for SLE or too much overhead, that is, delay, due to frequent set up or release in dynamic light path establishment. That means if you want to put it down and then set it up again, in that time frame this becomes too much of an overhead because, if you remember, for setting up a light path we require may be tens of milliseconds. So you cannot do it very fast. (Refer slide time: 29:16-30:05 min)   

Particular ? path specific pros and cons are that, they have very coarse granularity, as we have discussed i.e., OC 48 and above. OC 48 if you remember is 2.5 gbps or above that. They have limited number of wavelengths and number of light paths; no aggregation, that is, merging of the light paths inside the core. So inside the core, there cannot be any aggregation etc. Traffic grooming can only be done at the edge so this may be complex or inflexible. That OXC has a millisecond kind of switching time. So these are the pros and cons of ? paths. (Refer slide time: 30:06-30:10 min) 

So this takes us to the next half of our lecture, where we will have a look at the other way and other approach to this whole problem, namely, optical burst switching. Just as the previous one was circuit switching, where we were doing circuit switching; we were setting up light paths. Here we will try to take the packet route; that means we will send them packet by packet. How that can be done? Originally, the packets may have originated from some say TCP/IP or Ethernet or some computers as small kinds of packets. Doing it at that particular level becomes difficult at the optical level. So we have some technology for that; we will look at this now. We will discuss the next topic, which is optical burst switching. (Refer slide time: 31:13-32:19 min)

First of all, let us do one quick comparison between electronic versus optical switching. Data is transmitted optically in WANs, MANs and even some LANs. Electronic switch uses digital switching fabrics; converts data from optical to electronic for switching and then from electrical to optical for transmission. That means what we are doing is that, we are trying to do the switching in the electronic domain. As mentioned a number of times before, the electronic switching always has problem; at high speeds it is very costly and this is not very easily upgradeable, scalable, etc. So that is a problem; but if you have done the switching in the electronic domain, there will be no problem. The kind of switching and kind of logic, kind of algorithm that we use in the other parts of the network could be put over there but we want to avoid electronic switching and do optical switching as much as possible. So let us see how that can be done. (Refer slide time: 32:20-32:33 min)

The cost of electronic switching goes up steeply as the speed requirement increases and optical or photonic switching uses optical switching fabrics, keeps data in the optical domain. (Refer slide time: 32:35-32:51 min)

So, why not we keep the status quo; that means that do the switching in the electronic  domain only? Well, the trouble is that the data traffic growth is still doubling every year and this is different from what we have in the computing domain. In the computing domain or Moor?s law, it doubles may be in one and a half years and this is doubling every year. Actually if you see over the long range, although the computational speed has  increased many fold, the communication speed has grown even more, by an order of magnitude more maybe. So the point is that electronics is unable to move and that is not possible for developing at that particular rate of development. So pure electronic processing and switching can hardly keep up. Electronic MUX, DEMUX, space power consumption, heat dissipation, etc., are always a problem because heat dissipation is more, it requires most space etc. There is no transparency, meaning, it depends on the kind of system and the kind of the modulation system, the kind of multiplexing system that you are using, which you have to use in the electronic components. So when the technology moves on, these intermediate nodes will also have to be changed, whereas, the optical switching, since it is transparent, does not matter if you change what you are sending through that pipe. (Refer slide time: 34:33-34:51 min) 

The cost factor, of course, weighs the heaviest. Though the cost of OEO at 0C-48 is going down, the overall cost including WDM system at 0C-48, is still a dominant factor. OEO at 0C-192 and higher in the future will still be a dominant cost factor. (Refer slide time: 34:52-35:18 min)

So we want to go to optical switching, whose advantage is that, low cost and high capacity, transparency; that means it is independent of bit rate, format, protocol, etc. It is synergetic to optical transmission and future proof; that means when we upgrade, all these things might change but my intermediate node will not change in the optical switch. (Refer slide time: 35:19-35:41 min)

Opaque, that is OEO switches, are more mature and reliable, of course. So still they need some electronic processing and control; that means, when we are doing in the optical domain, we still require some electronic processing and control. What we do is, we try to minimize this, and optical 3R and performance monitoring are hard. You remember that we can amplify a signal quite easily but if you want to do all the 3Rs that means Regenerate, Reshape and Retime, then we prefer to take it to the electronic domain.   (Refer slide time: 35:58-36:30 min)

Packet switching: a packet contains a header; that means addresses and the payload. It is a variable or fixed length. The advantage of packet switching is that it has some kind of statistical multiplexing; it can be sent without circuit set-up delay ? if the line is there just simply send it. It also enables statistical sharing of link bandwidth among packets with different sources and destinations. So that makes it more efficient; bandwidth usage is more efficient. (Refer slide time: 36:31-37:04 min)

So packet switching is done usually this way. Store and forward at each node: it buffers the packet, processes its header, and sends it to the next hop. This is how usually it is done in the electronic domain; we will look at the details of this later on. But usually what would happen is that, when we are using a circuit switching, the circuit is set up from N to N and then we can send whatever we like. But when it comes to packet switching, since it is coming packet by packet, each packet has to be processed independently. We have to look at this packet, look at this header, see what the destination is, decide on which next link to take, etc., and send it. So you have to store this packet in a buffer and then examine, do some processing and then forward it ? this is the store and forward paradigm. This is usually done in packet switching. (Refer slide time: 37:37 ? 37:43 min) 


We have some problems with that optical domain, as we will see. It is statistical multiplexing and is inherently bandwidth efficient. (Refer slide time: 37:43 ? 38:25 min) 

Now if you have a packet core, well, we have the access or metro networks, optical buses, passive star couplers, etc. SONET WDM rings or token rings are used. We will talk about token rings later. It uses switched networks or gigabit Ethernet. So these are the kind of technologies that are deployed in the LAN/MAN side, whereas in the WAN side, we have the ? routed virtual topology, i.e., circuits or leased lines. We have dynamic ? provisioning; that means circuits on demand and optical burst switching, which we are talking about now. (Refer slide time: 38:27 ? 38:54 min)

The technology drivers for this are the explosive traffic growth, as I already mentioned, bursty traffic pattern, and to increase bandwidth efficiency. To make the core more flexible, naturally a packet?s system would be more flexible than those fixed light path kind of system to simplify network control and management and making the core more intelligent. (Refer slide time: 38:54 ? 40:05 min)

How important is this bandwidth efficiency? We are always talking about the bandwidth efficiency. Well there are two views to it ? one is the user?s point of view. Well, user wants some bandwidth today and if the bandwidth becomes cheaper and as it becomes available, he immediately thinks of another application and his bandwidth demand will grow. Previously, people were very happy to send some simple text. Now, they are downloading, then they want to download songs, files, they want to download entire videos, then they will try to do video conferencing with each other. So these are very bandwidth intensive applications and they require a lot of bandwidth. So from the users? point of view, the more the bandwidth you give, I will bring lot more applications and I will just use it up. So with more available bandwidth, new bandwidth intensive applications will be introduced. High bandwidth is like an addictive drug, cannot have too much of bandwidth from the users? point of view. (Refer slide time: 40:05 ? 40:38 min)

From the carriers? and vendors? point of view expenditure rate is higher than revenue growth sometimes; so longer-term equipment investment cannot keep up with the traffic explosion. So, you have to see that whatever I invest today, how long in future I can take so that my investment is lower. We need bandwidth efficient solutions on the infrastructure existing today that will be competitive. So these are the different issues which (Refer slide time: 40:39 ? 42:07 min)

brought us to optical packet switching or optical burst switching. I will come to that later on. But this is our goal; there are two problems; and one of them is the lack of optical buffer. In the optical domain, if some packet is coming, means some light pulse is coming now, how do you store them in the optical domain? There is no good buffer for optical domain; there is a thing called fiber delay lines. This is really a very bulky and not very good stuff, you can put it in the fiber delay line. As it comes, it goes to that fiber delay line and comes out of the other, the whole thing would be delayed a little bit; that is something akin to buffer in it. But there are severe limitations on how much you can delay, that is one thing. Secondly, it is bulky, expensive and not very good. So fiber delay lines are bulky and provide only limited and deterministic delays. Store and forward with feedback FDLs lead to fixed packet length and synchronous switching. So we cannot use because of this simple store and forward, and the other thing is that tight coupling of header and payload requires stringent synchronization and fast processing and switching. So these things are difficult. (Refer slide time: 42:07 ? 43:49 min)

So we go to this optical burst switching or OBS; a burst has a long and variable length payload. So first of all, variable length payload means we want to keep it as flexible as possible; that is one good aspect of it. That means we want to keep it as flexible as possible. The other thing that we want it to be is it is long, why is it long ? because we have to do some processing, some setting up, etc., for these burst of packets to travel. So what we do is, we will do some grooming in the electronic domain. We  collect a number of packets together forming a burst, which has the same source?destination pair and then we set up the path and send this burst along all in the optical domain. So that is the essential approach to optical burst switching. A burst has a long and variable length payload, if it is long and has low amortized overhead and no fragmentation. A control packet is sent out of band, that means using some other ? control and reserves bandwidth ? that is ? data reserves a particular bandwidth along a particular path ? and configures the switches. So it is like setting up a temporary light path from the source to the destination. A burst is sent after an offset time; it arrives at a switch after it has been configured. So no buffering is needed. Our original problem is of not having optical buffer, so buffers in the optical domain are avoided in this fashion. (Refer slide time: 43:49 - 44:51 min)

We have to do a burst assembly and disassembly at the edge at the source side. That means the client data may be the IP packets, which are the most dominant ones. They are assembled together into bursts; and burst switching or reservation protocol is done, that means, we send the control packet, an offset time t ahead of burst. So within this offset time, all the switches down the line will do their programming. That means they will set up all their mirrors or whatever it is their cross connects, etc. So that later on, when the burst does arrive, we do not have to do any processing on that and if you are do not doing any processing on them we do not need to store them either. They can go straight away in the optical domain. There is a dedicated control channel, which is out of band signaling for the control packets. (Refer slide time: 44:52 -45:25 min) 

The advantage is no fiber delay lines nor OEO conversions for burst at any intermediate nodes, photonic burst switching fabric inside the core. That means it leverages best of optics for burst switching and electronics for control packet processing and fabric control. So just for the control part, we do this OEO and for the bulk of it, the burst, we do not have to go to the electronic domain at all. (Refer slide time: 45:25 - 45:55 min)

This is a diagram, say assembly queues for different egress nodes; these are going to different channels. This is an ATM cell IP packet or SONET, and we have an IP packet over here. We have a SONET frame over there because if you are sending things in the purely optical plane, you do not really care what the payload contains. It may be an IP packet, it may be a SONET frame, it may be some cell, it may be anything else we do not care. Intermediate optics is transparent to all that. So what happens is that (Refer slide time: 45:16 ? 47:18 min )

we use the ATM cell for the control packet purpose. So we make a control packet, which assembles a burst, and as it assembles a burst, it knows what the time or length threshold reached is. The length of the burst may be variable, as we said the burst could be long and of variable length. But when all these different IP packets frames, SONET frames etc., are put together, what happens is that a control packet is generated and sent out. The control packet now knows the source, it knows the destination, it knows the length of the burst, so it sends through a separate control channel, so this control channel goes through the control plane as we will see. (Refer slid time: 47:18 -47:44 min)

So we have the assembly queues for different egress nodes; that means the destinations, for different destinations, all these packet frames etc., are getting queued up and forming into bursts. (Refer slide time: 47:44 -  48:04 min)

Now we see fiber delay line ? as I just mentioned it fiber delay line, feed forward or feed backward. So there is no optical RAM for store and forward; every FDL provides only limited delay and cannot perform most of useful buffer functions. So FDL units are bulky, affect signal quality etc. (Refer slide time: 48:04 ? 48:24 min)  

Now going back to this OBS, we have various schemes still involving in an active area of research. I will just present a simple scheme called just enough time or JET. There is an offset time between CP and burst. So what is done is that the control packet is sent and after the control packet is sent,  there is a delay. We give a delay and then we send a burst;  this delay is to cover all the programming time on the intervening nodes. (Refer slide time: 48:43 ? 49:05 min)

So an offset time between CP and burst: no fiber delay line required to delay the burst, when CP is processed and switch fabric is configured. CP carries the burst length info, facilitates delayed reservation for intelligent efficient allocation of bandwidth and FDL if any, including look ahead scheduling. We need not go into the details. (Refer slide time: 49:10-49:53 min)

We have the control packet here, which is moving in the control plane, and we have a burst over here and there is offset time T and we have just enough offset, JET, which we require for programming these intervening optical  nodes. So CP arrives at OEO node at time, let us say, T1. But the control packet is being taken to the electronic domain for processing, because this will have to be processed etc. So it is better done in the electronic domain. (Refer slide time: 49:54 ?50:05 min) 

Then CP goes through the optical to electrical conversion and configure the switch fabric and then it will move on. (Refer slide time: 50:06-50:12 min)

CP goes through EO conversion and leaves the OEO node at time t1 + ?. (Refer slide time: 50:13-50:53 min)

So what will happen is that, this will now move towards the other end, to the next node, and here this will again go through the O to E and then do the switch configuration and then again E to O and go to the next hop. And this delay is calculated in such a fashion that when the burst arrives, what happens is that at the intermediate node, the switch fabric is already configured. So you do not have to store it; you simply pass through in the optical domain. (Refer slide time: 50:54-51:16 min)

Offset of course is now T ? ?  because it spent delta amount of time over here. So without any delay, the burst goes through the optical switch fabric.  Depending on how many intervening nodes are there, you have to have this original T so that finally when T is exhausted, offset is exhausted but you have also reached your destination. (Refer slide time: 51:17-51:37 min)

That is just enough time. And finally to conclude, OBS is a programming switching   paradigm that offers many advantages over the existing technologies, but is not likely to be the end-all kind of solution. OBS has several variations and adopting OBS will be an evolutionary process. This is another problem ? when you have a new scheme and there are different researches, they will try to do research and come up with different suggestions and then at different places some different things may be adopted. But then, in order for the entire network to work in a very smooth fashion, you have to come to some standard, so OBS has not come to that standard yet, but it is a quite a promising approach. Now, we have talked about the two approaches in WDM, namely, this circuit switching path, that is the light path routing and wavelength assignment and setting up of light path, and we have talked about optical burst switching. In the next lecture, we will talk about SONET infrastructure. The SONET, which we have talked about, a lot of it is also in fiber optics. Then we have all these fiber optics; that means packet oriented fiber optic infrastructures are also coming in. That means large routers, etc., are coming into this picture, one thing we did not discuss although we mentioned it a while discussing about SONET was that this has an inherent capacity of fault recovery, recovering from fault. So that is one thing that we would like to discuss in the next lecture ? not only in SONET, in optical network, but in general, how do you handle faults. That is very important, because this optical network is at the core of the network and if the core of the network fails, its repercussion is tremendous, both economic and other repercussion may be tremendous. So we like to put in lot of reliability into all this fiber infrastructure that we have, and how exactly that is done, we will discuss in the next lecture. Thank you.
Preview
COMPUTER NETWORKS
Prof: Sujoy Ghosh
Dept of Computer science& engineering
IIT Kharagpur
Lecture 12 
Protection and Restoration 


(Refer slide time:53:37-53:41)

Good day. In  this lecturer, we are going to discuss the various protection and restoration  mechanisms, which are usually employed in  optical  networks 
(Refer slide time: 53:54-53:57 min)	

We will be talking about protection and restoration, now of course we have to discuss, what is protection and restoration.    Why we need them? (Refer slide time :54:05-54:16 min)

What is protection and restoration, comparison between the two and the different schemes of rotation? This could be   our general outline   of presentation. (Refer slide time:54:19-54:25 min)

Network is unreliable somehow , so many failures can occur,  node may fail a link, may get cut, some fiber optic line may cut in between because  somebody cut it , while digging a whole or something or node might fail there may power failures at nodes and so many things.   So there are failures in the network but if you remember  that  one of the most important places where we deploy optical networks is in the core of the network  and the core of the network connects so many people, to so many other people. It is so very vital  that we cannot allow the services to be a severely desalted because that would have very grave consequences. Anyway the service provider attempts to give a very high level of service.   So although failures are   unavoidable in real life, we have to find a some way of combating this, that means  if there is a failure, we want to recover from it as soon or as fast as possible so that, is what protection and restoration is all about. Another thing is that when we want to give some reliability, protection, restoration etc we always in some form, always have to bring in some redundancy. Without any redundancy a system  cannot be  protected, it cannot be restored without any replacement etc., so there has to be some redundant capacity in some form in the network, in order to achieve protection and restoration. How this is done that is what we will discuss. (Refer slide time: 56:14-56:25 min)

So   first let us talk about  path protection, it uses more than one path to guarantee that data is sent successfully.   so if you  look at this graph,  it will be having a six node graph, where 1 and 6 are communicating on the top  through the dash line.   We show the primary path, which is the primary connection between 1 and 6. What might happen is that the link between 2 and 3 might snap due to some reason. So we have already got a backup route which is calculated going through 1 4 5 & 6 . We will channel our communication through the backup link and please note that this  backup or secondary path from the source of the destination is link. This channel does not share any link with the primary path, so that is the requirement. If any link in the primary path fails,  I am assuming only one failure which means that the backup link is all intact and you can switch to the backup path for this particular communication. (Refer slide time:57:24-57:47 min)

Dedicated link protection is not always practical, sometimes it may have it shared link protection is practical, it is quiet often and it is implemented and   it may fail, when this link protection may cause failure here you are only provisioning for the failure of the link,   but if a node fails, then it may lead  to some complications as we will see. (Refer slide time:57:48-58:07 min)

So to compare between path switching and line switching path, switching of course is a coarse scheme and line switching is a finer scheme and line switching is can again be span protection, span would may be a several links together and that may be a span or  a line protection (Refer slide time:58:08-58:37 min)        

In mesh networks, of course the restoration is possible only if the graph is 2 edge connected that is by connected which means that there are 2 edge connected disjoint paths between any pair of nodes so that no single edge failure can disconnect the network. So this is a  necessity and usually  try to keep that way, unless it will be difficult or it is not a cost,   effective etc., (Refer slide time: 58:37-59:21 min)

Protection in a mesh networks of course more complicated, then a ring   simple minded scheme would be 2 edge node, no disjoint paths for each connection 1 +1 not as is  mentioned here, this not very efficient. There may be many paths and provisioning double the number of paths, which are pair wise mutually node are edge disjoint that may be very difficult. Provisioning in the network better approach would be line protection which of course have the problem of coordination.   I will show the later on are protection cycles in mesh net, again I will show this.
COMPUTER NETWORKS
Prof. Sujoy Ghosh
Dept of Computer Science& Engineering 
IIT Kharagpur 
Lecture 12 
Protection and Restoration

(Refer start time:  00:43)
Good day. In this lecture we are going to discuss the various protection and restoration mechanisms which are usually employed in optical networks.
(Refer slide time:  00:57-00:59)

We have to discuss what is protection and what is restoration and why we need them. (Refer slide time:  01:08-01:18)

What is protection, what is restoration, comparison between the two and the different schemes of protection would be our general outline of presentation .
(Refer slide time:  01:19-01:26)

Network is unreliable; and then so many failures can occur ? a node may fail, a link may fail or a link may get cut, some fiber optic line may cut in-between because somebody cut it while digging a hole or something. A node might fail, there may be power failures at nodes and so there are failures in the network. But if you  remember, one of the most important places where we deploy optical networks is in the core of the network. The core of the network connects so many people to so many other people and it is very vital that we cannot allow the services to be severely disrupted because that would have very grave consequences and anyway the service provider attempts to give a high level of service. Although failures are unavoidable in real life, we have to find some way of combating this; that means if there is a failure we want to recover from it as soon or as fast as possible. That is what protection and restoration is all about. Another thing is that when we want to give some reliability, protection, restoration, etc., in some form or the other we always have to bring in some redundancy. Without any redundancy, a system  cannot be  protected, or it cannot be restored without any replacement, etc. There has to be some redundant capacity in some form in the network in order to achieve protection and restoration. So how this is done is what we will discuss. (Refer slide time:  03:20-03:25)

Protection and restoration are the mechanism to recover from network failure: their difference will be discussed in the following parts. (Refer slide time:  03:26-03:37)

Why we need protection and restoration is now clear: to recover from network failure, to prevent the lot of data loss. Now another point is what would we mean by prevent lot of data loss? The point is that we will be talking about protection at the optical level, at the lower, transport level, that is, the low physical level. Above  this  physical level, there are a whole lot of other layers like data link layer, network layer, transport layer, etc. They may have their own protection mechanism and they might be able to tolerate. In some cases, if such a protection is sort of implemented at higher level, such protection usually would tolerate a small amount of data loss, which they will retransmit or do something in the protocol to take care of that. We will be talking about this later on. Within that limit we do not have a 100% tight case, but we have something to play with. Within that limit, if the physical layer can come back up, then that is nice; then the end user who is sitting at the top of the application layer would not notice that  a failure has actually occurred. So this is our general goal and, of course, to prevent lot of data loss. (Refer slide time:  05:13-05:18)

To provide reliable communication service is a reason for having protection and restoration. (Refer slide time:  05:19-05:39)

Protection is the primary mechanism; this is fast and routes are usually preplanned ? we will come to this part later on ? whereas restoration is the secondary mechanism, used to provide more efficient routes or additional resilience, etc., over and above protection. We will see both of these later on. (Refer slide time:  05:40-06:11)

Techniques for protection: we could protect a path, which is called path protection; we could protect a link; that is called link protection. There are various schemes of protection: like 1+1 protection, 1:1 protection, 1:N protection, M:N protection. This depends on what kind of redundancy we have built into the network. So we will look at some of these techniques one by one. (Refer slide time:  06:12-06:26)

So what are the considerations and tradeoffs that we have for protection? For support for fast protection, time is dictated by the client layer. This is what I was talking about earlier ? in the client layer, whatever the higher layer we are talking about, all of them are clubbed together, and we are calling them client layer, let us say. In the client layer we may have some resilience; that means, we can tolerate some small amount of data loss. So depending on that, and beyond that, it will be taken that the service will fail. This is dictated by the client layer and that is the constraint within which we try to do our protection or restoration at the bottom layer, that is, the physical layer. We require some switching technologies (Refer slide time:  07:13-07:33)

as we will see, and how to implement protection. It could be through dedicated hardware or through software. Here we will only be talking about ? since we are talking about the physical layer ? hardware protection, where some hardware has been put in order to take care of this protection. (Refer start time:  07:34-07:51)

Support for low priority traffic ? that is another consideration we might have. So low priority traffic supported using the protection bandwidth; traffic dropped in case of a failure. As I mentioned earlier that in order to give some protection capability into the network we have to build in some kind of redundancy over there. Now when there is no failure, naturally the redundant link would be idle. What you could do is that if you have some low priority traffic, which could be dropped whenever there is a problem, under normal circumstances you can use your provision in the network to carry the low priority traffic, and as soon as there is a failure, where naturally the provision is going to be used up for providing protection, restoration, etc., the low priority traffic would be discontinued. So that is one thing we could do. (Refer slide time:  08:46-09:03)

Support for mesh topologies ? mesh topologies are bandwidth efficient, fast signaling mechanism, flexibility in choice of routes by preplanned routes, etc.  If we have support for mesh topologies, it is also nice. As  a matter of fact, if you remember our discussion about different types of topologies ? we have star, tree, ring, mesh etc., so these are the different possible topologies. Out of all these,  point-to-point connection, ring and mesh are the three topologies, which dominate most of the WAN. When you want to communicate between two points, you first set a point-to-point link. Now consider that link to be quite important. Then what you would like to do is that you would like to put in some kind of reliability over  there, by making it into a ring. For example, we discussed SONET. A SONET gear quite often is put in the form of ring, rings that would touch, mutually touching rings. For example, the telecom people put up all these SONET rings through fiber optic networks. The rings are quite easy to handle and we will discuss the protection mechanism in ring quite a lot. But if you go to a wider geographical area ring, it may not to be feasible due to various reasons. What you have is a mesh. A mesh, if you recall, is just a graph, where the nodes are connected in some fashion. It has to be a connected graph and actually later on we will see that for giving proper protection, it has to be not only a connected graph, it has to be biconnected graph as well. That means between the two nodes, which are communicating, there have to be two alternatives paths, which are linked somehow. Otherwise, you will not be able to give protection. Giving protection in mesh networks is also an important consideration. (Refer slide time: 11:17-11:55)

Other important considerations are maintenance of large distributed routing tables, that is, precomputed routes or up to date topology maps. So you have to maintain this dynamically, because a network is a very dynamic thing as links come up or go down or nodes come up or go down, this may have to be recomputed and stored in a distributed fashion. We would like to have support for all failure modes: node failure for mesh networks and for ring networks, so or it may be of course ring failures.  (Refer slide time:  11:55-13:05)

First, let us talk about path protection. It uses more than one path to guarantee that data be sent successfully. If you look at this graph, it will have a 6-node graph where 1 and 6 are communicating and on the top, through the dashed line, we show the primary path, which is the primary connection between 1 and 6. Now what might happen is that the link between 2 and 3 might snap due to some reason. We have already got a backup route, which is calculated going through 1, 4, 5 and 6. So we will channel our communication through the backup link. Please note that this backup or secondary path from the source to the destination does not share any link with the primary path. That is the requirement: if any link in the primary path fails, assuming there is only one failure, which means that the backup link is all intact and you can switch to the backup path for this particular communication.  (Refer slide time:  13:06-13:16)

Now path protection: there are various ways to protect a path, that is, various ways of provisioning the extra bandwidth capacity in the network so that you can give protection. You need to build in some kind of redundancy in the  network. So there are various ways you can build in the redundancy and the schemes may be divided as dedicated path protection, (Refer slide time: 13:40-13:50)  

or shared path protection. Dedicated path protection may be shown as 1+1 protection and shared path is 1:1 protection or 1:N protection. So we will look at this one by one. (Refer slide time:  13:51-15:31)

This is an example of 1+1 protection. So you have the source on the left, and then, you have destination on the right. So it is communicating. What you can see is that from the source, the signal is coming to the splitter. If you remember, the splitter is going to split it into two halves, may be of equal power or something, and then the same signal is been carried through two different links and over there, we a switch. The switch determines which signal is better and may be switches to that and a communication is going on. If that particular link, say, this   top one fails, it automatically switches to the other protection link. So this is some kind of hot redundancy we have. That means there is redundant source of information in the destination side; so if the primary one fails, the secondary one is already on. So protection would be very fast. The only trouble is that for each such path, you will have to give an alternate path, which is also being used at the same time. This is a dedicate path protection ? for this particular path there is a dedicated alternative path; although this is very good, it is costly. (Refer slide time: 15:32-16:57)

Now we come to shared mode protection; in the shared mode, the figure looks almost the same, expect if you note, this splitter on the left has been replaced by a switch. So what we have is we have a working fiber and we have a protection fiber. Unlike the 1+1 protection, this is not a hot standby. This is cold in the sense that the protection fiber, to start with, is not carrying any signal, let us say. The source is just simply passing through the switch, the signal from the source is passing through the switch, then down the path through the destination switch to the final destination. If the working fiber goes down, then this switch will flip and the protection fiber would be in place. Since this is not a hot standby, meaning that since it is not carrying any signal under normal circumstances, you could share this path with something else. You can use this to send some other channel or some other information, etc. So that is why this is a shared mode of protection. (Refer slide time: 16:58-19:35)

Generalizing this, we get this 1:N protection, where N line is sharing 1 protection line. We have the inputs from 1 to N lines, so these are the N sources, and let us say so many destinations. So they may be going ? 1- to 2- and N to N- and so on; this is the normal mode of operation. This part of the network is for normal mode of operation and each of them is connected to a switch over here. Another link from the switch comes to one bigger switch, so this is an N input port over here and then there is a single link from this switch to this switch, on the destination side, which again feeds to all the switches. What would happen is that in case there is a failure in any one, let us say, ith link from 1,2 to N, out of that, the ith source to ith destination, which is going through the ith link and the ith link fails, what this ith switch could do is that the ith switch could switch the signal from the ith source to this particular switch at the bottom and now the protection fiber would be carrying the signal that was flowing down ith channel over there and  then again it will feed it to the ith switch on that side. Of course these two switches have to communicate that the ith one has failed over here, so you switch it to your ith mode, or this switch may sense it that this line has gone down, so it will take signal from this line. This one protection fiber is been shared by these N working fibers, and as I mentioned earlier, when everything is fine this protection fiber could be carrying some low priority data. When everything is fine these N working fibers are actually carrying the most of the important traffic so some low priority data could be flowing down the protection fiber. As soon as there is any failure anywhere, the low priority data would be stopped or it will be dropped and then this protection fiber would switch to give the services between the nodes that have experienced a link failure. Just as we have a path protection that means for a path you try to give an alternate path, similarly you could do it at the link level also. In general, it may be more efficient or more proper to do that because if you have N nodes and if it is slightly large, potentially you have very large number of paths through the networks and for each path having an alternate path may not always be a very good idea. So we concentrate at the link level and for each link level we may give a protection. (Refer slide time: 20:21-20:43)

So use an alternate path if the link has failed. This is the primary where the link may have failed so I have an alternate path to that from 2 to 3, 2 to 4, 4 to 5, 5 to 3. This alternative, please note, is for the link from 2 to 3. We will have another diagram for that. (Refer slide time: 20:44-21:09)


Dedicated link protection is not always practical although sometimes we may have it; shared link protection is practical and it is quite often implemented. This link protection may fail because here you are only provisioning for the failure of a link, but if a node fails, then it may lead to some complication as we will see. (Refer slide time: 21:10-21:28)

To compare between path switching and line switching, path switching is a coarser scheme and line switching is finer scheme; and line switching can again be a span protection. Span may be several links together; that may be span or a line protection.
(Refer slide time: 21:29-21:57)

In mesh networks of course restoration is possible only if the graph is two edge connected i.e., biconnected, which means that there are two edge disjoint paths between any pair of nodes so that no single edge failure can disconnect the network. This is a necessity and we usually try to keep it that way unless its very difficult or very cost it?s not cost effective, etc. (Refer slide time: 21:58-22:42)

Protection in a mesh network is more complicated then a ring. Simple minded scheme would be two edge or node disjoint paths for each connection, 1+1. As is mentioned, it is not very efficient. There may be many paths and provisioning double the number of paths, which are pair wise mutually node or edge disjointed may be very difficult. That may be a lot of extra provisioning in the network. A better approach would be line protection, which of course has the problem of coordination. I will show that and protection cycles in mesh net later on. (Refer slide time: 22:43-23:24)

In the path layer and mesh protection, there is protection of mesh networks to protect the mesh at the single unit. Pre-computed routes means all possible routes and alternative routes are pre-computed. 1+1 protection is protection route per light path, protection route per failure. We will discuss this later but, as I said, this is a costly alternative. Or what we might do is that we can do on-the-fly route computation; that means it is not pre-computed. As there is a failure, centralized route computation and coordination route computation and coordination are done at end nodes or distributed route computations ? all these are possibilities. (Refer slide time: 23:25-24:03)

This is an example of mesh network, where let us say this is the primary path and this is an alternate path. This alternate path may be pre-computed or it may be computed on the fly when there is a failure. Similarly, from here to here this may be the primary path and this is an alternate path. Please note that this communication as well as this communication are going through the same fiber. Maybe they are going through different wavelengths or maybe they are combined together; so there are various way of handling this. (Refer slide time: 24:04-24:51)

Let us look at some diagrams: this is a mesh network. Naturally once again, it has the same 6-node network and this is the normal operation, that is, communication from this node, node 1 to node 6. Now there is link failure over here; what you might do is you might switch the entire path like this. So this was the pre-computed path for this path. You switch to that ? this is one possibility. Or this particular span, the span could be live from here to here or from here to here. That may have an alternative path, to which you switch. (Refer slide time: 24:52-25:50)

Or this particular line may have this alternative; that means from this node to this node. If this is the link, the alternative to this link is this section from here to here to here to here. It is trying to go through the same path, but then it takes a diversion when there is a link failure through the alternative, which is provided for this particular link. I have shown only one node coming into this but you can appreciate that this path may be very long so you do not have to re-compute the path over the entire length; rather locally, you can re-compute that for this particular link as an alternative. So the alternative may well be through some local nodes only. So this path just gets a slight diversion; that is line protection. (Refer slide time: 25:51-25:59)

We talked about protection cycles in a mesh network, now for protection cycles, what we do is that for each of the paths we try to form a cycle ? cycle of  provisioning of light paths, let us say.  The point in the cycle is that suppose we are going through some arc of the cycle and if some link in-between breaks, you can always go in the other direction. So you try to form these cycles in the mesh which you keep ready and pre-computed. So whenever there is a failure you can switch. You can see here (Refer slide time: 26:38-27:03)

there are pairs of a fibers going in both the directions and there you can form  cycles over there. One of them would be protection edge, maybe the inner one; and one of them would be the working edge. If one of them fails, the other will automatically take over; the other direction will automatically take over. (Refer slide time: 27:04-27:25)

So this is another example of a network with both working and protection fibers.  The working fibers have been shown in solid lines whereas protection fibers have been shown with dashed lines. Once again you must realize that you may not provide the same level of protection to all paths or to all parts of the network; depending on which part you consider more sensitive or more important, you may put your protection there. A mesh network may be partially protected. Some of the parts may be protected or some of the parts may not be protected. (Refer slide time: 27:51-29:53)

So some more cases: line protection in a mesh network. What we have is a unidirectional light path from node A to node D; so from node A to node D we have an unidirectional light path going may be like this through nodes B, C, and E. We are talking about this path A to B to C and then to E. Now after the link BC fails, the light path is rerouted by nodes B and C along the route A B F E C E D. The unidirectional light path was going from A B C E and then to D; so this was the path A B C E D. Now BC has failed; so A B F E C E D. You can see what has happened is that there was no point in coming from E to C and then back from C to E. What has happened was that we were doing line protection; that means, for BC my protection was from B to F to E to C because I wanted to go from B to C so I am going from B to F to E to C and so now BC has failed. So that is what I do and from that point I continue wherever I was going and actually I was going from C to E to D, so once again from C, I go back to E and then to D. Such a thing is possible because we are taking a local decision; that means, for this particular link what to do in the case of failure that has already been pre- computed? We are not taking a global picture; here I have shown you a very small graph so you can immediately see the entire global picture with your own eyes. But for local nodes we may not have the global pictures where this node is coming from and where this path is finally going to. So all the intermediate nodes may not have the global pictures because if all of them had the global pictures maybe they could have computed a better path but this is for line protection.  (Refer slide time: 30:41-31:57)

Now line protection in mesh network: here what might happen is that erroneous connection due to the failure of a node is being treated by its adjacent nodes as link failure. This is one case of the so-called race condition. What might happen is that node 1 has failed; so what would 6 do is that it may assume that the link from 6 to 1 has failed, whereas 2 may assume that the link from 2 to 1 has failed. So both of them perceive as two different failures, so they take some local decisions, and it might lead to a funny situation where you are going in a cycle. Maybe other kinds of things may happen after node 1 fails ? node 5 gets connected to node 4 after node 6 and 2 invoke line protection independently. If they perceive the same failure, the actual failure was that of node 1; but if they perceived differently as failure of  link 6 1 and failure of link 2 1 and they have independent actions, it may lead to race conditions. (Refer slide time: 31:58-32:14)

The advantages and disadvantages of protection: we will also be talking a little bit of restoration. Protection is simple; it?s quick; does not require much extra process time ? and this is the important part, since this is quick. As I mentioned earlier, there will not be a lot of data loss. For example, a SONET ring would sort of come back up from a failure in a less than 15 milliseconds. So that is a benchmark; you are back in action in less than 15 milliseconds, so whatever little you might have lost during that time would be taken care of by the higher layer protocols. (Refer slide time: 32:51-33:08)

But usually they can only recover from single link faults. If there are multiple link faults, all kinds of funny things may happen. There is inefficient usage of resource, because protection needs a lot of resources, even if we are sharing them.
(Refer slide time: 33:08-33:19)

Dedicated protection needs even more resources; we talk about path restoration and link restoration.
(Refer slide time: 33:20-33:30)

What we do is that we compute the path after the failure and the resource is reserved and then used. So in restoration what you try to do that is that you try to look at the current actual situation. You try to have some protocol for keeping track of the present situation and then you compute some root and then you reserve the path and then restore the original service. This has got some software parts, so some data has to be processed, etc. This usually takes more time, so this has a hardware as well as software aspect to do it .(Refer slide time: 34:00-34:25)


The path is discovered at the end nodes of the failed link; but this is more practical than path restoration. We have both path restoration and link restoration. Path restoration means the path may be long and to find out an alternative, it may be more difficult, whereas links are between the adjacent nodes so they may quickly find an alternative. (Refer slide time: 34:26-34:53)

Advantages and disadvantages of restoration are the following: usually it can recover from multiplex element faults because you are sort of having some protocol to exchange information and then find the current situation and form the alternatives, etc. There is more efficient usage of resources; it is more complex; it is slower; it requires extra process time to set up path or reserve resources. (Refer slide time: 34:54-35:19)

So for comparison between protection and restoration, in protection the resources are reserved before the failure; they may not be used. In restoration the resources are reserved and used after the failure. So this is the main difference between the two. Route: in protection it is predetermined, in restoration it can be dynamically computed. Resource efficiency: in protection it is naturally low and the restoration is comparatively high. (Refer slide time: 35:20-35:55)

Time used for protection is short; for restoration it is longer. Reliability: protection is mainly for single fault, whereas restoration can survive under multiple faults. Well, it is not that it can always survive under multiple faults; it depends on the where the faults are, but if it is survivable after multiple faults, it will take global view and say that ok, still I can give the services. So restoration will take care of that, whereas in protection it may not be possible to handle multiple faults. Implementation: protection is simple and restoration is naturally more complex. (Refer slide time: 35:56-36:21)

For optical networks, we not only talk about physical links we talk about virtual wavelength paths. So in light paths also routing can be centrally controlled or distributed; resource reservation, forward reservation, as well as backward reservation are done as we do in optical networks. We will talk more about this later. (Refer slide time: 36:22-37:15) 

Now let us come to this all-important topic of fault management in ring networks. As I said, ring networks are very ubiquitous in WAN. All the telecom people will love these things because naturally it allows them to give very high level of service. So we have so many rings; these SONET rings are very common, and  rings are a very common kind of topology. One of the chief attractions of the ring topology is its capability to allow some kind of protection and restoration with some redundancy in-built, as we will see. We will look at two different cases, unidirectional path switch rings and bidirectional line switch rings or UPSR and BLSR. (Refer slide time: 37:16-39:22) 

This is a diagram of UPSR. We have AB, means A to B and BA, means B to A. First of all you see that we have these two rings ? we have this working fiber as well as a protection fiber. Working fiber is going in only one direction and protection fiber is going in the other direction, in the counter clockwise direction. So for connection from A to B and from B to A ? this is A, and this is B ? my working fiber is going in this direction. A to B is really going in this direction; B to A is coming in this direction. So A to B, B to A: this is the protection fiber part, A to B the working fiber is going in the other direction. A to B is going like this and B to A is coming all the way like this. Let us say the outer one is a working fiber, it is through the outer ring. For A to B, this is the path; and B to A, this is the path, whereas for protection purpose ? and please note that this is a 1+1 scheme; that means for this the alternative is already provisioned and maybe it is something like a hot standby. So A to B is through the protection fiber and now this path from A to B is going via D and C, so A D C B: that is A to B and B to A. The protection path is from this through the protection fiber. So if there is a failure anywhere, this can still continue, so this is a unidirectional path switched ring. (Refer slide time: 39:23-40:29)

There are some limitations of UPSR. It does not spatially reuse the fiber capacity; so what is happening is that since this is unidirectional, what is happening is that even if two nodes are side by side, if they are in wrong direction then it has to come all the way through and naturally all those links ? even if we are talking about particular ?s I mean WDM systems ? in the ring are entirely sort of covered by this. Otherwise even if we are talking about a WDM system what is happening is that at least one ? around the whole ring is getting occupied because two side by side things want to communicate. So it does not spatially reuse fiber capacity and if there is some, you could use it for some other purpose. That is not possible in a unidirectional ring. Each bidirectional SONET SDH connection uses up capacity on every link. That is the thing; if you look at the previous picture (Refer slide time: 40:30-40:44)


we are just talking about a connection from A to B and B to A; that?s all, so this entire ring now has been used up assuming only one wavelength. The entire ring now has been used up; these ADMs of course are the ADMs of SONET. (Refer slide time: 40:45-40:50)


So it is efficient for lower-speed access networks, one to multipoint only. (Refer slide time: 40:51-41:27)

And the other point, which may become a problem, is that delays are different for the two paths because one of them is small and the other one is quite large, all the way around the link. The relays in the two paths are quite different. A remedy could be bidirectional lines; that means, bidirectional line switched rings or BLSR. So we were in UPSR, now we will be talking about BLSR, bidirectional line switch rings. This provides spatial reuse capabilities and additional protection mechanisms and adopts span as well as line protection. We will have a look at both of these. (Refer slide time: 41:28-42:26)

This is  a four-fiber bidirectional line switched ring; so once again we have two working fibers, say, the outer two ones and two protection fibers or the inner ones. So we actually have four fibers. This is a four-fiber system and it is going, so we are using two such working fibers. One of them is going in one direction the other is going in the other direction, so this is bidirectional. We can communicate both ways, so if A to B and B to A both can communicate on the working fiber in this part at all, the rest of the space can be used for other communication. So spatial reuse is much better for bidirectional line switched rings, and the protection fibers are there because if one of them fails then the protection fiber may take up. (Refer slide time: 42:27-43:10)

This is a span protection so we are talking about bidirectional traffic supports, maybe 16 nodes or some distances, etc., from A to B. You may pre-compute a span that if this span goes down what is the alternative span. If this is the case of line protection, if this line fails what is the alternative line? Now the alternative line of course may go in the other direction, depending on what the failure is. So there are many different possibilities with this BLSR. (Refer slide time: 43:11-44:24)

There is also two-fiber version of BLSR, namely BLSR/2. Here both the fibers are working as well as protection fibers. That means if one of them fails the other one will give the protection in the other direction of the ring. If you note, the rings are going in the opposite directions and there is a line failure in both of these working as well as protection fiber. That means A to B is communicating in this direction, B to A is communicating in this direction. But if one of them fails, the other one will give the protection by going in the other direction. In that case we have to go all the way round, then you will have to reserve the resource and if there is something already going on here, you may be blocked or this may have to be dropped and so on. So naturally you have two fibers, we have less flexibility. But with two fibers we can get this bidirectional thing going on. (Refer slide time: 44:25-46:19)

Comparison of different types: these are all self healing rings. What we mean by self healing is that the nodes, these ADMs and the SONET, etc., are programmed in such a fashion that as soon as they sense a failure they know what local action to take and how to adjust the switch internally, so that it automatically switches from the working fiber or from the working span or working fiber or whatever through the protection side. That is why these are self healing rings, so they heal automatically. And as I said these SONET rings really do this in less than 15 milliseconds; the entire thing will again be up. So we can compare the three: UPSR, BLSR/4, and BLSR /2. For example, fiber pairs 1, 2 and  transmission receiver pair 2 4 and 2 spatial reuse in UPSR it is none, in BLSR it is there, and in BLSR/4 it is there. Production capacity is equal to the working capacity. Link failure is path protection in the case of UPSR or span or line protection in the case of BLSR/4. In case of BLSR/2, it is only line protection because it will have to go in the other direction. Node failure: it is path protection, line protection and line protection restoration is faster in UPSR; somewhat slower in BLSRs and restoration speed is this. Node complexity is low, high and high. Another thing we talked about is the dual homing. (Refer slide time: 46:21-47:50)

By dual homing we mean that suppose we want to deploy your network in such a way that it is very mission critical and no failure is acceptable and we want a hot standby. So what you might to do is that you may get connected through two different hubs and what you want to do is that you want dual home to these two hubs and these two hubs take independent paths to your destination. This shows a dual homing to handle hub on node failure; so we have these four ADMs: once again four nodes, let?s say A B C D. So the end node is A and these are the two hubs, B and C. What we want is that not only some link failure but even if one of the hubs fails, I should still be able to communicate. So what you do is on this ring you communicate with hub 1, let us say you are communicating with hub 1 through this A D C B and you are communicating with hub 2 through A B C. As we will note, even if one of the hubs fails, you can still communicate through other one. So this is another kind of  protection. (Refer slide time: 47:51-48:14)

And finally I have mentioned this point before ? just a reminder that a network consists of many layers and each layer may have its own protection mechanism built in, independent of other layers. So there are both advantages and disadvantages to this. We have already talked about the advantages. (Refer slide time: 48:15-49:51)

This is an example of a WDM link carrying SONET traffic. So there is a WDM link, so there is a SONET ADM. This is a working fiber pair and protection fiber pair. Please note that the pair has been shown as one line over here because usually as you know that fiber optic line is a simplex line; that means it goes in only one direction; there is a source in one side and the detector on the other side. Usually these fibers always come in pairs. The other side is for the communication in other direction so we have a working fiber pair over there and protection fiber pair through this WDM link. Look at this 1:2 protected scheme. What is happening is that there is one protection fiber pair, there may be protection fiber pair and through the working fiber there may be multiple virtual links going through the working fiber using different wavelengths. A is a normal operation and B link is cut and the traffic is restored by the optical layer. That means you automatically assign new wavelengths and new paths through the fibers etc., to bring it up. So this you may do at the optical layer rather than at the electronic layer. (Refer slide time: 49:52-51:58)

But the case we are talking about here is that the SONET is riding on the optical layer. So we have the optical layer at the bottom then you have a SONET on top of it. On top of SONET also there will be the data link layer and then so on all the other links. What we are saying is that each layer may have its own protection mechanism. So for example I mentioned that the SONET will also have a protection mechanism of its own. So if a SONET LTE, i.e., the SONET line terminal equipment, senses that it cannot communicate to the next LTE, this will automatically reroute the traffic and try to reroute the traffic in the other direction.  Sometimes it is good to have multiple protections at multiple layers but what might also happen is that they might sort of cancel each other or they might go into a race condition. So these are the disadvantages of having protection at various layers. There could be some disadvantage also if they are not very well coordinated, which usually would not be because these layers are sort of independent of each other. They talk only to their peers and go through their own protocol and give protection. Apart from these of course the other disadvantage is that you may redo some part of the protection unnecessarily. You may be unnecessarily duplicating the work at various places. So these are the disadvantages of having protection at multiple layers. (Refer slide time: 51:59-52:34)   

What is the advantage of optical layer protection ? speed and efficiency. Limitation would be detection of all faults may not be possible; protects traffic in units of light paths. So this is another problem. As I mentioned, in light path the granularity is very coarse. It may be 2.5 Gbps; so you are really giving the protection at a level of granularity, which may be quite high as I just now  mentioned; it could lead to race conditions when optical and client layers both try to protect against the same failure. (Refer slide time: 52:35-53:28)   

Of course, on the optical layer you have one more dimension to play with, which is in the case of a WDM. That means you have different wavelengths, so instead of 1+1 link, you can talk about 1+1 wavelength path selection. You can sort of try to select two independent light paths and the signal is bridged on both protection and working fibers if you are doing 1+1 protection kind of thing; the receiver chooses the better signal. In case of a failure, the destination switches to the operational link that is operational light path; there is revertive or non revertive switching; that means, if the original link comes back, it may revert or it may not revert; and no signaling is required. (Refer slide time: 53:29-53:30)

So that is the unidirectional light path. I have just shown you some of the schemes which are used for protection and restoration. And as I mentioned, the schemes may be partially deployed and some of the parts may be protected and some of the parts may not be protected  and so on. But the essential idea is the same ? that you build in some kind of redundancy and the redundancy may be in the form of entire fibers or the redundancy may be in the form of light paths or wavelengths. It may be pre-computed and pre-reserved like 1+1; it may be pre-computed but not pre-reserved like in a shared one or it may be computed as and when the failure occurs; that is, in the case of restoration. So there are various approaches to it. Depending on how critical the problem is or how critical the application is, what is the cost and how much extra provisioning you can do, you can choose your own way of protection and restoration. Thank you. 

Preview of the next lecture
Lecture ? 13
Multiple Access
Good day so today we will talk about multiple access ok now what is multiple access  (Refer slide time: 54:59-57:54)



If you remember  that we had  seven layer in the so called OSI stack the top on being application then we have presentation session transport network link and physical ok  we had been mostly talking about the physical layer till now although  for optical networks we sort of ventured into some of the hired layers  but from this lecture onwards we want  to concentrate on  the link layer ok now  what medium access does is it coordinates  competing  request request for what for medium that means that there is a medium  which may be an object of contention meaning that I mean several nodes may want to use it and  this medium access control protocol has to do with how to handle that so sharing of link and transport of data over the link that is an general the description of what the data link layer does  so when we share a link there is a question of committing request and we have to have some way of reserving that  and of course there is a also question of transport of data over a link  link if you remember when we say a link I mean that two nodes which are connected  the nodes may be or computers routers switches etc they are two networking nodes they are directly connected now the when  I do like this  it might mean   cable copper cable or a  fiber  or it might mean a shared medium like the free space ok  so there is a but there is some way of communicating directly between these two nodes that is what we mean by link that means that is just one hop in the network that is what we are talking about in the data link layer so there is a question of reliable transfer of data over these link and if this link like when you have a free space transmission  with so many nodes in the network now so many people would like to transmit so there is a question of sharing this medium and there is a question of who would access it when and just as I said free space could be a shared medium similarly  if you remember that if you have some kind of a bus  if you remember our discussion about topology of networks when we have some kind of a bus  from which the number of nodes are hanging and  that bus  may also be an object of contention so that bus is the medium through which communication is taking place and there is an  there is some kind of  competition or sharing between the of this shared medium between the nodes so we have to handle that that is the other thing  so examples of contention based or ALOHA and slotted ALOHA (Refer slide time: 57:57-59:00)



 these refers to some protocols which we are used in satellite communication so we will discuss these and we talk about satellite communication we have CSMA  it stands for  carrier sense multiple access or CSMA CD which is carrier sense multiple access with collision detection there are other variance of these like carrier science multiple access with collision avoidance  and things like that  so these ALOHA slotted ALOHA for satellite CSMA CSMA CD or specific specifically CSMA CD is used by Ethernet  in many situations and then we have   CSMA CA may be for cellular communication etc so these are contention based MAC and  Round Robin there are these are  token based protocols and so two very common ones are token bus and token ring  so actually we will discuss these  in the next in this lecture as well as the next we will be discussing these 



COMPUTER NETWORKS
Prof. Sujoy Ghosh
Dept of Computer Science and Engineering  
IIT Kharagpur
Lecture Name #13
Multiple Access
(Reference Time: 00:45) 
Good day. Today we will talk about multiple access. What is multiple access? (Refer slide time: 00:55 - 00:55) 

(Refer slide time: 00:56 - 02:21)

If you remember, we had talked about 7 layers in the OSI stack, the top one being application, then we have presentation, session, transport, network, link and physical.   We had been mostly talking about the physical layer till now, although for optical networks, we sort of ventured into some of the higher layers. But from this lecture onwards, we want to concentrate on the link layer. What medium access does is, it coordinates competing requests for medium; that means, there is a medium, which may be an object of contention, meaning that several nodes may want to use it and  this medium access control protocol has to do with how to handle that. Sharing of link and transport of data over the link is in general the description of what the data link layer does. When we share a link, there is a question of competing requests and we have to have some way of resolving that, and there is also a question of transport of data over a link. A link if you remember is two nodes, which are connected. The nodes may be computers or routers, switches, etc., the two networking nodes are directly connected; it might mean a cable, a copper cable, or a fiber, or it might mean a shared medium like the free space. There is some way of communicating directly between these two nodes; that is what we mean by link. That means it is just one hop in the network. That is what we are talking about in the data link layer. There is a question of reliable transfer of data over this link and if when you have a free space transmission with so many nodes in the network, many people would like to transmit. So there is a question of a sharing this medium and there is a question of who would access it when. Just as a shared free space could be a shared medium, Similarly, if you have some kind of a bus from which a number of nodes are hanging, that bus  may also be an object of contention. Bus is the medium, through which the communication is taking place and there is some kind of competition of sharing between this shared medium between the nodes. So we have to handle; that is the other thing.  So actually this (Refer slide time: 03:52- 04:14)


link layer may be divided into two sub layers, so to say: the upper one is the logical link control and the lower one is the medium access control. We will talk about logical link control later. We start with medium access control; there are a number of medium access control protocols and techniques. We will discuss a number of them. (Refer slide time: 4:15-5:00)



The situation we have is computers in a shared network environment and although the medium is shared, it is taken that only one computer can transmit at a time. If two computers try to use the same line at the same time for transmission, i.e., when one is transmitting one or more may receive. Receiving is not a problem; but when two  computers want to transmit at the same time, their messages get garbled. We say that there is a collision. How can we organize the transmission so that all computers are given an opportunity to exchange messages? If you remember, when we were talking about multiplexing, that is also in some sense sharing a medium. So we had this time division multiplexing and frequency division multiplexing; actually we also have time division multiple axis and frequency division multiple axis, etc., over here. The difference between the two cases is that in the multiplexing case, the lines are all coming together into the multiplexer, whereas in multiple access, the points are all geographically distributed and they may not know about each other. There may not be much of a coordination; in some schemes we do have some coordination in some schemes we do not have any coordination. So all these cases we will see. There may be (Refer slide time: 06:03 - 06:26)











point-to-point links in a network like PPP for dial-up access, etc., which is quite common ? point-to-point link between Ethernet switch and a computer. So these are examples of point-to-point links; we will come to this later on. Then there are broadcasts, like a traditional Ethernet, upstream in an HFC. HFC is a hybrid fiber coaxial; some part it is fiber and some part of it is coaxial, and the upstream traffic there  also uses that medium as a shared medium. Then we have 802.11, which is a wireless standard. We will have a separate lecture on this wireless networking, etc. 802.11 is the number of the standard, which defines some form of a wireless networking. Wireless is a shared medium because it is sharing our common shared   electromagnetic field that we already mentioned. There may be a single shared broadcast channel, (Refer slide time: 7:14 - 7:44)


 
there may be many also, or more than one also. Two or more simultaneous transmissions by nodes will mean interference. Only one node can send successfully at a time. Multiple access protocol is a distributed algorithm; so this is where it differs from multiplexing. It is a distributed algorithm that determines how nodes share channels; that is determined when a node can transmit. Communication about channel sharing must use the channel itself; that means, we have what we earlier called in-band control usually. That means if you have a, let us say, wireless network, if you are running any kind of protocol for control purpose, etc., that would also be using the same medium. The same question of how to coordinate that with all the other competing nodes ? which are trying to transmit data ? all that comes into play. (Refer slide time: 8:15 - 9:03) 	 	



There are centralized approaches to a MAC, i.e., medium access control. The centralized approach is simpler in the sense that there is a controller, which grants access to the medium. It is simple, has greater control, priorities, quality of service, etc. ? all this can be implemented very easily. For example, if you want to give higher priority to one particular node the controller simply just takes note of the priority when allocating the medium to a particular node. But there are problems with centralized approach also. One is that a single point of failure is a big problem. When we have a centralized system and if that system fails, then the whole network is down, which may not be a good idea in many many situations. Secondly, there will be a performance bottleneck; meaning, first of all everybody is communicating to it but when the network becomes bigger, the demand on its computational capacity, its control mechanism, etc., goes up proportionately and that is also another problem in centralized approaches. In decentralized schemes (Refer slide time: 09:46 - 09:57)


all stations collectively run MAC to decide when to transmit. So in a decentralized scheme we are running some kind of a distributed algorithm. (Refer slide time: 09:58 - 10:55 )



There are various kinds of MAC protocols: one is called something like a round robin MAC. In round robin there are a number of stations and each station takes its turn in a round robin fashion. There are some protocols based on round robin MAC. So here, each station is allowed to transmit; a station may decline or transmit. That means, if it has nothing to transmit, it may decline. Otherwise, it will transmit. It could be centralized; for example, polling, or distributed; for example, token ring.  Control may be centralized or distributed of who is next to transmit. When done, the station relinquishes and the right to transmit goes to the next station. This is efficient when many stations have data to transmit over an extended period. So that may be a good  scheme. (Refer slide time:10:56-11:18)



There are scheduled access MACs, like time is divided into slots just like our time division multiplexing; station reserve slots in the future; multiple slots for extended  transmissions and suited to stream traffic. There is a question of reservation of some slots. That would be a part of the protocol about how reservation will be done, who can reserve, and if two people want to reserve simultaneously then what happens. So we have to handle issues like that. This is the scheduled access MAC, when it is scheduled, some reservation is done beforehand. Then there are some contention based MACs. (Refer slide time: 11:41 - 12:14) 



Contention means there is hardly any control; there is no control. Stations simply try to grab the medium. This is distributed in nature and surprisingly, for no control, it performs quite well for a bursty traffic, but it can get very inefficient under heavy load. Actually, the round robin MAC that I mentioned before, and the contention based MAC are very common. The most common example is the Ethernet, which we will see especially later on; how it is done, or how satellite communications use contention based MAC. That means nodes simply track and try to grab the control but over a long period of time, the amount of data or the number of times the nodes on an average try to communicate and the transmission time ? these two factors together define the load on the system. If the load is light then this contention based protocols very surprisingly are much simpler and may be cheaper to implement. So examples of contention based are ALOHA and slotted ALOHA. (Refer slide time: 13:06 - 14:08)



These refer to some protocols which we use in satellite communication; so we will discuss these when we talk about satellite communication. We have CSMA ? it stands for carrier sense multiple access or CSMA CD, which is carrier sense multiple access with collision detection. There are other variants of this, like carrier sense multiple access with collision avoidance and things like that. ALOHA or slotted ALOHA are for satellite, CSMA, CSMA CD, specifically CSMA CD, is used by Ethernet in many situations; and then we have a CSMA CA, may be for cellular communication. These contention based MACs. Round Robin, are token based protocols and two very common ones are token bus and token ring. Actually, we will discuss these in this lecture as well as in the next. (Refer slide time: 14:09-15:38)


We can sort of summarize; first of all we can divide the networks into two types, depending on their topologies ? one is the bus type. Bus means some communication channel and everybody is connected to the same communication channel. It may be a coaxial cable or some fibers or it could be a ring. Then we have this token bus, which is one particular MAC protocol. It has got IEEE standard number 802.4 or polling in 802.11. We will come to this later on. These are examples of bus type topologies using Round Robin techniques. Token ring is a ring type topology and 802.5 is the  standard number. FDDI is a Round Robin,  but the topology is the ring. There are scheduled approaches to medium access like DQ DB or distributed queue dual bus  802.6.  We will be talking about this also. Contention based are CSMA  CD, CSMA CA in 802.4 and 802.11. The first one is Ethernet and the second one is a wireless network; so we will look at these in later lectures.  (Refer slide time: 15:39 - 15:49)


 What is the ideal of a multiple access protocol? Suppose we have a broadcast channel of rate R bps. Well, any broadcast channel would have some maximum limit or the rate you can communicate over; that depends on the characteristics of the medium. If their medium is different, this rate R may be different; for example, the rate for a fiber optical cable would be much higher than the rate for a coaxial cable. But the point is that there is always an upper limit due to various reasons; we need not go to that in this lecture. Due to various reasons of how we communicate, etc.,  there is a maximum rate R, which you can achieve on a particular medium using, let us say,  the technology, which is present now. When one node (Refer slide time: 16:43 -17:52)



wants to transmit, it can send at rate R. There is no problem; it can send all the way up to rate R if the technology permits. When M nodes want to transmit, this R is not going to change. So what you want to do is that you want to divide up this rate R and distribute it among these M nodes, which want to communicate. So each can send at an average rate R/M. If that happens, naturally we have an ideal multiple access protocol; we can not do better than that. But it is difficult to  achieve this theoretical rate because there will be some overhead for the protocol itself, which will eat up something. None of the protocols is 100 % efficient.  Secondly,  we would ideally like it to be fully decentralized. So there is no special node to coordinate transmissions, no synchronization of clocks and slots. Lastly, it will be simple. Naturally if it is decentralized, no synchronization, etc., that is one aspect of simplicity then the protocol itself will be simple. This is the other thing; and we want a fully decentralized scheme, I mean we preferred them because there would be no failure of single nodes, etc., and then there is no bottleneck. These problems should be avoided if you have a fully distributed system. In multiple access (Refer slide time: 18:18 - 18:57)



as I mentioned, many users are sharing a resource at the same time; it is needed because users must share the cells. First let us talk about three different multiple accesses: frequency division multiple access, time division multiple access and code division multiple access. Frequency division multiple access and time division multiple access are very simple. We have already talked about multiplexing; only thing is that the connotation is slightly changed over here. Code division multiple access is somewhat different; so we will discuss it in some more detail. So they use the same frequency, same time, but different codes; we will come to that. (Refer slide time: 18:58 - 19:16)



Let us start with FDMA. FDMA is frequency division multiple access. Just like frequency division multiplexing, channel spectrum is divided into frequency band. Each station has an assigned fixed frequency band. Unused transmission time in frequency bands goes idle. This is the simplest possible scheme. It is also not very efficient because many of the stations will be idle and that frequency band for that particular time is sitting idle; that part of the bandwidth is wasted. Also, we know that in a data networks the net traffic is very bursty; that means people want to transmit for a very short amount of time. Suppose the transmission time is T; then may be for 500 ? T amount of time, it just sits idle. It is very bursty; so a frequency division multiple access would be very inefficient. But this is the oldest; you know, radio stations are doing multiplexing since the communication is only one way.  But  if you are using some other kind of say radio frequency transmission using fixed channels, when there is both way of communication, then it is some kind of multiple access also. So suppose we have six stations LAN 1,3 (Refer slide time: 20:26 -  20:34)




and 4 have some packets to send. So they are sending;  frequency bands 2,5,6 are idle. This happens quite often an FDMA. (Refer slide time: 20:35 - 21:26) 



This is another picture; we have tried to show with colour the graphs of the different channels. The channels are differently colored (these bars), and we have the three   accesses ? f is the  frequency,  c are the channels and t is the time. There is only one channel at all times that uses the particular frequency; another channel for all times uses another frequency, and so on. Another frequency means it is not a single frequency; this is a frequency band kind of thing. You always require a band; you cannot communicate anything with a single frequency that will be just a single tone. (Refer slide time: 21:27 - 21:56)



Frequency division multiple access for each channel gets a band  (range) of frequencies used in traditional radio, TV, first generation cellular, etc. Advantage is that there is no dynamic coordination; it is absolutely distributed. Disadvantage is that it is inflexible and inefficient if channel load is dynamic and uneven. By today?s standards, it is quite high, but at one point of time it was a quite good.  (Refer slide time: 21:57 -22:10)


We will see another version of this FDMA, which is WDMA. WDMA means wavelength division multiple access; and as you can suspect, now we are talking about fibers. We just have a frequency division multiplexing and wavelength division multiplexing. They were actually the same thing; the only thing is that for fiber we choose to call it wavelength division multiplexing. Here also, we are talking about wavelength division multiple access. Previously all our fiber links that we talked about were point to point. Suppose the fiber link somehow is used for multiple access,  two stations in a very dynamic fashion, we want to give this bandwidth. A number of stations can connect to the same fiber through some splitter, coupler, etc. So we are not going into that. Suppose somehow they are hooked into this fiber and we want to use wavelength division multiple access. Let us just look at the scheme. (Refer Slide time: 23:03 - 23:56) 



Each station is assigned two channels; a narrow channel for control and a wide channel for sending data frames. The channels could also be of the same size, does not matter, but then for control we do not require so much bandwidth. For a particular node, these two are assigned. A node?s control channel is used by other stations to contact that node. So its control channel is fixed. Some other station, if it wants to communicate with A, will first talk to A on its control channel. Both channels are divided into n + 1 slots, which repeat endlessly. Slot 0 is especially marked to synchronize the nodes.  In each channel, there are, say, n + 1 slots and then a number of nodes; let us say, three nodes, B, C and D can communicate with A at the same time. Because of communicating to A it has to send to A in the channel or the wave length which has been assigned to A.  But how can all three send at the same time? If there are slots in that channel and if they have booked separate slots, in different slots their data can go. So B, C, and D simultaneously communicate with A. (Refer slide time: 24:33 - 25:42) 
 	 


Each station has two transmitters and two receivers. First it has to have a fixed wavelength receiver for listening to its own control channel. Its control channel is fixed, so it requires a fixed wavelength receiver. Then it wants to communicate to somebody else?s control channel. For that it requires another transmitter and that has to be tunable because A may want talk to B at one point of time and then talk to C at some other point of time B and C?s control channels are different. So we sort of have to have a tunable transmitter. We have a fixed wavelength transmitter for outputting data frames. Suppose it wants talk to B it can talk to B?s control channel, when A wants to communicate over the wavelength assigned to it. Then it can output the data frame at a fixed wavelength. (Refer slide time: 25:43 - 26:09)


 
It requires the tunable receiver for selecting a data transmitter to listen to. A is talking to B, so for talking to or listening to B it requires a tunable transmitter because it may want to listen to B now and later on, it may want to listen to C. The data channels for each station contains a special slot where the status of both the channels, which slots are free, etc., are reported. (Refer slide time: 26:10 -26:36) 

When station A wants to set up a connection oriented channel to station B, it tunes to B?s data channel and waits for the status slot. It puts in the request in any of B?s free control channel slots. B?s status slot will tell whatever control channel slots are free. So B just puts in a request over there. When B sees the request, it assigns the slot to A and announces it to others. (Refer slide time: 26:37 - 28:18)


There is a problem in a distributed system, in the sense that you always have to worry about this kind of thing. If two stations grab the same control slots simultaneously, both of them want to talk to B and both of them sort of want to grab that same control slot by some chance simultaneously. When we say simultaneously, they both are close together, and so the information will get garbled. So naturally if both of them are trying to put things in the same box the information will get garbled and both of them will notice. In that case both will back off for a random amount of time. When they do back off for a random amount of time, the random number generated by one station and the random number generated by the other station will be different. So both of them will back off for two different amounts of time, and which ever has the shorter waiting time, comes back and tries to put again on this control slot. Hopefully there will be no collision. This is one kind of collision and backing off. This kind of scheme is used elsewhere also; we will come to that. For two-way communication, B repeats the same algorithm for A for variable bit rate traffic. This is for synchronizing, so one slot can be sort of reserved for this communication between A and B. For variable bit rate traffic, slots in data channel are not booked so they can be sent  by some other mechanism. Let us not go into that at the moment.  (Refer slide time: 28:19 - 29:59)



Next, we come to time division multiple access. If you notice the figure, this looks just like time division multiplexing. We have the information or signal from this top one, high one coming and then it is sort of interspersed with the signal from this source go. They are  going in a time division multiplex fashion. Only thing to notice over here is that this line and this line they may not be actually physical wires. That is why some wireless has been put over here. So they are actually sharing the same medium. So immediately you see the problem ? what happens if this synchronizing behavior breaks down. So somehow you have to make sure that they do not break down and they do communicate at precise slots of time, which is assigned to them. Then you have the additional trouble of ensuring that the clocks synchronize. If he is running a clock of different zone these two clocks may not agree, so how to synchronize them, etc. These are the problems which you solve in a TDMA, time division multiple access. Otherwise the basic approach is the same as time division multiplexing. (Refer slide time: 30:00 - 30:15)      	                      	   


TDMA is time division multiple access. It enables access to channel in rounds. Each station gets fixed length slots, length is equal to packet transmission time in each round. The unused slots go idle; this is a simple scheme. (Refer slide time: 30:16 - 30:49)
	 

   
For example, suppose we have six stations: LAN 1,3,4 have packet and slots 2,5 and 6 are idle. This kind of frames sequence keeps on repeating. In each frame, 1 will have a slot, 2 will have a slot which is going idle, 3 will have a slot, 4 will have a slot, 5 and 6 will also have slots which are again going idle and so on. So this is a TDMA system.  (Refer slide time: 30:50 - 31:33)



Each channel gets entire spectrum for a certain time period, rotating time period. So  whenever the time is allotted to that particular channel it gets the entire spectrum to itself, so it can send the data at a very high rate. The advantage is that it can assign more time to senders with heavier loads. By doing that, it gains some kind of efficiency advantage over FDMA and another side of the same point is that you can reduce the power consumption. The disadvantage is that it requires precise synchronization, which we talked about earlier. This is a diagram for the (Refer slide time: 31:37 - 31:59)



time division multiple access. The entire frequency band is given to one particular channel for a short amount of time. Then, for the next short amount of time, another channel gets the entire frequency band and so on. As it is there in standard time division multiplexing, the same thing applies. (Refer slide time: 32:00 -32:51) 


Now we can combine TDMA and FDMA. Each channel gets a certain frequency band for a certain amount of time. An example is GSM; we will come to this may be  in slightly more detail later on. So, this is combining both TDMA and FDMA; that means, we are breaking it up not only in the time domain but also in the frequency domain. The advantage is that there is more robust against frequency selective interference because for a particular communication the frequency is sort of changing; there is much greater capacity with time compression; and there is inherent tapping protection. So it has got a lot of advantages and it is quite efficient. The disadvantage is that, once again, just as in TDMA, we have to synchronize the clocks, etc. So here, frequency changes must also be coordinated. (Refer slide time: 32:52 - 33:09)



This is the picture; basically, we have small blocks. So for this particular yellow block over here, at this particular time, this particular frequency range is given to this channel and so on. (Refer slide time: 33:10 - 33:46) 


Next we come to channel partitioning or CDMA. CDMA is code division multiple access. This may be a little interesting and new. We have a unique code assigned to each user, that is, code set partitioning. What is that code? We will come to that. It is used mostly in wireless broadcast channels like cellular satellites, etc. All users share the same frequency, but each user has his own chipping sequence, that is, code, to encode data. We will come to the details. Encoded signal is the original data; (Refer slide time: 33:54 - 34:13)


and then there is some operation with the chipping sequence. We will come to what is chipping sequence. Decoding is the inner product of encoded signal and chipping sequence; it allows multiple users to coexist and transmit simultaneously with minimal interference if codes are really orthogonal. (Refer slide time: 34:14 - 34:27) 

We have a number of stations sharing a number of channels. Each station transmits over the entire spectrum all the time, which means that it is not excluded either in a time dimension or in a frequency dimension. Each channel can  go on communicating  in the entire frequency band all the time. Now how will that happen ? because other people are also communicating in the entire frequency band all the time. All their signals will get mixed up; as a matter of fact, they do get mixed up, but the point is that the signals are encoded in such a clever fashion that from that mixed signal you can separate out all the different streams of communication that have gone into it. So multiple simultaneous transmissions (Refer slide time: 35:06 - 35:20)



are separated using coding theory. There is an assumption that the signals add linearly; if they do not, then you have to do some adjustment, etc. We will not go into that. (Refer slide time: 35:21 - 35:23)

CDMA is a form (Refer slide time: 35:29 - 36:17) 



of spread spectrum multiple access. It is spread over the entire spectrum. Instead of sending b bits per second for a particular node, we send m b chips per second. What is a chip? Each bit is encoded by m number of chips. They are sort of tiny fragments of bits and these chips may again be 0 and 1. There is a 0 1 sequence code for these bits of one particular station; another station will have another code. A 1 MHz channel with 100 stations gives 10 KHz per station. With fewer than 100 chips per bit, the effective bandwidth is higher and the channel allocation is also done at the same time; we will see how. (Refer slide time: 36:18 - 36:29)


This is the picture, which is funny because for the entire time for the entire frequency band, all the channels are using it at the same time (Refer slide time: 36:30 - 37:09) 




Now each channel has a unique code. All channels use the same spectrum at the same time but orthogonal codes. It is bandwidth efficient; its capacity also is quite good,  when I talk about simple TDMA. But you can sort of mix up FDMA and TDMA and get a good efficiency over there also. This fight between GSM and CDMA is ongoing and it will go on for some time. The disadvantage is that it has more complex signal regeneration. So this is how it is implemented. (Refer slide time: 37:10 - 37:24)



As I said, each bit time is subdivided into short intervals called chips and typically there are 64 to 128 chips per bit. That means for each bit we have a code, which is 64 bits long. The chips are again in a sequence of 1s and 0s. So we are sending a long sequence of 1s and 0s for sending may be 1. But we are sending it very fast; and we can send it very fast because we are using the entire spectrum. The entire spectrum is at our disposal; so we can send it very fast. (Refer slide time: 38:04 - 38:12) 


Each station is assigned a unique m bit code or chip sequence, which is used by the station to transmit 1; its complement is used for 0. (Refer slide time: 38:13 - 38:39) 


We will see an example: two codes, S and T, are said to be orthogonal; we have been talking about orthogonal codes. If under a certain operation we have S let me call it a  dot product kind of thing at the moment. So S dot T is equal to 0 and S dot S is equal to 1. You see that this is very similar to dot products in vector. If you have two orthogonal vectors, the dot product is going to be 0, whereas on the same vector when we take its dot product with itself it is going to give some value. We are representing it as 1. We define the following. Let us (Refer slide time: 38:57 - 39:19)

define this dot operation for our case. So S dot T is equal to 1/M  ? summation of Si TI;  i is equal to 1 to m and this will be equal to 0 if and only if S is not equal to T. We will see this. (Refer slide time: 39:20 - 39:29)

if one and zero are represented by  +1 and ?1 respectively, we find that S dot S equal to one. this is a very simple because if the ones have been represented as one and zero has been represented as a  ?1, so S dot S so ?1 will get multiplied with ?1 giving you  +1 and one and one will also give you one. so all of them would be one. so all M of them would be one. the sum total in the previous if you look at the previous  definition the sum (Refer slide time: 39:53 - 40:16)


Si Si is going to be a sigma Si Si is going to be M; you divide that with M so that will give you a 1. so s dot s would be 1 whereas for any other code, any other T, where the T is orthogonal to S, this sum is going to come out as 0. That is how we cleverly assign the codes. (Refer slide time:  40:17 - 42:35)



Consider a code T where the number of chips of T, which are the same as those in S is the same as the number of chips of T, which are different. Consider what it is saying: at code T, where the number of chips of T, which are the same as those in S are the same as the number of chips of T which are different, the number of chips of T which are the same as those in S. So the corresponding chips, when multiplied together, will give you so many 1s. If it is the same as the number of chips of T which are different, now if the chips are different in T and S when the corresponding chips are multiplied, they would give you ?1. The number of +1s and the number of ?1s, if they are the same when you add them together in the previous summation, the sum total will become 0. S dot T will be zero. Note that S dot not of T; not of T is the complement of T. That means the 1s and 0s are presented with 1s and ?1s. So 1s and ?1s are flipped in one of them. So naturally the number of chips which are different become the number of chips which are the same and the number of chips which were same earlier, the chip positions which were same earlier, now become different. But any way their numbers are equal. So once again we have S dot not of T is 0 and S dot not of S ? all the 1s in S will be ?1 in not S and all the ?1s in S would be 1 in not S. So in either case, we will get a product of ?1. M ?1s added together will give you ?M divided by M will give you ?1. We have S dot S equal to 1; S dot T is equal to 0; S dot not of T is equal to 0; S dot not of S equal to ?1. So these are the nice properties if we have orthogonal codes. (Refer slide time: 42:36 - 43:44)



So consider the following codes: suppose this U, R, S, and T are four different stations. Here a simple example has been shown using only eight chips. You note that between U and R, in one position they are same; in this position they are same; two they are different; in this fifth position they are same. That is the number three of the position in which they are same and then there is the number four position, which is the seventh position, where they are the same. In four positions they are the same and four positions namely the third position, fourth position, sixth position, and eighth position they are different. The number of positions in which they are the same is the same as the number of positions in which they are different. We know that if instead of 0 we have a ?1 these codes will come out to be orthogonal. Similarly you would see that U, R, S, T are all orthogonal to each other. So the above codes are all orthogonal under the operation defined. (Refer slide time 43:45 -45:09)


Note that S dot T plus R is S dot T plus S dot R, which means that under normal addition, if you take this is normal addition, this dot product is going to distribute. Thus under the assumption of linear addition of signals, S dot sigma of Ci is equal to S if and only if S is in Ci.. That means suppose there are a number of channels, which are transmitting. In that case, we have already assumed linear addition of signal strengths. This is nothing but some of the signal strengths like Q, T, R, U, S, etc., have been all added up. Now S dot will distribute over this summation, so we have S dot T and S dot R etc. If S happens to be in this set Ci, then that S dot S will come out as 1. It should be 1 if and only if S is in Ci; otherwise it will come out as 0. Assuming during any bit time U, R, S, and T transmit 101 and nothing, U is trying to transmit 1, R is trying to transmit 0, S is trying to transmit 1, and T is not transmitting at all. If you use the code shown in the previous slide over here (Refer slide time: 45:10 -45:13)



and then if you do the calculation, (Refer slide: 45:14 - 45:30)
 


this will come out to be like this: the signal strength as we see it, will come out as ?1 +1 ?3 +3 ?1 ?1 ?1 +1 ? so these are the eight signal levels we get. (Refer slide time: 45:31 - 45:50)


Now if we do that, C dot U will get a +1. That means C dot is trying to send 1; C dot R will be ?1; that means R is trying to send 0; C dot S will be +1; that means S is trying to send 1; and C dot T would be 0; that means C is not sending any thing at all. If you remember, (Refer slide time: 45:51 - 46:01)



101- that means a S R, U and T are transmitting in this fashion. (Refer slide time: 46:04 - 46:33)



One implicit assumption in the above is that all stations are synchronized and transmit with the same power. In  practice, perfect synchrony is difficult to achieve resulting in the use of longer chip sequences and lower channel capacity. Secondly, to tackle the problem of power, each mobile transmits to the base station at the inverse of the received power. So there are practical limitations. Although this looks very nice, what happens is that there are practical limitations and the theoretical maximum that we could achieve is less than that but this is an elegant system. Now we will look at some (Refer slide time: 46:50 - 47:18)



MAC protocols: two of them, two small ones we will discuss in this lecture, and then, in the next lecture, we are going to take two of the more involved ones. Taking turns MACs protocols: do you remember that we had channel partitioning MAC protocols and random access MAC protocols and taking turns protocols? In taking turns, what it tries to do is that it sort of allocates the turn and we will see how. (Refer slide time: 47:19 ? 47:55)



One could be through poling by a master node, which is some kind of a centralized system. There is poling of overhead, latency, single point of failure, etc., we are not going to discuss this at this point of time. We are going to focus on this in the next; that is token bus control. A token is passed from one node to the next sequentially;  there is a token message that concerns token overhead, latency, single point, etc. We will see some systems based on tokens now, which are sort of using MAC taking turns.  (Refer slide time: 47:56 - 48:00) 

And the first example we are going to talk about is the token bus. (Refer slide time: 48:01 - 49:05)



So they may be mainly used by assembly line factory. It is used in factories for the main reason that in the other kind of system, which is the contention based system, there is some randomness in the way communication can happen. There may not be any hard and fast guarantee, which people in process control and factories, etc. may not like. So they may use this token bus, which has got the IEEE number 802.4. Token bus is just like a common bus and the principle is like a token ring. Token is passed from high to low number of station. What is a token? Token is some kind of a bit pattern, which is passed from high to low number of stations. The station with token will transmit. It is difficult to add and remove stations in this particular case. (Refer slide: 49:06 - 49:50)



So IEEE 802.4 determines the logical ring of the physical bus by the numerical value of the addresses. A MAC or LLC data unit provides the utility for the lowest address to handle the token to the highest address. Otherwise, the higher address gives the token to the lower address so the predecessor gives the token to the successor then the successor gives the token to the next successor and so on, all the way down the chain. Then there is some protocol for sending from the lowest one to the highest one in one group. Then the token is passed from a predecessor station to the successor station. so this a sort of taking turns kind of thing. (Refer slide time: 49:51 - 50:35)


The token is passed from stations to stations in a descending numerical order of station address. When a station hears a token frame, that means it gets a token frame addressed to itself, it may transmit data frames. That means when it gets the token, which is addressed to itself, and which has been sent by the node which is just higher in number, which is its predecessor, that will be sending the token address to the next station. At that point of time, it may transmit its data frame. When a station has completed transmitting data frames, it passes the token to the next station in the logical ring. So if all of them are trying to transmit they will sort of form a nice queue and then they will come back in a very regular fashion; that is the worst case. (Refer slide time: 50:36 - 50:35)

At some particular point of time, a node may not have anything to communicate. In that case, it will simply pass on the token to the next lower address, next lower MAC address. By the way, I have used the term MAC address earlier also. MAC is for Medium Access Control; for that, we require some kind of address. A particular node, if it has nothing to transmit, will give the  control or the token to the next lower  MAC address and then the next lower MAC address will, if it has something to transmit, will transmit. What is the worst case? The worst is that when everybody wants to transmit; that will take some time, but there is a bound to that time and after that time your turn will come back again. So there is a bound to the worst case performance in  this token bus. But  this is not very efficient, and not very fast. So it is getting replaced now, but even in some factories it is still there. We are going to talk about another technology, named DQDB,  which was once proposed ? it also had a name S M P S ? as a solution for metro networks and that means for metro networking, DQDB was suggested and some were implemented. But DQDB is once again going out. It has a similar kind of principle; only thing is that instead of a single bus we now have two buses. So let us look at DQDB very quickly. (Refer slide time: 52:20 - 52:23)


DQDB is 802.6. (Refer slide time: 52:24 - 52.54)


So it is for a metropolitan area network spanning may be 50 to 100 kilometers and operating at a 34 to 45, even 155 mbps speed was talked about at one point of time.  This did not work out quite well later on; but any way, that is a difference story. It was originally designed like FDDI ? we will be talking about FDDI in the next lecture ? for connecting LANs; expanded to service packet switching at 2 Mbps and isochronous services. Isochronous means nearly synchronous. (Refer slide time: 52:54 - 53:30)



It features fixed length packets like 53 bytes long, and we will see ATM later, which was inherited from this DQDB later on. ATM is still a strong technology even today. Empty cells are generated by the head ends. So there are two buses as I mentioned, and then, at the end of the bus, there are these head ends, which generate a stream of empty cells. The streams of cells move in the opposite direction in the two buses and finally fall off the other end. (Refer slide time: 53:31 - 54:20) 



So to transmit to a destination, the node has to know which bus to use; that means whether it is to the left or right, or up or down. So it has to know which bus to use. So every node must be having this information. It sets a request bit in some cell, which is going in the opposite direction. This is for telling suppose the node to which it wants to communicate is downstream, say towards the left, then it tells all the other nodes towards the right that it wants to a communicate to this node on the left. It defers to downstream requests, counting such requests as they pass by; nodes are not greedy. So this is the heart of the protocol. Actually what it does is that if you simply get an empty cell and want to put in your data, then those nodes, which are towards the end, are favored. I mean it is not a fair system any longer. But then, in this scheme, if you think about it, you will queue the request on a first come first serve manner and that is why you put in your request to the other side so that those nodes on the other side would know that he is going to send something.(Refer slid time: 55:00 - 55:06)
      


So to transmit to a destination, the node has to know which bus to use, etc., and the nodes are not greedy. (Refer slide time: 55:07 - 55:18)
 


Its primary importance today is its close affinity to ATM, and the consequential association with SONET and SDH, for which we shall see ATMs provide the approved a switching fabric. Thank you. In the next lecture, we are going to discuss two more token-based protocols, namely, token ring, which was more common than this token bus or DQDB and FDDI. Thank you. (Refer slide time: 55:37 - 55:39)



good day in the last lecture  we talked about various multiple access schemes and a one of this set of schemes in token bus and DQDV etc where  using tokens ok now we will use the  we will see two other variance of it namely token ring and a (Refer slide time: 56:05 - 56:10)


FDDI so we are going to talk about token based MAC and specifically  (Refer slide time: 56:11 - 56:29)

 
so they are some kind of Round Robin MACs that means a the  chance to transmit comes to each of the station in a Round Robin fashion and this can be done as I mentioned earlier through poling or token busing here we will be specifically  talking about token busing (Refer slide time: 56:30 - 57:03)


so  the first  a system that will talk about is the token ring  so as as the name itself suggests that it is a ring topology  it is a ring on a token ring MAC works with a special pattern or token which is three bytes long so it is a three bytes words of bits called token which moves from one computer to the next priority indicators are used within the token how the priority indicators are used we will see later so (Refer slide time: 57:04 - 57:35)


 so data rate may be four sixteen or hundred mbps medium may be UTP STP or fiber  signaling may  is usually differential Manchester  we mention this earlier what is differential Manchester that how you represent here zeros and ones by electrical signals or optical signals as the case may be and the maximum frame size  would be about this four thousand five hundred and fifty bytes or write up to eighteen point two kilo hertz (Refer slide time: 57:38 - 58:11)



now let us talk about the type of network stations which may be a  connected to an FDDI ring one is a dual attached station which is connected to both the rings that means it is a station which is connected to both the rings that is why it is called dual attached then we have dual attached concentrator DAC which is connected to both rings and provides connection for additional stations and concentrators it is actually the root of a tree this is where the  tree comes from I have a (Refer slide time: 58:12 - 58:35)  


a picture so we have a so this is the picture of an FDDI concentrator so you can see that this the concern this is the main part of the concentrator and the two rings are there the counter rotating this is the primary ring and this is the secondary ring so the primary  ring is coming like this from A to B and the secondary ring is going like this it has some additional ports from which other stations may hang  




	 



COMPUTER NETWORKS 
	Prof. Sujoy Ghosh	
Dept of Computer Science & Engineering 
IIT Kharagpur 
Lecture - 14
Token-Based MAC
(Refer slide time: 00:45)
Good day. In the last lecture, we talked about various multiple access schemes and one of this set of schemes is token bus DQD etc. We will see two other variants of it, namely, token ring and FDDI. (Refer slide time: 01:08-01:15) 

We are going to talk about token-based MAC. (Refer slide time: 01:15 ? 01:34)

They are some kind of round robin MACs. That means the chance to transmit comes to each of the stations in a round robin fashion. This can be done as I mentioned earlier through polling or token passing. Here we will be specifically talking about token passing. (Refer slide time: 01:34 ? 02:07)

The first system that we will talk about is the token ring. As the name itself suggests, it has a ring topology. A token ring MAC works with a special pattern or token, which is 3 bytes long, called token, which moves from one computer to the next. Priority indicators are placed within the token. We will see later how the priority indicators are used. (Refer slide time: 02:07 - 02:42)

Data rate may be 14, 16 or 100 Mbps. Medium may be UTP, STP or fiber. Signaling is usually differential Manchester; we mentioned this earlier. Differential Manchester is how you represent your 0s and 1s by electrical signals or optical signals as the case may be; and the maximum frame size would be 4550 bytes or right up to 18.2 KB. (Refer slide time: 02:42 ? 03:11)

In token ring, like a token bus, a token is passed around the ring, and within the token is an indicator that senses the ring as free or busy. If the token is busy that means some frame is being communicated. At that time, the token circles continuously around the ring are passing each station. Each station is required to examine the token. (Refer slide time: 03:11 ? 03:38)   

If a station wishes to transmit data and the token is empty, it seizes or captures the ring by modifying the token to a start of user frame indicator, appending the data and control fields and sending the frame around the ring to the next station. The next station will now get the token as well as the frame, which will pass on till we get (Refer slide time: 03:38 ? 05:41)

to the node where the data is copied only if it is to be passed to the end user application attached to the node. That means there is a destination address. When the destination node sees that data, it knows that this is for him. So he absorbs, that means, he copies it back. He makes a copy of it and sends it to the application layer in that particular node through all the other layers; we are not concerned about that at the moment. But the token and the frame continue circulating in the ring till it comes back to the center. When the token arrives back at the original site, the token is once again made free and placed onto the network. You see in this scheme only one frame ? I mean if the ring busy at all then one frame ? is traveling along it. It has left the source station, then it has been copied by the intermediate nodes on to the frame as well as the token with the busy indicator over there. Then it finally comes to the end station and at the destination station, it makes the copy of the data for its own use and keeps on circulating this frame and the token right up to it. When it comes back to the original sender, the original sender will now strip all these data, make the token free, and put it on the ring. Now, some body else who ever wants to transmit next, will capture the token and send it in this fashion. This shared medium, namely the ring, is shared by all these nodes attached to it. (Refer slide time: 05:41 ? 06:05)

When a station wants to transmit, it has to wait for the token, then it has to seizes it, and then it transmits the frame. When the station seizes token and begins transmission, there is no token on the ring. Nobody else can transmit. There is no  contention or collision as such, because only the station that has got the token can transmit; so all others do not transmit. (Refer slide time: 06:05 ? 06:44)

What is the expected performance of token passing? First of all, it is fair because it is going in a round robin fashion. So everybody will have his chance; each computer is given in turn an opportunity to transmit even when the traffic is high. However, even if only one computer needs to transmit a message, it has to wait till the time that it receives back the token. Until it receives the token, it cannot start the transmission, so it has to wait. Again, long messages should not be allowed because otherwise one computer may hold the token for too long. (Refer slide time: 06:44 ? 07:49)

Several  tokens are there. Some variations of it use slotted rings, where several tokens or slots are used. These may be more useful and make it more efficient because if it is a very long ring and only one frame is traveling down it, it is rather inefficient way of using the system. So what we can do is that we may allow multiple frames, that means multiple slots, which are sort of distributed over the space. For example, if the speed is 200 m/?s of the frame, the data rate is 10 Mbps. These ten bits will span over 200 m over the ring. So a 2 km ring can hold 100 bits; that is the kind of performance with a single frame. (Refer slide time: 07:49 ? 08:42)

Let us look at how the priority works in the token ring; because what we can do is that we can do differential priorities to the nodes in the network and this is how it works. Let us go through one example; assume a token ring has five stations attached to a priority ring. Station A has priority access of 1. 1 is, let us assume, the lowest priority; stations B and D have priority of 2; and stations C and E have priorities of 3. So C and E have the highest priorities. Once again, assume that A had already seized the ring and is transmitting data frames. The token has a bit set to indicate that the token is busy. And that means because A has already put a frame in it, it is being sent from A. (Refer slide time: 08:42 ? 09:36)

Station B receives the frame it has data to transmit. Let us say that all of them also transmit some data to station B, which receives the frame. It has data to transmit but it cannot transmit at the moment because the ring is busy. But it places its priority of 2 in a reservation field within the token; it puts 2 over there in that reservation field, and sends the token and the frame sent by A along to C. It then passes the token to C. Station C also determines the ring is busy; it has data to send, so it places 3 in the reservation field, thus displacing the 2, which was inserted by B; 2 gets replaced by 3 in the reservation field, other thing remains as it is. It is still A?s frame, which is moving along. (Refer slide time: 09:36 - )  

Station C then passes the frame to D. D must defer because, if you remember, we had   the priority of 1 to A, 2 to B and D, and 3 to C and E. So it came from A to B, B put a reservation and its priority of 2, then C over wrote this with its priority of 3. Now D sees that there is a priority 3 that is waiting and D has only has its priority of 2; so it has to defer. It cannot do anything. So D must defer; it cannot place its priority of 2 into the field because the priority of 3 is already there. Consequently, it passes the frame to E, which examines the reservation field upon seeing the 3 in the field. It does nothing because since its priority is also 3, E is also a priority of 3, so E cannot do anything. So E simply sends it along. (Refer slide time: 10:39 ? 11:11)

Station A receives the frame back; it makes the ring free by resetting the token and passing the token to B. B is not allowed to use the token because the reservation field inside the token is equal to 3, one higher that the priority of B. Although B wants to transmit and the ring is free, B cannot start really transmitting because somebody with a priority 3 is waiting (Refer slide time: 11:11 ? 12:07)

C is allowed to seize the token because the priority field in the token says 3 and C    has the priority of 3, which means that C is the first node with that level of priority, which has got the token. So this sort of seizes the token; it places the data on the ring and sends the transmission to D. Now D is allowed to place its priority of 2. Although C is sending, C has already put its frame and D sees that naturally the reservation field is reset. Now D can place its priority of 2 into the reservation field. It does so and passes the frame to E. E also wants to send; so E replaces D?s priority of 2 with its priority of 3, and passes the frame to A. (Refer slide time: 12:07 ? 12:25)

A also wants to send again, but A must defer any reservation placement since its priority is 1. B must also forego any priority allocation since its priority is 2. C   receives its transmission back; it is required to make the ring free it does so and transmits the token to D. (Refer slide time: 12:25 ? 12:42)

D is not allowed to seize the ring, since its priority of 2 is less than the reserved priority, which has been put there by C. This is the priority indicator of 3; so it passes the token to E. E seizes the ring because its priority of 3 is equal to or greater than the reservation of 3. This is the way the priority ring works ? if you see, whoever put the reservation earlier at the same level, it come backs to him. So he puts the frame out there, but if the higher priority nodes have finished reservation, transmission, etc., then the lower parity nodes can start transmitting and so on. So you can set these priority levels in the token. This is how not only you can have a pure simple round robin, where everybody has the same priority, but you can have priority based token ring also. (Refer slide time: 13:28 ? 13:49)

There is a variation of this dedicated token ring, which is called dedicated token ring.  There is a central hub; there is a more centralized system, which acts like a switch and it?s more like a full duplex, a point-to-point link and the concentrator acts as frame level repeater, and there is no token passing.(Refer slide time: 13:49 ? 15:09)

Next we will take up another system called FDDI. This is still in use in some places where some specific applications are there, but then again FDDI is also sort of going out because other new technology is taking its place. FDDI was originally conceived as a high-speed network and this network could be used in a LAN, WAN or backbone as high-speed data. When it was conceived, at that time, 100 Mbps was considered very high speed; of course technology has changed, but still it is instructive to look at  these technologies. First of all, we?ll see how different MAC schemes can work, and new MAC schemes for new technology, etc. are also coming up all the time. We will, just as an instructive thing, will look into FDDI in some detail. (Refer slide time: 15:11 ? 16:04)

So FDDI was conceived as a high-speed backbone technology. It has a dual ring topology as just like the SONET rings; we talked about dual ring topologies in the   optical networks. By the way, this is based on fibers. This is fiber distributed data interface that forms the acronym FDDI. It uses dual ring topology, using fiber optic cable used to transmit light pulses. Optical fiber channel operates at a rate of 100 Mbps. Well, we can say 100 Mbps only today, but at one point of time, it was the standard in 1980s, which was proposed to be very high speed. It is frequently used in LANs to connect buildings together. (Refer slide time: 16:04 ? 16:27)

So ring circumference can extend to 200 kms; the distance between nodes can be up to 200 kms. FDDI network can host up to 1000 nodes on one optical fiber. That is how it was conceived. This optical fiber is not just continuous optical fiber. This optical fiber goes from hub to hub, that is, from node to node. (Refer slide time: 16:29 ? 17:54)

 This is an FDDI topology; we have two rings. A is known as the primary ring, which is shown in black. The primary ring is the one, which usually carries all the data, and then, there is a secondary ring, which is used for fault tolerance purpose. As matter of fact, this is one reason FDDI is still used in some places where the reliability of the network is of very high concern. We cannot allow it to remain down for any length of time. FDDI can quickly switch from the primary to the secondary ring. This is a production kind of system as you can see, because there is one ring fully dedicated as a secondary ring, which is there. In case of a fault either of a link or particular node, we can quickly have another ring in its place. (Refer slide time: 17:54 ? 20:14)

The FDDI standard specification came up in the 1980s. This has various parts: one is the media access control, MAC part, which deals with how the medium is accessed, the frame format, token handling, addressing, and error recovery. FDDI has a somewhat more complex MAC protocol because FDDI allows both synchronous as well as asynchronous traffic. If some traffic is synchronous, that means, if it is carrying some kind of voice or something, then you know that is 125 microsecond  length; it is very sacred and sacrosanct over there. Every 125 microseconds, some channel may have to send something, as well as we can have packets or data flowing in the network, may be with some kind of lower priority. So this can handle a mixture of both synchronous and asynchronous traffic. That is a peculiarity of FDDI system, which makes its MAC somewhat more complex than a plain vanilla token ring. So we will see details of this MAC later on. The physical layer protocol defines the data encoding and decoding, how data is encoded and decoded. We will see later the clocking requirements and framing. Under the physical layer, we will see medium characteristics of transmission medium, fiber optic link, power levels, bit error rates, optical components, connectors, etc. that is also a part of the standard for the physical layer medium. There is a standard for the station management, which defines station and ring configurations, initialization, scheduling, collection of statistics, fault isolation, and recovery from faults. As I said, the recovery from faults was and still is a very strong point of FDDI ? one reason FDDI may be preferred for some applications. (Refer slide time: 20:14 ? 20:36)

So, as mentioned earlier, the topology of FDDI network consists of two independent rings ? primary ring A is used for data transmission, while secondary ring B provides an alternative data path; this has already been shown. And secondary ring remains idle, unless primary ring fails. (Refer slide time: 20:36 ? 22:37)   

Optical fiber rings are counter rotating; that means one is moving in one direction while the other is moving in the other direction. Two signal paths are provided, one in each direction. Why do you make the rings counter  rotating? Well, we had seen this in our recovery lecture also; the reason is that if there is a node failure what you can do is that suppose you are a station and there are two counter rotating rings passing through you. That means in one, the signal will pass in this direction and in the other the signal will pass in this direction. In optical fiber, this is quite fixed because you want to have proper transmitter on one side and the receiver on the other side. So one ring is moving like this, the other ring is moving like this. Now what might happen is that suppose the next station has failed and this station understands that there is something wrong either with the link or with the next station. So what it might do is that it might make a quick connection over here so that the ring coming in this direction may take this other path and still we can make one ring. This ring will not have any  fault tolerance; there will be one ring by using part of the two rings A and B. So we can get a recovery through that; that is why we have two counter rotating rings with two signal paths provided, one in each direction. A station is a computer, workstation, or node connected to FDDI network. There are some network nodes also; we will talk about this. Or a station must be connected to both in order to use secondary ring as an alternate data path. If it is connected to only one of them, it cannot use the alternate path. (Refer slide time: 22:37 ? 23:05)

Media is, as I said, 1300 nanometer optical fibers, transmission method is base band; that means, there is no modulation. Only the pulses in the raw form travel down the ring; data rate is 100 Mbps and topology is a physical ring of trees and a logical ring. Why is it a ring of trees where do the trees come from ? we will talk about that. (Refer slide time: 23:05 ? 23:41)

Now let us talk about the type of network stations, which may be connected to an FDDI ring. One is a dual attached station, which is connected to both the rings. That means, it is a station, which is connected to both the rings. That is why it is called dual attached.  We have dual attached concentrator, DAC, which is connected to both rings and provides connection for additional stations and concentrators. It is actually the root of a tree. This is where the tree comes from. (Refer slide time: 23:41 ? 25:33)

We have this picture of an FDDI concentrator, so you can see that this is the main part of concentrator. The two rings are there; They are counter rotating ? this is the primary ring and this is the secondary ring. So the primary ring is coming like this from A to B and secondary ring is going like this. It has some additional ports from which other stations may hang and, actually, what might happen is that we may have a tree hanging from a concentrator. We have a tree of nodes here; so that is why this main FDDI ring may be a ring of trees. The way FDDI is actually deployed is also interesting in the sense that you may have a large ring; that is possible. What is done is that we make a very small ring in the core of the system, like wherever your main server is. We make a very small ring just within a room and what happens is that we have concentrators connected to this ring and from this concentrators, a tree spans out to all the other, may be near by buildings or whatever, so that all of them are connected to this FDDI backbone, but the backbone has got two rings. This backbone is fault tolerant. So that is a good thing about FDDI; that is one way FDDI may be deployed so we have a very small ring and tree is spanning out and going out of the building, may be to other building, and so on. Or alternatively, you can have a large ring also. (Refer slide time: 25:33 ? 25:42)

So we have this dual attached stations and dual attached concentrators; concentrators could be the roots of trees. (Refer slide time: 25:42 ? 26:15)

Dual attached stations and dual attached concentrators are more costly. we have a cheaper variety you think that it is good enough which is a single attached station which is attached only to the primary ring. We have single attached concentrators which is connected to only the primary ring through a tree, a double attached station or concentrator can reconfigure the dual ring as mentioned earlier into a single ring in the event of a failure. (Refer slide time: 26:15 ? 27:03)

What are the physical interfaces like ? as opposed to a basic token ring network, in which at any instant there is a single active ring monitor, which supplies the master clock for the ring, in FDDI, this approach is not suitable because of the high data rates. That is one thing; and the other thing is that the ring could be very large. So if the ring is quite large then having the central clock becomes difficult. Each interface has its own local clock and the outgoing data are transmitted using this clock. (Refer slide time: 27:03 ? 27:43)

All data to be transmitted are encoded, as I mentioned earlier, prior to transmission using a 4 of 5 group code, which means there are nearly 32 possibilities. So, for 4 bits of data, we actually have 5 bits, which are going over there. The additional capacity is used in some other way for control purpose; that we will see later. This means, for each 4 bits of data, a corresponding 5 bit code word or symbol is generated by the encoder. Some of these symbols or combinations are used for link control functions. (Refer slide time: 27:43 ? 29:13)

Now let us go through the ring operation ? there are two aspects to it. One aspect is similar to the token ring, which we have already discussed. The sending station waits for a token; sending station captures and strips token and then transmits frames; sending station issues token at the end of transmission. Now this is one point where the FDDI is different from a token ring. In a token ring, if you remember, only when the  transmitted frame with that busy ring etc., comes all the way back to the sender, the sender makes this token free and puts it back on the ring. Since FDDI was perceived as a high-speed ring, what was proposed was that as soon as its transmission of its frame is over, it can put a new token on the ring. Multiple frames may be circulating in the ring at the same time bringing up the speed. So sending station issues token at the end of transmission, destination station copies the transmitted frame and sets the A and C, which is the address recognized by the frame copied indicators. That means it has already copied the frame like what we have in a token ring. (Refer slide time: 29:13 ? 29:46)

The sending station removes the data from the ring by stripping the sent and acknowledged frame, etc. So it takes out the frame; the first few bytes of the frame are not stripped ? this is for some technical reason, we need not go into the details here ? and continue to circulate on the ring as a fragment. Each repeating station  strips 1 byte from the fragment and the transmitting station completely strips it. So there are some fragments also apart from the frames; some fragments are also moving around in the ring. (Refer slide time: 29:46 ? 30:52)  

Now we come to token passing scheme. It uses token passing protocol to move data around; the ring uses another protocol based on timers. We will look at this protocol later on. Timing is very critical to token passing scheme, as it is designed for delay sensitive synchronous data. As I mentioned earlier, the FDDI ring carries a mixture of data. It may carry lower priority packet data kind of thing, which we have been talking about, in the token ring. It may also carry synchronous data, which is time sensitive and which has somewhat higher priority than this other one. This is based on some timing protocol. We will go into timing protocol now. FDDI allows for high data rates, where each ring interface has its own clock. All outgoing data are transmitted using this clock. (Refer slide time: 30:52 ? 31:15)

A node will get packets within a specified amount of time. We are discussing the  timing part of it. A node will get packets within a specified amount of time. As a packet circles the ring with a token behind, each station retimes and regenerates the packets. (Refer slide time: 31:15 ? 31:35)

So this increases probability frame fragments, which will be propagated on the ring ? how fragments are eliminated. Early token release is required because of the high speed and extensive distance provided by FDDI. (Refer slide time: 31:35 ? 32:00)

FDDI rotation time: FDDI uses time to ensure equal access to the ring; measures rotation time by calculating distance of segments, processing time, and number of stations. This is the time you expect a packet to move around the entire ring. Rotation time refers to how it takes a signal to propagate around the ring. (Refer slide time: 32:00 ? 33:36)

So rotation time is used to control the priority operation of FDDI ring. We have several timings: one is measured by the clock that times the period between the receipt of tokens called the token rotation time, that means, how long is it that the token takes to come around the ring. The operation of MAC layer is governed by a MAC receiver and is calculated by target token rotation timer. That means there is a target rotation time, which is  prefixed and there is a TRT, which is measured. Usually you would expect under the normal conditions, when the load is moderate, the token rotation time would be less than the TTRT. When the node is moderately noted by comparing TRT with TTRT, we can find out how loaded the system is. If you have a synchronous link going through the synchronous traffic that is going through this FDDI ring, the synchronous traffic will have to be given the first priority and the asynchronous traffic of some lower priority data traffic, will be put on the ring or will not be put on the ring, depending on how loaded it is and this is how it is calculated. (Refer slide time: 33:36 ? 33:56)

There is a pre-negotiated target time called PTT. PTT is coordinated for the arrival of a transmission. Each node measures time it takes for the token to return to it; that is the TRT. It compares time to a pre-negotiated target time PTT for its arrival. (Refer slide time: 34:01 ? 34:36)

A node is allowed to transmit as long as its full transmission stream does not exceed the PTT. So there is a pre-negotiated target time, which is allowed, and the node is allowed to transmit as long as its full transmission stream does not exceed the PTT. If the token comes back sooner than PTT threshold, it is deemed as a light network load. If the token comes back later than PTT, it indicates the heavy traffic load. Low priority traffic must then be deferred until load on the network becomes lighter. (Refer slide time: 34:36 ? 35:26)

There is a token holding time, THT. If you look at the last point THT is actually equal to TTRT minus TRT. It is used to calculate maximum length of time a station can hold the token to initiate asynchronous transmissions. The point is that there is a target time and there is an actual measured time, by which the token has come back. If the actual measured time is low, that means the network is lightly loaded, you can put some asynchronous traffic. That is why THT is calculated. It calculates the difference between the arrival of the token and the TTRT. It keeps track of the amount of time a host can transmit. This is the formula, and then you have the following rules (Refer slide time: 35:26 ? 36:21)

If THT is less than zero that means traffic is heavy, the total rotation time is actually  more than the expected time, which was expected earlier. This means that all asynchronous traffic has to wait. The synchronous traffic will go through. So if the THT is less than zero, it is a heavily loaded station. Stations can only transmit synchronous traffic. If THT is greater than zero stations can transmit both synchronous and asynchronous traffic during THT. So it first sends the synchronous traffic and then sends the asynchronous traffic till THT falls to zero. If THT is equal to zero, the host cannot start any new packet. THT increases and number of stations  decreases.(Refer slide time: 36:21 ? 39:59)

The FDDI frame format: FDDI is a technology, which is not moving forward very much these days. As the matter of fact, it may be slowly on its way out because we have other ways of achieving the main point of FDDI, which is its fault tolerance.  The speed of FDDI has become 100 Mbps, which is rather not very fast as far as the backbone is concerned; it is taken as a very low speed these days. So FDDI may be on its way out. It is also expensive and the support to it is also dwindling. The reason we are looking at how different issues are handled by a typical MAC protocol is that this business about framing is common to all kinds of data link protocol. So later on, when we talk about Ethernet, which is the most common kind of network in the world today, we will see that it has some frame format. And if you remember our first day?s discussion, when we were talking about these different layers, we said that they are at the same level ? that means a network layer to network layer; network layer in this node to network layer in this node; similarly the transmission layer in this node to transmission layer in that node, etc. ? and have some protocol running. How do these protocols run? These protocols run by adding some header and in some cases a trailer also to the main payload. So whatever it gets from the upper layer is the payload to it for running its own protocol. It adds some header; that means, adds some information to the beginning of the frame and adds some information to the end of the frame to make a complete frame. The corresponding layer in the other node strips this particular information, does whatever it has to do, because it is also running the same protocol. So it knows what to do and then may be either it goes up or it goes down again to the next station and so on. This is an example frame format of FDDI. FDDI frame format has some preamble. We will not going to the details of this because this is not very important any more, but we will just mention that such fields are common in many frames. We have a start delimiter; we have a frame control; and we have a destination address. This has to be there because otherwise the destination will not know that this particular packet is meant for him. We have a source address field, so we have a DA and then an SA field; we have the data, frame check sequence for some error control. By the way, error is not handled by FDDI; error allows other layers to handle the error if there is an error. But we have to check the error; so there is frame check sequence, end delimiter, and frame status. (Refer slide time: 39:59 ? 40:30)

FDDI encodes all data prior to transmission, uses a 4 or 5 group code method, which was mentioned earlier. The encoder generates a corresponding 5 bit word or symbol. For every 4 bits transmitted, FDDI creates a 5 bit code. Bits provide clocking for the signal itself. The status of bit reflects a change of state of the light on the other side. (Refer slide time: 40:30 ? 41:57)

Symbols are light; taken with another symbol they form one byte. So there are 16 data symbols issued with 5 bits. You can have 32 different symbols; out of it, 16 data symbols are reserved for data. This is for 0 to F. so if you write your scheme of bytes in hex, that means in groups of 4 bits each, each of these 4 bits has got its corresponding code in the FDDI symbol. And then, we have 16 other symbols, which are left; so 8 are used as control symbols and 8 as violation symbols. The control symbols are called Q, H, I, J, K, etc. Coding the symbols prevents the occurrence of 4 consecutive 0s in a row. This is necessary to ensure each station?s clock is in sync with other stations for the transition that takes place. When you go from a 0 to 1,  that transition?s edge is used for synchronizing the clock. This is the very common method of synchronization; that is why we have the codes in such a manner, so that we do not have a long string of 0s because then the synchronization might drift.  (Refer slide time: 41:57 ? 42:16)

Token has the following fields: we have a preamble; we have a start delimiter; we have a frame control; and we have an ending delimiter. As I said, one of the main points of  FDDI is the (Refer slide time: 42:26 ? 44:14)

fault tolerance that it provides. There is a ring wrap. I mentioned this earlier that in a dual attached concentrator, if it senses that on the other side the node has failed, it can make a connection between the primary and the secondary ring within itself, so that the ring, while coming like this, starts traveling like this and completes its round. That is because if you have the node on the other side, the corresponding concentrator will also make a connection between the primary and the secondary ring. The failed node is essentially isolated on both the sides. There are connections within the concentrator, and instead of a single ring, you have two rings. Now the primary and secondary ring are fused together and then, suppose this was the primary ring coming, then it goes back to the secondary ring and then to the other concentrator on the other side and the connection is completed. You have one ring; that is why the physical diameter of an FDDI ring is kept within 100 miles. If there is a failure, the total ring diameter does not exceed 200 miles, which is the standard. Ring wrap technique is the technique that we mentioned earlier when we talked about protection and restoration; the technique is used to manage failures. When a station fails or a cable is damaged, dual ring is automatically wrapped. Two adjacent ports connecting to a broken link will be removed from the ring and both stations enter wrap state. (Refer slide time: 44:14 ? 44:33)

FDDI concentrator switches to a wrap state and the ring is doubled back onto itself. Data continue to be transmitted on FDDI single ring. Performance is not negatively impacted during wrap state. So this is the good thing about FDDI. When you have very mission-critical situations, FDDI is kept as a strength for very critical situations like, may be, stock exchange or may be some thing else, where even few seconds of network down time is not acceptable to anybody. So there you could deploy this kind of technology with its inherent fault tolerance. So this is the another picture of FDDI ring wrap.  (Refer slide time: 45:13 -  45:33)

As shown over here, you have one station 4, which has gone down so that 2 adjacent stations, namely, 2 and 3 wrap around and we have a single ring now going around. So this is clear. (Refer slide time: 45:33 ? 47:40)

We have discussed earlier how this is done. We have optical bypass switches used for two or more failures to occur. By the way, what would happen if more failures occur? Suppose there was a one single failure, one single node failure, and the two rings primary and secondary and the two adjacent concentrators were wrapped back and we had a single ring like this, what would happen if another node fails in-between? What would happen is its adjacent concentrators once again wrap around. But now, instead of only one single ring, which we had earlier in the case of a single failure, now with this double failure we may have two rings. These two rings are not connected to each other. So these two rings individually would still keep working. There is a part of the protocol, which I have not covered. It is that if there is a single token, which was on the other side, and now the token is lost; there is no token. There is a protocol for reclaiming and regenerating a token. Now what would happen is that, two individual rings will come into operation on the two sides, but they will not be able to do so. The nodes, which are connecting to this sub-ring and the nodes, which are connected to the other sub-ring, will not be able to communicate with each other across, but within themselves, they will very well communicate as usual. So rings are segmented back into two independent rings incapable of communicating with each other. Additional failures can cause further ring segmentation. Optical bypass switches can eliminate failed stations to prevent ring segmentation. We have all seen how this is done ? actually this is all done with mirrors. (Refer slide time: 47:45 - )

So optical bypass switch has optional optical mirrors that pass light from ring directly to DAS station during normal operation. DAS station experiences a power loss; optical bypass switch will pass the light through itself. It uses internal mirrors to maintain ring integrity. (Refer slide time: 48:04 ? 48:54)

The other technique of this protection, etc., which we had seen earlier is dual homing. This dual homing is also used in an FDDI context; a router or a DAS ? DAS you remember is a dual attached station ? is connected to two concentrator ports on FDDI concentrator. One port provides a connection to active fiber link, while the other port is in hot standby mode. Actually there are two nodes, A and B; usually the A node is in hot standby mode and B node is operating. So the port is in hot standby or passive mode; hot standby is constantly tested and will take over if the primary link fails. (Refer slide time: 48:54 ? 49:08)

A typical DAS configuration has a B port; it is designated as active port; and a port is configured as the hot standby. When the primary link fails, passive link automatically activates and hot standby becomes operational. So this is the same dual homing principle, which we had seen earlier when we were talking about recovery and protection. With this, we come to an end of the discussion about token bus and token ring and FDDI. We have discussed the number of these token-based protocols, namely, token bus, DQDB, token ring, and FDDI. The FDDI was quite good. Only thing was that FDDI was also quite costly. These technologies are actually going out in some sense. If you remember, when we were talking about DQDB, we said it can handle 53 byte cells; that came down to ATM and ATM is that sort of the technology that is still quite alive today. We will have an extensive discussion about ATM. Regarding the WAN technology, two other technologies, which are once again on their way out, are X 25 and frame relay. Well, frame relay is still there in many parts; X 25 is sort of going on but still we will just have a quick look at these. Then we have to consider these MAC levels. This is how the next set of lectures will go. What will happen is that once we finish this, then we will talk about the data link layer and specifically about the MAC sub-layer of the data link layer. Then we will have to talk about the LLC, that is, the logical link control. So we will discuss that and the other  important functionality of this data link layer, which is the error and flow controls.  Because whenever you have a transmission, somehow you have to assume that errors may occur. Depending on the medium and the technology, errors may be more or less frequent; for example, in a fiber, the error may be very low, error probability may be very low. When you are using wireless, the error probability is high. But, anyway, you have to consider the possibility of some error and how errors are handled. That means, we are talking about bit errors, which may come in due to noise and other things into the data in a particular link. That is one thing. And if there is a flow control to be done, that means, if there is a congestion or not because you are sending from one side whether the other side is receiving it or not, that you have to some how make out. We will take up these things next. Thank you. 
Preview
COMPUTER NETWORKS
Prof. Sujoy Ghosh
Dept of Computer Science & Engineering 
IIT Kharagpur 
Lecture - 15
Data Link Protocols 
(Refer slide time: 52:17 ? 52:18)

(Refer slide time: 52:18 ? 52:20)

Good day so today we will start  our discussion on data link layer as a matter of fact we have discussed a part of data link layer namely  the Mac sub layer(Refer slide time:52:30 ? 53:16)

we will see how that all fix in but to fit it into the broader picture if you remember when we were discussing  the seven layer OSI protocol starting from the application layer the bottom most layer was the physical layer  so we have finished our discussion on physical layer and just above the physical layer we have the data link layer so  we will look at the different components of data link layer and how they will be used different protocols etc  so that is what we will do  so our this main thing is data link protocols which are the protocols which are used in the data link layer (Refer slide time: 53:16 ? 54:03)

now what are the main tasks of data link layer it transfers data from the network layer of one machine to the network layer of another machine so  actually this is the part of the service it gives to the upper layer do  you remember that above the data link layer we have the network layer  so below the network layer we have the data link layer so the data link layer gives some service to the network layer and this service is the transfer of data from one network layer to another network layer so that is the service it gives it to the network layer and this in its turn uses the physical layer so it converts raw bit streams of the physical layer into groups of bits etc  or frames (Refer slide time: 54:03 ? 55:07)

so this is how we can look at it  so this is one node and this is another node above this   there may be other layers we are not concerned about this upper layers at the moment so this gets  some data to be sent from the network layer and this data link layer sends it to the next data link  layer remember once again that the network layer is concerned with transfer of data etc  across the network that means it may take several such hubs but  data link layer is just concerned with the single hub so this is how we simply the problem the problem of going  multi  hub  so this multi hub part we leave it to the network layer for this single hub so multi hub will naturally constitute the number of such single hubs and data link layer would  handle  the transfer of data from one  from one node to the next (Refer slide time: 55:08 ? 56:34)

so what are the kinds of services types of services that  the data link layer gives one is unacknowledged  connectionless so no attempt  to recover lost frame if some frame is lost due to noise error etc etc  there is no attempt to recover this and because there is no acknowledgement from the other side it is a connectionless system suited for low error rate network or for fault tolerant applications such as voice  what you mean that voice is a fault tolerant application  we mean that even if some of the bits in a voice stream digitized voice stream that is even if some of bits  drop obviously there will be some degradation on the other side if your are do not doing any kind of correction etc but to the human ear it may not be very perceptible  for example I am talking even if there is a momentary glitch you will more or less make out what I am talking about  so that is why in that sense it is more inherently fault tolerant so that is un acknowledgement unacknowledged connectionless service that is one kind of service acknowledged connectionless service this is another kind of service so each frame is acknowledged by the receiver so this is suited for unreliable channel  so require this acknowledgement for the special reliability  (Refer slide time: 56:36 ? 56:49)

 acknowledged connection oriented service ensures that all  frames are received  and each is received exactly once  and these services are accomplished using as I said simplex not usual but half duplex or full duplex channels(Refer slide time: 56:49 ? 57:33)

so this is some examples not very important  sorry so is a reliable message stream it may be connection oriented service or it may connection less service and it may be a reliable message stream or reliable byte stream  so reliable message stream sequence of page reliable byte stream they say remote login so they are coming byte by byte here it is coming page by page unreliable connection like digital voice unreliable datagram so these are but when you come to datagram this becomes connectionless service unreliable datagram acknowledged datagram request reply etc (Refer slide time: 57:32 ? 57:40)

now  let us look at just one thing  that where does this all these data link layer exact exactly where does it exist physical medium we understand it is a cable or it is this  electromagnetic field this free space etc  or fiber so we can see it we can fell it but where does the data link layer resides so to say(Refer slide time: 58:00 ? 58:25)

now frames could be fixed length like ATM  so ATM cells are of fixed length so you know once you have synchronized you know that  they are going to come with  fifty three byte kind of  regularity but frames could be variable length also in which case we use this byte count byte stuffing bit stuffing generic framing procedure Manchester encoding etc  
COMPUTER NETWORKS
Prof. Sujoy Ghosh
Dept of Computer Science and Engineering
I .I .T Kharagpur
Lecturer Name # 15
Data Link Protocols
(Start time: 00:45)

(Slide time: 00:53 - 00:54)
Good day. Today we will start our discussion on Data Link layer. As a matter of fact, we have already discussed a part of the data link layer, namely, the MAC sublink layer. We will see how it all fits in. To fit it into the broader picture, if you remember when we discussing the 7-layer OSI protocol, starting from the application layer the bottom most layer was the physical layer. Just above the physical layer we have the data link layer. We will see the different components of data link layer and how they are used, we will discuss different protocols, etc. 

(Slide time: 01:33 - 01:38)

These are protocols which are used in the data link layer.


      (Slide time: 01:39 - 02:27)
The main task of the data link layer is that it transfers data from the network layer of one machine to the network layer of another machine. This is a part of the services it gives to the upper layer. If you remember, above the data link layer, we have the network layer. The data link layer gives a service to the network layer, and this service is the transfer of data from one network layer to the other, and this in turn uses the physical layer. It converts the raw bit stream of the physical layer into groups of bits or frames.

(Slide time: 02:28 - 03:33)
This is how we can look at it: this is one node and this is another node. Above this is there could be other layers; we are not concerned with these at the moment. This gets some data to be sent from the network layer and the data link layer sends it to the next data link layer. Remember that the network layer is concerned with transfer of data, etc., across the network. That means it may take several such hops; but data link layer is concerned with just a single hop. This is how we simplify the problem of multi- hop; this we will leave to the network layer for the single hop. So, multi-hop will constitute several such hops and data link layer will handle transfer of data from one hop to the next hop. This is another way of looking at it. 

      (Slide time: 03:34 - 03:51)
There is a virtual data path from layer three to layer three, but actually this goes through the data link layer and this is the actual data path, which goes through the physical layer and some physical transmission medium. 

(Slide time: 03: 52 - 06:06)
Now, why do we require any controls in the main data link? These are the main functions of the data link layer. Actually, all the combinations are not used in all the situations, but these are the general categorizations of data link. The frame synchronization: the beginning and end of a data block, that means, a frame, should be recognizable. That means when you are sending number of bytes from one machine to other, they are formed into blocks. At that time, the other machine should be able to recognize the beginning and end of the block. So you have to organize these bytes into frames. That is the first job of the data link layer. Second job of data link layer that it might or might not do is flow control. The sender should not send frames at a rate faster than the receiver can receive and process them, because the sender may not know everything about the state of the receiver at that particular point of time. If it goes on sending data, the receiver may be forced to drop some of the data. In some cases, this is ignored. But in some cases, it is relevant and the data link layer may do some floor control. The third thing is error control, where any bit errors introduced by the transmission system should be corrected. Whenever you are sending something through a transmission medium there is always a chance of some error, and the data link layer has to take care of this error.

      (Slide time: 06: 24 - 08:00)
Addressing: on a multipoint link like LAN the identity of sender and receiver must be specified. In such a case, the identity of the receiver would be very important because in many such links, especially in broadcast links, what happens is that the data reaches everybody. The receiving station must know whether or not one of these frames is meant for him. The addressing of receiver is important for that reason. For all the above points, we require some data link layer protocol to run between the two layers. In order for this protocol to run, these two nodes need to have some kind of control or management information. The control and management information will be transmitted on the same medium. Control and data flow on the same link. The receiver must be able to distinguish control information from the data being transmitted. We also require link management ? the procedures for the management of the initiation, maintenance and termination of a sustained data exchange. 

      (Slide time: 08:08 - 09:22)
These are the different functions of the data link layer. The link layer services are framing and link access. It encapsulates datagram into frame, adding header or trailer. What the header or trailer contains depends on the protocol that you are using on that link; some do not have a trailer, some do not have header; some have both header and trailer. This is the part that is used for the protocol to run. Channel access is also there if it is a shared medium. MAC protocol is a part of it; if it is a shared medium, the medium access control has to be used. MAC addresses are used in frame headers to identify source and destination; it is different from another address we have over the entire network, called the IP address. Right now, we will be concentrating on MAC addresses.


(Slide time: 09:23 - 10:58)
Link layer services offer reliable delivery between adjacent nodes. If the medium is prone to noise or introduction to errors, the data link layer has to address the problem. Apart from this link, you may require resilience at a higher link because a particular node of the network link might fail. So, whatever protocol you might have for the data link might not work, so the chain may break down. This is for making individual links of the chain as good as possible. This is one of its jobs ? reliable data delivery. Just as fiber may have low error bit, similarly, some links like wireless links have high error rates. We have just discussed why it gives both link-level and end-end reliability. 


      (Slide time: 10:59 - 11:34)
The flow control is pacing between the adjacent sending and receiving nodes. They should always be in sync, whatever the sending station sends, the receiving station should be able to absorb that much. For that, you may require some explicit controls sometimes. We will look at floor control later on, in a later lecture. The other service is error detection, which includes errors caused by signal attenuation and noise. Receiver detects presence of errors; that is the error detection part. Once you detect that there has been some error, you may ask for some retransmission from the other side so that you may get the correct data, or you might be able to look at that faulty data and correct it locally. The receiver identifies and corrects bit errors without resorting to retransmission. All these would happen on the line, and the line could be simplex, half-duplex or full-duplex. Simplex is not very common, because simplex can go only in one direction.  

      (Slide time: 12:16 -12:46)
In half-duplex nodes at both ends of link can transmit, but not at the same time.

      (Slide time: 12:47 - 14:16)
DLL offers unacknowledged connectionless and acknowledged connectionless services. In unacknowledged connectionless, there is no attempt to recover lost frame and there is no acknowledgement from the other side. It is suited for low error rate networks or for fault tolerant applications such as voice. By voice tolerant application, we mean that even if some of the bits in a digitized voice stream drop, there will be some degradation on the other side. But to the human ear, it is imperceptible. That is why it is fault-tolerant. In acknowledged connectionless service, each frame is acknowledged by the receiver and it is suited for unreliable channels, where acknowledgement is required for special reliability. 

      (Slide time: 14:17 - 14:31)
Acknowledged connection-oriented service ensures that all frames are received and each is received exactly once and these services are accomplished using simplex not the usual, but half-duplex or full-duplex channels.

      (Slide time: 14:32 - 14:36)	
      
These are some examples. It is a reliable message stream. It may be connection-oriented service or connectionless service. It may be a reliable message stream (sequence of pages) or reliable byte stream	(reliable login): in the latter it is coming byte by byte and in the former, it is page by page. An example of unreliable connection is digitized voice; unreliable datagram (electronic junk mail) is connectionless service.

      (Slide time: 14:37 - 14:38)  
The data layer link exists in the network interface card. If you have a PC that is connected to a network, it is probably connected through an Ethernet card, or Network Interface Card (NIC) or something like that. The card has a socket where the Ethernet cable will be plugged in. So, there is a network adaptor. The adaptors implement most of these data link functions, so it is the adaptors that are communicating. The datagram from a higher layer is made into a frame by the adaptor and it is then sent, following the link layer protocol, to the other node. The adaptors communicating have two nodes: one sending node and one receiving node. 


      (Slide time: 14:39 -15:14)

	[WRONG SLIDE?]
The link layer in adaptor is also called NIC, examples of which are the Ethernet card, PCMCI card or the 802.11 card. On the sending side, the adaptor encapsulates the datagram in a frame. It adds error-checking bits, rdt, flow control, etc. On the receiving side, it looks for errors, rdt, flow control, etc., extracts datagram and passes to the receiving node. The adaptor is semi-autonomous and communicates directly with link and physical layers. The adaptor has some hardware and some in-built software. 

(Slide time: 15:15 - 16:48) [WRONG SLIDE?]

(Slide time: 16:49 -17:19)

(Slide time: 17:20 -17:45)

(Slide time: 17:59 - 18:42)
The data link layer is divided into two parts. One is the MAC and the other is the LLC. So in any broadcast network, the stations must ensure that only one station transmits at a time on the shared communication channel. That is the MAC part of it. The protocol that determines who can transmit on a broadcast channel is called Medium Access Control (MAC) protocol. We have seen a number of MAC protocols already.

(Slide time: 18:43 - 19:26)
The data link layer is divided into two sublayers. Above the data link layer, you see two network layers and below it is the physical layer. The data link layer itself is divided into two parts: the medium access control part, which is closer to the physical layer, and the logical link control. The MAC protocols are implemented in the MAC sublayer, which is the lower sublayer of the data link layer. The higher portion of the data link layer is often called logical link control or LLC. 

(Slide time: 19:27 - 20:20)
This is the broad picture: we have been referring to some numbers like 802.1, 2, 3, 4 etc. So IEEE 802 is a family of standards for LANs, which define an LLC and several MAC sublayers. So 802 encompass all these. This 802.2 is the LLC part and below the 802.2 there are various kinds of medium access controls. There are 802.3, 4, 5, 6 and then 10, 11, 12 etc., and now we have 15, 16, etc. Above the data link layer, there is a higher layer and below that, there is the physical layer.

(Slide time: 20:21 - 21:05)
802.1 gives you an overview; 802.2 is the LLC we will be talking about today. 802.3 is the famous Ethernet; CSMA/CD is the kind of MAC protocol that it uses. 802.4 is the token bus, which we have already seen. 802.5 is the token ring. 802.6 is the distributed queue dual bus. FDDI is the fiber distributed data interface and there are others. As new protocols come up, they keep adding to this list.

(Slide time: 21:06 - 21:53)
LLC, whatever it does, it requires some headers. So when the packet is coming from the network layer, the LLC header is added to the packet. So it will reach the LLC sublayer on the other side. Then it comes to the MAC sublayer and MAC sublayer will add its header and it may add some trailer also. The MAC, LLC, and original packet may constitute one frame and it is pushed on to the physical layer in the network.


(Slide time: 22:01 - 22:18)
The 802 LANs offer the best effort data frame services. Error control and flow control are handled by LLC. LLC runs on all the three 802 LANs and hides the differences to the network layer. 



(Slide time: 23:04 - 23:17)
The different physical layer protocols are transparent to the network. All that the data link network knows is that this network layer will provide a reliable service for sending the data from this node to the next. LLC adds its header to the network layer packet. It contains sequence and acknowledgment numbers. Resulting structure goes into the payload of 802.x frame for transmission. 


(Slide time: 23:18 - 23:46)
LLC operations are sometimes divided in this fashion. Type 1 operation supports un acknowledgement connectionless service. Type 2 operation supports connection mode service. Type 3 operation supports acknowledged connectionless service. 

(Slide time: 23:47 ? 24:31)
The LLC has protocol data units or PDU, which carries user information. The control field includes a 7-bit sequence number N(S), associated with this PDU. It also includes a piggybacking acknowledgment sequence number N(R). Unnumbered various protocol control PDUs.  These five bit M fields indicates a what kind of PDU it is. 

(Slide time: 24:32 - 24:52)
There are some supervisory PDUs used for flow and error control. It includes an acknowledgment and sequence number and a 2-bit S field to distinguish three different PDUs: receive ready (RR), receive not ready (RNR) and reject (REJ). 


(Slide time: 24:53 - 25:19)
The type 1 operation supports the unacknowledged connectionless service. The UI PDU is used to transfer user data. There is not acknowledgement, flow control or error control. The XID and TEST PDUs support management functions associated with all the three types of operation. 

(Slide time: 25:20 - 25:32)
An LLC entity may issue a command XID or TEST. The receiving LLC entity issues a corresponding XID or TEST in response.  

(Slide time: 26:00 - 26:08)
Type 2 operations involve three phases: connection establishment, data transfer and connection termination. 

(Slide time: 26:09 - 26:45)
With type 3 operation, each PDU transmitted is acknowledged. A new unnumbered PDU, the acknowledged connectionless (AC) information PDU, is defined. User data are sent in AC command PDUs and must be acknowledged using an AC response PDU. 

(Slide time: 26:46 - 27:45)
To guard against lost PDUs a 1-bit sequence number is used. The sender alternates the use of 0 and 1 in this 1 bit. The receiver responds with an AC PDU with opposite number of the corresponding command. Only one PDU in each direction may be outstanding at any time. 


(Slide time: 27:46 - 29:16)
Frame Synchronization: Two sides must be able to synchronize their movements. This synchronization is of two types. Suppose you are sending data 1 byte at a time, or you are sending blocks, each block containing a number of bytes. Synchronization of frames is necessary for this. When data are transferred from the transmitted to the receiver unless steps are taken to provide synchronization the receiver may start interpreting the data erroneously. Suppose you have taken the second byte as the first byte, you will never be able to know that it is not the first byte. There are two common approaches: Asynchronous Transmission and Synchronous Transmission. 

(Slide time: 29:17 - 30:45)
In asynchronous transmission, data are transmitted one character at a time. Timing of synchronization must only be maintained within each character. The receiver has the opportunity to resynchronize at the beginning of each new character. If you use encoding, you can use the transition time for synchronization between the sending and receiving nodes.

(Slide time: 30:11 - 30:41)
So when no character is being transmitted the line between transmitter and receiver is in idle state; so there must be some start and some stop. There is a start after which we can introduce some parity bits ? we will see later how parity bits are introduced ? and this is the stop.

(Slide time: 30:42 - 31:46)
In synchronous transmission, a block of bits is transmitted in a steady stream without start and stop codes. Actually asynchronous transmission is not as efficient. The block may be arbitrarily long. To prevent timing drift between the transmitter and receiver, clock signal is embedded in the data signal. (E.g. Manchester encoding) 

(Slide time: 31:47 - 32:35)
Apart from this synchronization of clock for the bit, you require another level of synchronization, so as to allow the receiver to determine the beginning and end of a block of data. Every block begins with a preamble bit pattern, and generally ends with a postamble bit pattern. The kind of preamble and postamble that are used is directly related to the kind of protocol that is used.

(Slide time: 32:36 - 33:55)
There are many kinds of framing available. You can observe that only the body is coming from the higher layer; the rest of it is being added in this layer. For example, DECNET?s DDCMP frame has SYNs, header, body and many other bits are there. The ATM cell has only the header, CRC and the body. IBMs have a BISYNC frame, header, body and bits. The ARPANET?s IMP-IMP frame has SYN, header and body bits. The ISO?s HDLC frame looks like this: header pattern, body and CRC, and again some specific pattern. 

(Slide time: 33:56 - 34:15)
A typical synchronous frame format would have an 8-bit flag, which would be the preamble, control fields, data field and some more control fields and an 8-bit flag (post amble). 


(Slide time: 34:16 - 34:42)
For sizeable blocks of data, synchronous transmission is far more efficient than asynchronous mode. Asynchronous transmission requires 20% or more of overhead.  The control information preamble and postamble in synchronous transmission are typically less than 100 bits. The overhead is low here that is why the efficiency of synchronous transmission is very high. So when you are sending large amount of data, you go for synchronous transmission, because it is efficient.

(Slide time: 34:54 - 35:34)
Framing translates the physical layer?s raw bit stream into discrete units called frames. The sender sends the message, which is transmitted in the form of frames. N frames are on transit and they are going from the sender to the receiver. Now how can the receiver detect frame boundaries? That is, how can the receiver recognize the start and end of a frame? This is done by four methods: length count, bit stuffing, character or bit stuffing and pulse encoding. We will look at some of these now.

(Slide time: 35:35 - 36:04)
Frames could be of fixed length, like ATM. When it is ATM, you know that it is of 53-byte kind of regularity. They could be of variable length also, in which case we use the byte count, byte stuffing, bit stuffing, generic framing procedure and Manchester encoding. ATM is a kind of fixed length frame. Variable lengths are byte count (DECNET), byte stuffing (SDLC), bit stuffing (HDLC), generic framing procedure, and Manchester encoding (802.5). 

(Slide time: 36:05 - 36:53)
Now in framing, we make the first field in the frame?s header as the length of the frame. That way the receiver knows how big the current frame is and can determine when the next frame ends. From the above slide, we can see that frame 1 contains 5 characters, frame 2 contains 5 characters, frame 3 contains 8 characters and frame 4 contains 8 characters.

(Slide time: 36:54 - 37:55)
Here the disadvantage is that the receiver loses synchronization, when bits become garbled. If the bits in the count become corrupted during transmission, the receiver will think that the frame contains fewer (or more) bits than it actually does. 

(Slide time: 37:56 - 38:17)
Checksum will detect the incorrect frames; the receiver will have difficulty resynchronizing to the start of a new frame. This technique is not used anymore, since better techniques are available. 


(Slide time: 38:18 - 39:04)
One of the better techniques is known as bit stuffing. Use reserved bit patterns to indicate the start and end of a frame. For instance, use the 4-bit sequence of 0111 to delimit consecutive frames. A frame consists of everything between two delimiters. So you have this one delimiter on one side, 0111, and then the frame and then the 011. So as soon as you get 011, you know that the frame is starting and as soon as you get another 011, you know that the frame has ended. So this way we can know the beginning or the end of the frame. 

(Slide time: 39:05 - 41:53)
The problem with this is as follows: what happens if the reserved delimiter happens to appear in the frame itself? If we do not remove it from the data, the receiver will think that the incoming frame is actually two smaller frames. Suppose we have the 0111 as the delimiter and the delimiter may contain data that came from the user in any bit pattern. You have to allow any bit pattern to the user. It could be that a picture is being sent in several bits and the bit pattern may be arbitrary. In that case, 0111 may appear in the body of the data; this is where the bit stuffing part comes. We introduce a new set of pattern, say, 0111, for each existing pattern. So now, the solution is to use bit stuffing. Within the frame, after every occurrence of two consecutive 1s, insert a 0. For example, append a 0 bit after each pair of 1s in the data. This prevents three consecutive 1s from ever appearing in the frame. 

(Slide time: 41:54 - 43:25)
Similar to bit stuffing we may have byte stuffing. For example, let us say a flag say some character is there. So, say the flag, which is also a part of the regular header, just happens to appear in the body of the frame or body of the packet. So what we do is that, we use this other character. These are character introductions, it is called byte stuffing. One byte is one character. So we introduce the character, escape character, just before the flag, and what happens if escape itself appears in the body? Well, we put escape. So if there are two escapes side by side we know that we have to interpret it as only one escape. If there are two escapes in the original data packet, just by chance, then actually this will be center 4 escapes and just one after the other, and at the receiving side, for every 2 escapes, it will reduce it to one escape and know that this is the just a part of the data. Only at the end, we will get flag, etc., bytes. If there is escape flag, we have escape escape escape flag escape escape and so on. So this is known as byte stuffing. 

(Slide time: 43:26 - 43:57)
      Point to point protocol.

(Slide time: 43:58 - 44:40)
We will now discuss point-to-point protocol. There will be a flag field, address field, control field, protocol field, and payload field. This payload is the one which is actually coming from the higher layers. That means from the network layer, some of it will actually come from the user, from the application layer itself. This is the payload so far as the data link layer is concerned; it ends with Checksum and then flag. This is what the general things look like and we will come to discussing how they are used.  

(Slide time: 44:41 - 46:09)
In a point to point data link control, there is one sender, one receiver and one link, which is easier than broadcast link. It has no media access control, no need for explicit MAC addressing e.g. dial-up link and ISDN line. The popular point to point DLC protocols are PPP, which is point to point protocol, and HDLC, which is high level data link control. 

(Slide time: 46:16 - 49:13)
In PPP design requirements are given in RFC 1551. RFC stands for Request For Comment and forms a very important part of networking. PPP uses packet framing; its requirements are encapsulation of network layer datagram in data link frame. This carries network layer data of any network layer protocol at the same time. It should have the ability to demultiplex upwards. PPP uses bit transparency also, which means, it must carry any bit pattern in the data field. 
 
(Slide time: 49:14 - 49:52)
We only require error detection, but no correction at the receiving end. The connection liveness: it should be able to detect, signal link failure to network layer. Network layer address negotiation means endpoint can learn/configure each other?s network address. 

(Slide time: 49:53 - 50:48)
The PPP non-requirements are no error correction/recovery, no flow control, out of order delivery is acceptable; and no need to support multipoint links; e.g., polling. Error recovery, flow control and data ordering are all relegated to higher layers. 

(Slide time: 50:49 - 52:36)
The PPP data frame has flag, address, and control and protocol bits. The flag is the delimiter. The address does nothing. The control also does nothing; in the future possible multiple control fields. The upper layer protocol is where frame is delivered. The check is for detecting errors. 

(Slide time: 52:37 - 52:58)
In the data frame some info that is upper layer data being carried is required and the check is the cyclic redundancy check for error. So info is the upper layer data. So this is the main body or the payload which is being carried. The check is for error detection. 



(Slide time: 52:59 - 53:50)
 
It uses byte stuffing. The data transparency requirement data field must be allowed to include flag pattern <01111110>. Now we can have the question, is the received <01111110> data or flag? Sender adds extra byte after each <01111110> data byte. At the receiver side two 01111110 bytes in a row, discard first byte, continue data reception and single 01111110 is flag byte. 

				(Slide time: 53:51 - 54:18)
So after PPP, instead of sending it will send first this B1 then B2 etc., and then instead of sending one of them 0 1110, it sends two of them and then B4 and B5. This is byte stuffing, which is used by PPP.

(Slide time: 54:19 - 54:38)
There are a few control issues like before exchanging network layer data: data link peers must continue PPP link and learn/configure network. 

(Slide time: 54:39 - 55:12)
The first thing is the link may be dead. Then the carrier is detected. So it will try to establish the link. For that, they will require some authentication, which means the two sides, configurations, etc. must agree if it fails to establish, then it goes back to dead. If it gets successful authentication, then the network is open and then there is some transmission of data and then finally it will terminate. So for all these, there is a data exchange.

(Slide time: 55:13 - 55:31)
Configure request, configure acknowledgement and configure not acknowledgement -that means your configured thing is not acknowledged. Some of the options are not accepted and some of the options are not negotiable. So this way, the two sides communicate and establish the link. We need not go into the details. This is not really necessary. This is a very simple protocol. Just use byte stuffing and use some data, some error detection and the framing. This is a very simple, but very widely used protocol. In the next lecture, we will see how the error control and error detection can be done by the data link layer. Thank you. 

(Slide time: 56:02 - 56:07)


      (Slide time: 56:08 - 56:11)

      Error Control 

(Slide time: 56:35 - 56:46)
 

	(Slide time: 56:47 - 57:03)
When data is transmitted over a cable or a channel, there is always a chance that some of the bits will be changed or corrupted due to noise signal distortion or attenuation.  for example, suppose you have a wireless channel and suddenly there is a burst of noise. What will happen is that, some of the data will get garbled. Similarly the data may become very attenuated. It may be due to some loose contact somewhere or something. The one that was sent was not received that way or may be it was received as a one zero or something. So whenever you are sending some data or something, there is some communication going on some transmission over some transmission line. You always have to assume that, a data may not  reach the other side in a perfect condition. So that is why CRC is preferred in many data link protocols. 

(Slide time: 57:47 - 56:48)
CRC is Cyclic Redundancy Code. 

(Slide time: 57:58 - 58:05)
In Cyclic Redundancy Code, essentially the data is regarded as being one very long binary number. After all, what you are sending is a string of ones and zeros so you can take a few of them and just look at it as a binary number although the original intention of the user. Place holder digits are added onto the end and it is divided by a generator polynomial using modulo 2 divisions. The remainder at the end of this division is the CRC. 


 
