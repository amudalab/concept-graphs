COMPUTER NETWORKS
Prof. Sujoy Ghosh
IIT Kharagpur
Lecture no 1
Emergence of Networks & Reference Models
(Refer slide time: 00:36)

Good day. I am Professor Sujoy Ghosh of IIT Kharagpur, I teach in the Computer Science and Engineering Department, I will be taking this course on computer networks. We will be having about 40 lectures in the series. Today, we will start with an introductory lecture, which is about the Emergence of Computer Networks, its brief history, and a little bit about the protocols and reference models, which is an abstract view of computer networks. The text books for the course will be these three: Computer Networking: A Top Down Approach Featuring the Internet by Kurose. Data and Communication by William Stallings and Computer Networks by A S Tanenbaum. Some of the material that we will be covering here verbally as well as in the slides, will be from these books. The first question is, what is a computer network? Computer network is a number of computers (also know as nodes) connected by some communication lines. We have these computers, which are connected by some communication lines. Two computers connected to the network can communicate with each other through other nodes if they are not directly connected, which means that computer does not have to be connected to all the other computers in the network in order to communicate with any of them. It may be an indirect communication via some other computer and other point is that these nodes are computers. But some of them are not computers; some of them are network devices. There are various kinds of network devices like switches, routers etc., so, they facilitate the communication and running of this entire network. They are also taken as nodes in the network. Finally, we have some of the nodes, which may be either computers or they may be network devices like switches and routers and there is a communication between them. This communication (Refer slide time: 1:28 -2:30)



 can be of various types ? we will come to that presently. The computers can communicate with each other. What do they communicate? They exchange information between different computers, this information could be of any kind of information ? they could be data used by a program or they could be some program itself or it could be any kind of information. This is one of the basic and major uses of computer networks. There are other specialized uses of computer networks. For example, interconnected, small computers can replace a large computer. For example, you have a very large computation to perform and so for that either you need a very powerful and very large computer, which is very costly. The other option could be that you break up this work into very small pieces and assign them to the small computers. The small computers do the computation on that small chunk of the problem, and then they communicate with each other to form the final solution. The other use of computer networks, which is coming into vogue very much these days, is that you can use this network as a communication tool. For example, you can send emails to almost anybody these days, and that is really a very cheap and very fast mode of communication. Secondly, the computer networks could also be used for direct communication like you could communicate through voice over a computer network, you can communicate through video over a computer network. All these different communication is converging into this computer network and nowadays when we talk about the network we do not look upon it just as a computer network. It is the bedrock on which, all these computing communication are converging and then you have some applications, which are necessarily of distributed nature. For example, railway reservation system ? now obviously if you had a large computer to handle all the railway reservations in the world in one place but we would not want all the people to go to one place and form a huge queue. So, we want to distribute this functionality that means what that of booking tickets all over the country. These are the examples of distributed systems so this is a distributed reservation system. Similarly, there could be other kinds of distributed systems, distributed databases and all kinds of applications, so that is the other application of networks. (Refer slide time: 3:30-5:59)

Now, a quick look at the history of the computers first, then we will focus on computer networks. In 1948, we have the first commercial computer installed; it was a UNIVAC ? that was a big company once upon a time, and then in 1958, the first US communication satellite became operational. The network always has these two component, one is that you have to have these computers and then you have to have these communication channels and satellite communication is one of the important medium of communication. In 1964, we have an airline reservation system, which uses some kind of a packet switching network; it was proposed by the RAND Corporation; and in 1969, ARPANET, the first packet switching network began its operation. This was actually a water shed event as we will see. This have been very far reaching impact on the way computer networks developed. (Refer slide time: 6:51 -8:03)

In 1971, we have the first computer chip, previous computers were made through very low level gates and all but the first four-bit small computer chip with 2300 transistors and they became available in 1971. In 1972, Ethernet specification was formulated ? this Ethernet is one protocol ? and Ethernet remains one of the most important network protocols that we use. So that was first formulated in 1972. In 1974, IBM introduced its own version of network in SNA. In 1975, Altair 8800, the first commercial microcomputer, was sold as a kit. (Refer slide time: 8:17-09:18) 

In 1975, we had Paul Alan and Bill Gates getting together who eventually formed Microsoft. In 1976, Woznaik and Jobs built Apple1. So Apple also has lot of contribution to make to computers in various aspects and Windows system was basically started from Apple computers. In 1979, we had VisiCalc first commercial spreadsheet; this is the popular application of computers these days. (Refer slide time: 09:18 - 10:07)

In 1981, IBM introduced IBM PC with one floppy drive; later on they changed it with drive and hard drive; and in 1983, TCP/IP is another network protocol that we are using. We use both Ethernet and TCP/IP protocol. We have different protocols, which we are running on the network and how this different protocols are arranged and how they are used that I will cover today in second part of the lecture. So, this TCP/IP is another important protocol that is still very much prevalent today ? that became the official protocol of the ARPANET. ARPANET is the network, which was set up in USA under the DRDO and which had a very far-reaching impact on the way the network developed. In 1984, Apple introduced GUI with Apple Macintosh and 1986 we see the first laptop PCs. (Refer slide time: 10:09 ? 11:07)

In 1988, IBM gives a multitasking OS for the PCs. In 1989, Microsoft releases Windows, which is yet another multitasking system that is the Microsoft version; in 1989, NSF, National Sound Foundation of USA replaces ARPANET as internet backbone and then www was invented by CERN Physicist Tim Burners Lee in 1991. So this advent of www, World Wide Web, made the network very popular. There was this kind of positive feedback going on, as it became more popular, more and more people jumped into it, as people showed their interest, so commercial companies also became interested because there would be a market for something in the networks that they do and when the commercial companies developed something, naturally it became more easier to use the network; they had more functionalities; so more people became interested, that was the positive loop. Actually the growth of network in the past decade ? in the 90s ? has been absolutely phenomenal. The basic so called killerapp, application which attracted so many people to networks was this www which was invented in 1991. (Refer slide time: 11:08 ? 12:48)

In 1992, Mosaic released first GUI web browser, so that was the web browser ? then Netscape became the popular web browser, Microsoft had its own version of Microsoft Explorer, that was an another browser and then it all built up into some kind of a frenzy and in 2000, we had this .com, so-called .com meltdown. That means people became absolutely frenzied about the growth of computers and such a frenzy can not go on forever so it has to come to an halt. So, it did in 2000 but the development of networks and its utility and importance remains. (Refer slide time: 12:48 ? 13:33)

Now, let us go to a brief history, more focused on the network itself. We were in general talking about the computer scene. In mid-1960s, USA?s Department of Defense wanted a command and control network that could survive a nuclear attack. If something had to survive a nuclear attack then you had to distribute it. So that was the one strong reason that we wanted a distributed system and if you have a distributed system, these systems must be able to communicate. So that was the seed of this project. The subnet should consists of some Honeywell 12 KW intermediate message processors so there are some kind of early network devices, which are connected by 64 Kbps lines. These IMPs ? Intermediate Message Processors ? are connected to the various computers and the IMPs themselves were connected by telephone lines from telephone companies. (Refer slide time: 13:34 - 14:36)

Later, the IMP software was changed to Terminal Interface Processor, which allowed the connection of terminals. In the 1970s, NSF setup a machine connected to the ARPANET to which other universities could dial up and connect. Now, this became very important in the sense that many people were interested in this so they upgraded their systems in various ways. In mid-1980s NSF built a new backbone to connect with super computing centers to some regional networks. Its backbone (Refer slide time: 14:37 - 15:17)

was upgraded to 448 Kbps and then to 1.5 Mbps fiber backbone, and once this fiber backbone was in place, communication became really fast. So it allowed network connection to thousands of universities, research labs, libraries, museums, etc. As the growth continued, commercial houses began to notice and to join, the decentralized model began to take hold (Refer slide time: 15:18 -15: 45)

As I described earlier, commercial people joined so there was a lot of innovation by a lot of people. One of the major driving forces in the computing networks in the past decades or even now may be has been the role of so-called Start Up Companies. Start-up companies means that some young engineers or some young people who had some very bright idea which they developed to a certain extent and when that was proven and then some big company would possibly buy their company or give them money to get access to this kind of technology. There was so many success stories that a lot of people got interested in it and when a lot of people think about a problem, there is a lot of innovation. So that is how computer networks had an explosive growth in the whole of 90s and even in the 21st century and the growth is going on. Although the frenzy in the stock exchanges, etc. about these .com companies crashed in a 2000 but the growth in the field of networks and its use in various walks of life, is continuing. With this brief historical background, now let us come to the networks and some kind of structure or some kind of an abstract view of this computer network. I have already mentioned that computer networks will be a number of nodes and nodes could be computers or network devices, which are connected by some communication lines. Sometimes things are not so simple; sometimes you have a line between two nodes, giving us point-to-point communication. That means one node A is communicating directly to one another node B, where A and B are connected by a direct line, it could be physically a copper line or fiber line, etc. This point-to-point connection could be the simplest case but actually it is dedicated and in the dedicated case also we have some different cases. For example, you could have simplex communication; it means that communication can go in only one direction in that line. It could be half duplex, it means the communication could go both ways either way from A to B or from B to A, but only in one direction at a time, and full duplex, that A and B can communicate with each other simultaneously, in parallel, at the same time. So that means there is communication from A to B and B to A at the same time. .These are some different kinds of dedicated lines. Then if the nodes are not computers, some network devices, specifically, if they are multiplexers, we could share a point to point medium. That means there is one line from A to B but a is connected to a lot of other nodes say, A1, A2, to An and B is connected again to a lot of other nodes say, B1, B2 to Bn and they could all these Ais and Bis could communicate through this one single line, which is between A and B, this we will see later how such a thing can be done. This business of sharing a link over time or whatever even parallely sometimes it is possible. Sharing this link by multiple people is called multiplexing. These are the different kinds of point-to-point links but point-to-point links are not the only things we have. For example we could have a broadcast kind of link ? if we have a satellite communication, satellite throws its signal all over the country or may be all over the region. That is something, which is being broadcast so you cannot really put it down as a point-to-point link between something, but you could use that shared medium and a broadcast in some way to make temporary point-to-point links or your application may be such that you want to broadcast something. For example, we broadcast TV signals; similarly there are things, which we want to broadcast all over. This broadcast media can be dedicated to some users or it could be shared; that means it is shared between multiple users. There the term used is multiple access medium, to which different users are connected at different points and it is a broadcast kind of medium. In a distributed fashion, they have some protocol of deciding how to share this because if it is in the same place, the business of sharing becomes somewhat easier so you can multiplex it. If nodes themselves are physically distributed over the broadcast medium you will have some kind of protocol called Multiple Access. So you can have a shared broadcast medium also; that is quite common. Finally, we have something which is in between broadcasting and point-to-point, which is called multicasting, this means that if we have a group, I have, say, some friends and I want to send some things not to one particular friend but to my group of friends and to no others ? so multicasting is basically communicating to a specified group. (Refer slide time: 15:46 - 22:21)

Using such communication link, we have this computer networks. Networks are divided into Local Area Network, Metropolitan Area Network, and Wide Area Network. Local Area Network means that it is local, limited to maybe one building or small group of buildings so its size is small. There are some other things about Local Area Network, which make the issues of Local Area Network somewhat different from a Larger Area Network and the Local Area Network usually could be privately owned. Now how does ownership come into picture? Ownership comes into picture because if you have the same owner for the entire network then you can have the same policy, which is not the case in wide area network. In Wide Area Network there may be various nodes which are connected to this Wide Area Network. The nodes may belong to very different people and they may have very different ideas about what should be done and all their policies. That makes Wide Area Networking somewhat different from Local Area Networking. Metropolitan Area Network means a network which is spread over a community or may be even a city, their size is bigger than Local Area Networks and one of the issues which is very important is the access issues, which means that how do you connect? For example, you want to make the entire community networked; so how do you connect each one of them because they are geographically more distributed then a Local Area Network. Local Area Network may be sometimes we would use validers or sometimes we would just simply draw valve and the cost will not be very prohibitive but in local area how we connect the people becomes an issue that is the access issues. We have talked about the Wide Area Network. Wide Area Networks are costly they are communicating over may be hundreds or thousands of kilometers. The communication is costly, the cost has come down and that is another thing, which has historically happened. As more and more people got interested in network, technology developed that is one side of the issue. The other side of the issue is that volumes went up; which means that the number of people who used network so as the volumes went up the cost went down. So the technology improved in one direction. That was one input into bringing down the cost and the other issue was that more and more people started using it. So the volume to a particular company developing something in the network area  went up and they could make things cheaper. And as the whole thing became cheaper and cheaper, more and more people wanted to join the network. Wide Area Network cost is still an issue. Wide Area Network, may connect various Local Area Networks, so its a network of networks or internetwork or internet. (Refer slide time : 22:26 - 26:17)

Let us look at the abstract idea about the networks in general. In formal framework, we have what are known as protocols, why do we need protocols in network? We need protocols in networks because networks by their very nature, might connect various different people as I said various different people may have very different ideas about how things have to be done. If people have different ideas about how things have to be done then they have to agree on some common basis to communicate and this common basis is the protocol. There are numbers of protocols, which are used in network and we will be getting into that. You and I may have very different ideas about how some particular thing may be done. I may be doing this in a certain way and you may be doing the same thing maybe in a completely different way. But if we want to communicate we have to agree on some minimum protocol. These protocols are the building blocks of this network architecture. So each protocol object has two different interfaces ? one is the service interface that defines the operations on this protocol and peer to peer interface which defines messages exchanged with peer. For example, any protocol, a protocol is between peers so if in this diagram there are the two Lis are the peers and they communicate with each other through this peer interface; that is the protocol. This protocol is supposed to achieve something, supposed to do something. For this it requires something maybe from some other called layer. Actually we have a layered architecture, I am coming to that in the next slide. How this protocol is to be invade etc for this we have a service interface and between the peers we have a peer interface. (Refer slide time: 26:18 to 28:50)

We have all these different layers, we have this N + 1 th layer, N th layer and N ? 1 th layer. How they are to be layered ? about that also there are some standards and we just look at two of them at least. For example, this networking business is broken up into  whatever the jobs you have to do for achieving for smooth functioning of computer networks we break them up into different functionalities and these functionalities are in different layers. This is all in one place, that means in one node. Similarly, another node, which is connected to the network, will have its own layers and at the same level N and N here they are peers. So there will be a protocol between this N and N, there will be a service interface between N + 1 th layer and N th layer. Similarly, there may be a service interface between N th layer and N ? 1 th layer and so on. (Refer slide time: 28:51 -30: 02)

So let us see what they are like. As I said most networks are organized as series of layers. The task of each layer is to give some service to the upper layer and any layer maintains a virtual connection with the corresponding layer in a peer. Any layer maintains a virtual connection; this virtual connection is used for running the peer-to-peer protocol, whereas this is the service interface. The task of each layer ? each layer performs certain subtask of this whole networking business, so each layer performs some task ? is a service to the upper layer. Similarly for performing this task, it may break it up into some subtask and some subtask may be delegated to a lower layer. This layer could have its own service interface to the lower layer whereas its own task, assuming that the subtask is done by the lower layer, is performed at the peer-to-peer level, using the corresponding protocol.(Refer slide time : 30: 03 - 31:15 )

There is peer-to-peer protocol running between any two corresponding and communicating layers. The interface between the layers in the same node is well defined. Another point is that for this peer-to-peer protocol mostly they go through a virtual connection. What is a virtual connection? Virtual connection means that physical connection; we understand let us say two computers are connected by wire. It is not necessary that two computers have to be directly connected in order to communicate; it may be an indirect connection also. That means it is going from one computer to another then, it is hopping from node to node then, it is finally reaching its destination, and so there is no direct physical communication. When you are surfing the internet, you have been connected to some computer, which may be connected to opposite part of the globe so, to you it appears that you are directly connected to that computer so whenever you are clicking something over here, something happens. So that clicking of the mouse somehow that gets communicated to that remote web server and may be the page changes or something. There seems to be a direct communication but this communication is only virtual. The physical communication is only at the lowest layer, where physically something is connected by a piece of wires or fiber optic links or by satellite links whatever it is, that is the physical connection. For running all these protocols, these peers need to communicate and this communication is through virtual connection. This is the main point of having this layer architecture: that the implementation of each layer, each node is transparent to the other two nodes. As I said, I may want to do this one particular subtask in some particular way ? I means some company X ? and then some company Y wants to do the same thing in a different way. Now you need not stop any of them so long as they agree on the protocol, so long as they agree on the service layers. What is the service interface between the different layers ? because upper layer service may be given by some product of some company, whereas the lower layer may be developed by some other company. Once again if they can decide on the interface between these two layers, then they are very free to do it in their own way. The same thing applies to the peers. For example say two particular layers in two particular nodes may be peers, these nodes are computers themselves. Now, one may be an IBM machine another may be a sun machine and they will have different operating system, they will have different processors etc. But so far as the peer to peer protocol is concerned, they agree. So these are the messages I will send and then if I am expecting these kinds of answer, these kinds of messages, I will accept and these kind of answer will be given etc, so that is how a protocol goes. So long as they agree on this protocol, these two companies are free to develop their product in their own way and when you allow that naturally, people can innovate, people can put in different things. So long as you are conforming to the standards of the protocols and the interfaces, etc., you can develop your own thing in your own way and that is really good for the development of the entire network. The other point is that this is one way of abstracting out all these unnecessary details. For example any of this network operating system in a computer or a networking device, they may be very complex indeed. Some of the complexity is special to the implementer. One implementer has decided to implement something in a particular way and that will have a whole lot of details. We are not really interested in that one, we have to abstract out of that. We concentrate on these protocols and the interfaces and the functionalities; that gives us a fairly general picture about the entire networking, how networking is done and then if you go into the business of developing some of these modules yourself, then you have to go into some more nuts and bolts about how this service can be given. The protocols between the peer layers can be changed if the peers all agree. Naturally if you are changing the protocol, all the peers have to agree; otherwise the communication will breakdown. However, it need not be referred to other layers ?  so that is another good thing about layering; that is whatever change I make it will not matter and the service definition says what the layer does and nothing else, means nothing about the specifics of implementation, which may vary from one vendor to another. The interface tells the process about how to access it; it specifies what the parameters are and what results can be expected.  (Refer slide time: 31:16 ? 37:57)

We have mentioned the three layered models, the most famous is the OSI reference model which we will look into in some detail and then TCP/IP reference model which is most widely used. OSI reference model mean some part of it is very somewhat theoretical because many people really do not do that and it is that they do not consider that part to be very important. TCP/IP reference model is something which is almost ubiquitous. And of course there is ATM, it is one networking technology which is rather complex technology; as I mentioned here just as an example; so that is another kind of reference model. There are all kinds of different reference models, sometimes these models are basically a description of the different layers that are there; and naturally if you talk about the layers, what the layer is supposed to do and that is what you see and then how it is to be interfaced with the layer above and layer below. Since this is number of layers, one on top of other, they are also referred to as stacks. We call about this as TCP/IP stacks, OSI stacks, ATM stacks and so on. Other protocol stacks exits and new ones are possible. However the extent to which a particular model is universally accepted is the key to its success. As I said a lot of thought went into OSI reference model but in practice TCP/IP became much more prevalent. (Refer slide time: 37:58-39:36)

OSI reference model has seven layers and these layers are; application layer, the presentation layer, the session layer, the network layer, the data link layer and the physical layer, which means that in order that people can communicate over this network in a very seamless manner, all the jobs that are involved have been broken down into seven parts. So they are different layers ? application layer is something and then presentation, session, network, data link and physical layer. Each layer has some kind of functionality and all these functionalities together give you the overall functionality in the network. (Refer slide time: 39:37-40:24)

Little bit more about peer-level communication: messages sent from one application to another application on different host, travels down to the layers of the sending machine and each layer adds a header to be used by its corresponding peer level and the bottom layer, which is the physical layer, sends the message to the TCP machine; this is the general scheme. The point is that suppose we start something at the very top, in the case of OSI, we call it the application layer. Some communication has been initiated at the application layer. This application layer, mind you, has its peer in the destination machine. In the destination machine also some application layer program is running. For example let us say you are doing a TELNET kind of thing, that means you are logging on to another machine, TELNET to another machine. So you will start a TELNET program, which is your TELNET client program and the destination machine will respond to a TELNET client. Who will respond to TELNET client; only a TELNET server can respond to a TELNET client. TELNET server is under the remote machine. These are the two modules which are in the application layer. How TELNET is an example of a protocol? This is an application layer protocol and what this protocol does, how the TELNET client will request and how the TELNET server will respond etc. are the internals of TELNET protocol. So far as the TELNET client or any of these application program is concerned ? when it tries to communicate with another application program in another machine ? it just knows about this protocol and how things have to be done at that layer but how this communication be able to go from this machine to another machine? For this, if you put the whole thing in the same program that becomes very complicated; that was the idea of layering. So give him a virtual connection, he will communicate to the target machine, he will have some data to send that will add up maybe to other kinds of data and then hand it over to the lower layer in the same node. The lower layer may be running a protocol with the corresponding layer in the remote machine. For that protocol it needs to do communication, for that communication what ever message has to be sent here that gets added to the original data, which was sent by the application layer. As the message to be sent moves down the stack from the originating machine at each layer, each layer is running its own protocol with its peer and it has some message to add to give to his pair. So, they will keep on adding it to this and this becomes fatter and fatter as it goes down. As it goes down to the physical layer, there it can communicate to the remote machine and in the remote machine this is now again moved up the stack and at each layer that particular program or whatever that module in that layer, he will pick out the message which has been sent by its peer in the source. This is how communication will go on for all these protocols. (Refer slide time: 40:25-44:06)

Sending a message is received on the receiving side, then passed up through each layer and each layer reads the corresponding header that means the corresponding header which has been sent by its own peer on the source machine. (Refer slide time: 44:11-44:32)

For example, we have two machines, and one machine is a DOS machine. That means some DOS is an old operating system which used to run on the PCs and this was Microsoft DOS, PC DOS, IBM DOS, etc. and Macintosh was a different company as I mentioned. So, these two can still communicate, that means two different machines but this original data which was originated from the application layer. There is the presentation layer and this presentation layer will add its own header to the original data. This whole thing becomes data for the next layer, namely, the session layer. Session layer will add its own header so as I said, that the original data becomes fatter and fatter and fatter till it comes to the physical layer. At the physical layer, data is actually sent in one hop, to the destination machine. Here, as this whole fat packet travels up, each layer will strip off its corresponding header which is coming from its peer in the source. That is how the each of the peers can communicate and each of them has a virtual connection with the destination machine, although the actual connection is the physical connection. (Refer slide time: 44:34-46:07)

These are the seven protocol layers and now we are going bottom up. First is the physical link, which is how to transmit the bits. Then we have the data link layer ? that is how to transmit frames. Now what is the difference between transmitting bits and transmitting frames? We will see to that; but a data link basically is a direct connection that is, how two computers which are directly connected or two nodes which are directly connected to each other. How they will communicate is the matter of data link. Computers need not be directly connected to each other in order to communicate; they have to be connected to the network. If there are two machines A and B or two nodes A and B which wants to communicate and there is no direct link between A and B, so it may go as C D E and then B. Now, how do you know you have to go through C D and E and then to reach B? Somebody has to keep track of the route. How to route the packets over the entire network? That is the job of the network layer. Then there is the transport layer that is how to send packets to the application. Packet is some data which has been segregated and put into a packaged together. But the original application layer need not have any concept of packet; somebody has to packetize all these data and there are may be other functions in the transport layer. That is they make packets out of the data given by the upper layer, this is a transport layer. The session layer, which manages the virtual connection between in the application layer. It manages the virtual connection. The presentation layer that means how we encode and decode messages that means two different machines may have two different ways of encoding things. So, that is the job of the presentation layer as well as the security comes here. Application, so whatever the actual job the human user is interested in comes in the application layer. (Refer slide time: 46:08-48:42)

The application layer contains a variety of protocols; it depends on what you are trying to do. The various users may want to do various things; examples are; FTP, TELNET, SMTP, HTTP etc. so these are names of some protocolsm By the FTP protocol, what u can do is you can download files from another machine, by HTTP, you can look at web pages, surf etc. and SMTP is used for sending mails. You may not have come across these protocols directly but the point is whenever you are sending a mail, let us say from a UNIX machine or something, you are using the SMTP protocol, so SMTP protocol has been built into that. Similarly, even if you are TELNETing that means the TELNET sever, TELNET client etc, they will be there in the machine as part of the OS. So, the application layer usually, request reliable and cheap connection to its peer. That is what the application layer is concerned about: that it must be connected reliably and cheaply to its peer. Some examples of peers we have given some nodes given service etc. (Refer slide time: 48:43-50:00)

Application layer hands this, whatever it has to send to the presentation layer which handles the format of the data: Protocol conversion, data translation with its ASCII or may be some other big or something and data compression, data encryption, these are handled in the presentation layer. (Refer slide time: 50:00-50:22)

In the session layer, it allows the application on different computers to share a connection. We will see about this connection later on.  I go to the next layer we will see that I will packetize this data but then so far as the user is concerned, he wants a continuous connection. This connection is handled in the session layer so it can provide check points; that is if there is come disruption, you can come back to it and get the original state back and then you can retransmit if there is some distance ? dialogue control, who can transmit etc. (Refer slide time:50:22-51:01)

The transport layer: the basic function of the transport layer is to accept data from the layer above, split it into smaller units if necessary. Now, why we need to split? That we will come to later on. So these are the packets. Pass these to the network layer and ensure that the pieces all arrive correctly and in the right order at the other end. For example, you have chopped them into pieces; they may have become out of order. That is the business of the transport layer. That should also be done in a cheap and efficient manner and etc. and the same thing applies (Refer slide time: 51:01-51:33)

There may be different types of transport services, there may be multiple protocols here. One could be error-free, point-to-point channels or it could be a very cheap kind of channel or broadcasting of messages to multiple destinations. So these are the different types of transport service, which are possible. (Refer slide time: 51:33-51:56)

Network layer decides on what route to take locally, so that the intended message ultimately reaches the destination. It controls the broadcasting by essentially segregating the different networks, etc. It handles technological mismatches so we will get into the details of this later on. (Refer slide time :51:56-52:12)

It does congestion control, it handles billing information etc. (Refer slide time: 52:12-52:20)

The data link layer makes the physical link layer appear like a channel that is free of transmission errors. Actually in the physical layer there may be error but the data link layer handles this error correction etc. at the very lowest level. (Refer slide time: 52:20-52:33)

Finally, at the lowest level we have the physical layer, where the data is actually transmitted as raw bits, etc. (Refer slide time: 52:3-52:41)

The other layer is TCP/IP reference model, where the session and presentation layer are not there. We have the application layer, transport layer, network layer or maybe something like the data link or the physical layer. (Refer slide time: 52:22-52:52)

So the rough correspondence is something like this; we have the TCP/IP model where we have just this few and we have the OSI reference model with so many. So more or less, they match the essential functions match. There are other kinds of protocols, which are also used but the TCP/IP and OSI, are the most common. In the next class, we will be discussing the different structures of networks. So, today we just had an abstract view of networks and in the next class we will handle different structures. (Refer slide time: 52:52-53:25)


Preview 
COMPUTER NETWORKS
Prof: Sujoy ghosh
IIT Kharagpur
Lecture 2
Network topology

(Refer slide time: 53:57)
Good day. In this lecture we will discuss the network topology. Now what do you mean by a network topology? (Refer slide time: 54:00-55:20)



We just have a quick recap of what we had learnt last time, basically a computer network, could be a number of nodes which are connected by some communication links. There is some kind of graph, this graph has certain structure so when we talk about the structure, this structure has an implication about how will go about communicating as I said it is in general not feasible to have one to one communication between each pair of nodes. That is not possible at all, so what kind of structure? That is why the nodes connected in each manner is the subject matter of our discussion today. A network may be represented as a graph, nodes representing computers or network devices like switches, routers etc. and the links represent communication links. Modes of communication may be broadcast or point to point. We have discussed this. (Refer slide time: 55:25 ? 55:35)

First let us talk about the LAN topologies. LAN, the local area network and the local area network topologies are three. They are very common; star, ring and a bus. Refer slide time: 55:49 ? 57:20)

First of all, we first take up this bus topology and why do you take up a bus topology? This a very simple kind of network which is based on a shared broadcast links. We still want to look at point to point communication, that means there may be so many nodes a, b, c, d, etc., which are connected to the network and a wants to communicate to b. so c, d, e are connected but a specifically wants to communicate to b. Each pair of communicating nodes used a link for the short time. So, a uses the link for the short time to send his message to b then may be c might sends something to b or something like that. Other nodes ignore the communication. Since, this is shared broadcast link all the nodes get the communication, that is not private in that sense. All the nodes get that communication but they usually ignore this communication whereas b will copy this for its own purpose. Now, they has to be a distributed protocol to decide who gets to use the link. There has to be some protocol otherwise if a wants to communicate with b and b wants to communicate c, as c wants to communicate with d their communications will go and collide in that shared broadcast medium, so the communication of both the pairs of node will get garbled. So by that I mean and sometimes actually that would happen that things will get garbled but that is not what we. 

COMPUTER NETWORKS
Prof: Sujoy Ghosh
Lecture#2
Network topology
(Refer slide time: 00:53)

Good day. In this lecture we will discuss the network topology. What do you mean by a network topology? (Slide reference time: 01:04 - 01:14)


(Refer slide time: 01:15 - 02:33)     

We can just have a quick recap of what we have learnt last time. Basically a computer network could be a number of nodes, which are connected by some communication links.  There is some kind of what we usually call a graph representing the network. This graph has a certain structure. We talk about the structure. This structure has an implication about how will we communicate. I said it is in general not feasible to have one-to-one communication between each pair of node. That is not possible at all. So, we need a structure formed by the nodes connected in some manner. That is the subject matter of discussion today. So a network may be represented as a graph. Nodes may represent computer or network devices like switches, routers, etc. and the links represent communication links. Modes of communication may be broadcast or point to point.  (Refer slide time: 02:34 - 02:49)

First let us talk about the LAN topologies. LAN is the local area network and local area network topologies are: star, ring and bus. We will just look at all of them.(Refer slide time: 02:59 - 04:30)

We first take up this bus topology as this is a very simple kind of network. This is based on a shared broadcast link. We want to have point-to-point communication. There may be so many nodes A, B, C, D etc., which are connected to the network. A wants to communicate to B; A specifically wants to communicate to B. So each pair of communicating nodes uses the link for the short time. So A uses the link for the short time to send his message to B. After that is over may be C might send something to D or something like that. So other nodes ignore the communication. Since this is shared broadcast link, all the links are shared by all the nodes for the communication. Hence that is not private in that sense. So all the nodes get that communication but they usually ignore this communication whereas B will copy this for its own purpose. There has to be a distributed protocol to decide who gets to use the link; otherwise, communications will   collide in that shared broadcast medium. The communication of both the pairs of node will get garbled. But we want A, B and C, D to get clear signal so there has to be some
distributed protocol to decide who gets to use the link. (Refer slide time: 05:12 - 05:59)

Now let us see bus topology. What do we require? The simplest thing is a single cable.   A single cable connects all the computers. It is as if the computers are hanging from the single cable. Each computer is connected to this shared cable. So computers must synchronize and allow only one computer to transmit at a time as per a protocol. But as such, the network is very simple network with a single copper cable (Refer slide time: 06:00 - 06:26)


So this is the picture of a bus. We have this shared cable and computers are somehow connected to this. So A will communicate to B. B will simply broadcast over this medium. B will capture it. C, D, E, etc. will all ignore this. (Refer slide time: 06:27 - 08:25)

The network is maintained by the single cable. Cable segment must end with a terminator. Otherwise, what will happen is that when some node, let us say A sends a signal on the cable not only  C, D, etc. will  get it but it will get reflected from that other end if it is not properly terminated. So it becomes a kind of unwanted noise in the medium which we do not want. So the coaxial cable is used. There are two types of coaxial cables traditionally used. One is a thin coaxial cable called thin LAN and the thick coaxial cable called thick LAN and extra stations can be added in a daisy chain manner. This technology is sort of getting outdated at the moment. You still have maybe  some thin and thick LANs here and there but mostly people are moving out of this and the main reason why the people are moving out of this is because this connection is likely to become loose and as soon as you have a loose connection you will have a problem in communication. There are some other reasons also why this technology has become obsolete these days. But still if you have shared medium, I mean the technology of using a single cable with a single broadcast medium and some machines are sharing. So this something like a bus topology. (Refer slide time: 08:26 - 10:23)

The standard which is used on this bus topology is IEEE 802.3. Actually what we do is to eliminate that cable and replace with something else. We will discuss that later. Thin Ethernet is the name of the technology when we use thin coaxial cable ? we call it 10Base2. In this 10 stands for that it has a 10 Mbps speed whereas 2 stands for the maximum range of segment length of 200 meters. The maximum number of connections is about 30 devices. Four repeaters may be used to a total cable length of 1000 meters. If you want to have a very long cable, what will happen is that, as the signal travels down the line, the signal will get weak and as the signal gets weak you have to regenerate the signal and one way to regenerate the signal is to use a repeater over there. The repeater will simply take the incoming weak signal, amplify it and send the stronger signal down the line. The maximum number of nodes that you can handle here is about 150 (Refer slide time: 10:24 - 11:03)



Thick Ethernet, which is the other version where you use a thick coaxial cable called 10Base5, also has the same speed of 10 megabits per second but length up to 500 meters. So they were used for backbones that are limited to 500 meters with a maximum of 100 nodes per segment. That is somewhat more than a thin net and a total of four repeaters of 2500 meters. That is also somewhat more than thin net. (Refer slide time: 11:05 - 12:42)


Now the bus topology has both advantages and disadvantages. It is very inexpensive as you have a simple cable. It is easy to add stations. You can have another punch on this cable and add new station that means a new computer. The amount of cable it uses is also much less and may be, it works well for very small networks. It is of course  no longer recommended which means you will not get these parts any longer. It is getting out of fashion because it is unreliable. This is much less reliable than the technology that we have today. The other disadvantage is that the limited number of devices that can be attached. It is difficult to isolate the problems. If there is a problem, then the whole network is down. Sharing the same cable slows the response rates (Refer slide time: 12:43 - 13:30)

Now we come to direct point-to-point communication. That means this is not shared. So this is some kind of a dedicated point-to-point communication. So computers are connected by communication channels that each connect exactly two computers. They are dedicated for this pair. So this forms either a mesh or a point-to-point communication, and thus a complete graph. We will come to that later. This is some point-to-point network. That means there is a dedicated link between some pairs of nodes. It allows flexibility in communication, hardware, and packet formats and also provides security and privacy because communication channels are not shared (Refer slide time: 13:52 - 14:25)
    
With point-to-point network, the network will look somewhat like this. We have these two nodes or three nodes may be connected like this. If we have four nodes and we want to interconnect everybody then we require six such cables. As the number of nodes goes to N the number of cables you require is going to be NC2 which is N (N ? 1)/2 which is approximately equal to N2 which means the number of cables and the total length of cables that is involved are really increasing. But the kind of cable which is usually used  is not as expensive as the previous thin LAN or the thick LAN. At the same time the amount of cable that you require becomes very high. Hence  this kind of structure is not really recommended for a local area network. Suppose you have, say, 10 nodes in a network and you are connecting everything to everything. That means every node to every other node and length of the cable for some reason, let us say, is not an issue. But then each node will be connected to 9 other nodes. This means that in this particular computer you will require 9 ports to which these cables will be connected. But that is not so easy to provide and this is another reason why, in a local area network, this kind of mesh topology is not recommended. We have an alternative for that. (Refer slide time: 15:55 - 16:02)

 We are coming to that alternative. Another reason why you cannot connect everybody to everybody else can be illustrated thus: suppose there are two different buildings. Now	the length of the cable will really become enormous because if you have N cables on one side and M cables, N nodes on one side and M nodes on the other side,  you have a total of M ? N number of cables which are running between two different buildings. It is lot of cables really. So if we want to reduce the number of communication channels we have to (Refer slide time: 16:34 - 17:18)

basically give up the idea of communicating directly in a LAN. It means that necessarily we share the connections among many computers. There are ways of doing this by multiplexing. Here the computer takes turns in a very orderly fashion. That is called time division multiplexing and it also must include some technique for synchronizing the use. That means whoever wants to use that shared channel has to wait for his turn (Refer slide time: 16:34-17.18)

We now come to this other topology, which is very common these days. It is star topology. A star topology looks like this. There is a central hub (the centre of the wheel is called a hub) and a number of nodes are connected to this hub. I may at this stage mention that there is a physical topology and a logical topology. This is a star topology because it looks like a star. Its physical topology is that it looks like a star. Sometimes the hub is made in such a way that still this is a shared medium. What is the advantage of converting to what was previously a bus? It has some practical utility in the sense that (although the length of the cable is more in this case) the cable is a different kind of cable. The cable connections are much better. They do not get disconnected very often. Furthermore when you bring all the connections together centrally, you can handle the connections at one central point. Whereas, if you have a long running cable, it will be difficult to locate loose connections. We will have to find out failures by going around so that becomes a bit difficult. So the physical star topology still maintains a logical bus topology, which is an important thing. Otherwise this hub or the central node may not be a shared medium at all. This is often the case these days (Refer slide time: 19:49 - 20:49)



Of course the previous star diagram is idealized. Usually you will have something like this. You have this hub somewhere and then you have these different computers connected and there is a number which is written over here: RJ-45 connector which is the kind of connection which has become almost ubiquitous. These cables are something like the kind of cables which connect your telephones. They are called UTP cables or unshielded twisted pair cables. They are basically easier to handle and are also cheaper. These connections go on to the RJ-45 connectors on both sides. They are much more reliable than the previous thin LAN or thick LAN but the physical diagram may look something like this. (Refer slide time: 20:50 - 21:36)

You can have an extended star topology, which means different stars may be there, which  are connected in some manner. That means the hubs in those stars are connected and those hubs may be shared but may not be shared medium at all. They may be what are known as switches. These switches are connected. If you look at this diagram, you have these different switches and from each switch we have a star connection to the different computers and the switches are connected in some fashion. You can call this also as a star. This is called extended star topology (Refer slide time: 20:50-21:36)

You may have a hybrid topology. Some part of the network may be on a shared medium Some part, maybe, is not shared and even in some part, you could also have a coaxial cable running, leaving a thin LAN or thick LAN although that is becoming less common these days. (Refer slide time: 22:04 - 23:03)

Now we come to another topology, which is also very important topology, called as ring topology. In ring topology, computers are connected in a closed loop. This has a lot of advantage. The way it works is that the first node passes the data to the second node; the second one passes the data to the third and so on. The data will go on hopping around the ring and what will happen is that, as it reaches the destination, if it will go around in a loop, it will surely reach the destination at some point of time. The destination will copy the same. In practice there is a short connected cable from the computer to the ring. Ring connections may run past offices with connector cables to sockets in the offices. (Refer slide time: 23:04 - 25:45)

This is an example of a ring topology. Actually, there are two rings. In a ring the data will always go in only one direction. If your destination is on the wrong side then the data has to come over the whole loop and then reach the destination. So there is one ring for that. What is the other ring for? We put two rings or sometimes may be even three or four (two is quite common). One important reason for putting two rings is that if there is a failure in the network there is a possibility of the communication still going on.  Look at the picture B. Let us say one particular node has come down. That means, there is some problem in this thing. If these nodes can sense that this has really gone down, they will sort of wrap the ring around. Now physically this still looks like a broken ring. But if you look at it as a single ring, this is still a ring going through the nodes. These nodes can still communicate although there is a breakage over here. Either this node may break or this link may fail. So many things may happen. So this fault tolerance is one good reason for using rings. Especially in the wide area network, we have a lot of things. For example, let us say, these telephone exchanges are connected in some fashion using fiber optics cable. We will be talking about all those things later on. But they usually put them on a ring because if the cable is cut at some place they can still communicate by using the trick that I have just shown. (Refer slide time: 25:46 - 28:24)

This is the summary of the characteristics of the ring topology. There is no beginning or end. It is actually a ring. All devices have equal access to media. One good thing about it is that all of them should get data at some particular point of time. So they have equal access to this media. In a single ring, data travels in one direction. Only a double ring allows fault tolerance. Each device has to wait its turn to transmit. That means you cannot start transmitting as soon as you want to transmit. You have to take your turn; and most common type is the token ring. Token ring has got its IEEE standard number (IEEE 802.5). In a token ring what happens is that the token contains the data. It reaches the destination. Data is extracted, acknowledgement of receipt sent back to transmitting device, removed, empty token passed for another device to use. That is one way a ring can be used. For example, you make a data packet with this token as well as whatever you want to send to the final user. So this whole packet will go over to the next node in the ring this way, it keeps hopping from one node to another till the destination when the destination gets this packet, it knows that. The destination had this or some kind of identification which will be there somehow in this whole bunch of data. The destination node will know that this is meant for it. It will extract the data. It may be put in an acknowledgement if you are using the protocol which uses acknowledgement and send it along the ring. It will go around the ring and come back to the original sender and the sender will know that this data has been received. That will remove this acknowledgement also and the empty token is passed and this token keeps on circulating in the ring. Whoever wants to transmit has to wait till it gets the token .As soon as it gets the empty token it will put in the data that it wants to send and send it along in the same way. (Refer slide time: 28:25 - 30:07)

Ring topology has its advantages and disadvantages. The advantages are that, Data packets travel at great speed. Now one reason why data packets can travel at great speed is that this is a very synchronous operation. If you remember that in a shared medium where the nodes are not synchronizing their actions, what might happen is that two different nodes may start transmitting at the same time and then their data will collide in the broadcast medium, hence killing both the communications. Here such a thing cannot happen because it has all been synchronized with the help of the token. There are others ways of synchronization. We will come to all that later on. Data packets can travel at great speeds. There are no collisions. It is easier to find faults and rectify faults and terminators are not required as in an ordinary cable. The disadvantages are: it requires more cable than a bus and a couple of breakdowns may bring the entire ring down. Smaller rings exist nowadays but anyway they are not very common. This is not as common as the bus so far as the local area network is concerned. In a wide area network a ring is very common. As a matter of fact the ring is the most common topology that wide area network could use. (Refer slide time: 30:08 - 31:00)

Many LAN technologies that use ring topology use token passing for synchronized access to the ring. The ring itself is treated as a single shared communication medium. Bits pass from transmitter past other computers, and are copied by destination. Hardware should be designed to pass token even if attached computer is out of commission. This is a small technicality. If the interface of the computer to the ring can function independently of the computer, that will be nice, because even if the computer is shut down, other people?s communications are not affected. (Refer slide time: 31:01 - 31:38)

This is a picture of the token ring, in which the sender holding token transmits bits of frames. Computer not holding the token passes the bits as they go around, circulate at a very high speed. The destination passes the bits but it makes a copy and the sender receives the bits of a frame and it goes out. This is one way the network could work. (Refer slide time: 31:39 - 31:53)



When a computer wants to transmit, it waits for the token after transmission. Computer transmits token on the ring to the next computer ready to transmit. It receives token and then transmits. This token ring was originally part of a LAN technology. (Refer slide time: 31:54 - 34:22)

But then it fell into disuse. First of all the token ring devices were less common. They were more costly. Hence the token ring as a dominant LAN technology went out with time but one version of it called FDDI lived on and that lived on due to the very specific advantage that the ring topology has which is fault tolerance. This technology is called FDDI. In a FDDI, the ring is made of fiber optic cables. Multi mode fibers or even single mode fibers can be used. They use fiber optic cables in which we can send data at a fairly high speed. But the speed available is not comparable to the level possible. Nowadays  FDDI technology is also not that new. It is aging and that way its speed may sound less today. But when it was proposed it was taken as a very high speed. The basic reason why somebody still would like to use FDDI is its inherent fault tolerance capability. That is the technology, which is used in LAN, and it is fault tolerant. Ring technology, which is fault tolerant, is extensively used. Some other technology is extensively used in WAN but in the LAN we have this FDDI technology whose full form is fiber distributed data interconnect. This is another about ring technology which uses fiber optic between stations, which transmits data at 100 Mbps. When we were talking about the thin LAN and the thick LAN we were talking about 10 Mbps speed ? 100 Mbps was  an order of magnitude high when it was proposed. It uses pairs of fibers to form two concentric rings in order to get the fault tolerance we require with two fibers or two rings. (Refer slide time: 34:23 - 35:48)

As soon as a fault occurs somewhere, the FDDI will automatically switch off and communication will not get disturbed. Furthermore you can have hybrid topology in the sense that you can have a ring, which emanates from the various nodes of the rings. Stars   will form; they will be the different hubs of the different stars and these different hubs can be put in a ring. As a matter of fact, this is the very common way how FDDI is configured. Some FDDI switches form a ring and from each of these FDDI switches, we can start other kind of technologies like Ethernet etc., which form a tree-like structure. The  other reason why you could have hybrid topology is that older networks are updated and replaced, leaving some older segments. You have replaced one part of network; you will get a hybrid kind of topology. Thus it combines two or different more different types of topologies. The common one is star bus or star ring. Star ring uses an MAU (multi-station access unit), which is basically a part of FDDI. (Refer slide time: 35:54 - 38:48)

Next we take up the study of Mesh Topology. By a mesh or a full mesh, it is meant a complete graph as shown here. As discussed earlier, a complete mesh would take a lot of cable. But at the same time with complete mesh we have lot of alternative paths from any node to any other node. The diagram shows eight nodes and each one of them is connected to every other node now. Between any two nodes, you will have direct path and indirect paths as well. We have many paths for going from the same source to the same destination. This means that when one of the links breaks, all of them can  still communicate without any problem. As this arrangement takes lot of cables, we do not use this. What we use is a partial mesh. In a partial mesh some of these links would be dropped, taking care that none of the nodes becomes disconnected. But all the links of the mesh need not be there even in the partial mesh. We can still have some form of redundancy and thus have some level of fault tolerance. But it may be not as much as one available in a full mesh. For such a partial mesh the network topology is not well  specified and it is difficult to evolve a protocol like token passing protocol.                                                                   So here you have to have some other method of sensing the faults and correcting them. That becomes more complicated but the partial mesh topology is used in many places. (Refer slide time: 38:49 - 39:55)


Mesh Topology of course is not common on LANs. These are most often used in WANs to interconnect LANs because WAN links usually are costly. The users of a WAN link naturally expect a certain grade of service. That is why the WAN service providers usually keep alternative paths. If one of the paths becomes unavailable due to some node failure or due to some link failure then it can still send the traffic across the WAN through some alternative paths. (Refer slide time: 39:56 - 40:35)

So the pros and cons of mesh topology are like this. The advantage is of course it improves fault tolerance and it can carry more data. The disadvantages of course are increased cost installation, difficulty in management and troubleshooting. (Slide Reference time: 40:41 - 41:20)
                                                    

Now we have seen that each of these topologies has advantages as well as disadvantages. Rings ease synchronization, but it may be disabled if a cable breaks. Star is easier to manage and more robust but it requires more cable. Bus requires fewer cables, may be disabled if cable breaks and is not so reliable. Bus would not have fault tolerance really. So as far as the LAN is concerned, people are gravitating towards star topology. That means some nodes or some computers may be connected to a switch and then some other computers may be connected to some other switch and then these switches will get connected in some fashion. Extended star topology is the most common topology as this is emerging in local area networks. In wide area networks we have both ring as well as mesh topologies. (Refer slide time: 41:21-46:12)




 


The physical and the logical topologies may be different. Something will look like a physical star but it may actually be a ring. Let us look at the following examples. Suppose you have this as the hub of the star and we have these three stations A, B, and C; we have say cable ducts running over there like this, through which a number of cables are passing. At the central hub you might connect two cables like this and then more pairs of cables in a similar manner. Because this is one central place you can do this. What you essentially have is a ring. What you essentially have is a ring. Similarly, you can have any other kind of topology. Suppose you have a number of cables coming in like this. Now if we have a connection like this and similarly in all other places, then you can actually have a mesh. Although all this is within one physical duct and another physical duct going to nodes A, B, C, and D, it looks very much like a physical star but the actual connection is something like a mesh. Just as I could make a star, I could make it a mesh also. The media is the physical topology whereas the way in which data access the medium and transmits packets is the logical topology. A LAN at a network is not always revealing. Cables emerging from a hub do not make it necessarily a star topology. It may actually be a bus or a ring. In a star topology, there is some piece of network box, which is called a hub. This hub actually replaces the single cable. This is again a shared medium. If they are connected like a star it has got the star connection. People use hubs these days but they do not use the single cable like thin LAN and thick LAN. Though it looks like a star, depending upon what kind of box it is, it could be actually a bus. Similarly as I have shown you, it could be a ring also. (Refer slide time: 46:14 - 47:27) 


Now the choice of logical topology is going to affect the physical topology and of course the other way around. The kind of physical topology you will have will also dictate the kind of logical topology that you can have. The kind of logical topology you can have will depend on two things ? one the actual physical topology, the number of cables you have etc., another is the type of network boxes like switches, hubs or whatever you have as the nodes. We have to design carefully because it may be difficult to change the part way through the installation. When you think about designing a network, you have to put the physical topology min place to get the correct logical topology. Your choice will determine cable installation network, network connections and protocols and spots where you will drill holes in the building. (Refer slide time:  47:33 - 50:48)

The different factors which you take into account while deciding on a particular topology are: the first thing that comes is what kind of technology you are going to use ? there are different kinds of LAN technologies ? for example, if you are using FDDI, then you have to have a logical ring somewhere. If you are using the Ethernet, you may have an extended star topology. The kind of technology you are using is a factor and of course this technology will have some cost benefit. As a matter of fact, in a LAN, you will find that apart from the network boxes (in Greenfield kind of situation), the cost of the cable is also significant. Then there is a question of scalability. Often what people will do is that, they will work with one kind of network and then, very soon, the network will get choked or people will want more bandwidth or more users will join. This keeps on happening. A network is never a stable thing in any organization; but whenever you are putting some kind of topology or some kind of technology you have to think that 3 years to 5 years down the line, you will have to scale the network up. You may have to change the number of your network boxes; some of the boxes for example in old technology you may have had a bus. By putting in a hub it was a physical star, but it is a bus now. You put a switch over there and make it a star topology kind of thing and then later on people want more bandwidth. When more people want to join, you have to design your physical topology that way for scalability, the bandwidth capacity and the ease of installation because how you take the cables around and how you scale up down and how you can install it are always problems. Ease of finding fault is another important factor. We will talk more about it in the next class. It is good to have a central cabling plan where all cables are coming. You can manage all your cables in one central place .There are standards like structured cabling which are used for easy fault finding and maintenance. 
Thank you! 

Preview of next lecture
Good day!  Today we will be talking about multiplexing (Refer slide time:  51:15 - 51:18)


 (Refer slide time:  51:25 - 51:32)

Multiplexing is about sharing a medium. Different users share the same                         medium for communication at the same time.(Refer slide time:  51:40 - 52:00)

A medium can normally carry one signal at any moment because if there are two signals over there they are going to interfere and then the signals will get garbled .For multiple signals to share one medium, the medium must somehow be divided to give each signal a portion of total bandwidth. A particular frequency range around one particular frequency is called bandwidth and this bandwidth is the most valuable resource as far as communication is concerned. We try to use this bandwidth somehow to facilitate the communication between a number of pairs of senders and receivers.That is the idea of multiplexing. There are various reasons why we want to use multiplexing and the chief one is that of transmission services which are 
(Refer slide time:  52:43 - 53:35)

very expensive . In leased lines packet switched networks, for example, laying of line in itself  fairly expensive and complex proposition and once you lay a line you really like to utilize it to the maximum. The other thing is that, if you can use that for the maximum amount of communication multiplexing and compression techniques save a lot of money for the business. When you can send a lot of data through the same line the data capacity of the line increases It becomes more cost effective for the company .Most data devices individually require  modest amount of data but with a number of users their aggregated  requirements  may occupy quite a substantial bandwidth (Refer slide time:  53:52 - 54:03)


 The current technique to accomplish this include frequency division multiplexing, wavelength division multiplexing, time division multiplexing and code division multiplexing. (Refer slide time: 54:04 - 54:49)

In the scheme of multiplexing , you have one multiplexer and then you have n  inputs on one side .So these n inputs are coming to the same multiplexer. They are getting mixed  in some fashion and they are being sent over the same physical link .On the other side, depending on how you have put them together, they are separated into different lines. These different lines can now go to different recipients. On the left we have different senders and we have different receivers over here. A number of sender receiver pairs are utilizing the same physical link in between (Refer slide time:  54:50 - 55:32)

The alternative to multiplexing should be direct point to point connection. This has number of problems .First problem is that you need those lines that we need for each device. You thus need a large amount of wiring. If they are on different floors, you need large number of IO ports on the computer side, which really is not feasible. You may have a few IO ports, but you cannot have hundreds of IO ports. It is really difficult to have hundreds of IO ports .That is also another bottleneck . (Refer slide time: 55:33 - 55:54)

 In a somewhat older approach (multi drop line), the host polls machine to see who wants to send and then uses the same lines. Total communications load is not greater than the rate of line. (Refer slide time:  55:55 - 56:15)
 	
Using a multiplexer approach all the device data?s are multiplexed on one side, sent through one a line. Out link carries multiple channels of information (Refer slide time: 56:16 - 56:20)

The signal will be restored depending on the quantum of noise present .If the noise is  high then  we will lose some information. There will be some distortion and some loss of information would be there. In case of digital signals it is more fault tolerant and more resilient compared to analog signals (Refer slide time: 57:01 - 57:17)


We can correct errors in transmission. Using digital signals we add few bytes of error checking information and can ask for retransmission if an error is detected. I will give you  a simple example : suppose we are sending groups of eight bits called byte Now what can we do is that associated with each byte we can add an extra byte and this extra byte would make  the number of ones in whole nine bits to be odd . This is an example of odd parity. Now if at the receiver end we find that the number of ones have become even we know that, some bits must have flipped that means some bits must instead of zero have got a one and instead of one we got a zero. It must have happened otherwise because the number of bits are supposed to be odd and of course by adding one bit we can always dictate that the number of bits in the original. That means the sending station was odd .In that case  the receiver may request the sender to retransmit the whole bunch once again hopefully there will not be error next time This is of course a very primitive kind of error checking. There are more sophisticated error checking methods employed in networks and communications (Refer slide time: 58:40 - 58:48)


COMPUTER NETWORKS
PROF .SUJOY GHOSH
INDIAN INSTITUTE OF TECHNOLOGY
I.I.T. KHARAGPUR
Lecture No. # 3
 Physical Medium - I
Good day. In this third lecture we will be looking at Physical Medium. In the last lecture we have seen two protocol stacks, we have seen OSI protocol stack in detail and we had a look at its CPI pre reference stack. But whatever be the model, at the bottom-most layer you have the physical medium because, after all, if two computers have to communicate, there has to be some physical medium between them. So we will just look at some general characteristics of physical medium and how data can travel along them, today. In the next lecture we are going to look at some specific media which are used in networks. (Refer slide time: 01:36 - 2:26) 


As we talk about the Physical Medium, we have to talk about two different modes of communication, so to say. One is Analog and the other is Digital, so you have before you that top one is the analog signal; you can see that it can take any shape, and the digital signal is some kind of signal, which represents a series of ones and zeros ? stream of bits, but analog signals of course can take any value in them. (Refer slide time: 2:27 - 3:20)


And one important consideration for any medium is its bandwidth. Bandwidth is the capacity of a media to carry information. Now, the total capacity may be divided into channels ? I mean there is one total capacity you can sort of divide it into channels ? and  give different channels to different users; specific portion of the channel may not be used for direct communication but for control purpose and other specific purpose. But whatever it is, any particular medium and technology will have a certain capacity and this is really important because, after all, your speed of communication will depend on the capacity of the medium. (Refer slide time: 3:21 - 5:19 ) 


This inherent capacity of the medium is of course intermittently linked with the kind of signal one particular channel can carry. For example, on the left-hand side, you see some digital signal and just on its right you have something that looks like a bargraph ? actually this is a representation of the same signal in a different way, what this right-hand side says is basically if you have a series of
a Sin x + b sin 2x + c sin 3x + d sin 4x and so on 
 where a, b, c, d represent the values shown by the graph on the right-hand side, if you sum all that sin values up, you will get the signal on the left-hand side. So the channel, if it has to carry the digital signal, has to be able to carry all these harmonics ? harmonics means the sin x constant, sin 2x constant, sin 3 etc. So all these harmonics it should be able to carry; whereas if you look at the second figure b, it is a pure sin wave. So this just has one harmonic, a sin x or something like that. This third figure has two harmonics, say a sin x + b sin 2 x; so depending on the complexity of the wave form that you want to carry, you want more and more bandwidth for the channel. (Refer slide time: 5:20 - 6:24)


Now for this we make a comparison of analog verses digital signals. Digital signals have some basic advantages the most basic advantages is that it is that less error prone that means if there is some kind of distortion in the signal between the source and destination that can be rectified very easily and to limited extent errors can be corrected. Whereas if you are talking about analog signal there is little control over signal distortion of course this also old technology and this has got some limitations in technology in the sense that how many people can use, etc. There are some limitations over here. So digital signals are of course in general preferred, although analog signal still rules in some niche areas (Refer slide time: 6:25 - 07:30)



So let us see what we mean by the statement that digital signals are less error-prone. In this figure we see in the left-hand side have some analog signal and then some noise has  come; that means, some distortion has come and the signal looks like the one below.   When noise affects an analog signal, it is hard to deduce the original signal from this whereas the same kind of noise or a similar kind of noise if it is gets superimposed on the  digital signal as we see on the right, the signal gets distorted, but since we know that the original signal was sort of square shaped, we can deduce the original signal from the distorted signal. So, despite the noise, noise can be eliminated easily in digital communication. That is the greatest advantage of the digital communication. (Refer slide time: 07:31 -8:51 )


And so far as the bandwidth of a channel is concerned, suppose we know the bandwidth (H) of the channel and the number of signal levels (V) being used, what is the maximum number of bits that we could transmit? Suppose we are trying to transmit bits in terms of digital signal; so there is a Nyquist limit. It says that 
maximum bits per second = 2 * H * log2 V
for example if the bandwidth is 3100 Hz and we are using 16 level modulation then the maximum number of bits per second is log2 (16) is four so 4 ? 2 ? 3100, that is 24800 bps 24.8 kbps. So this is the maximum number of bits that we can send through the channel of this particular bandwidth. So the higher the bandwidth of the channel, the higher the number of bits that we can send through it, but depending on the bandwidth, there is always a hard limit on how fast we can communicate. (Refer slide time: 8:52-10:42)


Let us look at this regeneration of the digital signals once more. We say that it is reliable because it can regenerate slightly damaged signals. Since there are only two states, for example, this is the original signal, which is a square, and it is grossly distorted as it is received. This becomes weaker in strength and the shape has also been deformed but we can regenerate it since we know that the original shape of the signal must have been a square one. So we regenerate it as a square one. If there are two states of a voltage, let us say, + 10 volts ? 10 volts and the signal is + 8 volts, let us say, then it is quite possible that this that this + 8 volts is a result of noise, which is superimposed on + 10 volts rather than a result of noise superimposed on ?10 volts. So whenever you get +8 volts, simply make it 10 volts with some electronics and the original signal will be restored. Of course I mean if the noise is very high, there is a limit to how much you can restore. If the noise is very high then it will not be possible then we will know because ? 10 volts also becomes 8 volts in the case of a spike, and then whatever restoration we do will be a wrong one. So we lose some information; there will be some distortion and some loss of information will be there. So there is always a limit but in the case of digital signals it is more fault-tolerant or resistant to noise, more resilient compared to analog signals. (Refer slide time: 10:42 - 12:34)

So we can correct errors in transmission ? that is another advantage of digital signals. So what we do is that we add a few bytes of error-checking information and can ask for retransmission if an error is detected. I will give you a simple example: suppose we are sending groups of 8 bits called bytes; now what we can do is that associated with each byte, we can add an extra byte and this extra byte would make let us say the number of 1s in the whole 9 bits to be odd, this is an example of an odd parity. Now, if at the receiver?s end, we find that the number of 1s have become even, we know that some bit must have flipped. That means, some bit must have flipped; instead of a 0, we got 1 and instead of a 1, we got a 0. It must have happened. Otherwise, because the number of bits was supposed to be odd ? and of course by adding 1, we can always dictate the number of bits in the original that the sending station was odd ? in that case the receiver may request the sender to retransmit that whole bunch once again; hopefully there will not be an error next time. This is of course a very primitive kind of error-checking; there are more sophisticated error-checking methods employed in networks and communication. So this  error-checking and correction of error at the receiver?s end is possible if you have adopted a very sophisticated coding technique. So that is possible in case of digital transmission; this is not possible in analog transmission.  (Refer slide time:  12:36-14:04) 
      
Another advantage is that a digital signal can be encrypted, which makes it possible to carry sensitive information on the network. For example, e-banking is coming in vogue ? that means, somebody does a bank transaction on the network. Similarly e-commerce ? that means you want to buy something online and then pay through your credit card, etc. online. Now how does the transaction take place? The point is that you are sending some information, may be your credit card number, may be some password, etc. Somebody can snoop on you in-between, so they will know your secrets and use the same set of numbers  to may be buy something else, and the money will go from your bank account; that is obviously not acceptable. In this case when we send it down the network, we encrypt it in a way that even if somebody snoops and finds out the stream of bits that is going, he will not be able to make out the original numbers from this current stream. So this is encryption and this is a very important application these days. (Refer slide time: 14:05 - 15:54)  
      
So encryption is another advantage of digital transmission. Compression is another; that means, we can compress a message before transmission and decompress it at the other end. So the overall load on the circuit and the network is lighter; since the load is lighter it is less expensive. Sometimes when you compress information there may be some loss of information; there are lossy compression techniques and they are non-lossy compression techniques. Lossy compression technique is where you may lose some information, but it may not matter. For example, let us say you want to send a digital photograph. A digital photograph may have a very high resolution; if you compress it and decompress on the other end, you may lose a few bits but the image that you get on the other end would still look almost as good as the original one. Of course, trained persons can look at it and find out there is something missing but it will almost serve the purpose.  Sometimes loss compression is acceptable and of course there is loss-less compression also. When you compress it and send it, it is a less expensive way of sending. I mean you are sending the same information but only your are taking less amount of space; of course you have to do the compression and decompression at the two ends, so that is some kind of overhead on the two sides. (Refer slide time: 15:55 - 17:07)

Suppose we have some data; how do we encode this digitally? Well, the simplest way, of course, is by the presence or absence of a signal. Let us say a positive voltage might represent a binary zero or a binary one or vice versa. Maybe if you have high voltage, you say that is I will interpret that is as one being sent and if I have a low voltage I interpret this as a zero being sent. Of course, when I am saying voltage, I am assuming that we are having some kind of electrical signal traveling; that is not always the case. For example, if you are using light for communication the presence of light may indicate one and the absence of light may indicate zero. So whatever we code it, whatever be the medium, this is the simplest wave to encode data into a digital form. The current state indicates the value of the data. (Refer slide time: 17:08 - 18:31)

If your signal is digital, sometimes you are forced to send some digital signal by analog transmission, but this is usually not preferred. Analog signal will transmit whatever signal comes regardless of whether it represents digital or analog data. So it uses amplifiers. Other than the kind of regeneration that is possible in digital voice, the trouble with amplifiers is that it also boosts noise, whereas in the digital case we could take out the noise and regenerate the original shape of the signal. Here we can amplify it, so that if a signal becomes weak we can make it stronger; but whatever noise is coming with the signal that also gets amplified. This is okay for voice, but for digital data using analog transmission usually is not done because we have so little control over the correctness of whatever we are sending. (Refer slide time: 18:32-19:58)


Digital transmission is concerned with the content of the signal; it uses repeaters which recover the pattern of zeros and ones and then retransmits. This can be used with analog signal if it carries digital data. You know sometimes analog signal carries digital data; it recovers the digital data from the analog signal, generates new clean analog signal. This is becoming more standard; for example, when you are connecting to the network, to the internet, from your home computer you may have use the modem and the telephone connection. Now what is happening over here is that the telephone line is essentially an analog medium; so an analog signal will go over there on that, but whatever is coming out of your computer is digital. So what you do is that you make this digital data ride on some analog signals to this modem pairs so that it reaches the telephone exchange. At the telephone exchange it is converted back to digital data and the backbone and the core of the telephony system is purely digital. (Refer slide time: 19:59 - 22:26)

So there are various ways in which you can encode digital information in the analog domain. One is the amplitude shift keying, which means the two binary numbers 0 and 1 are represented by two different amplitudes of carrier waves. This is not very efficient; uses up to 1.2 kbps on voice grade lines; and is used to transmit digital data over optical fiber. In the amplitude shift keying, although the signal is analog we know that there are only two levels, which this signal is supposed to take, i.e., two amplitudes: low amplitude and high amplitude. It may be 0 and some amplitude. If a 1 is to be transmitted may be we give this high amplitude and if a 0 is to be transmitted, we may use a low amplitude analog signal. You may say this is the kind of thing which is done in the optical domain; but in the electrical domain this is not very efficient. More common is the so-called frequency shift keying. In frequency shift keying, we use the two binaries; that means we use two different frequencies for the two binary digits, 0 and 1. So this is less susceptible to error than amplitude shift keying, and again, it sends up to 1.2 kbps on voice grade lines and is commonly used for high-frequency radio. Another way of encoding is the so-called phase shift keying. Two binary numbers may be represented by phase shift of carrier wave; so this is more efficient and noise resistant than FSK; this is PSK. It can be raised up to 9.6 kbps (kilo bits per second) on a voice grade line. This is for a simple phase shift keying; of course, we use more complex phase shift keying with other techniques in order to achieve even higher data rates nowadays. (Refer slide time: 22:27 - 24:01)

These techniques can be combined. So it is common to combine phase shift and amplitude shift keying so we can get about 56 kbps on a voice grade line. The modems that we buy usually are all 56 kbps, connected with some technique called multilevel signaling. Each signal represents more then one bit. So this is also possible. One is the signal, which indicates the level of the voltage or whatever it is, the phase or frequency, the level of the signal; the other is the bit or bits that it may represent. So, for example, if you have a simple technique, it will represent a 0 or a 1 by the current level of the signal, which is the simplest kind of technique, these two are the same. One is the bit rate, that is, the number of bits of information that you are sending; the other is the baud rate, that is, the number of times the signal will change per second ? the maximum number of times the signal will change per second. So in the simplest case, baud rate and bit rate may be the same but that is not always the case. (Refer slide time: 24:02 - 29:29)

So we look at the other side of the coin, which is the digital encoding of analog information. And a very common case is the telephones. For example, as I just now told you, the heart of the telephone exchanges and the telephone switches and the trunk exchanges and the core network of the telephone system are nowadays entirely digital. Whereas our voice, of course, is analog. By the way, the real world, in which we move around, is mostly analog. Now this digital is something that we adopt in-between in order to get these advantages of robustness, resiliency, error correction, etc. Our voice is analog, but the core telephone system is digital. So at some point, our voice, an analog signal, has to be converted into a digital signal. Of course, our handsets also are mostly analog. So they are going to be carried may be in this analog fashion right up to the nearest exchange; but there, there will be a set of codecs, coders and decoders, which will convert this analog voice signal to a digital signal. Now, what kind of digital signal will a voice give? The calculation is something like this: the voice data in a telephone system is limited to a maximum of 4 KHz. This telephone is a very good quality, the first so-called gold quality but this is not high fidelity kind of sound. It is less than high fidelity, but very very clear, for all practical purposes for the telephone at least. If I limit the bandwidth to 4 KHz, we get some distortion, but nothing very noticeable. So voice is still very clear. We have to send this 4 KHz of analog signal, whatever information is there, within 4 KHz bandwidth. We have to send this analog signal to the other side in a digital form; that is, we have to digitize it. There is a Nyquist rate, which is like the Nyquist limit that we have seen some time back. Nyquist rate, which says that if we sample any analog signal at double the maximum frequency that the particular analog signal has, then essentially we will have all the information that is needed to fully reconstruct the original analog signal from this sample. So if you have 4 KHz of signal, 4 KHz of bandwidth, we require 8000 samples per second. Let us say you have this analog voice; so per second we will take 8000 samples, sample values at different points. So, of course, at some points my voice may be low so I may have a small amplitude; at some point my voice may be very high so I can have a large amplitude. Now that we break up each of these into a code of 8 bits, the point is that with the 8 bits, you can really distinguish between 256 levels. And our assumption is that from the 0 level to the highest level that is admissible, we take it; whatever it is, if you break it up in to 256 parts, that is good enough for capturing how high or low the volume is. So a level of 256 is really quite a lot. We require 8 bits for each of the sample and we require 8000 samples. That makes it 8 ? 8, that is, 64 kilo bits per second, 64 Kbps. Please remember this rate ? 64 kilo bits per second ? it is the fundamental unit on which a whole lot of all other technology, etc. has been built. And basically, it has come especially from telephony, because the telephone people had the first network and 64 kbps happened to be the rate which was set for a voice. So naturally, of course, if you want to carry 1 voice channel, we will have a 64 kbps. If you want to carry 1000 voice channels, you will require 64 kbps ? 1000, that is, 64 mbps capacity. So they planned their network depending on how many voice channels they want to carry where; but 64 kbps is the basic unit in most places in telecommunications. (Refer slide time: 29:31 - 32:53)

Now let us take a quick look at how data is encoded. The simplest one is of course using the level of the voltage; for example, over here this is the non return to zero level, the so-called NRZ encoding. It uses two different voltage levels to represent 0s and 1s; typically negative voltage equals 1 and positive voltage equals 0 and the signal never returns to 0 voltage. So it flip flops between a negative voltage and a positive voltage. The value during bit time is a level voltage; that means, during the duration of a bit 0 or 1 the voltage really remains the same. So there is a transition at the boundaries of bit and of course, if there are two 1s then there will be no transition as we can see; if there are two 0s consecutively, then there will be no transition. So the number of transition is much less and it is a very simple kind of scheme. This has some problem. We will come to that later on; but let us look at another variant of this non return to zero, which is NRZI. That means non return to zero and invert on 1s. That means once again we keep constant voltage during bit time ? no transition represents 0 and transition from low to high or from high to low is a 1. So the same bit pattern, 0 0 1 1 0 1 is represented by a different voltage pattern over here. There is no transition 0 0 and then there is a 1, so there is a transition; there is another 1, there is a transition; 0 no transition; then again the 1, then another transition. So this is another way of encoding. Now the trouble with these encodings is that suppose you have a large number of 0s, a large train of 0s, there is no transition really. By the way, there is one point ? the reason they use the positive voltage and negative voltage rather than a positive voltage and zero voltage or zero voltage and negative voltage is that you want to distinguish between a 0 being sent and no signal being sent. So that is one point and the other point is that as I was saying if there is a constant train of 0s in the original data then we will have a constant level signal in the system. Now if that be so, the synchronization of the clocks between the sender and the receiver tends to drift off and then the receiver, I mean in the worst case, the signal may go one off or it may interpret the signal in a very wrong fashion. So this is the problem of this NRZ and NRZI, which is taken care of (Refer slide time: 32:54 - 33:46)


by some other kind of encoding; we will come to that later on. So this is a just a quick look at the disadvantages of NRZ code ? hard to tell where one bit ends or starts; with long string of 0s or 1s, any drift between timing of transmitter and receiver results in errors. So there is a bit phase encoding, which uses the least transition per bit time. So if you use at least one transition per bit time, what would happen is that the sender and the receiver clocks can remain in very close synchronization with each other ? that is a nice property ? and there is predictable bit transition during each bit time. So an absence of a transition indicates an error. So two examples are Manchester and differential Manchesters (Refer slide time: 33:47-34:29)

So this is Manchester encoding. You see that at the middle of each bit 0 or 1, 
there is a transition. Now, low to high transition means 1 and high to low transition means 0. So the same pattern ? 0 0 1 1 0 1 ? is there for each bit; you will see there is a transition in-between, and low to high transition is 1 and high to low transition is a 0. So this is the encoding, which is used in Ethernets and many LANS (Refer slide time: 30- 35:42)

There is another variant of this called differential Manchester; in this, the midbit transmission is really for clock synchronization purposes. They do not represent either 0 or 1. So what it does is that transition at the beginning of a bit period is equal to 0 and the absence of a transition is bit period 1. So 0 0 ? when this 0 starts, there is a transition. Transition at the beginning of the bit period is a 0; but when the 1 starts, there is no transition. There is a transition in the middle anyway; that is for clocking purpose. Then at the next level, once again if there is no transition if there is a second one coming, there is no transition and so on. This was used in token ring networks; of course token ring networks are becoming less popular these days but anyway, this is another one. There are other kinds of encoding techniques, but these are the common ones. So you see that there are different ways of encoding the digital data into these different signal levels. (Refer slide time: 35:43 - 38:29)

Signal levels and bit rates are different. The rate at which the signals change is the baud rate, and the rate at which bits are sent is the bit rate. The other thing we can consider is the analog encoding of analog information. Of course, this does not come too much in computer networks, but this is an important area in communications in general. So of course, if you have an analog signal, it can directly be sent. Sometimes it is sent in the raw form; sometimes that is not very practical, so we have to convert this to an analog signal maybe at higher frequency, and the most common example of this is the radio. For example, a radio is carrying voice or may be some sound, etc. The voice, as you know, is only 4 KHz, but if you take a 4 KHz signal and try to send it over the atmosphere, it will not reach any where, excepting in its close vicinity. So, in order to reach out to all these different locations, you have to send your radio signal at a very high frequency; but the information that is it carrying is still at that 4 KHz bandwidth. So this 4 KHz bandwidth is sort of translated; so there is a very high-frequency so-called carrier wave, which is carrying this 4 KHz of signal if it is only voice. Of course if it is music, etc., you may like to have a somewhat higher bandwidth also; but anyway, that is also analog signal and this is also analog signal. So one analog signal is carrying the other analog signal and there are various ways of doing this so-called modulation. The three most common ones are amplitude modulation, frequency modulation, and phase modulation. So we will not talk much about this because in amplitude modulation as you can understand, the amplitude of the carrier wave is changed according to the signal; similarly, in frequency modulation the frequency of the carrier wave is changed according to the signal; and then in PM the phase is changed according to the signal. (Refer slide time: 38:30 -39:58)

So this is another picture of digital signaling so called current state; that means zero represents the low voltage one is represented by the high voltage and digital signaling and state transition; so if there is no transition that is zero if there is a transition that is the one so these are all different examples of encoding. (Refer slide time: 38:59 - 39:35)	










As I was telling, the bit rate and the baud rate are different. The maximum number of times a signal can change in a second is called the baud rate and the number of bits, 1s and 0s transmitted in a second is called the bit rate. In the examples we have seen so far, the bit rate and baud rate are the same; this is not always the case. The bit rate can be higher than the baud rate if we use more then two signal levels; and the other one is also possible: the bit rate may be lower than the baud rate. (Refer slide time: 39:36 - 41:15)

Now let us look at some of the inherent trouble with all these physical media. That means, when we are throwing some signal, when we are trying to send some signal down some transmission medium ? it may be a piece of wire it may be this whole atmosphere or it may be a fiber optic cable ? there will be some kind of distortion, some kind of change in the original signal, that will inevitably come into play. So we will look at some of these; we have to really take care of all these. These signals and these kinds of effects cannot be eliminated. I mean the world is not perfect and they are basic properties of the medium so we will have to design our systems in such a way that in spite of all these efforts the whole communication will still go on very correctly. As a signal propagates, the signal changes as it travels; so the receiver may not be able to recognize it as such. For example, this shows an example of an original signal and the final signal. This kind of change and distortion etc. may be attributed to the following effects: (Refer slide time: 41:16 - 42:51)

the first one, the very first one, is attenuation. A signal get weaker as it propagates; attenuation becomes greater with distance naturally, and of course, finally, the signal will become too weak to recognize. So the original signal strength is something but as the distance grows signals become weaker and weaker. For example, let us say this kind of weakening is dependent on the property of the medium; it also depends on the kind of frequency you are using for sending. For example, let us say an AM signal will not be received at a very large distance, whereas a short wave signal may travel some other way and come over here. Similarly, if you take a local area network or a cable, then if you make the cable very long ? there are other technical problems for making cable very long ? but even from the signal propagation point of view, the signal will get weaker and weaker. Because there will be leakage currents in the cable, current will leak out, etc. Finally, the signal that we will get will be very weak. So this is a very fundamental property of any medium. Similarly, if you take an optical medium and if you send a light pulse through it, as it travels, the light pulse will get weaker ? that is one of the effects. There are other effects, but this is one of the major effects. (Refer slide time: 42:51 - 44:27)

Apart from attenuation, there is distortion. That means, the signal changes shapes as it propagates. Now if it changes shapes in this way, the adjacent bits may overlap, may make recognition impossible for the receiver, so what might happen if the signal changes shape? Why does it change the shape? If you remember in the very beginning I had shown you one slide of a square wave kind and we talked about some harmonics. So the square wave may be looked upon as a sum of all these harmonics, a sum of all these sign waves. Now all the harmonics or sign waves are of different frequencies ? we had talked about the sin x sin 2 x and sin 3 x and so on ? so they are of different frequencies. Now signals of different frequencies change or get affected by the medium differently. So a signal of a particular frequency may be attenuated less; the signal at a much higher frequency may be attenuated much more or vice versa. The point is that signals at different frequencies will get affected by the medium in different ways. So finally, the relative strengths of the different frequencies will be different when you go to the far end; that means, when you go to the receiving end, when you add that all up, you will get a different shape. You will not get the old square shape or whatever shape you have sent; you will not get that back. So this is called distortion. (Refer slide time: 44:27 - 45:31)

Then, of course, it is going to pick up noise. There is thermal energy in wire, which is everywhere. So this energy is some kind of inherent noise that will always come; then there will be random signals. So spikes may sometimes occur; say, some car is starting somewhere so there will be some static discharge; the lightning strikes somewhere, there is somebody running a motor, somebody is running a washing machine, all these things add to the general noise in our atmosphere and this medium, which is carrying a signal, will also catch some of this noise. Sometimes of course, the noise is so sharp that it may  change the transmission in such a manner that you will not get the original signal back. So sometimes such a thing may happen. (Refer slide time: 45:31 - 46:47)

What we want is a high signal-to-noise ratio in order to distinguish between the signal and the noise. If the noise is very low and the signal is high, we can still distinguish at the receiving end between what is a signal and what is a noise and take the signal out, take out the noise. But for this we want a high signal-to-noise ratio. Signal strength divided by the average noise strength is the SNR and as SNR falls, error will increase. So in this figure, you can see that as the signal becomes weaker and weaker and touches the noise floor may be goes below it. When it goes below it, the noise becomes more than the signal; then no meaningful communication can be carried out. So in general, we want a strong signal-to-noise ratio. So one thing we might do quite often is that suppose we do have to send a signal to a very long distance, the signal will naturally get weak. So in-between, we will put some amplifiers or repeaters or regenerators, which will take the signal, try to filter out the noises as much as possible, then amplify the signal and make it stronger, and then push it for the next span. So after some distance, again another repeater will take it up and regenerate the signals. We have to go on doing that so that I maintain an acceptable signal-to-noise ratio even at the receiving end. (Refer slide time: 47:26 - 47:38)

Then of course, as I said that, there is interference. That means energy from outside the wire adds to signal, like noise, which is often intermittent, and is very hard to control or diagnose. (Refer slide time: 47:38 - 48:49)

One important point is that a very strong interference can occur at cable termination. So there are two things: first of all, if two cables are running in parallel, what might happen is the signal in one may affect the signal in the other. So there is some kind of linkage between the two the so-called cross talk ? that is one thing. This is more pronounced at the termination end; at the termination end, if you do not terminate the thing properly, if you just leave it like this, the kind of distortion and the kind of noise that will be introduced will be much more. So whenever we are putting some wired kind of network infrastructure, we have to be very careful about the termination, because that is where a lot of noise can come in. So often there will be multiple wires in a bundle; each radiates some of its signal, causes interference in nearby wires and this is especially bad at terminations where wires are unwound and are parallel. (Refer slide time: 48:49 - 49:36)

Finally I just mentioned this, that when carrying digital data over analog lines, we require modems. Modem stands for modulation and demodulation. This is used to connect a digital computer to an analog phone system usually, but modems can also be used elsewhere. It can be installed internally or a card inserted into the motherboard or can be connected to the serial port, etc. So now we understand why modems are required because essentially, digital data will have to be carried on the analog system. The field of communication, as you know, is a very interesting field and we could not go too much into the details of communications. But we have just talked about the very basics of communications used in computer networks, because remember computer network or some computers or some network nodes are connected via some communication lines. So communication is absolutely important; it is all-important in this; so we have talked about this.In the next lecture, we will look at some of the medium that is actually used or deployed in common networks. Thank you. 


Preview of next Lecture
Lecture No. #4
Physical Medium - II

Today we will ] have the second lecture on the physical media in the last lecture we have seen the different ways  the digital signals and analog signals, etc how they can be used for communication how digital data or analog data can be encoded and some of the general  impairments of physical medium. in this lecture we are going to look at specific media which are used in networking (Refer slide time: 51:18 - 51:20)


 (Refer slide time: 51:21 ? 52:30)


so there are basically two types of  mediawhich are used for communication one is cable i mean they are two different classes are together so one is a cable  now this cable  could be some  copper cable they could be coaxial cable or twisted pair or shielded twisted pair so the twisted pair I mean we sometimes there actually unshielded twisted pair or UTP sometimes we simply called them twisted pairs since they are more common there are shielded twisted pairs also and then  of course the one of the most important cables these days is the fiber optic these are the different types of cables that we use in networks. similarly we can also communicate without the use of any cables by simple electromagnetic radiation and these radiation could be in different ranges of the frequencies like they could be infrared or microwave or radio or satellite. etc So we will look at all these  one by one (Refer slide time: 52:31 - 55:25)



first let us look at the kind of  media that we use in a LAN. LAN as you remember is a local area network and a local area network could vary from let us say one meter to one kilometer so its is local strictly by some means of size in some way so one meter means two computer connected siding on the same table connected side by side it could be all the  computer in one building and all the  computers in several buildings and so on so that is the kind of range that we have for LAN . and the medium that we use for LAN they could be coaxial cables, UTP, fibers, wireless so you would note that basically most of the types of cables that we talk about find their way  in some ways or other  in local area networks. and of course UTP or unshielded twisted pairs are of different varieties or categories so that cat is for categories so we could have cat3 cable cat 5cable, cat 5e cable, cat 6 cable, cat 7 cable and so on .You must have seen cat 3 cable if you looked all your telephones that is the landline phones they are connected by unshielded twisted pair cables of categories three.  for computer networking I mean cat three cables could also be used for LAN although we usually prefer cat five onwards.  the kind of things  that are on the network that could be PBX, PBX is your local telephone exchange that could also have some digital ports etc. Ethernet is one technology which is used in LAN ATM is also used, FDDI is sometimes used FDDI if you remember is fiber distributed  that is the FDDI ring kind of structure. so  there are some networks even smaller than LAN. they have called personal area networks some networks so i may have be varying three or four computers and they or connected in a network on my body. o we will come to pan later on but  going on the other side of the spectrum that means from LAN to a bigger network this is man that is a metropolitan area network. typically one campus or one city may be several small towns which are closed together they would come under the preview of the MAN, so MAN by ballpark figure man would be extent of about ten kilometers, so it could be 5 kilometers or it could be twenty kilometers so something in that range (Refer slide time: 55:26 ? 57:48)

and the kind of medium that we use for a metropolitan area networks coaxial cables both base band and broadband, we will come to this what they are. sometime UTP are used but that is not very preferred way of  connecting although UTP are  used one way UTP is definitely used are when you try to network to telephone lines. That is I mean telephone linesi  told you they use UTP. it could be a hybrid kind of system, some part of it could be coaxial some part of it could be UTP and some part of it could be fiber. fiber means , fiber optic cables. and kind of things of course that kind of technology that goes into a MAN would be PBX different kinds of modems for example if you are using a broad band coaxial cable you might use something what is known as a cable modem . then you have this long  Ethernet that means on the Ethernet on the over may be some lines cable tv DSL digital subscriber lines or it could be wireless. one issue which is important in MAN is the issue of access alright that means how do you reach to each individual users.I mean  think of a town or a city so you  really have to in order to network the entire town you have to reach to each individual residence and reaching to each individual residence involves cost ok, if it is a small building like in a LAN you can take very  good say UTP cable to his room . now there are problems of taking UTP cable to each individual house I mean  there could be i mean electromagnetic disturbance and things like that  but apart from that there is a question of cost. Ok if you are trying to build so much copper into that is quite costly. so how to access to users who are distributed over somewhat wide area in the sense of several kilometers that is an issue in metropolitan area networks (Refer slide time: 57:49 - 58:41)

Finally at the extreme of the spectrum we have WAN that is wide area network so they may stretch from ten kilometers to may be 10000 kilometers, so 10000 kilometers almost to the other side of the globe.  so they can involve very very long distance . medium is usually fiber or satellite and the kind of technology that is used in WAN are SONET or SDH that the almost similar technology ATM IP that is the internet protocol, DWDM dense wavelength division multiplexing, Geo and Leo Geo stands for Geostationary satellite is a short form of Geostationary satellite and Leo is low art orbiting satellite. 
COMPUTER NETWORKS
Prof .SUJOY GHOSH
Dept of Computer Science & Engineering
I.I.T., Kharagpur
Lecture No. #4
Physical Medium - II
(Refer start time: 0:45)
Good day. Today we will have the second lecture on the Physical Medium; in the last lecture we have seen the different ways these digital signals and analog signals, etc. can be used for communication, and how digital data or analog data can be encoded. Well, we are mostly interested in the digital form of data, how they can be encoded etc., and some of the general impairments of physical medium. In this lecture we are going to look at specific media, which are used in networking. (Refer slide time: 01:33 ? 01:34) 

Refer slide time:  01:34 ? 02:43)



There are basically two types of media, which are used for communication: one is cable. They are actually two classes of cable. Now this cable could be copper cable or could be coaxial cable or twisted pair or shielded twisted pair. The twisted pairs are actually unshielded twisted pairs or UTP. Sometimes we simply called them twisted pairs since they are more common; there are shielded twisted pairs also. Then, one of the most important cables these days is the fiber optic. These are the different types of cables that we use in networks. Similarly, we can also communicate without the use of any cable, by simple electromagnetic radiation and this radiation could be in different ranges of frequencies, like infrared or microwave or radio or satellite, etc. We will look at all these one by one. (Refer slide time: 02:44 ? 05:09)

First let us look at the kind of media that we use in a LAN. LAN, as you remember, is a local area network that could vary from, let us say, 1 m to 1 km. So it is local strictly by its size; 1 m means two computers connected on the same table or connected side by side; it could be all the computers in one building and all the computers in several buildings; and so on. That is the kind of range that we have for LAN. The medium that we use for LAN could be coaxial cables, UTP, fibers, and wireless. Most of the types of cables that we talked about find their way in some niche or other in local area networks and, of course, UTP or unshielded twisted pairs are of different varieties or categories. We could have Cat 3 cable, Cat 5 cable, Cat 5E cable, Cat 6 cable, Cat 7 cable and so on.You must have seen Cat 3 cable ? if you have looked in your telephones that is the landline phones, they are connected by unshielded twisted pair cables of category 3. For computer networking, Cat 3 cables could also be used for LAN, although we usually prefer Cat 5 onwards. The kind of things that are on the network could be PBX; PBX is local telephone exchange that could also have some digital ports, etc. Ethernet is one technology, which is used in LAN; ATM is also used; FDDI is sometimes used. FDDI if you remember is fiber distributed, that is, the FDDI ring kind of structure. We have network boxes like hubs, switches, routers, etc. So these are the terms you would come across. Of course we would talk about some of these in more detail later on, but these are some of the technologies that come into LAN or local area networks. (Refer slide time: 05:09 ? 07:22)

Then we come to slightly bigger networks, called MAN or Metropolitan Area Networks. There are some networks which are even smaller than a LAN; they are called Personal Area Networks. I may be wearing three or four computers and they are connected in network on my body. So we will come to PAN later on. Going on the other side of the spectrum, that means, from LAN to bigger network, which is MAN, a metropolitan area network, typically one campus or one city may be several small towns, which are close together. They would come under the purview of a MAN. MAN is a ballpark figure; it would extend may be about 10 kilometers ? so it could be 5 kilometers, or it could be 20 kilometers or something in that range. The kind of medium that we use for metropolitan area networks is coaxial cables, both base band and broadband; we will come to what they are. Sometimes UTP are used but that is not a very preferred way of connecting. One way where UTP is definitely used is when you try to network through telephone lines. UTP  could be a hybrid kind of system wherein some part of it could be coaxial and some part of it could be UTP and some part of it could be fiber. Fiber means fiber optic cables. the kind of technology that goes into a MAN would be PBX, different kinds of modems, for example if you are using a broadband coaxial cable you might use something that is known as a cable modem. Then you have this long-reach Ethernet that means Ethernet over may be some lines, cable TV DSL digital subscriber lines or it could be wireless. One issue which is important in MAN is the issue of access. That means how do you reach to each individual user? Think of it as a town or a city; you really have to. In order to network the entire town you have to reach to each individual residence and reaching to each individual residence involves cost. If it is a small building like in a LAN you can take  UTP cable to his room. There are problems of taking UTP cables to each individual house; there could be electromagnetic disturbance and things like that but apart from that there is a question of cost so if you are trying to build so much copper into it that is quite costly. So to access users who are distributed over a somewhat wide area of several kilometers that is an issue in metropolitan area networks. (Refer slide time: 08:32 ? 10:08)

Finally, at the extreme of the spectrum, we have WAN, wide area network. So they may stretch from 10 kilometers to may be 10000 kilometers or almost to the other side of the globe. They involve very very long distances. The medium is usually fiber or satellite and the kind of technology used in WAN is SONET or SDH. It?s almost similar technology ATM, IP ? that is the internet protocol ? DWDM, dense wavelength division multiplexing, Geo and Leo. Geo stands for Geostationary satellite and Leo is low art orbiting satellite. So these are the kind of technologies that go into WAN. One issue in MAN is of course always cost and bandwidth, WAN bandwidth because as you can imagine that when you have to take some message across hundreds of thousands of kilometers away there is a lot of cost involved in it. So WAN bandwidth is usually costly and we try to make the maximum use of this bandwidth. Of course in recent times lot of fiber has been laid internationally so the bandwidth problem is becoming less and less, but even then WAN bandwidth remains costly. (Refer slide time: 10:08 ? 12:43)

Now let us see some of the cable specifications that we use, first we take up this UTP cable of category 5 so CAT 5 UTP cables specification says about 100 MHz and these can stretch up to 100 meters. These are the figures which are normal, you can stretch things little bit this way or that way at times depending on your operating circumstances, etc. but this is normally the figure; so you can have some kind of idea from these. The CAT 5 cable looks like your standard telephone cable; the only thing is that they are made to a slightly better specification. The connector at the end of a CAT 5 cable has got a specific scientific number; this is called RJ 45. So we use RJ 45 connectors for connecting CAT 5 cables. There is a standard ANSI or TIA or EIA standard for how CAT 5 cable is to be used. What are the standard ways and how they are twisted, etc. There are a number of wires inside the twisted pair of cables; so which wire stands for what, etc. I simply refer to the name of the standard here, which is EIA 568 A and then a cable has to be tested according to some standards; TSB 6795 is the standard for testing cables. CAT 5 cable is usually under these circumstances 100 MHz to say 100 meters. If you are within the 100 meters, if you have done everything properly it is expected that it can work up to 100 mbps. The point is that if the distance is much less than that then naturally the same cable can work at a much higher speed. So inside the same room the CAT 5 cable may be used for 1000 mbps connection between computers or network nodes. (Refer slide time: 12:43 ? 13:26)

There is some enhanced version of CAT 5 cable; I will not go into the details of this, which are given over here. The specifications are almost the same; only thing is that this is slightly better in some way. And it can work for 1000 base TX for small distance. I said within a room if you are using CAT 5 e cable instead of simple CAT 5, you may be able to operate at 1000 mbps without much of a difficulty. (Refer slide time: 13:27 ? 15:24)

Then we have the CAT 6 cables, which is again slightly higher than CAT 5 cables here: the specification is 250 MHz. this specification is much higher; so naturally CAT 6 cable would be somewhat costlier than CAT 5 or CAT 5 e cables. The same kind of connector is actually the recommended cable for operating at 1000 mbps. So here is a point. When networking started, people thought that 10 mbps is a fairly high speed. And soon people were using networks a lot and 10 mbps was not enough, so people went to 100 mbps. Now people are saying that 100 mbps is slow; so we want to go to 1000 mbps. You see that in a comparatively short period of time, may be 15?20 years, our demand for bandwidth has grown a thousand-fold. When you are planning for an infrastructure, say copper infrastructure, in any organization or somewhere, then you should keep this in mind that whatever specification people are giving you today, after some time people may want to upgrade. What I mean to say is that if you put in CAT 6  kind of cables and use it for only 800 mbps today, then 1 or 2 years later you can use the same cable for 1000 mbps saving a lot of cost at that time; but of course, that would mean a higher upfront cost today. So that is always there. (Refer slide time: 15:25 -16:44)

So for testing these kind of cables there are all kinds of testing parameters. I will not go to the details of these at all. This is a rather technical part. Return loss is a measure of the reflected energy caused by impedance mismatch in the cabling system. When you put a cabling system you can test it. And one of the things you might test for is return loss and another thing you might test is a cross talk as reflected from the other end. ELFEXT is defined as the measure of signal coupling from a transmitter at the near end into a neighboring pair measured at the far end, relative to the received signal level on the same pair. These kinds of testing parameters are there for testing copper cables. Similarly if we have put fiber optic cable somewhere, then fiber optic cables would have some other testing parameters. It?s a good idea to test the passive networking infrastructure, meaning all these cables and connections etc., and how they perform. (Refer slide time: 16:45 ? 18:23)


Now from unshielded twisted pair cables, we go to the base band coaxial cables. We have already mentioned about the base band coaxial cables. These are the cables we were talking about when we talked about the bus topology if you remember; so it is basically a shielded cable. There is a copper cord surrounded by an insulating material, which is surrounded by braided outer conductor, which is surrounded again by a protective outer covering. So this is considerably thicker than a UTP. And then in the coaxial cables also there are two varieties, the so-called thick coaxial cable and thin coaxial cable. This technology is becoming obsolete now, but just for our knowledge, a thick coaxial cable is also presented as 10base5. This 10 represents that it works at 10 mbps and 5 represents that it can go up to 500 hundred meters. The thin coaxial cable is 10base2, so it works at 10 mbps and goes up to 200 meters. As I told you, nowadays almost everybody wants a 100 mbps to their desktop, in which case this thick and thin coaxial cables, that is the base band coaxial cables, are no longer of much use today. (Refer slide time:18:23 ? 20:57)



But another type of coaxial cable, which is very much in vogue and which is used even today, is the kind of cable that your cable operator puts. Many of you have seen that cable operator puts some kind of coaxial cable into your room, which goes into your TV. Now it is possible to use the same infrastructure for computer communications also, i.e., computer networking also. Remember, I told you while discussing MAN that one of the most important issues in metropolitan area network is the issue of access; that is, how do you reach each individual subscriber at his or her home? This cable operator has reached lot of homes anyway. So naturally, people started thinking whether we can use these cables from the cable operators for network communication, and special kinds of modems were designed for that called cable modem. So we have a broadband cable infrastructure rather than the base band that we were talking about. This is quite often used and now they are being deployed in many cities for computer communication. This is, as I said, analog signal carrying TV signals and has been used for carrying data traffic. Older cables carry up to 450 MHz and the newer cables that have been put can go to higher frequencies, that means, higher bandwidth, and these cables use amplifiers at regular intervals. Now these amplifiers in the old days were only one-way amplifiers because the way the cable operators wanted it is that they are broadcasting their signal from some central location and it just has to reach people?s TV. But when you want to use this for computer communication the communication has to be two-way so the amplifier also has to be a two-way amplifier. So such small technicalities are there; it requires an amplifier at regular intervals and is a shared medium. Usually this medium is shared, that means, you and your neighbors ? all your data is in the same medium. (Refer slide time: 20:57 ? 22:27) 



As I said, for carrying data, two-way amplification is necessary. Dual cable systems are like two trees, both rooted at the head-end; the head-end is where all the service providers? equipments etc. are placed. So there are all kinds of split systems, they use 5 to 30 MHz for inbound traffic and the rest for outbound traffic; mid-split system and there are all kinds of systems for using this for data communication. By the way, there is also some asymmetry that people have found in the network traffic for an average user in a city. One of the most popular uses of computer network nowadays is just going onto the internet, may be surfing it, may be getting some information, etc. But the amount of traffic in terms of query etc., which flows from the user to the computer, wherever it may be, is relatively small. Whereas you may be downloading large files, images, may be even songs, movies etc. Your bandwidth requirement ? the upstream bandwidth requirement and the downstream bandwidth requirement are usually not the same; there is an asymmetry. (Refer slide time: 22:27 ? 24:03)

Now we come to another very important kind of cable, having looked at copper cable, namely, optical fibers. Optical fiber basically is made of glass, very pure and very transparent silica glass is used, which is very very transparent in the sense that it can go up to tens of kilometers without too much of attenuation on the way. So think of a glass slab, which is several kilometers thick and you can see through it quiet easily, so that kind of transparency is required. So, naturally, there have to be some very strict standards for making this glass. But after all, it is simply a glass, which is drawn into fiber for giving us this optical fiber. At moderate dimensions, light is restricted to the fiber because of total internal reflection for ordinary light; this is called a multimode fiber. At still smaller dimensions, say something like 8 to 10 nanometers, the fiber acts like a wave guide for lasers. This is known as a single-mode fiber. Multimode fibers are used for local area networks whereas in all wide area networks and even in local area networks these days single-mode fibres are more preferred for carrying signals. (Refer slide time: 24:03 ? 26:55) 

Finally what is optical networking? One thing is, naturally, optical transmission. By the way, optical transmission could be fibers or free space. Free space means suppose you direct a laser from the source to the destination on the free space, light will travel straight or it is expected at least to travel straight and then reach the  receiver. So you do not need any fiber or anything at all but there is a problem. The first obvious problem is that the sender and the receiver have to be on the same line of sight without any hindrance in-between, that is one point. And then there are other complications, which might come; in the sense that the intervening air may get hot and its refractive index may change. So the light may actually bend, like they do in a mirage in a desert. That kind of things may happen, so it might miss the receiver altogether. So free space optic communication is not very much preferred. People prefer to put fiber optic cables wherever they can. These cables or free space we are talking about is for fiber optic transmission. Then there is the question of optical switching; can we provide the network level functionalities in the optical domain itself?  This is a topic of research these days as this is quite promising. So maybe we will just look at this later on. Finally, why optics? The point is that optical fiber capacity is huge, very huge ? you see 50 terabits per second is mentioned over here. The point is that this has got an absolutely fantastic bandwidth, so if you can put a fiber optic connection between two endpoints your present as well as foreseable future requirements will surely be met. So that is one good thing about optics. The other very good thing about optics is that it is immune to electromagnetic interference, if there is a lightning strike nearby, then it is not going to affect this very thin strand of glass that we have put, which is very good because if the same thing happened to a copper cable, there would be a surge and it could maybe burn some of your equipment. So this is another reason people favor fiber optics a lot. (Refer slide time: 26:55 ? 29:27)

Now the point is that this 50 terabits per second bandwidth that we were talking about is only theoretical. Actually sometimes, I have been asked what the bandwidth of this fiber optic cable is? This really does not make much sense because the bandwidth of any fiber optic cable you would find would ultimately be dictated by the kind of boxes that you connect to it. The limitations would be there; the limitation is not inherent in the fiber. Fiber can take up a huge bandwidth as I just mentioned, but at the two ends you will have some optics and then you will have some electronics. Now electronics cannot work at that kind of speed. A single piece of electronics suddenly cannot work at that kind of speed; so the overall speed at the bandwidth that you can get out of this fiber would depend on the kind of electronics that you put at the two ends. That is why there is a lot of interest these days in whether some of these electronics can be replaced by optics so that we can get much higher bandwidth. Now not content with just sending data through only one wavelength through this fiber, we have what is known as wavelength division multiplexing. That means different wavelengths of light carry different signals through the same fiber optic channel; that is called multiplexing. We will come to WDM later on. With this WDM, you can get higher kind of bandwidth. Lucent, for example, demonstrated 64 lambda; that means, 64 different wavelengths into 40 Gbps that is each of them operating at 40 gigabits per second over a distance of 4000 kilometers, so I think that is really great. There are two kinds of wavelength division multiplexing, namely, coarse wavelength division multiplexing, which is somewhat cheaper, and dense wavelength division multiplexing, which is used in WAN but is much more costly. Sothe number of wavelengths that we can use today varies from may be 8 or 12 to may be 64 or so. People are working on this ultra dense wavelength division multiplexing, which can carry us to may be 100 wavelengths in the same fiber. (Refer slide time: 29:27 ? 31:02)

For a quick history, in 1958 laser was discovered. Then in the mid-60s guided wave optics was demonstrated, in 1970, we saw the production of low loss fibers. Because we want this low loss, we will be talking about this loss presently after sometime. So this made long-distance optical transmission possible. There was the invention of semi conductor laser diode, which was important. Then in 70s to 80s we used fiber in telephony in the form of SONET or SDH. Do not bother about what they are right now; we will be talking about them later on. In mid-80s, LANs and MAN were sort of being put in place and in late 80s, we had a breakthrough in optically amplifying optical signals. So there is EDFA and RBM, dot fiber amplifier ? we will be talking about that too in this lecture itself; and then in mid- to late 90s, this dense wavelength division multiplexing system started being put and in late 90s and in 21st century optical switching systems, different components of optical switching systems are been developed all the time. The point is that fiber optics has such a tremendous potential that there was a lot of interest in this and there is a lot of work going on in this area at the moment. (Refer slide time: 31:02 ? 32:17)


So here is some kind of a picture of a cross-section of a fiber optic cable. As you can see, when you see a fiber optic cable, you will see this fad jacket, which is just a productive jacket made of plastic, rubber; then inside it, there will be glass fiber. But what you see is actually not the fiber; that is also actually a cladding but that is made of glass. Inside this cladding there is this very fine core, which actually carries the signal. In one fiber optic cable like this, you will have several such individual fibers. So these are all individual fibers. What has been shown over here are the individual fibers and each of them will carry one; I have shown only 3 over here but it is typical to have something like 4 cores, 8 cores, 24 cores, 120 cores in one cable. (Refer slide time: 32:17 ? 34:02)

Let us look at this diagram of how it is. So if you take a cross section, there is cladding on two sides and core in-between. If you sort of cut it side ways, this is the kind of picture you get and what happens is that once the light comes into the core it sort of refracts like this and it impinges on this wall at a very wide angle of incidence. There is a difference in the refractive index between the core and the cladding. That is always maintained. So the point is that the core is at the higher refractive index and the cladding at lower refractive index. There is total internal reflection over here, so even if the fiber bends a little bit ? although light usually would travel straight but through this total internal reflection it would follow this fiber ? it can take small bends and come out of the other side of the fiber. But of course there are limitations to how much you can bend. First of all, if you bend this core too sharply, this core will get damaged. So there are limitations on what is your bending radius, radius of curvature, etc. but in general to this total internal reflection the light will follow this core. So this is another picture ? light is input from one end so this sort of follows the fiber and gets out of the other end. (Refer slide time: 34:02 ? 35:02)

Well the light gets out, but how do you get it into something? That means, how do you connect these fiber cables, which are actually glass fibers, how do you connect them to other things? There are two ways of termination; we can terminate a fiber by plugging it into a fiber socket and then sort of clamping them together. We can splice them mechanically by putting the two cut ends into a sleeve and clamping them or we can fuse them to form a solid, which is a better method of connecting fibers. Even when you fuse them what will happen is there will be some kind of disturbance ? that glass material will not be totally homogenous at the junction. So there will still be some loss but this loss is quite low if you are very careful in this splicing. (Referred slide time: 35:02 ? 35:27)



And what about light sources? We have two different light sources; they are called semiconductor lasers and light emitting diodes. Naturally, the first one gives out lasers whereas the other one, the simple LEDs, give non-coherent lights. So they are simple LEDs. Both have their advantages and disadvantages. (Refer slide time: 35:27 ? 37:19)

When you are using the semi-conductor laser ? laser as you know is coherent light and simple LED gives you non-coherent light ? so the data rate for lasers can be much higher with lasers and this is low fiber type. You can use multimode fibers over here and you can use single mode fibers. We prefer single mode fibres for lasers. The distance that an LED can cover is short whereas the distance that single mode fibers, that means lasers, can travel through a single mode fiber is really long. So all our long-distance cables are actually single mode types and they carry laser. Since we are going to higher and higher speed all the time, we tend to prefer single mode fibers these days. Of course, LED has a very long life. So this is one area in which LED is definitely better whereas laser?s lifetime seems to be short, although not very short. If proper care is taken, they will certainly run for several years. The temperature sensitivity for LED is minor so for semiconductor laser it is substantial, so its life source has to be maintained at a particular temperature. LED naturally is low cost and laser is more expensive. These are the pros and cons of the two sources. (Refer slide time: 37:19 ? 39:06)


Let us took take a closer look at the kind of losses that we have in fiber. Remember, when we were discussing about electrical transmission through some copper medium ? we were talking about it in the last class ? there are several kinds of distortion to the signal. First of all, the signal strength tends to weaken as we go over a longer and longer distance; that is called attenuation. The same thing happens in the case of fiber also, in the sense that as we go further and further the signal will get weakened. You can look at this way that if the signal did not weaken, then we can look through a glass of any thickness. That is not true; the transparency of the glass, although it is very highly transparent, is not 100%. That is why as it travels down the fiber its strength will become lower and not only that, it will also get its wave shape and also get somewhat distorted due to certain phenomena. For example, there is this phenomena of Rayleigh Scattering; that means due to slight changes in the refractive index of the glass some of the light will get scattered. Then there are other kinds of scattering called Mie Scattering due to the imperfection of the cylindrical structure. These losses are non uniform for different wavelengths. That is why on the other end your signal will arrive first of all weakened, and secondly in a slightly distorted fashion. (Refer slide time: 39:06 ? 40:09)


This figure, this famous figure, is the plot of the loss that will happen to a light signal at different wavelengths. As you can see, this loss in the fiber, given in DB per kilometer, is not uniform. You can see that it is not uniform across all the wavelengths. So there are some wavelengths, namely here, where the loss is very low. Then there are other wavelengths where the loss is quiet high. What we do is that we try to find some kind of windows, where the loss would be low and we can go to high distance. Because, remember that if the loss is low (Refer slide time: 40:09 ?  41:17)

we can go much longer without any amplification or repeated kind of action and that is good. I mean that would make the thing cheaper and of course we would be able to carry data for longer distance. These are the three useful windows so to say: one is the 850 nanometer window, the other is the 1300 nanometer window, and another is the 1550 nanometer window. Around these wavelengths, if we send our signal, it will go a long way. So these are the three windows, which are used for data transmission. Because of the very high frequencies involved ? you can look at the frequencies from these wavelengths, you can calculate and you will see that the frequencies are very high ? it can potentially have a very huge bandwidth. So the theoretical capacity of the fiber is very high; it is limited by the optoelectronic devices and boxes at the two ends. (Refer slide time: 41:17 ? 43:23)

Now let us see if we want to take the fiber optic signal to a long distance; naturally, I mean whatever window you use after some distance depending on the kind of source you are using, the kind of frequency you are using, the quality of fiber you are using, it may be 5 kilometers or it may be 50 kilometers. But when you are going thousands of kilometers, at some point of time the signal will become very weak so you need some kind of repeater function. What is the repeater? The incoming signal may amplify or make the shape all right; so there are all kinds of repeaters. Repeaters are different gates; there are some repeaters, which simply amplify the signal. There are other repeaters, which can also make corrections to the shape of the incoming signal, etc. You need this repeater function in order to go to a very large distance. So we will just take a look at this interesting phenomenon: how do you repeat optical signals? Optical signals traveling for long distances through fiber need to be strengthened. This may be done through OLTs; now what is an OLT? That is optical line transmitter, which takes the signal to the electronic domain and, of course, you can use a fresh source over there to make a nice and strong wave form and push it for the next leg of its journey. The important thing is that this may also be done in the optical domain and the advantage of doing it in the optical domain is that you are no longer restricted by the bandwidth limitations of electronics. So you can really operate at very high speed always in the optical domain and one way to do that is with (Refer slide time: 43:24 ? 43:58)

erbium doped fiber amplifiers. So erbium has a large number of excited states and from some of the excited states it gives out 1550 nanometer light, exactly the wavelength used in the third window in the glass. So a few meters of optical fiber are doped; that means a small quantity of this erbium is introduced in that; the glass is doped with the few parts per million of erbium and is pumped with 1480 or 980 nanometer laser, to give amplification. So what would happen is (Refer slide time: 43:58 ? 44:39)

you look at this. Suppose this is the ordinary fiber, which is carrying the signal and there is some portion of it which is doped with erbium and a pump laser of certain frequency is fed into this. So what will happen is that the erbium atoms in this region will be in an excited state, so erbium will be in a excited state. When a signal photon comes, this sort of induces the erbium to come down to the ground state, giving an amplified signal. So this is a very simplified version of how erbium doped amplification is used. (Refer slide time: 44:39 ? 44:57)

There are other kinds of this; depending on what kind of pump laser you are using, you may go to this kind of excited state or this kind of excited state from which you can go to excited state one and so on. But anyway, the basic idea remains the same. (Refer slide time: 44:57 ? 45:38)

Because of dispersion, that is chromatic dispersion and polarization, mode dispersion wave shaping is necessary after some distance. There are other kinds of combination of wave forms known as solitons;  these are waves shaped specially to match the dispersion characteristics of fiber so that a soliton would not be distorted for a long distance. Distortion can also be counteracted by reverse fiber. So all kinds of technology are there; we do not have the time to go into the details of these, but this is a very up-coming and growing field. (Refer slide time: 45:38 ? 46:25) 

Once again, why do we like fiber? ? because repeaters are needed only every 50 to 100 kilometers as opposed may be 2 to 5 kilometers for copper; not affected by power surges, electromagnetic interference, power failures, corrosive chemicals, etc.; communication in a fiber is usually simplex. That means there is the source on one end, receiver on the other end, so in one fiber the data is really going in one direction. So for two-way communication, we require a pair of fibers, but these two fibers have more capacity than 1000 strands of twisted pair, and that is really really great. (Refer slide time: 46:27 ? 47:05)

We will have just one quick look at where fibers are going? People are putting fiber at a feverish pitch now. One is the ultra long hall ? ultra long hall means we have fiber for a distance of may be thousands of kilometers. Then in the long hall we have this WDM or SONET, SGAs etc., which is put by the telecom company or other communications companies. Then we have the WDM, ATM, SONET, ETHERNET, etc. in the metro, that means, within the same city, and now finally fiber is finding its way in the access side ?  that means to individual users also. (Refer slide time: 47:05 ? 47:41)

These are the different ways fibers can reach your home, which is the ultimate goal of networking. Either you can take individual fibers to homes or we can take them to a switch, which is placed in the neighborhood and from there individual fibers go to every home. There is also a concept of passive optical networking in that instead of keeping a switch out in the open, you put a passive sort of splitter, which splits the signal into so many parts. So PON, passive optical network, is another possibility. (Refer slide time: 47:41 ? 48:22)

Let us come to the other most important physical medium, namely, wireless. One disadvantage of each of the data transmission methods that we have seen so far is that they are all wired connections. Moving from a wired connection to a wireless connection results in the ability to connect a network without having a physical connection.  What is the great advantage of that? The greatest advantage of course is mobility. That means if you are connected in a wireless fashion, you can move around but still remain connected. (Refer slide time: 48:22 ? 49:24)

This entire electromagnetic spectrum is used for communication. This just shows a picture of that. On the low end we have the radio, followed by microwave, which is of a higher frequency, followed by infrared, which is of still higher frequency. Then we have the visible light; of course we talked about the fiber optics and this part has not been used for communication as yet, may be in future. These are the kind of frequencies we have. So the frequency range is tremendous ? from 104 or may be 1014 or 1015 or so. We have these twisted pairs in this region, coaxial cables, maritime signaling, AM radio, FM radio, TV, terrestrial microwave, satellite, fiber optics. By the way, radio, and sometimes TV also, terrestrial microwave, satellite are all used for computer communication also. (Refer slide time: 49:24 ? 49:40)

These are the different names for the different ranges: we have LF for low frequency, medium frequency, high frequency, very high frequency, ultra high frequency, super high frequency, extremely high frequency, and so on. (Refer slide time: 49:40 ? 50:08)

Transmitting data: data is transmitted through a wave just as with coax; that means the higher the frequency, the more the data that can be encoded and transmitted. Narrow band is often used as the number of watts/hertz is high, and there are different pros and cons at different frequency bands. We will just look at some of them. (Refer slide time: 50:08 ? 50:35)

Sometimes a wide band is used; using a wide band allows the transmitter to jump from one frequency to another in order to avoid communication jams and to hide detection. That is just one example and this is nowadays used in IEEE 802.11, which is a wireless LAN standard. We will look at this later on. Bluetooth is another wireless LAN standard (Refer slide time: 50:35 ? 51:20)

To start at the low end of the spectrum, at the radio transmission, this is very easy to generate: travels long distances and it is omni directional; that means you have an omni directional antenna and you send your signal over there. So it will travel in all directions and it will reach everybody around in a certain area. That will not happen if you are using a very high frequency like microwave; so at low frequencies, waves go to walls and power falls off sharply with distance from the source. At high frequencies waves will bounce off of obstacles. (Refer slide time: 51:20 ? 51:34)

So there is a limit to the distance that a radio wave can cover. VLF, LF, and MF bands are limited by distance on the ground. High-frequency and very high-frequency waves can be bounced off the ionosphere, sending it back to earth, which is a very important thing. (Refer slide time: 51:35 ? 52:01)

This is a picture of what might happen. In short-wave range ? you know the short wave radio ? you can listen to the radio transmission from USA. What is happening is that it is getting repeatedly reflected off the ionosphere and reaching the other end of the earth. (Refer slide time: 52:01 ? 52:23)

There are problems with this wireless communication; we will not go into all the details. First of all there will be selective kind of attenuation, then there is a very irritating thing called multipath fading. So what happens is: (Refer slide time: 52:23 ? 52:47)

for example, something like this, your signal may reach directly or your signal may reach through a path after getting reflected somewhere and these two may be out of phase. So when they are out of phase, they will sort of cancel each other and you will have a very bad signal. These are the problems we encounter in mobile phones, etc. So you have to do something about these. (Refer slide time: 52:47 ? 53:21)

For microwave transmission, the transmission can be very narrowly focused. So data can be moved from one tower to another. You may have seen those microwave towers; so they are sort of in line of sight with each other. Alignment is important because this is a focused beam. If it is not aligned, your receiver will miss the signal altogether. Beams do not pass through buildings well. These are the disadvantages of microwave. If the waves are delayed, they may arrive out of sync, which is known as multipath fading. (Refer slide time: 53:21 ? 54:13)

Finally, we have infrared and millimeter waves at the very high end of these; of course, we have talked about optical fiber. They are mainly used for short-range communication and they are strictly directional. As we are going more towards the light, it is becoming more and more directed, as light is very directed. So they are directional, cheap, and easy to use but they don?t pass through solid objects. You cannot use them for a mobile phone, let us say, but you can use them for controlling your TV or for short-range connection to your computer from some handheld device, etc. So this can be an advantage or disadvantage ? one way of looking at it is that since it does not go through walls, whatever you do in one room is going to not affect whatever you are doing in the other room. So that may be an advantage also. We tried to cover all the different ranges that we use in communication; we have the satellites. Also. as I mentioned, you have this geostationary satellites, which are nothing but repeaters on the sky. (Refer slide time: 54:36 ? 54:50)

So we can sort of send the signal from earth to the satellite, where it will be amplified and sent it back to earth. These satellites could be geostationary or the satellites could be sort of low, earth orbiting satellites. So they are also used for communication over long distances. (Refer slide time: 55:00 ? 55:10)



Note: The rest is part of Lec 5. 
Faculty Name
Prof. Sujoy Ghosh
Lecture No. #5
Multiplexing
(Shareing a Medium)
Today we will talk about Multiplexing (Refer slide: 55:27- 55:34)

Multiplexing is about sharing a medium that means different users are sharing the same medium for communication at the same time (Refer slide time:55:42 ? 59:54)

Another  simplest condition a medium can  carry one signal at any moment  because if there are two signals over  there  they are going to interfere and then the signal will get garble but for multiple signals can share one medium the medium must somehow be divided giving each signal a portion of the total bandwidth if you remember that a particular frequency range around one particular frequency is called bandwidth and this band width is the most valuable resource so far as communication is concerned so what we tried to do is that we tried to use this bandwidth some how to to facilitate the communication between a number of pairs of senders and receivers so that is the idea of multiplexing (Refer slide time : 56:34 ? 56:43)

these line increases  it becomes more cost effective for the company  and most data devices individually the require modest amount of data but when the when a number of users their requirement are aggregated together the sum total may be  quite substantial band width. (Refer slide time: 56:55 ? 57:10)

the current technique accomplish this includes frequency division multiplexing wavelength division multiplexing  time division multiplexing  and code division multiplexing  we will look at many of  some of these at least(Refer slide time: 57:10- 57:55)

Let us see so this is the scheme of multiplexing  you have one multiplexer and then you have n inputs on one side so these n inputs are coming to the same multiplexer they are getting mixed up in some fashion and they are being sent over the same  physical link and on the other side depending on in which fashion you have put them together they started separated in two different lines so these now these different lines in the right they  can now go to different recipients so  just as on the left we have different senders and we have different receivers over here so the so a number of sender receiver pair is utilizing  the same physical link in (Refer slide time:  57:57 ? 58:12)

Now to for the types of multiplexer as I mentioned that we have  FDM that is the frequency division multiplexing, TDM that is the time division multiplexing,  STDM that is the statistical time division multiplexing so we will look at  these (Refer slide time: 58:11 ? 58:36)

Just quickly before we go into get into the details of each of these time division multiplexing each user periodically gets the entire bandwidth that means the entire channel is dedicated to one user but only for a short period of time for a small burst of time after that it is somebody else  so we will look at the details of this later 
(Refer slide time: 58:37 ? 58:38 )



  




COMPUTERS NETWORKS
PROF: SUJOY GHOSH
INDIAN INSTITUTE OF TECHNOLOGY
IIT KHARAGPUR
LECTURE: 5
Multiplexing 
(Sharing a Medium)
(Refer slide time: 00:53)
Good day. Today we will talk about multiplexing. 
(Refer slide: 00:56 - 1:08)

Multiplexing is about sharing a medium; that means different users are sharing the same medium for communication at the same time. (Refer slide time: 1:09 - 2:13)

Under the simplest condition, a medium can carry one signal at any moment because if there are two signals over there, they are going to interfere and then the signal will get garbled; but for multiple signals to share one medium, the medium must somehow be divided, giving each signal a portion of the total bandwidth. If you remember, a particular frequency range around one particular frequency is called bandwidth and this bandwidth is the most valuable resource so far as communication is concerned. We try to use this bandwidth to facilitate the communication between a number of pairs of senders and receivers. That is the idea of multiplexing. There are various reasons we want to use multiplexing, (Refer slide time: 2:15 - 3:20)

and the chief one is that transmission service is very expensive ? leased line, packet switching networks, etc. For example, laying of lines is in itself fairly expensive and a complex proposition and once you lay a line you like to utilize it to the maximum. If you can use that for the maximum amount of communication, multiplexing and compression techniques are the techniques, which we use for this purpose ? it saves a lot of money for the business. When you can send a lot of data through the same line, the data capacity of the line increases, it becomes more cost-effective for the company; most data devices individually require modest amount of data but when there are a number of users, their requirements are aggregated together and the sum total may be of quite a substantial bandwidth. 
(Refer slide time: 3:21 - 3:32)

The current techniques that can accomplish this include: frequency division multiplexing, wavelength division multiplexing, time division multiplexing, and code division multiplexing. We will look at some of these, at least. (Refer slide time: 3:33 - 4:19)

This is the scheme of multiplexing; you have one multiplexer and then you have n inputs on one side. These n inputs have come to the same multiplexer; they are getting mixed up in some fashion and they are being sent over the same physical link. 
And on the other side, depending on the fashion in which you have put them together, they have separated into different lines. These different lines on the right can now go to different recipients. So just as on the left we have different senders and we have different receivers,  a number of sender?receiver pairs are utilizing the same physical link in-between.  (Refer slide time: 4:20 - 5:01)

The alternative to multiplexing should be direct point-to-point connection; this has a number of problems. The first problem is that you need those lines that we were talking about, you need lines for each device, and you need a large amount of wiring, if they are on different floors. Another important point is that you need a lot of I/O ports on the computer side, which really is not feasible. You may have a few I/O ports, but you cannot have hundreds of I/O ports, it?s really difficult to have hundreds of I/O ports there. That is also another bottleneck that we wish to address. (Refer slide time: 5:02 - 5:22)


Another approach could be, a somewhat older approach, which is that of a multi-drop line. The host polls machines to see who wants to send and then uses the same lines, saves I/O port; the total communications load is not greater than the data rate of line. (Refer slide time: 5:23 - 5:42)

These are actually the simple approaches. Using a multiplexer approach, all the devices, their data is multiplexed on one side, sent through one line, and the number of lines in is equal to the number of lines out; the link carries multiple channels of information. (Refer slide time: 5:43 - 5:59)

The types of multiplexer we have are: FDM, that is, frequency division multiplexing; TDM, that is, time division multiplexing; STDM, that is, statistical time division multiplexing. We will look at these. (Refer slide time: 6:0 - 6:26)

Just quickly before we get into the details of each of these, in time-division multiplexing, each user periodically gets the entire bandwidth. That means the entire channel is dedicated to one user but only for a short period of time, for a small burst of time. After that it is somebody else?s time. We will look at the details of this later. 
(Refer slide time: 6:26 - 6:51)

Another common approach is the frequency division multiplexing; here the frequency spectrum is divided among the logical channels. Here we have only one physical channel, but we want to have a number of logical channels, so the frequency spectrum of the channel is divided, each user has exclusive access to his channel. (Refer slide time: 6:52 - 8:52)

This FDM sends signals in several distinct frequency ranges: one of the oldest uses of frequency division multiplexing is in radio. For example, we have this electromagnetic field in our atmosphere, let us say, through which electromagnetic radiation can pass. Electromagnetic radiation can be of very large range of bandwidth, the whole range of bandwidth is there. Out of that, the so-called radio frequencies constitute one part of it, a fairly important part of it. This whole bandwidth of radio frequencies is divided into small channels and each channel is given to one particular station. And on the receiver side, on the radio side, what we do is that we tune our radio, let us say, to one particular frequency, so that it receives signal from that particular station only; although a number of stations are all transmitting at the same time. This is an example of multiplexing. Another example of multiplexing, the same frequency division multiplexing, we see nowadays in the cable TV. The cable TV providers give one cable, one coaxial cable to the premise connected to the TV. That one coaxial cable apparently is carrying a number of channels, may be hundreds of channels these days. What is done once again is that, all the frequencies, which can travel down this cable, are broken into a number of logical channels and each channel is dedicated to one particular station. That is how we carry multiple video channels on a single cable. (Refer slide time: 8:53 - 10:10)

Of course I mean point-to-point is out of question. Imagine the cost of stringing wire for each channel; you?ll have a hundred cables coming into your building which is not really possible. Each signal is modulated on to a different carrier frequency; carrier frequencies are separated by guard bands. This guard band is important because one particular channel, maybe a radio channel or a TV channel, the principle is the same everywhere. So, that uses some middle frequency, maybe some range of frequencies. The next station maybe a TV station or a radio station ? whatever it is, that can occupy; first of all that has to be non-overlapping. The range of frequencies which is assigned to the second station has to be non-overlapping with the range of frequencies which is given to the first station and not only that, between these two there must be some separation in frequencies. Otherwise what will happen is these two signals are going to interfere with each other. So that is a guard band. (Refer slide time: 10:11 - 10:17)

The bandwidth of the transmission medium exceeds required bandwidth of all signals because of these guard bands which have to come in-between. (Refer slide time: 10:18 - 11:02)

Usually for frequency division multiplexing, analog signaling is used to transmit signals. Broadcast radio and television, cable television and amps cellular phone system use frequency division multiplexing. Amps is an old cellular phone system that was there in USA. Nowadays of course, the kind of cellular technology that we have in our country uses more complex multiplexing techniques, maybe I?ll mention that later on. These systems all use frequency division multiplexing. This technique is the oldest of the multiplexing techniques. (Refer slide time: 11:03 - 11:22)

Assignment of non-overlapping frequency ranges to each user or signal on a medium ?all signals are transmitted at the same time, each using different frequencies. A multiplexer accepts inputs and assigns frequencies to each device. (Refer slide time: 11:21 - 12:07)

The multiplexer is attached to a high-speed communications line because all these frequency bands on individual users are going to adapt to a fat range of frequencies. The communication line must be able to handle this whole frequency range or in other words, the communication line has to be high speed. A corresponding multiplexer or demultiplexer is on the end of the high-speed line and separates the multiplexed signals, because on the other end you have to separate them out. Since it involves analog signaling, it is more susceptible to noise. Thus we have seen that the analog signals are more susceptible to noise. In digital signals we can sort of clean the noise more easily. (Refer slide time: 12:08 - 12:52)

This is a diagram. If you note that for a particular user, suppose user 1, we give channel 1; along the time this channel 1 is entirely dedicated to user 1 for all time. Similarly channel 2, which is at a different frequency ? frequency is on this side and time is on this side ? once again user 2 gets to use this frequency for the whole time. This is the simplest kind of scheme, so that for this whole frequency band, frequency is being divided, which is why it is called frequency division multiplexing. (Refer slide time: 12:52 - 13:13)

As was mentioned, the frequency channel is divided into logical channels; each user hangs on to a particular frequency, the radio spectrum and a radio are examples of the media and the mechanism of extending information from the media. (Refer slide time: 13:13 - 12:07)

This is another picture of the same thing, see channel 1 and then channel 2, channel 3 ? they all send some signals. If you plot them across the frequency versus the signal strength, you get a picture like this, so this is the first channel, this is the second channel and this is the third channel. In-between this part here is the so-called guard band. This frequency is not used by either of them. There is some overlapping due to noise and other issues; we will not get into that;  due to that this is what it looks like, the frequency bands are all separated. (Refer slide time: 14:05 - 14:47)

Here is another diagram which shows, let?s say you have the host and a 300 bps line; a number of 300 bits per second lines are being multiplexed to 1200 bps line and then there are some guard bands: maybe 300 to 1013 Hz is one and then 1433 to 2206 Hz is another line; maybe 2686 to 3400 Hz is another line. These are the 300 bps sub-channels and you have the guard bands in-between. On the other side again you have this multiplexer?demultiplexer, which takes it to different users. (Refer slide time: 14:48 - 15:35)

One problem with FDM is that it cannot utilize the full capacity of the cable. It is important that the frequency bands do not overlap so that is why you have to give the guard bands, so the full capacity of the cable is not utilized. The other disadvantage ? we will summarize these advantages and disadvantages later on ? is that it is sort of bit more prone to noise because we are dealing with analog signals. FDM is usually used to carry analog signals although modulated digital signals can also be sent using this technique. (Refer slide time:15:35 - 16:22)

We have another type of multiplexing, which is wavelength division multiplexing. Wavelength division multiplexing is the same as the frequency division multiplexing. Only thing is that here the operating frequencies are much higher, actually they are in the optical range; that is why somehow it is given different name like wavelength division multiplexing, you know different wavelengths means different frequencies. If you are doing wavelength division it is same as doing frequency division and this is used in optical fibers. This is the same as FDM but applied to fibers. (Refer slide time: 16:23 - 17:08)

There?s a great potential for fibers since the bandwidth is very huge. As I mentioned, the operating frequencies are very high, even a slight percentage diviation from the mean operating frequency gives you a very large bandwidth. So you can sort of compact or pack a lot of channels at those high frequencies. Fibers have a large bandwidth capacity and with different energy bands are passed through a diffraction grating prism combined on the long distant link and then split at the destination. This is got high reliability as well as high capacity, two very interesting properties. (Refer slide time: 17:08 - 18:23)

So multiplexing and demultiplexing for a wavelength division multiplexing is of course very simple. If you take a prism, you know if you send a white light through a prism it breaks up into all the different colours, because the refractive index of the prism varies with the frequency. So different frequency can come in,  you can make the same light source also to say in this and on the other side you can use a prism to break it up. Of course I have shown a prism here for simplicity; usually something like a diffraction grating is used. So if you look at the fiber spectrum, if you plot the power versus the frequency so one fiber maybe giving at this frequency, another fiber maybe delivering power at this frequency. If you put them together simply then you get this power versus ? or power versus ?, kind of plot that you get (Refer slide time: 18:24 - 19:37)

Wavelength division multiplexing multiplexes multiple data streams onto a single fiber optic line. Different wavelength lasers called  ?s; traditionally ? is used for denoting a wavelength, some are quite extensively used in this optical communication domain. Different frequencies are  called different ?s and they transmit multiple signals. Each signal carried on the fiber can be transmitted at a different rate from the other signals. This is one very interesting thing of fiber that is, different colours although they may get mixed but finally again they will be separated. What rate of data, what protocol etc. are they carrying? The fiber system is really transparent to all that so that is a good thing ? different systems can be put to the same fiber. As I mentioned that fiber have a very high capacity so you get to push in a lot of channels into the same fiber. (Refer slide time: 19:39 - 21:09 )

Dense wavelength division multiplexing also called DWDM. They may combine as many as 30, 40, 50, 60 or more channels these days into one fiber and each of these channels has a very high capacity; so DWDM channel has a very high capacity and it is improving all the time. People are finding ways of packing more ?s into the same fiber and it is supposed to be quite scalable into the future; but of course if you want to pack those wavelengths in a very dense fashion, your equipment becomes somewhat costly. A cheaper alternative which has come up recently which are called CWDM so Dense Wavelength Division Multiplexing is called DWDM, cheaper version is called Coarse Wavelength Division Multiplexing or CWDM. In this CWDM the channels are more widely spaced on the spectrum. Multiplexer, demultiplexer, and all these costs go down and become cheaper alternatives but still you can push in a number of wavelengths into the same fiber. Now we come to the other very important multiplexing technique called Time Division Multiplexing. (Refer slide time: 21:10 - 22:10)

As a matter of fact this is extensively used in our computer communication or telecommunication. Time division multiplexing is very extensively used especially on the service provider side; we will look at these later on. Sharing of the signal is accomplished in time division multiplexing by dividing the available transmission time on a medium among users; so time is being divided. What is done is that, as I mentioned before, that each user gets a small burst of time in which at that time the entire channel and the entire bandwidth is at his disposal; but as soon as that short duration of time is over, then it is somebody else?s turn to send and this maybe in a round-robin fashion after some time it will come back to the original user, so that he can send his next burst of data or voice or whatever it is. (Refer slide time: 22:12 - 22:40 ) 

Usually we use digital signaling in time division multiplexing, almost always. Digital signaling is used almost exclusively. Time division multiplexing comes in two basic forms: one is synchronous time division multiplexing, the other is statistical or asynchronous time division multiplexing. We will look at synchronous time division multiplexing first and because that is the simpler scheme and then you will look at statistical multiplexing (Refer slide time: 22:41 - 23:42)

So the same multiplexing scheme now looks like this. If this is the frequency, the same plot if you remember in the previous diagram when we were doing TDM we were slicing this channel in this fashion. We are now slicing it in this fashion; that means time is divided; channel 1 channel 2, channel 3, channel 4, channel 5, channel 6; they get the entire frequency range for a small burst of time. And suppose only 6 channels are being multiplexed, after 6 again it will be the turn of channel 1. So channel 1 sends the first burst of data  etc. using the entire frequency band during this time period and then during this time period. During this time channel 1 is not sending anything so it is quiescent. Time is getting divided in this fashion, this is why this is called time division multiplexing. (Refer slide time: 23:45 - 25:23)

This is a schematic of a time division multiplexing system. Like FDM time division multiplexing saves money by allowing more than one telephone call to use a cable at the same time. You have terminal 1, terminal 2 and terminal 3. Terminal 1 is giving at this particular time then terminal 2 and this terminal 3. And over here on the other side this sequence and the way it goes that is known that has to be known; so whatever signal comes at one particular slot; this multiplexer knows that this must be for receiver number 1 and next slot is for receiver number 2 and next slot for receiver number 3. So on the receiver side, it just knows if it is this time slot, it is for a particular user. This is a synchronous division multiplexing because this clock and this clock have to be synchronized otherwise, there will be a problem. Instead of dividing the cable frequency bands, TDM splits cable usage into time slots.  Each channel is given a regular time slot in which to send a PCM signal. (Refer slide time: 25:25 - 27:41 )

 This is another view of the same time division multiplexing. Actually in actual practice, we will go into the details of this later on. Actually this time division multiplexing can be done in a hierarchy of manner. We will look at this hierarchy later on. If you remember from our digital signals discussion, that our voice, because it is up to say 4 kHz is our voice signal, because of Nyquist theorem, we require 8 kilo samples; that means double the rate, that means 8,000 samples/second for capturing voice. And for each sample, it has to be converted. Each sample is an analog value; so that value has to be converted to a digital signal, an 8 bit digital signal, which is called PCM ? pulse coded modulation. That means there are 256 different levels that this is distributing, which is very fine. Our amplitude of voice if you divide it into 256 parts we are making a very fine distinction; that gives you 8 bits per sample 8,000 samples/second; that comes to 64,000 bits per/second. This is a very basic rate and this rate has got a name; we will go to these names later on. A number of such channels may be multiplexed together. This 64,000 bits/second is a very basic rate. A number of such channels may be grouped together to form one particular higher level channel and different higher lever channels may again join together to make a hierarchy of channels, this is what is shown over here. (Refer slide time: 27:41 - 28:38)


These T1 etc., don?t  bother about this figure at this moment because we will look at this later on. So T1 is one particular rate at which some of these signals come. There are world wide standards actually there are two standards, we will talk about it later on. Standards of the rate at which these signals come; they sort of join together and form a stream of T2 stream and different T2 streams, different waves have different names like T1, T2, T3, etc., will form a T3 kind of frame and as you can see that the bandwidth is increasing all the time. The T1 rate is 1.544 Mb/second.  (Refer slide time: 28:38 - 32:10)

Time division multiplexing used for digital signals or analog signals carrying digital data, data rate of transmission media exceeds data rate of signals; so this has to be there otherwise your TDM system will not work. Because in that small burst of time which is given to a particular user, in that burst of time that user should be able to send whatever digital data it had accumulated during that time, as well as the previous epoch during which it was quiescent; because at that time it was other people?s turn. It may be a buffer or kind of thing where all these bits are getting accumulated and when it gets its small burst of time, during that burst of time it sends all the bits in that buffer, to make it empty. And then it becomes quite again, so it will accumulate the bits again in the buffer and then again when it gets the next burst of time it will send all the bits. Thus data rate of transmission media has to exceed data rate of signals. It uses a frame; frame means a number of bits and bytes put together to form one particular unit at which to send it over; one slot for each slice of time. What would happen is that a slow device during his slot will send a lower number of bits; very high-speed device will send a large number of bits, but your  slot will come periodically. One or more slots of each device is equal to channel and time slots are transmitted whether source has data or not. This is one particular problem with time division multiplexing. Think about this ? that on the receiving side, receiver side is blind; it does not know this stream of bits which has come is for whom. The only way he knows that this is the time slot for receiver I, so he will send those bits to receiver I and so on in a round robin fashion. What happens is, suppose the  senders are S1, S2 to Sn, receivers are also S1, S2 to Sn; and then we have this. The SI will be sending to RI; what happens if SI has nothing to send at that point of time? During that time it is not possible for the multiplexer or the demultiplexer, in the normal scheme of things, to send somebody else maybe Sj?s signal during that time although physically it would be possible; but then on the demultiplexing side it would cause a problem because the demultiplexer would not know whether this bit is for Ri or Rj. That is why even if Si has nothing to send, that slot will have to run empty; you can not utilize it any further. This is the weakness of time division multiplexing which we try to sort of address in a different scheme, we will be talking about just now. (Refer slide time: 32:11 - 32:33)

The synchronous time division multiplexing, remember that it is synchronous because the multiplexer and the demultiplexer has to agree about the slot of time; the original time division multiplexing, the multiplexer accepts input from attached devices in a round robin fashion and transmit. T1 and ISDN telephone lines are common examples of synchronous time division multiplexing. (Refer slide time: 32:34 - 33:01)


If one device generates data at a faster rate than other devices, then the multiplexer must either sample the incoming data stream from that device more often than it samples the other devices or buffer the faster incoming stream. If a device has nothing to transmit the multiplexer must still insert a piece of data from that device into the multiplexed stream, it may be a dummy data. (Refer slide time: 33:02 - 34:08)

Before I talk about the other kind of TDM, which is the statistical time division multiplexing, I must mention that there are hybrid multiplexing schemes also. That means in the hybrid scheme, what you do is that you take a frequency band; it is like frequency division multiplexing, so you break up your entire available channels into a number of bands; but each of the bands instead of in the pure FDM, we are giving it to only one user but this band is allotted to may be a number of users who use TDM on this band. This is a combination of FDM and TDM, so such kind of hybrid multiplexing systems are quite widely used in these days and one good example is the cell phone communication that goes on; that may use hybrid kind of multiplexing systems. (Refer slide time: 34:10 - 34:21)

In hybrid multiplexing system, both FDM and TDM may be combined; the available channel is broken up into frequency bands; in each band multiple channels are accommodated through TDM. (Refer slide time: 34:22 - 36:38)


Now we talk about the other kind of  time division multiplexing, namely, statistical multiplexing. This is time division but on demand rather than fixed, so we reschedule link on a per packet basis and packets from different sources are interweaved on the link. Remember that I mentioned the disadvantage of time division multiplexing is that even if the sender does not have to send anything, that time slot will go empty. We cannot utilize it because otherwise on the demultiplexer side we will not be able to decipher who it is in the pure time division multiplexing. This is to be addressed in statistical time division multiplexing or STDM; what is done is that the packets are sent on demand; that means if somebody has to send and the channel is free, it will be sent. Of course if everybody has to send, then you have to once again do some kind of scheduling in a round robin fashion in a simple vanilla case or you can do more fancy kind of scheduling. I am not going into that. So this utilizes the channel in a much more efficient manner; but you have a problem. We know what the problem is ? the problem is on the demultiplexer side. On the demultiplexer side, one particular time slot being allotted to one particular sender or a receiver, that relationship is no longer there, and if that relationship is no longer there, how do we make out on the other side whether this is meant for whom? So you have to have that address inside that data stream. That means inside the data stream, there must be something which tells the receiver and the demultiplexer that this is meant for that receiver.  So you have to have the address, at least the destination address in the packet so to say, now I am calling it a packet that what ever the burst of data sent in a particular time slot. 
 (Refer slide time: 36:41 - 38:20)

It allows connection of more nodes to the circuit than the capacity of the circuit. How is this possible? This is because usually our communication is always bursting, for example, when I am talking I may require 4 kHz, when I am making a sound;  but then there are periods when I am quiescent that means I?m not talking at all .So overall if I can utilize that, maybe my effective rate of utilization of the channel will become more than 4 kHz; because one user is using 4 kHz. And this way in each of the channels, the same thing happens and much more dramatically, in the world of sending and receiving data. Data traffic is inherently bursting, which means that one computer is communicating with other computer, this will do so in bursts. It will want to send a lot of stuff; suddenly it has something to send, so it will want to send it fast in a burst. Then for a long time it will be quiet, as there will be nothing because it will go there, maybe get processed over there, maybe some user sees it, sees that data, and things like that. So it is quiet for a long time, then again it sends another burst. And in-between these bursts there is a lot of time. While the burst is there, I am using at a very high speed but then other people can share it, so that is why in some sense it allows connection of more nodes to the circuit than the capacity of the circuit. (Refer slide time: 38:21 - 39:00)

This works on the premise that not all nodes will transmit at full capacity at all times; must transmit a terminal identification, as I said the destination identification has to be there; and may require storage because two different sources may want to send at the same time and both of them land up hoping that the channel will be free. I am calling it a multiplexer at this point, at the multiplexer end you have to store them and then forward them over the channels, and then the channel becomes available. (Refer slide time: 39:01 - 39:56)

This is a picture of once again getting multiplexed ? they are getting multiplexed as they come. This is the queue in-between and you have this buffer over here, this is the memory where the incoming packets are getting stored and they are making a queue so there are a number of packets here and from this queue one by one as they come, it is not necessary that they have to go in a strict round robin fashion; as they arrive they have are being pumped into this transmission channel. On this side, on the demultiplexing side, it will look at some receiver address which is contained within this packet or cell, whatever it is, and then send it to the proper receiver. (Refer slide time: 39:57 - 40:47)

The buffer packets that are contending for the link, the packet queue may be processed first in and out, but as I said this is the most vanilla kind of scheme; there are other fancy schemes. For example you may have a high-priority queue for some of the users who may be paying more ? that queue is processed first before the other queues and all kinds of possibilities are there. But of course since you have a buffer, it is theoretically possible and since your capacity of the link cannot handle if all the senders decide to send at the same time, which may happen, in which case it will be filling it in the buffer, but the buffer may also overflow, which means that there is a congestion in the network and then some data loss is there.
(Refer slide time: 40:48 - 41:16)

To summarize, a statistical multiplexer transmits only the data from active work stations. If a work station is not active, no space is wasted on the multiplexed stream; a statistical multiplexer accepts the incoming data streams and creates a frame containing only the data to be transmitted and also as I said this frame will also contain the some kind of destination address. (Refer slide time: - 41:16 - 45:41) 


This is an example to identify each piece of data and the address is included. So the data of c as well as address of c, data of a as well as address of a ? the address also has to go with the data. This of course is under the assumption that all these data packets and these addresses, together all these packet lengths are fixed, which is the case in some systems like ATM, etc. The size is fixed but the size may vary also, if the size varies then you will have to include the length of a also, that means this is the data of a; what is the total length of a or maybe the data part, if this part is fixed depending on the scheme then the address to which this is destined. All these are put in the form of a frame and then it is sent. Finally I would like to summarize the advantages and disadvantages of each of the schemes. By the way, in the beginning I mentioned something called code division multiplexing, which is a very interesting scheme. We will talk about code division multiplexing when we talk about multiple access. There are actually two terms; one is multiplexing and the other is multiple access. Usually this is how we mean it but distinction is getting blurred these days; usually when you are multiplexing, the sources are sort of converged to the multiplexer in some sense. The sources are converged to the multiplexer in some sense and  it goes to the demultiplexer where it diverges. There is a specific convergence which is there, that means a number of telephone lines are getting multiplexed; a number of telephone lines will come to the local telephone exchange. There these may be all destined for some other exchange, so all these lines will be multiplexed together so they are happening physically in one place. If the problem is this, then once again you have a shared medium but the users are distributed. In that case we call it a multiple access system. CDMA is a multiple access system as well as you can in some sense call it a multiplexing system also. As I said this distinction is slowly getting blurred these days; for example, you are doing some kind of multiplexing  when you come to cell phones. We are doing some hybrid kind of multiplexing; that means we have got a combination of FDM and TDM; i.e., we have some bands of frequencies which are being used by a number of users; but these users are not at the same place. They are carrying some wireless station. That means they are carrying their mobile handsets and are at various locations. Even for the same base station, under the same base station, they are at various locations in that area so to say. Thus the distinction between multiplexing and multiple access is getting a little blurred these days. Anyway we will discuss code division multiplexing or multiple access when we talk about other kinds of multiple access. As you can guess, there is time division multiple access and then there is frequency division multiple access and then of course this code division multiple access is also there; so we will discuss it there. (Refer slide time: 45:43 - 48:20)

Coming back to the scheme that we discussed, namely, FDM, what are the advantages and good things about each of these schemes? FDM is of course simple and being simple means that it is cheap and is very popular, and as I mentioned this is the oldest multiplexing technique. Your radio uses this and lots of other systems use this. TDM is specifically for digital signals, so for digital signals TDM is very nice and then you can have a multiplexing hierarchy; that is another advantage of TDM. Why it is an advantage? Well the advantage is because of this: at different places we may require different levels of bandwidth and speed. Just think about this ? suppose we have this telephone network, you may have a small exchange; the small exchange will naturally have a small number of users and only a few of those users will want to communicate or make an STD call or to communicate with some other telephone, which is under some other exchange. So the few will have to be multiplexed together at the small exchange and it will be sent to the trunk exchange and from there it may go to another trunk exchange and so on and finally reach. At the trunk exchange what is happening is that once again various such connections from different local exchanges are coming and converging to the same trunk exchange. When this trunk exchange is communicating with another big trunk exchange, what is happening is that there are a lot of tributaries flowing into it, there are a lot of channels. So basically in the same signal, we are doing time division multiplexing. That means the rate at which the receiver at this end is sending, that same rate has to be maintained throughout; but then instead of flowing in small tributaries, it is moved into a very fat channel. We require a multiplexing hierarchy. So such multiplexing hierarchies are easy to implement using time division multiplexing; we will see more of this when we talk about synchronous digital hierarchy later on. (Refer slide time: 48:21 - 50:47)

STDM?s advantages: we are only talking about advantages. We will talk about disadvantages later on. In statistical time division multiplexing, this has certainly more efficient bandwidth use. It  is much more efficient because as I mentioned, that specially data usage and usage in general may be very bursty and if it is bursty, then during the quiescent period, ordinary TDM is just wasting the channel whereas STDM is making full use of it. So it has more efficient bandwidth use; that is a good thing about STDM. Another advantage of STDM is that the frame can contain control information; since you are putting a frame, may be with the address of the receiver or may be other kinds of control information can be put in the frame. And then, the packets can be of varying sizes. These are the advantages of STDM; not all STDM systems will allow this varying sizes; for example, ATM will not allow varying size cells. In ATM, the packets are called cells, they will have a fixed size but some systems may allow. WDM: we are writing it separately because although as I said this is some form of FDM, this works with a very high capacity, is scalable, and this has got low noise sensitivity. That is another good thing about the communication in the optical domain. Suppose you are doing it in a electrical domain through a wire, may be, if there is some kind of electrical noise, somewhere some motor has started or very bad if there is some lightning somewhere nearby, lot of noise will get into it and this electrical wire will catch all this noise. Whereas an optical fiber is not going to catch any of these noises, it is very immune to noise, which is why it?s got a very low noise sensitivity, which is very good and very high capacity and you can scale it to still higher capacity using DWDM. 
(Refer slide time: 50:48 - 51:20)

Just a quick look at the disadvantages: FDM, as I mentioned earlier, is susceptible to noise, wasted bandwidth, because of this guard band, etc., and of course one frequency is whatever the small band is being permanently given to one user and if his usage is very bursty, then a lot of bandwidth is wasted. Then, we have a limited frequency range. In TDM we have wasted bandwidth for the same reason. (Refer slide time: 51:21 - 52:06) 

STDM takes care of that problem in a TDM but this is very expensive; traditionally we did not have any other alternative so we used STDM a lot when high speed was necessary especially in the service provider side, we used STDM. WDM is somewhat more costly than TDM and FDM but since this is a developing technology, this cost is coming down and that means the WDM cost is coming down but it is still somewhat more costly than TDM and FDM but may be less costly than STDM. We have looked at the different multiplexing techniques today; in the next lecture we will look at one kind of network, which is very widely used for data and all kinds of communication, namely, the telecom network, the telecommunication network. Because remember that although it is called telecommunication network and it was traditionally for people talking over telephones, for voice, etc., that is the network which carries most of the wide area data traffic today, and that is a very important. We will talk about it in the next lecture.

Preview of next lecture
Computers Networks
Prof: Sujoy Ghosh
Indian Institute of Technology
IIT Kharagpur
Lecture: 6
Telecom Networks
(Refer slide time: 52:52)



Today we will talk about telecom networks, means the kind of network that is used by telephones and of course it is by host of other things as we will discuss. This telecom network is very important in the sense that first of all, it is one of the earliest network that we had and that is one thing that is quite old and secondly, the telecom network even today is mostly used for a wide area communication. Comparatively, we have much fewer, I mean entirely or exclusive data networks. We will look at the evolution of telecom networks.(Refer slide time:53:54-55:29)

Well when Alexander Graham Bell developed the first telephone, it was a couple of telephone instruments connected by wire. Now if you are trying to connect more than one people that means more than one subscribers are there; taking this principle forward you will have to connect everybody to everybody else through a wire which of course becomes very unwieldy very soon; because connecting or stringing a wire between every pair of telephones, that might want to communicate was not a good long term strategy at all. As you can see that very soon we?ll have veritable jungle of wires and of course its costly and it is very difficult to manage. At the subscriber end you cannot handle so many wires coming into your premise, so better idea was to connect all the telephones to a central switching office. There an operator could connect one telephone to another via switchboard. That is the next figure, you see that the number of wires have come down. As a matter of fact in the figure at the top if you have n subscribers, naturally you have n(n -1) / 2 links which is approximately n2 kind of links. Whereas in the latter case you have only n links so the number of links as n becomes large, and you know today there are so many telephone subscribers so there is no question of connecting them individually, you have to connect them to a central switching office.(Refer slide time:55:46-56:59)

Even then once, again I mentioned this earlier in the lecturer, you must remember that it is not that we have a whole parallel network for data from computers etc. all over the globe, it is not there. The telecom network has been around for a long time. It had a long and very excellent history so that is why even today most of the wide area network traffic still rides on this telecom network. Originally it might have been a packet then it might be sort of made into some kind of  a session for which there is some part of the connection is dedicated and all kinds of technology has come in but even today most of the wide area networks are based on the telecom networks and telecom . That is one side of the story, the other side of the story is that telecom networks the telecom people are actually in the process of routing some of the voice calls and may be some of the other kinds of calls into this packet kind of technology.(Refer slide time:57:00-57:22)

So DSL at subscriber end- voice traffic is transmitted as standard analog telephony signals; data traffic is transmitted over the same line via a DSL modem that transmits the data as high frequency digital broadband signals on the same line. If you have DSL modem you can have both your voice as well as data at the same time, unlike very ordinary  analog lines. (Refer slide time:57:23-57:35)


So DSL at the central office end, CO end, and the signal passes through a splitter as I have shown and a local loop management system. The subscriber line access multiplexer is there in this figure (Refer slide time: 57:36-57:39) we have already seen (Refer slide time: 57:40-57:43)





In this figure, we have DSLAM and the loop management over there, (Refer slide time:57:44-57:59)


The broadband digital signals are directed to a DSLAM that terminates and coordinates traffic from multiple local loop lines. From the DSLAM, the traffic is sent to a router and then to the internet.  And finally we have the loop management system, (Refer slide time: 58:02-58:24)

 
which may be in front of the splitter, or behind the splitter. This comes to various  functions like giving the connections and  testing and all this service kind of testing are done through the local loop management. Thank you.







COMPUTER NETWORKS
PROF:SUJOY GHOSH
DEPT OF COMPUTER SCIENCE & ENGINEERING
IIT KHARAGPUR
LECTURE NO # 6
TELECOM NETWORKS
Good day! Today we will talk about telecom networks. That means the kind of network that is used by telephones and, of course nowadays, by a host of other things as we will discuss. This telecom network (Refer slide time: 01:10 - 01:38)

is very important in the sense that first of all it is one of the earliest networks that we had;  which means it is quite old. Secondly, the telecom network even today is mostly used for wide area communication. We have much fewer entirely or exclusive data networks. So we will look at the evolution of telecom networks.
(Refer slide time: 01:46 - 03:22)

When Alexander Graham Bell developed the first telephone, it was a couple of telephone instruments connected by a wire. Now if you are trying to connect more than one people, that means more than one subscribers, taking this principle forward, you will have to connect everybody to everybody else through wire, which of course becomes very unwieldy very soon. It?s because connecting the wires between every pair of telephones that might want to communicate was not a good long-term strategy at all. Very soon we could have a veritable jungle of wires and, of course, it means it is costly and very difficult to manage. Even at the subscriber end, you cannot handle so many wires coming into your premise. So a better idea was to connect all the telephones to a central switching office; there an operator could connect one telephone to another via a switch board. That is the next figure that you see; that the number of wires have come down. As a matter of fact, in the figure at the top, if you have n subscribers, naturally you have n(n)?1/2 links, which is approximately n2 kind of links; whereas in the latter case, you have only n links. So the number of links increases as n becomes large. And you know today there are so many telephone subscribers; so there is no question of connecting them individually. You have to connect them to a central switching office and when some subscriber A wants to connect to subscriber B, it goes via the switching office and finally a connection is made.  (Refer slide time 03:49-07:00)

Now in the switching office in this system, a telephone user could connect to any other telephone in the town by cranking a handle; this was the old system. You know in the old system ? if you have seen some of older movies or movies depicting an older time ? somebody would go to the telephone, lift it up, and go with the handle like this ? crank crank crank ? and what this would do is that basically this would ring some kind of a bell  or give some indication in the central switching office. The switch operator was of course a person, may be lady, who would know that this is the person who wants to talk to somebody. Then he would talk to that person and find out who it is or who the subscriber is to whom he wants to be connected. When the subscriber tells connect me to such and such, through a wire, jumper wire, the telephone operator would connect this subscriber to the indented subscriber. That is how it would operate in the beginning, which of course is a rather a difficult thing to do and it is unbelievable that with today?s teledensity we can handle in that way. So after this, this role of the person was replaced by some electromechanical gadget and those exchanges were called Stowager exchanges. There is an interesting story about the Stowager exchange ? it seems that there was a person called Mr Stowager; he was an undertaker. You know who an undertaker is; he sort of prepares coffins for a person who has passed away. It just so happened that there was another undertaker in the same town, whose wife was actually the telephone operator. So whenever somebody wanted to connect to an undertaker necessarily her husband would get the job. So Mr Stowager thought that was not a good idea at all; so he went ahead and designed some kind of electromechanical system so that there would be relays and   mechanical switches would sort of rotate, depending on what you have dialed on the other side and connect. This electromechanical exchange served us quite well. And even may be 15 or 20 years back in this country you could see Stowager exchanges were operating. But of course nowadays, these electromechanical things have been replaced by  electronic switches and the architecture also has become more complicated. So this is how the switching office used to operate previously. There would be an operator physically who would make a connection, who would make a physical copper connection through a wire from the caller to the callee, using jumper cable on a plug board and  there would be a connection between the two telephones. So that is how it used to operate. (Refer slide time: 07: 01 - 08:42)

Nowadays we have a picture which is something like this; you can see that there is a local exchange building, where there is a local exchange switch, the kind of signal that is used  for connecting this part ? the local exchange ? to the individual subscribers. By the way this part is also called the local loop in telephone parlance. So the telephones still use analog carrier systems mostly. And you see there are two types of wires used over here:  one is sort of connecting the premise of each individual subscriber to some kind of a housing, may be some kind of a hut over here, through a category 3 cable, which is a thick kind of black cable, which actually comes into your home. So these cables (yellow colored) are much thicker, may be 50 pair or 100 pair kind of cables. These category 3 cables in this hut are connected to this through this; it goes to the local exchange where switching is done. 
What is switching? Suppose this user wants to talk to this subscriber so there would be a switch over here because there are individual pairs of wires going from each subscriber to the local exchange and the switch would connect these two pairs of wires and  they would have get connected. This is the scheme. (Refer slide time: 08:43 - 10:18)

Now  when telephones developed more and more, naturally people found it very useful. So it was soon apparent that callers did not just want to talk to people in the same town, but also to people in  neighbouring towns. Now this was the next hurdle to be solved. The first one was that the all pairs of telephones cannot be connected together so every subscriber was only connected to the local exchange or the local switching office, where individual subscribers would be connected. Now if you have two different towns in two different locations, what would happen is that you cannot connect all of them to the same local exchange. This is a problem; first of all from one town to another the distance would be quite large. So irrespective of where you put your exchange, whether in this town or that town or somewhere in-between,  there will be lot of wires involved and naturally this is not feasible. So what is required is that there will be a local exchange for this town and there will be another local exchange for that town and these two exchanges have to be connected. So this was the next step in telecom evolution; so to connect every telephone in a number of towns to a single switching office was obviously impractical as the wires were too long. (Refer slide time: 10:19 - 12:17)

So we came to the so-called trunk exchanges; now what are trunk exchanges? Well, there are local exchanges and if one local exchange wants to go through to another local exchange, it usually goes through a trunk exchange; this is a two-tier hierarchy. There will be some trunk exchanges and then each of the trunk exchanges would be connected to a number of local exchanges and the trunk exchanges would be connected between themselves in some fashion. Automatic switching office enables connections in a fraction of a second. That means your connection request ? if it is local, then the connection request is just served by the local exchange. That means if you just want to make a local call between two subscribers who are under the same exchange, it need not go anywhere else; it can just simply be locally connected. If the caller and callee are under two different local exchanges, it is possible that their local exchanges are connected. But the general scenario is that they would go to the trunk exchange and the trunk exchange would connect to other local exchange and then that local exchange would make the connection available to the callee. So this is how connections are set up. As telecom grew bigger and bigger, you not only want to talk to the person in the next town, you want to talk to the person in next state and may be in the next county or a country around the world. So in that case also more levels of hierarchy came into the picture as we will see here. (Refer slide time: 12:17 - 12:47)

And soon customers wanted to talk to people in different regions, states and other countries. To cope with this, even more tiers were added to the hierarchy. To make a call, we now dial a number. This number is examined by the local exchange, which decides if it can connect you with local telephone or if it needs to connect you via a higher level switching office so this is the public switched telephone network that we have today or namely PSTN. (Refer slide time: 12:48 - 13:36)

At the bottom of this figure we have these individual telephones, which are connected to the local exchanges and the local exchanges may be connected to some trunk exchanges. These trunk exchanges may be connected to still other trunk exchanges and so on. Not only that, the picture is even messier than it is shown here. For example, some of the local exchanges would connect between themselves straight away. If there are lots of calls between two local exchanges, then the telephone company might like to connect fiber between these two exchanges. For example, let us say in India, if you are making a local call, that is fine. If you are making a call which is to an exchange that is just adjacent, then you would go through some connection between the exchanges. If you want to make an STD inside the same state then this trunk exchange would somehow go to the trunk exchange on the  remote side. If you are going out of a circle ? for example, West Bengal may be a telecom circle and then Bihar would be another telecom circle. So the main trunk exchange of West Bengal would be in Kolkata and the main trunk exchange of Bihar would be in Patna; so these two from Kolkata to Patna main trunk exchanges would be connected through very high speed lines and what would happen is that, it will go from the subscriber to the local exchange to the local trunk exchange to the main trunk exchange at Kolkata and then to main trunk exchange in Bihar, let us say  in Patna, and then to further trunk exchanges then further to local exchanges and then to the final callee. Of course, if you have international calls, if you are making ISD calls, then you have to go through still other hierarchy; that means, there are some particular gateways through which you will have to come before you can go out of the country.    (Refer slide time: 15:15 - 16:24)


There is a question of routing a telephone call. Routing a telephone call means, how do you find this way? That from one exchange to another exchange to trunk exchanges to other trunk exchanges and finally to the destination, you have to make a connection; that means on each of the switch on the way, you will have to make some provision or some connection for this particular telephone connection. So how do you find out? For example a particular trunk exchange may be connected to a number of local exchanges and then number of other trunk exchanges also. So how do you know which trunk exchange to go to next? This is the problem of routing. So you have to route a telephone call. In older telephone systems, when you made a call the switching office set up a physical link called a copper path between your telephone and the callee?s telephone. The way that the copper path is set up is called routing. In a telephone network, routing is usually straightforward and I will come to the reason. (Refer slide time: 16:26 -20:12)

A call is routed up through higher level switching offices until it reaches a switching office that can reach the destination telephone by connecting with lower level switching offices. The switching offices examine the digits of the telephone number you dialed to make these connections. So this is really in some sense fairly simple, in the sense that the way the telephone numbers are distributed in any country is in quite a systematic manner. For example, let us say if you are making an STD call in India, usually we start with the  digit 0. Now the digit 0 is not really a part of the number of the callee; digit 0 is just indicating to the local exchange that we will have to be connected to some other exchange. This is not the same local exchange. So that is some kind of an escape from its local exchange to somewhere else. But as I said, the telephone numbers are distributed in a fairly methodical manner. For example, after 0 if it is 3, you immediately know that this will be somewhere let say near Kolkata; at least it will go through the Kolkata trunk exchange. Similarly if it is 011, you immediately know; you do not even have to look at the other digits and as a matter of fact for a remote caller or to the exchange to which the remote caller is connected, the other digits do not make any sense because they are really a number under an exchange which is far away, but it knows just by looking at the first few digits that that is the trunk exchange which I have to reach. For example, somebody is making a connection from let us say Kolkata to Delhi. Now immediately you would know: suppose from Kolkata so you start typing say 011 and then something, the local exchange would know that this is not only a trunk call, it is the so-called subscriber trunk dialing, STD, call. So this will have to go through the main tax exchange in Kolkata in order to connect to that main tax exchange in Delhi. So it just makes the connection just by looking at those say first two digits and after that whatever digits follow, they will be looked at by the exchange at let us say Delhi and then the next few digits may indicate a particular exchange in Delhi. So it will look at the next few numbers, maybe two or three digits and decide that that is the exchange to which it is connected. So it is immediately routed to that exchange and that exchange will look at the last few digits may be and know that this is the subscriber, so the connection would be made. So you do not have to really have to search around in the entire country to find out where this number belongs to; just by looking at the numbers, looking at the digits one or two at a time, you can find out what the next step is that the connection has to take. This makes the routing problem easy, unlike some other cases which we will get to later on. But  for telephone, the routing is fairly straightforward so it will look at the digits and make up the connection. (Refer slide time: 20:12 - 23:17)

And one point is that we are always talking about connections because essentially telephone service is a connection-oriented service. What do we mean when we say that telephone service is a connection-oriented service? It means that ? usually this is sort of changing somewhat a little bit ? the overwhelming majority of telephone connections are still connection-oriented. In a connection-oriented service, what happens is that a dedicated connection between the end points is maintained throughout the session. When you are actually calling, during the time by looking at the call number the path will be set up and at each point, at each switch, let us assume there is a physical connection, which may not be physical connection, but the connection may be some kind of virtual connection. But for our purpose at the moment, let us assume that they are physical connections, so there is a physical connection at each switch so there is a continuous path from the source to the destination from the caller to the callee and this path is maintained, is held reserved, for this particular session during the entire session. This is not shared with anybody else and this has number of advantages. Usually it means the quality of service can be reasonably guaranteed to the extent that the bandwidth for the channel that is established ? whatever bandwidth this channel might be having ? is exclusively for this particular call, for this particular session. So there is no contention for bandwidth by multiple users. Usually we are talking about just landline phones for the time-being. So first of all, the bandwidth is held constant, hence the quality of service is good. Second thing is that since the path, the actual path is held constant, the message bits arrive in the same order in which they are sent. By the way, please note that this always need not be so. For example if this path was not reserved and held constant for a particular session, that means if some part of your call is going through one path and some other part of your call is going through another path, then later bits may arrive earlier, depending on what these two paths are. So such a thing would not happen in a telephone connection usually, so message bits arrive in the same order. These are the advantages of having a connection-oriented service. (Refer slide time: 23:18 - 23:50)

So traditional telephone circuit is a circuit switched, that means actual physical  connections are made with a connection-oriented system. The downside of this is that it is less efficient use of the total available bandwidth. The reason that this is less efficient use of the total available bandwidth is that whenever we speak there are ? for example, this is just one of the factors, there are many other factors ? considerable periods of time or epochs of time during which nobody is speaking. At that time the channel is lying idle. Remember this particular channel that has been set up for one particular call is held constant during the duration, during the entire duration of the call. So at that time nobody else can use it. So this is one source why this is sort of inefficient use of bandwidth. Another problem, actually a very serious problem ? this happened due to historical reasons ? is that this traditional telephone equipment tends to be more costly. Now why is that so? Well, the reason is that this telephone system and the way the switches, etc., were designed at least the earlier ones had a long history, and naturally in this particular field, the rate of development of technology is very high and the next generation of technology would always have a tremendous cost benefit advantage compared to the previous generation of technology. So it so happened that the computer technology started leading the technology base; of course it does not mean that the traditional telecom technology was steady only at one point. This was also evolving and the computer communication field was also evolving; but it so happened that some competing technology, which is the so-called connectionless service, came about. We have been talking about connection-oriented service in telephone, so in the connectionless service it always or it quite often has the advantage of using the next generation of technology, making it much cheaper. And this proved to be a great hindrance to the traditional technology in telephones and that is why most of the traditional service providers are slowly now depending on how they are moving to this new technology, which essentially sort of sprang from another field. And that is the so-called connectionless service. (Refer slide time: 26:43 - 29:36)

Now, what is a connectionless service? Connectionless service is packet-by-packet; this is not over the entire session; one packet of data or that particular data may be containing first data. So one particular packet is sort of thrown into the system and routed independently of all other packets. So as we had seen earlier that this means that you have to have the destination address at each packet. That is one thing; so if you have the destination address in each packet, may be you can route it fast enough to reach the destination and the next packet from between the same caller and callee will come as another packet. It will also have the same destination written on it and it may so happen that some router in-between is going to route it in a different path. This connection is not held constant like the connection-oriented service. This is a connectionless service; it is packet-by-packet service. This has some disadvantages also. For example, one reason is that some of the lines may be good, some other lines may be bad and some of the packets may get dropped in-between. Secondly, what might happen is that a packet, which had been sent later, may, through a different path, arrive earlier at the destination. So all these problems are there in a connectionless service; but the major advantage of connectionless service, one major advantage is that it is cheaper. It is cheaper firstly because it is somewhat more efficient than the traditional circuit switch systems, and secondly it is usually more efficient because it usually tends to use more recent technology in terms of silicon, in terms of IC, etc. It has higher volume; its pricing is lower. That is attracting  all these other telephone service people also into this technology and there is a definite shift towards this kind of technology. By the way, you can take special care to guarantee that the packets are correctly received by the final recipient and in proper order but you have to put in some extra effort for that. So we will look into this later. (Refer slide time: 29:37 - 30:07)

As mentioned earlier, each packet must contain the source and the destination address in order to be routed. IP networks are based on packet switching technologies. So this is what most computer systems use; special techniques are employed to get some guarantee about the quality of service. The technology being cheaper, it is beginning to dominate all networks: traditional networks as well as the current networks. (Refer slide time: 30:08 - 31:57)

By the way, I mentioned this earlier in the lecture that you must remember that it is not that we have a whole parallel network for data from computers, etc., all over the globe. It is not there because the telecom network has been around for a long time; it had a long and very excellent history. So that is why even today most of the wide area networks traffic still rides on this telecom network. Originally it might have been a packet; then it might be sort of made into some kind of a session for which some part of the connection is dedicated. So all kinds of technology have come in, but even today most of the wide area networks are based on the telecom networks and telecom. So that is one side of the story. The other side of the story is that telecom networks, the telecom people, are actually in the process of routing some of the voice calls and may be some of the other kinds of calls into this packet kind of technology. Now about the transmission media in telephone systems, we have already talked little bit about it earlier, so I am not going into the details. In traditional analog telephone systems the telephone is connected to the local exchange via category 3 UTP cables, and the connection is called the local loop. It is typically between 1 km and 10 km in length.   (Refer slide time: 31:58 - 35:01)

Higher up in the hierarchy, higher bandwidth cables are used to carry multiple telephone calls. This is far cheaper than using separate cables for separate calls. Specifically digital lines on fiber are used. Analog systems use a technique called frequency division multiplexing to do this. Nowadays of course time division multiplexing is the most dominant standard. So if you remember the earlier figure we had, we had this local exchange; so your particular subscriber would be connected to the local connection hut maybe through a category 3 cable, from that hut a fat cable would usually run to the local exchange and this fat cable ? in most cases today are still basically a collection of pairs of cables ? is just in one sheath so that is why it is a fat cable, physically it is fat. Maybe there are 50 pairs of cables or 100 pairs of cables; so if there are 50 subscribers coming into a hut, they can be connected through individual pairs in this bunch of pairs of cables that run from the hut to the local exchange. So you may say that up to the local exchange we have these pairs of copper cables running. But once you go to the exchange level and beyond, that means, higher up in the hierarchy, like, to the trunk exchanges and so on, there of course you need to carry lot of calls and usually this connection is through fiber. So there are all kinds of multiplexing techniques but specifically time division multiplexing, which is the dominant one. Some specific version of time division multiplexing, which we will see later on, is used. Today a fiber may easily carry say 1000 voice calls or even 3000 voice calls just on a fiber; so that is no problem at all. For example, think of a connection between let us say two big cities. A lot of subscriber pairs from both sides will want to talk to each other at the same time. That is the great thing about these fibers, because they can accommodate a large number of channels. So it has got a huge bandwidth so they are connected through fiber and all these calls are sort of multiplexed may be in time division multiplex fashion, which goes to the other end, gets the multiplex, and finally feeds to the local loops, which again are individual copper pairs. (Refer slide time: 35:02 - 36:32)

Local loop: The subscriber handsets are powered by a battery bank in the exchange. For example, I mean with at least the ordinary handsets, you do not need to connect it to any power; it actually derives its power from the same cable which is coming and connecting your telephone set. And with echo suppressors, the transmission is half duplex and with echo cancellors, it is possible to have full duplex communication. So if you remember we talked about half duplex and full duplex communication. Half duplex communication means that the communication can go in either direction from this side to this side or from this side to this side: A to B or B to A ? but then, only in one direction at any one particular point of time. So at some point of time A may be connecting to B or A may be transmitting to B and then A will become quiescent and B will be communicating to A through the same the line. This is half duplex communication. We require some kind of echo suppressors for this because otherwise we will be hearing our own voice, which is really irritating. There is a way of echo cancellation and it is possible to have full duplex communication on the same line. (Refer slide time: 36:32 - 39:07)

Since local loop is still analog we need modems for sending digital data. That is, this local loop being analog has quite a bit of implications; one of the implications is that when you try to communicate digital data over it ? and we are talking about this telecom network in the context of computer networks, but if you want to connect a computer network to a local loop ? your computer produces digital data. So they have to be modulated and demodulated. So modems always come in pairs for sending the digital data; since one bit is used for control purpose we usually get 56 kbps speed on this local line. You remember that the computer lines are designed to handle 64 kbps; 64 kbps is the basic rate for communication purpose. If you remember, we have this 4 k bandwidth, which has sampled at doubled the rate, which is the Nyquist rate. This 8 kilo samples, 8 bits per sample is for indicating the level or the intensity of the signal; so we have 64 kbps. So this line is really designed for 64 kbps. When you are sending a computer data over it, you are using a modem and these modem pairs, etc., they need to communicate with each other and there is one bit used for control purpose. So finally we get a 56 kbps speed; so that is why now almost everywhere you will see that the modem has 56 kbps speed. People talk about 56 kbps modem; that is where this particular figure comes from. But of course although you may be having a 56 kbps modem you may not always get a 56 kbps speed because depending on the line condition, maybe some of the connections are weak, etc. So however, it is noisy so it is picking up a lot of noise. So depending on that, the two modems will automatically negotiate a lower speed and in actual practice you might find the lower speed for your computer communication. (Refer slide time: 39:08 - 43:56)

And just one another point before I go to signaling ? I want to mention that another   implication of having fully analog technology in the local loop is that on an analog signal before you can use TDM or other kinds of multiplexing techniques higher up than all those in-between for connecting between exchanges, etc., you need to convert it to the digital signal over there. So an exchange usually will have a  bunch of codecs for coders and decoders and there are various coding and decoding standards. But any way it will have a bunch of codecs, which will convert the analog signal which is coming through this local loop into a digital signal and now this digital signal is ready to be multiplexed with other digital signals to reach its final destination. Now let me tell you a little bit about signaling: this is a rather vast area; we will just be touching upon it. We will just mention some names and basically what signaling is. Signaling refers to the information exchange between terminal devices, exchanges, and routers for setting up circuits, termination, billing, advanced network services, etc. So what it means is that when you are making a telephone call ? of course, suppose you are making a simple voice call, what you are finally interested in is that your voice should be audible to the callee at the other side and his voice should be audible to you. So this voice should communicate; that is the main communication which is going on through this channel, but in order to achieve this, all the intermediate nodes ? that could be a local exchange, that could be a trunk exchange, that could be sort of switches in-between, there could be routers and multiplexers ? have to get some information and then need to exchange some information for basic setting up of the line. Now apart from basic setting up of the line, you need other kinds of services. Of course from the service provider side, you need to bill a person. If he is making a call, you want to bill him, and depending on whether he is making a local call or whether he is making an STD call or an ISD call, you may bill him at different rates. Similarly, for making the connections, we have to capture those digits and those digits have to be fed into the particular exchange; it may be computers, routers, switches, etc. So they have to be fed over there in order to make the connection. All this is a part of signaling. Apart from the actual transfer of, let us say, voice or it could even be data but whenever you want to set up some call and take some service, some signaling will always be required. So there are various types of signaling systems but it is sort of converging to some standards. In common channel signaling there is in band; by in band we mean the same bandwidth, which is used by the voice, is used by the signaling also. Why does it not interfere with voice? Well, what is done is that first of all usually this signaling would be suppressed even if they go on during the call but usually most of the signaling is done before the call is set up, so common channel signaling works very fine. Some of the bits in the frames are used for this purpose. That means that finally, even your analog signal is converted to digital signals and you make a frame out of it, you could stuff this frame with some extra bits for this control purpose. SS7, signaling system 7, is a widely accepted standard and this is slowly spreading; this is a complex standard (Refer slide time: 43:57 - 46:39)

but this is slowly spreading now. Whether in band or out of band, logically, the switch controllers may be considered to be in an overlay network in the control plane. That means, you have all the switches through which the actual connection is taking place but then you have some switch controllers, which will actually give instructions to this switch, to switch in a particular fashion, or maybe do some billing, etc. There is a network of switch controllers; first of all, there is this network of subscribers that are connected through their voice set or may be through modem, etc., all over. So that is the primary service-giving network but overlaid on this same network, you have these switch controllers at various points who are talking to each other for giving all these services and they are using the same network. So this is some kind of an overlay network over the basic network that we have. There are all kinds of extra services being given these days. For example, you know that nowadays where ever SS7 or any other advanced signaling system is there, you can get the caller id. That means if your local exchange supports it, who ever is the caller, his id, that means his telephone number, will appear in your hand set so that even before you answer, you know who it is that is calling and then you may choose not to answer his call. Anyway, you can make a note of it. Now how is that done? The numbers that he dialed at his end ? circuit has been set up there and not only that, because the local exchange has to know the source number, the source number is also transmitted and finally fed to the receiver?s end so that he gets the caller id number. So all kinds of intelligent services are being given these days. This is the signaling system 7 and there is an out of band signaling, which is different from in band signaling. That means in out of band, for signaling purpose, a separate channel is always earmarked so they do not interfere with this regular channel at all. So this is more flexible as it allows arbitrarily complex message transfer. (Refer slide time: 46:39 - 48:43) 

So this is a state diagram ? a very rough diagram ? of the kind of states that the system goes through when a call is being made. First of all, it is quiescent. That means it is not doing anything. Then you take the receiver off the hook. Nowadays of course telephone services have improved tremendously; otherwise, you would have noticed that the moment you take it off the hook and the moment you get the dialing tone there may be a gap. Actually these two are not the same thing: you pull it off the hook, which gives a signal to the local exchange that you want to make a call and the local exchange then finds some resource to serve you and when it finds the resource to serve you, it gives you a dial tone. So when you get the dial tone, you know that the local exchange is ready to receive the numbers that you dial. Then you will dial some numbers and you have to wait for some time for the call to be set up. If it is simply a local call, it may be a small fraction of a time; if it is a long-distance call, you may have to wait for some time, for some appreciable time, when the call is being set up. So, of course, the hand set on the other side starts ringing but that is not the ring that you hear. When the local switch knows that the entire path has been set up, it sends the signal, which comes as the ringing noise to your  phone. So you know that the other side the line must have been set up and then they start talking. The other side will also go off the hook and then they start talking and they will disconnect and at the time of disconnection, you want to know how much to bill this person, etc. So we will quickly go through this control plane protocols of SS 7. I will just mention some names, for example, names of some elements ? you know that this is the very huge area, (Refer slide time: 48:56 - 49:17)

this  software development for a telecom. So these are some of the elements which are there in SS 7: application service element, that is application level functionality such as interpreting signaling messages, then the transaction capabilities, which allow systems to invoke procedure calls on remote machines, (Refer slide time: 49:17 - 49:36)

signaling connection control part SCCP, connections, sequence numbering, segmentation, reassembly, flow control, etc. Some of these terms may not make any sense at the moment so we will discuss this later on in other context. Then there is a message transfer part3, part2, (Refer slide time: 49:37- 49:52)

and part 1. One of the main applications of SS 7 is the telephone user part, which is responsible for setting up voice calls. The TUP interprets dialed digits, routes, reserves resources, and maintains accounts etc. (Refer slide time: 49:53 - 50:19)

Now we will take a quick look at the digital technology in the telephone network. As mentioned earlier, traditionally everything was analog. Then the exchanges started getting converted  to digital technology, so a device called a codec is used to convert analog voice signals into digital information that can be handled by digital technology. (Refer slide time: 50:19 - 50:47)

The codec is also used to convert the digital signals back into analog on the other side, so that voice signals that can be handled by the older analog technology. Now most of the telephone network is digital; only the local loop is still mostly analog. There have been   attempts to make this digital also and one of the items was this integrated service digital network, the so-called ISDN. Let us take a quick look at ISDN. (Refer slide time: 50:48 - 52:07)

ISDN was envisaged as an end-to-end digital service; that means it starts as a digital  service from the user?s premise and goes straight that way as a digital service right to the  other end. Now home users would be connected by the same category 3 cables; that was important because people are already connected by cat 3 cables and it is a very difficult and costly proposition to change all that cabling. So the idea was to use the same category 3 cables for this new digital service. There would be some kind of network termination on the end so there were two rates; one was the basic rate, which was the 2B + D or 144 kbps; 2 B means 2 voice channel of 64 kbps each, so that is 128 kbps; and 1 D channel that is for the control purpose. So you could do out-of-band signaling over here of 16 kbps, giving you a total of 144 kbps. That is the basic rate for ISDN. For higher users, you have a primary rate equivalent to E 1, so there are 30 channels for this, maybe voice channels, and 1D channel, etc., for an E 1 line, there is some thing else in USA; that we will see later. (Refer slide time: 52:08 - 52:25)

The system uses out-of-band signaling and uses the D channel for that. NT1 is the network terminating device, which connects to the ISDN exchange on one side and a local passive bus on the other.  (Refer slide time: 52:26 - 52:33)

For larger users we have another kind of termination called NT 2. The ISDN turned out to be very expensive, which was a problem. ISDN was originally envisaged as a very broadband service with voice data, etc. on this same ISDN, but unfortunately, because of historical and technological reasons, the ISDN turned out to be quite expensive. So, although it was deployed in some places ? it is also deployed some places in India and a lot of places in Europe ? since it was so expensive, it did not become popular. So that was the problem with ISDN; so people thought about an altogether new technology called ATM and we will discuss ATM later. (Refer slide time: 53:12 - 53:34)

Another direction, in which this digitizing the local loop went, was the digital subscriber loop or DSL. So it had two basic approaches: one was that for the large user, let us say, a particular office or company. You can take a fiber to the premise of that company, which can be integrated like a sort of outlet and you can give all kinds of services ? that is one thing. The other is to use the same category-3 cable ? maybe different bands of it using some advanced technology of multiplexing signals and then give both data and voice at the same time on the same line. (Refer slide time: 54:01 - 54:07)

So you have this remote subscriber terminal, which is connected to the individual users. (Refer slide time: 54:08 - 54:19)

The copper twisted pair remains the same, but the new equipment is integrated, enabling a service provider to deliver voice and high-speed data service at the same time. (Refer slide time: 54:20 - 54:56)

So this is the picture that shows subscribers who are connected through the analog  and DSL lines. Now in the centre of the side, you have some kind of a splitter and in this splitter, one side is the data: one side the data goes through this D SLAM, which is digital service digital service access multiplexer. So that goes maybe to the internet, the other part goes to the voice switch to the PSTN, which is the other, plain old previous voice services. (Refer slide time: 54:57 - 55:22)

So DSL at subscriber?s end has voice traffic that is transmitted as standard analog telephony signals; data traffic is transmitted over the same line but via a DSL modem that transmits the data as high-frequency digital broadband signals on the same line. So if you have DSL modem, you can have both your voice as well as data at the same time, unlike  ordinary analog lines. (Refer slide time: 55:23 - 55:35)

So with DSL at the central office end, CO end, the signals pass through a splitter as I have shown and a local loop management system, so the subscriber line access multiplexer is there. (Refer slide time: 55:36 - 55:38)

  (Refer slide time: 55:39 - 55:43)

You can just have this figure where we have DSLAM and the loop management over there. (Refer slide time: 55:44 - 55:56)

The broadband digital signals are directed to a DSLAM that terminates and coordinates traffic from multiple local loop lines, from the DSLAM the traffic is sent to a router and then to the internet. And finally we have the loop management system, (Refer slide time: 56:03)

which may be in front of the splitter or behind the splitter. So this performs various   functions as they are like giving the connections; testing; and all these services are done through the local loop management. Thank you.

Preview of next Lecture
Lecture Name # 07
Switches - I
Good day! so today and  in the next  next lecture we will be talking about switches,  switches are the most important part of   computer networks and we will just look into  some  aspects of the different types of switches; so first  in this lecture we will  start with the some kind of switches which are  used in the telecom networks. then there is special  special version of these which we will cover in a  later lecture and  then in the next  lecture we will be  talking about packet switches   which are closer to the heart of data networks. so we talk about switches first thing is why do we need switches well????? (Refer slide time: 57:29 - 58:05)

  the problem is that each user can potentially call any other user as we have seen in telephone networks so and we can not have direct lines we can not have order in   square kind of lines so   every user is  connected to a essentially a  switch in the switching office and switches establish   temporary circuits that is they establish a temporary connection between the caller and the collie at  the switching station and switching system  comes in two parts one is the switch and the other is the switch controller   (Refer slide time: 58:06 ) 

you see   this fellow say zero zero zero zero wants to talk to zero zero zero one so he will simply go like this and similarly another   another line over here may wants to  go straight  down to somewhere over here so many calls can go through together but this is not always the case. now always the case as there may be contention for the same switching element as we will see in this diagram you know because if  if you produce something in very large number you can ammonites the cost of its development etc.over a large number of switches so this cost of this switch comes down so Ethernet switches have become very cheap. These days a switch which use to cost may be lack or so may be 10,15 years back cost only few 1000 rupees today so this is the most common type of switch which we see every where. 
COMPUTER NETWORKS
Prof. Sujoy Ghosh
Dept of Computer Science and Engineering:
IIT Kharagpur
Lecture # 07
Switches-1
(Reference Time: 00:45)

Good day. So today and in the next lecture we will be talking about switches. Switches are the (Refer slide time: 01:01 - 01:03)

most important part of a computer networks and we will look into some aspects of the different types of switches. In this lecture we will look at the kind of switches that are used in telecom networks. There is a special version of these, which we will cover in a later lecture and then in the next lecture we will be talking about packet switches,  which are closer to the heart of data networks. First thing is why do we need switches? (Slide Refer time: 01:38 ? 02:15)

The problem is that each user can potentially call any other user, as we have seen in telephone networks and we cannot have direct lines. We can not have an order n2 kind of lines. So every user is connected to essentially a switch in the switching office.  Switches establish temporary circuits; that means they establish a temporary connection between the caller and the callee at the switching station. Switching systems come in two parts: one is the switch and the other is the switch controller.  (Refer slide time 02:16 ? 03:41)

As you remember from our discussion about telecom networks, people want to talk to  other subscribers, who are in maybe a different location, under a different switching station. The switching stations have to connect among themselves. For making this connection through the signaling that we talked about, there is a control plane over here. Control planes are some kind of computers, which are capable of sending some kind of signals or instructions to the individual switches. In the control plane, we set up the path and there is a corresponding signal from the controller to the corresponding switch for setting up a particular circuit or a particular connection, so that an end-to-end connection from one user to the other is set up. This control plane is sort of an overlay network over the actual switch, and once the connection is set up naturally the call and all the signals, etc., flow in this plane. (Refer slide time 03:42 ? 04:10)



There is a network device called a router and there is routing, which is very close to switching. The only thing is that in routing, we decide about long-range paths. If you remember from our discussion about the seven-layer OSI protocols, you remember that there was a network layer and the router is a device in the network layer whose job it is to find out that in order to reach the ultimate destination station, what is the local link that I have to take here? For a switch it is slightly simpler; it is strictly a local decision but for the router there is a global implication ? what is the local link on the output side that I must take for a signal coming from input line to a particular destination, so that I will finally go through all these connections through various other routers to reach my destination, whereas for a switch the job is a little simpler in the sense that when the signal is coming from one incoming link and is going to go out to a particular outgoing link, which is known at the switch, how to set up the connection? Actually, a router is a switch with some added functionality. (Refer slide time 05:34 ? 06:22)










Many connections will need paths through more than one switch, so that is when we required routing. Static routing uses the same approach all the time; that means same destination is routed through the same path all the time. A dynamic routing may allow changes in routing depending on traffic; that means some part of the network may be congested, so it might want to avoid that path and go to the same destination through some other path. As we have seen at least in telephone networks, because of the way we distribute the telephone numbers to subscribers, it is easy to route. You can look at those incoming digits and just decide on the route. Switching is the local connectivity between input and output lines. Now we will discuss some basic concepts and terminology.   (Refer slide time: 06:31- 07:21)

One is the transfer medium ? transfer medium in a switch, could be buses, electronic buses, through which you can send a stream of cells. Queues are memory locations that temporarily store cells. Switches often require some queues; the queues may be on the input side or the queues may be on the output side; queues may be in various places. So we will look at all these different cases. But these are temporary memory locations, where we temporarily store some cells till we know what to do with it or forward it to its destination. A switching element is the building block for the switch. A switch may be either ON or OFF and if it is ON that connection is there or if it is OFF the connection is not there. Sometimes the switch is like this ? at a particular point a switch may connect either to one point or to another point. A switch is just a simple element and in a big switch, we have a lot of these switching elements. These switching elements form the so-called (Refer slide time: 07:58- 08:09)

switching fabric; that means the shape of the switch: like, how the switches are organized so that all the incoming calls can be handled. There is a term called blocking and non-blocking. (Refer slide time:08:18 - 08:35)

Blocking means a network is unable to connect stations because all paths are in use. A blocking network allows this. This is used in voice systems and the main reason is that we have short-duration calls. The point is that there are so many subscribers, may be thousands of subscribers; if we want to provide the switching capability to the maximum possible extent, that means, if everybody is busy talking to somebody or the other, that means, if every subscriber is using this, then if we try to provision our equipment based on this assumption that everybody may be talking to everybody at the same time, then the switch will become very big and very expensive. So what the telephone people have done is that they have made an extensive study of how many people are likely to talk during a particular time of the day, especially Bell Labs in USA. They were the pioneers and a giant telephone company, which was the largest at one point of time, and later was broken up etc. During the early part of may be 1940s and 1950s, they made a very extensive study about people?s calling habits. They found out that during the busy time or the peak hour when all the officers are open, how many people out of a subscriber base are likely to call. That is one kind of statistic they collected; and the other kind of statistic they collected is that if one person starts talking what is the distribution of the call duration; for how long is this particular subscriber likely to hold on to that line. They found the statistical distribution; and based on that they decided that if I provision this much in terms of our switching elements and switching capacity, then almost all the time everybody will be able to get the service, but exceptions may occur. This is based on some statistical study at some particular point of time. It may so happen that the number of subscribers who wanted to speak is just larger than the overall switching capacity that you have. In that case some of the calls will be blocked. I am just talking in terms of switching capacity. This kind of blocking may occur elsewhere in the network also.  We provision based on some statistical averages but at exceptional times, for short durations of time, we may have to block some of the calls because we do not have the capacity to handle so many calls at the same time. So that is a situation where it is blocking and what people try to do is that they try to calculate the blocking probability, which may be 1%, 2% or 5% ? 5% is quite high, as nowadays people do not even like that. That is the blocking probability that we wish to achieve ? we are provisioning this much for these many subscribers. (Refer slide time:12:08 ? 12:38)


Non-blocking permits all stations to connect in pairs at once. This may be used for some data connection where blocking is not allowed or blocking is not acceptable under any circumstances. If you want to make it fully non-blocking, that solution would be more expensive than if you allow a limited amount of blocking. Of course a high degree of blocking will not be acceptable to the users at all. (Refer slide time: 12:39 ? 13:21)


There are various kinds of switches. I am trying to present here a taxonomy of different types of switches ? not that all actual switches neatly fit into the boxes. Some of them take some portions of this and there is some kind of a hybrid or mixture. But this is one way in which the different types of switches can be classified.  We have space division switching and time division switching. Like in multiplexing we have a time division multiplexing and frequency division multiplexing, here we have space division switching and time division switching. Time division switching is very close to time division multiplexing as we will see. Now space division switches (Refer slide time: 13:37- 14:22)

maybe single stage or multistage. In single stage we have a switching matrix; then we have a knockout network, shuffle exchange networks. In multistage, it may be a single path or multipath like delta networks or Benes networks, sorting networks, etc. In time division switches we have shared memory and shared medium. Shared memory could be a central memory or shared medium could be a bus or a ring etc.; so these are different types of switches.  We will not have  the time to look at all these in detail. So we will just look at some of the very common types of switches that are used. (Refer slide time: 14:23 ? 15:25)

Let us first talk about space division switching. It was developed mainly for analog environment. Why do we call it is space division ? because the different calls go through separate physical paths. That is why they are called space division switches, unlike time division, where different calls may be going through the same physical path at different time slots; we will come to that when we discuss time division switching. But in space division switching, different calls go through separate physical paths. The simplest of space division switches is the cross bar switch. The number of cross points grows as the square of number of stations; loss of cross point  prevents connection; if a particular cross point becomes bad it is an inefficient use of cross points; all stations are connected but only a few cross points are actually in use; and this is fully non-blocking. Let us first have a quick look at a cross bar switch. A cross bar switch is very simple. (Refer slide time 15:32- 16:36)

What do we have in a switch? In a switch you have n input lines coming on from one side and n output lines ? you have to connect some of the line. Let us say I want to connect this particular line to let us say this particular output line, i.e., the fourth input line to be connected to the fifth output line. I want this particular switch to be ON and the other switch OFF. If this particular switch is ON, there is a connection between this line and this line. So I have a direct physical path between the fourth input line and the fifth output line. And each of these crosses that you see is a single switching element and as you can see, since there are n lines i.e., if there are n lines on the input side and n lines on the output side, we have n2 switching elements, which actually is  quite a lot.  (Refer slide time (16:37- 16:38)

So if you go back (Refer slide time: 16:38 -  17:49)

the number of cross points grows as the square of the number of stations. If a particular switch, which is at the crossing point between the fourth input line and the fifth output line, if that particular switching element, somehow has gone bad, then four and five cannot talk to each other ever. That is again another drawback of this kind of switches. Of course it is an inefficient use of cross point ? at every cross point we have a switch and all stations are connected but only a few cross points are actually in use. At most n/2 cross points can actually be in use, whereas we are using n2 cross points, which is a really a poor utilization of the switches. One good thing about this kind of switch is that it is totally non-blocking; that means, all pairs of subscribers can talk to each other at the same time. (Refer slide time: 17:50 ? 18:52)

The cross bar switch is the simplest form of n input lines and n output lines that feed to n2electronic switches. If all lines are full duplex, that means all lines allow communication in both the directions, then we require only half the number of switches. We possibly do not require the diagonal switches, the switches which are in the diagonal. I do not want to talk to myself;  the other thing is that only either the top half or the bottom half of the switches ? top half means  top half of the principal diagonal or the bottom half of the principal diagonal ? only those switches are required . In general n2/ 2 switches would be required if you have full duplex lines.   The  other problem about the  cross bar switches is that apart from the number of switches, the number of I/O pins is also a problem. If you want to a make  let us say  1000/1000  line switches for 1000 subscribers, you may have 1000 incoming lines, 1000 outgoing lines, but how do you put all these 2000 pins on the chip?  It also becomes a problem ? people have gone to more sophisticated design for switches. They are classified  in various ways: through input queuing, output queuing (Refer slide time: 19:32 ? 19:34)

or central queuing. We will look at these one by one later on. (Refer slide time: 19:35- 20:05)

Let us look at another kind of a space division switches known as knockout switches, which were introduced in 1987  by AT&T Bell Labs. Essentially we require  using output queuing over here. We will look at output queuing and the issues in detail later on.  If  the switch is trying to pump in more number of  bits or more number of bits to  a particular output line than that an output line can handle, you have to either drop them or you have to store them somewhere, maybe in a queue,  and that queuing  is associated with the output ports. That is what we mean by output queuing. So N inlets (Refer slide time: 20:26 ?  20:44)










and N outlets operate at the same speed. Transfer medium consists of  N broadcast buses, one for each inlet. N bus interface units, one for each outlet, takes input from all the broadcast buses. Each delivers an output to its own outlet. Let us take a look at the figure. (Refer slide time: 20:47- 21:52)

So we have the N input lines on one side. A call may come from anywhere, let us say   subscriber 1 is trying to call to subscriber 2. So what subscriber 1 will do is that it will broadcast the signal over this bus. There are N buses over here and all these N bus interface units are connected to all the buses, each of the bus interface units ? 1, 2, 3 up to N ? will receive that signal from N but this bus interface unit 2 knows that this is supposed be meant for me so he will allow this signal to transfer to the output line, whereas others will simply ignore it. This is again another fairly simple scheme. We have, of course, reduced the number of switching elements that are required. (Refer slide time: 21:53 - 22:01)

It is not using a bus, and you will not get a very fast switching, but this works fairly well. If you want to go beyond this, that means, you do not want so many lines, etc., as I said and even if you have 1000 subscribers you want to service from the same switch, even in knockout switch you have 1000 buses inside, which is not technically feasible. So we go to the other kind of switches which I showed earlier; namely, multistage switches and multistage switches have one of the most commonly used  switching architecture even today, so far as space division is concerned. In multistage switches we may actually provision less number of switching elements than are strictly required for the highest use, as we want to reduce the cost of the switch. All the input lines can go to all the output lines and it is broken up into several stages as we will see. (Refer slide time: 23:19 ? 24:04)

The advantage of a multistage switch is that it has reduced number of cross points, more than one path through the network; this increases reliability. As you remember, in our simple cross bar switch, if one particular switch is bad that particular pair of users cannot talk to each other; that is not really desirable. Since there are more than one path through the network, even if one particular switching element becomes bad they are still able to communicate. The control becomes more complex and it may be blocking. So, as I said, in most of the cases it will not block, but in some cases, it may be blocking. (Refer slide time: 24:05 ? 24:36)

They have a large number of inlets and outlets which are, sort of, grouped into small groups so that these problems with pins and input lines are not there. Often they have identical switching building blocks, forming a fabric. So there are many switching building blocks consisting of some number of switches and such modules are used all over the switch. As you can guess, in a multistage switch, we will have different stages. The input will come from one side and will go to one stage, then may be another stage, and a third stage, and then may be out. So there are multiple stages and each stage is broken up into small modules. These modules may or may not be identical, so it is sort of cheaper to manufacture and it also handles all the pins, etc., in this fashion. So it often has (Refer slide time: 25:11- 25:15)

identical switching building blocks forming a fabric, and a fabric has multiple stages. (Refer slide time: 25:16 ? 26:16)

This is just one example of a small multistage interconnection network. So as you see, this is one stage, this is another stage and this is the third stage. There are three stages over here. Each stage is broken up into small blocks; so this is an 8 ? 8  sort of switch. If you remember to have 8 ? 8 switches in our cross bar switch, we require 64, or even if we take half of it, maybe 30 or above 30 switches. Here each is a 2 ? 2 switch; so actually you can handle it with only one switch. The number of switches has come down drastically but there are problems, as we will see, that it may be blocking kind of system.  (Refer slide time: 26:17-28:01)

So how do you know that something, which is coming over from an input line, would reach the particular output line? Suppose the output lines are numbered and since there are eight output lines, we can code them using these three bits, from 000 to 111  in a binary fashion. This particular line, which really is the input line number 011,  wants to have a call to the output line number 101. If you know to which output line you want to go to, 1, 0, and 1 really become control signals for these switches on the way. So 1 means go to the lower output line; 0 means go to the upper output line. And you can see that those that are starting with 0 come at the top, those that are starting with 1 come over here and that is the same binary number order that we are maintaining. The switch is simply a rule that if it looks at bit, if it is 0 it connects to the upper line and if it is 1 it is connected to the lower line. So 1 will connect to the lower line, 0 it will connect it to the upper line. 1 over in this stage will be connected to the lower line and we will reach 101. This is almost self routing through the switching fabric and because of this, routing is easy. (Refer slide time: 28:02 ? 28:34)


Delta network is one kind of a multistage interconnection networks.  It is constructed of identical k ? k switching elements. It has regular interconnection patterns suitable for a large-scale chip integration. It is self routing as we have shown, requiring log_k N number of digits and a log-k N number of stages each, with N/k switching elements. So (Refer slide time: 28:34 - 29:53) 

this is the picture that we had seen earlier; this is the diagram of a bigger delta network, may be with 16 input lines and 16 output lines. As you can see, this is a four-stage switch, because in order to code for 16 lines, it will require four bits from 0000 up to 1111. Each bit decides whether that particular switch connects you to the upper output line or the lower output line and from any of these input lines there is a path to any of the other output line. This is the 16 ? 16 Banyan network. As you can see, by doing it in this multistage fashion in a regular cross bar switch, while we require some thing like 125 switching elements here, you require only, say, 32 switching elements. So the number of switching elements is much less. (Refer slide time: 29:54 ? 30:11)

But there may be a problem of contention in a delta network. Cells can be simultaneously switched through the network. However cells may have to contend for the same switching element. Let us go back to the earlier diagram (Refer slide time: 30:13 ? 30:39)

You see this, say, 0000 wants to talk to 0001, so he will simply go like this and similarly another line over here, may want to go straight down to somewhere over here. So many calls can go through together but this is not always the case. There may be a contention for the same switching element as we will see in this diagram. (Refer slide time: 30:42-31:58)

Let us say 0 1 1 0: so 0,  1, and then 1,0; 0 will come here, then 1 will go straight; 0 1 1 0; and then 0 1 0 0. So 0, upper 1,  1, and you see that we have potentially got a contention for the same switching element from two different points in the network; it is sort of converging to the same switching element before it can proceed. So there is going to be a contention over there. If there is a contention, obviously  both of them cannot be handled at the same time; either one of them has to wait. You have to make it wait by putting it in some buffer, etc., or one of them will have to be blocked or dropped. Although we save on the number of switching elements, sometimes we get a  contention. Some ways to reduce contention is (Refer slide time: 32:26 ? 32:37)

to provide buffers in every basic switching element, increase the internal link speed relative to the external speed, and use a backward feedback. These are different  sort of techniques to reduce contention. One is of course providing buffers as we have already discussed; you could increase the internal link speed relative to the external speed. That means  if the internal link is much faster than the external speed at which you are pumping in, then it may be able to handle both of them. So far as the input streams (Refer slide time :33:04 ? 33:31)

or the output streams are concerned, they will not notice the faster switching that is going on within the switch. So that they will think that there is no contention. You can use backward feedback; that means, at some later point in the main if you find that there is a contention then give one of them feedback so that it sort of re-circulates. (Refer slide time: 33:32 ? 34:07)

The other ways are to use multiple networks in parallel; provide multiple links internally between switching elements; and shuffle cells first so that they do not collide later. This is also another kind of switch  that is quite common; it is known as the Batcher Banyan switch. We have seen the Banyan switch part; in Batcher part there is some shuffling of the input cells so that in the Banyan part they do not collide by contending for the same switching element. (Refer slide time: 34:08 ? 34-57)

In the space division switch, the big switch is broken into multistage smaller chunks.  I will show a three-stage switch. Suppose I want an N/n switch; suppose it were a cross bar, then it would be an N/n switch. This is replaced by N/n number of n ? k switches in the first stage, followed by k (N/n) ? (N/n) switches in the second stage, followed by N/n number of k ? n switches in the third stage. So let us take a first look at a simple example. (Refer slide time: 34:58-35:57)

So this is a three-stage switch; in the first stage, we have 10 of them. So N is 10; and k is 2. So we have 5 ? 2 switches. We have 1 2 3 4 5 at the input stage, so N/n, that is, two of them or 5? 2 switches and then two 2 ? 2 switches followed by 2 ? 5 switches. Let us say 5 wants to speak to 2, so 5 comes here; gets switched in this fashion to 2; and 6 wants to talk to 1. So 6 goes to this switching fabric in this fashion in the third stage. (Refer slide time: 35:58 ? 37:59)

If you take the general case of N = 1000, n = 50, k = 10, this reduces the number of switches to 24,000 from about 500,000 because 1000 ? 1000/2  is 500,000; that is, the number of switching elements you would require for a simple cross bar kind of switch, whereas by breaking it up into three stages in this fashion taking this particular value of n = 50 and k = 10, you reduce the number of switches to 24,000. However, the switch can handle only N*k/n. So if N = 1000 and k = 10 and n = 50, so N * k/n would be 200. So only 200 simultaneous calls, so only 400 users can use it at the same time. As I mentioned, it may be fairly alright because you have a phone but you are not talking on that phone all the time; most of the time actually it is just lying idle. If the telephone company makes a good study and finds out that 400 is the maximum number that is sufficient for a very low blocking probability even during busy hour, then that is fine. (Refer slide time: 37:34 -38:24)



The general scheme of this is that you have N lines fitting into each of these N/n  modules at the first stage. So you have a total of N lines fitting into one module. So you have n ? k kind of switches so that this switch can be directed to any of the switches on this side. So there are k of them here there are N/n switch modules in this first stage, k modules in the second stage, and again N/n modules in the last stage,  third stage. So you have (N/n + k + N/n)  modules. (Refer slide time: 38:25 ? 39:28)

In the multistage interconnect network, you may either have a connection based  kind of system, that means, once a connection is made it is permanent till the call is held, as we have in a usual telephone network, or it may be a cell-by-cell, that means, although the user sort of goes on having the conversation, his stream of bits is divided into small cells and each cell is individually routed through the main. Many connections will need paths through more than one switch ? cell-based means routing tag versus network-based, which is routing tables. So if you have tags it is faster; if it is table it is slightly slower and as I said, the static routing uses the same approach all the time; dynamic routing allows for change in routing depending on the traffic. So what is a tag? (Refer slide time: 39:32 ? 39:35)

You remember that in a delta network we were putting 001 etc.; we were encoding the input or the output, specifically the output lines, and then using that address so that the cell can route itself through the network. That is an example of a tag, which means that within a switch each of the lines has got some kind of code in it; you might call it tag. This tag will be used for routing. Now if it is self-routing, like the one shown in delta network, which is very simple, it will be very fast. If it is some other kind of  tag we are using where such a simple scheme does not hold, then maybe we have to look up to some table to find out where I want to go. With tags, cells can be (Refer slide time: 40:27 ? 40:58)

augmented to include extra header information when they enter a switch. That means you look at which is the output line it wants to go to and then add the code for that output line on to this cell, which sort of becomes the tag. For example, with the Siemens ATM chip set, each 53-byte octet or byte cell is augmented to a 64-byte cell as it enters the switch so these extra bytes are for putting in this tags, etc. This extra information speeds up the routing process. (Refer slide time: 40:59 ? 42:09)

In a connection-based system ? we discussed this earlier, but once again ? the path is determined once for the duration of the connection. That means, once the connection is made, for the entire duration of the connection the path remains the same. All cells of the connection follow the same path. Now it may be a connection-based system with cells; that means, once the connection is set up, a particular path is designated that this is the path all the cells, which are originating for this particular call, will take. So, all cells of the connection follow the same path. This guarantees proper sequencing of  cells; of course, they will now go in an order. The connection, of course, may be refused: that means when you are trying to set up a connection you find that in an intermediate stage there is not enough resource. There is too much contention for that, so the connection may be refused. Paths can be determined by a combination of random selection to distribute the load. If you remember, in a multistage switch specially, there are various paths from the input to the output; there are various possible paths, so that if one path is not alright because of some defect or one path is not very good because at that particular point of time that path is loaded, you may route your  path through some other path of the switch. (Refer slide time 42:32 ? 42:49)

You might do a random selection amongst the paths to distribute the load, do a careful selection to arrive at the correct destination ? if this and this selection can be done either centrally or in a step-by-step fashion as is done in a delta network. (Refer slide time: 42:50 ? 43:49)

We have different types of switching ? we have connections plus tags, you have connectionless, which are tags. Of course we will look at connectionless switches in more detail when we discuss packet switches. So the same discussion that we have now, part of the discussion that we are having here, could also be useful in the next lecture. We have connectionless plus tables, that means, if tags are not possible you have to do some table lookup because this table lookup will always be slightly costly in terms of time. The kind of speed that you can achieve with a table lookup becomes limited because of that; but any way if this is again a possibility that you can have connections-oriented system by using tables, you can have connectionless systems using tables; this is also used in routers specially. (Refer slide time 43:50 ? 44:40)

We will look into buffers in more detail when we discuss packet switches. Buffers may be complete sharing, which is the best utilization of memory and best for bursts of equal loads. Complete sharing means everybody can share all, so there is a central memory kind of thing, which is shared. It may be a complete partitioning; naturally complete partitioning would be faster and best for unbalanced load. Partial sharing means some of it may be centralized, some of it may be distributed, etc. There may be various strategies regarding buffers. (Refer slide time 44:41 ? 44:43)

Next we come to time division switches ? time division switches follow almost the same principle as time division multiplexing, in which the same line was being used for communication by different channels; what we did was that we kept a specific time slot for one channel and these time slots go in a round robin fashion. So after a particular frame is over, that means, all the input lines have had their time slots then one particular channel will get back its second time slot, and so on. So it will go on in a round robin fashion. We use the same principle in time division switches; in time division switches the bits or bit streams may follow the same path, that means, different calls are following the same path but they are using different time slots. So a fixed (Refer slide time:  45:55 ? 46:17)


The number of bits from each line is read into a RAM in order ? this constitutes a single frame. That means a fixed number of bits from each frame, each line. A mapping table reads from the RAM in a different order for switching from line j to line k, the j th location is read in the k th step. (Refer slide time 46:18 ? 47:27)

This is shown here. Suppose these are the input lines 0 1 2 3 4 5 6 7. The output is read in a somewhat different fashion 4 7 6 3 0 5 2 1. That means actually whatever the 0, its line 0, puts in this slot would be read by line 4, because line 4 is reading in that time slot. So line 0 is, in effect, connected to line 4. Similarly, line 1 is connected to line 7, line 3 is not connected to anybody. It?s idle at the moment, so 3 3; similarly a 5 5; so it is idle. Similarly, just as we have 0 4 we 4 0; so this is a bidirectional communication, duplex communication going on. So in the first slot or, let?s say, the 0 th slot, the line number 4 reads it and in the 4 th time slot, line number 0 reads it. So line 0 is switched to line 4, 1 to 7, etc. Lines 3 and 5 are idle. (Refer slide time 47:28 ? 48:29)

Basic components of a TDM switch ?we will not have the time to go into the details of this. It has a dedicated internal buffer memory, distinct from the RAM memory used for program code and data. And this buffer will sort of contain the bits which are to be written into and read often on a completely separate physical module, that means, printed writing card. Usually it has at least two DMA bidirectional serial ports. If you have studied operating system, that is direct memory access, which is used for fast communication without the intervention of CPU, that is DMA. Of course we want this as fast as possible; so DMA bidirectional serial ports, input and output, are simultaneous on each port with dedicated hardware for each operation. So our memories are very special; that means, they have multiple port memories and serial I/O on each port. This is a very simplified block diagram. (Refer slide time 48:32 ? 49:25)

Usually in your line, the bit stream will come serially but on the bus internally we want to put them together, maybe 1 byte at a time. So we have a serial to parallel converter for getting 8 bits and converting it to a byte, which will be fed in parallel. So database goes into the buffer RAM memory ? there is a connection memory which is a mapping table that we have talked about. So when you are setting up a connection, you will make entries into the connection memory. We will just have a look at the connection memory, that is, what we have already seen and then it gets  consecutive address generated here and then from the buffer memory once again it  goes to the output side and then there is a parallel to serial converter for feeding into the individual lines. So this is the data bus and this is the address bus. (Refer slide time: 49:26 ? 49:47)

Connection memory contains a list or table for mapping input time slots to output time slots. When a call is set up, you make an increase in this connection memory. The pointer data values are set from the CPU on a per call basis. The data output of this memory is used as an address to access the buffer RAM; decimal representation is shown. (Refer slide time 49:48 ? 50:02)

This is another example. So in address 0, we have pointer or content is 14; that means line number 0 may be connected into line number 14 in this example. (Refer slide time: 50:03 ? 52:06)

Now just one more thing ? that assuming 1 byte is read from each line, each frame is to be processed in 125 micro seconds. You remember how this 125 micro second comes about?  Remember we talked about the voice channel, and each voice channel requires a capacity of 64 kbps as we mentioned. So 64 kbps means that what is the gap between every bit in 1 second ? there are 64,000 of them. So it would be 64 kbps. If we  handle 1 byte at a time, it will be 8 kbps and 1/8 kbps would be 125 micro seconds. So this is the basic time for frame. And in the entire time division hierarchy, as we will see in time division multiplexing, this 125 micro second is a very sacred figure; this is what everybody sort of sticks to so that a voice call can smoothly fit into this. Of course if you want to have a higher speed channel then what you might do is  in the same 125 micro seconds? slot, you may pack in a large number of bits. That is alright but we keep the frame size to 125 micro second. If the memory cycle time is T micro second and there are n lines we have 2nT, because each of them has two operations: one read and one write. So 2nT would be 125. So a 1000 line switch will require a cycle time of about 60 nanoseconds. That is the kind of memory speed, and as we will see, as the number of lines goes up or as the speed of switches goes up,  this memory speed becomes a constraint as with space to division switches, it is possible to device multistage switches. (Refer slide time: 52:07 ? 53:15)

Now just one last point about capacity allocation;  I mentioned that the service providers actually want to provision less than the maximum possible. But what would be the blocking probability if you provision it in a certain manner, let us say, in the C stage switch? So I will just give you the formula. If the average call arrival rate is, say, ? calls per second, which is an exponential distribution as people have seen for  a voice network ? for data network it is different; we will come to that in the next lecture ? the average holding time is m seconds per call, which is Poisson distribution. The call load in erlangs is A = ?m. This is called the load and given a load A, we try to allocate a channel capacity N such that the call blocking probability B is at an acceptable level, typically around 1% or less. This is the typical figure that we try to achieve. The difference is that within the busy hour, we get not more than 1% call blocking probability and the formula for calculating this is this (Refer slide time: 53:16 ? 53:39)

B is equal to A ** N / N ! *(?(A**i/i!) ) ? so you can use this formula or may be write a small program using this formula to calculate the call blocking probability giving values of A, N, i, etc. (Refer slide time: 53:40 ? 54:19)

So you can verify this. This is one last this thing ? verify using Erlang-B and Stirlings approximation for handling the factorial, that for large N, say 100, even with 70 % average channel utilization, that is A/N,  we get very small blocking probability. But of course, if the channel utilization or A/N becomes 80% and then 90%, what will happen is that you will find that your channel probability is going up and then you can do the same thing for a small n and try to see what happens. Thank you. (Refer slide time: 54:22 - 54:25)


good day so  in this lecture we will discuss about packet switches that means packet switches are  switches which as specifically designed for handling packets we have  seen the telecom switches like space division time division switches etc ok we have also discuss something about multistage switches which are used quit often in packet switches also but in this lecture we will specifically discuss about  a packet switches (Refer slide time: 54:59 - 55:01)

which is  at the heart of  all most  all your  computer networks ok  so (Refer slide time 55:11 - 55:26)

first  just to  give you some motivation  that why do we require switches the performance of a LAN can be considerably enhanced by moving over to switched LANs moving over from what  if you remember that some of the earliest   kinds of LANs that we have just seen a brief glance of  some of them used shared memory like a hub or a bus etc now in a if you are using a shared medium what is going to happen is that only  two  nodes connected to this LAN can communicate with each other at a particular point of time where as if you have a switch multiple pairs of  users or multiple pairs of nodes can communicate with each other at the same time so naturally you get a  very good boost in performance of the over all LAN so that is one reason why we require switches of course  we  require switches in other place also (Refer slide time: 56:18 - 56:23)

multiple distinct source destination pairs are able to communicate at the same time thorough a switch so that is why we require a switch in  (Refer slide time ? 56:24- 56:34)

packet networks a packet switch needs to look at the header of each packet to find the destination address of the packet so  you do not have to do a full scale  sort of  lookup of a very big table a  Refer slide time 56:43-57:52)




so you have a you lookup only a  very small table of  V C I and V P I in the V C table replace  but of course virtual connection just as their setup they may be  taken down  so replace old V C I V P I with new ones forward cell to outgoing interface transmit cell onto link so this is the  this is how an a t m  switch works so if you so you  see that the basic  concept of the cell   remains the same   that means  we have   we have some  we have to find out   the output port from the address and the here the address once it is setup after that the output  the address is simply a small table of V C I V P I which makes this lookup process much faster so this together with the fact that the  cells are of same size so you can develop your hardware in such a way so that  this a t m switches will work very fast otherwise the rest of the switch and the switching principle remains the same as we have discussed already (Refer slide time  57:53-58:08)





next comes  comes Ethernet switch which is perhaps the common most common type of switch that you  see  almost every where  as a matter of fact  this  number of Ethernet switches  which are being used and which hare being put to use the almost regularly  is so high that the cost of this switches have also come down you know because if  if you produce something in very large number you can  amountize the cost of is development etc over a large number of switches so this cost of the switch comes down  so Ethernet switches  have become  very cheap  these days ok   switch which are used to cost may be a lack or so may be ten  fifteen years back  cause only a few thousand rupees today so  this is the most common type of switch which we see every  where




COMPUTER NETWORKS
Prof. Sujoy Ghosh
Department of Computer Science and Engineering
I.I.T., Kharagpur
	Lecture Name #8	
Packet switches 
(Refer slide time: 00:40)
Good day. In this lecture, we will discuss about packet switches; packet switches are switches, which are specifically designed for handling packets. We have seen the telecom switches like space division, time division switches, etc. We have also discussed something about multistage switches, which are used in packet switches. In this lecture, we will specifically discuss about  packet switches, which are at the heart of almost all your  computer networks. (Refer slide time: 01:22 ? 01:37) 

First,  to give you some motivation, why do we require switches? The performance of a LAN can be considerably enhanced by moving over to switched LANs. If you remember, some of the earliest kinds of LANs that we have just seen, a brief glance of some of them, used shared memory like hub or a bus. If you are using a shared medium, what is going to happen is that only 2 nodes connected to this LAN can communicate with each other at a particular point of time, whereas if you have a switch, multiple pairs of users or multiple pairs of nodes can communicate with each other at the same time. Naturally, you get a very good boost in performance of the overall LAN; that is one reason we require switches. We require switches in other place also. (Refer slide time : 02:28 - 02:48)

Multiple distinct source destination pairs are able to communicate at the same time through a switch. That is why we require a switch in packet networks. A packet switch needs to look at the header of each packet to find the destination address of the packet. If you remember, when we discussed about connection-oriented and connectionless systems, telephone networks are predominantly connection-oriented, although connectionless technology is also getting there. Data networks are predominantly connectionless although some connection-oriented network technologies are also used in some parts of the network. Today when we are talking about packet switches, we are specifically talking about connectionless systems. In a connectionless system, since there is no fixed connection between the source and destination which is held during a session etc., such concepts are not there. So each packet is on its own and that packet is perishable and contains data from the source node, which is meant for the destination node. It will contain lots of others things like headers, etc., which we talked about earlier but now we are specifically interested in the packets ? that means, the data stream has been broken up into packets. So each packet must contain the destination address of where the packet will finally reach because since there is no space connection, each packet must carry (Refer slide time: 04:12 ? 04:38)

the destination address. And this destination address is sometimes also called the MAC address; MAC is for media access control. There are various types of MAC address. We will come to that later on. Depending on the address, the packet is forwarded to another port. What happens in a packet switch is that through one port, some data has come in; this data will contain a MAC address, this MAC address is the address of the node, the address of the destination where it is supposed to reach, the destination node. This destination node ? the switch has to know that this destination is connected to this particular outgoing link. There is a question of mapping from the MAC address to a particular output line and if that output line has a tag as we had seen in the earlier lecture on some of the switches that they use tags. If they have a tag, depending on this MAC address, this tag could be sort of maybe looked up from table or something and added to the packet so that packet can make its way to the switch and reach the final, correct output line, and from then on to the destination station.  (Refer slide time: 05:42 ? 06:57)

Depending on the address, the packet is forwarded to another port. Since more than one packet may arrive at different ports destined for the same output port, they may have to be internally queued. This was also discussed in the last lecture, that if we have more than one packet which is sort of destined for the same output port, obviously we cannot take both of them together. One of them has to wait in some memory or in some buffer or maybe it will be queued because there maybe other packets coming up for the same port so they will be all queued and this queue could be serviced by the output line 1/1, that means 1/1 will be popped out of the queue. You may queue it right at the input, if you see that the output port is busy, you may queue it at output port or somewhere in-between. Not dropping packets and being fair to all input lines are desirable criteria for packet switch design. That means we do not like to drop packets; we want to buffer them as far as possible. Of course if a switch is of small capacity, which is swamped by a huge number of incoming streams, etc., maybe for the same output port or something the queue, the switch may be forced to drop some packets. But usually we would not like to do that. We would not like to drop any packet and at the same time we do not want to starve any particular input line. In general, we like to serve all of them in a fair fashion. These are the criteria we use for evaluating a design for a particular packet switch. (Refer slide time: 07:37 ? 07:56)

The first-generation packet switch went through several generations, we are in the 4th generation or something. Anyway, the 1st generation packet switch is here, it simply uses a CPU, stores packets received online cards in its main memory.  These packets are routed to output queues, the packets in the same queue are scheduled by software for access to the output line card. Let us look at this, the simple kind of a situation. (Refer slide time : 07:56 ? 09:27)


We have a CPU with some memory. There are particular input and output queues; these are the line cards. What are the line cards? This is a 6 port switch as you see; there are 6 lines, and there are maybe so many ports. These ports, some of the functionalities are collected together and they are implemented in a small dotter card kind of thing, which is called a line card. A line card will handle just a few of the lines, and there will be several line cards on the same switch. If you go back these packets are routed through the CPU, which does the lookup ? it looks at the MAC address, the particular output port to which it must go. CPU will have all that information. The CPU looks up and then it directs that packet to the particular output port. There maybe a queue and this queue is scheduled by software for access to the output line card. This was the first-generation packet switch, which is fine ? excepting that because of this  CPU and memory storage, looking up, tables, etc., it takes time. These switches were  (Refer slide time: 09:27 ? 10:56) 

comparatively slow. Next we came to the second-generation switches. Here some of the routing functions are involved. Do you remember what the routing function is? The routing function in general means that we look at the destination address and decide which output line this packet must go to. And this routing function, if you are doing centrally, naturally it will be slower. You can distribute it somehow, like distributing some of the routing functions to the  line cards themselves. Line cards which can switch locally if the destination is on the same line card ?  there is another point you can do that ? and if a particular line cards sees that the output destination will go through the same line card through a particular line, then the line card need not send it to the CPU at all. This can switch locally. So that was one improvement over the first-generation switch. Secondly, there is contention for the shared bus; that is of course a problem: a contention for the shared bus, and packet switch cannot be routed and the same is handed over to a central processor. This cannot be the local line card, which does not have enough intelligence and knowledge to know how to route it. In that case that is sent to the central processor. (Refer slide time: 10:56 ? 11:32)

In third-generation switches, the shared bus is replaced by a switch fabric, which is an interconnection of buses and switching elements. The shared bus is replaced. If you remember, we can use a bus for broadcast, etc., like we did in knockout switch. That may become a problem because of the contention for this shared bus. So we replace that by the switch fabric, which is the interconnection of buses and switching elements. (Refer slide time: 11:32 -11:52)



When a packet arrives in this third-generation switch, which we will discuss in detail, when a packet arrives at an input port it is tagged with the output port ID by a control processor. The switch elements automatically routes it to the correct output port, as we have seen, this routing takes place in a MIN. The buffers in the switches can be associated (Refer slide time : 11:52 ? 12:21)

with the input or the output or with individual switching elements. The switch fabric may be cross bar, broadcast, Banyan or other configuration. We have seen some of these configurations; remember the cross bar switches? That means just a simple connection of switches; then we have some broadcast like the knockout switch, maybe a Banyan like a multistage switch or other kinds of configuration. (Refer slide time: 12:23 ? 12:54) 

There are three types of switching fabrics, one is that we are using through the memory, other is one which you switch through a shared bus or you have a fabric of switching elements like cross bar ? it is the simplest kind of fabric. Of course you can have more complex fabric over here as we have seen, like multistage interconnect network. These are the three different types of switching fabrics that we have. (Refer slide time: 12:54 ? 13:48 )

Switching via memory: this was the first-generation routers as we have discussed. Packets are copied by a system, a single CPU. Now, speed is limited by memory bandwidth at two bus crossings per datagram, what is happening is that from the input port, some packet is coming through the system; the bus sent to the memory where the CPU is looking into it and then decides where it must go, then sends it back to the data bus in that output port, which catches it and throws it out. This is the sort of part which gives a bottleneck to this entire scheme because this bus part tends to be slow. (Refer slide time: 13:48 ? 14:04)

In modern routers, as I mentioned input port processor performs lookup, copy into memory, etc. An example is ? this is short of commercial system ? like Cisco Catalyst 8,500, which would take this kind of approach. (Refer slide time: 14:04-14:40)


Switching via bus: datagram from input port memory to output port memory is via a shared bus. Bus contention: switching speed is limited by bus bandwidth. You have very fast buses like 1 Gbps for Cisco 1900, which is low and Cisco switch, which has now been discontinued. It has sufficient speed for access and enterprise routers, not regional or backbone. But this bus structure does not really do well for higher level switches. And one example of bus structure is a knockout switch. We have already discussed, just to recapitulate, we have n bus interface units, one for each outlet and each takes input from all the broadcast buses; each delivers output to its own outlet. This is, if you remember the diagram of the knockout switch, you have these buses on which the data is broadcast from the input side. In the output side, particularly bus interface unit for which it is destined, it catches the particular packet; others ignore that packet and so it is queued up over here ? like q1, q2, q3, q4, etc., and it is pumped out as per this queue: on a first come first serve basis; this basis could be something else. Also if you want to have some other pair, also in particular packets, etc., it?s possible. But in the simplest case, this is a first come first serve kind of queue. (Refer slide time: 15:47-16:16)


   


Switching via an interconnection network are just as in the other kind of switches we have seen. It overcame bus bandwidth limitations, that is the basic implication. As we want to go faster and faster, we have to migrate from buses to an interconnection network. We have already seen Banyan networks. Other interconnection was initially developed to connect processors and multiprocessors, etc. It became useful in this class of  packet switches. (Refer slide time: 16:17 ? 16:29)


Advanced design may involve fragmenting datagram into fixed length cells, switch cells through the fabric ? this was the idea. For example, this is done in, let us say ATM switches. In ATM switches what they do is that whatever the packets may come in the form of packets, it may come in the form of streams originally; it is finally broken into fixed length cells. The good thing about a fixed length cell is that you can, if you know length in advance, then you can design hardware so that it can be handled in faster a possible way. Although the ATM cell has 53 bytes inside the switch, it is something like 64 bytes, these 64-byte cells are fixed length cells and they are pumped as fast as possible. It is for designing very fast interconnection network, in which we fragment them into fixed length cells. Cisco 12,000 switches gbps through the interconnection network: when you go to this gigabit kind of switches, you require an interconnection network or maybe something more as we will see. (Refer slide time: 17:38 - 17:47) 


As was discussed earlier, we have three kinds of approaches to handling, contention and buffering, etc. One is on the input side, another is on the output side; the other one is somewhere in-between. We will discuss these one by one; first let us come to the input side but before we go forward, I will digress a little bit, I will just explain a little bit about what is involved in the input side, the input part of the switch. If you remember in our discussion about the several OSI models, in the bottom-most layer, we had the physical layer. Above that was the data link layer, above that we had the network work layer, above that we had transport and other layers, which do not occupy us at the moment. We will look at these later but we are having the physical layer, the data link layer and then network layer or the routing layer, these three are sort of very closely linked to switches operation. First is the physical layer, that means, physically the link is coming in some fashion. That means there will be certain kind of connecters, certain number of lines are coming per input line when we are saying one input line, but the one input line may consist of a number of wires when you are connecting. For example, if you are connecting through a cat -5 cable, there are a number of wires which will connect to that particular port; that is, a single port. Similarly, in other cases, you have a physical layer, you have an optical connection coming in, and you must have an optical detector over there, etc. This also means physical characteristics of the media have to be handled by the input port. If you look at this picture again, we have just the input part shown over here: this side is the switch fabric, that means, getting inside the switch so here it is sort of the  line card itself. First there is a line termination, which takes it specification, etc., from the particular physical layer that is being used. We have something like a data link layer functionality. An example of a data link layer kind of protocol is the Ethernet. We will, of course, look at details of Ethernet later on. What does the data link layer contain? The data link layer contains the address called the MAC address, which we have talked about in the last lecture ? the MAC address, the media access control address ? of the next machine to which it is destined. Please remember that these addresses are addresses of the machine, because the source node knows the destination, let us say. It may not know it also directly. Anyway let us forget about the large network, let us now focus on a particular switch and some nodes connected to this switch. This node A knows that I  want to communicate with the node B. Of course node A, B, etc., do not have any meaning at this level. So at this level, some particular address is given called the hardware address, which actually is the address of the networking card. If it is a computer that you have as a node, then the networking card of the computer, the network interface card, the NIC card, it will have some address built into it in the hardware. Similarly, the destination will also have another hardware address. This hardware address is put in this data link layer part and that means the data link layer puts this hardware destination hardware address into the packet. (Refer slide time: 21:53 - 22:22) 


This address has to be read and then somehow you have to make a mapping from this address. This particular address is connected to this particular line. For that you need some lookup, forwarding, and queuing ? if you are doing input queuing, you will do the input queuing also. What kind of lookup? This lookup will tell you that for this particular MAC address that is the output line card. Nowadays, what the switches do is that they  keep on sort of getting this information in a dynamic fashion. When a switch is connected to a number of computers, the switch will listen to the network traffic and find out all the hardware addresses, which are connected to these different lines. It will sort of slowly gather this information and store it inside; it will know that whenever some particular node is sending, it will give it the source address, also called the source MAC address. It will now get the source or MAC address and this particular MAC address must be connected to this line. This switch keeps on gathering kind of information, and storing inside; and part of it maybe stored locally in the line card. Some part of it maybe stored centrally and there are thousands ? nowadays, a switch can remember thousands of such MAC addresses, which are directly connected to the switch. There is another level of problem, which is finding about the addresses of remote machines, machines that are not connected to this switch at all. Machines, which are connected through others switches, routes, etc., in a different country, maybe. So that is a different problem; that is a routing problem, but in the data link problem we are concerned with all the MAC addresses of all the nodes which are connected to this particular network which is local. So that is stored in this switch; you will now look up the table (Refer slide time: 24:07 ? 24:21)

and decide that that is the final destination output line, which is the forwarding line and then you do the queuing. Then you send it to the switching fabric for its final destination. (Refer slide time: 24:22 - 26:04)

We will now talk about decentralized switching: given datagram destination, there is lookup of output port using routing table in input port memory, the goal is complete input port processing at ?line speed;? so this is what I want to do. We do not want to hold up this particular packet; you get the maximum throughput at the maximum speed. We do not want to hold up any packet in any buffer ? ideally that is what we would like to do. So we want to look it up locally at the input port itself, if that is possible. But please remember that even if you can do all this lookup business at the line speed at which the packet comes, even then you maybe forced to store the packet for some time. Because either the output port where this particular packet is headed is already occupied because it is sending some other packet through it at the moment to some intermediate switching element, there maybe a contention for intermediate switching elements, etc. For all this contention, etc., we maybe forced to put this packet on hold even if we do the lookup at line speed. So we have to queue this in the datagram and the other possibility is that if the datagram arrives faster, then forwarding it into switching fabric and forwarding maybe constrained by all this contention, etc. Then you have to put it in a queue. So what you can do is that you can put it in the input (Refer slide time: 26:05- 27:00) 


 
Side; you can buffer it at input called input buffering. Packets are buffered at the input. They are released when they gain access to both the fabric and the output trunk. That means both the internal switching elements as well as the output port must be free and then this packet maybe released from the buffer, which is held at the input. All elements except the arbiter, which is combinational, needs to run only as fast as the lines. So this is a good point about input buffering that elements do not need to be faster than the rate at which the lines operate. In some other cases, we will see that at faster speed, this is a big problem, this input buffering, which is known as the head of line blocking. Let us look at head of line blocking. (Refer slide time: 27:03 ? 28:33)



Look at this head of the line blocking ? you see what is happening is, here you see there are three input ports, and this is the switch fabric. There are three input ports; the destination is shown by the color, that means the following: red means it is destined for this particular port; blue one is destined for this particular port; green one is destined for this particular port. See what is happening, the first packet, red, comes here and goes to the output port; this one is also red, which means it is destined for the same output port although there is no contention in the switch fabric; it has to wait. That is accepted. The trouble is, this green one, which was sort of destined for something in the middle, and which could have gone through this fabric at the same time as this red packet, is flowing at the top. This has to wait because the queue is blocked by this particular red packet. This is called head of line blocking. At the head of the queue, there is somebody who is blocked and all the people who are behind, all the packets which are behind, have to wait, this is called head of line output port contention time t, when only one red packet can be transferred. So the green packet experiences head of line blocking. This is the head of line blocking, which is a problem with input buffering. Other reason you might require input port queuing is that the fabric is slower than input ports combined. Input ports combined means it is a special case when all the lines are trying to pump data at the same time. What you require is a sum total of all the speed that you get over there, which is not always possible, because then surely there will be contention inside for the switching element. So you will be forced to queue some of it at the input head of line blocking. We have already discussed that queue datagram in front of a queue prevents others in queue from moving forward. So there is queuing delay and loss due to input buffer overflow. This is the other thing, which might happen; what might happen is that this queue is given for each input port. At each input port since this buffer is not shared, this one is only for that particular input port. Naturally the size of the queue is limited; if it so happens that data is coming in at a fast rate and the fabric is very congested, it is not getting a chance to go through, this queue at the input port may overflow. If they overflow, which means that some packets have been lost, it is a very undesirable situation; but this may happen. This is the head of line blocking. So the other option is to buffer (Refer slide time: 30:33 -30:58)



them at the output; that means, you have a queuing buffer management, this part, then the output part is being fed from the switching fabric; queuing buffer management; data link processing, that means, protocol decapsulation for the next jump, etc., and finally the line termination. Once again you have to go to the physical layer over here. This is the output port queuing. (Refer slide time: 31:00 ? 31:13)

What are the implications of output port ? queuing buffering required. When datagrams arrive from fabric faster then the transmission rate, the fabric is pushing a datagram into a particular output line at a faster rate than the output line can pump it out. Naturally packets are getting accumulated near the output port, so packets are getting queued over there. So you develop a queue.  (Refer slide time: 3:30 -32:11) 
	


There is a scheduling discipline, which we might choose among the queued datagrams for   transmission ? which is the one that gets the higher priority, if you are putting some higher priority etc., and even if you are trying to give the same priority to everyone, it might so have happened that we might have kept one particular packet quite long in other places; we may have sort of fed it back; I did something with it as it was already late. We might like to give higher priority than other packet, etc. So there maybe a scheduling which is going on in this. Queued buffers are maintained for each output line. (Refer slide time : 32:11 ? 32:25)


This is a great advantage, because it does not suffer from head of line blocking. Naturally, because it is been buffering as soon as possible, but there (Refer slide time: 32:26 ? 3:42 )

are problems. Later stages of switch fabric and buffers have to be N times faster in the worst case to avoid dropping of packets within this space. This rule makes the switches more expensive. Another thing is that suppose, in the worst case, all the N inputs, say N ? 1 input lines, are trying to feed data into the same output port. What is going to happen is that these packets, which are coming at the line speed through N ? 1 lines, they sort of start converging to the Nth port. How can they converge? Obviously, if everything is operating at the same speed, then there is going to be contention in these switching elements or one way to avoid this contention is to make switching elements faster. That means suppose two of them have to go to the same switching element but suppose that the switching element operates at double the speed ? I am just giving a simple example ? at double the speed but at the same rate at which it is coming, it can push it forward. So as you go more towards output port, the speed requirement becomes higher and higher. 
Finally, near the output, at the very point of the output port, it has to be N times faster approximately, which is not done like N times faster. That is not possible but the point is this is a difficulty which we have with output buffering; although we do not have head of line blocking, they tend to be more expensive if you have a buffer fabric. I mean there is always a sort of trade-off between how expensive a particular switch is and what its  performance is. Naturally if you want to go for higher and higher performance where it is very important, then you need to sort of get switches that are more expensive. (Refer slide time : 34:47 ? 35:31 )




We have the buffer at the input and buffer at the output. The buffer maybe distributed in-between all over the fabric, the buffer is associated with each switching element, which is used when the next stage is not available. It needs a lot of memory and quality of connection is difficult to control. The point is that if, throughout switching fabric, if you have distributed our buffer, it is very difficult to have any kind of policy about priorities, etc. The switches will handle it any way they can, so we cannot have a policy which holds over all the buffer because we do not have processing capability over there; it is just a buffer and a switch. (Refer slide time : 35:32 ? 35:44)
	

This is an example of a switching fabric element. You look at this switch in the center. We have the switch and then with two input lines coming in, you have a buffer over here. If the next switching element is busy, then that particular thing can be put into the buffer, from the buffer it will come back to the switch and then get switched when the next element is available. (Refer slide time: 36:10-37:04)



Buffers need to be only as fast as an element?s fan-in ratio, and during overload, the resulting queue is distributed. There is one good thing about distributing the buffer ? which is that instead of putting all the buffer either on input side or the output side as the output side gets more and more congested in one particular area, what will happen is that the previous switching element will also get congested. They will buffer something and then the buffer will be pushed back. What is happening is that there is a distribution of this buffer all over the fabric. These later stages need not be all the way up to N times faster than the initial stages, which is not always possible. So that is one good thing about buffer fabric. (Refer slide time: 37:04 ? 37:35 )





We will look at some examples of packet switches, we will talk about three switches. That is, Ethernet switches, ATM switches and IP routers ? these are all packet switches. All of them hold very important positions in the world of networking; we will just look at these one by one. (Refer slide time: 37:35 - 37:51) 







Let us take up an ATM switch first. If you remember, in the ATM system, we have fixed sized cells ? in the ATM world, there called cells, then in the Ethernet world, etc., they are called cells packets, datagrams, etc. Anyway they are called cells; these are of fixed size, like 53 bytes etc. Each cell will contain a so-called destination address and the destination address is in two parts: one is called virtual path identifier and the other is virtual connection identifier ? VCI and VPI in the virtual connection table. What happens is that ATM is a set of virtual connections; what is a virtual connection? A virtual connection is somewhere in-between a physical connection on one side and completely connectionless, let?s say Ethernet packets, on other side. In a completely connectionless Ethernet packet, that packet contains the full destination and at each node in-between, this destination is interpreted, looked up into some table, etc., some algorithm is done, and then you decide what is the next step to go to. In a connection-orientated system, physical connection through the switches is set up from the source to the destination. In a virtual connection, virtual connection is also set up. So there is initial set-up time just as there is for connection-oriented, purely physical connection-oriented systems. Similarly for virtual connection also, we need a set-up phase and then you can start communicating. In the set-up phase, all the nodes in-between know that we are going to have a session, where the destination is going to be; what would be its next address, and so on ? it is computed, let?s say, as we do in Ethernet, etc. But once it is computed, it is coded in a very small VCI/VPI. The port one has to go to for this particular session is identified by the VCI and VPI. So this computation is done, recomputed at each of the intermediate nodes, and then the virtual connection is set up; that is all. There is no physical connection. So as soon as there is some cell, which is for this virtual connection, an intermediate node will immediately know that this is the next step and that is where it will push it. So you do not have to do a full-scale look-up, you have (Refer slide time: 40:29-41:40)




a very big table so we look up a very small table of VCI and VPI in the VC table. But, of course, just as the virtual connections are set up, they maybe taken down. So we replace the old VCI/VPI with new ones; forward cell to outgoing interface transmit cell onto link. This is how an ATM switch works. You will see that the basic concept of the cell   remains the same; that means, we have to find out the output port from the address. Once the output, the addresses are set up, it is simply a small table of VCI/VPI, which make this look-up process much faster. With this, together with the fact that the cells are of same size, you can develop your hardware in such a way, so the ATM switch will work very fast. Otherwise, the rest of the switch and switching principle remains the same as we have discussed already. (Refer slide time: 41:40- 41:55)

Next comes an Ethernet switch, which is perhaps the most common type of switch that you see almost everywhere. As a matter of fact, the number of Ethernet switches which are being used almost regularly, is so high that the cost of switches has also come down. It is because if you produce something in large numbers, you can amount the cost of its development, etc., over a large number of switches, so the cost of switches comes down. Ethernet switches have become very cheap these days (switches which used to cost maybe 1 lakh or so 10, 15 years back, cost only few thousands of rupees today). This is the most common type of switches, which we see everywhere. If it is an Ethernet packet, then it has to handle the Ethernet addresses which are in the packet. The Ethernet address is actually a 6-byte address, so we will have a 48-bit address. Usually we do not get to see it as users, because this is 6 byte; it is very difficult to remember such a long address, and these addresses come in a very random fashion. It is very difficult to remember Ethernet addresses, but human beings need not remember Ethernet addresses. Ethernet addresses are learnt by the system; when a system is put in a network where the system maybe a computer or a node or the system maybe a switch or a router, they learn about the Ethernet addresses, local Ethernet addresses, which are there. We will get into the details of Ethernet, etc., later on. But anyway this lookup is necessary. The first step in Ethernet switch is look up frame destination address in forwarding table; if known, forward to correct port; if unknown broadcast to all ports. So this is what an Ethernet switch does. Suppose a packet comes with a particular destination address, and if the destination address is known, the node having this particular address is connected to this particular port. Then the output port, for this packet?s output port, is forwarded to the correct port. If unknown, you broadcast it to all ports. Why is this? If you remember as I mentioned earlier, these switches automatically learn from whatever is going on; they automatically learn the MAC addresses of the local  machines. But suppose it is put in a network, in the beginning it does not know anything. What does it do? What it does is that, as soon as it comes from some port, it broadcasts it to all the other ports. It will listen and find out which is the one that really was successful and there are various protocols for finding these things out; it will slowly learn. It may so happen that this switch may get a packet for which the destination address is not known to the switch. In that case, it broadcasts to all the ports. Then we have, learn source address of incoming frame ? one thing of course is learned from this frame, although the destination address is unknown, it knows the source address; the source address is the particular incoming link it came in through. This particular address must be the node having a particular address, must be connected to this port so that later on if this particular node comes as a destination, it knows where to send it to. Forward frame to outgoing interface ? that is the other thing, transmit frame onto a link. These are the various functions of an Ethernet switch. (Refer slide time: 46:07-46:17)



We have something called an IP router; router as you know works at the network layer; it is not in the data link layer. That means a router has a different set of address called IP address; so IP address is a global kind of addressing scheme. There is some scheme for this IP address although not as good as the telephone numbering scheme, but anyway, there is some central control. The job of the IP router is to find out this global address that I have got. Maybe this particular node is in some other country; it has to understand that for going to that, reaching that final destination, I have to take this link locally. Then it will send it to the next level, that is, the data link level, for hopping that particular local layer. Once again, we have the same problem that we have got an address, some kind of destination address, which happens to be an IP address. We have to do that, look up, we have to decide this is the port it must go to and then send it to the port. So the same switch is used with some added processing capability for doing the process router ? usually it comes with all kind of other functionalities and also it needs some processor of its own and it needs some memory naturally for running programs, etc. There is some kind of an operating system which is there in a router, so a router is more complex but the hardware part, the major part of an hardware, is still the switch and the input and output line ports. Usually a router can handle different types of ports; for example, usually an Ethernet switch will handle only Ethernet port. So the physical characteristics or the physical layer  properties of the ports are constant for all the ports, but a router can cut different types of cards, which can handle different types of ports: maybe serial port,  high-speed serial port, and all kinds of things.  (Refer slide time: 48:27 ? 49:13)


But the basic thing remains the same look-up, the packet destination address in forwarding table ? if known, forward to correct port; if unknown, well, the router cannot broadcast it because if all routers start broadcasting then there will be broadcast all over the world through all the routers, which is not really acceptable at all. You can broadcast something only locally by an Ethernet switch; so if the destination is unknown, and it has no way of finding where to send it, it simply drops the packet. When it sends, it decrements the time to leave, update header, checksum ? so these are some of the side-effects which it has to go through ? let me not go into the details of this ? and finally forward the packet to outgoing interface, transmit packet on the link as usual. (Refer slide time: 49:14 ? 49:26)


This is a router ? once again we have a switching fabric, we have input port and we have output port, and we have a routing processor. (Refer slide time: 49:27-50:55)

Let me quickly look at some of the issues in developing fast packet switches. First thing you must know, understand, is that the demand for communication speed is growing much faster than that for computational speed. You know that in computers there is this Moore?s law, which tells that every one-and-half years maybe, the speed doubles; the memory capacity maybe doubled; etc. But if you look at the way growth has taken place in the communication field, especially with this speed of communication which is required, this is much faster than the similar developments in the computational area. There are two reasons for this: one is, of course, people are trying to communicate more. People previously were just satisfied with voice; now they want to send data, they want to upload or download large files, they want to send movies, etc. The individual demand for bandwidth is growing, that is one thing; and the other thing is that more and more people are getting connected to the network. As more and more people get connected to the network, the number of possible connections goes up as N2; if there are N people, as N goes up, it goes up in the squares so all this is going through the central backbone, etc. At the backbone level, the speed requirement is very high, so we require very fast packet switches and we will look at  (Refer slide time: 50:57 ? 51:08)

just some issues over here. The main bottleneck in an ultra fast switch is the speed of memory used in buffers. This is the main bottleneck. (Refer slide time: 51:09 ? 52:17)

For example, if you have a 5-nanosecond SRAM, that is, static RAM ? it is the faster kind of RAM rather than DRAM and you have 64-byte wide bus ?you have 5 ns per memory operation: that is your memory speed, two memory operations per packet. You have to write it there and you have to read it, so that makes it 10 ns and you have 64 bytes, which means 64 ? 8, that is, 512 bits ? so  you are handling 512 bits in 10 ns, which comes to 51.2 Gb/s. This looks like a very high figure but now, we get switches which have speeds of maybe 100 or even 200 Gb/s. They are very high speed, so the buffer memory is the maximum. This is not as simple as that; ultimately it comes to about 40 Gb/s or so. We use the fastest possible RAM but even that is not enough. So we can take some possibility: (Refer slide time: 52:18-52:33)

one is the ping pong buffering, which means the packet comes over here. There are two buffer memories: one is that either you send it or you put it in a buffer like this. (Refer slide time: 52:34 ? 52:59)

For example, when it comes, maybe one of them is being sent while the other is being buffered. Then this one will be sent and the next one that comes, will be buffered. So you are trying to double the speed at which it is operating. You require two buffer memories for it. That is one approach.  (Refer slide time: 53:00 ? 53:01) 


(Refer slide time: 53:02 ? 53:27) 

 The other approach is to use multiple parallel buffers; instead of two, you have many of them and you put them in buffer in parallel, so you can go about it maybe in a round robin fashion, put in an multiple buffer. And the other bottleneck is that, especially in a router, if that your processor, if that is centralized, then that becomes the bottleneck for speed. (Refer slide time: 53:27-53:35)

We have the fork, the so-called fork join router, where maybe you have put a number of routers in parallel. These lines are coming at a particular rate; they are distributed over all the routers so the routers individually do the look up, etc., at whatever speed they operate on. Then they keep on pumping this side as well as this side ? these are combinational so they can operate very fast ? the look up, etc., is paralyzed. These do not have any buffers in them, this part has the buffer. So this way, by going in for parallel operation, we try to make as fast switches and routers as possible. This speed is going on increasing as they have to increase in order to cater to the growing demand of packet switches, speed of packet switches, and capacity of packet switches, all over the world. Thank you. 



Preview of next lecture
Lecture No # 09
SONET/SDH
This lecture, we will discuss about SONET. The word SONET stands for Synchronous Optical NET work sonnet in the USA, Canada and Japan,synchronous digital hierarchy. elsewhere for example in India, We will be calling SDH, this is a time division multiplexing system that transmits a constant stream of information  (Refer the slide time: 55:14 ? 56:57)



Please look at this figure so we have some multiplexers. As the figure shows, we have a multiplexer in this side another which is an output fits to another multiplexer, this multiplexer is going in this direction and then after some time, the signal becomes weak. We want a repeater, that means what is a repeater? A repeater is something which boosts the signal strength, so there is a repeater then it travels some more distance and then there is a repeater again and then it travels some more distance and the other side. We have the corresponding demultiplexer and then it fits into the other  demultiplexer. From repeater to repeater, we call it as section so all from repeater to multiplexer this is also a section. Multiplexer to repeater, repeater to repeater, these are called sections and then from multiplexer to multiplexer we call it a line. At the repeater nothing happens except  the signal is cleaned up, signal maybe boosted or there maybe other cleaning operation synchronizing operation etc that maybe done at the repeater but as such the signals which are traveling here are the same set of signals are traveling here. At a multiplexer some of the signals may go off in another direction, some signals may go in some other direction etc. At the multiplexer there maybe convergence or divergence depending on which way the signals is flowing so that may happen at a multiplexer so from multiplexer to multiplexer, we call it a line and then from the end user part point to end user point, we called it a path. 
(Refer slide time: 56:57 ? 57:09)




The SPE does not necessarily start in column 4 which mean that the SPE that does not necessarily stay within 1 frame, these are 2 very important  points in. On it, the point is    that all through you have 87 columns but actually the data may start getting transmitted as some arbitrary  point  inside that 87 columns. What is the idea? Why do want to leave something and they only start from the middle?  The point is that if there are some kind of mismatches because of miss matches of rate etc, if everything in the world were absolutely synchronous, all activities an all equipments etc then would have started from the beginning but that is not the case. This is where we absorbed this kind of variation  and this gives a great flexibility to solve it which was not there earlier. Other interesting thing is that SPE does not necessarily stay within 1 frame which means, that the SPE start in 1  frame and then 1 in another, we will just look at a  diagram of this. Let us have a diagram of this, you see the SPE, this light green color that really starts from somewhere.   After leavening some of the root, starts hear in the path overheat is somewhere hear and there are 2 frame is here so the SPE is really spent in both  the frames. 
 

COMPUTER NETWORKS
Prof.Sujoy Ghosh
Department of Computer Science and Engineering
IIT, Kharagpur
Lecture#9
(Refer start time: 00:47)

Good day. In this lecture we will discuss about (Refer slide time: 00:54 - 01:16) 


SONET. The word SONET stands for Synchronous Optical Network, SONET in the USA, Canada, and Japan, Synchronous Digital Hierarchy elsewhere. For example in India we will be calling SDH. So this is a time division multiplexing system that transmits a constant stream of information. (Refer slide time: 01:17 - 02:10)


SDH is actually a successor of PDH. Few years back we used to have a PDH gear in our telecom infrastructure in the wide area network part that is plesiochronous multiplexing (nearly synchronous). This business of being a nearly synchronous introduces us a lot of problems and complications. So, when from this nearly synchronous we went to synchronous, that was a major achievement as well as improvement of services as we will see later on. In the PDH multiplexing in which two or more signals are transmitted and nominally in the same digital way and the significant instance occur at nominally the same time.  This was PDH. (Refer slide time: 02:10 - 2:34)


When SONET was introduced, it had a number of achievements to its credit. Firstly, it is a standard multiplexing using multiples of 51.84 mbps, that is an STS-1 rate and STS-N ? we will look at this rates. This was used as building blocks. This is something that is to be understood that why it is that particular rate value is so important. The point is when you are multiplexing the original source may come from various sources, and these signals will travel, will get together, will separate out and then mixed with others, etc. That is possible only when we have in international multiplexing standard and this international multiplexing standard was first achieved in SONET. What happened was previously of course the rate which people used were must less and they had all kinds of differences. As technology grew and different sort of countries on different  things came together, they came together at a certain rate of transmission and this is a basic building block of SONNET. There is a standard multiplexing using multiples of this particular day. That in itself was a big achievement. (Refer slide time: 03:50 - 04:23) 


Secondly, it also first stated the optical signal standard for interconnecting multiple vendor equipment. the point was previously of course at lower rates they were all electrical signal standards, SONET has both has electrical signal standard as well as optical signal standard and in this optical signal standard it was possible to bring together multiple vendors to agree on to some particular format. (Refer slide time: 04:23 - 05:23)

And the third achievement in SONET was extensive OAM & P capabilities. So what are OAM & P? O is for operation; a is for administration; and m is for maintenance; P is for protection. Maintaining the system, administrating the system, operation of the system etc., are much more flexible in SONET when compared to others. What kind of flexibility, etc., that we will see. Regarding protection also, as a matter of fact it is so strong in SONET, that we will specifically discuss this aspect in a separate lecture when we talk about protection. These are very strong points in SONET and that?s not all. (Refer slide time: 05:23 - 06:50)

The fourth one was multiplexing formats for existing digital signals. It?s not that such a development can take place in vacuum ? that means they had some history and the trouble was that different countries have different kinds of histories. It is not feasible for a technology to come and say throw away whatever you have been doing and put this. There will never be forklift upgrade; it is never possible because of cost, practical considerations, and all kinds of things. So an evolving technology, in order to be successful, has to bring together previous technologies so that they can merge into this new technology and that was another SONET achievement. These existing digital signals ? these DS1, DS2, etc., are different multiplexing standards at the low end. By the way there is also DS0 and DS0 rate is our venerable 64 kbps line rate. Do you remember once again that for voice channels, we require a 64 kbps because of PCM, etc. we have already discussed. So that is a DS0 rate and these several DS0 get together to form DS1  and so on, and that way, there is a hierarchy of rates. (Refer slide time: 06:50 - 08:25)



Then the fifth achievement of SONET was that it supports ITU hierarchy: E1, etc.  so this ITU hierarchy was more popular in Europe, India, etc. and they had rates like E1, E2, E3, etc. E1 was something like 2 mbps, and then E2 was 1mbps, and E3 was 34 mbps, whereas the development in USA was on a different track. They had a tone rate T1, T2, T3, etc. Their rates were as above: DS1, DS2, etc. What happened was that when SONET got introduced, these two sort of came together and although they are not perfectly identical ? these SONET and SDH ? for most of the part, they are identical; they interoperate with only very slight modification at the boundaries, which is not very important. That is a great thing and that means that the same standard is being adopted worldwide, so that any signal can be transported in any way. If there is an infrastructure, we can transport in anyway to another part of world; there is no problem. So bringing together of these, that means bridging the Atlantic Ocean of these two standards, that was another good achievement of SONET. (Refer slide time: 08:25 - 08:49)
 

The next is that it accommodates other applications. The other applications which were not a part of this kind of hierarchy, like BISDN, that is broadband ISDN, also can be accommodated in SONET and that way you see SONET was quite flexible; and how this flexibility is achieved we will see that later on. (Refer slide time: 08:49 - 09:32)

Finally it allows quick recovery from failure, talking about protection, etc. So if there is a failure like a line failure or if there is a terminal equipment failure you can deploy a SONET in a particular fashion and SONET can recover from this failure and this  retransmission, etc., can take place in a short period of time. That is very important when you want to give the so-called career great service, where arbitrary down time is absolutely not acceptable. As I said, we will discuss this separately in another lecture. (Refer slide time: 09:32 - 11:02)



Some of the broad features of SONET and SDH: it was first standardized by ANCI/ECSA, SDH by ITU?T. So SONET was by this ANCI/ECSA and SDH by this ITU?T. SONET is time division multiplexing, pure. We know what time division multiplexing is, and we will see later on how frames, etc. are made up. It is a pure time division multiplexing system. SONET encompasses optical and electrical specifications, so there are optical specifications as well as electrical specifications. You know that usually at the user end, quite often things start at the electrical level and the rates are low. But as you go more towards the backbone of the network, the rates that are needed at the backbone start becoming higher and higher and finally at the real backbone it has to be very high-speed network, and such high-speed networks are only possible through optical communication and optical networking. Once again, we will see about optical networking in the next couple of lectures. Our specification, the SONET specification, spans both the electrical side as well as the optical side, and that is a very good feature of SONET. (Refer slide time: 11:02 - 11:42)



SONET uses octet multiplexing, octet means the same thing as a byte that means 8 bits,  so sonet uses octet multiplexing. They are multiplexed byte by byte. SONET uses extremely precise timing, something like in 30 years, maybe; SONET has very precise timing and that is why things are synchronous. And if things become synchronous, then we derive a lot of advantages out of that. And SONET provides support for operation, maintenance, and administration (OAM) as we have already mentioned (Refer slide time: 11:42 - 14:24)
 

SONET is actually superior to T3 and T4, etc. with improvements over the T carriers; these T3, T4 are still in use but they feed into SONET nowadays. But earlier, they were used to feed into this PDH and these T3, T4 have particular rates which existed, and their specification left something to be desired. Because of this lack of synchronicity,  handling the signals from different sources is not easy. What could happen is that when things are not synchronous, but just almost synchronous, then to handle this ?almost? part, you have to do something; you have to incur some overhead; and you have to incur some complexity. That was the difficulty with PDH; in SDH or SONET, this is eliminated, and we get better transport performance. Then, we have the ability to identify sub streams. This was another advantage of SONET over PDH, which is that a particular user uses may be using a very small kind of bandwidth ? small in relative sense ? and then, as more and more users, as I said as all these data streams or communications streams come towards the backbone of the network, the pipes tend to get fatter. That means, we need faster and faster communication. So between say two points in the backbone, there may be a very fast communication going on and then after going to some other hops, this will again diverge. SONET has this ability that different streams can get together, travel for some time, and then again diverge. So the ability to identify sub streams is very important, and that is also allowed in SONET, which was more difficult in the RDR system. And of course international connectivity, as I said that it breached Atlantic and that was great. It enhanced control and administrative function, that was also very good from the point of view of service providers. (Refer slide time: 14:24 - 16:00) 


We have talked about this seven-layer OSI protocol; where does a SONET SDH really fit in? SONET SDH goes to the bottom of this. If you remember, starting from the application layer, we go  right up to the physical layer. There are several layers in OSI model, and there are other models. Anyway, usually the bottom-most layer is always the physical layer. So SONET really fits into the physical layer in some sense. So what would happen is that the layer just above the physical is the data link layer, may be, or layer two. So after all this encapsulation, etc. is over through all these six other layers including the data link layer, SONET takes it over for transporting it from one point to another. So SDH is placed at the bottom of the protocol stack in the physical layer along with the fiber. Any IP traffic even if it is the IP traffic of a packet oriented traffic ? and remember that SONET is a TDM system ? it can sort of travel within a sort of TDM transport as they quite often do. So any IP traffic that is destined to be transmitted across a fiber-based SDH network will be framed by a layer two protocol before being ready to take its orders from the SDH equipment. (Refer slide time: 16:00 - 17:59)


These are some of the multiplexing standards ? I have not given all of them I just indicate some of them. If you remember as I mentioned DS0 is a 64 kbps channel and 24 of them constitute a T1 line. So T1 rate is approximately about 1.5 mbps; 4 T1 gives T2 and 6 T2 gives T3 and so on. Similarly 30 DS0 ?this is a European system ? gives E1 line. So E1, if you remember, is about say 2 mbps: 4 E1 gives E2; E3 is a 34 mbps line. And then I suddenly jump right up to this thing called OC3;  this o is for optical. So this way, this 155 mbps is 3 of the basic STS 1 rates that I mentioned earlier; I will come to this later on. So these are some of the standards. There is a whole hierarchy of standards; for example, this name SDH is also synchronous digital hierarchy, this is a hierarchy. For the SONET, the basic rate is STS 1 that is synchronous transport signal level 1, and the speed is 51.84 mbps. This is designed to carry what was DS3 RDR or a combination of DS1 DS1 c, and DS2 etc. As I said a combination of different streams can flow through a SONET pipe or SONET infrastructure. So that is good and that means DS3 is a fat pipe or DS3 is almost the same as STS-1. So it is a fat pipe through which multiple pipes, say may be DS2 or DS1, etc. may travel. (Refer slide time: 18:03 - 18:25)

 
And this net goes up to STS-N, whereas synchronous transport signal level is N; so this has a speed of N ? 51.84 mbps designed to carry multiple STS -1. I mentioned that these are byte multiplexed STS-1 means 1 byte from one source and another byte from another source and so on. (Refer slide time: 18:25 - 19:49)

Fundamental SDH frame is STM -1;  SDH if you remember is the other standard, which came from Europe and they sort of came together and that is what we are talking about. SDH frame is STM -1 synchronous transport module and the SONET version is OC -3, that is, optical container, each providing 155 mbps. So when we come to this rate this 155 mbps OC 13, different rates etc. and different systems, are culminated here, at this 155 mbps, almost 155. STM 4 provides four times the STM -1 capacity, STM 16 provides a further fourfold increase, which means STM4 may be about 620 mbps, and then, if you go to STM16, which is four times that of about 2.5 giga bit/s, then you have STM 64, which is about 10giga bit/s. So all these rates are there; that means, from this point onwards, these two streams have converged and we are going to higher and higher rates in a sort of universal fashion, which makes things easy across the world. (Refer slide time: 19:50 - 20:17)

It is worth noting that the inter networking between SDH and SONET systems is possible at matched bit rates; for example STM4 and OC12; so they interoperate. A slight modification to the overhead is required as they are structured little differently so there will always be a little something; but anyway that is not very serious. So they do interoperate. (Refer slide time: 20:17 - 20:41)

We have seen the SONET electrical hierarchy; now we look at the SONET optical signal hierarchy: OC-1 is the optical career, level 1; it carries STS-1; OC 3 carries STS-3 or STM -1 at 155 mbps; OC-N optical career level N. (Refer slide time: 20:41 - 22:16)

OC - N as I mentioned is an optical carrier, which uses N ? 51.84 mbps, so  OC - 48 is about 2.4 gbps; overhead percentage is about 3.45%. OC signal is sent after scrambling to avoid a long string of 0s and 1s to enable clock recovery. This is a small technical point; that means in order to keep the whole thing synchronized,  the SDH  units use the transitions which happen when there is a 1. So the point is that if there is no 1 for a very long period in the data stream, then the clock on one side may drift relative to the clock on the other side; that is always possible. So we try to avoid long streams of 0s in this SONET or SDH, and we do that by scrambling the data from various streams, etc., or descrambling them. The idea is that even if one of them is sending a long stream of 0s, there will be quite a few 1s from the other streams and then the clock will be maintained. An STS -N is synchronous transport signal electronic equivalent of optical carriers. (Refer slide time: 22:16 - 23:29)

OC 3, OC12, OC 24 and OC 48 rates are common in telecom circuits ? if you remember OC 48 is 16 times of OC 3; that is, 16 times 155 mbps, which is about 10 gbps. Upto 10 gbps is very common these days. Actually right now, with DWDM systems, OC 192 rate is already in operation, and OC 768, which is 40 gbps, is being talked about. So that was another disadvantage earlier that this digital hierarchy of standard rates did not exist beyond a very small rate ? I mean small in today?s comparison. But now we have an extended and open system where, as technology improves, we can always go for higher and higher rates; so from OC 3, which is 155 mbps, we can go to maybe OC 192, which is 10 gbps or OC 768, which is 40 gbps that we are talking about now. (Refer slide time: 23:29 - 24:11)

How do you use these high-speed links? These high-speed links of course have to be on fiber ? we can look at details of fiber later on, but please note that in practical application, an SDH line system will have a multiplexer that takes its inputs from a variety of sources in different layer 2 data formats. So here we are talking about these different signals coming in the electronic domain, and they are coming from a variety of sources, may be coming with different layer 2 data formats. These are aggregated up to form frames at a line rate of system, for example up to STM 64 for a 10 gbps bit rate system. (Refer slide time: 24:11 - 25:08)

Now these frames at 10 gbps cannot be pumped anywhere. It is very difficult to pump it on a copper. So these frames are transmitted out onto optical fiber links. There is a possibility of multiple SDH multiplexers to each give out one wavelength of a WDM system. As we will see later on, this WDM stands for Wave Length Division multiplexing, which is some form of frequency division multiplexing. I mentioned about it when I talked about frequency division multiplexing. In fiber optics, we talk about wavelength multiplexing so it is possible that one multiplexer is feeding into one wavelength, another multiplexer is feeding into another wavelength, and all these different wavelengths are traveling together in the fiber. (Refer slide time: 25:08 - 25:44)

At the end of the system, there will be an SDH demultiplexer on the other end, just as we have a multiplexer on one side. Naturally, you have to have a demultiplexer on the other side that now accesses the individual data streams from the STM 64 frames as required. So STM 64 is carrying lots of frames in a very short time; they are sort of separated out and then fed into slower streams down the line. So there may also be an SDH add drop multiplexer with the ability to remove and insert lower bit rate streams from the signal. (Refer slide time: 25:44 - 26:33)

Alternatively a digital cross connect may be present with the ability to switch individual VC4s. Well, this is virtual container four, which is another concept, we will talk about later. So between different fiber links there is a digital cross connect; if you have the digital cross connect in the optical level, the advantage is that you need not go into the electronic domain at all. So the advantage of not going into electronic domain is that you are handling a huge, very fat, pipe; that means, a large number of channels, and you can just switch them from one fiber to another fiber simply in the optical domain without doing any kind of processing; and that is always an advantage. (Refer slide time: 26:33 - 27:50)

We will talk about some SONET terms now; for example, envelope. This envelope is the payload. Basically, after all encapsulation, etc., you remember that finally near the bottom we have this layer 2 and this layer 2 protocol will encapsulate it and then hand it over to SONET at the lower level, maybe at the physical level. So whatever this layer 2 hands over to SONET is the payload; the rest of it are kind of system overheads ? payload plus some end system overhead also goes into this payload. So these together form what is known as the envelope; this is a SONET term. Other bits and bytes which are used for management, that means OAM and P portion, goes as the overhead of SONET. Then there is the concept of concatenation; that means, unchannelized envelope can carry super rate data payload, for example, ATM, etc. So, the method of concatenation is different from that of T carrier hierarchy; we need not bother about it at the moment. (Refer slide time: 27:50 - 28:23)

Then there are some nonstandard functional names in SONET, like 
TM is for terminal multiplexer, also known as line terminating equipment or LTE. These are ends of point-to-point links. ADM is for add drop multiplexer; we have mentioned this. DCC is for digital cross connect wideband and broadband; MN is for matched nodes  and D + R means drop and repeat, etc. Anyway, these are just some terms. (Refer slide time: 28:23 - 28:40)

Now let us come to some important concepts in SONET namely: section, line, and path.  What is a section? I will just show you figure first and then come back to this. (Refer slide time: 28:40 - 30:24)

Please look at this figure: we have some multiplexers. So as the figure shows, we have a multiplexer in this side, another is an output that fits to another multiplexer. This multiplexer is going in this direction and after some time, the signal becomes weak. So we want a repeater; what is a repeater? A repeater is something which boosts the signal strength. So there is a repeater, then it travels some more distance then there is a repeater again and then it travels some more distance and then on other side we have the corresponding demultiplexer and then it fits into the other de-multiplexer. From repeater to repeater, we call it a section. So from repeater to multiplexer, this is also a section. So multiplexer to repeater, repeater to repeater, these are called sections. And then, from multiplexer to multiplexer, we call it a line. At the repeater, nothing happens excepting the signal is cleaned up. The signal may be boosted or there may be other cleaning operation, synchronizing operation, etc., that may be done at the repeater; but as such, the signals which are traveling here, the same set of signals are traveling here. At the multiplexer, of course, some of the signals may go off in another direction; some signals may go in some other direction, etc. So at the multiplexer, there may be a convergence or divergence, depending on which way the signal is flowing. That may happen at the multiplexer, so from multiplexer to multiplexer, we call it a line; and then from the end user point to end user point, we call it a path. (Refer slide time: 30:30 - 30:57) 

Look at this once; the portion from a multiplexer to a repeater is known as a section or it could be a repeater to a repeater also; the portion from a multiplexer to another multiplexer is a line. The portion from source to destination multiplexer is a path; below path line and section is the photonic sub layer; that means photonic sub layer is whatever is happening in the optical domain, and we are not discussing that at the moment. (Refer slide time: 30:57 - 31:14)

Sections are bounded by repeaters or multiplexers that terminate the line; lines may carry several tributary signals and are bounded by multiplexers, a path goes end to end between terminating multiplexers. (Refer slide time: 31:15 - 33:49)

Each STH frame lasts 125 microseconds. As I mentioned, this 125 microseconds time period, time epoch, is sort of sacred in this whole domain because 125 microseconds is what is required for a DS0 channel. Remember this is a time division multiplexing, which means that if you have a 125 microsecond kind of slot, then some of the DS0 bytes can  take these bytes. Actually if you have to take it as 8 kbps and if it is 8 kbps, inverse of that is 125 microsecond. So if you have a 125 microsecond slot, if 1 byte travels in this frame, then that is enough for 1 DS0 channel. In SONET we have very very sophisticated and very fast equipment; that means this is a time division multiplexing system; within this 125 microseconds, not only 1 byte can go but lot of other bytes can go. That means a lot of channels can travel together in this 125 microseconds frame. This is the idea. So each STH frame lasts 125 microseconds; how many bytes are going in there depends on whether it is STS -1 or STS -2 or STS ?N, etc. So 125 microsecond as I mentioned is 8000 frames/s. STS -1 frame has 6480 bits or 810 bytes. That means in one, 125  microsecond slot or frame, we are putting in 810 bytes. Theoretically, of course, that means it can carry 810 DS0 or voice signals; actually it is not 810, it is lesser than that because a number of these bytes are used for different types of overheads. We will talk about this. We have these 810 bytes, the octets are understood in terms of a table of 9 rows and 90 columns; so let us look at this figure. (Refer slide time: 33:49 - 36:16)

We have a SONET frame or an SDH frame, which has 9 rows, you can see the 9 rows on this side and then 90 columns, total 90 columns. Out of these 90 columns, 3 columns have been shown in yellow. These are sort of used for overhead and these 87 columns are  used for payload or for envelope. If you remember, the envelope contains the payload as well as little bit of overhead, which we will come to later on. This is how after every 90 bytes, we come back to again another 3 bytes of this overhead. This is how it is to be understood: the first 3 columns contain transport overhead and TOH has 9 rows by 3 columns, that means 27 bytes, which is subdivided into section overhead SOH (section overhead), 9 bytes, 3 rows of 3 columns; LOH, that is, line overhead, which is 18 bytes, that is, 6 rows of 3 columns. So we have section overhead and we have a line overhead ? remember we have these three concepts like section, line, and path. We have not talked about path overhead. There is some path overhead and it goes into the envelope; so there is some path and as far as these things as line and section are concerned, these are the overhead bytes. Just to clarify why do we require the over bytes ? the point is that the multiplexers or the repeaters have to have some communication between them in the control plain so as to give you this OAM  capability. For that some information needs to be sent or exchanged between the two points; anywhere there is a section, the section overhead would consider those things which are central to the section about the signal strength and other kind of things; line overhead maybe would contain something else and similarly path overhead would contain something else. But these are required for these OAM  capabilities that we have in SONET. (Refer slide time: 36:16 - 36:49)

Let us look at these overheads separately; first section overhead, which defines and identifies frames and monitors section errors and communication between section terminating equipment. So these are its functions: it identifies frames; monitors section errors ? if there are errors, it monitors section errors; and communication between section terminating equipment, maybe two repeaters or a repeater and multiplexer, and so on. (Refer slide time: 36:49 - 37:29)

Line overhead locates first octet of SPE and monitors line errors and communication between terminating equipment. We will come back to this locating of the first octet of SPE. This is a very interesting feature and we will talk about this separately. Previously we talking about section errors; so line errors and communication between terminating equipment, etc., is taken care of by the line overhead. Apart from that, line overhead contains this pointer, really, which points to the first byte of the SPE. (Refer slide time: 37:29 - 38:36)



And then there is a path overhead; and as I said path overhead is really inside the envelope and we will look at all these later. Path overhead verifies connection  path; you remember path means from end to end; that means from the end to end multiplexer is a path. Whether the connection has been established or not, it monitors path errors, receivers? status, communication between path termination equipment, and so on. This is the POH . We talked about the synchronous payload envelope or SPE that I was talking about. That is, the other 87 columns hold the SPE (synchronous payload envelope). So SPE has 9 waves by 87 columns, which are divided into path overhead and payload, which means the path overhead goes along with the envelope that is in the SPE,   whereas other overheads have separate bytes or separate columns associated with them as shown. (Refer slide time: 38:36 - 39:52)

Now this SPE does not necessarily start in the column 4, which means that the SPE does not necessarily stay within one frame; these are two very important points in SONET. The point is that although you have these 87 columns, actually the data may start getting transmitted at some arbitrary points inside those 87 columns. What is the idea? I mean why do you want to leave something and then only start from the middle? The point is that if there are some kind of mismatches of late, etc., if everything in the world were absolutely synchronous, all activities and all equipments, etc., then you could have started from the beginning. But that is not the case and this is where we absorb this kind of variation and this gives great flexibility to SONET, which was not there earlier. And the other interesting thing is that the SPE does not necessarily stay within one frame, which means that the SPE may start in one frame and then end in another. We will just look at a diagram of this; let us have a diagram of this. You see (Refer slide time: 40:00 - 40:21)

the SPE in light green color; it really starts from somewhere. I mean somewhere after leaving some of the rows: it starts here, and the path overhead is somewhere here, and there are two frames here. So SPE is really spanning both the frames. (Refer slide time: 40:25 - 40:57)

SPE is not frame aligned; it overlaps multiple frames; avoids buffer management complexity and artificial delays. Whenever there is something to send, you can just send it in the envelope; just put that pointer to that yellow edge, so that yellow edge will point to the first byte of the SPE. It allows direct access to byte synchronous lower level signals, for example, DS1, with just one frame recovery procedure. (Refer slide time: 40:5 7 - 41:33)

These are the advantages of the SONET frames. This is one frame coming in may be 125 microseconds; this is the next frame; and SPE, as I have already shown, can overlap. I mean it may start somewhere within the first frame and then continue in the second frame in this fashion and then be over here. Actually after this, some other envelope may come in over here. (Refer slide time: 41:35 - 42:34)

Now of course where is the path overhead? There are two fields, H1 and H2 in LOH; LOH means line overhead, which points to the beginning of the path overhead. Path overhead beginning floats within the frame; 9 bytes that is one column may span frame along with the SPE; it is originated and terminated by all path devices; and this gives you end-to-end support. These are the features of path overhead. The point is that if you remember the path is end to end, that means it is close to the end users; just as the end user may start somewhere arbitrarily in-between, a path overhead also goes along with the SPE and it starts over there and at LOH, we keep a pointer to this path overhead. (Refer slide time: 42:40 - 44:17)

Just as some of the equipment that we use in SONET, one of the most important of these is the add drop multiplexer. They are important because at certain point in the network, what might happen is that there are some sources which want to send into the network. They will sort of go so there is this SONET equipment, which is ADM let us say, and SONET stream is flowing let us say like this. There may be something that wants to upload and travel along with this thing. At the same time, this may be the destination location for some of the other signals which originated elsewhere; they have to be dropped here. So some signals have to be dropped, some signals have to be added. So this multiplexer can handle that and that is very important. That is why they are called add drop multiplexers. This stream is itself of course flowing at a tremendous rate, whatever that rate is. So SONET SDH is a synchronous system with the master clock accuracy of 1 in 109, which you will see is highly accurate. It shows when you come in some kind of   CCM clock somewhere and then there is a protocol for distributing and maintaining this clock over the entire network. Frames are sent byte by byte and ADMs can add drop smaller tributaries into the main SONET SDH stream and I have explained how that is done. Within that frame you can send lot of bytes; you can take out some of the bytes and add some of the bytes. That is how you take out some of the smaller tributaries and add some of the smaller tributaries. (Refer slide time: 44:22 - 44:58)

Digital cross connect, which is an optical layer equipment, is also very important. It cross connects thousands of streams and software control, so it replaces patch panel; that is a good thing about the digital cross connect and a software control is coming where the control is coming from the control plane of the switches. You can connect the streams from may be one fiber to another; it handles performance monitoring, PDH SONET streams, and also provides ADM functions; that means add drop multiplexing functions. (Refer slide time: 44:58 - 45:47)

Finally we have this concept of grooming in SONET. Grooming means, we group the traffic in some format. So you want to keep this group in one particular way; it could be that there is a one group of streams for whom you want to give higher priority or you want to give higher quality of service. So you have to group them together. Similarly there may be multiple groups; so it enables grouping traffic with similar destination, QoS, etc., which is a part of grooming. It enables multiplexing or extracting streams also ? that is also part of grooming. Narrow wider broadband and optical cross connects may be used for grooming. (Refer slide time: 45:47 - 46:40)

If you look at this figure, you have this narrow band, this SONET layer and optical layer. In the narrow band, we have this DS0 grooming and then in the DS1 grooming, there is a white band and then the broadband DS3 grooming ? so the rates are going up, starting from the 64 kbps, it is going up. When you are going up for the STS 48, you are in optical domain; that means STS 48 is STM 16, so that is a high rate. The point is that, at that rate, most probably, you are well in the optical domain. Then, finally, you can go to all optical domain; that means wavelength, waveband, and fiber grooming ? there are different levels of grooming, depending on what you want to do.  (Refer slide time: 46:42 - 48:00)

Lastly we will just talk a little bit about virtual tributaries or containers. We have already talked a little bit about it. This is the opposite of STM; actually in some sense this is called sub multiplexing; that is, different streams coming together to form one very fat or very fast stream. This is the other thing ? how do we, sort of, differentiate these sub streams within this, which has to do with sub multiplexing? STS -1 is divided into 7 virtual tributary groups, SDH uses the term virtual containers or VCs. We talked about VCs, we just mentioned what are called VTs or virtual tributaries in SONET lingo. So we have 7 virtual tributaries, 12 columns each, which can be subdivided further. You see that there are 12 columns each, with 7 virtual tributary groups ? we have got 84 columns and these 84 columns are out of the 87 you have in STS -1. (Refer slide time: 48:01 - 50:00)

VT groups are byte interleaved to create a basic SONET SPE. So this VT groups are byte interleaved. They may be again extracted from each other. VT 1.5 is the most popular, quickly accessed, T1 line within the STS-1 frame. So the idea is that you have a T1 line, which is approximately 1.5 mbps line, which is coming out of your small business, and you have a 1.5 mbps line. So that is your bandwidth requirement, you want to connect it to a distant location somewhere. And you do not want your thing to get mixed up with others. At the same time, as a small business you cannot have infrastructure of connecting to another location which is wide apart. So you will go with this public infrastructure or public switched tele PSDN network or whoever is maintaining this communication equipment. Usually telecom people maintain it in most of the places. Anyway, they have a sort of fiber going from one place to another, which contains very high-speed links. What you want is your T1 line should join them, sort of get transported over the distance and then go and feed into another T1 line at the destination. That is what you want. You want your T1 line to sort of have a separate sort of existence ? just like in a compartment, we have different passengers. Passengers have their own individual entity but together they are packed into one compartment and then they travel. Similarly your T1 line is going to ride onto to this very fast stream and travel to the destination. So VT 1.5 gives your T1 line. (Refer slide time: 50:00 - 51:16)

How do you find out about the difference? How do you separate them in the SP? The point is, you require one more level of pointer used to access it. You can access a T1 with just a 2-pointer operation, first from the LOH ? you remember, you go to the SP, just like that. Similarly, you go to the different tributaries or different containers using just one more level of pointer. This flexibility was not there earlier; so it was very complex to do the same function in DS3. For example, accessing DS0 within DS3 requires full demultiplexing, stacked multiplexing, etc. So you require full demultiplexing; that is not required in  SONET. The point is that the other streams may go; you know where in that frame your bytes really are traveling for the stream or for the container or for the tributary that you are interested; you just extract it, others keep on traveling as they are. So you do not demultiplex the whole thing and that gives a great advantage of add drop multiplexing. (Refer slide time: 51:16 - 52:02)

This is just a figure showing that you can have various types of lines, all feeding into the same infrastructure. You may have what we have put over here: DS1, which is 1.544 mbps, E1, 2.048 mbps, DSIC DS2, DS3, ATM .48.384, E4, which is 139.264 mbps, ATM is about 150 mbps, etc. They are sort of traveling; they are getting in different containers. From VT 1.5, different tributaries, that is 1. 5236 etc., form a VT group and  ride on a higher strength or higher speed stream. (Refer slide time: 52:02 - 53:55)

Just as I said, these are sort of identified through a pointer; so we have this transport overhead. We use some bytes for that out of those 87 columns we have. So we use some columns of that and then we put a pointer, which gives to the STS payload pointer. Then there is a VT pointer, virtual tributary pointer, and this much is the VT SPE within the overall STS-1 SPE, which is the payload. Even now SONET is the most widely used technology in wide area networking that is existing today. Of course, as you know, as technology grows, may be we will go out of SONET. People are already talking about going out of SONET because one disadvantage of SONET is that its equipment tends to be expensive. Well, expensive compared to what we think today. What is cheap today and what we think is cheap today may sound very expensive tomorrow; that is how the technology grows. So people are talking about direct transport over the optical layer, etc. May be we will touch those aspects later on. But all that is still in a sort of experimental stage and on the field, actually, SDH or SONET equipment is almost everywhere; all types of telephone companies are connected through that and major service providers use this as a means of transport. Thank you. (Refer slide time: 53:58 - 54:11)


Good day,so today we will be speaking about  fiber optic  components (Refer slide time: 54:11 - 55:22)

and fiber optic communication  as might of heard this lecture as well as the next couple of lectures, we will concentrate on fiber optic components. We have looked at some of the physical  layer components of  fiber optic systems before so we will sort of quickly review that, some of  the stuff we will be talking about today is going to be common and then from that point we will take out take it up into WDM systems, how wavelength division  multiplexing is done and how systems are  handled in fiber optic domain, this fiber optic domain happens to be very crucial because   a lot of  traffic in terms of volume   may be  as much as forty to fifty percent, actually  goes through the fiber  as days are going by and as more and more demand for bandwidth  is coming up fiber optics is becoming more and more important , we will be talking about fiber optic components today.   (Refer slide time: 55:22 - 55:56) 

In fiber optic component of course the basic fiber is there we have already talked about it, so we will talk little bit more about this then we have light source and receivers on two end  because we know that in fiber optic  cables light is the carrier of information then we require these different components like amplifiers, couplers, modulator, multiplexer and switches so we will look up at these components one by one and then we will start our discussion on wavelength division multiplexing.  (Refer slide time: 55:56 - 56:42)

The next set of components are multiplexers filters gratings, just talk little bit about it ,if you look at this wavelength , these are all; wavelength selective , devices multiplexers, filters , these are wavelength selective devices in a wavelength filter and what we want is suppose ?1, ?2 etc so many are coming, I want only ? 1 out   2 ?3 ? 4 etc are absorbed or something where as if you are a multiplexer I want the difference  this ? coming in different lines, I want all to be mixed together and use the same  line, these are wavelength multiplexer  (Refer slide time: 56:42 - 58:07)

so application could be particular wavelength or a particular wave band selection. Wave band is nothing but some contiguous  operating wavelengths which all are side by side,  if you remember  that in the operating window what ever be that  1550  what ever may be the window you are using there you can have a number of ? all side by side, there is a guard band between each of these operating ? so where the guard band that is given by the i q t has specified, how much guard band etc you have to have but so you can have large number of ? all group together in the same window. Aband out of that means a bunch of sequence is out of that you can short select instead of selecting only , that is wavelength band selection static wavelength cross connects and OAM is optical add drop multiplexers, you have come across this term optical add drop multiplexers in the context of  Sonet but optical domain we require optical add drop multiplexers, we will come to that. Equalization of gain so that is another application filtering of noise ideas used in laser operation and dispersion compensation modules etc, these are the different applications (Refer slide time: 58:08  -  58:15)

one of the standard wavelength selective component is arrayed waveguide gratings ,we have seen this before? 

COMPUTER NETWORKS
Prof: Sujoy Ghosh
Department of Computer Science and Engineering
IIT, Kharagpur
                  Lecture#10
(Refer Start Time 00:43)
Good day. so tToday, we will be speaking about [noise] fiber optic [noise] components (Refer slide time: 00:53 ? 00:56 min)

and fiber optic communication. as might of heard tThis lecture as well as the next couple of lectures will concentrate on fiber optic components. We have looked at some of the physical [noise]layer components of [noise]fiber optic systems before, so we will quickly review that. that so sSome of [noise]the stuff we will be talking about today is going to be common and then, from that point, we will take out take up WDM systems, details of how wavelength division multiplexing is done and how systems are [noise]handled in fiber optic domain. tThe fiber optic domain happens to be very crucial because [noise]a lot of [noise]traffic in terms of volume, okmay be as much as forty to fifty percent 40?50%, actually follows [noise] goes through the fiber. As days are going by and as more and more demand for bandwidth [noise]is coming up, fiber optics is becoming more and more important. [noisesSo we will be talking about fiber optic components today. [noise] (Refer slide time: 02:05 - 02:39 min)

sSo in fiber optic component, of course, the basic fiber is there; we have already talked about it, so we will talk little bit more about this. Then we have light sources and receivers on twotwo ends because we know that in fiber optic [noise] cables, light is the carrier of information. Then we require these different components like amplifiers, couplers, modulators, multiplexers and switches. sSo we will look at these components one by one. and tThen we will start our discussion on wavelength division multiplexing. [noise](Refer slide time: 02:40 ? 03:34 min)

sSo optical fiber as was mentioned earlier is very pure and very transparent silica glass is used. so aAt a moderate dimension, the light is restricted to the fiber because of total internal reflection for ordinary light; this is a multimode fiber. ok mmf MMF is used in lans LANs for low speeds or short distances. so mmf MMF may be used in lans LANs; that is another kind of fiber. bBut the multimode fiber happens to be the cheaper variety and this is used in lan LANs, but this is good only for low speed. wWhen iI say low speed iI mean comparatively low speed, ok for this say hundred mbps 100 mbps or hundred and fifty five mbps 155 mbps may be a low speed and by short distances may be couple of kilometers at [noise] the maximum. (Refer slide time: 03:34 - 03:51 min)

At a still smaller dimension, that part of the fiber that actually carries the signal ? if you remember our discussion about, the fibers in physical layer, we have shown some diagrams regarding the cross section of  a fiber, so at the very core ? is a very thin strand of glass fiber. tThis is surrounded by a cladding which is also made of glass; actually we will not be able to see the actual part of the fiber, which is carrying the signal. tThe surrounding is also made of glass and that is again coated or covered etc., by outer protection. So if that strand happens to be even smaller, say 8 to 10 nanometer range, tthen it acts like a wave guide and a single mode of operations. (Refer slide time: 04:39 - 05:10 min)

wWe will not go in details of eElectro mMagnetics and wave guide propagation of optics to this fiber, but any ways, the support of the single mode of propagation is called single Mode Fiber. SMF is used for higher speed, so all these speeds of 2.5 GB/sec or 10 GB/sec etc., are possible on Single Mode Fiber and it also goes over longer distance ok and nowadays, we have fibers which span across oceans. We have fiber from one continent to another, which is really a marvel of engineering and technology. (Refer slide time: 05:31 - 
05:50 min)

So these fibers are actually single mode fibers. There are a few transmission windows like 1310 and 1550 nm bandwidth etc. fifteen fifty nanometer 1550 nm window is preferred for long haul applications because it has less attenuation, wider window, and we can get very easily get good optical amplifiers in this range. (Refer slide time: 05:50 - 
06:30 min) 

so t,This is the diagram you have seen before, so you will see that at around fifteen fifty nanometer1550 nm range, we have some the [noise] kind of low attenuation. [noise] sSimilarly,thirteen ten 1310 nm  [noise] nanometeris another window, but this window is quite wide. Although it may not look very wide in this diagram, in actual practice, this is quite wide. So this is the wider window and we get good optical amplifiers. We will come back to this point when we discuss optical amplifiers. (Refer slide time: 06:38 - 07:37 min) 

Why do we require optical amplifiers, because there are losses in fiber,losses in fiber may be due to various reasons: One, of course, the main thing is absorption. That is what we showed in the previous diagram; it loses energy to the atom and absorbs some of the photons. So we get lower magnitude or lower strength of the light signal, as we go to longer and longer distances. Then there is scattering of photons by the medium. So there is Rayleigh scattering, due to slight changes in the refractive index of glass; then there is Mie scattering due to imperfection of the cylindrical structure, it?s made to rigorous specification, but it?s never exact in the engineering world ? it can never be exact. So we get all these different types of scattering, absorption, etc., leading to losses in fiber. (Refer slide time: 07:39 - 07:53 min)

Apart from these just lowering of signal strength, one problem is that the losses are in nonuniform for different wavelengths. ok that means that tThe [noise]loss may be more for one particular frequency and may be less for another particular another frequency. The trouble is, when you take a waveform, specially a digital waveform, which is a square waveform and if you analyse it, you will get a lot of spectral component. iIf you do a fuller transformation on that, you will get the different harmonics or components of that particular wave shape. And the and these for that for the perfect square shape to come up, these different harmonics have to be at specific strengths compared to each other. dDue to differential losses at different frequencies, what would happen is that their balance would get disturbed. What you will actually see is that your pulse shape has changed. (Refer slide time: 08:54 ? 09:23 min)

This is known as chromatic dispersion ? chromatic because it depends on the wavelength or frequency, or the color of the light that is why it?s called chromatic. Different spectral components of pulse travel at different velocities. [noise] tThis is another problem and there is something called group velocity dispersion. so wWe get some kind of velocity for all these different components and this leads to some kind of dispersion. [noise](Refer slide time: 09:24 ? 
09:46 min)

So this is an example of chromatic pulse ? we need not go into the details of this, but this is the input pulse whereas in the output pulse, it has a much more flattened shape because these different components have been attenuated differently as they went along the fiber. For compensating these dispersions, nowadays some special types of fibers have come up; we cannot go into details of these ? one is the reduced dispersion fiber. By dispersion shifted fibers we mean the natural dispersion is that is sort of acted on nonzero dispersion shifted fibers. Anyway, the point is that normal SMF is there in most of the places, say, more than ninety five percent95% of the deployed plant. Dispersion is measured in pico second per nanometer kilometer; so dispersion is much lower. For some of the interesting areas or the window, we get an almost zero dispersion for these fibers. So we can get very special fiber these days, which may be utilized for very long haul applications, where dispersion becomes a problem. By the way, for short haul  applications, when you are travel through a few kilometers, then the dispersion is not much of a problem and we need not bother about dispersion shifted characteristics of the fiber because in ordinary fiber, whatever the dispersion it gives in that small distance, will not matter so much. (Refer slide time: 11:20 - 11:25 min)

We have a bandwidth span product; that means, how much bandwidth for what kind of distance. This is very common in almost all kinds of transmission lines. ok [noise] sSo a transmission line, which operates at a particular speed quite well for some distance, will not operate that well if you either increase the rate at which you are pumping in the data (increase the frequency) or if you keep the data rate constant and increase the length. That would also not work very well. sSo for any kind of medium, we get the bandwidth distance product, which tends to be more or less sort of constant. so aA fiber which is good for at a particular speed for lets 2 kilometers may be just good enough for 1 km when you double the data rate. (Refer slide time: 12:25 -  13:26 min)

For older kind of SMF at 1310 nm, we get high speed ? mean these are some typical figures, they are not exact figures; these are some typical figures to get some ideas. sSo we can operate at two point five giga 
2.5 gbits/sec for six forty kilometers640 kms without amplification, or 10 gb/sec for, let?s say, 100 kms. A decent SMF can take up to two point five giga bits per second 2.5 gb/sec for four thousand four hundred kilometers 4400 kms,ok which spans from one continent to another or ten giga bits per second 10 gb/sec for five hundred kilometers 500 kms. Of course, you should multiply these figures for dwdm DWDM. We will talk about WDM presently, where it gives you a large number of channels, may be forty 40 channels. So you can see that you can realize really tremendous data rate, very very high data rate, using this fiber optic communication and that is the major advantage of fiber. tThere are, of course, other advantages like have i mean you are not susceptible to electromagnetic radiations; that is one good thing because nowadays with so many gadgets all around and so many things moving around, we get all kinds of very noisy electromagnetic ambience; but [noise] i mean fiber is immune to all that. that s That is the good point about fiber. (Refer slide ime: 13:59 - 4:46 min)

Next, from fiber, we come to light source that is the LED. Light source is of course of two types, depending on whether we are using multi mode fiber or single mode fiber. Usually, we would use ordinary light in a multi mode fiber and the source of the ordinary light would be LED. One good thing about the led LED is of course that it is very cheap. sSo the good thing about multi mode fiber is that it is cheap; the only thing is that it will not scale up very well with that bandwidth distance product. An led LED is just a forward biased 
pn-junction; what happens is that recombination of injected minority carriers by spontaneous emission produces light and it is a broad spectrum up to gain bandwidth of the medium; so that is an LED. (Refer slide time: 14:47 - 
15:42 min)

It is usually of a low power. ok power You remember that for  power you have to divide it by time. Although you might get a continuous source of light, it will be comparatively low power like twenty d b m 20 dbm, of low internal modulation. So you can modulate it if you internally modulate it. you can modulate it at best of with hundreds of mega bits/sec, per second which of course, for some applications, may be more than enough speed. But then again,, when you are talking about the core of wide area network, then it may not it may be it will is a very low speed. So obviously in the core of a wide area network, we will not be using multimode fibers or LEDs. LED slicing is LED plus a filter. iIt gives some power loss; we need not bother about that at the moment. (Refer slide time: 15:43 - 15:50 min)

The other kind of light source, which is very important, is the laser; ok of course, we use semiconductor lasers here almost always. It gives a high i mean much higher power output. So in a short duration of time we get quite intense pulses of light. That is very good; it has a high power output; it has got a sharp spectrum; that is it is coherent; that is, it is not at the wide range of spectrum. (Refer slide time: 16:12 - 16:42 min)

So it is sharp spectrum, that is the property of a laser that reduces the chromatic dispersion. It can be modulated either internally or externally, so that is also a good point. It is good for longer distances and larger bit rates compared to mlm MNF. (Refer Slide Time: 16:43 - 17:45 min)

mlm We need not bother about MLM. We have some special kind of devices, the tunable lasers. Tunable lasers means a laser, but we can change the color of light over a certain range. And as we will see later, the tunable laser may be quite important  in some cases. We will see what kind of time we require for tuning; it?s fairly rapid in the sense that it is in less than milliseconds? range. In the order of milliseconds, we can change the frequency emitted by the lasing system. It has a wide and continuous range of over hundred nanometer 100 nm; has a long lifetime and is stable over lifetime; and it is easily controllable and manufacturable. These are the good points about the tunable lasers. (Refer slide time: 17:46 - 18:21 min)

The methods could be eElectro optical; changing the refractive index by injecting the current or applying an electromagnetic field. sSo that is one way; it could be temperature tuning although it is not much preferred. First of all its range is narrow and then it may degrade the lifetime of a laser if you want to do it through temperature; or mechanical tuning using mems MEMS. This is compact but one problem of this tunable laser is that it is costly; it is quite costly and that is why this is not very common; also, it is slightly more complex to manage. But in some instances, it gives some advantages. We will mention this point later on when we discuss wdm WDM. (Refer slide time: 18:43 - 19:20 min)


then wWe come to receivers ?are of course the light pulse coming will have to be detected by something: whether a pulse has come or whether the pulse has not come, whether it?s one 1 or a zero 0. sSo if an photon comes, it will sort of push up an electron to a conduction band. sSo that is the standard photo detector, which you must have studied at school. So if it is sort of higher than this gap, this energy gap, then we get the electron in the conduction band, which will naturally show up by conducting; so that is the easiest thing. (Refer slide time: 19:21 - 21:03 min)

I was mentioning that nowadays we have fibers running for thousands of kilometers. sSo if we have such long haul fibers, what would happen is that naturally, after some distance, what you have to do is that your signal will become weak, so you have to amplify this. Previously this distance would be something of the order of four five kilometers 4?5kms. ok nNowadays twenty five kilometers 5 kms is very common, you can go hundreds, or in special cases, you can go even up to many hundreds of kilometers; you can go without amplification. But whatever it is, after some distance if, because of this loss, absorption, dispersion, etc., you will have to amplify the signal. For optical signals traveling long distance through fiber, it needs to be strengthened. This may be done through olt OLT, which was the older technology. In OLT, what was done was that, the optical signal was converted back to the electronic domain. And then you amplify the signal and then push it back to the optical domain. Obviously this has quite a number of disadvantages, main disadvantage being cost and the speed; that means cost is higher and speed is lower that has some advantages also. I will mention it later; this may be done in optical domain. The idea was the that, it could be done in optical domain through erbium doped fiber amplifiers. There are other kinds of dopers that I will mention; we will see that. 
(Refer slide time: 21:04 - 21:27 min)

So this is the scenario ? we have this light propagating and as it propagates with distance, the signal level comes down. When it comes down to minimum operating level, at that point, we will have an amplifier that will amplify the signal back; and then again, after sometime, it will sort of decay and then we again amplify. So these are some kind of repeaters. (Refer slide time: 21:28 - 23:33 min)

We come to one point where simple amplification is not always enough; sometimes we require regeneration. These are the so-called 3Rs: reamplify, reshape and retime. As far as absorption is concerned, if you simply amplify, that means increase the size, the strength of the signal is good enough. So the absorption loss or the loss of strength can be handled that way. But when you talk about very long distances, due to chromatic dispersion, etc., wave shape will become distorted as we have seen. In very long distance, on the other side there will be tremendous amount of errors that may not be acceptable. So we have to get a wave into shape. Now there are some special kinds of fibers; try to do it in optical domain. But more commonly they are deployed today to bring them to electronic domain and then give them the right square shape once again. So that is the second R; and the third R is the timing; that means how do you keep all the clocks synchronized. Because, after all, some TDM signals, etc., are traveling; so you will have to have a very strict control over the time. So these reamplify, reshape, and retime, are the 3 R kind of regeneration. Sometimes we only do with two r 2 Rs or simply one r 1 R, which is simply reamplify, which can be done simply with an EDFA, that is, the erbium doped fiber amplifiers; and at certain distances, we simply reamplify this. (Refer slide time: 23:34 - 24:32 min)

If you want to do some reshaping, suppose this is the input shape, which has come. This goes through the OE transformation, the optical domain, and we come to the electronic domain. We amplify this; shape this properly; and then again push it back from the electronic domain to the optical domain. So we again get a nicely shaped pulse and the output, which has been strengthened as well as reshaped. The problem of this is that, naturally, we can do the reshaping as well as amplification at the same time. But one problem that we face in this case is that the cost is high; and the other problem is that optics inherently can operate at a very high speed. But we can get fairly high-speed electronics also. But it becomes more and more difficult in the electronic domain as the speed becomes higher and higher. And when that happens we would like to do it in optical domain if we could and there are a number of schemes today for handling this in the optical domain. For example, we can have some very specially shaped optical waves, whose shape will not change over a distance. They are called solitons; they are sort of done in a way that different spectral components are mixed in such a manner that after dispersion, they cancel out each other. Similarly there are fibers which give the dispersion in the opposite direction to the standard fiber so that it can be brought back to shape. So these are all the atoms to handle this reshaping in the optical domain, but the most widely deployed system as of today is to take it to the electronic domain and do it there. (Refer slide time: 25:51 - 26:57 min)

Regenerators are specific bit rate, so that is another problem with the electronic domain. It is an opaque unit; that means. it has specific bit rate and modulation format that is used; whereas optical amplifiers, in optical domain what ever is coming is being amplified. So it is transparent to the bit rate, the modulation format, the protocol, etc.; it is transparent to all that. Whatever is coming, is simply amplifying. so that is the good thing about optical amplifiers. The system with optical amplifiers can be more easily upgraded to higher bit rate, without replacing the amplifiers. If you are going to use the same infrastructure for higher bit rate as service providers often want to do, in that case, if you are taking it to the electronic domain, you have to replace. Optical amplifiers have large gain bandwidth; they are also key enablers of dense wavelength division multiplexing. (Refer slide time: 26:58 - 27:31 min)

One of the standard kinds of fiber amplifiers, which is very widely deployed, is the erbium doped fiber amplification ? we have talked about this earlier. Erbium has a large number of excited states and from some of the excited states, it gives out this 1550 nm light, exactly the wavelength used in the third window. A few meters of optical fiber doped with a few parts per million of erbium is pumped with 1480 or 980 nm laser to give amplification. (Refer slide time: 27:32 - 27:46 min)

So EDFAs amplify all lambdas in the 1550 window simultaneously. So key performance parameters include saturation output power, noise figure, gain flatness, pass band, etc. (Refer slide time: 27:47 - 28:23 min)

We have input signal coming through regular fiber and this is the part which is erbium doped and you pump some laser. So these are combined using some kind of couplers; I will be talking about couplers later on. Because of the pumping laser, the erbium ions become excited and when the incoming signal hits these, they fall back to the ground state may be emitting more photons, so the signal is amplified. (Refer slide time: 28:24 - 29:11 min)

There is another kind of amplifier called Raman amplifier, which uses longer lengths of fiber. This has other advantages, basically using Raman scattering, so we are not going to the details of this once again. It has pumping through some kind of pump and signal. This pump may be in the same direction as the signal is going or it may be counter pumped in the other direction; the whole point is that, this pump keeps the atoms excited and, due to Raman scattering, so more and more photons come out, so we get an amplified signal finally. (Refer slide time: 29:12 - 29:45 min)

We have other kinds of dopants also: erbium doped for 1550 nm range, praseodymium doped fluoride fiber (PDFFA) for 1310 nm, thorium doped for 1350?1450 nm, thulium doped ? well, this is somewhat more academic, because thulium is considered as a rare material; it is not easily available. And even if it is available, it is quite costly. Anyway this is in the 1450?1530 nm range, (Refer slide time: 29:46 - 30:03 min)

tellerium erbium doped fibers in 1532?1608 nm range. Raman amplifiers address an extended spectrum using standard single mode fiber. That is the good thing about Raman amplifier. (Refer slide time: 30:03 - 30:09 min)

So EDFAs are popular in C-band, Raman are proposed for S-band, and gain shifted EDFA for L-band etc. (Refer slide time: 30:10 - 30:32 min)

We can have a look at this ? depending on the wavelengths, different kinds of doped and different kinds of fiber amplifiers become is more relevant, like EDFA for this range from 1550 etc.,  TDFA, PDFA, Raman amplification for this entire range, etc. Now we talk about some more components: the first component we talk about is 
(Refer slide time: 30:45 - 31:07 min)

some sort of passive device called a coupler. Optical coupler combines and splits signal wavelengths independent or selective; that means, the coupler can be wavelength independent as well as wavelength selective; fabricated using waveguides in integrated optics. Light couples from one waveguide to a closely placed waveguide because the propagation mode overlaps the two waveguides. (Refer slide time: 31:08 - 31:40 min)

So this is the picture: we have an input waveguide coming in and another input may come; and there are 2 outputs. So what might happen is that it may be used as a coupler; that means, 2 signals joining together, like we wanted to do when we tried to put in a pump in the EDFA amplifier. Or it may be used for splitting the signal coming from one and it is getting split into two directions. So this can be used in various ways. (Refer slide time: 31:41 - 32:3 min)

So if ? is the coupling ratio, power output 1 is ? times power of input 1. Whereas power output 2 is 1?? times power of input 1. So you see that together the input power is split into two parts if you want to have ? = ?. 
These are so-called 3-dB couplers. We put half the power in input 1 and half the power in output 2. If you want to broadcast the same signal to two different destinations, you can use a 3dB coupler. As I said, light couples from one waveguide to a closely placed waveguide, (Refer slide time: 32:31 - 32:38 min)

iIdentical waveguides complete coupling and back periodically. sSo this is the couple mode theory. (Refer slide time: 32:39 - 33:06 min)

Of course we have to follow the conservation of energy constraint so you cannot get more ? since this is a more passive device, we cannot get more out of it. Then when you put in actually you get less, so it is possible that electric fields at two outputs have the same magnitude. So they are exactly the same, but will be 90? out of phase and lossless combining is not possible, so nothing is really  100% efficient.
(Refer slide time: 33:07 - 33:50 min)

Passive star is a sort of generalization of this. It?s a broadcast device to more than one recipient. It divides the received signal to all output ports at original wavelength; of course, if you divide the same signal into so many different signals, the received strength of the signal will be proportionally less. You have to handle it by either amplification or some other thing, or maybe, weak signal is good enough for your application etc. So N ? N passive stars can route N simultaneous connections through. (Refer slide time: 33:51 - 34:12 min)

This is an example of an eight-port splitter. Whatever signal is coming, it is getting divided into two and again divided into two. So we get eight signals over here; we can actually use some 3-dB couplers to form this eight port splitter from 
Y-couplers. (Refer slide time: 34:13 - 34:35 min)

This is another example of an 8 ? 8 star coupler. There are 8 lines; any of these might communicate something which will be broadcast to all the other seven ports. So such things are used for broadcasting. (Refer slide time: 34:36 - 35:26 min)

We now come to optical modulation; that means how we modulate it. Of course the simple modulation scheme ? since we are talking only about digital systems ? is on/off keying, that means either on or off or 1 or 0. There are two types of modulation techniques, namely, direct modulation versus external modulation. In modulation, the extinction ratio of output power for bit = 1 to output power for bit = 0 is very important. We want this to be as high as possible. Some lasers cannot be directly modulated; that is one problem. Another problem with direct modulation is that you are modulating at the source, modulating at the same place, from where that light is being generated. Whatever diode or whatever you are using for generating it, we want to modulate through that only. So that is the direct modulation, whereas in indirect modulation, what we will do is that there is a continuous source of light and just as the light comes out, we will modulate it. We will put it on or off by making it go through something. So that is the external modulation. (Refer slide time: 36:00 - 36:22 min)

So the solution is ? naturally since direct modulation has the problem about the chirp, etc. ? external modulation for higher speeds, longer distance dispersion, limited regimes, etc. We prefer external modulation. The light source is continuously operated. External modulation turns light signal on or off; so this is the optical modulation. (Refer slide time: 36:23 - 36:49 min)

They can be integrated in the same package as laser; the laser source is there and the modulator is external, but they can be packaged. Electro absorption or EA modulator is one important kind of modulator; it applies an electric field, shrinks the band gap and photons are absorbed. (Refer slide time: 36:50 - 
37:03 min) 

So this is the picture, this is the continuous source of this light, which is being modulated, that is, being put on or off depending on whether you want to transmit 1 or 0. (Refer slide time:37:04 - 37:52 min)

The next set of components are multiplexers, filters, and gratings. We will talk a little bit about it. If you look at this, these are all wavelength selective devices: multiplexers, filters, and gratings. In a wavelength filter suppose ?1, ?2, etc. So many ?s are coming, but I want only ?1 out. ?2, ?3, ?4, etc., are absorbed; whereas if you are a multiplexer, I want the different ?s coming in different lines. I want all to be mixed together and use the same line. That is a wavelength multiplexer. (Refer slide time: 37:53 - 39:20 min)

So the application could be particular wavelength or a particular wave band selection. Wave bands are nothing but some contiguous, operating wavelengths, which all are side by side. In the operating window, whatever be the window you are using ? 1550 or whatever ? you can have a number of ?s, all side by side. There is a guard band between each of these operating ?s ?  the IQT has specified how much guard band etc., you will have to have. You can have large number of ?s, all grouped together in the same window. Now you can select a band out of that, a bunch of frequencies out of that, instead of selecting only one; that is wavelength band selection. Static wavelength cross connects and oadms OADMs (optical add drop multiplexers): you have come across this term, optical add drop multiplexers in the context of SONET, but in the optical domain we require optical add drop multiplexers; we will come to that. Equalization of gain: that is another application; filtering of noise; ideas used in laser operation and dispersion compensation modules etc. These are the different applications. (Refer slide time: 39:21 - 40:20 min)

One of the standard wavelength selective components is arrayed waveguide gratings. These are curved selection of silica acting as waveguides. Each waveguide is slightly different in length. The incoming signal is split. To be slightly different in length is like a running track bend ? in a running track you know that the outer track, in any athletic event, in an 800 m race or something, the outer track is longer than the inner track. That is why athletes are given proper handicap, because we want to make all the distances same. Here deliberately, we want the distances to be different. If they are different, they are going to sort of arrive out of phase at the output. The incoming signal is split; every wavelength then travels down each waveguide. (Refer slide time: 40:21 - 40:34 min)

Time delayed signals recombine to give each wavelength its own waveguide; can be reversed to act as a multiplexer rather than a demultiplexer; usable in optical integrated circuits; easily combined with other functions. (Refer slide time: 40:35 - 41:11 min)

This is the picture we had seen earlier: if your light is going in this direction, what you are doing is that you are demultiplexing. One bunch of frequencies are coming; we want all these different colors to get separated out. So that is the demultiplexing action going on. If different wavelengths or different colors of light are coming in the other direction, it?s just the direct opposite thing, and we will get a multiplexer, so it is an AWB acting either as a multiplexer or demultiplexer, depending on how you operate. (Refer slide time: 41:12 - 43:07 min)s

We come to optical switches; we have seen electrical switches. Now we will talk little bit about optical switches. What is optical switching? You remember electrical switching? In electrical switching, there are some one input lines coming in line i and I want to get that signal out through line j. So we want to operate the switch. Through the switch or something, want to connect the ith line and jth line. The basic idea was that the signal coming down from ith line has to go out of the jth output line. That was the simple switching element. It is the same thing here also; some wavelength is coming through some fiber, that is, one input port. We want to get it out of another fiber. For the time being, we do not have any wavelength conversion kind of thing, thus the same wavelength has to push it to another fiber. So that is my switching at the optical plane redirecting light from one optical fiber to another without electrical conversion. So we are always harping on that without electrical conversion, we can operate it at a much higher way and higher speed. Secondly it may be cheaper also, to operate at high speeds; it will be cheaper and then if you upgrade eventually, this is going to be transparent. This does not depend on the underlying protocol, etc., that is being used at the higher layer. These are the other advantages of doing at the optical plane. Now, the most advanced optical switching technology is MEMS, that is, tiny movable mirrors. (Refer slide time: 43:08 - 
43:13 min)

So this is the crossbar switch 4 ? 4 switch. (Refer slide time: 43:14 - 44:09 min)

We have seen this picture also. That is a MEMS optical cross connect. You see that these are all tiny mirrors; what I can do is that, to follow the red line which is coming, we are using these two mirrors to push it to line number 3 over here ? from line number 1 to line number 3. This way, by just adjusting the angles of the mirrors which we can do through the MEMS technology, we can have a simple and elegant kind of switch. The light goes and bumps off a couple of mirrors and goes out the other fiber. Whatever signals it is carrying, what protocol it is carrying is immaterial; similarly, the data rate also is immaterial. This is a MEMS optical cross connect. Now all this technology sort of enables what is known as the WDM technology, which is a wavelength division multiplexing. As I was mentioning when we were discussing multiplexing, wavelength division multiplexing is nothing but frequency division multiplexing; that means, you want different channels to come at different frequencies. It?s just the opposite of optical domain. In wavelength division multiplexing, different wavelengths and different ?s are getting together and light with different wavelengths can very well mix together and go to the other end. For example, sunlight has all the frequencies that are mixed up to appear as white light to us; if you send this through a prism, all the frequencies split up so we will get some kind of a demultiplexing action. So you want to use this property for wavelength division multiplexing for achieving very high data rates. There are two kinds of wavelength division multiplexing, or WDM, that people talk about. Mostly in the backbone, people use DWDM. DWDM means dense wavelength division multiplexing. By dense we mean that we put a lot of channels, lot of ?s side by side, so we get a lot of channels. Of course, DWDM would not usually be deployed in a LAN because DWDM is costly. But then, at the backbone, where you are talking about very high speed that cost is effective; whereas another kind of wavelength division multiplexing is coming into LANS, which is called CWDM or coarse wavelength division multiplexing. There the wavelengths are not so closely packed; they are sort of more sparsely placed, which is a good thing because then, the stability of these laser sources, detectors etc., are less of an issue. So CDWM tends to be cheaper than DWDM and CWDM is coming into use these days. We will start our discussion on wavelength division multiplexing in this lecture and then we will continue in the next lecture with the details of wavelength division multiplexing. (Refer slide time: 46:53 - 47:07 min)

WDM increases the capacity of optical fibers; different wavelength lasers, each transmit at same time down the same fiber; multiplexing is combining wavelengths; demultiplexing is splitting of wavelengths. (Refer slide time: 47:08 - 48:03 min)

Usually the number of wavelengths is in the power of 2, 4, 8, 16, 64, 128, etc., things like 32, 64 etc., are big. 16, 32, 64 are deployed; now people are talking about hundreds of wavelengths, may be even thousands. Wavelengths are separated by multiples of 0.8 nm guard band: I mentioned this is equivalent to 100 GHz. There is 100 GHz separation between two ?s; that is the minimum separation, which is mandated by the ITU standard. Coarse WDM has widely separated wavelengths so that the components can be little less sophisticated and much more cheaper. (Refer slide time: 48:04 - 48:25 min)

This is a WDM system; different lasers of different lights coming together to the multiplexer, flowing down the same fiber at the same time, being demultiplexed on the other end. The multiplexer, demultiplexer could be an AWG or some other. (Refer slide time: 48:26 - 48:59 min)

In point-to-point WDM system, one point is connected to another point through multiple wavelengths. WDM is the most cost-effective technology in point-to-point technology, where the distance is about greater than 50 kms. In shorter distances, multi fiber is cheaper because in DWDM naturally your end equipment tends to become quite costly. If you have some extra strands of fiber, then that may be a cheaper option. (Refer slide time: 49:00 - 49:48 min)

Wavelength add drop multiplexer is one thing we require, when we want to add on to a stream of WDM that is going through a fiber, or we want to add on some extra wavelengths on the wave or we want to take just one wavelength out and let the others pass through. So that is a wavelength add drop multiplexer needed for routing and wavelength assignment. It performs the same functions as the electronic counterpart at the level of wavelengths. We had come across its electronic counterparts like ADM in SONET; the same thing happens in the optical domain. One problem is that granularity is high, because of inherent capacity of wavelengths; so even if you take out one single wavelength out of a whole bunch of wavelengths, that one single wavelength can carry a large amount of traffic, let us say, 2.5 gbps ? that is a high amount of traffic. If there are small amounts of traffic which you want to add or drop, then this is not a very effective technology. (Refer slide time: 50:12 - 50:36 min)

WADM has multiplexers and a set of 2 ? 2 switches, one for each wavelength. They are managed electronically; that means, these switches, etc. are programmed electronically to control which incoming length flows through and which is dropped. (Refer slide time: 50:37 - 51:01 min)

So fiber and wavelength cross connects are important components of this WDM, needed in real networks. Point-to-point connection does not need a passive star, or a passive router or active switch, but in WDM, we may require all these for an entire communication network. We have discussed this. (Refer slide time: 51:02 - 51:51 min)

Passive router: It can route separately each wavelength, no wavelength conversion; it allows wavelength reuse, same wavelength can carry multiple connections through the router. For example, the same wavelength coming from fiber 1 going through fiber 3 and the same wavelength coming in from fiber 2 and going out through fiber 4 is perfectly possible. If enough wavelengths are there, N ? N router, can route N2 simultaneous connections; some routing issues are there. We will discuss the routing issues in the next lecture. (Refer slide time: 51:52 - 52:17 min) 

Finally, we may have some active switch, which has all the features that a passive router has. The difference is that active routing matrix, which has some functionality, has to be powered. This is 
of course an issue when you are talking about a very long haul. (Refer slide time: 52:18 - 52:20 min)

With this we conclude our initial portion of our discussion and in the next lecture we are going to discuss the details of WDM; that is, how different wavelengths are routed through the network how we can get an entire network output. Thank you.

PREVIEW OF NEXT LECTURE
COMPUTER NETWORKS
Prof. Sujoy Ghosh
Dept of Computer Science& engineering
IIT Kharagpur
Lecture 11
Routing and Wavelength Assignment
In WDM all optical networks
(Refer slide time:52:49 min)
Good day, so iIn this lecture, we are going to continue our discussion on wavelength deviation and multiplexing. specifically we are going to talk about routing and wavelength assignment. (Refer slide time:53:09-54:14 min)



rRouting and wavelength assignment to what? well routing and wavelength assignment means that we have some stream of packets or whatever data or whatever communication is going on from one source to one destination. now tThese sources and destination, first of all one thing is of course of point to point connection is if there is point to point connection is directly sends to the fiber that is [noise] simple but in general, they will not be directly connected. general  tThey will go through a network, [noise] they go through some intermediate notes to reach the destination. sSo this for this stream, we have to route this that is one problem and the other thing is that may be they are all the stream right some particular wavelength for the time being. lLet us assume that it is continued on the same wavelength. sSo we have to assign one wave length, so we have this problem of routing and wavelength assignment. (Refer slide time:54:15-
54:23 min)


in WDM all optical networks ok of course we can do[noise] routing etc., very easily in the electronic domain, [noise] that is known [noise] routing in the electronic domain how it is done etc., are we are not discussed is as easier we were discussed it, later on iIn the this these series of lectures but routing, I mean we are talking about a simple kinds of routing problem here. sSo we will talk about routing and wave length assignment. (Refer slide time: 54:43-56:53 min)

sSo this is an example of light path establishment, suppose you have these a b c d e  ABCDE these are connected. so iIn the first part, the left half of the figure what we have is the physical connection from aA to bB, there is a connection and bB to dD there is a connection and so on. iIn this one[noise] iI show some[noise]that means some[noise] RW routing and wavelength assignment has already been done and some of the wavelengths and some of the links have been used. sSo here only two 2 wavelengths are used, let us say aA to bB [noise] they are not connected  say  a to c A to C the a to c A to C light path has been established via bB so and aA to bB, there will be a switch which will switch the [noise] and suppose this dashed line is the lambda one  LAMBDA 1. sSo the lambed LAMBDA1 a one coming through this fiber from a to b A to B that is switched to lambda one LAMDA 1 [noise] using the same wavelength to the outgoing fiber from b to c B to C. so wWe have light a paths established  between b to c B to C. similarly  c to d C to D, there is a light path b to d B to D, there is a light path d to e D to E there is a light path e to f E to F there is a light path d to f D to F, there is a light path and so on, using the same lambda one LAMDA 1 then iI want to connect e to c E to C. now iI cannot go from e to c E to C after say lambda one LAMDA 1 has been [noise] assign iI cannot go, but from eE by lambda one LAMDA 1 to anywhere because all the outgoing fibers the lambda ones LAMDA 1s have been used up this side, for a this side for bB and this side for f F [noise] sorry sSo inorder to connect from e to c E to C, iI use the another wavelength. which say lambda two LAMDA 2 which connects me higher dD so and the dD cross connect will connect this particular lambda LAMDA from this fiber to this fiber. so wWe will get a direct [noise] light path from e to c E to C. sSimilarly we will get a light path from b to f  B to F using lambda  two LAMDA 2A to B a to b using  lambda  two LAMDA 2 and this way, [noise] they are all connected. (Refer slide time:56:55-57:40 min)

aA burst has a long and variable length payload, if it is long low amortized overhead no fragmentation. aA control packet is sent out of band that means using some other lambda lambda LAMDA control  and reserves bandwidth that is  lambda LAMDA data reserves a particular bandwidth along a particular path and configures the switches. sSo it is like a setting up temporary light path from the source to the destination ok aA burst is sent after an offset time, it arrives at a switch after it has been configured. sSo no buffering is needed, so our original problem is of not having optical buffer. sSo buffers in the optical domain that is avoided in this fashion. (Refer slide time:57:42-58:20 min)


sSo what will happen is that, this will now moving towards the other end to the next node and  [noise] here this will again [noise] do some [noise] do the go through the O to E o two e and then do the [noise] switch [noise] configuration and then again e to o E to O and go to the next half and this delay etc., is calculated in such a fashion that when the burst arrives, what happens is that we when the burst arrives at the intermediate node, the switch fabric is already configured. sSo you do not have to store it, you simply simply passes through in the optical domain. sSo that is (Refer slide time :58:21-58:44 min)

nice so offset of course is now t minus delta T-?, because it spent delta ? amount of time over here. sSo without any delay, the burst goes through the optical switch fabric, so depending on how many intervening notes are there, you have to have this original tT, so that finally when the tT is exhausted offset is exhausted but you have also reached your [noise] destination.
COMPUTER NETWORKS
Prof. Sujoy Ghosh
Dept of Computer Science& engineering
IIT Kharagpur
Lecture 11
Routing and Wavelength Assignment
In WDM all optical networks
(Refer slide time: 00:48 min)

Good day. In this lecture we are going to continue our discussion on wavelength deviation multiplexing. Specifically we are going to talk about routing and wavelength assignment ?  (Refer slide time: 01:02 -01:04)

routing and wavelength assignment to what? Well, routing and wavelength assignment means that we have some stream of packets or whatever data or whatever communication is going from one source to one destination. Now these sources and destination, first of all, if there is point-to-point connection they directly send it through the fiber. That is   simple; but in general, they will not be directly connected. They will go through a network; they go through some intermediate notes to reach the destination. So for this stream we have to route this; that is one problem and the other thing is that may be the stream rides on some particular wavelength for the time being. Let us assume that it is continued on the same wavelength; we have to assign one wavelength, so we have this problem of routing and wavelength assignment (Refer slide time: 02:09-02:19 min)

in WDM and all optical networks. Of course we can do routing etc. very easily in the electronic domain ? that is known ? but we have not yet discussed routing in the electronic domain, how it is done etc. We will discuss it later on in this series of lectures. But in routing, we are talking about simple kinds of routing problem here. So we will talk about routing and wavelength assignment. (Refer slide time: 02:39- 03:11 min)

The optical WDM networks are future backbone for wide area networks. The reason we want it is because the bandwidth demand from the users is going up at a very fast rate;   of course, we have to go for fiber, we have to go for higher speed and then we have to pack in a lot of these channels over the same fiber. So we require an optical WDM network. (Refer slide time: 03:18 ? 03:57 min)

The speed of electronics compared to optics is a major constraint at the backbone. So it is always preferable to handle the traffic at the optical layer, where we can achieve much higher speed and the whole thing is transferred. The physical topology that we will consider at the optical wavelength routers is connected by some fiber links. So these are the same kind of topology we had talked about; that means there are some nodes, which are connected by some communication links. In this particular case, the communication links happen to be optical fibers. (Refer slide time: 03:58-
05:10 min)

Most of the node pairs in the backbones are not physically connected. So we have 30 or 40, let?s say 40 nodes. Now for 40 nodes,  there are 780 possible pairs, that is, 40 C2, that is, 40 ? 39/2, that is,  780 pairs, which is a very large number. Of course, only few nodes are connected actually through physical fibers. So they can reach each other only through other intermediate nodes. A light path or connection: this is the concept that we will be talking about most in this part of the lecture. This is the path between two end nodes and a wavelength on that path, so we want to route, that means, we want to find this path between two end nodes and we want to select the wavelength on which to send all our communication from the source node to the destination node. (Refer slide time: 05:11-05:21 min)

The intermediate nodes can cross connect that particular wavelength from the incoming fiber to the outgoing fiber. Do you remember our discussion on digital cross connect? May be through MEMS, that means, a particular wavelength is coming through one particular fiber, we direct it through some mirror to another fiber on that same wavelength. It is as simple as that. Thus  with programmable optical switches, the traffic can be routed entirely in the optical plane.  (Refer slide time: 05:51-06:14 min)

So no wavelength conversion for the present, and any light path uses the same wavelength on all the links its path spans. So 16 or 32 wavelengths in each fiber are common these days and hundreds of ?s are being talked about. A single fiber could theoretically accommodate as many light paths. (Refer slide time: 06:13-06:39 min)

What are the basic concepts of the light path? End users use light paths to communicate, each light path passing along the same fiber occupies different wavelengths; that means on the same fiber there may be different light paths going through the same fiber, each of them would be writing on a different wavelength or different lambda two ?s. Two light paths with the same wavelength cannot share a fiber, that means two different light paths who are on the same wavelength they cannot have any fiber in common because then their signals will get mixed up with each other. Of course, we can have (Refer slide time: 06:52-07:36 min)

WDM with wavelength conversion also. As I mentioned in the previous lecture that wave length conversions are possible although they are quiet costly. So it is possible in different extents, for example none that means no wavelength conversion or we can have a partial wavelength conversion that means some of the nodes have WC capabilities while others do not have or it may be full. If you have wavelength conversion we get some kind of flexibility in routing and wavelength assignment. We will not concentrate on that but you can understand that if we could convert the wavelength midway that means, as the signal is going, we can convert the signal midway, then maybe we can have better possibility of accommodating many more light paths. For our purpose, we will be concentrating only without wavelength conversion. (Refer slide time: 08:00-08:21 min)

So how to establish light paths? Two parameters to be decided: path from source to destination ? this is the routing part ? and wavelength along the path, which is the wavelength assignment part. Traffic types subjected to optical networks may be static or dynamic; that means you know the specific source destination pairs, which have to be connected in advance and that is quiet stable. So now you may run some centralized algorithm kind of things and find the best way to assign the light paths. The other situation could be dynamic; dynamically there is a request from some node for a connection to some other node and you want to set up the light path on the fly and take it down. That is a slightly more complex situation. (Refer slide time: 08:59-
09:52 min)


So two versions of RWA are realized depending on the characteristics of the traffic applied: static light path establishment or dynamic light path establishment. Now routing and wavelength assignment. Several, signals can share a single fiber. All signals must have different wavelengths, so these are the constraints. Of course, technology sets the upper limit to the number of wavelengths. You cannot have  unlimited number of wavelengths, you can have a large number these days but even then this is not unlimited. So these are the constraints, which you will have to respect while doing the routing and wavelength assignment. The problem is how to choose a route and a wavelength to each connection, so that signals won?t block each other. How can signals block each other? Well, the signals can block each other in this way. For example, suppose you have routed two particular source destination pairs in a certain manner using a certain wavelength, let us say ? 1. Another pair of source destinations was, let us say, N1N2; then we want to put a connection between N3 N4,  for which the route happens to share some of the links. In these links now you cannot use ?1 for this, because ?1 on those links is already occupied by the link from N1 N2. So if N3 N4 share even one link, you cannot use the same ?1; you will have to use ?2, but then ?2 has to be blocked because of some other links and so on. (Refer slide time: 10:52-11:23 min)


So the objective of SLE, that is, static light path establishment, is to minimize the total number of used wavelengths connecting the maximum number of nodes. They are the same; if you can connect some particular set of nodes using the minimum number of wavelengths, using the number of wavelengths, which are given to you, you may connect the maximum number of nodes. Other objective functions we may also consider are the load in the most loaded link, the total number of optical switches, etc. (Refer slide time: 11:24-13:48 min)

So this is an example of light path establishment. Suppose you have these A B C D E connected. So in the first part the left half of the figure what we have is the physical connection from A to B there is a connection and B to D and so on. In this, I show some  RW routing and wavelength assignment has already been done and some of the wavelengths and some of the links have been used. So here only two wavelengths are used. Let us say A to B are not connected; say a 
light path has been established via B for A to C. At B, there will be a switch, which will switch this dashed line, which is ?1. So ?1 coming through this fiber from A to B is switched to ?1, using the same wavelength to the outgoing fiber from B to C. So we have light path established between B to C.  

Similarly, from C to D there is a light path;  there is a light path from B to D; there is a light path from D to E; there is a light path from E to F; there is a light path from D to F and so on, using the same ?1. Then I want to connect E to C. Now I cannot go from E to C. After say ?1 has been assigned, I cannot go from E by ?1 to anywhere because all the outgoing fibers of the ?1s have been used up this side for A, this side for B, and this side for F. So, in order to connect from E to C, I use another wavelength, say ?2, which connects me via D and the D cross connect will connect this particular ? from this fiber to this fiber. So we will get a direct light path from E to C. Similarly, we will get a light path from B to F using ?2, A to D using ?2 and this way they are all connected. As you can see, a large number of light paths have been established using just two ?s. (Refer slide time: 13:49-14:35 min)

Dynamic light path establishment, on the other hand, is appropriate for network having dynamic traffic, where the traffic pattern changes very dynamically and a communication request may arrive at any time. The goal is to maximize the number of the incoming communication requests accepted. Well, if it so happens that you have already made some  kind of reservation and some kind of allocation to an earlier request and when a new request comes, you find that there are no paths on which a ? is consistently free throughout. So in that case, you have to block that request or regret that request. So we want to minimize such regrets and maximize the number of incoming communication requests that can be accepted. That is the goal of DLE. (Refer slide time: 14:36-14:57 min) 


 In order to solve this problem, let us see what the complexity of the problem is. The RWA problem is difficult; it can be divided into two sub-problems ? routing and wavelength assignment. Both of these sub-problems are NP complete and tightly linked together. If you remember, by NP complete we mean that class of problems for which no  polynomial time algorithm is known and so, if you can solve any one of them in  polynomial time, all others can be solved in polynomial time. It is not known whether  any polynomial time algorithm exists, but since the number of such problems is so large  and people have thought about these problems in so many different guises, it is very unlikely that you would suddenly come up with a polynomial time solution to any of these. And of course, when an algorithm has an exponential complexity, it takes a lot of time. So that is a difficult problem in that sense; both its parts are NP complete. So there is no exact solution. So we cannot expect to get the so-called ideal solution or the optimal solution in all cases, but we can try good heuristics. (Refer slide time: 16:00- 16:31 min)

Now what we want from a good algorithm ? maximize the number of connections; use the shortest routes; and minimize the number of wavelengths. If you can maximize the number of connections, this is of course our basic requirement. If you use the shortest routes or the number of hops should be minimum, delays and all kinds of things should be minimum. So that is another good thing to have, and if you minimize the number of wavelengths that have been used, future requests can also be possibly accommodated.  (Refer slide time: 16:32- 17:06 min)


So for routing, there are different techniques like fixed routing; that means the path that is predefined is used; that means the for all pairs we have some fixed paths and fixed alternate routing, meaning multiple predefined paths are there and one of them is selected. So the criteria could be shortest path, least loaded path, least congested path, and so on. (Refer slide time: 17:07-17:27 min)

There could be adaptive routing techniques; the path is found on the fly. That means as the network is in operation dynamically, you find the least congested path or the most suitable path depending on the current situation in the network, and once again we may try to find the shortest path or least loaded path. (Refer slide time: 17:28-18:03 min)

Now how do the wavelengths affect routing? There cannot be two signals with same wavelength per fiber for two different light paths; that is not possible. So the shortest route may be blocked by other signal and in the worst case all the routes are blocked. Then, of course, either you have to reassign the wavelengths of other earlier assignments or you have to block this request. If the wavelength conversion is possible, the signal can use other free wavelengths but the conversion is not always supported. (Refer slide time: 18:04-18:15 min)

Wavelength assignment problem arises under the wavelength continuity constraint; that means the wavelength has to be continuously available in all the links down the line and there are various algorithms for that. One of the most commonly used one is the first fit,  which means, you have decided on the path by some means; that is the routing path. Once you have decided on the path you have a list of these wavelengths, ?1 to ?N, and you try them in order. The first wavelength, which is free on all the links in the route is the one which is assigned to this particular light path request. That is called first fit, and it has some good points in the sense that since you are always trying from the same end, you tend to use up these earlier ?s more and more, giving you very good utilization of the ?s. So you may be having a lot of other ?s in the network. You have them on reserve to accommodate the request. So that is a first fit algorithm. But then again you must remember this is just a heuristic, in the sense that there is no way you can prove ? as matter of fact you can disprove it. In some cases, this would not give you an optimal algorithm. In some cases it may give you an optimal algorithm. In many of the cases, it has been found that this gives good results, but in some cases, it may give very bad results. It may be random. You may want to distribute the load all over, so it may  be random or people have used the least used; that means the least used ?, which is most free, is the one that is picked up. When you try all the ?s one by one, you reach the ? that can fit this particular route. So this will have the opposite effect, in the sense that this will try to distribute the load evenly across all the ?s or the most used ones; it need not be the first fit or the least loaded, etc. So these are the different algorithms or different heuristics, which you might use for assigning your ?. Of course, what you could do also is that at some point you could go back and instead of asking about your routing path you can ask, give me the next alternative path. So from the shortest path, which it might have given in the first instance, it might give you the second shortest path on which you may try the same kind of wavelength assignments using that. So all kind of combinations of routing and the wavelengths, all these strategies are possible. (Refer the slide time: 21:18-21:34 min)

What are all the good points about wavelength routing? Setting up a light path is like setting up a circuit. You remember we have talked about circuit switch network and packet switch networks? So in circuit switch networks, specifically the telephone circuits, even today may be majority of their networks are circuit switches; that means, a call from one node to another, one caller to one callee, a continuous circuit is set up. All those things have become more complicated as we have seen because of the TDM and all kinds of packets and all these things are coming in between. Anyway, the essential circuit switching is that the circuit is switched. Just in the same manner, when we are assigning a particular light path to two nodes, which have to be connected, it is just like setting up a circuit. And once the light path is set up, the route is fixed and the wavelength is assigned and the light path is set up. Then these two nodes can communicate, pumping as many packets as they like through this, of course limited by the technology constraints. But this is quiet a high constraint. So the inherent advantages of setting up this circuit switching is that in this particular approach to the problem specifically the quality of service is good. What we mean by the quality of service is smooth traffic and QOS guarantee can be given due to fixed bandwidth reservation and SLE is easy to manage, that means static light path establishment is easy to manage. That is the advantage of wavelength routing. (Refer slide time: 23:22-23:39 min)   

The disadvantages are the following: long circuit set-up time. This circuit time will be in the order of tens of milliseconds. Actually the circuits are set up in such a way that the source sends some request and the request goes down all the nodes on the way. The local switches have to be set up in that particular fashion and then they will acknowledge and say that ok the light path has been set up. So all this communication takes the order of tens of milliseconds. Now tens of milliseconds may be a lot of time depending on  the situation. When a very high-speed communication is going on, then tens of milliseconds is a lot of time. But if you are going to set up a light path and then use it for one month or one year, then tens of milliseconds may not be much. So it depends on the situation; but if your traffic is changing dynamically then you want to adopt what light paths you want to assign etc., in a dynamic fashion, then tens of milliseconds may be a lot of time. The other problem is that the huge capacity of 1? can carry how much traffic, let us say of the order of 2.5 gbps or something like that or may be even more these days. Its capacity is very huge. The point is that for just two nodes communicating, it may be a gross underutilization if proper traffic grooming is not done at the edge. What you mean by traffic grooming? You remember that at the edge, first of all, when we are talking about the edge, we are talking about the backbone network, which is where usually all your WDM systems will be deployed. So we have this backbone network and these backbone networks have been fed by all different networks, are routed from different networks etc., so number of streams are coming into this backbone network. They are converging and then traveling to the backbone, and then going out on the other side. So, the routers which communicate with this backbone are the edge routers and the light path you have got from N1 to N2, as we have seen is the granularity is quite coarse; that means a quite high amount of traffic can be handled. So if the edge router get together a number of streams from different sources and then uses it, it?s good traffic roaming; it loads it quite well then that is good. So we are achieving high efficiency if it is two single users and the two end then it is a gross under utilization of this ?. Unfortunately in this circuit inherently you cannot have less than 1? for a circuit, if you are setting up a circuit. You have that capacity but you are underutilizing it. This is the same problem that we had seen in circuit switching versus packet switching, and as a matter of fact there is a development as we will see for the packet switching part also, which we will see in the next half of this lecture. (Refer slide time: 27:03-27:13 min)

So bandwidth is inefficient for bursty traffic. This is another term that you should know, if you remember I told you while discussing about the telephone traffic that the Bell telephones people had done a lot of study about what kind of traffic is there, and how long the people talk and how frequently they talk at different times of the day, etc. We know that this is the some kind of Poisson distribution with exponential inter arrival time etc., so  those are very well-behaved kind of systems. Unfortunately, in recent years first of all the voice traffic has shrunk to may be less than 5% of the overall traffic in the network, 95% of it is data traffic and this data traffic has seen to be very bursty. They also follow some kind of statistics, but it is a complicated kind of statistics, the whole thing is that the data traffic seems to come in bunch. When they come lot of packets are coming and then for a long time there is no traffic. It is also self similar, and it is complex kinds of statistics but in anyway the point is, for long time for which when it is not sending anything, the circuit is remaining unutilized; so this is a gross under utilization of the circuit. The busy time versus the lean time may be of the ratio of 1:500 or something like that. So that is why this becomes an inefficient way. Another disadvantage is wasted bandwidth during either off or low traffic periods for SLE or too much overhead, that is, delay, due to frequent set up or release in dynamic light path establishment. That means if you want to put it down and then set it up again, in that time frame this becomes too much of an overhead because, if you remember, for setting up a light path we require may be tens of milliseconds. So you cannot do it very fast. (Refer slide time: 29:16-30:05 min)   

Particular ? path specific pros and cons are that, they have very coarse granularity, as we have discussed i.e., OC 48 and above. OC 48 if you remember is 2.5 gbps or above that. They have limited number of wavelengths and number of light paths; no aggregation, that is, merging of the light paths inside the core. So inside the core, there cannot be any aggregation etc. Traffic grooming can only be done at the edge so this may be complex or inflexible. That OXC has a millisecond kind of switching time. So these are the pros and cons of ? paths. (Refer slide time: 30:06-30:10 min) 

So this takes us to the next half of our lecture, where we will have a look at the other way and other approach to this whole problem, namely, optical burst switching. Just as the previous one was circuit switching, where we were doing circuit switching; we were setting up light paths. Here we will try to take the packet route; that means we will send them packet by packet. How that can be done? Originally, the packets may have originated from some say TCP/IP or Ethernet or some computers as small kinds of packets. Doing it at that particular level becomes difficult at the optical level. So we have some technology for that; we will look at this now. We will discuss the next topic, which is optical burst switching. (Refer slide time: 31:13-32:19 min)

First of all, let us do one quick comparison between electronic versus optical switching. Data is transmitted optically in WANs, MANs and even some LANs. Electronic switch uses digital switching fabrics; converts data from optical to electronic for switching and then from electrical to optical for transmission. That means what we are doing is that, we are trying to do the switching in the electronic domain. As mentioned a number of times before, the electronic switching always has problem; at high speeds it is very costly and this is not very easily upgradeable, scalable, etc. So that is a problem; but if you have done the switching in the electronic domain, there will be no problem. The kind of switching and kind of logic, kind of algorithm that we use in the other parts of the network could be put over there but we want to avoid electronic switching and do optical switching as much as possible. So let us see how that can be done. (Refer slide time: 32:20-32:33 min)

The cost of electronic switching goes up steeply as the speed requirement increases and optical or photonic switching uses optical switching fabrics, keeps data in the optical domain. (Refer slide time: 32:35-32:51 min)

So, why not we keep the status quo; that means that do the switching in the electronic  domain only? Well, the trouble is that the data traffic growth is still doubling every year and this is different from what we have in the computing domain. In the computing domain or Moor?s law, it doubles may be in one and a half years and this is doubling every year. Actually if you see over the long range, although the computational speed has  increased many fold, the communication speed has grown even more, by an order of magnitude more maybe. So the point is that electronics is unable to move and that is not possible for developing at that particular rate of development. So pure electronic processing and switching can hardly keep up. Electronic MUX, DEMUX, space power consumption, heat dissipation, etc., are always a problem because heat dissipation is more, it requires most space etc. There is no transparency, meaning, it depends on the kind of system and the kind of the modulation system, the kind of multiplexing system that you are using, which you have to use in the electronic components. So when the technology moves on, these intermediate nodes will also have to be changed, whereas, the optical switching, since it is transparent, does not matter if you change what you are sending through that pipe. (Refer slide time: 34:33-34:51 min) 

The cost factor, of course, weighs the heaviest. Though the cost of OEO at 0C-48 is going down, the overall cost including WDM system at 0C-48, is still a dominant factor. OEO at 0C-192 and higher in the future will still be a dominant cost factor. (Refer slide time: 34:52-35:18 min)

So we want to go to optical switching, whose advantage is that, low cost and high capacity, transparency; that means it is independent of bit rate, format, protocol, etc. It is synergetic to optical transmission and future proof; that means when we upgrade, all these things might change but my intermediate node will not change in the optical switch. (Refer slide time: 35:19-35:41 min)

Opaque, that is OEO switches, are more mature and reliable, of course. So still they need some electronic processing and control; that means, when we are doing in the optical domain, we still require some electronic processing and control. What we do is, we try to minimize this, and optical 3R and performance monitoring are hard. You remember that we can amplify a signal quite easily but if you want to do all the 3Rs that means Regenerate, Reshape and Retime, then we prefer to take it to the electronic domain.   (Refer slide time: 35:58-36:30 min)

Packet switching: a packet contains a header; that means addresses and the payload. It is a variable or fixed length. The advantage of packet switching is that it has some kind of statistical multiplexing; it can be sent without circuit set-up delay ? if the line is there just simply send it. It also enables statistical sharing of link bandwidth among packets with different sources and destinations. So that makes it more efficient; bandwidth usage is more efficient. (Refer slide time: 36:31-37:04 min)

So packet switching is done usually this way. Store and forward at each node: it buffers the packet, processes its header, and sends it to the next hop. This is how usually it is done in the electronic domain; we will look at the details of this later on. But usually what would happen is that, when we are using a circuit switching, the circuit is set up from N to N and then we can send whatever we like. But when it comes to packet switching, since it is coming packet by packet, each packet has to be processed independently. We have to look at this packet, look at this header, see what the destination is, decide on which next link to take, etc., and send it. So you have to store this packet in a buffer and then examine, do some processing and then forward it ? this is the store and forward paradigm. This is usually done in packet switching. (Refer slide time: 37:37 ? 37:43 min) 


We have some problems with that optical domain, as we will see. It is statistical multiplexing and is inherently bandwidth efficient. (Refer slide time: 37:43 ? 38:25 min) 

Now if you have a packet core, well, we have the access or metro networks, optical buses, passive star couplers, etc. SONET WDM rings or token rings are used. We will talk about token rings later. It uses switched networks or gigabit Ethernet. So these are the kind of technologies that are deployed in the LAN/MAN side, whereas in the WAN side, we have the ? routed virtual topology, i.e., circuits or leased lines. We have dynamic ? provisioning; that means circuits on demand and optical burst switching, which we are talking about now. (Refer slide time: 38:27 ? 38:54 min)

The technology drivers for this are the explosive traffic growth, as I already mentioned, bursty traffic pattern, and to increase bandwidth efficiency. To make the core more flexible, naturally a packet?s system would be more flexible than those fixed light path kind of system to simplify network control and management and making the core more intelligent. (Refer slide time: 38:54 ? 40:05 min)

How important is this bandwidth efficiency? We are always talking about the bandwidth efficiency. Well there are two views to it ? one is the user?s point of view. Well, user wants some bandwidth today and if the bandwidth becomes cheaper and as it becomes available, he immediately thinks of another application and his bandwidth demand will grow. Previously, people were very happy to send some simple text. Now, they are downloading, then they want to download songs, files, they want to download entire videos, then they will try to do video conferencing with each other. So these are very bandwidth intensive applications and they require a lot of bandwidth. So from the users? point of view, the more the bandwidth you give, I will bring lot more applications and I will just use it up. So with more available bandwidth, new bandwidth intensive applications will be introduced. High bandwidth is like an addictive drug, cannot have too much of bandwidth from the users? point of view. (Refer slide time: 40:05 ? 40:38 min)

From the carriers? and vendors? point of view expenditure rate is higher than revenue growth sometimes; so longer-term equipment investment cannot keep up with the traffic explosion. So, you have to see that whatever I invest today, how long in future I can take so that my investment is lower. We need bandwidth efficient solutions on the infrastructure existing today that will be competitive. So these are the different issues which (Refer slide time: 40:39 ? 42:07 min)

brought us to optical packet switching or optical burst switching. I will come to that later on. But this is our goal; there are two problems; and one of them is the lack of optical buffer. In the optical domain, if some packet is coming, means some light pulse is coming now, how do you store them in the optical domain? There is no good buffer for optical domain; there is a thing called fiber delay lines. This is really a very bulky and not very good stuff, you can put it in the fiber delay line. As it comes, it goes to that fiber delay line and comes out of the other, the whole thing would be delayed a little bit; that is something akin to buffer in it. But there are severe limitations on how much you can delay, that is one thing. Secondly, it is bulky, expensive and not very good. So fiber delay lines are bulky and provide only limited and deterministic delays. Store and forward with feedback FDLs lead to fixed packet length and synchronous switching. So we cannot use because of this simple store and forward, and the other thing is that tight coupling of header and payload requires stringent synchronization and fast processing and switching. So these things are difficult. (Refer slide time: 42:07 ? 43:49 min)

So we go to this optical burst switching or OBS; a burst has a long and variable length payload. So first of all, variable length payload means we want to keep it as flexible as possible; that is one good aspect of it. That means we want to keep it as flexible as possible. The other thing that we want it to be is it is long, why is it long ? because we have to do some processing, some setting up, etc., for these burst of packets to travel. So what we do is, we will do some grooming in the electronic domain. We  collect a number of packets together forming a burst, which has the same source?destination pair and then we set up the path and send this burst along all in the optical domain. So that is the essential approach to optical burst switching. A burst has a long and variable length payload, if it is long and has low amortized overhead and no fragmentation. A control packet is sent out of band, that means using some other ? control and reserves bandwidth ? that is ? data reserves a particular bandwidth along a particular path ? and configures the switches. So it is like setting up a temporary light path from the source to the destination. A burst is sent after an offset time; it arrives at a switch after it has been configured. So no buffering is needed. Our original problem is of not having optical buffer, so buffers in the optical domain are avoided in this fashion. (Refer slide time: 43:49 - 44:51 min)

We have to do a burst assembly and disassembly at the edge at the source side. That means the client data may be the IP packets, which are the most dominant ones. They are assembled together into bursts; and burst switching or reservation protocol is done, that means, we send the control packet, an offset time t ahead of burst. So within this offset time, all the switches down the line will do their programming. That means they will set up all their mirrors or whatever it is their cross connects, etc. So that later on, when the burst does arrive, we do not have to do any processing on that and if you are do not doing any processing on them we do not need to store them either. They can go straight away in the optical domain. There is a dedicated control channel, which is out of band signaling for the control packets. (Refer slide time: 44:52 -45:25 min) 

The advantage is no fiber delay lines nor OEO conversions for burst at any intermediate nodes, photonic burst switching fabric inside the core. That means it leverages best of optics for burst switching and electronics for control packet processing and fabric control. So just for the control part, we do this OEO and for the bulk of it, the burst, we do not have to go to the electronic domain at all. (Refer slide time: 45:25 - 45:55 min)

This is a diagram, say assembly queues for different egress nodes; these are going to different channels. This is an ATM cell IP packet or SONET, and we have an IP packet over here. We have a SONET frame over there because if you are sending things in the purely optical plane, you do not really care what the payload contains. It may be an IP packet, it may be a SONET frame, it may be some cell, it may be anything else we do not care. Intermediate optics is transparent to all that. So what happens is that (Refer slide time: 45:16 ? 47:18 min )

we use the ATM cell for the control packet purpose. So we make a control packet, which assembles a burst, and as it assembles a burst, it knows what the time or length threshold reached is. The length of the burst may be variable, as we said the burst could be long and of variable length. But when all these different IP packets frames, SONET frames etc., are put together, what happens is that a control packet is generated and sent out. The control packet now knows the source, it knows the destination, it knows the length of the burst, so it sends through a separate control channel, so this control channel goes through the control plane as we will see. (Refer slid time: 47:18 -47:44 min)

So we have the assembly queues for different egress nodes; that means the destinations, for different destinations, all these packet frames etc., are getting queued up and forming into bursts. (Refer slide time: 47:44 -  48:04 min)

Now we see fiber delay line ? as I just mentioned it fiber delay line, feed forward or feed backward. So there is no optical RAM for store and forward; every FDL provides only limited delay and cannot perform most of useful buffer functions. So FDL units are bulky, affect signal quality etc. (Refer slide time: 48:04 ? 48:24 min)  

Now going back to this OBS, we have various schemes still involving in an active area of research. I will just present a simple scheme called just enough time or JET. There is an offset time between CP and burst. So what is done is that the control packet is sent and after the control packet is sent,  there is a delay. We give a delay and then we send a burst;  this delay is to cover all the programming time on the intervening nodes. (Refer slide time: 48:43 ? 49:05 min)

So an offset time between CP and burst: no fiber delay line required to delay the burst, when CP is processed and switch fabric is configured. CP carries the burst length info, facilitates delayed reservation for intelligent efficient allocation of bandwidth and FDL if any, including look ahead scheduling. We need not go into the details. (Refer slide time: 49:10-49:53 min)

We have the control packet here, which is moving in the control plane, and we have a burst over here and there is offset time T and we have just enough offset, JET, which we require for programming these intervening optical  nodes. So CP arrives at OEO node at time, let us say, T1. But the control packet is being taken to the electronic domain for processing, because this will have to be processed etc. So it is better done in the electronic domain. (Refer slide time: 49:54 ?50:05 min) 

Then CP goes through the optical to electrical conversion and configure the switch fabric and then it will move on. (Refer slide time: 50:06-50:12 min)

CP goes through EO conversion and leaves the OEO node at time t1 + ?. (Refer slide time: 50:13-50:53 min)

So what will happen is that, this will now move towards the other end, to the next node, and here this will again go through the O to E and then do the switch configuration and then again E to O and go to the next hop. And this delay is calculated in such a fashion that when the burst arrives, what happens is that at the intermediate node, the switch fabric is already configured. So you do not have to store it; you simply pass through in the optical domain. (Refer slide time: 50:54-51:16 min)

Offset of course is now T ? ?  because it spent delta amount of time over here. So without any delay, the burst goes through the optical switch fabric.  Depending on how many intervening nodes are there, you have to have this original T so that finally when T is exhausted, offset is exhausted but you have also reached your destination. (Refer slide time: 51:17-51:37 min)

That is just enough time. And finally to conclude, OBS is a programming switching   paradigm that offers many advantages over the existing technologies, but is not likely to be the end-all kind of solution. OBS has several variations and adopting OBS will be an evolutionary process. This is another problem ? when you have a new scheme and there are different researches, they will try to do research and come up with different suggestions and then at different places some different things may be adopted. But then, in order for the entire network to work in a very smooth fashion, you have to come to some standard, so OBS has not come to that standard yet, but it is a quite a promising approach. Now, we have talked about the two approaches in WDM, namely, this circuit switching path, that is the light path routing and wavelength assignment and setting up of light path, and we have talked about optical burst switching. In the next lecture, we will talk about SONET infrastructure. The SONET, which we have talked about, a lot of it is also in fiber optics. Then we have all these fiber optics; that means packet oriented fiber optic infrastructures are also coming in. That means large routers, etc., are coming into this picture, one thing we did not discuss although we mentioned it a while discussing about SONET was that this has an inherent capacity of fault recovery, recovering from fault. So that is one thing that we would like to discuss in the next lecture ? not only in SONET, in optical network, but in general, how do you handle faults. That is very important, because this optical network is at the core of the network and if the core of the network fails, its repercussion is tremendous, both economic and other repercussion may be tremendous. So we like to put in lot of reliability into all this fiber infrastructure that we have, and how exactly that is done, we will discuss in the next lecture. Thank you.
Preview
COMPUTER NETWORKS
Prof: Sujoy Ghosh
Dept of Computer science& engineering
IIT Kharagpur
Lecture 12 
Protection and Restoration 


(Refer slide time:53:37-53:41)

Good day. In  this lecturer, we are going to discuss the various protection and restoration  mechanisms, which are usually employed in  optical  networks 
(Refer slide time: 53:54-53:57 min)	

We will be talking about protection and restoration, now of course we have to discuss, what is protection and restoration.    Why we need them? (Refer slide time :54:05-54:16 min)

What is protection and restoration, comparison between the two and the different schemes of rotation? This could be   our general outline   of presentation. (Refer slide time:54:19-54:25 min)

Network is unreliable somehow , so many failures can occur,  node may fail a link, may get cut, some fiber optic line may cut in between because  somebody cut it , while digging a whole or something or node might fail there may power failures at nodes and so many things.   So there are failures in the network but if you remember  that  one of the most important places where we deploy optical networks is in the core of the network  and the core of the network connects so many people, to so many other people. It is so very vital  that we cannot allow the services to be a severely desalted because that would have very grave consequences. Anyway the service provider attempts to give a very high level of service.   So although failures are   unavoidable in real life, we have to find a some way of combating this, that means  if there is a failure, we want to recover from it as soon or as fast as possible so that, is what protection and restoration is all about. Another thing is that when we want to give some reliability, protection, restoration etc we always in some form, always have to bring in some redundancy. Without any redundancy a system  cannot be  protected, it cannot be restored without any replacement etc., so there has to be some redundant capacity in some form in the network, in order to achieve protection and restoration. How this is done that is what we will discuss. (Refer slide time: 56:14-56:25 min)

So   first let us talk about  path protection, it uses more than one path to guarantee that data is sent successfully.   so if you  look at this graph,  it will be having a six node graph, where 1 and 6 are communicating on the top  through the dash line.   We show the primary path, which is the primary connection between 1 and 6. What might happen is that the link between 2 and 3 might snap due to some reason. So we have already got a backup route which is calculated going through 1 4 5 & 6 . We will channel our communication through the backup link and please note that this  backup or secondary path from the source of the destination is link. This channel does not share any link with the primary path, so that is the requirement. If any link in the primary path fails,  I am assuming only one failure which means that the backup link is all intact and you can switch to the backup path for this particular communication. (Refer slide time:57:24-57:47 min)

Dedicated link protection is not always practical, sometimes it may have it shared link protection is practical, it is quiet often and it is implemented and   it may fail, when this link protection may cause failure here you are only provisioning for the failure of the link,   but if a node fails, then it may lead  to some complications as we will see. (Refer slide time:57:48-58:07 min)

So to compare between path switching and line switching path, switching of course is a coarse scheme and line switching is a finer scheme and line switching is can again be span protection, span would may be a several links together and that may be a span or  a line protection (Refer slide time:58:08-58:37 min)        

In mesh networks, of course the restoration is possible only if the graph is 2 edge connected that is by connected which means that there are 2 edge connected disjoint paths between any pair of nodes so that no single edge failure can disconnect the network. So this is a  necessity and usually  try to keep that way, unless it will be difficult or it is not a cost,   effective etc., (Refer slide time: 58:37-59:21 min)

Protection in a mesh networks of course more complicated, then a ring   simple minded scheme would be 2 edge node, no disjoint paths for each connection 1 +1 not as is  mentioned here, this not very efficient. There may be many paths and provisioning double the number of paths, which are pair wise mutually node are edge disjoint that may be very difficult. Provisioning in the network better approach would be line protection which of course have the problem of coordination.   I will show the later on are protection cycles in mesh net, again I will show this.
COMPUTER NETWORKS
Prof. Sujoy Ghosh
Dept of Computer Science& Engineering 
IIT Kharagpur 
Lecture 12 
Protection and Restoration

(Refer start time:  00:43)
Good day. In this lecture we are going to discuss the various protection and restoration mechanisms which are usually employed in optical networks.
(Refer slide time:  00:57-00:59)

We have to discuss what is protection and what is restoration and why we need them. (Refer slide time:  01:08-01:18)

What is protection, what is restoration, comparison between the two and the different schemes of protection would be our general outline of presentation .
(Refer slide time:  01:19-01:26)

Network is unreliable; and then so many failures can occur ? a node may fail, a link may fail or a link may get cut, some fiber optic line may cut in-between because somebody cut it while digging a hole or something. A node might fail, there may be power failures at nodes and so there are failures in the network. But if you  remember, one of the most important places where we deploy optical networks is in the core of the network. The core of the network connects so many people to so many other people and it is very vital that we cannot allow the services to be severely disrupted because that would have very grave consequences and anyway the service provider attempts to give a high level of service. Although failures are unavoidable in real life, we have to find some way of combating this; that means if there is a failure we want to recover from it as soon or as fast as possible. That is what protection and restoration is all about. Another thing is that when we want to give some reliability, protection, restoration, etc., in some form or the other we always have to bring in some redundancy. Without any redundancy, a system  cannot be  protected, or it cannot be restored without any replacement, etc. There has to be some redundant capacity in some form in the network in order to achieve protection and restoration. So how this is done is what we will discuss. (Refer slide time:  03:20-03:25)

Protection and restoration are the mechanism to recover from network failure: their difference will be discussed in the following parts. (Refer slide time:  03:26-03:37)

Why we need protection and restoration is now clear: to recover from network failure, to prevent the lot of data loss. Now another point is what would we mean by prevent lot of data loss? The point is that we will be talking about protection at the optical level, at the lower, transport level, that is, the low physical level. Above  this  physical level, there are a whole lot of other layers like data link layer, network layer, transport layer, etc. They may have their own protection mechanism and they might be able to tolerate. In some cases, if such a protection is sort of implemented at higher level, such protection usually would tolerate a small amount of data loss, which they will retransmit or do something in the protocol to take care of that. We will be talking about this later on. Within that limit we do not have a 100% tight case, but we have something to play with. Within that limit, if the physical layer can come back up, then that is nice; then the end user who is sitting at the top of the application layer would not notice that  a failure has actually occurred. So this is our general goal and, of course, to prevent lot of data loss. (Refer slide time:  05:13-05:18)

To provide reliable communication service is a reason for having protection and restoration. (Refer slide time:  05:19-05:39)

Protection is the primary mechanism; this is fast and routes are usually preplanned ? we will come to this part later on ? whereas restoration is the secondary mechanism, used to provide more efficient routes or additional resilience, etc., over and above protection. We will see both of these later on. (Refer slide time:  05:40-06:11)

Techniques for protection: we could protect a path, which is called path protection; we could protect a link; that is called link protection. There are various schemes of protection: like 1+1 protection, 1:1 protection, 1:N protection, M:N protection. This depends on what kind of redundancy we have built into the network. So we will look at some of these techniques one by one. (Refer slide time:  06:12-06:26)

So what are the considerations and tradeoffs that we have for protection? For support for fast protection, time is dictated by the client layer. This is what I was talking about earlier ? in the client layer, whatever the higher layer we are talking about, all of them are clubbed together, and we are calling them client layer, let us say. In the client layer we may have some resilience; that means, we can tolerate some small amount of data loss. So depending on that, and beyond that, it will be taken that the service will fail. This is dictated by the client layer and that is the constraint within which we try to do our protection or restoration at the bottom layer, that is, the physical layer. We require some switching technologies (Refer slide time:  07:13-07:33)

as we will see, and how to implement protection. It could be through dedicated hardware or through software. Here we will only be talking about ? since we are talking about the physical layer ? hardware protection, where some hardware has been put in order to take care of this protection. (Refer start time:  07:34-07:51)

Support for low priority traffic ? that is another consideration we might have. So low priority traffic supported using the protection bandwidth; traffic dropped in case of a failure. As I mentioned earlier that in order to give some protection capability into the network we have to build in some kind of redundancy over there. Now when there is no failure, naturally the redundant link would be idle. What you could do is that if you have some low priority traffic, which could be dropped whenever there is a problem, under normal circumstances you can use your provision in the network to carry the low priority traffic, and as soon as there is a failure, where naturally the provision is going to be used up for providing protection, restoration, etc., the low priority traffic would be discontinued. So that is one thing we could do. (Refer slide time:  08:46-09:03)

Support for mesh topologies ? mesh topologies are bandwidth efficient, fast signaling mechanism, flexibility in choice of routes by preplanned routes, etc.  If we have support for mesh topologies, it is also nice. As  a matter of fact, if you remember our discussion about different types of topologies ? we have star, tree, ring, mesh etc., so these are the different possible topologies. Out of all these,  point-to-point connection, ring and mesh are the three topologies, which dominate most of the WAN. When you want to communicate between two points, you first set a point-to-point link. Now consider that link to be quite important. Then what you would like to do is that you would like to put in some kind of reliability over  there, by making it into a ring. For example, we discussed SONET. A SONET gear quite often is put in the form of ring, rings that would touch, mutually touching rings. For example, the telecom people put up all these SONET rings through fiber optic networks. The rings are quite easy to handle and we will discuss the protection mechanism in ring quite a lot. But if you go to a wider geographical area ring, it may not to be feasible due to various reasons. What you have is a mesh. A mesh, if you recall, is just a graph, where the nodes are connected in some fashion. It has to be a connected graph and actually later on we will see that for giving proper protection, it has to be not only a connected graph, it has to be biconnected graph as well. That means between the two nodes, which are communicating, there have to be two alternatives paths, which are linked somehow. Otherwise, you will not be able to give protection. Giving protection in mesh networks is also an important consideration. (Refer slide time: 11:17-11:55)

Other important considerations are maintenance of large distributed routing tables, that is, precomputed routes or up to date topology maps. So you have to maintain this dynamically, because a network is a very dynamic thing as links come up or go down or nodes come up or go down, this may have to be recomputed and stored in a distributed fashion. We would like to have support for all failure modes: node failure for mesh networks and for ring networks, so or it may be of course ring failures.  (Refer slide time:  11:55-13:05)

First, let us talk about path protection. It uses more than one path to guarantee that data be sent successfully. If you look at this graph, it will have a 6-node graph where 1 and 6 are communicating and on the top, through the dashed line, we show the primary path, which is the primary connection between 1 and 6. Now what might happen is that the link between 2 and 3 might snap due to some reason. We have already got a backup route, which is calculated going through 1, 4, 5 and 6. So we will channel our communication through the backup link. Please note that this backup or secondary path from the source to the destination does not share any link with the primary path. That is the requirement: if any link in the primary path fails, assuming there is only one failure, which means that the backup link is all intact and you can switch to the backup path for this particular communication.  (Refer slide time:  13:06-13:16)

Now path protection: there are various ways to protect a path, that is, various ways of provisioning the extra bandwidth capacity in the network so that you can give protection. You need to build in some kind of redundancy in the  network. So there are various ways you can build in the redundancy and the schemes may be divided as dedicated path protection, (Refer slide time: 13:40-13:50)  

or shared path protection. Dedicated path protection may be shown as 1+1 protection and shared path is 1:1 protection or 1:N protection. So we will look at this one by one. (Refer slide time:  13:51-15:31)

This is an example of 1+1 protection. So you have the source on the left, and then, you have destination on the right. So it is communicating. What you can see is that from the source, the signal is coming to the splitter. If you remember, the splitter is going to split it into two halves, may be of equal power or something, and then the same signal is been carried through two different links and over there, we a switch. The switch determines which signal is better and may be switches to that and a communication is going on. If that particular link, say, this   top one fails, it automatically switches to the other protection link. So this is some kind of hot redundancy we have. That means there is redundant source of information in the destination side; so if the primary one fails, the secondary one is already on. So protection would be very fast. The only trouble is that for each such path, you will have to give an alternate path, which is also being used at the same time. This is a dedicate path protection ? for this particular path there is a dedicated alternative path; although this is very good, it is costly. (Refer slide time: 15:32-16:57)

Now we come to shared mode protection; in the shared mode, the figure looks almost the same, expect if you note, this splitter on the left has been replaced by a switch. So what we have is we have a working fiber and we have a protection fiber. Unlike the 1+1 protection, this is not a hot standby. This is cold in the sense that the protection fiber, to start with, is not carrying any signal, let us say. The source is just simply passing through the switch, the signal from the source is passing through the switch, then down the path through the destination switch to the final destination. If the working fiber goes down, then this switch will flip and the protection fiber would be in place. Since this is not a hot standby, meaning that since it is not carrying any signal under normal circumstances, you could share this path with something else. You can use this to send some other channel or some other information, etc. So that is why this is a shared mode of protection. (Refer slide time: 16:58-19:35)

Generalizing this, we get this 1:N protection, where N line is sharing 1 protection line. We have the inputs from 1 to N lines, so these are the N sources, and let us say so many destinations. So they may be going ? 1- to 2- and N to N- and so on; this is the normal mode of operation. This part of the network is for normal mode of operation and each of them is connected to a switch over here. Another link from the switch comes to one bigger switch, so this is an N input port over here and then there is a single link from this switch to this switch, on the destination side, which again feeds to all the switches. What would happen is that in case there is a failure in any one, let us say, ith link from 1,2 to N, out of that, the ith source to ith destination, which is going through the ith link and the ith link fails, what this ith switch could do is that the ith switch could switch the signal from the ith source to this particular switch at the bottom and now the protection fiber would be carrying the signal that was flowing down ith channel over there and  then again it will feed it to the ith switch on that side. Of course these two switches have to communicate that the ith one has failed over here, so you switch it to your ith mode, or this switch may sense it that this line has gone down, so it will take signal from this line. This one protection fiber is been shared by these N working fibers, and as I mentioned earlier, when everything is fine this protection fiber could be carrying some low priority data. When everything is fine these N working fibers are actually carrying the most of the important traffic so some low priority data could be flowing down the protection fiber. As soon as there is any failure anywhere, the low priority data would be stopped or it will be dropped and then this protection fiber would switch to give the services between the nodes that have experienced a link failure. Just as we have a path protection that means for a path you try to give an alternate path, similarly you could do it at the link level also. In general, it may be more efficient or more proper to do that because if you have N nodes and if it is slightly large, potentially you have very large number of paths through the networks and for each path having an alternate path may not always be a very good idea. So we concentrate at the link level and for each link level we may give a protection. (Refer slide time: 20:21-20:43)

So use an alternate path if the link has failed. This is the primary where the link may have failed so I have an alternate path to that from 2 to 3, 2 to 4, 4 to 5, 5 to 3. This alternative, please note, is for the link from 2 to 3. We will have another diagram for that. (Refer slide time: 20:44-21:09)


Dedicated link protection is not always practical although sometimes we may have it; shared link protection is practical and it is quite often implemented. This link protection may fail because here you are only provisioning for the failure of a link, but if a node fails, then it may lead to some complication as we will see. (Refer slide time: 21:10-21:28)

To compare between path switching and line switching, path switching is a coarser scheme and line switching is finer scheme; and line switching can again be a span protection. Span may be several links together; that may be span or a line protection.
(Refer slide time: 21:29-21:57)

In mesh networks of course restoration is possible only if the graph is two edge connected i.e., biconnected, which means that there are two edge disjoint paths between any pair of nodes so that no single edge failure can disconnect the network. This is a necessity and we usually try to keep it that way unless its very difficult or very cost it?s not cost effective, etc. (Refer slide time: 21:58-22:42)

Protection in a mesh network is more complicated then a ring. Simple minded scheme would be two edge or node disjoint paths for each connection, 1+1. As is mentioned, it is not very efficient. There may be many paths and provisioning double the number of paths, which are pair wise mutually node or edge disjointed may be very difficult. That may be a lot of extra provisioning in the network. A better approach would be line protection, which of course has the problem of coordination. I will show that and protection cycles in mesh net later on. (Refer slide time: 22:43-23:24)

In the path layer and mesh protection, there is protection of mesh networks to protect the mesh at the single unit. Pre-computed routes means all possible routes and alternative routes are pre-computed. 1+1 protection is protection route per light path, protection route per failure. We will discuss this later but, as I said, this is a costly alternative. Or what we might do is that we can do on-the-fly route computation; that means it is not pre-computed. As there is a failure, centralized route computation and coordination route computation and coordination are done at end nodes or distributed route computations ? all these are possibilities. (Refer slide time: 23:25-24:03)

This is an example of mesh network, where let us say this is the primary path and this is an alternate path. This alternate path may be pre-computed or it may be computed on the fly when there is a failure. Similarly, from here to here this may be the primary path and this is an alternate path. Please note that this communication as well as this communication are going through the same fiber. Maybe they are going through different wavelengths or maybe they are combined together; so there are various way of handling this. (Refer slide time: 24:04-24:51)

Let us look at some diagrams: this is a mesh network. Naturally once again, it has the same 6-node network and this is the normal operation, that is, communication from this node, node 1 to node 6. Now there is link failure over here; what you might do is you might switch the entire path like this. So this was the pre-computed path for this path. You switch to that ? this is one possibility. Or this particular span, the span could be live from here to here or from here to here. That may have an alternative path, to which you switch. (Refer slide time: 24:52-25:50)

Or this particular line may have this alternative; that means from this node to this node. If this is the link, the alternative to this link is this section from here to here to here to here. It is trying to go through the same path, but then it takes a diversion when there is a link failure through the alternative, which is provided for this particular link. I have shown only one node coming into this but you can appreciate that this path may be very long so you do not have to re-compute the path over the entire length; rather locally, you can re-compute that for this particular link as an alternative. So the alternative may well be through some local nodes only. So this path just gets a slight diversion; that is line protection. (Refer slide time: 25:51-25:59)

We talked about protection cycles in a mesh network, now for protection cycles, what we do is that for each of the paths we try to form a cycle ? cycle of  provisioning of light paths, let us say.  The point in the cycle is that suppose we are going through some arc of the cycle and if some link in-between breaks, you can always go in the other direction. So you try to form these cycles in the mesh which you keep ready and pre-computed. So whenever there is a failure you can switch. You can see here (Refer slide time: 26:38-27:03)

there are pairs of a fibers going in both the directions and there you can form  cycles over there. One of them would be protection edge, maybe the inner one; and one of them would be the working edge. If one of them fails, the other will automatically take over; the other direction will automatically take over. (Refer slide time: 27:04-27:25)

So this is another example of a network with both working and protection fibers.  The working fibers have been shown in solid lines whereas protection fibers have been shown with dashed lines. Once again you must realize that you may not provide the same level of protection to all paths or to all parts of the network; depending on which part you consider more sensitive or more important, you may put your protection there. A mesh network may be partially protected. Some of the parts may be protected or some of the parts may not be protected. (Refer slide time: 27:51-29:53)

So some more cases: line protection in a mesh network. What we have is a unidirectional light path from node A to node D; so from node A to node D we have an unidirectional light path going may be like this through nodes B, C, and E. We are talking about this path A to B to C and then to E. Now after the link BC fails, the light path is rerouted by nodes B and C along the route A B F E C E D. The unidirectional light path was going from A B C E and then to D; so this was the path A B C E D. Now BC has failed; so A B F E C E D. You can see what has happened is that there was no point in coming from E to C and then back from C to E. What has happened was that we were doing line protection; that means, for BC my protection was from B to F to E to C because I wanted to go from B to C so I am going from B to F to E to C and so now BC has failed. So that is what I do and from that point I continue wherever I was going and actually I was going from C to E to D, so once again from C, I go back to E and then to D. Such a thing is possible because we are taking a local decision; that means, for this particular link what to do in the case of failure that has already been pre- computed? We are not taking a global picture; here I have shown you a very small graph so you can immediately see the entire global picture with your own eyes. But for local nodes we may not have the global pictures where this node is coming from and where this path is finally going to. So all the intermediate nodes may not have the global pictures because if all of them had the global pictures maybe they could have computed a better path but this is for line protection.  (Refer slide time: 30:41-31:57)

Now line protection in mesh network: here what might happen is that erroneous connection due to the failure of a node is being treated by its adjacent nodes as link failure. This is one case of the so-called race condition. What might happen is that node 1 has failed; so what would 6 do is that it may assume that the link from 6 to 1 has failed, whereas 2 may assume that the link from 2 to 1 has failed. So both of them perceive as two different failures, so they take some local decisions, and it might lead to a funny situation where you are going in a cycle. Maybe other kinds of things may happen after node 1 fails ? node 5 gets connected to node 4 after node 6 and 2 invoke line protection independently. If they perceive the same failure, the actual failure was that of node 1; but if they perceived differently as failure of  link 6 1 and failure of link 2 1 and they have independent actions, it may lead to race conditions. (Refer slide time: 31:58-32:14)

The advantages and disadvantages of protection: we will also be talking a little bit of restoration. Protection is simple; it?s quick; does not require much extra process time ? and this is the important part, since this is quick. As I mentioned earlier, there will not be a lot of data loss. For example, a SONET ring would sort of come back up from a failure in a less than 15 milliseconds. So that is a benchmark; you are back in action in less than 15 milliseconds, so whatever little you might have lost during that time would be taken care of by the higher layer protocols. (Refer slide time: 32:51-33:08)

But usually they can only recover from single link faults. If there are multiple link faults, all kinds of funny things may happen. There is inefficient usage of resource, because protection needs a lot of resources, even if we are sharing them.
(Refer slide time: 33:08-33:19)

Dedicated protection needs even more resources; we talk about path restoration and link restoration.
(Refer slide time: 33:20-33:30)

What we do is that we compute the path after the failure and the resource is reserved and then used. So in restoration what you try to do that is that you try to look at the current actual situation. You try to have some protocol for keeping track of the present situation and then you compute some root and then you reserve the path and then restore the original service. This has got some software parts, so some data has to be processed, etc. This usually takes more time, so this has a hardware as well as software aspect to do it .(Refer slide time: 34:00-34:25)


The path is discovered at the end nodes of the failed link; but this is more practical than path restoration. We have both path restoration and link restoration. Path restoration means the path may be long and to find out an alternative, it may be more difficult, whereas links are between the adjacent nodes so they may quickly find an alternative. (Refer slide time: 34:26-34:53)

Advantages and disadvantages of restoration are the following: usually it can recover from multiplex element faults because you are sort of having some protocol to exchange information and then find the current situation and form the alternatives, etc. There is more efficient usage of resources; it is more complex; it is slower; it requires extra process time to set up path or reserve resources. (Refer slide time: 34:54-35:19)

So for comparison between protection and restoration, in protection the resources are reserved before the failure; they may not be used. In restoration the resources are reserved and used after the failure. So this is the main difference between the two. Route: in protection it is predetermined, in restoration it can be dynamically computed. Resource efficiency: in protection it is naturally low and the restoration is comparatively high. (Refer slide time: 35:20-35:55)

Time used for protection is short; for restoration it is longer. Reliability: protection is mainly for single fault, whereas restoration can survive under multiple faults. Well, it is not that it can always survive under multiple faults; it depends on the where the faults are, but if it is survivable after multiple faults, it will take global view and say that ok, still I can give the services. So restoration will take care of that, whereas in protection it may not be possible to handle multiple faults. Implementation: protection is simple and restoration is naturally more complex. (Refer slide time: 35:56-36:21)

For optical networks, we not only talk about physical links we talk about virtual wavelength paths. So in light paths also routing can be centrally controlled or distributed; resource reservation, forward reservation, as well as backward reservation are done as we do in optical networks. We will talk more about this later. (Refer slide time: 36:22-37:15) 

Now let us come to this all-important topic of fault management in ring networks. As I said, ring networks are very ubiquitous in WAN. All the telecom people will love these things because naturally it allows them to give very high level of service. So we have so many rings; these SONET rings are very common, and  rings are a very common kind of topology. One of the chief attractions of the ring topology is its capability to allow some kind of protection and restoration with some redundancy in-built, as we will see. We will look at two different cases, unidirectional path switch rings and bidirectional line switch rings or UPSR and BLSR. (Refer slide time: 37:16-39:22) 

This is a diagram of UPSR. We have AB, means A to B and BA, means B to A. First of all you see that we have these two rings ? we have this working fiber as well as a protection fiber. Working fiber is going in only one direction and protection fiber is going in the other direction, in the counter clockwise direction. So for connection from A to B and from B to A ? this is A, and this is B ? my working fiber is going in this direction. A to B is really going in this direction; B to A is coming in this direction. So A to B, B to A: this is the protection fiber part, A to B the working fiber is going in the other direction. A to B is going like this and B to A is coming all the way like this. Let us say the outer one is a working fiber, it is through the outer ring. For A to B, this is the path; and B to A, this is the path, whereas for protection purpose ? and please note that this is a 1+1 scheme; that means for this the alternative is already provisioned and maybe it is something like a hot standby. So A to B is through the protection fiber and now this path from A to B is going via D and C, so A D C B: that is A to B and B to A. The protection path is from this through the protection fiber. So if there is a failure anywhere, this can still continue, so this is a unidirectional path switched ring. (Refer slide time: 39:23-40:29)

There are some limitations of UPSR. It does not spatially reuse the fiber capacity; so what is happening is that since this is unidirectional, what is happening is that even if two nodes are side by side, if they are in wrong direction then it has to come all the way through and naturally all those links ? even if we are talking about particular ?s I mean WDM systems ? in the ring are entirely sort of covered by this. Otherwise even if we are talking about a WDM system what is happening is that at least one ? around the whole ring is getting occupied because two side by side things want to communicate. So it does not spatially reuse fiber capacity and if there is some, you could use it for some other purpose. That is not possible in a unidirectional ring. Each bidirectional SONET SDH connection uses up capacity on every link. That is the thing; if you look at the previous picture (Refer slide time: 40:30-40:44)


we are just talking about a connection from A to B and B to A; that?s all, so this entire ring now has been used up assuming only one wavelength. The entire ring now has been used up; these ADMs of course are the ADMs of SONET. (Refer slide time: 40:45-40:50)


So it is efficient for lower-speed access networks, one to multipoint only. (Refer slide time: 40:51-41:27)

And the other point, which may become a problem, is that delays are different for the two paths because one of them is small and the other one is quite large, all the way around the link. The relays in the two paths are quite different. A remedy could be bidirectional lines; that means, bidirectional line switched rings or BLSR. So we were in UPSR, now we will be talking about BLSR, bidirectional line switch rings. This provides spatial reuse capabilities and additional protection mechanisms and adopts span as well as line protection. We will have a look at both of these. (Refer slide time: 41:28-42:26)

This is  a four-fiber bidirectional line switched ring; so once again we have two working fibers, say, the outer two ones and two protection fibers or the inner ones. So we actually have four fibers. This is a four-fiber system and it is going, so we are using two such working fibers. One of them is going in one direction the other is going in the other direction, so this is bidirectional. We can communicate both ways, so if A to B and B to A both can communicate on the working fiber in this part at all, the rest of the space can be used for other communication. So spatial reuse is much better for bidirectional line switched rings, and the protection fibers are there because if one of them fails then the protection fiber may take up. (Refer slide time: 42:27-43:10)

This is a span protection so we are talking about bidirectional traffic supports, maybe 16 nodes or some distances, etc., from A to B. You may pre-compute a span that if this span goes down what is the alternative span. If this is the case of line protection, if this line fails what is the alternative line? Now the alternative line of course may go in the other direction, depending on what the failure is. So there are many different possibilities with this BLSR. (Refer slide time: 43:11-44:24)

There is also two-fiber version of BLSR, namely BLSR/2. Here both the fibers are working as well as protection fibers. That means if one of them fails the other one will give the protection in the other direction of the ring. If you note, the rings are going in the opposite directions and there is a line failure in both of these working as well as protection fiber. That means A to B is communicating in this direction, B to A is communicating in this direction. But if one of them fails, the other one will give the protection by going in the other direction. In that case we have to go all the way round, then you will have to reserve the resource and if there is something already going on here, you may be blocked or this may have to be dropped and so on. So naturally you have two fibers, we have less flexibility. But with two fibers we can get this bidirectional thing going on. (Refer slide time: 44:25-46:19)

Comparison of different types: these are all self healing rings. What we mean by self healing is that the nodes, these ADMs and the SONET, etc., are programmed in such a fashion that as soon as they sense a failure they know what local action to take and how to adjust the switch internally, so that it automatically switches from the working fiber or from the working span or working fiber or whatever through the protection side. That is why these are self healing rings, so they heal automatically. And as I said these SONET rings really do this in less than 15 milliseconds; the entire thing will again be up. So we can compare the three: UPSR, BLSR/4, and BLSR /2. For example, fiber pairs 1, 2 and  transmission receiver pair 2 4 and 2 spatial reuse in UPSR it is none, in BLSR it is there, and in BLSR/4 it is there. Production capacity is equal to the working capacity. Link failure is path protection in the case of UPSR or span or line protection in the case of BLSR/4. In case of BLSR/2, it is only line protection because it will have to go in the other direction. Node failure: it is path protection, line protection and line protection restoration is faster in UPSR; somewhat slower in BLSRs and restoration speed is this. Node complexity is low, high and high. Another thing we talked about is the dual homing. (Refer slide time: 46:21-47:50)

By dual homing we mean that suppose we want to deploy your network in such a way that it is very mission critical and no failure is acceptable and we want a hot standby. So what you might to do is that you may get connected through two different hubs and what you want to do is that you want dual home to these two hubs and these two hubs take independent paths to your destination. This shows a dual homing to handle hub on node failure; so we have these four ADMs: once again four nodes, let?s say A B C D. So the end node is A and these are the two hubs, B and C. What we want is that not only some link failure but even if one of the hubs fails, I should still be able to communicate. So what you do is on this ring you communicate with hub 1, let us say you are communicating with hub 1 through this A D C B and you are communicating with hub 2 through A B C. As we will note, even if one of the hubs fails, you can still communicate through other one. So this is another kind of  protection. (Refer slide time: 47:51-48:14)

And finally I have mentioned this point before ? just a reminder that a network consists of many layers and each layer may have its own protection mechanism built in, independent of other layers. So there are both advantages and disadvantages to this. We have already talked about the advantages. (Refer slide time: 48:15-49:51)

This is an example of a WDM link carrying SONET traffic. So there is a WDM link, so there is a SONET ADM. This is a working fiber pair and protection fiber pair. Please note that the pair has been shown as one line over here because usually as you know that fiber optic line is a simplex line; that means it goes in only one direction; there is a source in one side and the detector on the other side. Usually these fibers always come in pairs. The other side is for the communication in other direction so we have a working fiber pair over there and protection fiber pair through this WDM link. Look at this 1:2 protected scheme. What is happening is that there is one protection fiber pair, there may be protection fiber pair and through the working fiber there may be multiple virtual links going through the working fiber using different wavelengths. A is a normal operation and B link is cut and the traffic is restored by the optical layer. That means you automatically assign new wavelengths and new paths through the fibers etc., to bring it up. So this you may do at the optical layer rather than at the electronic layer. (Refer slide time: 49:52-51:58)

But the case we are talking about here is that the SONET is riding on the optical layer. So we have the optical layer at the bottom then you have a SONET on top of it. On top of SONET also there will be the data link layer and then so on all the other links. What we are saying is that each layer may have its own protection mechanism. So for example I mentioned that the SONET will also have a protection mechanism of its own. So if a SONET LTE, i.e., the SONET line terminal equipment, senses that it cannot communicate to the next LTE, this will automatically reroute the traffic and try to reroute the traffic in the other direction.  Sometimes it is good to have multiple protections at multiple layers but what might also happen is that they might sort of cancel each other or they might go into a race condition. So these are the disadvantages of having protection at various layers. There could be some disadvantage also if they are not very well coordinated, which usually would not be because these layers are sort of independent of each other. They talk only to their peers and go through their own protocol and give protection. Apart from these of course the other disadvantage is that you may redo some part of the protection unnecessarily. You may be unnecessarily duplicating the work at various places. So these are the disadvantages of having protection at multiple layers. (Refer slide time: 51:59-52:34)   

What is the advantage of optical layer protection ? speed and efficiency. Limitation would be detection of all faults may not be possible; protects traffic in units of light paths. So this is another problem. As I mentioned, in light path the granularity is very coarse. It may be 2.5 Gbps; so you are really giving the protection at a level of granularity, which may be quite high as I just now  mentioned; it could lead to race conditions when optical and client layers both try to protect against the same failure. (Refer slide time: 52:35-53:28)   

Of course, on the optical layer you have one more dimension to play with, which is in the case of a WDM. That means you have different wavelengths, so instead of 1+1 link, you can talk about 1+1 wavelength path selection. You can sort of try to select two independent light paths and the signal is bridged on both protection and working fibers if you are doing 1+1 protection kind of thing; the receiver chooses the better signal. In case of a failure, the destination switches to the operational link that is operational light path; there is revertive or non revertive switching; that means, if the original link comes back, it may revert or it may not revert; and no signaling is required. (Refer slide time: 53:29-53:30)

So that is the unidirectional light path. I have just shown you some of the schemes which are used for protection and restoration. And as I mentioned, the schemes may be partially deployed and some of the parts may be protected and some of the parts may not be protected  and so on. But the essential idea is the same ? that you build in some kind of redundancy and the redundancy may be in the form of entire fibers or the redundancy may be in the form of light paths or wavelengths. It may be pre-computed and pre-reserved like 1+1; it may be pre-computed but not pre-reserved like in a shared one or it may be computed as and when the failure occurs; that is, in the case of restoration. So there are various approaches to it. Depending on how critical the problem is or how critical the application is, what is the cost and how much extra provisioning you can do, you can choose your own way of protection and restoration. Thank you. 

Preview of the next lecture
Lecture ? 13
Multiple Access
Good day so today we will talk about multiple access ok now what is multiple access  (Refer slide time: 54:59-57:54)



If you remember  that we had  seven layer in the so called OSI stack the top on being application then we have presentation session transport network link and physical ok  we had been mostly talking about the physical layer till now although  for optical networks we sort of ventured into some of the hired layers  but from this lecture onwards we want  to concentrate on  the link layer ok now  what medium access does is it coordinates  competing  request request for what for medium that means that there is a medium  which may be an object of contention meaning that I mean several nodes may want to use it and  this medium access control protocol has to do with how to handle that so sharing of link and transport of data over the link that is an general the description of what the data link layer does  so when we share a link there is a question of committing request and we have to have some way of reserving that  and of course there is a also question of transport of data over a link  link if you remember when we say a link I mean that two nodes which are connected  the nodes may be or computers routers switches etc they are two networking nodes they are directly connected now the when  I do like this  it might mean   cable copper cable or a  fiber  or it might mean a shared medium like the free space ok  so there is a but there is some way of communicating directly between these two nodes that is what we mean by link that means that is just one hop in the network that is what we are talking about in the data link layer so there is a question of reliable transfer of data over these link and if this link like when you have a free space transmission  with so many nodes in the network now so many people would like to transmit so there is a question of sharing this medium and there is a question of who would access it when and just as I said free space could be a shared medium similarly  if you remember that if you have some kind of a bus  if you remember our discussion about topology of networks when we have some kind of a bus  from which the number of nodes are hanging and  that bus  may also be an object of contention so that bus is the medium through which communication is taking place and there is an  there is some kind of  competition or sharing between the of this shared medium between the nodes so we have to handle that that is the other thing  so examples of contention based or ALOHA and slotted ALOHA (Refer slide time: 57:57-59:00)



 these refers to some protocols which we are used in satellite communication so we will discuss these and we talk about satellite communication we have CSMA  it stands for  carrier sense multiple access or CSMA CD which is carrier sense multiple access with collision detection there are other variance of these like carrier science multiple access with collision avoidance  and things like that  so these ALOHA slotted ALOHA for satellite CSMA CSMA CD or specific specifically CSMA CD is used by Ethernet  in many situations and then we have   CSMA CA may be for cellular communication etc so these are contention based MAC and  Round Robin there are these are  token based protocols and so two very common ones are token bus and token ring  so actually we will discuss these  in the next in this lecture as well as the next we will be discussing these 



COMPUTER NETWORKS
Prof. Sujoy Ghosh
Dept of Computer Science and Engineering  
IIT Kharagpur
Lecture Name #13
Multiple Access
(Reference Time: 00:45) 
Good day. Today we will talk about multiple access. What is multiple access? (Refer slide time: 00:55 - 00:55) 

(Refer slide time: 00:56 - 02:21)

If you remember, we had talked about 7 layers in the OSI stack, the top one being application, then we have presentation, session, transport, network, link and physical.   We had been mostly talking about the physical layer till now, although for optical networks, we sort of ventured into some of the higher layers. But from this lecture onwards, we want to concentrate on the link layer. What medium access does is, it coordinates competing requests for medium; that means, there is a medium, which may be an object of contention, meaning that several nodes may want to use it and  this medium access control protocol has to do with how to handle that. Sharing of link and transport of data over the link is in general the description of what the data link layer does. When we share a link, there is a question of competing requests and we have to have some way of resolving that, and there is also a question of transport of data over a link. A link if you remember is two nodes, which are connected. The nodes may be computers or routers, switches, etc., the two networking nodes are directly connected; it might mean a cable, a copper cable, or a fiber, or it might mean a shared medium like the free space. There is some way of communicating directly between these two nodes; that is what we mean by link. That means it is just one hop in the network. That is what we are talking about in the data link layer. There is a question of reliable transfer of data over this link and if when you have a free space transmission with so many nodes in the network, many people would like to transmit. So there is a question of a sharing this medium and there is a question of who would access it when. Just as a shared free space could be a shared medium, Similarly, if you have some kind of a bus from which a number of nodes are hanging, that bus  may also be an object of contention. Bus is the medium, through which the communication is taking place and there is some kind of competition of sharing between this shared medium between the nodes. So we have to handle; that is the other thing.  So actually this (Refer slide time: 03:52- 04:14)


link layer may be divided into two sub layers, so to say: the upper one is the logical link control and the lower one is the medium access control. We will talk about logical link control later. We start with medium access control; there are a number of medium access control protocols and techniques. We will discuss a number of them. (Refer slide time: 4:15-5:00)



The situation we have is computers in a shared network environment and although the medium is shared, it is taken that only one computer can transmit at a time. If two computers try to use the same line at the same time for transmission, i.e., when one is transmitting one or more may receive. Receiving is not a problem; but when two  computers want to transmit at the same time, their messages get garbled. We say that there is a collision. How can we organize the transmission so that all computers are given an opportunity to exchange messages? If you remember, when we were talking about multiplexing, that is also in some sense sharing a medium. So we had this time division multiplexing and frequency division multiplexing; actually we also have time division multiple axis and frequency division multiple axis, etc., over here. The difference between the two cases is that in the multiplexing case, the lines are all coming together into the multiplexer, whereas in multiple access, the points are all geographically distributed and they may not know about each other. There may not be much of a coordination; in some schemes we do have some coordination in some schemes we do not have any coordination. So all these cases we will see. There may be (Refer slide time: 06:03 - 06:26)











point-to-point links in a network like PPP for dial-up access, etc., which is quite common ? point-to-point link between Ethernet switch and a computer. So these are examples of point-to-point links; we will come to this later on. Then there are broadcasts, like a traditional Ethernet, upstream in an HFC. HFC is a hybrid fiber coaxial; some part it is fiber and some part of it is coaxial, and the upstream traffic there  also uses that medium as a shared medium. Then we have 802.11, which is a wireless standard. We will have a separate lecture on this wireless networking, etc. 802.11 is the number of the standard, which defines some form of a wireless networking. Wireless is a shared medium because it is sharing our common shared   electromagnetic field that we already mentioned. There may be a single shared broadcast channel, (Refer slide time: 7:14 - 7:44)


 
there may be many also, or more than one also. Two or more simultaneous transmissions by nodes will mean interference. Only one node can send successfully at a time. Multiple access protocol is a distributed algorithm; so this is where it differs from multiplexing. It is a distributed algorithm that determines how nodes share channels; that is determined when a node can transmit. Communication about channel sharing must use the channel itself; that means, we have what we earlier called in-band control usually. That means if you have a, let us say, wireless network, if you are running any kind of protocol for control purpose, etc., that would also be using the same medium. The same question of how to coordinate that with all the other competing nodes ? which are trying to transmit data ? all that comes into play. (Refer slide time: 8:15 - 9:03) 	 	



There are centralized approaches to a MAC, i.e., medium access control. The centralized approach is simpler in the sense that there is a controller, which grants access to the medium. It is simple, has greater control, priorities, quality of service, etc. ? all this can be implemented very easily. For example, if you want to give higher priority to one particular node the controller simply just takes note of the priority when allocating the medium to a particular node. But there are problems with centralized approach also. One is that a single point of failure is a big problem. When we have a centralized system and if that system fails, then the whole network is down, which may not be a good idea in many many situations. Secondly, there will be a performance bottleneck; meaning, first of all everybody is communicating to it but when the network becomes bigger, the demand on its computational capacity, its control mechanism, etc., goes up proportionately and that is also another problem in centralized approaches. In decentralized schemes (Refer slide time: 09:46 - 09:57)


all stations collectively run MAC to decide when to transmit. So in a decentralized scheme we are running some kind of a distributed algorithm. (Refer slide time: 09:58 - 10:55 )



There are various kinds of MAC protocols: one is called something like a round robin MAC. In round robin there are a number of stations and each station takes its turn in a round robin fashion. There are some protocols based on round robin MAC. So here, each station is allowed to transmit; a station may decline or transmit. That means, if it has nothing to transmit, it may decline. Otherwise, it will transmit. It could be centralized; for example, polling, or distributed; for example, token ring.  Control may be centralized or distributed of who is next to transmit. When done, the station relinquishes and the right to transmit goes to the next station. This is efficient when many stations have data to transmit over an extended period. So that may be a good  scheme. (Refer slide time:10:56-11:18)



There are scheduled access MACs, like time is divided into slots just like our time division multiplexing; station reserve slots in the future; multiple slots for extended  transmissions and suited to stream traffic. There is a question of reservation of some slots. That would be a part of the protocol about how reservation will be done, who can reserve, and if two people want to reserve simultaneously then what happens. So we have to handle issues like that. This is the scheduled access MAC, when it is scheduled, some reservation is done beforehand. Then there are some contention based MACs. (Refer slide time: 11:41 - 12:14) 



Contention means there is hardly any control; there is no control. Stations simply try to grab the medium. This is distributed in nature and surprisingly, for no control, it performs quite well for a bursty traffic, but it can get very inefficient under heavy load. Actually, the round robin MAC that I mentioned before, and the contention based MAC are very common. The most common example is the Ethernet, which we will see especially later on; how it is done, or how satellite communications use contention based MAC. That means nodes simply track and try to grab the control but over a long period of time, the amount of data or the number of times the nodes on an average try to communicate and the transmission time ? these two factors together define the load on the system. If the load is light then this contention based protocols very surprisingly are much simpler and may be cheaper to implement. So examples of contention based are ALOHA and slotted ALOHA. (Refer slide time: 13:06 - 14:08)



These refer to some protocols which we use in satellite communication; so we will discuss these when we talk about satellite communication. We have CSMA ? it stands for carrier sense multiple access or CSMA CD, which is carrier sense multiple access with collision detection. There are other variants of this, like carrier sense multiple access with collision avoidance and things like that. ALOHA or slotted ALOHA are for satellite, CSMA, CSMA CD, specifically CSMA CD, is used by Ethernet in many situations; and then we have a CSMA CA, may be for cellular communication. These contention based MACs. Round Robin, are token based protocols and two very common ones are token bus and token ring. Actually, we will discuss these in this lecture as well as in the next. (Refer slide time: 14:09-15:38)


We can sort of summarize; first of all we can divide the networks into two types, depending on their topologies ? one is the bus type. Bus means some communication channel and everybody is connected to the same communication channel. It may be a coaxial cable or some fibers or it could be a ring. Then we have this token bus, which is one particular MAC protocol. It has got IEEE standard number 802.4 or polling in 802.11. We will come to this later on. These are examples of bus type topologies using Round Robin techniques. Token ring is a ring type topology and 802.5 is the  standard number. FDDI is a Round Robin,  but the topology is the ring. There are scheduled approaches to medium access like DQ DB or distributed queue dual bus  802.6.  We will be talking about this also. Contention based are CSMA  CD, CSMA CA in 802.4 and 802.11. The first one is Ethernet and the second one is a wireless network; so we will look at these in later lectures.  (Refer slide time: 15:39 - 15:49)


 What is the ideal of a multiple access protocol? Suppose we have a broadcast channel of rate R bps. Well, any broadcast channel would have some maximum limit or the rate you can communicate over; that depends on the characteristics of the medium. If their medium is different, this rate R may be different; for example, the rate for a fiber optical cable would be much higher than the rate for a coaxial cable. But the point is that there is always an upper limit due to various reasons; we need not go to that in this lecture. Due to various reasons of how we communicate, etc.,  there is a maximum rate R, which you can achieve on a particular medium using, let us say,  the technology, which is present now. When one node (Refer slide time: 16:43 -17:52)



wants to transmit, it can send at rate R. There is no problem; it can send all the way up to rate R if the technology permits. When M nodes want to transmit, this R is not going to change. So what you want to do is that you want to divide up this rate R and distribute it among these M nodes, which want to communicate. So each can send at an average rate R/M. If that happens, naturally we have an ideal multiple access protocol; we can not do better than that. But it is difficult to  achieve this theoretical rate because there will be some overhead for the protocol itself, which will eat up something. None of the protocols is 100 % efficient.  Secondly,  we would ideally like it to be fully decentralized. So there is no special node to coordinate transmissions, no synchronization of clocks and slots. Lastly, it will be simple. Naturally if it is decentralized, no synchronization, etc., that is one aspect of simplicity then the protocol itself will be simple. This is the other thing; and we want a fully decentralized scheme, I mean we preferred them because there would be no failure of single nodes, etc., and then there is no bottleneck. These problems should be avoided if you have a fully distributed system. In multiple access (Refer slide time: 18:18 - 18:57)



as I mentioned, many users are sharing a resource at the same time; it is needed because users must share the cells. First let us talk about three different multiple accesses: frequency division multiple access, time division multiple access and code division multiple access. Frequency division multiple access and time division multiple access are very simple. We have already talked about multiplexing; only thing is that the connotation is slightly changed over here. Code division multiple access is somewhat different; so we will discuss it in some more detail. So they use the same frequency, same time, but different codes; we will come to that. (Refer slide time: 18:58 - 19:16)



Let us start with FDMA. FDMA is frequency division multiple access. Just like frequency division multiplexing, channel spectrum is divided into frequency band. Each station has an assigned fixed frequency band. Unused transmission time in frequency bands goes idle. This is the simplest possible scheme. It is also not very efficient because many of the stations will be idle and that frequency band for that particular time is sitting idle; that part of the bandwidth is wasted. Also, we know that in a data networks the net traffic is very bursty; that means people want to transmit for a very short amount of time. Suppose the transmission time is T; then may be for 500 ? T amount of time, it just sits idle. It is very bursty; so a frequency division multiple access would be very inefficient. But this is the oldest; you know, radio stations are doing multiplexing since the communication is only one way.  But  if you are using some other kind of say radio frequency transmission using fixed channels, when there is both way of communication, then it is some kind of multiple access also. So suppose we have six stations LAN 1,3 (Refer slide time: 20:26 -  20:34)




and 4 have some packets to send. So they are sending;  frequency bands 2,5,6 are idle. This happens quite often an FDMA. (Refer slide time: 20:35 - 21:26) 



This is another picture; we have tried to show with colour the graphs of the different channels. The channels are differently colored (these bars), and we have the three   accesses ? f is the  frequency,  c are the channels and t is the time. There is only one channel at all times that uses the particular frequency; another channel for all times uses another frequency, and so on. Another frequency means it is not a single frequency; this is a frequency band kind of thing. You always require a band; you cannot communicate anything with a single frequency that will be just a single tone. (Refer slide time: 21:27 - 21:56)



Frequency division multiple access for each channel gets a band  (range) of frequencies used in traditional radio, TV, first generation cellular, etc. Advantage is that there is no dynamic coordination; it is absolutely distributed. Disadvantage is that it is inflexible and inefficient if channel load is dynamic and uneven. By today?s standards, it is quite high, but at one point of time it was a quite good.  (Refer slide time: 21:57 -22:10)


We will see another version of this FDMA, which is WDMA. WDMA means wavelength division multiple access; and as you can suspect, now we are talking about fibers. We just have a frequency division multiplexing and wavelength division multiplexing. They were actually the same thing; the only thing is that for fiber we choose to call it wavelength division multiplexing. Here also, we are talking about wavelength division multiple access. Previously all our fiber links that we talked about were point to point. Suppose the fiber link somehow is used for multiple access,  two stations in a very dynamic fashion, we want to give this bandwidth. A number of stations can connect to the same fiber through some splitter, coupler, etc. So we are not going into that. Suppose somehow they are hooked into this fiber and we want to use wavelength division multiple access. Let us just look at the scheme. (Refer Slide time: 23:03 - 23:56) 



Each station is assigned two channels; a narrow channel for control and a wide channel for sending data frames. The channels could also be of the same size, does not matter, but then for control we do not require so much bandwidth. For a particular node, these two are assigned. A node?s control channel is used by other stations to contact that node. So its control channel is fixed. Some other station, if it wants to communicate with A, will first talk to A on its control channel. Both channels are divided into n + 1 slots, which repeat endlessly. Slot 0 is especially marked to synchronize the nodes.  In each channel, there are, say, n + 1 slots and then a number of nodes; let us say, three nodes, B, C and D can communicate with A at the same time. Because of communicating to A it has to send to A in the channel or the wave length which has been assigned to A.  But how can all three send at the same time? If there are slots in that channel and if they have booked separate slots, in different slots their data can go. So B, C, and D simultaneously communicate with A. (Refer slide time: 24:33 - 25:42) 
 	 


Each station has two transmitters and two receivers. First it has to have a fixed wavelength receiver for listening to its own control channel. Its control channel is fixed, so it requires a fixed wavelength receiver. Then it wants to communicate to somebody else?s control channel. For that it requires another transmitter and that has to be tunable because A may want talk to B at one point of time and then talk to C at some other point of time B and C?s control channels are different. So we sort of have to have a tunable transmitter. We have a fixed wavelength transmitter for outputting data frames. Suppose it wants talk to B it can talk to B?s control channel, when A wants to communicate over the wavelength assigned to it. Then it can output the data frame at a fixed wavelength. (Refer slide time: 25:43 - 26:09)


 
It requires the tunable receiver for selecting a data transmitter to listen to. A is talking to B, so for talking to or listening to B it requires a tunable transmitter because it may want to listen to B now and later on, it may want to listen to C. The data channels for each station contains a special slot where the status of both the channels, which slots are free, etc., are reported. (Refer slide time: 26:10 -26:36) 

When station A wants to set up a connection oriented channel to station B, it tunes to B?s data channel and waits for the status slot. It puts in the request in any of B?s free control channel slots. B?s status slot will tell whatever control channel slots are free. So B just puts in a request over there. When B sees the request, it assigns the slot to A and announces it to others. (Refer slide time: 26:37 - 28:18)


There is a problem in a distributed system, in the sense that you always have to worry about this kind of thing. If two stations grab the same control slots simultaneously, both of them want to talk to B and both of them sort of want to grab that same control slot by some chance simultaneously. When we say simultaneously, they both are close together, and so the information will get garbled. So naturally if both of them are trying to put things in the same box the information will get garbled and both of them will notice. In that case both will back off for a random amount of time. When they do back off for a random amount of time, the random number generated by one station and the random number generated by the other station will be different. So both of them will back off for two different amounts of time, and which ever has the shorter waiting time, comes back and tries to put again on this control slot. Hopefully there will be no collision. This is one kind of collision and backing off. This kind of scheme is used elsewhere also; we will come to that. For two-way communication, B repeats the same algorithm for A for variable bit rate traffic. This is for synchronizing, so one slot can be sort of reserved for this communication between A and B. For variable bit rate traffic, slots in data channel are not booked so they can be sent  by some other mechanism. Let us not go into that at the moment.  (Refer slide time: 28:19 - 29:59)



Next, we come to time division multiple access. If you notice the figure, this looks just like time division multiplexing. We have the information or signal from this top one, high one coming and then it is sort of interspersed with the signal from this source go. They are  going in a time division multiplex fashion. Only thing to notice over here is that this line and this line they may not be actually physical wires. That is why some wireless has been put over here. So they are actually sharing the same medium. So immediately you see the problem ? what happens if this synchronizing behavior breaks down. So somehow you have to make sure that they do not break down and they do communicate at precise slots of time, which is assigned to them. Then you have the additional trouble of ensuring that the clocks synchronize. If he is running a clock of different zone these two clocks may not agree, so how to synchronize them, etc. These are the problems which you solve in a TDMA, time division multiple access. Otherwise the basic approach is the same as time division multiplexing. (Refer slide time: 30:00 - 30:15)      	                      	   


TDMA is time division multiple access. It enables access to channel in rounds. Each station gets fixed length slots, length is equal to packet transmission time in each round. The unused slots go idle; this is a simple scheme. (Refer slide time: 30:16 - 30:49)
	 

   
For example, suppose we have six stations: LAN 1,3,4 have packet and slots 2,5 and 6 are idle. This kind of frames sequence keeps on repeating. In each frame, 1 will have a slot, 2 will have a slot which is going idle, 3 will have a slot, 4 will have a slot, 5 and 6 will also have slots which are again going idle and so on. So this is a TDMA system.  (Refer slide time: 30:50 - 31:33)



Each channel gets entire spectrum for a certain time period, rotating time period. So  whenever the time is allotted to that particular channel it gets the entire spectrum to itself, so it can send the data at a very high rate. The advantage is that it can assign more time to senders with heavier loads. By doing that, it gains some kind of efficiency advantage over FDMA and another side of the same point is that you can reduce the power consumption. The disadvantage is that it requires precise synchronization, which we talked about earlier. This is a diagram for the (Refer slide time: 31:37 - 31:59)



time division multiple access. The entire frequency band is given to one particular channel for a short amount of time. Then, for the next short amount of time, another channel gets the entire frequency band and so on. As it is there in standard time division multiplexing, the same thing applies. (Refer slide time: 32:00 -32:51) 


Now we can combine TDMA and FDMA. Each channel gets a certain frequency band for a certain amount of time. An example is GSM; we will come to this may be  in slightly more detail later on. So, this is combining both TDMA and FDMA; that means, we are breaking it up not only in the time domain but also in the frequency domain. The advantage is that there is more robust against frequency selective interference because for a particular communication the frequency is sort of changing; there is much greater capacity with time compression; and there is inherent tapping protection. So it has got a lot of advantages and it is quite efficient. The disadvantage is that, once again, just as in TDMA, we have to synchronize the clocks, etc. So here, frequency changes must also be coordinated. (Refer slide time: 32:52 - 33:09)



This is the picture; basically, we have small blocks. So for this particular yellow block over here, at this particular time, this particular frequency range is given to this channel and so on. (Refer slide time: 33:10 - 33:46) 


Next we come to channel partitioning or CDMA. CDMA is code division multiple access. This may be a little interesting and new. We have a unique code assigned to each user, that is, code set partitioning. What is that code? We will come to that. It is used mostly in wireless broadcast channels like cellular satellites, etc. All users share the same frequency, but each user has his own chipping sequence, that is, code, to encode data. We will come to the details. Encoded signal is the original data; (Refer slide time: 33:54 - 34:13)


and then there is some operation with the chipping sequence. We will come to what is chipping sequence. Decoding is the inner product of encoded signal and chipping sequence; it allows multiple users to coexist and transmit simultaneously with minimal interference if codes are really orthogonal. (Refer slide time: 34:14 - 34:27) 

We have a number of stations sharing a number of channels. Each station transmits over the entire spectrum all the time, which means that it is not excluded either in a time dimension or in a frequency dimension. Each channel can  go on communicating  in the entire frequency band all the time. Now how will that happen ? because other people are also communicating in the entire frequency band all the time. All their signals will get mixed up; as a matter of fact, they do get mixed up, but the point is that the signals are encoded in such a clever fashion that from that mixed signal you can separate out all the different streams of communication that have gone into it. So multiple simultaneous transmissions (Refer slide time: 35:06 - 35:20)



are separated using coding theory. There is an assumption that the signals add linearly; if they do not, then you have to do some adjustment, etc. We will not go into that. (Refer slide time: 35:21 - 35:23)

CDMA is a form (Refer slide time: 35:29 - 36:17) 



of spread spectrum multiple access. It is spread over the entire spectrum. Instead of sending b bits per second for a particular node, we send m b chips per second. What is a chip? Each bit is encoded by m number of chips. They are sort of tiny fragments of bits and these chips may again be 0 and 1. There is a 0 1 sequence code for these bits of one particular station; another station will have another code. A 1 MHz channel with 100 stations gives 10 KHz per station. With fewer than 100 chips per bit, the effective bandwidth is higher and the channel allocation is also done at the same time; we will see how. (Refer slide time: 36:18 - 36:29)


This is the picture, which is funny because for the entire time for the entire frequency band, all the channels are using it at the same time (Refer slide time: 36:30 - 37:09) 




Now each channel has a unique code. All channels use the same spectrum at the same time but orthogonal codes. It is bandwidth efficient; its capacity also is quite good,  when I talk about simple TDMA. But you can sort of mix up FDMA and TDMA and get a good efficiency over there also. This fight between GSM and CDMA is ongoing and it will go on for some time. The disadvantage is that it has more complex signal regeneration. So this is how it is implemented. (Refer slide time: 37:10 - 37:24)



As I said, each bit time is subdivided into short intervals called chips and typically there are 64 to 128 chips per bit. That means for each bit we have a code, which is 64 bits long. The chips are again in a sequence of 1s and 0s. So we are sending a long sequence of 1s and 0s for sending may be 1. But we are sending it very fast; and we can send it very fast because we are using the entire spectrum. The entire spectrum is at our disposal; so we can send it very fast. (Refer slide time: 38:04 - 38:12) 


Each station is assigned a unique m bit code or chip sequence, which is used by the station to transmit 1; its complement is used for 0. (Refer slide time: 38:13 - 38:39) 


We will see an example: two codes, S and T, are said to be orthogonal; we have been talking about orthogonal codes. If under a certain operation we have S let me call it a  dot product kind of thing at the moment. So S dot T is equal to 0 and S dot S is equal to 1. You see that this is very similar to dot products in vector. If you have two orthogonal vectors, the dot product is going to be 0, whereas on the same vector when we take its dot product with itself it is going to give some value. We are representing it as 1. We define the following. Let us (Refer slide time: 38:57 - 39:19)

define this dot operation for our case. So S dot T is equal to 1/M  ? summation of Si TI;  i is equal to 1 to m and this will be equal to 0 if and only if S is not equal to T. We will see this. (Refer slide time: 39:20 - 39:29)

if one and zero are represented by  +1 and ?1 respectively, we find that S dot S equal to one. this is a very simple because if the ones have been represented as one and zero has been represented as a  ?1, so S dot S so ?1 will get multiplied with ?1 giving you  +1 and one and one will also give you one. so all of them would be one. so all M of them would be one. the sum total in the previous if you look at the previous  definition the sum (Refer slide time: 39:53 - 40:16)


Si Si is going to be a sigma Si Si is going to be M; you divide that with M so that will give you a 1. so s dot s would be 1 whereas for any other code, any other T, where the T is orthogonal to S, this sum is going to come out as 0. That is how we cleverly assign the codes. (Refer slide time:  40:17 - 42:35)



Consider a code T where the number of chips of T, which are the same as those in S is the same as the number of chips of T, which are different. Consider what it is saying: at code T, where the number of chips of T, which are the same as those in S are the same as the number of chips of T which are different, the number of chips of T which are the same as those in S. So the corresponding chips, when multiplied together, will give you so many 1s. If it is the same as the number of chips of T which are different, now if the chips are different in T and S when the corresponding chips are multiplied, they would give you ?1. The number of +1s and the number of ?1s, if they are the same when you add them together in the previous summation, the sum total will become 0. S dot T will be zero. Note that S dot not of T; not of T is the complement of T. That means the 1s and 0s are presented with 1s and ?1s. So 1s and ?1s are flipped in one of them. So naturally the number of chips which are different become the number of chips which are the same and the number of chips which were same earlier, the chip positions which were same earlier, now become different. But any way their numbers are equal. So once again we have S dot not of T is 0 and S dot not of S ? all the 1s in S will be ?1 in not S and all the ?1s in S would be 1 in not S. So in either case, we will get a product of ?1. M ?1s added together will give you ?M divided by M will give you ?1. We have S dot S equal to 1; S dot T is equal to 0; S dot not of T is equal to 0; S dot not of S equal to ?1. So these are the nice properties if we have orthogonal codes. (Refer slide time: 42:36 - 43:44)



So consider the following codes: suppose this U, R, S, and T are four different stations. Here a simple example has been shown using only eight chips. You note that between U and R, in one position they are same; in this position they are same; two they are different; in this fifth position they are same. That is the number three of the position in which they are same and then there is the number four position, which is the seventh position, where they are the same. In four positions they are the same and four positions namely the third position, fourth position, sixth position, and eighth position they are different. The number of positions in which they are the same is the same as the number of positions in which they are different. We know that if instead of 0 we have a ?1 these codes will come out to be orthogonal. Similarly you would see that U, R, S, T are all orthogonal to each other. So the above codes are all orthogonal under the operation defined. (Refer slide time 43:45 -45:09)


Note that S dot T plus R is S dot T plus S dot R, which means that under normal addition, if you take this is normal addition, this dot product is going to distribute. Thus under the assumption of linear addition of signals, S dot sigma of Ci is equal to S if and only if S is in Ci.. That means suppose there are a number of channels, which are transmitting. In that case, we have already assumed linear addition of signal strengths. This is nothing but some of the signal strengths like Q, T, R, U, S, etc., have been all added up. Now S dot will distribute over this summation, so we have S dot T and S dot R etc. If S happens to be in this set Ci, then that S dot S will come out as 1. It should be 1 if and only if S is in Ci; otherwise it will come out as 0. Assuming during any bit time U, R, S, and T transmit 101 and nothing, U is trying to transmit 1, R is trying to transmit 0, S is trying to transmit 1, and T is not transmitting at all. If you use the code shown in the previous slide over here (Refer slide time: 45:10 -45:13)



and then if you do the calculation, (Refer slide: 45:14 - 45:30)
 


this will come out to be like this: the signal strength as we see it, will come out as ?1 +1 ?3 +3 ?1 ?1 ?1 +1 ? so these are the eight signal levels we get. (Refer slide time: 45:31 - 45:50)


Now if we do that, C dot U will get a +1. That means C dot is trying to send 1; C dot R will be ?1; that means R is trying to send 0; C dot S will be +1; that means S is trying to send 1; and C dot T would be 0; that means C is not sending any thing at all. If you remember, (Refer slide time: 45:51 - 46:01)



101- that means a S R, U and T are transmitting in this fashion. (Refer slide time: 46:04 - 46:33)



One implicit assumption in the above is that all stations are synchronized and transmit with the same power. In  practice, perfect synchrony is difficult to achieve resulting in the use of longer chip sequences and lower channel capacity. Secondly, to tackle the problem of power, each mobile transmits to the base station at the inverse of the received power. So there are practical limitations. Although this looks very nice, what happens is that there are practical limitations and the theoretical maximum that we could achieve is less than that but this is an elegant system. Now we will look at some (Refer slide time: 46:50 - 47:18)



MAC protocols: two of them, two small ones we will discuss in this lecture, and then, in the next lecture, we are going to take two of the more involved ones. Taking turns MACs protocols: do you remember that we had channel partitioning MAC protocols and random access MAC protocols and taking turns protocols? In taking turns, what it tries to do is that it sort of allocates the turn and we will see how. (Refer slide time: 47:19 ? 47:55)



One could be through poling by a master node, which is some kind of a centralized system. There is poling of overhead, latency, single point of failure, etc., we are not going to discuss this at this point of time. We are going to focus on this in the next; that is token bus control. A token is passed from one node to the next sequentially;  there is a token message that concerns token overhead, latency, single point, etc. We will see some systems based on tokens now, which are sort of using MAC taking turns.  (Refer slide time: 47:56 - 48:00) 

And the first example we are going to talk about is the token bus. (Refer slide time: 48:01 - 49:05)



So they may be mainly used by assembly line factory. It is used in factories for the main reason that in the other kind of system, which is the contention based system, there is some randomness in the way communication can happen. There may not be any hard and fast guarantee, which people in process control and factories, etc. may not like. So they may use this token bus, which has got the IEEE number 802.4. Token bus is just like a common bus and the principle is like a token ring. Token is passed from high to low number of station. What is a token? Token is some kind of a bit pattern, which is passed from high to low number of stations. The station with token will transmit. It is difficult to add and remove stations in this particular case. (Refer slide: 49:06 - 49:50)



So IEEE 802.4 determines the logical ring of the physical bus by the numerical value of the addresses. A MAC or LLC data unit provides the utility for the lowest address to handle the token to the highest address. Otherwise, the higher address gives the token to the lower address so the predecessor gives the token to the successor then the successor gives the token to the next successor and so on, all the way down the chain. Then there is some protocol for sending from the lowest one to the highest one in one group. Then the token is passed from a predecessor station to the successor station. so this a sort of taking turns kind of thing. (Refer slide time: 49:51 - 50:35)


The token is passed from stations to stations in a descending numerical order of station address. When a station hears a token frame, that means it gets a token frame addressed to itself, it may transmit data frames. That means when it gets the token, which is addressed to itself, and which has been sent by the node which is just higher in number, which is its predecessor, that will be sending the token address to the next station. At that point of time, it may transmit its data frame. When a station has completed transmitting data frames, it passes the token to the next station in the logical ring. So if all of them are trying to transmit they will sort of form a nice queue and then they will come back in a very regular fashion; that is the worst case. (Refer slide time: 50:36 - 50:35)

At some particular point of time, a node may not have anything to communicate. In that case, it will simply pass on the token to the next lower address, next lower MAC address. By the way, I have used the term MAC address earlier also. MAC is for Medium Access Control; for that, we require some kind of address. A particular node, if it has nothing to transmit, will give the  control or the token to the next lower  MAC address and then the next lower MAC address will, if it has something to transmit, will transmit. What is the worst case? The worst is that when everybody wants to transmit; that will take some time, but there is a bound to that time and after that time your turn will come back again. So there is a bound to the worst case performance in  this token bus. But  this is not very efficient, and not very fast. So it is getting replaced now, but even in some factories it is still there. We are going to talk about another technology, named DQDB,  which was once proposed ? it also had a name S M P S ? as a solution for metro networks and that means for metro networking, DQDB was suggested and some were implemented. But DQDB is once again going out. It has a similar kind of principle; only thing is that instead of a single bus we now have two buses. So let us look at DQDB very quickly. (Refer slide time: 52:20 - 52:23)


DQDB is 802.6. (Refer slide time: 52:24 - 52.54)


So it is for a metropolitan area network spanning may be 50 to 100 kilometers and operating at a 34 to 45, even 155 mbps speed was talked about at one point of time.  This did not work out quite well later on; but any way, that is a difference story. It was originally designed like FDDI ? we will be talking about FDDI in the next lecture ? for connecting LANs; expanded to service packet switching at 2 Mbps and isochronous services. Isochronous means nearly synchronous. (Refer slide time: 52:54 - 53:30)



It features fixed length packets like 53 bytes long, and we will see ATM later, which was inherited from this DQDB later on. ATM is still a strong technology even today. Empty cells are generated by the head ends. So there are two buses as I mentioned, and then, at the end of the bus, there are these head ends, which generate a stream of empty cells. The streams of cells move in the opposite direction in the two buses and finally fall off the other end. (Refer slide time: 53:31 - 54:20) 



So to transmit to a destination, the node has to know which bus to use; that means whether it is to the left or right, or up or down. So it has to know which bus to use. So every node must be having this information. It sets a request bit in some cell, which is going in the opposite direction. This is for telling suppose the node to which it wants to communicate is downstream, say towards the left, then it tells all the other nodes towards the right that it wants to a communicate to this node on the left. It defers to downstream requests, counting such requests as they pass by; nodes are not greedy. So this is the heart of the protocol. Actually what it does is that if you simply get an empty cell and want to put in your data, then those nodes, which are towards the end, are favored. I mean it is not a fair system any longer. But then, in this scheme, if you think about it, you will queue the request on a first come first serve manner and that is why you put in your request to the other side so that those nodes on the other side would know that he is going to send something.(Refer slid time: 55:00 - 55:06)
      


So to transmit to a destination, the node has to know which bus to use, etc., and the nodes are not greedy. (Refer slide time: 55:07 - 55:18)
 


Its primary importance today is its close affinity to ATM, and the consequential association with SONET and SDH, for which we shall see ATMs provide the approved a switching fabric. Thank you. In the next lecture, we are going to discuss two more token-based protocols, namely, token ring, which was more common than this token bus or DQDB and FDDI. Thank you. (Refer slide time: 55:37 - 55:39)



good day in the last lecture  we talked about various multiple access schemes and a one of this set of schemes in token bus and DQDV etc where  using tokens ok now we will use the  we will see two other variance of it namely token ring and a (Refer slide time: 56:05 - 56:10)


FDDI so we are going to talk about token based MAC and specifically  (Refer slide time: 56:11 - 56:29)

 
so they are some kind of Round Robin MACs that means a the  chance to transmit comes to each of the station in a Round Robin fashion and this can be done as I mentioned earlier through poling or token busing here we will be specifically  talking about token busing (Refer slide time: 56:30 - 57:03)


so  the first  a system that will talk about is the token ring  so as as the name itself suggests that it is a ring topology  it is a ring on a token ring MAC works with a special pattern or token which is three bytes long so it is a three bytes words of bits called token which moves from one computer to the next priority indicators are used within the token how the priority indicators are used we will see later so (Refer slide time: 57:04 - 57:35)


 so data rate may be four sixteen or hundred mbps medium may be UTP STP or fiber  signaling may  is usually differential Manchester  we mention this earlier what is differential Manchester that how you represent here zeros and ones by electrical signals or optical signals as the case may be and the maximum frame size  would be about this four thousand five hundred and fifty bytes or write up to eighteen point two kilo hertz (Refer slide time: 57:38 - 58:11)



now let us talk about the type of network stations which may be a  connected to an FDDI ring one is a dual attached station which is connected to both the rings that means it is a station which is connected to both the rings that is why it is called dual attached then we have dual attached concentrator DAC which is connected to both rings and provides connection for additional stations and concentrators it is actually the root of a tree this is where the  tree comes from I have a (Refer slide time: 58:12 - 58:35)  


a picture so we have a so this is the picture of an FDDI concentrator so you can see that this the concern this is the main part of the concentrator and the two rings are there the counter rotating this is the primary ring and this is the secondary ring so the primary  ring is coming like this from A to B and the secondary ring is going like this it has some additional ports from which other stations may hang  




	 



COMPUTER NETWORKS 
	Prof. Sujoy Ghosh	
Dept of Computer Science & Engineering 
IIT Kharagpur 
Lecture - 14
Token-Based MAC
(Refer slide time: 00:45)
Good day. In the last lecture, we talked about various multiple access schemes and one of this set of schemes is token bus DQD etc. We will see two other variants of it, namely, token ring and FDDI. (Refer slide time: 01:08-01:15) 

We are going to talk about token-based MAC. (Refer slide time: 01:15 ? 01:34)

They are some kind of round robin MACs. That means the chance to transmit comes to each of the stations in a round robin fashion. This can be done as I mentioned earlier through polling or token passing. Here we will be specifically talking about token passing. (Refer slide time: 01:34 ? 02:07)

The first system that we will talk about is the token ring. As the name itself suggests, it has a ring topology. A token ring MAC works with a special pattern or token, which is 3 bytes long, called token, which moves from one computer to the next. Priority indicators are placed within the token. We will see later how the priority indicators are used. (Refer slide time: 02:07 - 02:42)

Data rate may be 14, 16 or 100 Mbps. Medium may be UTP, STP or fiber. Signaling is usually differential Manchester; we mentioned this earlier. Differential Manchester is how you represent your 0s and 1s by electrical signals or optical signals as the case may be; and the maximum frame size would be 4550 bytes or right up to 18.2 KB. (Refer slide time: 02:42 ? 03:11)

In token ring, like a token bus, a token is passed around the ring, and within the token is an indicator that senses the ring as free or busy. If the token is busy that means some frame is being communicated. At that time, the token circles continuously around the ring are passing each station. Each station is required to examine the token. (Refer slide time: 03:11 ? 03:38)   

If a station wishes to transmit data and the token is empty, it seizes or captures the ring by modifying the token to a start of user frame indicator, appending the data and control fields and sending the frame around the ring to the next station. The next station will now get the token as well as the frame, which will pass on till we get (Refer slide time: 03:38 ? 05:41)

to the node where the data is copied only if it is to be passed to the end user application attached to the node. That means there is a destination address. When the destination node sees that data, it knows that this is for him. So he absorbs, that means, he copies it back. He makes a copy of it and sends it to the application layer in that particular node through all the other layers; we are not concerned about that at the moment. But the token and the frame continue circulating in the ring till it comes back to the center. When the token arrives back at the original site, the token is once again made free and placed onto the network. You see in this scheme only one frame ? I mean if the ring busy at all then one frame ? is traveling along it. It has left the source station, then it has been copied by the intermediate nodes on to the frame as well as the token with the busy indicator over there. Then it finally comes to the end station and at the destination station, it makes the copy of the data for its own use and keeps on circulating this frame and the token right up to it. When it comes back to the original sender, the original sender will now strip all these data, make the token free, and put it on the ring. Now, some body else who ever wants to transmit next, will capture the token and send it in this fashion. This shared medium, namely the ring, is shared by all these nodes attached to it. (Refer slide time: 05:41 ? 06:05)

When a station wants to transmit, it has to wait for the token, then it has to seizes it, and then it transmits the frame. When the station seizes token and begins transmission, there is no token on the ring. Nobody else can transmit. There is no  contention or collision as such, because only the station that has got the token can transmit; so all others do not transmit. (Refer slide time: 06:05 ? 06:44)

What is the expected performance of token passing? First of all, it is fair because it is going in a round robin fashion. So everybody will have his chance; each computer is given in turn an opportunity to transmit even when the traffic is high. However, even if only one computer needs to transmit a message, it has to wait till the time that it receives back the token. Until it receives the token, it cannot start the transmission, so it has to wait. Again, long messages should not be allowed because otherwise one computer may hold the token for too long. (Refer slide time: 06:44 ? 07:49)

Several  tokens are there. Some variations of it use slotted rings, where several tokens or slots are used. These may be more useful and make it more efficient because if it is a very long ring and only one frame is traveling down it, it is rather inefficient way of using the system. So what we can do is that we may allow multiple frames, that means multiple slots, which are sort of distributed over the space. For example, if the speed is 200 m/?s of the frame, the data rate is 10 Mbps. These ten bits will span over 200 m over the ring. So a 2 km ring can hold 100 bits; that is the kind of performance with a single frame. (Refer slide time: 07:49 ? 08:42)

Let us look at how the priority works in the token ring; because what we can do is that we can do differential priorities to the nodes in the network and this is how it works. Let us go through one example; assume a token ring has five stations attached to a priority ring. Station A has priority access of 1. 1 is, let us assume, the lowest priority; stations B and D have priority of 2; and stations C and E have priorities of 3. So C and E have the highest priorities. Once again, assume that A had already seized the ring and is transmitting data frames. The token has a bit set to indicate that the token is busy. And that means because A has already put a frame in it, it is being sent from A. (Refer slide time: 08:42 ? 09:36)

Station B receives the frame it has data to transmit. Let us say that all of them also transmit some data to station B, which receives the frame. It has data to transmit but it cannot transmit at the moment because the ring is busy. But it places its priority of 2 in a reservation field within the token; it puts 2 over there in that reservation field, and sends the token and the frame sent by A along to C. It then passes the token to C. Station C also determines the ring is busy; it has data to send, so it places 3 in the reservation field, thus displacing the 2, which was inserted by B; 2 gets replaced by 3 in the reservation field, other thing remains as it is. It is still A?s frame, which is moving along. (Refer slide time: 09:36 - )  

Station C then passes the frame to D. D must defer because, if you remember, we had   the priority of 1 to A, 2 to B and D, and 3 to C and E. So it came from A to B, B put a reservation and its priority of 2, then C over wrote this with its priority of 3. Now D sees that there is a priority 3 that is waiting and D has only has its priority of 2; so it has to defer. It cannot do anything. So D must defer; it cannot place its priority of 2 into the field because the priority of 3 is already there. Consequently, it passes the frame to E, which examines the reservation field upon seeing the 3 in the field. It does nothing because since its priority is also 3, E is also a priority of 3, so E cannot do anything. So E simply sends it along. (Refer slide time: 10:39 ? 11:11)

Station A receives the frame back; it makes the ring free by resetting the token and passing the token to B. B is not allowed to use the token because the reservation field inside the token is equal to 3, one higher that the priority of B. Although B wants to transmit and the ring is free, B cannot start really transmitting because somebody with a priority 3 is waiting (Refer slide time: 11:11 ? 12:07)

C is allowed to seize the token because the priority field in the token says 3 and C    has the priority of 3, which means that C is the first node with that level of priority, which has got the token. So this sort of seizes the token; it places the data on the ring and sends the transmission to D. Now D is allowed to place its priority of 2. Although C is sending, C has already put its frame and D sees that naturally the reservation field is reset. Now D can place its priority of 2 into the reservation field. It does so and passes the frame to E. E also wants to send; so E replaces D?s priority of 2 with its priority of 3, and passes the frame to A. (Refer slide time: 12:07 ? 12:25)

A also wants to send again, but A must defer any reservation placement since its priority is 1. B must also forego any priority allocation since its priority is 2. C   receives its transmission back; it is required to make the ring free it does so and transmits the token to D. (Refer slide time: 12:25 ? 12:42)

D is not allowed to seize the ring, since its priority of 2 is less than the reserved priority, which has been put there by C. This is the priority indicator of 3; so it passes the token to E. E seizes the ring because its priority of 3 is equal to or greater than the reservation of 3. This is the way the priority ring works ? if you see, whoever put the reservation earlier at the same level, it come backs to him. So he puts the frame out there, but if the higher priority nodes have finished reservation, transmission, etc., then the lower parity nodes can start transmitting and so on. So you can set these priority levels in the token. This is how not only you can have a pure simple round robin, where everybody has the same priority, but you can have priority based token ring also. (Refer slide time: 13:28 ? 13:49)

There is a variation of this dedicated token ring, which is called dedicated token ring.  There is a central hub; there is a more centralized system, which acts like a switch and it?s more like a full duplex, a point-to-point link and the concentrator acts as frame level repeater, and there is no token passing.(Refer slide time: 13:49 ? 15:09)

Next we will take up another system called FDDI. This is still in use in some places where some specific applications are there, but then again FDDI is also sort of going out because other new technology is taking its place. FDDI was originally conceived as a high-speed network and this network could be used in a LAN, WAN or backbone as high-speed data. When it was conceived, at that time, 100 Mbps was considered very high speed; of course technology has changed, but still it is instructive to look at  these technologies. First of all, we?ll see how different MAC schemes can work, and new MAC schemes for new technology, etc. are also coming up all the time. We will, just as an instructive thing, will look into FDDI in some detail. (Refer slide time: 15:11 ? 16:04)

So FDDI was conceived as a high-speed backbone technology. It has a dual ring topology as just like the SONET rings; we talked about dual ring topologies in the   optical networks. By the way, this is based on fibers. This is fiber distributed data interface that forms the acronym FDDI. It uses dual ring topology, using fiber optic cable used to transmit light pulses. Optical fiber channel operates at a rate of 100 Mbps. Well, we can say 100 Mbps only today, but at one point of time, it was the standard in 1980s, which was proposed to be very high speed. It is frequently used in LANs to connect buildings together. (Refer slide time: 16:04 ? 16:27)

So ring circumference can extend to 200 kms; the distance between nodes can be up to 200 kms. FDDI network can host up to 1000 nodes on one optical fiber. That is how it was conceived. This optical fiber is not just continuous optical fiber. This optical fiber goes from hub to hub, that is, from node to node. (Refer slide time: 16:29 ? 17:54)

 This is an FDDI topology; we have two rings. A is known as the primary ring, which is shown in black. The primary ring is the one, which usually carries all the data, and then, there is a secondary ring, which is used for fault tolerance purpose. As matter of fact, this is one reason FDDI is still used in some places where the reliability of the network is of very high concern. We cannot allow it to remain down for any length of time. FDDI can quickly switch from the primary to the secondary ring. This is a production kind of system as you can see, because there is one ring fully dedicated as a secondary ring, which is there. In case of a fault either of a link or particular node, we can quickly have another ring in its place. (Refer slide time: 17:54 ? 20:14)

The FDDI standard specification came up in the 1980s. This has various parts: one is the media access control, MAC part, which deals with how the medium is accessed, the frame format, token handling, addressing, and error recovery. FDDI has a somewhat more complex MAC protocol because FDDI allows both synchronous as well as asynchronous traffic. If some traffic is synchronous, that means, if it is carrying some kind of voice or something, then you know that is 125 microsecond  length; it is very sacred and sacrosanct over there. Every 125 microseconds, some channel may have to send something, as well as we can have packets or data flowing in the network, may be with some kind of lower priority. So this can handle a mixture of both synchronous and asynchronous traffic. That is a peculiarity of FDDI system, which makes its MAC somewhat more complex than a plain vanilla token ring. So we will see details of this MAC later on. The physical layer protocol defines the data encoding and decoding, how data is encoded and decoded. We will see later the clocking requirements and framing. Under the physical layer, we will see medium characteristics of transmission medium, fiber optic link, power levels, bit error rates, optical components, connectors, etc. that is also a part of the standard for the physical layer medium. There is a standard for the station management, which defines station and ring configurations, initialization, scheduling, collection of statistics, fault isolation, and recovery from faults. As I said, the recovery from faults was and still is a very strong point of FDDI ? one reason FDDI may be preferred for some applications. (Refer slide time: 20:14 ? 20:36)

So, as mentioned earlier, the topology of FDDI network consists of two independent rings ? primary ring A is used for data transmission, while secondary ring B provides an alternative data path; this has already been shown. And secondary ring remains idle, unless primary ring fails. (Refer slide time: 20:36 ? 22:37)   

Optical fiber rings are counter rotating; that means one is moving in one direction while the other is moving in the other direction. Two signal paths are provided, one in each direction. Why do you make the rings counter  rotating? Well, we had seen this in our recovery lecture also; the reason is that if there is a node failure what you can do is that suppose you are a station and there are two counter rotating rings passing through you. That means in one, the signal will pass in this direction and in the other the signal will pass in this direction. In optical fiber, this is quite fixed because you want to have proper transmitter on one side and the receiver on the other side. So one ring is moving like this, the other ring is moving like this. Now what might happen is that suppose the next station has failed and this station understands that there is something wrong either with the link or with the next station. So what it might do is that it might make a quick connection over here so that the ring coming in this direction may take this other path and still we can make one ring. This ring will not have any  fault tolerance; there will be one ring by using part of the two rings A and B. So we can get a recovery through that; that is why we have two counter rotating rings with two signal paths provided, one in each direction. A station is a computer, workstation, or node connected to FDDI network. There are some network nodes also; we will talk about this. Or a station must be connected to both in order to use secondary ring as an alternate data path. If it is connected to only one of them, it cannot use the alternate path. (Refer slide time: 22:37 ? 23:05)

Media is, as I said, 1300 nanometer optical fibers, transmission method is base band; that means, there is no modulation. Only the pulses in the raw form travel down the ring; data rate is 100 Mbps and topology is a physical ring of trees and a logical ring. Why is it a ring of trees where do the trees come from ? we will talk about that. (Refer slide time: 23:05 ? 23:41)

Now let us talk about the type of network stations, which may be connected to an FDDI ring. One is a dual attached station, which is connected to both the rings. That means, it is a station, which is connected to both the rings. That is why it is called dual attached.  We have dual attached concentrator, DAC, which is connected to both rings and provides connection for additional stations and concentrators. It is actually the root of a tree. This is where the tree comes from. (Refer slide time: 23:41 ? 25:33)

We have this picture of an FDDI concentrator, so you can see that this is the main part of concentrator. The two rings are there; They are counter rotating ? this is the primary ring and this is the secondary ring. So the primary ring is coming like this from A to B and secondary ring is going like this. It has some additional ports from which other stations may hang and, actually, what might happen is that we may have a tree hanging from a concentrator. We have a tree of nodes here; so that is why this main FDDI ring may be a ring of trees. The way FDDI is actually deployed is also interesting in the sense that you may have a large ring; that is possible. What is done is that we make a very small ring in the core of the system, like wherever your main server is. We make a very small ring just within a room and what happens is that we have concentrators connected to this ring and from this concentrators, a tree spans out to all the other, may be near by buildings or whatever, so that all of them are connected to this FDDI backbone, but the backbone has got two rings. This backbone is fault tolerant. So that is a good thing about FDDI; that is one way FDDI may be deployed so we have a very small ring and tree is spanning out and going out of the building, may be to other building, and so on. Or alternatively, you can have a large ring also. (Refer slide time: 25:33 ? 25:42)

So we have this dual attached stations and dual attached concentrators; concentrators could be the roots of trees. (Refer slide time: 25:42 ? 26:15)

Dual attached stations and dual attached concentrators are more costly. we have a cheaper variety you think that it is good enough which is a single attached station which is attached only to the primary ring. We have single attached concentrators which is connected to only the primary ring through a tree, a double attached station or concentrator can reconfigure the dual ring as mentioned earlier into a single ring in the event of a failure. (Refer slide time: 26:15 ? 27:03)

What are the physical interfaces like ? as opposed to a basic token ring network, in which at any instant there is a single active ring monitor, which supplies the master clock for the ring, in FDDI, this approach is not suitable because of the high data rates. That is one thing; and the other thing is that the ring could be very large. So if the ring is quite large then having the central clock becomes difficult. Each interface has its own local clock and the outgoing data are transmitted using this clock. (Refer slide time: 27:03 ? 27:43)

All data to be transmitted are encoded, as I mentioned earlier, prior to transmission using a 4 of 5 group code, which means there are nearly 32 possibilities. So, for 4 bits of data, we actually have 5 bits, which are going over there. The additional capacity is used in some other way for control purpose; that we will see later. This means, for each 4 bits of data, a corresponding 5 bit code word or symbol is generated by the encoder. Some of these symbols or combinations are used for link control functions. (Refer slide time: 27:43 ? 29:13)

Now let us go through the ring operation ? there are two aspects to it. One aspect is similar to the token ring, which we have already discussed. The sending station waits for a token; sending station captures and strips token and then transmits frames; sending station issues token at the end of transmission. Now this is one point where the FDDI is different from a token ring. In a token ring, if you remember, only when the  transmitted frame with that busy ring etc., comes all the way back to the sender, the sender makes this token free and puts it back on the ring. Since FDDI was perceived as a high-speed ring, what was proposed was that as soon as its transmission of its frame is over, it can put a new token on the ring. Multiple frames may be circulating in the ring at the same time bringing up the speed. So sending station issues token at the end of transmission, destination station copies the transmitted frame and sets the A and C, which is the address recognized by the frame copied indicators. That means it has already copied the frame like what we have in a token ring. (Refer slide time: 29:13 ? 29:46)

The sending station removes the data from the ring by stripping the sent and acknowledged frame, etc. So it takes out the frame; the first few bytes of the frame are not stripped ? this is for some technical reason, we need not go into the details here ? and continue to circulate on the ring as a fragment. Each repeating station  strips 1 byte from the fragment and the transmitting station completely strips it. So there are some fragments also apart from the frames; some fragments are also moving around in the ring. (Refer slide time: 29:46 ? 30:52)  

Now we come to token passing scheme. It uses token passing protocol to move data around; the ring uses another protocol based on timers. We will look at this protocol later on. Timing is very critical to token passing scheme, as it is designed for delay sensitive synchronous data. As I mentioned earlier, the FDDI ring carries a mixture of data. It may carry lower priority packet data kind of thing, which we have been talking about, in the token ring. It may also carry synchronous data, which is time sensitive and which has somewhat higher priority than this other one. This is based on some timing protocol. We will go into timing protocol now. FDDI allows for high data rates, where each ring interface has its own clock. All outgoing data are transmitted using this clock. (Refer slide time: 30:52 ? 31:15)

A node will get packets within a specified amount of time. We are discussing the  timing part of it. A node will get packets within a specified amount of time. As a packet circles the ring with a token behind, each station retimes and regenerates the packets. (Refer slide time: 31:15 ? 31:35)

So this increases probability frame fragments, which will be propagated on the ring ? how fragments are eliminated. Early token release is required because of the high speed and extensive distance provided by FDDI. (Refer slide time: 31:35 ? 32:00)

FDDI rotation time: FDDI uses time to ensure equal access to the ring; measures rotation time by calculating distance of segments, processing time, and number of stations. This is the time you expect a packet to move around the entire ring. Rotation time refers to how it takes a signal to propagate around the ring. (Refer slide time: 32:00 ? 33:36)

So rotation time is used to control the priority operation of FDDI ring. We have several timings: one is measured by the clock that times the period between the receipt of tokens called the token rotation time, that means, how long is it that the token takes to come around the ring. The operation of MAC layer is governed by a MAC receiver and is calculated by target token rotation timer. That means there is a target rotation time, which is  prefixed and there is a TRT, which is measured. Usually you would expect under the normal conditions, when the load is moderate, the token rotation time would be less than the TTRT. When the node is moderately noted by comparing TRT with TTRT, we can find out how loaded the system is. If you have a synchronous link going through the synchronous traffic that is going through this FDDI ring, the synchronous traffic will have to be given the first priority and the asynchronous traffic of some lower priority data traffic, will be put on the ring or will not be put on the ring, depending on how loaded it is and this is how it is calculated. (Refer slide time: 33:36 ? 33:56)

There is a pre-negotiated target time called PTT. PTT is coordinated for the arrival of a transmission. Each node measures time it takes for the token to return to it; that is the TRT. It compares time to a pre-negotiated target time PTT for its arrival. (Refer slide time: 34:01 ? 34:36)

A node is allowed to transmit as long as its full transmission stream does not exceed the PTT. So there is a pre-negotiated target time, which is allowed, and the node is allowed to transmit as long as its full transmission stream does not exceed the PTT. If the token comes back sooner than PTT threshold, it is deemed as a light network load. If the token comes back later than PTT, it indicates the heavy traffic load. Low priority traffic must then be deferred until load on the network becomes lighter. (Refer slide time: 34:36 ? 35:26)

There is a token holding time, THT. If you look at the last point THT is actually equal to TTRT minus TRT. It is used to calculate maximum length of time a station can hold the token to initiate asynchronous transmissions. The point is that there is a target time and there is an actual measured time, by which the token has come back. If the actual measured time is low, that means the network is lightly loaded, you can put some asynchronous traffic. That is why THT is calculated. It calculates the difference between the arrival of the token and the TTRT. It keeps track of the amount of time a host can transmit. This is the formula, and then you have the following rules (Refer slide time: 35:26 ? 36:21)

If THT is less than zero that means traffic is heavy, the total rotation time is actually  more than the expected time, which was expected earlier. This means that all asynchronous traffic has to wait. The synchronous traffic will go through. So if the THT is less than zero, it is a heavily loaded station. Stations can only transmit synchronous traffic. If THT is greater than zero stations can transmit both synchronous and asynchronous traffic during THT. So it first sends the synchronous traffic and then sends the asynchronous traffic till THT falls to zero. If THT is equal to zero, the host cannot start any new packet. THT increases and number of stations  decreases.(Refer slide time: 36:21 ? 39:59)

The FDDI frame format: FDDI is a technology, which is not moving forward very much these days. As the matter of fact, it may be slowly on its way out because we have other ways of achieving the main point of FDDI, which is its fault tolerance.  The speed of FDDI has become 100 Mbps, which is rather not very fast as far as the backbone is concerned; it is taken as a very low speed these days. So FDDI may be on its way out. It is also expensive and the support to it is also dwindling. The reason we are looking at how different issues are handled by a typical MAC protocol is that this business about framing is common to all kinds of data link protocol. So later on, when we talk about Ethernet, which is the most common kind of network in the world today, we will see that it has some frame format. And if you remember our first day?s discussion, when we were talking about these different layers, we said that they are at the same level ? that means a network layer to network layer; network layer in this node to network layer in this node; similarly the transmission layer in this node to transmission layer in that node, etc. ? and have some protocol running. How do these protocols run? These protocols run by adding some header and in some cases a trailer also to the main payload. So whatever it gets from the upper layer is the payload to it for running its own protocol. It adds some header; that means, adds some information to the beginning of the frame and adds some information to the end of the frame to make a complete frame. The corresponding layer in the other node strips this particular information, does whatever it has to do, because it is also running the same protocol. So it knows what to do and then may be either it goes up or it goes down again to the next station and so on. This is an example frame format of FDDI. FDDI frame format has some preamble. We will not going to the details of this because this is not very important any more, but we will just mention that such fields are common in many frames. We have a start delimiter; we have a frame control; and we have a destination address. This has to be there because otherwise the destination will not know that this particular packet is meant for him. We have a source address field, so we have a DA and then an SA field; we have the data, frame check sequence for some error control. By the way, error is not handled by FDDI; error allows other layers to handle the error if there is an error. But we have to check the error; so there is frame check sequence, end delimiter, and frame status. (Refer slide time: 39:59 ? 40:30)

FDDI encodes all data prior to transmission, uses a 4 or 5 group code method, which was mentioned earlier. The encoder generates a corresponding 5 bit word or symbol. For every 4 bits transmitted, FDDI creates a 5 bit code. Bits provide clocking for the signal itself. The status of bit reflects a change of state of the light on the other side. (Refer slide time: 40:30 ? 41:57)

Symbols are light; taken with another symbol they form one byte. So there are 16 data symbols issued with 5 bits. You can have 32 different symbols; out of it, 16 data symbols are reserved for data. This is for 0 to F. so if you write your scheme of bytes in hex, that means in groups of 4 bits each, each of these 4 bits has got its corresponding code in the FDDI symbol. And then, we have 16 other symbols, which are left; so 8 are used as control symbols and 8 as violation symbols. The control symbols are called Q, H, I, J, K, etc. Coding the symbols prevents the occurrence of 4 consecutive 0s in a row. This is necessary to ensure each station?s clock is in sync with other stations for the transition that takes place. When you go from a 0 to 1,  that transition?s edge is used for synchronizing the clock. This is the very common method of synchronization; that is why we have the codes in such a manner, so that we do not have a long string of 0s because then the synchronization might drift.  (Refer slide time: 41:57 ? 42:16)

Token has the following fields: we have a preamble; we have a start delimiter; we have a frame control; and we have an ending delimiter. As I said, one of the main points of  FDDI is the (Refer slide time: 42:26 ? 44:14)

fault tolerance that it provides. There is a ring wrap. I mentioned this earlier that in a dual attached concentrator, if it senses that on the other side the node has failed, it can make a connection between the primary and the secondary ring within itself, so that the ring, while coming like this, starts traveling like this and completes its round. That is because if you have the node on the other side, the corresponding concentrator will also make a connection between the primary and the secondary ring. The failed node is essentially isolated on both the sides. There are connections within the concentrator, and instead of a single ring, you have two rings. Now the primary and secondary ring are fused together and then, suppose this was the primary ring coming, then it goes back to the secondary ring and then to the other concentrator on the other side and the connection is completed. You have one ring; that is why the physical diameter of an FDDI ring is kept within 100 miles. If there is a failure, the total ring diameter does not exceed 200 miles, which is the standard. Ring wrap technique is the technique that we mentioned earlier when we talked about protection and restoration; the technique is used to manage failures. When a station fails or a cable is damaged, dual ring is automatically wrapped. Two adjacent ports connecting to a broken link will be removed from the ring and both stations enter wrap state. (Refer slide time: 44:14 ? 44:33)

FDDI concentrator switches to a wrap state and the ring is doubled back onto itself. Data continue to be transmitted on FDDI single ring. Performance is not negatively impacted during wrap state. So this is the good thing about FDDI. When you have very mission-critical situations, FDDI is kept as a strength for very critical situations like, may be, stock exchange or may be some thing else, where even few seconds of network down time is not acceptable to anybody. So there you could deploy this kind of technology with its inherent fault tolerance. So this is the another picture of FDDI ring wrap.  (Refer slide time: 45:13 -  45:33)

As shown over here, you have one station 4, which has gone down so that 2 adjacent stations, namely, 2 and 3 wrap around and we have a single ring now going around. So this is clear. (Refer slide time: 45:33 ? 47:40)

We have discussed earlier how this is done. We have optical bypass switches used for two or more failures to occur. By the way, what would happen if more failures occur? Suppose there was a one single failure, one single node failure, and the two rings primary and secondary and the two adjacent concentrators were wrapped back and we had a single ring like this, what would happen if another node fails in-between? What would happen is its adjacent concentrators once again wrap around. But now, instead of only one single ring, which we had earlier in the case of a single failure, now with this double failure we may have two rings. These two rings are not connected to each other. So these two rings individually would still keep working. There is a part of the protocol, which I have not covered. It is that if there is a single token, which was on the other side, and now the token is lost; there is no token. There is a protocol for reclaiming and regenerating a token. Now what would happen is that, two individual rings will come into operation on the two sides, but they will not be able to do so. The nodes, which are connecting to this sub-ring and the nodes, which are connected to the other sub-ring, will not be able to communicate with each other across, but within themselves, they will very well communicate as usual. So rings are segmented back into two independent rings incapable of communicating with each other. Additional failures can cause further ring segmentation. Optical bypass switches can eliminate failed stations to prevent ring segmentation. We have all seen how this is done ? actually this is all done with mirrors. (Refer slide time: 47:45 - )

So optical bypass switch has optional optical mirrors that pass light from ring directly to DAS station during normal operation. DAS station experiences a power loss; optical bypass switch will pass the light through itself. It uses internal mirrors to maintain ring integrity. (Refer slide time: 48:04 ? 48:54)

The other technique of this protection, etc., which we had seen earlier is dual homing. This dual homing is also used in an FDDI context; a router or a DAS ? DAS you remember is a dual attached station ? is connected to two concentrator ports on FDDI concentrator. One port provides a connection to active fiber link, while the other port is in hot standby mode. Actually there are two nodes, A and B; usually the A node is in hot standby mode and B node is operating. So the port is in hot standby or passive mode; hot standby is constantly tested and will take over if the primary link fails. (Refer slide time: 48:54 ? 49:08)

A typical DAS configuration has a B port; it is designated as active port; and a port is configured as the hot standby. When the primary link fails, passive link automatically activates and hot standby becomes operational. So this is the same dual homing principle, which we had seen earlier when we were talking about recovery and protection. With this, we come to an end of the discussion about token bus and token ring and FDDI. We have discussed the number of these token-based protocols, namely, token bus, DQDB, token ring, and FDDI. The FDDI was quite good. Only thing was that FDDI was also quite costly. These technologies are actually going out in some sense. If you remember, when we were talking about DQDB, we said it can handle 53 byte cells; that came down to ATM and ATM is that sort of the technology that is still quite alive today. We will have an extensive discussion about ATM. Regarding the WAN technology, two other technologies, which are once again on their way out, are X 25 and frame relay. Well, frame relay is still there in many parts; X 25 is sort of going on but still we will just have a quick look at these. Then we have to consider these MAC levels. This is how the next set of lectures will go. What will happen is that once we finish this, then we will talk about the data link layer and specifically about the MAC sub-layer of the data link layer. Then we will have to talk about the LLC, that is, the logical link control. So we will discuss that and the other  important functionality of this data link layer, which is the error and flow controls.  Because whenever you have a transmission, somehow you have to assume that errors may occur. Depending on the medium and the technology, errors may be more or less frequent; for example, in a fiber, the error may be very low, error probability may be very low. When you are using wireless, the error probability is high. But, anyway, you have to consider the possibility of some error and how errors are handled. That means, we are talking about bit errors, which may come in due to noise and other things into the data in a particular link. That is one thing. And if there is a flow control to be done, that means, if there is a congestion or not because you are sending from one side whether the other side is receiving it or not, that you have to some how make out. We will take up these things next. Thank you. 
Preview
COMPUTER NETWORKS
Prof. Sujoy Ghosh
Dept of Computer Science & Engineering 
IIT Kharagpur 
Lecture - 15
Data Link Protocols 
(Refer slide time: 52:17 ? 52:18)

(Refer slide time: 52:18 ? 52:20)

Good day so today we will start  our discussion on data link layer as a matter of fact we have discussed a part of data link layer namely  the Mac sub layer(Refer slide time:52:30 ? 53:16)

we will see how that all fix in but to fit it into the broader picture if you remember when we were discussing  the seven layer OSI protocol starting from the application layer the bottom most layer was the physical layer  so we have finished our discussion on physical layer and just above the physical layer we have the data link layer so  we will look at the different components of data link layer and how they will be used different protocols etc  so that is what we will do  so our this main thing is data link protocols which are the protocols which are used in the data link layer (Refer slide time: 53:16 ? 54:03)

now what are the main tasks of data link layer it transfers data from the network layer of one machine to the network layer of another machine so  actually this is the part of the service it gives to the upper layer do  you remember that above the data link layer we have the network layer  so below the network layer we have the data link layer so the data link layer gives some service to the network layer and this service is the transfer of data from one network layer to another network layer so that is the service it gives it to the network layer and this in its turn uses the physical layer so it converts raw bit streams of the physical layer into groups of bits etc  or frames (Refer slide time: 54:03 ? 55:07)

so this is how we can look at it  so this is one node and this is another node above this   there may be other layers we are not concerned about this upper layers at the moment so this gets  some data to be sent from the network layer and this data link layer sends it to the next data link  layer remember once again that the network layer is concerned with transfer of data etc  across the network that means it may take several such hubs but  data link layer is just concerned with the single hub so this is how we simply the problem the problem of going  multi  hub  so this multi hub part we leave it to the network layer for this single hub so multi hub will naturally constitute the number of such single hubs and data link layer would  handle  the transfer of data from one  from one node to the next (Refer slide time: 55:08 ? 56:34)

so what are the kinds of services types of services that  the data link layer gives one is unacknowledged  connectionless so no attempt  to recover lost frame if some frame is lost due to noise error etc etc  there is no attempt to recover this and because there is no acknowledgement from the other side it is a connectionless system suited for low error rate network or for fault tolerant applications such as voice  what you mean that voice is a fault tolerant application  we mean that even if some of the bits in a voice stream digitized voice stream that is even if some of bits  drop obviously there will be some degradation on the other side if your are do not doing any kind of correction etc but to the human ear it may not be very perceptible  for example I am talking even if there is a momentary glitch you will more or less make out what I am talking about  so that is why in that sense it is more inherently fault tolerant so that is un acknowledgement unacknowledged connectionless service that is one kind of service acknowledged connectionless service this is another kind of service so each frame is acknowledged by the receiver so this is suited for unreliable channel  so require this acknowledgement for the special reliability  (Refer slide time: 56:36 ? 56:49)

 acknowledged connection oriented service ensures that all  frames are received  and each is received exactly once  and these services are accomplished using as I said simplex not usual but half duplex or full duplex channels(Refer slide time: 56:49 ? 57:33)

so this is some examples not very important  sorry so is a reliable message stream it may be connection oriented service or it may connection less service and it may be a reliable message stream or reliable byte stream  so reliable message stream sequence of page reliable byte stream they say remote login so they are coming byte by byte here it is coming page by page unreliable connection like digital voice unreliable datagram so these are but when you come to datagram this becomes connectionless service unreliable datagram acknowledged datagram request reply etc (Refer slide time: 57:32 ? 57:40)

now  let us look at just one thing  that where does this all these data link layer exact exactly where does it exist physical medium we understand it is a cable or it is this  electromagnetic field this free space etc  or fiber so we can see it we can fell it but where does the data link layer resides so to say(Refer slide time: 58:00 ? 58:25)

now frames could be fixed length like ATM  so ATM cells are of fixed length so you know once you have synchronized you know that  they are going to come with  fifty three byte kind of  regularity but frames could be variable length also in which case we use this byte count byte stuffing bit stuffing generic framing procedure Manchester encoding etc  
COMPUTER NETWORKS
Prof. Sujoy Ghosh
Dept of Computer Science and Engineering
I .I .T Kharagpur
Lecturer Name # 15
Data Link Protocols
(Start time: 00:45)

(Slide time: 00:53 - 00:54)
Good day. Today we will start our discussion on Data Link layer. As a matter of fact, we have already discussed a part of the data link layer, namely, the MAC sublink layer. We will see how it all fits in. To fit it into the broader picture, if you remember when we discussing the 7-layer OSI protocol, starting from the application layer the bottom most layer was the physical layer. Just above the physical layer we have the data link layer. We will see the different components of data link layer and how they are used, we will discuss different protocols, etc. 

(Slide time: 01:33 - 01:38)

These are protocols which are used in the data link layer.


      (Slide time: 01:39 - 02:27)
The main task of the data link layer is that it transfers data from the network layer of one machine to the network layer of another machine. This is a part of the services it gives to the upper layer. If you remember, above the data link layer, we have the network layer. The data link layer gives a service to the network layer, and this service is the transfer of data from one network layer to the other, and this in turn uses the physical layer. It converts the raw bit stream of the physical layer into groups of bits or frames.

(Slide time: 02:28 - 03:33)
This is how we can look at it: this is one node and this is another node. Above this is there could be other layers; we are not concerned with these at the moment. This gets some data to be sent from the network layer and the data link layer sends it to the next data link layer. Remember that the network layer is concerned with transfer of data, etc., across the network. That means it may take several such hops; but data link layer is concerned with just a single hop. This is how we simplify the problem of multi- hop; this we will leave to the network layer for the single hop. So, multi-hop will constitute several such hops and data link layer will handle transfer of data from one hop to the next hop. This is another way of looking at it. 

      (Slide time: 03:34 - 03:51)
There is a virtual data path from layer three to layer three, but actually this goes through the data link layer and this is the actual data path, which goes through the physical layer and some physical transmission medium. 

(Slide time: 03: 52 - 06:06)
Now, why do we require any controls in the main data link? These are the main functions of the data link layer. Actually, all the combinations are not used in all the situations, but these are the general categorizations of data link. The frame synchronization: the beginning and end of a data block, that means, a frame, should be recognizable. That means when you are sending number of bytes from one machine to other, they are formed into blocks. At that time, the other machine should be able to recognize the beginning and end of the block. So you have to organize these bytes into frames. That is the first job of the data link layer. Second job of data link layer that it might or might not do is flow control. The sender should not send frames at a rate faster than the receiver can receive and process them, because the sender may not know everything about the state of the receiver at that particular point of time. If it goes on sending data, the receiver may be forced to drop some of the data. In some cases, this is ignored. But in some cases, it is relevant and the data link layer may do some floor control. The third thing is error control, where any bit errors introduced by the transmission system should be corrected. Whenever you are sending something through a transmission medium there is always a chance of some error, and the data link layer has to take care of this error.

      (Slide time: 06: 24 - 08:00)
Addressing: on a multipoint link like LAN the identity of sender and receiver must be specified. In such a case, the identity of the receiver would be very important because in many such links, especially in broadcast links, what happens is that the data reaches everybody. The receiving station must know whether or not one of these frames is meant for him. The addressing of receiver is important for that reason. For all the above points, we require some data link layer protocol to run between the two layers. In order for this protocol to run, these two nodes need to have some kind of control or management information. The control and management information will be transmitted on the same medium. Control and data flow on the same link. The receiver must be able to distinguish control information from the data being transmitted. We also require link management ? the procedures for the management of the initiation, maintenance and termination of a sustained data exchange. 

      (Slide time: 08:08 - 09:22)
These are the different functions of the data link layer. The link layer services are framing and link access. It encapsulates datagram into frame, adding header or trailer. What the header or trailer contains depends on the protocol that you are using on that link; some do not have a trailer, some do not have header; some have both header and trailer. This is the part that is used for the protocol to run. Channel access is also there if it is a shared medium. MAC protocol is a part of it; if it is a shared medium, the medium access control has to be used. MAC addresses are used in frame headers to identify source and destination; it is different from another address we have over the entire network, called the IP address. Right now, we will be concentrating on MAC addresses.


(Slide time: 09:23 - 10:58)
Link layer services offer reliable delivery between adjacent nodes. If the medium is prone to noise or introduction to errors, the data link layer has to address the problem. Apart from this link, you may require resilience at a higher link because a particular node of the network link might fail. So, whatever protocol you might have for the data link might not work, so the chain may break down. This is for making individual links of the chain as good as possible. This is one of its jobs ? reliable data delivery. Just as fiber may have low error bit, similarly, some links like wireless links have high error rates. We have just discussed why it gives both link-level and end-end reliability. 


      (Slide time: 10:59 - 11:34)
The flow control is pacing between the adjacent sending and receiving nodes. They should always be in sync, whatever the sending station sends, the receiving station should be able to absorb that much. For that, you may require some explicit controls sometimes. We will look at floor control later on, in a later lecture. The other service is error detection, which includes errors caused by signal attenuation and noise. Receiver detects presence of errors; that is the error detection part. Once you detect that there has been some error, you may ask for some retransmission from the other side so that you may get the correct data, or you might be able to look at that faulty data and correct it locally. The receiver identifies and corrects bit errors without resorting to retransmission. All these would happen on the line, and the line could be simplex, half-duplex or full-duplex. Simplex is not very common, because simplex can go only in one direction.  

      (Slide time: 12:16 -12:46)
In half-duplex nodes at both ends of link can transmit, but not at the same time.

      (Slide time: 12:47 - 14:16)
DLL offers unacknowledged connectionless and acknowledged connectionless services. In unacknowledged connectionless, there is no attempt to recover lost frame and there is no acknowledgement from the other side. It is suited for low error rate networks or for fault tolerant applications such as voice. By voice tolerant application, we mean that even if some of the bits in a digitized voice stream drop, there will be some degradation on the other side. But to the human ear, it is imperceptible. That is why it is fault-tolerant. In acknowledged connectionless service, each frame is acknowledged by the receiver and it is suited for unreliable channels, where acknowledgement is required for special reliability. 

      (Slide time: 14:17 - 14:31)
Acknowledged connection-oriented service ensures that all frames are received and each is received exactly once and these services are accomplished using simplex not the usual, but half-duplex or full-duplex channels.

      (Slide time: 14:32 - 14:36)	
      
These are some examples. It is a reliable message stream. It may be connection-oriented service or connectionless service. It may be a reliable message stream (sequence of pages) or reliable byte stream	(reliable login): in the latter it is coming byte by byte and in the former, it is page by page. An example of unreliable connection is digitized voice; unreliable datagram (electronic junk mail) is connectionless service.

      (Slide time: 14:37 - 14:38)  
The data layer link exists in the network interface card. If you have a PC that is connected to a network, it is probably connected through an Ethernet card, or Network Interface Card (NIC) or something like that. The card has a socket where the Ethernet cable will be plugged in. So, there is a network adaptor. The adaptors implement most of these data link functions, so it is the adaptors that are communicating. The datagram from a higher layer is made into a frame by the adaptor and it is then sent, following the link layer protocol, to the other node. The adaptors communicating have two nodes: one sending node and one receiving node. 


      (Slide time: 14:39 -15:14)

	[WRONG SLIDE?]
The link layer in adaptor is also called NIC, examples of which are the Ethernet card, PCMCI card or the 802.11 card. On the sending side, the adaptor encapsulates the datagram in a frame. It adds error-checking bits, rdt, flow control, etc. On the receiving side, it looks for errors, rdt, flow control, etc., extracts datagram and passes to the receiving node. The adaptor is semi-autonomous and communicates directly with link and physical layers. The adaptor has some hardware and some in-built software. 

(Slide time: 15:15 - 16:48) [WRONG SLIDE?]

(Slide time: 16:49 -17:19)

(Slide time: 17:20 -17:45)

(Slide time: 17:59 - 18:42)
The data link layer is divided into two parts. One is the MAC and the other is the LLC. So in any broadcast network, the stations must ensure that only one station transmits at a time on the shared communication channel. That is the MAC part of it. The protocol that determines who can transmit on a broadcast channel is called Medium Access Control (MAC) protocol. We have seen a number of MAC protocols already.

(Slide time: 18:43 - 19:26)
The data link layer is divided into two sublayers. Above the data link layer, you see two network layers and below it is the physical layer. The data link layer itself is divided into two parts: the medium access control part, which is closer to the physical layer, and the logical link control. The MAC protocols are implemented in the MAC sublayer, which is the lower sublayer of the data link layer. The higher portion of the data link layer is often called logical link control or LLC. 

(Slide time: 19:27 - 20:20)
This is the broad picture: we have been referring to some numbers like 802.1, 2, 3, 4 etc. So IEEE 802 is a family of standards for LANs, which define an LLC and several MAC sublayers. So 802 encompass all these. This 802.2 is the LLC part and below the 802.2 there are various kinds of medium access controls. There are 802.3, 4, 5, 6 and then 10, 11, 12 etc., and now we have 15, 16, etc. Above the data link layer, there is a higher layer and below that, there is the physical layer.

(Slide time: 20:21 - 21:05)
802.1 gives you an overview; 802.2 is the LLC we will be talking about today. 802.3 is the famous Ethernet; CSMA/CD is the kind of MAC protocol that it uses. 802.4 is the token bus, which we have already seen. 802.5 is the token ring. 802.6 is the distributed queue dual bus. FDDI is the fiber distributed data interface and there are others. As new protocols come up, they keep adding to this list.

(Slide time: 21:06 - 21:53)
LLC, whatever it does, it requires some headers. So when the packet is coming from the network layer, the LLC header is added to the packet. So it will reach the LLC sublayer on the other side. Then it comes to the MAC sublayer and MAC sublayer will add its header and it may add some trailer also. The MAC, LLC, and original packet may constitute one frame and it is pushed on to the physical layer in the network.


(Slide time: 22:01 - 22:18)
The 802 LANs offer the best effort data frame services. Error control and flow control are handled by LLC. LLC runs on all the three 802 LANs and hides the differences to the network layer. 



(Slide time: 23:04 - 23:17)
The different physical layer protocols are transparent to the network. All that the data link network knows is that this network layer will provide a reliable service for sending the data from this node to the next. LLC adds its header to the network layer packet. It contains sequence and acknowledgment numbers. Resulting structure goes into the payload of 802.x frame for transmission. 


(Slide time: 23:18 - 23:46)
LLC operations are sometimes divided in this fashion. Type 1 operation supports un acknowledgement connectionless service. Type 2 operation supports connection mode service. Type 3 operation supports acknowledged connectionless service. 

(Slide time: 23:47 ? 24:31)
The LLC has protocol data units or PDU, which carries user information. The control field includes a 7-bit sequence number N(S), associated with this PDU. It also includes a piggybacking acknowledgment sequence number N(R). Unnumbered various protocol control PDUs.  These five bit M fields indicates a what kind of PDU it is. 

(Slide time: 24:32 - 24:52)
There are some supervisory PDUs used for flow and error control. It includes an acknowledgment and sequence number and a 2-bit S field to distinguish three different PDUs: receive ready (RR), receive not ready (RNR) and reject (REJ). 


(Slide time: 24:53 - 25:19)
The type 1 operation supports the unacknowledged connectionless service. The UI PDU is used to transfer user data. There is not acknowledgement, flow control or error control. The XID and TEST PDUs support management functions associated with all the three types of operation. 

(Slide time: 25:20 - 25:32)
An LLC entity may issue a command XID or TEST. The receiving LLC entity issues a corresponding XID or TEST in response.  

(Slide time: 26:00 - 26:08)
Type 2 operations involve three phases: connection establishment, data transfer and connection termination. 

(Slide time: 26:09 - 26:45)
With type 3 operation, each PDU transmitted is acknowledged. A new unnumbered PDU, the acknowledged connectionless (AC) information PDU, is defined. User data are sent in AC command PDUs and must be acknowledged using an AC response PDU. 

(Slide time: 26:46 - 27:45)
To guard against lost PDUs a 1-bit sequence number is used. The sender alternates the use of 0 and 1 in this 1 bit. The receiver responds with an AC PDU with opposite number of the corresponding command. Only one PDU in each direction may be outstanding at any time. 


(Slide time: 27:46 - 29:16)
Frame Synchronization: Two sides must be able to synchronize their movements. This synchronization is of two types. Suppose you are sending data 1 byte at a time, or you are sending blocks, each block containing a number of bytes. Synchronization of frames is necessary for this. When data are transferred from the transmitted to the receiver unless steps are taken to provide synchronization the receiver may start interpreting the data erroneously. Suppose you have taken the second byte as the first byte, you will never be able to know that it is not the first byte. There are two common approaches: Asynchronous Transmission and Synchronous Transmission. 

(Slide time: 29:17 - 30:45)
In asynchronous transmission, data are transmitted one character at a time. Timing of synchronization must only be maintained within each character. The receiver has the opportunity to resynchronize at the beginning of each new character. If you use encoding, you can use the transition time for synchronization between the sending and receiving nodes.

(Slide time: 30:11 - 30:41)
So when no character is being transmitted the line between transmitter and receiver is in idle state; so there must be some start and some stop. There is a start after which we can introduce some parity bits ? we will see later how parity bits are introduced ? and this is the stop.

(Slide time: 30:42 - 31:46)
In synchronous transmission, a block of bits is transmitted in a steady stream without start and stop codes. Actually asynchronous transmission is not as efficient. The block may be arbitrarily long. To prevent timing drift between the transmitter and receiver, clock signal is embedded in the data signal. (E.g. Manchester encoding) 

(Slide time: 31:47 - 32:35)
Apart from this synchronization of clock for the bit, you require another level of synchronization, so as to allow the receiver to determine the beginning and end of a block of data. Every block begins with a preamble bit pattern, and generally ends with a postamble bit pattern. The kind of preamble and postamble that are used is directly related to the kind of protocol that is used.

(Slide time: 32:36 - 33:55)
There are many kinds of framing available. You can observe that only the body is coming from the higher layer; the rest of it is being added in this layer. For example, DECNET?s DDCMP frame has SYNs, header, body and many other bits are there. The ATM cell has only the header, CRC and the body. IBMs have a BISYNC frame, header, body and bits. The ARPANET?s IMP-IMP frame has SYN, header and body bits. The ISO?s HDLC frame looks like this: header pattern, body and CRC, and again some specific pattern. 

(Slide time: 33:56 - 34:15)
A typical synchronous frame format would have an 8-bit flag, which would be the preamble, control fields, data field and some more control fields and an 8-bit flag (post amble). 


(Slide time: 34:16 - 34:42)
For sizeable blocks of data, synchronous transmission is far more efficient than asynchronous mode. Asynchronous transmission requires 20% or more of overhead.  The control information preamble and postamble in synchronous transmission are typically less than 100 bits. The overhead is low here that is why the efficiency of synchronous transmission is very high. So when you are sending large amount of data, you go for synchronous transmission, because it is efficient.

(Slide time: 34:54 - 35:34)
Framing translates the physical layer?s raw bit stream into discrete units called frames. The sender sends the message, which is transmitted in the form of frames. N frames are on transit and they are going from the sender to the receiver. Now how can the receiver detect frame boundaries? That is, how can the receiver recognize the start and end of a frame? This is done by four methods: length count, bit stuffing, character or bit stuffing and pulse encoding. We will look at some of these now.

(Slide time: 35:35 - 36:04)
Frames could be of fixed length, like ATM. When it is ATM, you know that it is of 53-byte kind of regularity. They could be of variable length also, in which case we use the byte count, byte stuffing, bit stuffing, generic framing procedure and Manchester encoding. ATM is a kind of fixed length frame. Variable lengths are byte count (DECNET), byte stuffing (SDLC), bit stuffing (HDLC), generic framing procedure, and Manchester encoding (802.5). 

(Slide time: 36:05 - 36:53)
Now in framing, we make the first field in the frame?s header as the length of the frame. That way the receiver knows how big the current frame is and can determine when the next frame ends. From the above slide, we can see that frame 1 contains 5 characters, frame 2 contains 5 characters, frame 3 contains 8 characters and frame 4 contains 8 characters.

(Slide time: 36:54 - 37:55)
Here the disadvantage is that the receiver loses synchronization, when bits become garbled. If the bits in the count become corrupted during transmission, the receiver will think that the frame contains fewer (or more) bits than it actually does. 

(Slide time: 37:56 - 38:17)
Checksum will detect the incorrect frames; the receiver will have difficulty resynchronizing to the start of a new frame. This technique is not used anymore, since better techniques are available. 


(Slide time: 38:18 - 39:04)
One of the better techniques is known as bit stuffing. Use reserved bit patterns to indicate the start and end of a frame. For instance, use the 4-bit sequence of 0111 to delimit consecutive frames. A frame consists of everything between two delimiters. So you have this one delimiter on one side, 0111, and then the frame and then the 011. So as soon as you get 011, you know that the frame is starting and as soon as you get another 011, you know that the frame has ended. So this way we can know the beginning or the end of the frame. 

(Slide time: 39:05 - 41:53)
The problem with this is as follows: what happens if the reserved delimiter happens to appear in the frame itself? If we do not remove it from the data, the receiver will think that the incoming frame is actually two smaller frames. Suppose we have the 0111 as the delimiter and the delimiter may contain data that came from the user in any bit pattern. You have to allow any bit pattern to the user. It could be that a picture is being sent in several bits and the bit pattern may be arbitrary. In that case, 0111 may appear in the body of the data; this is where the bit stuffing part comes. We introduce a new set of pattern, say, 0111, for each existing pattern. So now, the solution is to use bit stuffing. Within the frame, after every occurrence of two consecutive 1s, insert a 0. For example, append a 0 bit after each pair of 1s in the data. This prevents three consecutive 1s from ever appearing in the frame. 

(Slide time: 41:54 - 43:25)
Similar to bit stuffing we may have byte stuffing. For example, let us say a flag say some character is there. So, say the flag, which is also a part of the regular header, just happens to appear in the body of the frame or body of the packet. So what we do is that, we use this other character. These are character introductions, it is called byte stuffing. One byte is one character. So we introduce the character, escape character, just before the flag, and what happens if escape itself appears in the body? Well, we put escape. So if there are two escapes side by side we know that we have to interpret it as only one escape. If there are two escapes in the original data packet, just by chance, then actually this will be center 4 escapes and just one after the other, and at the receiving side, for every 2 escapes, it will reduce it to one escape and know that this is the just a part of the data. Only at the end, we will get flag, etc., bytes. If there is escape flag, we have escape escape escape flag escape escape and so on. So this is known as byte stuffing. 

(Slide time: 43:26 - 43:57)
      Point to point protocol.

(Slide time: 43:58 - 44:40)
We will now discuss point-to-point protocol. There will be a flag field, address field, control field, protocol field, and payload field. This payload is the one which is actually coming from the higher layers. That means from the network layer, some of it will actually come from the user, from the application layer itself. This is the payload so far as the data link layer is concerned; it ends with Checksum and then flag. This is what the general things look like and we will come to discussing how they are used.  

(Slide time: 44:41 - 46:09)
In a point to point data link control, there is one sender, one receiver and one link, which is easier than broadcast link. It has no media access control, no need for explicit MAC addressing e.g. dial-up link and ISDN line. The popular point to point DLC protocols are PPP, which is point to point protocol, and HDLC, which is high level data link control. 

(Slide time: 46:16 - 49:13)
In PPP design requirements are given in RFC 1551. RFC stands for Request For Comment and forms a very important part of networking. PPP uses packet framing; its requirements are encapsulation of network layer datagram in data link frame. This carries network layer data of any network layer protocol at the same time. It should have the ability to demultiplex upwards. PPP uses bit transparency also, which means, it must carry any bit pattern in the data field. 
 
(Slide time: 49:14 - 49:52)
We only require error detection, but no correction at the receiving end. The connection liveness: it should be able to detect, signal link failure to network layer. Network layer address negotiation means endpoint can learn/configure each other?s network address. 

(Slide time: 49:53 - 50:48)
The PPP non-requirements are no error correction/recovery, no flow control, out of order delivery is acceptable; and no need to support multipoint links; e.g., polling. Error recovery, flow control and data ordering are all relegated to higher layers. 

(Slide time: 50:49 - 52:36)
The PPP data frame has flag, address, and control and protocol bits. The flag is the delimiter. The address does nothing. The control also does nothing; in the future possible multiple control fields. The upper layer protocol is where frame is delivered. The check is for detecting errors. 

(Slide time: 52:37 - 52:58)
In the data frame some info that is upper layer data being carried is required and the check is the cyclic redundancy check for error. So info is the upper layer data. So this is the main body or the payload which is being carried. The check is for error detection. 



(Slide time: 52:59 - 53:50)
 
It uses byte stuffing. The data transparency requirement data field must be allowed to include flag pattern <01111110>. Now we can have the question, is the received <01111110> data or flag? Sender adds extra byte after each <01111110> data byte. At the receiver side two 01111110 bytes in a row, discard first byte, continue data reception and single 01111110 is flag byte. 

				(Slide time: 53:51 - 54:18)
So after PPP, instead of sending it will send first this B1 then B2 etc., and then instead of sending one of them 0 1110, it sends two of them and then B4 and B5. This is byte stuffing, which is used by PPP.

(Slide time: 54:19 - 54:38)
There are a few control issues like before exchanging network layer data: data link peers must continue PPP link and learn/configure network. 

(Slide time: 54:39 - 55:12)
The first thing is the link may be dead. Then the carrier is detected. So it will try to establish the link. For that, they will require some authentication, which means the two sides, configurations, etc. must agree if it fails to establish, then it goes back to dead. If it gets successful authentication, then the network is open and then there is some transmission of data and then finally it will terminate. So for all these, there is a data exchange.

(Slide time: 55:13 - 55:31)
Configure request, configure acknowledgement and configure not acknowledgement -that means your configured thing is not acknowledged. Some of the options are not accepted and some of the options are not negotiable. So this way, the two sides communicate and establish the link. We need not go into the details. This is not really necessary. This is a very simple protocol. Just use byte stuffing and use some data, some error detection and the framing. This is a very simple, but very widely used protocol. In the next lecture, we will see how the error control and error detection can be done by the data link layer. Thank you. 

(Slide time: 56:02 - 56:07)


      (Slide time: 56:08 - 56:11)

      Error Control 

(Slide time: 56:35 - 56:46)
 

	(Slide time: 56:47 - 57:03)
When data is transmitted over a cable or a channel, there is always a chance that some of the bits will be changed or corrupted due to noise signal distortion or attenuation.  for example, suppose you have a wireless channel and suddenly there is a burst of noise. What will happen is that, some of the data will get garbled. Similarly the data may become very attenuated. It may be due to some loose contact somewhere or something. The one that was sent was not received that way or may be it was received as a one zero or something. So whenever you are sending some data or something, there is some communication going on some transmission over some transmission line. You always have to assume that, a data may not  reach the other side in a perfect condition. So that is why CRC is preferred in many data link protocols. 

(Slide time: 57:47 - 56:48)
CRC is Cyclic Redundancy Code. 

(Slide time: 57:58 - 58:05)
In Cyclic Redundancy Code, essentially the data is regarded as being one very long binary number. After all, what you are sending is a string of ones and zeros so you can take a few of them and just look at it as a binary number although the original intention of the user. Place holder digits are added onto the end and it is divided by a generator polynomial using modulo 2 divisions. The remainder at the end of this division is the CRC. 


 
COMPUTER NETWORKS
Prof. Dr. Sujoy Ghosh
Department of Computer Science and Engineering
IIT Kharagpur
Lecture -16
Error Control
Good day.
In the last lecture, we have seen some functionalities of the data link layer namely framing etc. And we have been talking off and on about the error control. Error control means error detection correction, etc. We will be talking about error detection correction in this lecture. We will be talking about error control now. 
 
      (Refer slide time: 01:14 - 03:35)
Now data errors, when data is transmitted over a cable or a channel there is always a chance that some of the bits will be changed or corrupted due to noise, signal distortion or attenuation etc. So many things may happen; for example, suppose you have a wireless channel and suddenly there is a burst of noise, so some of the data will get garbled. Similarly the data may have become very attenuated due to some loose contact somewhere or something and the one that was sent was not received or maybe it was received as a 0 or something. So, whenever you are sending some data or something, there is some communication going on over some transmission line, you always have to assume that data may not reach the other side in a perfect condition and error control has to deal with how to handle that. So if errors do occur then some of the transmitted bits will either change from 0 to 1 or from 1 to 0. Now what do you mean by error? Some bits were transmitted from 1 and were changed to 0 or from 0 they was changed to 1. So this is the only kind of error that may occur and if you knew where the position of that error is, we can correct it. Because the correct one will be the reverse of the faulty one. So if it is 0, then the correct one is 1 and if it is 1 then the correct one is 0. The whole trick is to find out whether any errors or any change has occurred and if such a change has occurred, where it has occurred and how to handle it. There may be a lot of noise and a lot of data may get corrupted in this fashion. So we will see how or what we can do about this. This data error will also depend on the kind of transmission medium we are using. So that is always there. 

(Refer slide time: 03:37 - 04:20)
So, random errors change bits unpredictably. Each bit transmitted has a probability of being changed. These errors are often caused by thermal noise; because this thermal noise is something very general and is always present in any kind of these things. There is higher thermal noise or there may be low level of thermal noise but this thermal noise is always present. So these cause some random errors. There are some burst errors or a change in a number of bits in succession. They are often caused by faults in electrical equipment or by interference on some neighboring things. So, electrical interference is a very major cause of such burst errors. 

(Refer slide time: 04:21 - 05:17)
So, we take transmission errors as inevitable, resulting in the change of one or more bits in a transmitted frame. The rate of error may be high or low, but there will always be some error; that is what we assume. Let us just see what it means, if there is a finite probability of error for a particular bit. Let us just have a look at some notations.
Let Pb be the probability of a single bit error called bit error rate. So the probability of a single bit error is the bit error rate or b e r as they are sometimes called. P1 is probability that a frame arrives with no bit errors, that means there is no error. And P2 is the probability that a frame arrives with one or more undetected bit errors. So, P1 is no error and P2 is one or more errors.

(Refer slide time: 05:17 - 05:55)
P3 is a probability that a frame arrives with one or more detected bit errors, but no undetected bit errors. Sometimes we can detect which bits are in error and sometime we cannot. So, we will come to this later. 
 Let F be the number of bits in a frame, if no error detection scheme is used. Suppose we are not detecting it specifically.
 So P3 = 0; 
We cannot detect which bits are in error.
 So P1 = (1 ? Pb)f. 
                              
                                            (Refer slide time: 05:55 - 06:01)
If you remember, Pb is a bit error rate. Since Pb is a bit error rate probability of a single bit error, P1 would be 1? Pb for one particular bit. 


(Refer slide time: 06:02 - 06:04)
In one particular bit, the probability of error is Pb. So, the probability for that particular bit has no error, that is, 1? Pb. And we are assuming that these errors are sort of independent, which may not always be the case with burst errors. But for thermal noise, we assume that there are bit errors. We will see that the noise will introduce an error or some other error will creep in. Let us say they are independent. So if there are F bits in a frame, the probability that a frame arrives with no error is (1 ? Pb)f. 

(Refer slide time: 06:47 - 06:58)
And P2 arrives with one or more errors, (1? P1),
P2 = 1 ? P1
So, clearly longer frame and higher Pb leads to lower P1.
Let us try to look at some numbers. 

(Refer slide time: 06:59 - 07:11)
So, this is an illustrative example. A defined specification for ISDN connections ? you remember ISDN means integrated service data network which is the digital service given through the telephone lines. So that is the ISDN. So in ISDN connections, bit error rate on a 64-kbps channel should be less than 10?6 on at least 90% of the observed 1-minute intervals. Suppose that looks quite low, rate of error is 1 in 1016. That means only in a million bits 1 one bit may come in error.
Suppose we want that at most one frame with an undetected bit error should occur per day on a continuously used 64-kbps channel, with a frame length of 1000 bits. So we want not more than one frame should come with undetected bit error. So we are using this for a whole day on the 64-kbps channel and the length of the frame is 1000 bits. 

(Refer slide time: 08:59 - 09:34)

So number of frames transmitted per day = (64K/1000) ? 60 ? 60 ? 24 = 5.5 ? 106.
 So if one of them is desired frame error rate, P2 = 1/(5.5 ? 106 ) = 0.18 ? 10?6 . 
 But, if we assume Pb =10?6, then
 P1 = (0.999999)1000 = 0.999. 
 So if P1 = 0.999, P2 = 10?3, which is about three orders of magnitude too large to meet our requirements. That means it is thousand times more, although one bit in a million looks a fairly good error rate. But we find that at the end of the day, we may have a thousand frames which are in error. Well, it does not work out that way that by multiplying with 1000, we will get so many frames in error. But the point is that whatever probability we require for a faulty frame we have got a much higher probability than we were prepared. We were prepared for say one faulty data frame per day but that is not the case at all. So this necessitates the use of error detection; that means, we have to do something to bring down this error rate. The other point is that sometimes one will not have control over the environment or the transmission line directly. It is quite difficult, because if you are having a wireless transmission. Now, some noise will certainly come from some other source. May be somebody starting a car will also give out some kind of electrical noise and somebody starting a machine somewhere will give some kind of electrical noise, some  mixer or grinder running in the house may give some kind of error. All kinds of sources are there, and you do not have any control over it. So the point is that, whatever the error that is coming, you have to do something at your end to get better error rates. So, we want to detect such errors and if possible we want to correct them. So this is what we are going to discuss now. 

(Refer slide time: 11:11 - 12:49)

Error detection: One general principle of getting greater reliability or less error, as I said is always redundancy. That means you put in some kind of redundancy in the system in order to get better reliability. We have seen this when we were talking about the reliability of optical networks. But in this case, the redundancy comes in the form of extra bits that we introduce into the data, which means that whatever data is being sent, we also send in some extra bits specifically for the purpose of error detection and correction. So EDC are the error detection and correction bits which are actually redundant. That means they are not original part of the data, and they are extra bits, which have been introduced.
Suppose D = Data protected by error checking; it may include header fields.
So the data which we are protecting may or may not include it; it depends on the protocol. It may just be on the payload and the header, all together, we may like to detect the error. Error detection is never 100% reliable. But the protocol may miss some errors but usually we can bring down the level of errors to a very low figure. The larger EDC fields yield better detection. If you put larger redundancy you have better error detection and correction. 

(Refer slide time: 12:50 - 13:42)
So you have the datagram coming. We add some extra bits EDC from there. We send it to the bit error prone link, which arrives at D? and EDC?, now D may be the same as in D? and has no error and it itself may have corrupted or both may have been corrupted. But any way we run some algorithm or some method for finding out whether these are alright and if we think they are alright we say yes. Otherwise, we say that an error has been detected. Sometimes, the errors may be such that although we say everything is ok, it may not be so; in that case, we say that the error has gone undetected. 

(Refer slide time: 13:42 - 14:25)
Now for this error detection and correction, one concept, which is used, is the so-called hamming distance between the two bit patterns. And then there are hamming codes also. We will not go in to the details of hamming codes here, but let me just tell you what is hamming distance. Suppose we have two code words, like it has been shown here. This one code word is 10101110 and the other one is 0100110, these are the code words. By hamming distance, what we mean is that we have to see in how many bit positions the two code words are different. 

(Refer slide time: 14:35 - 16:07)
  
So, in order to find that, we simply XOR the two. So, if We XOR these, we get this. If you can go through 0 1 is 1, 1 0 is 1 and so on. So you get 1 1 1 0 0 0 1 1. Now we count the number of 1s in this XOR, which is 5. So we say that their hamming distance is 5.
 Please note that if the two corresponding bits are different in the two code words 0 and 1, 1 and 0, then we get a 1 in this XOR. If they are the same like 1 1 or 0 0, we get 0. This H hamming distance is the number of positions where the two code words differ. If you have a set of code words, say between these two hamming distance is 4, between these two, the hamming distance is 3, between these two also the hamming distance is 3. Let us see this differs in one position, two positions, these are the same in the third position. This is again the same. This differs in three positions. Similarly, this other H is equal to 3 for other peers. In this set as a whole, the minimum hamming distance between two code words is actually 3. H of the code word is actually 3. We take the minimum of all these H values and this comes out to be 3. So we say that for this set of code words, the hamming distance is 3. Now, if the hamming distance is 3, we have an interesting situation. Let us say these code words are what we are sending to the other side; now if there is a single error; that means if a single bit has flipped from 1, it has become 0. So what has happened is that this code word will move to another code word. So this will appear as another code word in the receiving end; and the one which is received and the one which was sent ? the hamming distance between the two will be only 1, because only one bit has flipped. So, only one position, what was 0 has become a 1 there, or may be what was 1 here has become 0 there. So it has changed 1 in one position. Assuming that there is a single bit error, the hamming distance between the transmitted and the received code word will be 1. Now if the system is known, then this is the set of code words which is being used on both the sides; and first of all you will immediately know that for this particular set of code words H = 3 was there. That means, say, this is the set of keywords that I showed you; those are the only code words, which had been sent. So the hamming distance was 3. So for whichever value, this new transmuted or new changed code that was received is not going to be a valid code, because this is only at a hamming distance of 1 from the transmitted code. So this is not going to be a member of the set. So immediately, we will detect that there has been some error. And not only that, if you assume that there is most likelihood that we have got only one error then it is the one which is at a hamming distance of 1 from this transmitted code. It will be at a hamming distance of 1 or 2 from the code, whereas it is at a hamming distance of 1 from the code word which was already sent. And if you know this and if you assume that there can be only one error then you know that that is the code word. So you can not only detect that there has been an error, you can also correct the error by going to the nearest code word from which the hamming distance is minimum, namely, the one which was transmitted. So this is the general principle. So we will see practically how this is done. There is a coding scheme, for which it goes as hamming code. We will not discuss that, but there are other schemes, but all of them use this idea of a hamming distance. 

      (Refer slide time: 19:08 - 19:50)
The simplest error detection scheme is the parity check. It appends a parity bit to the end of a block of data. What is a parity bit? It is an extra bit, which is sort of added to some data. There are two kinds of parity: odd parity and even parity. Odd number of bit errors can be detected. Why odd number of bit errors can be detected, I will explain that. If an even number of bits is in inverted due to error an undetected error occurs. The technique is not foolproof, as noise impulses are often long enough to destroy more than one bit, particularly at high data rates. 
       
     (Refer slide time: 19:08 - 23:09)
But let us first of all see what a parity bit is.	
Suppose 1 0 1 1 0 1 1 1 was the original bit pattern. In even parity, what we do is that we make the number of 1; we add a bit which is 0 or 1, depending on whether the number of 1s in the original pattern is even or odd. If it is even parity, we want to make the number of 1s even, so the number of 1s were originally even over here. So in 1 0 1 1 0 1 1 1, we have six 1s. So we already have an even number of 1s here, in the added parity bit we just give it a value 0, whereas  in the other one, that is, 1 1 0 0 0 0 0 1, we have three 1s. So this was odd, so the added parity bit was also made 1, so that overall the number of 1s in this data plus parity bit taken together is always even. That is, the number of 1s is always even in even parity if everything is fine; and the number of 1s is always odd. Similarly, the number of 1s is always odd in odd parity. Now if you just think about it a bit, if you are using even parity then all the valid code words have even number of 1s in them. So what is the number of position and what is the minimum number of positions in which they have to differ ? these two different code words? Well, the minimum number is 2; that means, the hamming distance of this set of code words will be minimum 2. And if the hamming distance is 2 and if there is a single error, what is going to come is that we are going to get a code word in the receiving end, which has got an odd number of 1s. And, if it has got an odd number of 1s, we immediately know that this could not be a valid code word. So we know that there has been some error, although we cannot say where that error is, or what the original intended transmitted code is, because if you think of two code words which are at a distance of 2 and the transmitted one ? let us say we had even parity, that is, even number of 1s; their hamming distance minimum was 2. Let us take the worst case: the minimum is 2 and then the transmitted one of this code word was this. Where one of the bits flipped, we get something in the middle, which is at the hamming distance of 1 from the two valid code words. So we know that this has been transmitted as invalid. In that sense, this is not a valid code word. So an error has occurred. But I do not know whether something from here came here or the original transmitted code was here and it came here, because we do not know where the error has occurred. So in this particular case, we can nearly detect an error. We cannot correct it at the receiving end. So the case of parity is a very simple kind of scheme. 
       
      (Refer slide time: from 23:09 to 24:22)
So once again, if you go back to the earlier slide, I mentioned that odd number of bit errors can be detected. Now as I said if there is a single bit error, it will be detected because now we have got an odd number of points. Well, if there are odd number of errors, that means if there are one error or three errors or five errors in that bit stream, then again, we will get an odd number of 1s on the receiving end. So we can detect that there has been an error. We do not know the number of errors, but in any case, we can detect it. But if there are only two errors, then it will again become an even number of ones on other side which may be a valid code word. So we will not know whether there has been any error or not. So if there are an even number of errors, this parity check will go undetected by this single parity bit error checking. Running the parity check is very easy. 
       
     (Refer slide time: 24:22 - 24:37)
You just run them through some XORs and invert it if you want it in odd parity or if you want an even parity, you can get it and this is very simple.
       
       	(Refer slide time: 24:22 - 24:59)
Now parity testing: the receiving device can work out if a bit has been corrupted in a character by counting the number of 1s in the character and parity bit. If even parity bit is being used but there are an odd number of 1s then an error must have occurred. Similarly, if odd parity is being used but there is an even number of 1s then an error must have occurred. 
			
				(Refer slide time: 24:59 - 25:37)
One drawback with parity testing errors is that if two errors occur in the same character, they will not be detected. This is because if there were an even number of 1s originally, if two bits are changed, then there will still be an even number of 1s because either two 1s will become 0. So the even number of 1s remain even or two 0s will become 1, also even number of 0s remain even or 1 1 has become 0 and 1 0 has become 1. So the number of 1s does not change and they still remain even. So it remains undetected. 

			
      (Refer slide time: 25:37 - 26:12)
There is some improvement over this and we can see how to make it more efficient. This is an extension of parity bit, which is called block parity testing. This is how it works. Suppose we have a long stream of 1s and 0s, 1s and 0s, 1 0 1 1, etc. So what we do is, we break them up into blocks of bits. Let us say 8 bits; we take the first 8 bits as the first pattern. Then the next 8 bits are the next pattern, and so on. 
	                    
                    (Refer slide time: 26:12 - 27:11)

So what we do is, for each group of 8 bits a parity can be added. In this case let us say even parity, so the first 8 bits were 1 0 1 0 1 1 1 0. So for even parity if there are five 1s here so I have to add a 1, there are three 1s here, I have to add 1, there are four 1s here; so I add a 0. In this way for each block of 8 bits, we add a parity bit. 
Then for every N blocks the parity is calculated as a new character (in this case 8 ? 8). So this is how it goes. What we do is, if you look at this as some kind of a matrix. So, we calculate the parity on this side. We calculate the parity along the column also. So five 1s, so we make it a 1 over here for this parity. (Refer slide time: 27:12 - 27:40)
Then for every N blocks the parity is calculated as a new character. I have mentioned that. I have got parity on this side. So for this block of, say, 64 characters, I have got say 8 + 8 + 1 = 16 + 1 = 17 parity bits. 
       
     (Refer slide time: 27:42 - 28:43)
Now if there is a single error, which has occurred, what is going to happen is that where ever it may have occurred in that row the parity test will fail. Similarly in the column also the parity test will fail; and since one column and one row intercept exactly at one cell, we know that that is the bit which has become corrupted. So now if an error occurs, we will know which character and which column it has occurred in. Not only can we detect the error but we can also correct a single bit error by simply changing it. So since we know that this is the bit which is in error, I can simply flip it at the receiving end itself without any referring back to the transmitter. And we know that we have got the correct one. We can not only detect errors but we can also correct single errors in block parity testing. 
			
				(Refer slide time: 28:43 - 29:46)
So this is just a better diagram for this; we have got this system. This is using odd parity. I mean there is no reason to use the same kind of parity in both the cases. You can use odd parity for the transverse parity bits and you can use even parity for this and calculate this. So if there is some error we can compute it. If there are more than one error then you may get into some kind of trouble because you will not know which bit is in error. If  there are two rows and two columns which show parity error then you will not know which row and which column will match together to get that particular bit, which has been corrupted. 
			
      (Refer slide time: 29:45 - 30:09)
So we not only have error detecting codes but we have some error correcting codes also. By   comparing error detection and error correcting codes, error correcting codes require more number of bits because we want to correct it. So we require more number of bits, which is a problem.
 
			
      (Refer slide time: 30:09 - 30:43)

In error correcting codes, rather than just detecting an error, sometimes it is useful to be able to correct an error as well. For example, CDs use error correcting codes to ensure high-quality sound reproduction even if the CD is slightly damaged. So we are talking about some music CD. So even if a few bits are in error still it will use error correcting code to handle that. We have already seen that we can correct a single error in a block parity check. There are other techniques that can correct more number of errors. 
			
      (Refer slide time: 30:44 - 32:06)
So what happens is that your one scheme could be error detection. So suppose some alphabet A was sent, this was a valid code word, and there was a noisy channel. So some error got introduced. So, 1 0 1 1 0 1 became 1 0 0 1 0 1. That means the third one from the left has become a 0. So this is an invalid code word. We know that the original one was using odd parity kind of thing, so even number and odd number of 1s were there. But in the even number there was even parity and hence even number of 1s were there. But I have got an odd number of 1s, I do not know where the error is, so what we do is, we request for retransmission; that could be one approach. But if I can correct a single error on the other side, then I do not need to ask for retransmission. Overall my efficiency may improve but of course for error correction, I have to send more bits. So that is the one area where I am sort of losing something. So it all depends on how you do your engineering and how you come to your particular point of say optimal amount of error correction. 
			
      (Refer slide time: 32:06 ? 32:41)
So another way of checking for errors was to use a checksum. This checksum is also used in other fields. So this has been borrowed not just from communication from other fields for accounting also, sometimes checksum is used. This is just used for this sum just for control purpose. So in some arbitrary order you sum it and then with all the data you also check with the sum.  

(Refer slide time: 32:41 ? 33:06)
This is a number calculated from the data and sent along with the data. If any errors occur during transmission then checksum for the received data will differ from the transmitted checksum. Of course, the data may arrive alright, but the checksum may become faulty. But even then in that case the checksum and the data will not match. So checksum can only detect errors; it will not be possible to correct it.  
			
      (Refer slide time: 33:06 ? 33:46)
A simple checksum can be calculated by adding all the data together. For example if the data is 121, 17, 29 and 47, then the checksum (add all the data) will be 214. We could transmit 121, 17, 29, 47 and 214. We know that the last number is the checksum, which should be the sum of all the numbers which have been sent. 
A checksum is usually restricted to a certain number of bits, typically 16 bits. If the checksum is longer than this, then only the lower 16 bits will be transmitted. But checksum is not very efficient and not widely used.
			
      (Refer slide time: 33:46 ? 34:38)
One scheme, which is widely used, is the so-called cyclic redundancy code. What is a cyclic redundancy code?
 A cyclic redundancy code (CRC) is a more sophisticated type of checksum. If a CRC is used, it is extremely unlikely that any errors will go undetected. A lot of different types of errors are caught by the CRC. Although the technique may sound a little complicated, in practice calculating a CRC is easy and can be implemented in hardware using linear shift registers which is a very simple kind of circuit; through that, we can compute it very easily and quite fast. So, the CRC is preferred in many data link protocols. 
			
      (Refer slide time: 34:38 - 34:49)
 So what is CRC?
 Let us just look at the details of it. 
			
       (Refer slide time: 34:49 ? 35:21)
Essentially the data is regarded as being one very long binary number. After all what you are sending is a string of 1s and 0s, and you can take a few of them and just look at it as a binary number. If you are adding the header also, then it got even more mixed up. I mean it was not to be interpreted as a number. But for CRC purpose, we interpret it as a binary number.  
			
      (Refer slide time: 35:21 ? 35:44)
So to this we add some place holder digits at the end and it is divided by a generator polynomial using modulo 2 division. So for modulo 2 division, you do the same kind of division. Only thing is that if they match then we get a 0, if they do not match we get 1. The remainder at the end of this division is the CRC. So let us just look at this. 
			
      (Refer slide time: 35:44 ? 36:48)
So given a k bit block of bits, the transmitter generates an n bit sequence, known as a frame check sequence (FCS), so that the resulting frame, consisting of k + n bits, is exactly divisible by some predetermined number. The receiver divides the incoming frame by that number and if there is no remainder, assumes there was no error. You look at it as a number; find the CRC; add it to the end; so what will happen is that you will get a longer number. This longer number is supposed to be divisible by the polynomial, which the receiver side knows. So the receiver side will do its own division and if the division is ok, then it will assume that there is no error. If it is not ok, then obviously there is some error. 
			
      (Refer slide time: 36:48 ? 37:02)
So we have D data bits and R: CRC bits. So how are the CRC bits computed? 
D * 2r XOR R. So this is done by some division. So we will see how it is done. 
			
       (Refer slide time: 37:02 ? 38:52)
This is the formula:
 D ? 2r/G. This is the generated polynomial. Why it is a polynomial ? that we will see. Actually the bit pattern is represented as a polynomial; ultimately, this so-called polynomial will be a binary number like this. Suppose the G is 1 0 0 1 and what you are trying to send is 1 0 1 0 1 1. So this is what we are trying to send; and this is the generated polynomial. What we are going to do is that we are going to add some extra bits. So this D ? 2r means, adding so many 0s at the end. 
So in this case, say, r = 3; so we have added three 0s to the end and I do a division as it is done. So 1 0 0 1 only thing is we do a modulo 2 kind of operation. So 1 1 becomes 0 1 0. As it gives 1, you got a 1 0 1 and so you got a 0 and so on. So in this way the division goes on till we sort of exhaust this and we get 0 1 1 as the remainder. And what will happen is that if you add this 0 1 1 over here, instead of sending 1 0 1 0 1 1, what we do is, we add 1 0 1. Any way, if you add it what will happen is that this will become exactly divisible modulo 2. The modulo 2 divisions will come out exactly. 

(Refer slide time: 38:53 ? 40:22)
So the CRC process can be very easily implemented in hardware using (LFSR) Linear Feedback Shift Register. The LFSR divides a message polynomial by a suitably chosen divisor polynomial; the remainder constitutes the FCS, which is added to the data bits. Commonly used divisor polynomials are:  
CRC 16 = X16 + X 15 + X 2 + 1.
So actually it is a 16 bit binary number as I said the G is called as generator polynomial. So G will have one in the sixteenth position, one in the fifteen position, one in the second position, and one in the first, that is, 0th position, which means that there are going to be four 1s in this G and all the rest are going to be 0. So how it is generated polynomial?
So we are not going into that, how this particular polynomial, why was this particular polynomial chosen rather then some other polynomial. But as we have seen , they cover a lot of different types of errors. Previously if you remember, in our parity bit we were just getting odd numbers of errors. If there are an odd number of errors then we can detect it here and we can detect a lot of different cases of error. So it is extremely unlikely that if the CRC is alright, then there has been some error. So there are different protocols and different systems with different generator polynomial.

(Refer slide time: 40:57 ? 41:02)
So CRC CCITT = X16 + X12 + X5 + 1  and so on. 
			
       (Refer slide time: 41:03 ? 41:35)
From the above slide we can see CRC 32 and CRC 8. So CRC 8 means this is an 8-bit thing. This is the eighth position, second position, first position, and zeroth position are all 1s, rest are 0s. So this is used in ATM. CRC 10: this may also be used; and CRC 12. So these are the different CRCs. Actually not all polynomials will give you the same kind of error coverage; that is known, and people have found out some properties which are good properties to have in a generator polynomial and then people have found out some good polynomials which will cover a lot of errors. So HDLC uses some kind of errors. 
			
       (Refer slide time: 41:59 ? 42:09)
 So CSMA/CD, FDDI, ATM ? they use CRC 32 and so on. I am just showing you, but you need not remember any of these.
			
       (Refer slide time: 41:59 ? 42:25)
Now, we come to a slightly higher level, data link protocols. We are still talking about errors and error control, but the point is that, we have been talking about this parity and CRC that is error control at a lower levels. We say a lower level because it is a small amount of error. We can possibly handle it using such error detection or error correction kind of schemes. But what might happen is, because of a very vast noise or something it may garble the data so badly, that it is entirely unrecognizable on the other side. Even if you have an error correction scheme, because none of the error correction and error detection schemes are 100% infallible. So there is still a possibility that there will be some undetected errors and then there will be some data which becomes so bad that there is no question of any error detection or correction. We require some higher-level schemes also in addition to whatever we have talked about this parity CRC etc. So we will now see higher layer data link protocols. 
			
       (Refer slide time: 43:34 ? 44:07)
 Once again, data link protocols could be of different types; 
for example, unrestricted simplex protocol: this does not do much as we will see; stop and wait protocol; stop and wait ARQ for a noisy channel means that ARQ stands for acknowledgement request. That means an acknowledgement is expected. One bit sliding window protocol: this is similar to stop and wait, sliding window with go back N ARQ, sliding window with selective reject ARQ, and so on.
						  
       (Refer slide time: 44:07 ? 44:57
The first is Unrestricted Simplex Protocol. Well, it simply sends, it does not do any kind of error control, and so this is the simplest situation. We have a sender to fetch a packet from a network layer, construct a frame; and send the frame to the physical layer. So the sender is just going on sending without bothering about what is happening on the receiver end. So if things are alright, the receiver waits for an event, receives a frame from physical layer and passes the packet to network layer. The faulty things, which have gone in, may be it is the responsibility of higher layer systems to take care of that. But the higher layer application does not bother, if a few bits are bad say in the case of a voice channel. 
			
       (Refer slide time: 44:58 ? 45:02)
			
       (Refer slide time: 45:03 ? 45:04)
			
       (Refer slide time: 45:05 ? 45:46)

When data is sent as a sequence of frames, two types of errors can occur:
One is lost frame: the frame has been lost entirely and the frame fails to arrive at the receiver. This may be due to a noise burst destroying the frame beyond recognition. It may have a recognizable frame arriving at the receiver but some bits are in error. A frame may be damaged or a frame may be entirely lost. See sender and the receiver. The sender is sending a number of frames one, two, to frame n. Frame one is going first. So the point is that some of the frames in between may get lost all together. We have to make the worst case assumptions. 

(Refer slide time: 45:58 ? 46:11)
Error control is concerned with insuring that all frames are eventually delivered (possibly in order) to a destination. So, how?
 Three items required are acknowledgment, timer and sequence number. 


(Refer slide time: 46:12 ? 46:57)

Acknowledgment ? what do we do with acknowledgment? 
Typically reliable delivery is achieved using the ?acknowledgments with retransmission? paradigm, whereby the receiver returns a special acknowledgment (ACK) frame to the sender indicating the correct receipt of a frame; that means the receiver, after receiving the same will send an acknowledgment. In some systems, the receiver also returns a negative acknowledgment for incorrectly received frame. So this is a hint to the sender, so that it can retransmit a frame right away without waiting for a timer to expire. We will come to this timer to expire. The point is that there is a two way communication. The sender is sending the frames, and the receiver is sending the acknowledgments. If it gets a damaged frame, then it will send a negative acknowledgment. Of course, if it does not get any frame, then it will not know whether anything was sent at all. In that case, it may not send any acknowledgment or negative acknowledgment anything at all.  

(Refer slide time: 47:20 ? 48:05)The situation is something like this: suppose 0 was sent by the sender and it arrived. The receiver sent an acknowledgment 0 then a 1 was sent. Suppose I am just taking this bit by bit, just to make it simpler. Suppose 1 was sent it came with some error, we do not know about what kind of error it is. So the frame number 0 was acknowledged, frame number 1 was received, but it was damaged, so a negative acknowledgment was sent. That means, frame number 1 was not rightly received, so frame number 1 is going to be retransmitted by the sender. This is the argument.

(Refer slide time: 47:20 ? 48:50)

The other thing we require is a timer. One problem simple acknowledgment, negative acknowledgment schemes fail to address is recovering from a frame that is totally lost, and as a result, fails to solicit an acknowledgment or a negative acknowledgment. What happens if an acknowledgment or negative acknowledgment becomes lost?
 Retransmission timers are used to resend frames that do not produce an acknowledgement. When sending a frame, schedule a timer to expire at some time after the acknowledgment should have been returned. If the timer goes off, retransmit the frame. 

(Refer slide time: 48:09 ? 50:51)
So this is the scheme, you see a frame which was sent was lost. So the receiver of course did not know anything. What happens is that as soon as the frame was sent, at the same time a clock was started in the sender?s end. I have to assume that the system has transmission time, and this is the time it might require for the receiver to process it and send an acknowledgment. Then the acknowledgment will take some time to reach the sender. So, all that is taken into the account with some amount of time thrown in, and that is set in the timer. So if the timer goes out that means, if there is a time out, now the sender knows that the frame must have been lost, otherwise an acknowledgment would have come. So he sends the frame again. The other thing that might happen is that suppose the frame was sent the 0 th frame was sent it was received and then an acknowledgment was sent, then the acknowledgment was lost. He would not know whether reached at all or the acknowledgement was lost. So after a time out, he will send this frame again. So this is a case where the frame has been duplicated. Depending on what kind of protocol we are using the receiver may or may not understand that this frame is actually a duplicate of this. Maybe two such exactly similar frames were supposed to come one after the other because it is after all coming from some application, which is absolutely unknown to the data link layer. So the frame may get duplicated. But then there are some protocols which take care of that where if the receiving side will know that this is actually a duplicated frame. We will come to that protocol later on. 

(Refer slide time: 50:51 ? 51:39)
And the third technique that we use is the so-called sequence number. So retransmission introduces the possibility of duplicate frames. To suppress duplicates, add sequence number to each frame, so that a receiver can distinguish between new frames and old copies. So that if the frames are numbered sequentially like say 0, 1, 2 etc. as I was showing you the case where the receiver gets two frames which have the same number 0, it knows that acknowledgment must have got lost or something like that happened. The sender sends the same frame twice, so he will discard just one of them. 

(Refer slide time: 51:40 ? 52:01)
So collectively, the mechanisms stated are referred to as automatic repeat request (ARQ). The objective is to turn an unreliable data link into a reliable one. 
Three versions of ARQ have been standardized: stop and wait ARQ, Go back N ARQ, and Selective Reject ARQ.

(Refer slide time: 52:02 ? 52:39)
 So we have the stop and wait ARQ, which is based on the stop and wait flow control technique. The source station transmits a single frame and then waits for an acknowledgment to arrive. No other data frames can be sent until the destination stations reply arrives at the source station. So it waits for the acknowledgment; if the acknowledgment does not come it will resend after a automatic repeat request. So it will automatically repeat the frame. 

(Refer slide time: 52:40 ? 53:39)
So this is the scheme, we have the sender which fetches a packet from network layer, construct a frame, send the frame to physical layer, wait for an event. So if it gets an acknowledgment  then it is fine then it is going to send a second frame. If it does not get an acknowledgment it will send the same frame again. So until it gets an acknowledgment or the timer runs out, the frame which was already sent that has to be stored locally in some buffer in whichever system is handling this data link control. And the receiver side also waits for an event, receive a frame from the physical layer, pass a packet to the network layer, send an acknowledgment to the physical layer. So this is the stop and wait protocol. This is a very simple protocol. Unfortunately, this is not very efficient as we will see that later.  

      (Refer slide time: 53:30 ? 53: 35)
And so we will continue our discussion on this error control and then we will move on to flow control in the next lecture. So this is a simple kind of protocol but this has got efficiency problem, in the sense that this is not very efficient. So we will continue this discussion in the next lecture. Thank you.
Preview of next Lecture
Lecture # 17
Stop and Wait Protocol
Good day, so in the last lecture we were discussing about stop and wait protocol .we have seen what it is. It is really a simple kind of protocol which is used both for error control as well as for flow control in some cases. But since this is a simple protocol we will look at its performance now. 

(Refer slide time - 54:34 ? 54:38)
So we will first finish our discussion on this stop and wait protocol. 

(Refer slide time: 54:39 -55:21)
And this we have already seen , so each  sender station sends one  packet which  are ACK?ed. If  it is comes with some error it may send a  negative  acknowledgement , it may not arrive at all .  In that case the sender will time out in either case that means, it is getting a negative acknowledgement or whether it is getting a time out. It will retransmit that frame.  Retransmit whichever frame was just transmitted earlier and the point to note is that there is only one frame, which is in transit in the channel at any point of time. Whether it is the frame or the acknowledgment, now we have discussed some of the major techniques which are used in the data link layer, so what we will do now is that we will look at just one more protocol. This is somewhat general version that we have discussed.  PPP that is the point to point protocol which is used for point to point communication but there could be a point to multi point communication also.  For this we have this high level data link control, this is the name of the protocol. So this is the name of the protocol or HDLC in short.  I mean this is not so high level any longer this is not considered, so high level any longer but even then this is an important protocol. So there are some variations of this,  which  are used in various places. So we will just look at some generic the way how this HDLC is important. 

(Refer slide time:56:32 - 56:59)

Now this is an important data link protocol this HDLC or some variant   say this is widely used, it also is the basis for many other important data link control protocols.  So we suggest three types of stations. One could be primary station this is responsible for the operation of the link. By  the way  you  do not  have to remember  all these  because when  you are  trying  to use it  or design  it for some  particular  situation you can always look up the reference  and  see what exactly the different frames  are what are their formats etc . I am trying to give you  a  flavor  of the kind of information about which  a typical protocol  would need  to exchange.  So  with  this example  we  come  to the end of this general data link control, but still  we  have seen  some  examples of data link control, for example we have seen some examples of MAC  like with token bus,  token ring, etc. We will see other examples of MAC as we take up other kinds of networks. Similarly we have seen some kind of error and data control.  By the way the similar approach is used in some other higher layers also. We will talk about it when we come to it and we will talk about more specific and more widely used systems in the next class onwards. So  in  the next class  what we will do  is,  we will look at  another  type of communication  and how MAC is done on that in satellite communication .So that will be the next topic. Thank you.



 
Computer Networks
Prof. Sujay Ghosh
Department Of Computer Science and Engineering
IIT Kharagpur
Lecture Name #17
Stop & Wait Protocol
(Refer slide time - 00:40)
Good day. So in the last lecture we were discussing about Stop and Wait Protocol. It is really a simple kind of protocol, which is used both for error control as well as for flow control in some cases. This is the simple protocol we will look at its performance now. (Refer slide time: 01: 06 - 01:08) 
  
So we will first finish our discussion on this Stop and Wait Protocol. (Refer slide time: 01:12 -02:01)


And this we have already seen. So each sender station sends one packet which are ACK if it is comes with some error it may send negative acknowledgement. It may not arrive at all in that case the sender will time out; in either case, that is, when it is getting a negative acknowledgement or whether it is getting a time out it will retransmit that frame transmitted earlier. The point to note is that there is only one frame, which is in transit in the channel at any point of time, whether it is the frame or the acknowledgment or the negative acknowledgement. So this is the Stop and Wait ARQ, which we have automatic repeat request we have already discussed this. (Refer slide time: 02:03 - 02:39)
 

And we have seen that the main advantage of Stop and Wait ARQ is its simplicity. And the disadvantage is that it is a highly inefficient use of communication links. So we will see what its efficiency is. This protocol is sometimes also known as Alternate Bit Protocol. It?s called Alternate Bit because one person sends with a bit 0; the other person makes it 1 and sends it back; so this way 1 and 0 alternate. So you know that when you have sent an acknowledgement, which acknowledgement it is. (Refer slide time: 02:42 - 03:39)
 

Now let us look at link utilization in Stop and Wait ? how much we are utilizing the link. With the use of multiple frames for a single message, the Stop and Wait protocol does not perform well; only one frame at a time can be in transit. So let us illustrate this with an example. Suppose the transmission time, that is, the time it takes for a station to transmit a frame ? this is an important parameter ? let us normalize it for the time being to a value of 1. By transmission time, we mean the time the sender takes from the time it starts sending the first bit of the frame up to the time it takes to transmit the last bit of the frame. So this depends on the rate at which the sender is pushing the frame into the channel. So we will take it is a normalized value of 1; that is one component of the time. (Refer slide time: 03:40-04:58)

And the other component is the propagation delay, the time it takes for a bit to travel from the sender to the receiver. There will be always be a finite propagation delay, because, depending on how long the channel is, depending may be on the distance, or the speed at which the signal travels, etc., there will be a propagation delay ? the time it takes for the first or any bit to move from the source to the destination. Let us call this A. Now if A<1 ? remember we have normalized our transmission time to 1 ? which means that the first bit of the frame has already reached the receiver when some of the latter bits are being sent by the sender. So the frame is sufficiently long such that the first bits of the frame arrive at the destination before the source has completed transmission of the frame. And if A>1, that means the sender completes transmission of the entire frame before leading bits of the frame arrive at the receiver. Of course, the sender now has to wait for the acknowledgement to arrive. (Refer slide time: 04:59-05:40)

Out of this time, only tframe is actually is spent transmitting data and the other time is idle. Therefore, the efficiency of the utilization is tframe /TD, which is the total time. TD has two components, one is tframe, time to send the frame or transmission time, plus twice the propagation time. Why twice? Because the signal or the message or the frame has to reach the destination and the destination has to send an acknowledgement back. So 1 tprop this way and 1 tprop that way; that is why twice t prop. So that is the efficiency. (Refer slide time: 05:41-06:13)

Now if we define a = tprop/ tframe, which means that if a = 1, we have the propagation time equal to the time frame; so it just fits. Anyway, U = 1 + 2a; if you look at the previous formula, if we divide the numerator and denominator by tframe, we get one from here one here and t prop /t frame.

That is our a. So U = 1 + 2a. So this is the efficiency. (Refer slide time: 06:19 - 07:02)

So well let us just see an example with some numbers. A channel has a bit rate of 4 kbps and a propagation delay of 20 ms.For what range of frame size does the stop and wait protocol give an efficiency of 50%? My channel has a bit rate of 4 kbps, that means, this is the rate at which we are pumping the data into the channel. This is the maximum rate the channel can carry and it has got a propagation delay of 20 ms. So I want to find the optimum frame size for getting 50% efficiency. (Refer slide time: 07:03-07:11)

So R the rate is 4 kbps tprop is 20 ms we want to find L that is the length of the frame. (Refer slide time: 07:12-07:45)

So U = tframe/ (tframe+2* tprop). So we want U the efficiency to be 0.5. So 0.5 = tframe/ (tframe+2* tprop). So t frame = 2*t prop = 2 ? 20 ms, which is 40 ms. So t frame better be 40 ms. Then we will get a value of 0.5 on this side. (Refer slide time: 07:46 - 08:45)

So length of the frame = R (rate at which we are pumping in) ? tframe which is 4 ? 103 ? 40 ? 10?3 and this 40 is in ms. So all this 10?3 cancels so we get 160. Thus for frame sizes L = 160 bits the Stop and Wait Protocol gives an efficiency of 50%. Please note that if L = 160 bits then your tframe = 40 ms, which is 2(tprop). (Refer slide time: 08:46- 10:36)

Now we come to the topic of flow control. If you remember, our data link layer had these several functionalities, chief functionalities, and one of them was framing, which we have discussed. Then there was an error control; associated with error control or somewhat closely linked with error control, we have the flow control. So the flow control is the other functionality; we will see how flow control is achieved ? some schemes for that. Flow control is a technique for assuring that a transmitting station does not overwhelm the receiving station with data, which means that transmitting station is transmitting at a very fast rate and the receiving station cannot receive it or cannot process it; its buffer over flows etc. So the receiving station gets overwhelmed and some of the frames or some bits may get lost. So this is what we want to avoid, which is the topic of flow control. The receiver typically allocates a data buffer of some maximum length. You cannot store an arbitrary amount of data on the receiving side; so there will be a finite, quite small buffer on the other side. So if the receiver cannot process; it first goes into the buffer, but then since we have a small buffer or you may have limited sized buffer, eventually the buffer will overflow and this is what we want to avoid. So after receiving data the receiver must do a certain amount of processing before passing the data onto the higher-level software. Higher-level software may be the network layer. (Refer slide time: 10:37 - 10:57)

Now this is an example why flow control is necessary. This data is torrentially coming from the above; the receiving side has just got this much buffer. Buffer is full, and then what ever  is coming, they are simply discarded. There is no way to accommodate them. (Refer slide time:10:58 - 11:31)

One way of seeing this is the same Stop-and-Wait flow control just as we use Stop-and-Wait for error control. Similarly the Stop-and-Wait can be used for flow control; it is the same thing. So this is the simplest form of flow control. The source transmits a data frame. After receiving the frame, the destination indicates its willingness to accept another frame by sending back an ACK frame, acknowledging the frame it just received. The source must wait until it receives the ACK frame before sending the next data frame. (Refer slide time: 11:32 - 12:24)

It works fine when a message is sent in a few large frames as we have seen. But if we have a large number of frames to send, then of course what happens is that its efficiency becomes a factor. So this is not a very good situation for that case when we have to send a large amount of data or a large number of frames specifically. Often, the source splits a large message and transmits it in many frames. Buffer size of the receiver may be limited. Handling errors for smaller frames is cheaper, because if you have a very large frame your error correction and error detection will become a problem. On a shared medium like LAN, large frame sizes are not desirable. (Refer slide time: 12:25 - 13:25)

So we sort of make a more general case of this stop and wait protocol by giving a finite amount of buffer. One way to look at stop and wait is that on the receiving side we have a buffer of size 1. If you have a larger buffer, then in this case, you have to have a buffer on both the sides; you have to have a buffer on the transmitter side and the receiver side. So in stop and wait flow control, if a>1, serious inefficiencies result. Efficiency can be greatly improved by allowing multiple frames to be in transit at the same time. So suppose two stations A and B are connected by a full duplex link. The link has aroused on both sides, which means that we take them to a full duplex because the message has to flow from A to B the ACKs have to come from B to A. (Refer slide time: 13:26 -16:16)

Station B allocates buffer space for n frames. Thus B can accept n frames, and A is allowed to send n frames without waiting for any acknowledgement. This is alright because on the receiving side we have n buffer locations. I am assuming that each location can contain one full frame. So n frames can be pumped in even if the receiver does not process any of them but then at least they can be accommodated in the buffer; so it will not overflow. The transmitter can also transmit n frames. But why does the transmitter require a buffer? Because if you remember, the frames which the transmitter has sent have to be stored in the transmitter side because they may have to be retransmitted. Retransmission may be due to the frame getting a lot of noise, which cannot be corrected on the receiving end. You may get a negative acknowledgement; that is one thing. Secondly, the frame may get lost altogether. Thirdly, the acknowledgement ? this frame may have reached but the sender would know that the frame has reached only after it gets the acknowledgement and the acknowledgement, which the receiver might have sent, may have got lost. So, till it gets an acknowledgement, the transmitter has to store the frame which has already been sent. So on the transmitter side also we require a buffer if n frames are to be in transit. Then the transmitter has to store all of them because any of them might have to be retransmitted. So A can send n frames without waiting for any acknowledgement. Each frame is labeled with a sequence number to keep track of the frames, which have been acknowledged. Because now since more than one frame may be in transit, may be the first frame is lost and the second frame has reached. Now the receiver somehow has to know that this is the second frame if the first frame has been lost. Otherwise it will send an acknowledgement and the transmitter would think that the first frame has been acknowledged. So you have to identify what is it that you are acknowledging in this particular case so you have to have a sequence number. The acknowledgement comes with this sequence number or some sort of small variation of it, so that the transmitter knows what is it that is being acknowledged. (Refer slide time: 16:17-17:59)

So B acknowledges a frame by sending an ACK that includes the sequence number of the next frame expected. This is what I mentioned as a slide variation. Instead of directly acknowledging a particular frame, let us say frame number 53, instead of sending ACK 53, it sends ACK 54. That means 54 is the next sequence number, which is expected in the next frame. So it means that up to 53 everything has been received. That is the small variation instead of the sequence number of the next frame expected. This also explicitly announces that B is prepared to receive the next n frame. Let?s say beginning with the number specified which means suppose n was 10 ? B is the receiver ? so when B gets the frame number 53, the next frame it is expecting is really 54 so it sends an ACK 54, which also means that B must be having, let us say, this buffer size of 10. So from 54 to 63, it is ready for all these packets; it is not ready for frame number 65 because the buffer size is only 10. (Refer slide time: 18:00-19:16)

This scheme can be used to acknowledge multiple frames. B could receive frames 2, 3, 4 but withhold ACK until frame 4 has arrived. By returning an ACK with sequence number 5, B acknowledges frames 2, 3, 4 at one time. So in this protocol, when a particular frame is acknowledged ? say frame number 53 is acknowledged indirectly by sending ACK 54 ? that means all frames up to 53 have been received. So one particular ACK may act for a number of frames; this also has an advantage that if one of the ACK frames, suppose 3, was acknowledged; and 4 was acknowledged. The acknowledgement for 3 had for some reason vanished; that means it got killed on the way. But say if the transmitter gets an ACK 5, that is acknowledgement for up to frame number 4 that means 2, 3, 4 everything has been received. (Refer slide time: 19:17-21:14)


So now we come to the sliding of this window. So the sender maintains a list of sequence numbers that it is allowed to send; this is the sender window. Receiver also maintains a list of sequence numbers that it is prepared to receive; that is the receiver window. So the buffers on the two sides represent the two windows. So on the transmitter side these are the frames, which the transmitter is allowed to send if transmitter has got all these frames from the higher layer. But it can accommodate only that many frames now in its buffer. So that is the window of transmission. And the receiver also has a window in which there are the frames which it is now ready to receive. Now what the transmitter might do is that it might transmit some of these frames and they are in transit, the receiver window will still be the same because the receiver still has not got them. Now as it gets them one by one, the receiver windows slides and as these ACKs reach the sender, the transmitter?s window slides. So we have a sender window and a receiver window. Since the sequence number to be used occupies a field in the frame, it is clearly of bounden size. So you cannot have arbitrarily long sequence numbers. So even if you are sending a very large number of frames, your sequence number will be bounded depending on how many bits you are using for this sequence number. (Refer slide time: 21:15-21:43)

So for a k bit field, the range of sequence numbers is 0 through 2k?1 and frames are numbered modulo 2k naturally. So now the frame numbers are going to repeat because of this bounded size of k. If k = 3, after sequence number 7 you have to go back to sequence number 0. The next slide shows a depiction of the sliding window for three bit sequence numbers. (Refer slide time: 21:44-22:54)

The actual window size need not be the maximum possible size for a given sequence number length; it could be less actually. So for a three-bit sequence number, a window size of 4 can be configured. If two stations exchange data, each needs to maintain two windows. To save communication capacity, a technique called Piggybacking is used. So this is the case we are talking about when both sides are trying to communicate; previously also both sides were communicating in the sense the so-called receiver was sending an ACK. Now let us say suppose two-way communication is going on. We need a pair of buffer locations, that is, a pair of windows on both sides. So we want two sets of buffers on this side for the two windows and two sets of buffers on the other side for the other two windows. On each side, we will have one receiver window and one sender window. And in this particular case, another thing we can do is that we can piggyback the acknowledgements. So you need not send an acknowledgement frame by itself. (Refer slide time: 22:55-24:39)

So this is Piggybacking. Each data frame includes a field that holds the sequence number of that frame plus a field that holds the sequence number used for acknowledgement. If a station has an ACK but no data to send, it sends a separate ACK frame. So if the communication is only one sided, that means one transmitter and one receiver, the receiver only sends an ACK frame; but if the communication is from both sides, what they can do is that for the communication that is messaged from here to here, its corresponding ACK piggybacks on the messages or the actual frames, which are going from this side to this side. So you need not send an acknowledgement frame by itself, which of course, saves a lot of time, because if you remember in stop and wait the acknowledgement was travelling all alone. But for that, it took all its tprop, that is, the propagation time. But all that is because you are using it for sending a real data, so that way piggybacking is nice. Of course, you must remember that if you are using piggybacking and if you are sending some packet and you do not have anything to acknowledge, still you will have to put in something in those bits. Otherwise, those bits will have some values; anyways, it cannot be blank; each of the bit will be either a 0 or 1. So it may be misinterpreted on the other side so something will have to be sent. (Refer slide time: 24:40-25:52) 

So this is what the picture looks like for sliding window. These are the frames which are to be sent 0,1,2,3,4,5,6,7 assuming there are only 3 bits for the sequence number; again the numbering starts with 0,1,2,3,4. This 0th frame and this 0th frame are really two different frames. So these are the frames 7,0,1,2,3,4, etc., which have already been received. If the frames have already been received, what happens is that up to 3, suppose this frame has been acknowledged. So the  first unacknowledged frame is here and suppose you have a window size of, let?s say, 4 or 5. It is 1,2,3,4,5 ? so up to this is your window size; once he sends the acknowledgement this window will slide and once he gets the acknowledgement of 4 then this window will slide. (Refer slide time: 25:54-26:40)


There are some variations of these acknowledgements we can do; we will discuss that. These variations just try to make it a bit more efficient. We will just look at a couple of variations now. One is Go-back-N ARQ. This is once again based on the same basic sliding window protocol, which is mostly commonly used. A station may send a series of frames sequentially numbered modulo some maximum value as usual. The number of unacknowledged frames outstanding is determined by window size, using the sliding window flow control technique. (Refer slide time: 26:41-27:56)

While no error occurs, the destination will acknowledge incoming frames as usual using receive ready or RR frames. If the destination detects a frame error, or it receives a frame out of order, it sends an NAK or negative acknowledgement for that frame using reject or REJ frame. So it sends a negative acknowledgement for a particular frame if it is received badly. Previously we were only sending ACKs or not sending anything at all; now if we have an erroneous frame, we send a negative acknowledgement for that. And Go-back-N means negative acknowledgement in sending the particular frame that may have come in as faulty. Now what the sender will have to do is that the sender will have to go back to this particular frame and retransmit from that frame onwards all the other frames, which it might already have transmitted. So that is the Go-back-N.  (Refer slide time: 27:57-28:48)

The destination will discard the frame in error and all future frames until the frame in error is correctly received. If it has got an erroneous frame and it might have got a couple of frames or later frames which may have come in, they are discarded because until and unless the erroneous frame is correctly received it will not move. So the source station, on receiving an REJ, that is a reject, must retransmit that particular frame, which went in an erroneous form, plus all succeeding frames. That is the Go-back-N.  (Refer slide time: 28:49-29:26)

Let us just quickly look at the window size limit for Go-back-N ARQ. For a k bit sequence number field, the maximum window size must be limited to 2k?1. Why is it that the window size cannot be greater than 2k?1? This is the reason. Suppose that data are being exchanged in both directions. Station B must send piggybacked acknowledgements to station A?s frames in the data frames being transmitted by B. This is required even if the ACK has already been sent, since B must put some number in the ACK field of its data frame. (Refer slide time: 29:27-29:54)

Assuming a 3-bit sequence number suppose that a station sends frame 0 and gets back an RR1. That means from 1 onwards it has to send back all the frames. So 1,2,3,4,5,6,7,0 it has to send all these and get another RR1. (Refer slide time: 29:55-31:49)

So this might either mean that RR1 is a cumulative ACK for all 8 frames received correctly, or all 8 frames were damaged, and the receiver is repeating its previous RR1. A receiver might have to repeat the previous acknowledgement because it has to put something in that acknowledged field. This problem can be solved if the maximum window size is kept at 7, so that they will not overflow into the next set of sequence numbers. That is why the window size has to be kept smaller than this 2k?1. If you make a very large window size, there will be a number of packets here or frames here with the same number. So if you get an acknowledgement or negative acknowledgement, you will not know which frame it is that is being talked about. Now people tried to improve on this because, if you remember, in GO-back-N, when you get a negative acknowledgement the transmitter has to go back to that frame, send that frame again and send all the succeeding frames again. Although it might have transmitted them once and actually what might have happened is that the receiver might have received them also in correct order, but since the receiver is not moving the transmitter has to retransmit all that. Though the frames  were received correctly earlier they are rejected. In order to adjust this people then thought was to allow selective reject, that means, one particular frame is rejected; and some subsequent frames are accepted. (Refer slide time: 31:50-32:55)

So here the only frames retransmitted are those that receive a negative acknowledgement, in this case called selective reject, that is, SREJ or time out. Of course if an acknowledgement does not come and times out, it has to be sent again because the frame might have been lost altogether. But otherwise, only those which get a specific negative acknowledgement are sent again. So this would naturally be more efficient than Go-back-N. Receiver requires storage buffers to contain out-of-order frames until the frame in error is correctly received. So if a particular frame is wrongly received and then some subsequent frames are correctly received now they have to be stored in the buffer so that when this wrongly received frame is retransmitted by the transmitter and it comes in a proper shape over here it has to be inserted in the middle of that list somewhere. (Refer slide time: 32:56-34:17)

If this inserting into this buffer management; inserting into the buffer; and taking out of the buffer, etc., are being done by hardware, then you have to have the logic circuit for putting things inside the list somewhere. Similarly for the transmission side it may have got all these frames lined up, which have already been transmitted. Out of that, one of them gets negative reject; so instead of the one at the head of the queue, something from middle may have to be taken out and retransmitted. So the transmitter must also have that capacity. The receiver must also have the appropriate logic circuitry needed for reinserting the frames in the correct order. And transmitter is more complex because it must be capable of sending frames out of sequence. Receiver is also slightly more complex. Another drawback is that the maximum window size can be no more than half the range of sequence numbers. This is the small problem in selective reject. Previously remember it was up to 2k?1; now we can use only half of that and not the full thing; we will see why. (Refer slide time: 34:18-35:23)

Assume 3-bit sequence numbers as in the previous example and a window size of 7. Consider the following scenario: Station A sends frames 0 through 6 ? because we have a window size of 7 so we can send frames numbered from 0 through 6 ? to station B. B receives all 7 frames and cumulatively acknowledges with RR7. So it has received and the next one it is expecting is the 7th one. RR7 unfortunately gets lost in transit. Station A times out and retransmits frame 0. It has not received any of the acknowledgements because for 0 through 6 all the acknowledgements were clubbed together and one RR7 was sent, which unfortunately got lost in transit. But station A has not got any acknowledgement so it is holding all these original 0 through 6 frames in its buffer so now it times out and retransmits frame 0.  (Refer slide time: 35:24-36:12)

B has already advanced its received window to receive frames 7, 0, 1, 2, 3, 4, 5. Now B wrongly assumes that frame 7 has been lost and that this is the new frame 0, and accepts it. So this old frame 0 is interpreted as the new 0 number frame by B and it assumes that 7 has been lost in transit so may be later on a negative acknowledgement or selective reject for 7 would be sent. But this 0 frame now has gone out of order. So the problem arose due to overlap between sending and receiving windows. For k bit sequence numbers, maximum window size therefore has to be smaller. (Refer slide time: 36:18-38:25)

Go-back-N and Selective Reject: these are the diagrams for the two and we already know what these are. Then frame number 2, which is actually the 3rd frame, came in in an erroneous fashion; an error was detected and the negative acknowledgement was sent. Once the negative acknowledgement is sent, although 3,4,5 had been sent and may be they have been reaching over here, this window over here has not moved beyond 1. So it sends 2 again and all these 3,4 are being sent again over here ? 2,3,4. These are discarded on the receiver side although they were correctly received, and this 0,1,2,3,4,5, etc., goes on. This was the Go-back-N scheme. In the selective reject scheme,  0, 1 was received as usual; the third frame, number 2, came in erroneous so a negative acknowledgement or NACK 2 was sent. So when it gets a NACK 2, 2 will be retransmitted but 3, 4 have already been received correctly and acknowledgement of 3 and acknowledgement 4 have also reached the transmitter. So the sender will now send frame number 5, 6 and so on. So please note the difference ? here it has to send this 2,3,4 again. Here this 3,4 need not be sent again, 2 of course has to be sent again because it was specifically rejected but then you can go on with 5,6, etc. So that is why this one is slightly more efficient than Go-back-N. (Refer slide time: 38:26-39:38)

This is another picture of how the window slides. So what happened is that this was the window. 0,1,2 have been sent and as 0,1,2 have been sent, the receiver window has slid and then it has ACK2. After 0,1 it sends an ACK2 and then it gets 2 also. Suppose 3 were lost. So although it might have received 4, 5 after that, they are discarded in Go-back-N. And when there is a selective reject, the difference is that suppose any number 3 got lost on the way, 4 and 5, which are coming over here are buffered, so that when you get back at 3, you can plug 3 in its proper position and the window can slide very quickly on the receiver side. (Refer slide time: 39:39-39:48)

 Maximum window size is going to be 2k?1 in case of go-back. (Refer slide time: 39:49-40:56)

We have discussed some of the major techniques, which are used in data link layer. What we will do now is that we will look at just one more protocol. This is a somewhat general version. We have already discussed PPP, that is, point-to point protocol, which is used for just point-to-point communication. But there could be a point-to-multi-point communication also; for this we have this high level data link control. This is the name of the protocol, or HDLC in short. This is not considered so high level any longer; but this is an important protocol. So there are some variations of this which are used in various places. (Refer slide time: 41:01-42:10)

So this is an important data link protocol. This HDLC or some variant of this is widely used. It is also the basis for many other important data link control protocols. Its formulation has three types of stations: One could be primary stations. These are responsible for the operation of the link. That means primary station is something like a master; for setting up of the link, and for all other control purposes, etc., the primary station holds the key responsibility. So there is some kind of a master; it may not be the only one master. We will come to that later on. But primary station is the one who will initiate the setting up of connections, etc. Frames issued by primary station are called commands. (Refer slide time: 42:11-43:28)

And then we have secondary station; it operates under the control of primary station. Frames issued are called responses. Just as the frames issued by the primary station are called commands, the frames issued by the secondary stations are called responses. Primary maintains a separate logical link with each secondary. This need not only be point-to-point, this could be a point-to-multi-point communication also. That means there is one primary station, may be a number of secondary stations. So the primary is communicating may be with each of them may be on the same medium. If that is so, then actually the primary is maintaining a virtual link with each of the secondary. So it will maintain its own data, etc., for each of this virtual connection this primary station has to do. Secondary station is of course under one primary, so this multipoint business does not come into picture on the secondary side. And then, we can we have combined station; it combines the features of primary and secondary. In that case, it may issue both commands and responses and combined stations actually are very common. 

And then there can be two types of link configurations. Unbalanced configuration consists of one primary and one or more secondary stations. So if you have one primary and may be one secondary station, this is more like a point-to-point link. If you have one primary and a number of secondary stations, then it is more like a point-to-multi-point link. An example of a point-to-multi-point link would be suppose if you have a server and a number of terminals, which are all hanging on the same cable and the sever is holding. In this case, the server would be the primary station and these terminals would be all secondary stations. They are communicating only with the primary station, which is the server, and the server maintains a virtual link with each of them. The terminals or the secondary stations, just maintain the link with the primary station. So this is an unbalanced configuration. In a balanced configuration, this consists of two combined stations. Combined station is the station, which can act both as a primary station as well as a secondary station. So in a balanced link, on both the sides we have combined stations, both of them can act as primary as well as secondary. (Refer slide time: 45:01-45:31)

There are three data transfer modes. One is normal response mode, which is in the unbalanced case; it is used with unbalanced configuration. Primary may initiate data transfer to a secondary. As I mentioned before, it is the responsibility of the primary station to initiate data transfers. A secondary may only transmit data in response to a command from primary. So this is the normal response mode used with unbalanced configuration. (Refer slide time: 45:38-46:35)

Then we can have asynchronous balanced mode (ABM), used with balanced configuration. So either combined station may initiate transmission. In this case, since it is on the balanced configuration, that means, on both sides of the link we have the two combined stations, both of them can act as primary and secondary and since both of them are combined stations, any of them can initiate the transmission and the transmission can start ? this is ABM. And the other is ARM asynchronous response mode used with unbalanced configuration. Here the secondary may initiate transmission. Primary still retains responsibility for the line (initialization, error control etc.) that is done by the primary ARM. This is not very common. (Refer slide time: 46:36-47:27)

The NRM is used on multi-drop lines. For example, a number of terminals are connected to a host computer; the computer polls each terminal for the input. This is the example I talked about earlier. This is used on multi-drop lines also. It could also be used on point-to-point links. The link connects a peripheral, let us say, terminal to the computer. So it is a point-to-point link and what is happening is that the computer side is the primary station and the peripheral side is the secondary station. In all these unbalanced cases, the NRM may be used. (Refer slide time: 47:28-47:55)

Then ABM this is the most widely used. On both sides, we have combined stations. It makes most efficient use of a full duplex point-to-point link, as there are no polling overheads. You do not have to do any polling since this is point to point. ARM is rarely used; it may be used in very special situations, whereas secondary may need to initiate transmission. (Refer slide time: 47:56-48:52)

Now there are different types of HDLC frames. One type of frame is called information frame, which contains user data. Then piggybacked ACKs: that means the next frame is expected. So when you are piggybacking acknowledgement is also going on actual data and those acknowledgements are telling you about which is the next frame to expect. Poll or final is the command or response. Supervisory frames: Flow and error control. Both go-back-N and selective reject are possible. And then there is a frame, which we say is the final one, when there is no more data to send. (Refer slide time: 48:53- 49:14)

Then there can be some unnumbered frames, which are used for control purpose; some mode setting commands and responses; information transfer commands and responses; Recovery commands and responses; miscellaneous commands and responses. By the way, you do not have to remember all these because when you?re trying to use it or design it for some particular situation, you can always look up the reference and see what exactly the different frames are, what are their formats, etc. What I am trying to do is to give you a flavor of the kind of information a typical protocol will need to exchange and some typical ways in which these protocols work. (Refer slide time: 49:54-51:36)

So this is an example of HDLC operation. For this line set-up from the primary station, you will send an SABM. SABM stands for set asynchronous balanced mode. It is the initiator and it wants to send an ABM kind of a set ABM. On the other side it sets an ABM. May be what has happened is that after some time this SABM was not acknowledged. So it times out and sends us an SABM again. Now it gets a response back, UA. There are some unnumbered frames, if you remember, so this is an unnumbered acknowledgement. After some time, possibly, there will be some transfer of data etc. And then, there is a disconnect frame, which is a DISC, which is going from here. On the other hand, you may get frames like RR and RNR. RR stands for receiver ready and RNR for receiver not ready. If you look at the right-hand side, there are some RRs and RNRs; these are going back and forth; and in between, there may be a two-way data exchange. Actual data are being exchanged through these I frames; I is for information, so this is information frame. So a lot of  link set-up and ACK is traveling on both sides. (Refer slide time: 51:37-52:27)

From the reject there has to be a recovery. Suppose it is using a selective reject, so reject 4 is going on this side. I,4,0 is coming from the other side; that is, 4 is now being retransmitted. I,4,0 had been transmitted before it got lost on the way, so a reject 4 had come. That same selective reject is working over here. Similarly, there is time out recovery; that means, after some time, I, 3,0 was sent and was never received. So after sometime, it timed out; so it says that some recovery frame is required. This RR sends receiver ready, then I, 3, 0 is transmitted again. These are the typical kinds of transactions, which are going on between the two stations. Once again, as I said, this gives you some idea about how this data link protocols etc., work and how they do error and flow control, etc. (Refer slide time: 52:48-53:54)

We will now take just a quick look at the various fields. There are some flag fields. Each frame starts and ends with the special bit pattern. Of course, if you use a special pattern, in order to accommodate this pattern within the data itself, bit stuffing is used to unambiguously identify the flag fields. Bit stuffing this is not 100% reliable. You must remember when the same flag is used to mark both the beginning and the end, a 1-bit error may merge two frames into one. A 1-bit error in the ending patterns will not be interpreted as an end of frame pattern by the receiving side; so the two frames may get merged. Similarly, a 1-bit error inside the frame could split it into two. This is not 100% correct, but bit stuffing would work most of the time. (Refer slide time: 53:55-54:27)

Then there is the address field. It identifies the secondary station transmitted. If you remember, it is not used for or needed for point-to-point links, but is included for the sake of uniformity. For point-to-multi-point links, you need to have the address because when the primary station is sending something it is addressed to some particular secondary stations, out of all the secondary stations connected to it. Both single octet, that is, single-byte and multi-octet or multi-byte addressing are possible. (Refer slide time: 54:28-54:39)

Then there are some control fields. It defines three types of frames. Information frames carry the data. Flow and error control information are also piggybacked using ARQ. (Refer slide time: 54:40-54:51)

Supervisory frames are also there. Some unnumbered frames provide supplemental link control functions. So I and S frames use 3-bit sequence numbers. (Refer slide time: 54:52-55:11)

Information field: It is present only in I frames and some U frames. It can contain any sequence of bits (multiple of 8), but is usually limited in length from consideration of error control. Frames with empty I field are transmitted continuously on the idle point-to-point lines in order to keep the connection alive.  (Refer slide time: 55:14-55:17)


Finally, we have the frame check and sequence fields. So with this example, we now come to the end of the general data link control. We have seen some examples of data link control; for example, we have seen some examples of MAC like with token bus, token rings, etc. We will see other examples of MAC as we take up the other kinds of networks. Similarly, we have seen some kinds of error and data control. A similar approach is used in some other higher layers also. We will talk about it when we come to it and we will talk about some more specific and more widely used systems from the next class onwards. So in the next class, what we will do is that we will look at another type of communication and how MAC is done on that in satellite communication. So, that will be the next topic. Thank you.



COMPUTER NETWORKS
Prof. Sujoy Ghosh
Department of Computer Science and Engineering
IIT, Kharagpur
Lecture-18
Satellite Communication (Refer slide 1)

Good day! In this lecture we will be discussing one very important component of world wide communication, which is satellite communication.
(Refer slide 1)

Slide 1

Communication through satellite has been going on for quite sometime now.   Satellite Communication is fairly mature technology although some improvements are taking place. Satellite Communication is an important component of world wide communication infrastructure. We will also see how specifically MAC, which is Media Access Control, is handled in Satellite Communication. This is important, since we obviously need MAC because the communication medium is the electro magnetic field around us, which is common to everybody. (Refer slide 2)


Slide 2
 We now talk about satellite communication. Satellite can be looked upon as a big microwave repeater. Repeater is something, which repeats the signal. It takes in the incoming signal, amplifies it and then sends it back. It contains several transponders. Transponders listen to some portion of the spectrum. Each transponder is listening to some portion of the spectrum so that several transponders together can listen to the wider band of the spectrum. It amplifies the incoming signal and broadcasts in another frequency back to earth. Satellites are up in the space; they take the incoming signal, amplify it and broadcast it back. So satellite has to broadcast in a different frequency so as to avoid interference with the incoming signals. (Refer slide 2)


Slide3
This can relay signals over long distance ? this is one strength of satellite  communication, because, if you send it all the way up and then when it is sending all the way down, and if it does so at an angle, the signal can reach a very long distance. There are different kinds of satellites and the most commonly known ones are  geostationary satellites because they are above the equator at a distance of 2300 miles approximately and are in the geosynchronous orbit. They travel around the earth in exactly the time the earth takes to rotate. (Refer slide 3)


Slide 4


We have an uplink station from where some signal is going and then it is being listened to by one particular transponder in the satellite. It amplifies signal and sends it back to earth where the downlink station (another dish) which is facing the satellite and tracking the satellite receives the signal over here. This is the just reflected part which then gets on the horn. This signal is taken down and amplified. Then we can use it. (Slide 4)


Earth stations communicate signals to satellite on an uplink. They	 are	called earth or base stations on the ground. The	satellite then repeats these signals back to the down link. This feature is attractive for distribution of TV	programmes. It is known that many of our television channels(unless coming through cable) are coming	from satellite, which is beaming back  the signal over a very wider area and all the receivers around can receive the signal and then amplify it and use it. So it is very popular for such kind of applications like distribution of television programming.

Slide 5
There are other applications. These signals are used to transmit signals and data over long distances for weather forecasting, television, internet communication and     global positioning system. These are the various applications of a satellite. The space orbit allows more surface coverage (Refer slide 5)

Slide 6
The spectrum is usually used by the satellite and is divided into sections; each of these bands have names like C band, Ku band or ku band and Ka band or ka band. The C band may have four downlinks and six uplinks; C band is from 3.7 to 4.2 GHz. This part of the spectrum is reserved for the C	band communication through satellite; that is, 3,7 to 4.2 GHz for downlink and 5.925 to 6.425 GHz for uplink. There is a 0.5 GHz  bandwidth for downlink and another 0.5 GHz bandwidth for uplink.
Since the uplink is at a higher frequency, you can have more channels over there. The capacity is not very high by today?s standard, especially, if you compare with fiber, this is really low. But at the same time, at one point of time, when transoceanic fibers were not there, satellite was the chief medium of communication across continents.	The capacity is low but still it is useful; however, terrestrial interference is a problem because when the weather is bad or when there are other kinds of extraneous sources of some electromagnetic noise, etc., these interfere with the signals. But still it has got a lot of strong points; that is why it is still an important component of communication these days. (Refer slide 6)




Slide 7

Next set of transponders come in Ku band. They can accommodate greater number of transponders ? 12	on the	downlink to 14	on uplink. Rain interference	is the problem here. Ku band is from 11.7 to 12.2 GHz and 14.0 to 14.5 GHz. In this band, rain is a problem though it has higher capacity and is less crowded than C band. C band is very crowded, if you consider people with a sort of dish antenna for receiving satellite signal ? I am not talking about the base station ? those using 1-meter antenna are possibly using C band. For Ku band, the antenna size is smaller and pizza shaped at something like 18 to 20 inches. They have a higher bandwidth; however, rain and terrestrial interference are the problems faced in C band. Different parts of electromagnetic spectrum are most susceptible to interference and noise, etc., but Ku band has higher capacity. The Ka band is at an even higher frequency ? 19 downlink and 29 uplink transponders are needed. The equipment needed for this is quite	expensive.	It is from 17.7 to 21.7 GHz and 27.5 to 30.5. It covers   a greater bandwidth, but it is still not very widely used.

Slide 8
Satellite can be used for point-to-point transmission, to transfer large volume of data; voice data and communication and for video conference. Satellite is not just for broadcast; satellite can be used for point-to-point communication. You may look it as point-to-multi-point communication. So we can say that point-to-point and point-to-multi-point communication are supported. Also, standard broadcast is supported. Point-to-multi-point communication can be data communication, internet, and video conference. Broadcast services include television. (Refer slide 8)

Slide 9

The advantages are that you can reach a large geographical area and have a high bandwidth; it is cheaper over long distances. It is certainly cheaper than wiring it up, but if you are covering long distance, because of its inherent high capacity, the economy is in favor of the fiber optic cable and it is almost even in some cases. But in many situations satellite retains its advantage. It can transmit to places where the cable cannot reach. These are applications where satellites have an undoubted advantage over any wired kind of system. It may be a remote area, which is not very easily reachable and road or other kind of communication is not well established to take cable over there. But the satellite, since it is sitting high up in the sky, can cover any area without any difficulties. Hence this is a very strong point in favor of satellite. It is especially useful for technology deployed at multiple sites regardless of location, like mobile technology. Nowadays the first thing people think about to be mobile is the cellular phone, which uses the nearest base station, which is usually wired. But in some places where it cannot be wired, it may go through  the base station and may be connected to the backbone through a satellite. That is one thing; and if you want 100% roaming all over the earth wherever you go, then you may not have a base station around the place where you are. In that case, some satellite or the other would be visible to you and you can communicate through the satellite. So mobility and accessibility to places which are difficult to reach otherwise are very strong points of satellite and of course standard communication is also a point.

Slide 10


The disadvantage is the high initial cost. You have to build the satellite and then to put it up in the space through some launch vehicle involves high investments. But  laying fibers all over is also costly so we have to work it out on a case by case basis and determine which one comes out to be better. We cannot do much at the physical level about susceptibility to noise and interference because the physical media is such that all kinds of noises are being generated all over the place and crowding this shared medium. So we have to handle it in some other way at a higher layer. Then there is a propagation delay; and this is a significant disadvantage compared to terrestrial communication for many satellites, especially the geostationary satellites. We will come back to this point later on. Geostationary satellites are thousands of miles above earth. So although your electromagnetic signal is travelling at the speed of light which is very high but even then to go all the way up to the satellite and then come all the way back down takes significant amount of time and this has lot of implications. It has the implications on the MAC and it has the other implications like delay and may be quality of service in some cases so this is an issue, a potential disadvantage and security can also be an issue since this medium is open to everybody. Whatever you are communicating, anybody else can listen on to it. If you are trying to send some very sensitive data through satellite and you do not want other people to listen to what you are sending, then you have to take some other measure like encryption, etc. We will talk about encryption much later in the course. (Refer slide 10)


Slide 11


There are different types of satellites and they have different types of orbits, circular or elliptical orbits. The circular orbits will centre at the earth?s centre. There is the   elliptical orbit with one foci at earth?s centre. There are some equatorial orbits above earth?s equator. This is quite common and necessary, especially for geostationary satellite. It is necessary that they go around the equator but then you could also have a satellite pass over both the poles. Other orbits are referred to as inclined orbits. (Refer slide 11)


Slide 12

The altitude of the satellites and their distance from the earth have significant implication in terms of the time it takes for the signal to travel. We have three different classes of satellites: geostationary orbit satellites, GEOs; medium earth orbit satellites, MEOs; and low earth orbits, LEOs. Out of these, GEO is the most common one.  (Refer slide 12)
GEOs go around the equator and have a high bandwidth. But they also require high power ? this is an important point ? and long latency. These are the important issues. When you are trying to communicate with some satellite, which is very far away, then your transmitter has to be strong enough so that the signal reaches the satellite. If the transmitter power has to be large then you require lot of power. In order to power this, it may be difficult to power it with a battery. That kind of battery power may not be enough to reach a geostationary satellite. The mobility becomes difficult from this power angle. So this is an important issue. (Refer slide 13)




Slide 13



MEO has a high bandwidth; medium kind of power; and medium kind of latency. LEO has low power and latency, but you require more number of satellites. They have smaller footprint. We have VSAT, which means very small aperture. When we talk about VSAT, we are not actually referring to any satellite. We are referring to the ground equipment that we use and VSATs have small apertures, which private WANs can use with smaller antenna. But if they are using C band or Ku band they use antenna which has a dimension of either 1 meter or 18 inches. But for the main base station of the satellite, usually a much larger antenna is put in place. We will first talk about geostationary satellite.
This is the most common type of satellite today and it is in a circular orbit. We say it in miles about 23,000 to 22,000 odd; and in kilometres it is 35,000 odd kilometres above the earth in the equatorial plane These satellites remain in the same position over the earth as it rotates.
(Refer slide 14 and Slide 15)


Slide14 and 15


Slide 16
As the earth is moving, the satellite is moving along with that. From some point on the earth, it would appear that the satellite is stationary over its head at all times.  That is why this is so sacrosanct about this geostationary satellite, this distance of 35,785 kilometres. You can always calculate this distance by finding out about the centripetal force and the earth?s gravitation etc. You can put it in that equation and calculate that this is the exact distance at which, if you put a satellite with a particular angular velocity, which is the same angular velocity as that of the earth, what will happen is that to the people directly under it, it will appear as if the satellite is stationary. So this is the good thing about this geostationary satellite and this is why this distance is so fixed. (Refer to 16)



Slide 17

From such a long distance, if you send a beam which has a reasonable solid angle over here, it will cover a large portion of the earth. As a matter of fact, it has been  calculated that with three or four satellites you can cover the entire surface of the earth; but of course, three or four satellites would not be sufficient to handle the bandwidth that we require these days. Furthermore since this distance is so fixed, there is only one band in space where the satellites can be parked. The other point is that if two such satellites are very close to each other, the signals will start interfering with each other. So there has to be a minimum distance between two geostationary satellites. The geostationary orbit and these parking slots are internationally decided, but these parking slots are quite crowded. Some nations may not put the satellite but would have reserved some parking slots. These parking slots are quite crowded today with so many geostationary satellites up in the space. (Refer slide 17)


Slide 18


These have a  coverage of about a fourth of the earth and have good tracking properties. That means you can track it very easily but their signal weakens over great distance and the propagation delay can be large. The propagation delay we are talking about is of the order of 0.24 seconds. Usually we talk in terms of milliseconds but here we are forced to talk in terms of seconds. It may also be hard to get coverage at the polar regions because this geostationary satellite has to be over the equator; it is difficult to get coverage at the far northern and southern hemispheres. (Refer slide 19)



Slide19

GEO satellite provides universal connectivity in its footprint. Its footprint means the area of earth, which is covered by one particular transponder, let us say on one particular satellite. So it covers all that area at the same time. So now within its footprint it covers universal connectivity. From any particular point, you can communicate with the satellite and possible satellite parking slots are quite crowded. Wide beams are circular, whereas spot beams, which are more focused, are elliptical. Apart from broadcasting, they may also be used with VSATs for point-to-point communication. We will come back to this point later. (Refer slide No.20).

Slide 20
So they are at this kind of distance that requires large transmitter power making them large and expensive. There is considerable space delay and large cell size, which means smaller number of channels. This is one particular point and we will come back to this point in greater detail when we discuss terrestrial wireless communication. But the point is that a satellite has some amount of bandwidth which is assigned to it which it can handle; now within that bandwidth for communication,  namely, the voice channels require about 64 kbps rate. One particular transponder can handle 800 such channels. If the footprint of the satellite is very large, that means it is  covering a large number of people. But all these large number of people are constrained with those 800 channels. A large cell size also necessarily means a smaller number of channel density on earth. (Refer slide 21)


Slide 21
A typical satellite has 12 to 20 transponders, each with a 30 to 50 MHz bandwidth; a  transponder can carry about 800 voice channels. FDM, Frequency Division Multiplexing, was used in early satellites. Nowadays TDM is also used; as a matter of fact, a mixture of TDM and FDM is also used. The cost of transmission is independent of distance. Sometimes this is an advantage. When you are communicating via a communication satellite with somebody who is, say, 10 kilometres away or  with somebody who is 100 kilometres or 1000 kilometres away, the cost is constant over the entire footprint. That may be an advantage in some cases.  (Refer slide 22)


Slide 22
For some applications this may be useful. Security and privacy pose a problem as mentioned earlier. So encryption is essential. Mobility is easily achievable and setup time is not required. If the satellite is in space, then you do not require any setup time. If the satellite is not there, you have to send it there. GPS is an interesting application we will see. (Refer slide 22)


Slide 23


We now come to the middle earth orbit satellites. These are used for global wireless communication coverage. They maintain orbit about 8000 miles from earth. The moment you come out of the geostationary orbit, the satellite cannot remain stationary over somebody?s head any longer. Now the satellite will necessarily move over your head so in MEOs and LEOs what will happen is that you will find that the satellite is coming over your head just as other stars move. So the satellite will move over in the sky and it will be there for sometime and then it will be out of your reach because it will go down below the horizon. This is what is going to happen with MEOs as well as LEOs. How soon will they come back depends on how far they are and what is the speed, etc. So it orbits around earth once in about every 2 to 12 hours depending on its parameters. So more satellites are needed and some handoffs are required as the satellites orbit now. What is this handoff we are talking about? You are communicating through the satellite to somebody else but the satellite has moved away. So now the communication link will break. In order that the communication link does not break what will have to happen is that another satellite will have to come and take its place and there has to be a handoff of communication from this satellite to this satellite before this satellite disappears altogether so that this communication between the two end points can continue. So these handoffs are required as satellite?s orbit and transmit data rate at 9.6 Kbps to 38.4 Kbps. Transmission delay is less than 0.1 seconds. So this is considerably less than the 240 milliseconds we had earlier. (Refer slide 23)


Slide 24
Taking it still further down, we have  the low earth orbiting satellites, launched like a large flock of birds. You require a lot of satellites because each of these satellites is visible to one particular point in earth only for a short duration of time. So in order to give continuous coverage, you need a large number of satellites. This cell size or the footprints will also become smaller. So in order to cover the entire earth, you need a very large number of satellites orbiting at constant altitude of 400 to 1000 miles. They must travel very fast to avoid gravity forces because in that case they will fall down to lower orbit, which allows for  transmission of the 2.4 to 9.5 Kbps. It travels at 17000 miles per hour, circles earth approximately every 90 minutes. So this goes around very fast. It is used for mobile voice low- and high-speed data; internet access via mobile phones and PDAs and GPS, etc. So the low earth orbiting satellites are what are used for the so-called SAT phones. It is dual linked due to some reason. Since this is low earth orbiting, the power that you require to reach the satellite is much less. So now you have a handheld device, which may be a little bigger than your standard cell phone, but not very much bigger. You can still carry it in your hand and with that now you can phone from anywhere from the earth because you are just communicating through the satellite. (Refer slide 24)


Slide 25
Low earth orbiting satellites are individually cheaper.  They also give lower space delay, which is much less because we are that much nearer. However a large number of satellites need to be deployed. As the satellites keep moving, ground stations which are communicating, will have to switch from one satellite to another and quick handoffs will be required. (Refer slide 25)


Slide 26
One of the examples was the IRIDIUM set of satellites. There were 66 satellites, which offered mobile telephony, paging and data communication. Unfortunately by 1990 they went out of service. What happened was that, SAT phone services became very costly. By that time, other kinds of technology like fiber technology and the terrestrial wireless technology developed to such an extent that a major part of the market was captured by those, which were giving the same service to the end user at a much cheaper rate. Furthermore we require 66 of them, all of them moving very fast and you have to have this complicated handoff from satellite to satellite to make it low enough so that you can communicate from a handled device etc. This could not be supported in the market since there are very few people going to such remote places where there cannot be any other communication infrastructure. Majority of the users would have some base station for some terrestrial wireless nearby and that is why this price became very uncompetitive. There were some efforts to revive this company,  but finally it did not work out and they became bankrupt. At present there is the ambitious ongoing project called TELEDESIC in LEO, which includes 288 LEO satellites to provide low-cost high-speed internet access, networking and teleconferencing across the globe. (Refer slide 26)

Slide 27
Here, as I mentioned, the space delay is lower and becomes comparable to terrestrial lines in the L band. It is possible to communicate with it using battery powered handheld devices. However cell sizes are still too large compared to terrestrial cells. (Refer slide 27)


Slide 28
IRIDIUM system had 66, so each satellite was to have 48 spot beams, giving a total of 1600 cells, each with 174 channels. But 174 channels or total of 183,000 channels are not so many globally because terrestrial service providers give millions of channels over the area. So finally, the cost turned out to be too high and the project went bankrupt. (Refer slide 28)


Slide 29


Now we talk about VSATs, which are small terminals with about 1-meter antenna and 1 watt transmitter power. Often the downlink capacity is more than the uplink speed for point-to-point communication usually goes through a hub. If two persons want to communicate the communication will go from here to the satellite and from the satellite to a central hub, which will have a very big antenna and it can handle all the MAC part of it. We are not going into the details of this but anyway this will go to the hub and from the hub again it will go to the satellite and then go the point B. Let A be the sender and B be the receiver. So it goes in two hubs; so the space delay is doubled because we are going in two hubs, sometimes with the help of the satellite MODEM kind of thing, you can go in one hub. So those channels are more costly. Otherwise, with the usual two hubs, there is considerable delay of the order of a half a second for this communication between A and B, but then there is no setup time.  So various combinations of TDM, TDMA, FDM, and FDMA are used for handling the MAC and all this is controlled by that central hub. (Refer slide 29)


Slide 30						As I mentioned, one important application was the GPS satellite constellation. This is a global positioning system and this is operated by US Air Force. Another GPS system is on the drawing board; it will be deployed by some other consortium. So there are 28 satellites in this and it has six orbital planes at a height of about 20,200 kilometres. Since this is not the geostationary orbit, they keep on moving and a minimum of five satellites are visible at all times. So with this what you can do is that you can locate any position on the ground, if you have a GPS terminal somewhere. A very common application of GPS is to put that antenna on a car so the car can be tracked anywhere on the earth. (Refer slide 30)


Slide 31
For GPS we do a Trilateration. Suppose there are a number of satellites which are visible to any of the ship, plane or a car as shown in the picture. Suppose we are measuring the car?s distance from a number of satellites. Since for the satellites we know relevant parameters, from three different readings and three different distance measurements, if measured quite precisely and accurately, you can get the accurate position of the car. There are so many different applications, which are possible. For example, there is a central database, if you give information about where you want to go, then it can tell you  the path to take or the alternative paths and which of them is crowded. In many places in Europe, you can get very precise information about where you are, which road you should take to reach the final destination. (Refer slide 31)


Slide 32
The advantages of AVL (automatic vehicle location) are fast despatch; customer service; safety and security; digital messaging; dynamic route optimization and driver compliance. Slide 32 shows a mobile GPS unit located on a truck. With GPS satellites, you can give faster dispatch of goods. For example, some company has to deliver a lot of goods to a lot of  warehouses. So it may ship something and now the customer wants to know where it is. You can immediately know where it is because you know into which truck you have put these goods and where it is exactly.  So, for courier service and similar services, you can give enhanced and sophisticated level of service through GPS.   (Refer slide 32)


Slide33
In conclusion, we can say that satellite communication will continue to serve where broadcasting is essential or where terrain is hostile or very sparsely populated. It also has a niche where rapid deployment is extremely critical. Let us consider battlefield kind of situation where you want to rapidly set up some communication network. If you have a satellite up in the sky already, you can take that transponder. May be there is a disaster where you want to quickly set up disaster recovery and relief operations. You can use this satellite because satellite communication will still go on whatever may happen. Even if there is a flood and everything is flooded, the satellite communication will still go on moving with your mobile units. In other combination with terrestrial radio links fiber is likely to hold the advantage (Refer slide 33)


Slide34

Now we come to an interesting part of the satellite communication. We see how this MAC works for satellite communication. This is another class of media access control technique, which we mentioned earlier. This is the first example that we see. It is Random Access Protocol. (Refer slide 35)


Slide 35
We mentioned Random Access Protocols earlier just as we talked about token based protocols, etc. We also talked about random access protocols. This is the first example of a Random Access Protocol. Random Access Protocol is actually very simple. When a node has a packet to transmit at full channel data rate, so just do not bother whether somebody else is transmitting or not transmitting or wants to transmit or it?s going to transmit. Right away, you just do not bother about anything. You just try to put in whatever you want to transmit on to the channel. Now what will happen. If you are lucky since you are doing the random thing without any knowledge or consideration about what other people are doing, your transmission may get go through. So there is no a priori coordination among the nodes but your transmission may still go through. What will happen is that two or more stations may try to   communicate at the same time or very nearly the same time. What may happen is that the packet or the frame sent by one station A and the frame sent by the station B  may collide and what will happen is that both the frames will be lost and will become garbled. Since you are talking about a medium, which is shared by everybody, these two transmitting stations A and B would also be able to listen to this and find out   that both of their messages are garbled. If they find that both are garbled, they will  retransmit as a sort of backup for a random amount of time. That is very important once again the second random is also very important  this backing a random amount of time because if the protocol says no both of them backup for a fixed amount of time and after the lapse of that fixed amount of time both of them will start communicating again. It is important  that the backoff for a random amount of time and hopefully the random number generated by one station and the random number generated by the other station happen to be two different numbers. So now they are going to stay communicating at two different points of time but they will not collide. They may collide with other stations. That is a different issue. To ensure the random access protocol, if two or more transmitting nodes start transmitting at the same time we have a collision. Random access MAC protocol specifies how to detect collisions and how to recover from collisions; for example, via delayed  retransmissions. These are the two parts of any random access protocol.  (Refer slide 36)


Slide 36
This satellite protocol, which is called ALOHA or slotted ALOHA is the simplest kind of random access protocol. We will see other examples like CSMA CSMACD CSMACA etc later on. So   these are the examples of Random access MAC protocols   ALOHA which was the original grandfather of all these protocols first came, then slotted ALOHA and then all these CSMA CSMACD and CSMACA etc were used (Refer slide 36)


Slide 37
The ALOHA protocol was originally developed for packet radio in 1970s. This was applicable to any system with uncoordinated users competing for a shared channel. There was no carrier sensing. Carrier sensing means you do not try to find out whether or not somebody is already communicating. That may sound a little strange at first; because if you could find out that somebody is already communicating, then by trying to transmit at that point of time you are not only sure to fail because your own message will get garbled and somebody else?s frame will also get garbled, but then  you will also have a more crowded media. For satellites there is a problem. You cannot always do this carrier sensing, due to space delay. Since we have a 240 milliseconds space delay, whatever you are listening to now was actually transmitted 240 milliseconds earlier. So if you find that the channel is quiet now, there is no guarantee that after 240 milliseconds also it will also remain quiet. That is why it is difficult to do carrier sensing in the case of a satellite. That is why this latency, space delay, has a very significant impact on how we handle the MAC. Here we do not do it; 240 milliseconds is a lot of time in which a number of frames may be sent. So we do not do any carrier sensing. We simply send whenever we have to send something and then later on listen to the medium to find out whether or not there has been a collision. This is the pure ALOHA protocol. (Refer slide 38)


Slide 38
Users transmit whenever they have data. If there is collision within a time frame, dictated by the space delay, it backs off for a random amount of time and sends it again. If the first bit of a new frame collides with the last bit of a frame that is just finishing, both are taken as garbled. So there is no such thing that the frame has gone through 95%. Either the frame has gone entirely, or even if one bit has collided, both the frames are taken as garbled and lost. (Refer slide 38)


Slide 39
Collision detecting by listening to broadcast channel or by absence of acknowledgement: if collision occurs, each user waits random length of time as already mentioned. Various collision resolution algorithms are available. Station does not transmit new frame until old frame has been successfully transmitted. The station is stuck with the frame until it can successfully send it. (Refer slide 39)


Slide 40
The slotted ALOHA has better performance. All frames are of the same size. Time is divided into equal size slots and each slot is to transmit one frame with some   distance. If the transmitters and the receivers are synchronised they try to send frames only at the beginning of a slot. So nodes start to transmit frames only at the beginning of slots. Nodes are synchronized; if two or more nodes can still try to transmit in a slot, all nodes detect collision. There may be collisions even after making the slots. For example, suppose within the span of the fifth slot three different stations are ready to transmit something. What will happen is that at the beginning of the sixth slot, all three will be communicating and all three will collide. (Refer slide 40)

Slide 41

When node obtains fresh frame it transmits the next slot. As soon as it gets a frame to send, it sends it in the next slot. No collision node can send new frame in the next slot. If there is a collision, the node retransmits the frame in each subsequent slot with probability p until it attains success. (Refer Slide 41)

Slide 42

This is a picture of a slotted ALOHA. There is a collision over three nodes ? nodes 1, 2, and 3. All three of them wanted to transmit. They start transmitting and then collision occurred. Then all of them backed off. It may so happen that the second slot went empty. Then in the third slot, what happened was that number 3 had backed off randomly for a long period of time but node 1 and 2 may have decided on the same number over here. So 1 and 2 collide again. What might happen is given as an example: may be in the fourth slot, 2 has tried again and it has been successful. So there is a success over there. Then next slot is empty. Node 1 and node 3 are trying and then they collide again. Then may be in the next slot, 3 succeeds and so after 9 slots, the three have been able to communicate these three frames. So you see because of this, collision affected efficiency. (Refer slide 42)


Slide 43

In slotted ALOHA pros a single active node can continuously transmit at full rate of channel. If nobody else is transmitting one active node can go on transmitting. This is highly decentralized. Only slots in nodes need to be in sync; so that may be taken care of from the hub or through the satellite etc.; it is a very simple kind of protocol. (Refer slide 43)

The cons are that there are idle slots and hence low efficiency. (Refer slide 44)


Slide 44
Now what is the efficiency? Efficiency is the long run fraction of successful slots. There are many nodes each with many frames to send. Suppose N nodes with many frames to send each transmitting slots with the probability P. So probability that first node has success in a slot is equal to p (1 ? p*(N ? 1)). So it is the probability P that that particular node sends and it is the p1 ? p that all the other N ? 1 do not send. So only then you will have a success; and probability that any node has a success is Np(1?p)N?1. (Refer slide 44)


Slide45


So for maximum efficiency with N nodes, we have to find a p* that maximizes Np(1 ? p)*(N ? 1).Hence this value. For many nodes, we take the limit of Np*(1 ? p*)N ? 1 as N goes to infinity and this gives a value of 1/e, which is 0.37. So this is the efficiency ? 37% is the maximum theoretical efficiency that you can achieve in slotted ALOHA. So at best, channels have useful transmissions; 37% of time, which may not look like a very high figure, but then you are absolutely uncoordinated; but if you have low load then that may be alright. (Refer slide 45)


Slide 46.

For pure or unslotted ALOHA, is simpler because there is no need for synchronization. When a frame first arrives, it is transmitted immediately. So collision probability actually increases. A frame sent at t0 collides with other frames sent in t0 ? 1 assuming that each frame takes one unit of time to send up to t0 + 1; so there is a big interval from t0 ? 1 to t0 + 1, where things may collide. (Refer slide 45)


Slide 47
So this will overlap. So this frame will overlap and now there is no question of any slots, so anybody can start transmitting at any point of time. (Refer slide 47)


Slide 48

What will the efficiency be like? P of success by a given  node is equal to P that the node transmits multiplied by the probability no other node transmits in [t0 ? 1, t0] ? P, a probability that no other node transmits in t0, t0 + 1. So this is p(1 ? p) N ? 1 ? (1 ? p)N ? 1 so p (1 ? p)2(N ? 1). So choosing optimum p and then letting N tend to infinity gives as an efficiency of 1/(2e) or about 18%. So this is a rather low efficiency for  transmission.  (Refer Slide 48)


Slides 49 and 50
When a computer network developed there were a number of  competing  LAN technologies  but today ethernet has a come to dominate a LAN almost totally excepting for the newly emerging  wireless part which we will discuss later.  So this ethernet is not only a LAN. Nowadays people are talking about ethernet in the  MAN that is metropolitan network area also. So Ethernet, as you understand is very important. So we will talk about Ethernet. It?s a dominant data link layer technology (Refer slides 49 and 50)

Slide51

The multiple access scheme of ethernet is CSMACD now  (Refer Slide 50)

Slide 52

So this minimum time is which may vary from system to system, that means a network to network. So this is how it is calculated so far. Let us see how the minimum time would comes. Suppose this is the shared bus we put two nodes at the two extreme ends. Suppose now packet starts at over here at time t equal to zero. Packet is almost reached b at time t-? , at which point of time,  it does not transmit. Suppose the propagating time of the packet for moving from one end to the other is ?, at ?- ? it has almost reached b. At that point of time b starts transmitting, because we have been finding that the bus has been very quiet. There is no signal  at end so b can start .So b starts and if there is a collision and this collision is  at one this end of the of the medium .So  the collision bus  starts transmitting back the jam signal. .So it again take s almost tau to reach .Soa so the total time is ?
noise. So collision detection can take as long as 2 ?. ( Slide 52)
COMPUTER NETWORKS
Prof.Sujoy Ghosh
Lecturer Name # 19
Ethernet ? CSMA/CD
(Refer slide time: 00:41)
Good day. Today, we will talk about Ethernet. This is possibly the most ubiquitous LAN technology today. As a matter of fact, when computer network developed, there were a number of competing LAN technologies but today Ethernet has come to dominate LAN almost totally, excepting for the newly emerging world spot also, which we will discuss later. Not only LAN, nowadays people are talking about Ethernet in the MAN, i.e., Metropolitan Area Network also. Ethernet as you understand is very important so we will talk about Ethernet. (Refer slide time: 01:37 - 01:59)  

It is a dominating data link layer technology and the multiple access scheme of Ethernet is CSMA/CD. We will talk about Ethernet and CSMA/CD;  this will be our first lecture on this. We will continue on this topic in the next lecture also. (Refer slide time: 02:00 - 02:54)

What is the broad outline of the Ethernet? First of all, it was conceived as a broadcast network, so single communication channel that is shared by all the machines on the network. This would be the kind of medium we will have. We have a shared medium where everybody broadcasts. It is supposedly one of the common broadcasts in the early days, one of the common physical organizations of Ethernet where some machines were connected to a coaxial cable. This is a LAN technology unlike the satellite technology that we have talked about ? remember that in satellite technology, which is also a shared medium, we of send data and then detect collision. This is further advancement. The multiple access technique of Ethernet is just some refinement over that but this is essential in a LAN environment, meaning it is for a small geographical area. We have packets; that means packets are sent over this, we will see later on how it frames them. So short messages sent by any machine are received by all others. Since this is a broadcast network, a message which is sent by any machine is received by all networks and so naturally you have to give the destination address. There are various fields in the Ethernet header and we will look at that later on. One of them has to be the destination address so that we understand because otherwise the recipients will not know for whom this frame is meant. (Refer slide time: 04:05 - 04:57) 


Since it is a shared medium, all these machines are supposed to be in the so-called same collision domain; because if two machines are sending simultaneously, their packets will collide. Suppose you have the sender over here and sender has a packet to send, it puts it on this common bus. The packet travels both ways and then it is received by all the recipients. All of them copy the frame into their NIC. They would compare the destination address with their own address, if it is not for them, they will simply ignore it. If it is meant for this node, it will absorb it and then send it to the higher layer.  (Refer slide time: 04:58 - 05:58)

All machines receive the packets but only one would process it; i.e. the mode of operation is broadcasting. It is also possible to address a packet to a subset of machines; this is called multicasting. This mode of operation is called multicasting, meaning we have two ends of the spectrum: one is a point-to-point communication, one sender and one receiver. On the other end of spectrum is a broadcast where one sender and everybody else is a receiver. Somewhere in between is a multicast; meaning there is one sender and several receivers. In order that several receivers can receive it, we have to do something with the address. So we have to give a special kind of address; that is also possible. It is possible to multicast since it is possible to broadcast. (Refer slide time: 05:59 - 07:22)


Just to remind you about our discussion on satellite communication, if you remember, the satellite communication used a MAC protocol called ALOHA. There were two versions of ALOHA and one was the pure ALOHA, whereby if anybody has any data to send he simply sends it. If there is a collision, he would back off and maybe send it again. That is a pure ALOHA scheme and we saw that this is the form of S = Ge?2G where s is the throughput, that means how much you are actually able to communicate successfully, and G is the number of attempts per packet time. This is the s versus gee of pure ALOHA, which gives about 18% of efficiency at the maximum. Slotted ALOHA is where you can send only from the beginning of a slot time; that has considerably better performance then it comes to about 37%. The formula is S = Ge?s. Refer slide time: 07:23 - 09:39)

Ethernet is something similar. That means we broadcast and then we detect collision, so it should be the same as some kind of ALOHA. The only thing is that since this is a local area network, a small network, what we can do is that we can sense the carrier; that means we can sense whether somebody is sending. If somebody is sending, we refrain from sending anything; so that is called carrier sensing. We discussed this earlier that it is not possible to do carrier sensing in satellites because the space delay is so high, whatever you are listening to now must have happened may be 250 milliseconds earlier; that is a long time back. But that is not the case in a small LAN. The timeframes are much smaller, so you can actually listen to the medium and find out whether somebody is actually broadcasting something at that particular point of time. In that case, even if you have something to send, you do not do so. That reduces the collision considerably and that naturally leads to an improvement in the efficiency. So this stream is called carrier sense multiple access and we find that even if you are doing carrier sensing, there can be some collisions although there will be reduced number of collisions compared to pure ALOHA, slotted ALOHA. We have to detect the collision and this CD part. The full MAC protocol that is used by Ethernet is the CSMA CD; this is the dynamic channel allocation technology or the random access MAC for this, used in old Ethernet. Nowadays we have moved to fast Ethernet, where we have reduced collision and switched Ethernet, which we will discuss these in the next lecture. (Refer slide time: 09:40 ? 09:55) 

So we have carrier sense multiple access. We improve the performance of our simple network greatly if we introduce carrier sensing; with carrier sensing each hosts listens to the data being transmitted over the cable. (Refer slide time: 09:56 ? 10:25)

A host will only transmit its own frames when it cannot hear any data being transmitted by the other hosts. When a frame finishes an inter frame gap of about 9.6 ?s is allowed to pass before another host starts transmitting its frame. So he listens to somebody sending and then he naturally waits and then after that frame is over, he gives a gap of about 9.6 ?s and then starts transmitting (Refer slide time: 10:27 ? 13:03)

Collisions can still occur mainly because of the propagation delay. Propagation delay means two nodes may not hear each other?s transmission. As before, if there is a collision, the entire packet transmission time is wasted; and naturally the distance and propagation delay has a strong role to play in the collision probability. Let us look at this. Suppose this node B starts transmission at time t0. Naturally, the transmission goes both towards A as well as towards C and D. So there is a cone like this ? if you plot it against time and distance ? over which B?s transmission is going. What happens is that C was listening and at this particular point of time, t0 or even later than that, C did not have any carrier to sense, because B?s transmission, although it had started in terms of absolute time, had still not reached C. So it will reach C only at this particular point of time. During this period, the transmission from B will reach D at a much later time, let us say t2 or something. If D starts its transmission t1, what will happen is that somewhere in-between they are going to collide; and these garbled bits will now start propagating on both sides, which is shown in this hashed figure. What happens is that, naturally B?s transmission is lost and D?s transmission is also lost. So there will be a finite probability of collision and this probability will depend on the time it takes for the frame to reach all the nodes. Because if it had reached some other node, that particular node would not start transmitting; but it takes a finite amount of time, and within that finite amount of time, if somebody else starts transmitting, we have a collision. So we can still have collision; that is why CSMA is not enough; we have to do CD also. (Refer slide time: 13:04 ? 14:53) 

So CSMA, as we mentioned, means listen before transmit. If a channel is sensed busy, defer transmission; and persistent CSMA means retry immediately when channel becomes idle ? this may cause instability. This is called persistent CSMA. That means retry immediately when channel becomes idle; because what might happen is that during the time when the frame is being sent ? that may be a considerable amount of time ? more than one node may become ready to transmit.
If more than one node is ready to transmit, naturally all of them will pounce on it after giving a 9.6-?s gap and start transmitting, and naturally they are going to collide. If they collide they know that is bad, so they allow that collision to subside and then they start again. By this time, quite a lot of time has elapsed. Somebody else may become ready and somebody else may again start persisting; so this may grow to some kind of instability. Being totally persistent is not very good, as we will see the performance of each of this; but that is persistent CSMA. We call a strategy as pure persistent; that means, we retry in the next immediate available time with the probability of p, which means, with the probability of 1 ? p, we do not persist. That means, we defer still more. (Refer slide time: 14:54 ? 15:34)

There is a non-persistent CSMA; that is, somebody is ready to send something; finds that it is busy. Then he will retry only after the random interval. He generates some random numbers and decides by himself to wait for this random amount of time. Collisions may still exist, since two stations may sense the channel idle at the same time or within a vulnerable window, which is equal to the round-trip delay. In case of collision, the entire packet transmission time is wasted; so we have pure ALOHA. This is the group of protocols: we have pure ALOHA, then we have slotted ALOHA, persistent CSMA, p persistent CSMA, non-persistent CSMA; and here is how they behave. (Refer slide time: 15:53 - 17:54)

We have already seen these two curves, pure ALOHA and slotted ALOHA. They are used in satellite communication because we cannot do area sensing over there because of the significant space delay. If you do carrier sensing and if it is persistent, the performance improves somewhat; so it may go slightly above 50% and then comes down. You can have different values of p, remember p is the probability that the node will try immediately when it senses that the channel is available; that probability will be 0.5. At probability 0.5, it will not try at all. At 0.5, persistent CSMA is good, 0.1 persistent CSMA is even better; that means what it will do is with 0.1 probability it will try and with 0.9 probability it will differ. If it is non-persistent, this is the 0.01 persistent CSMA and non-persistent CSMA. Non-persistent means it will always go back. If you draw the throughput versus load curve, this is the kind of curve you get. But the exact throughput will depend on how many nodes are there and what their distances are, etc. These are somewhat idealized figures but you get something like this. As you can see, with non-persistent or very low persistent CSMA, we get very high throughput even when the load is going up. So when the load is going up, it is not falling down steeply like ALOHA or slotted ALOHA. This is the channel utilization by the various MAC protocols. (Refer slide time: 17:55 ? 18:08)

 So CSMA/CD is carrier sensing, deferral as in CSMA; collision is detected within a short time; 
colliding transmissions are aborted, reducing channel wastage. (Refer slide time: 18:09 ? 18:39)

Collision detection is easy in wired LAN; it measures signal strengths and compares transmitted and received signals. This is how you detect a collision. It is somewhat more complicated in wireless LANs. We will discuss wireless LANs later on as we move on, but the receiver is usually shut off while transmitting. This is a problem with the wireless LANs, where collision detection is difficult. We go for some other kind of scheme over there, which is called collision avoidance, we will discuss it later. (Refer slide time: 18:54 ? 19:49)

As I mentioned, Ethernet is the dominant LAN technology, and since this is dominant, people who followed the Ethernet line had the great advantage of having a huge market. That really drove down the cost and today Ethernet is cheap. Ethernet network interface card costs in the order of some $20 or something of that order. For a 100 Mbps card, which is quite fast, it is very cheap. It is the first widely used LAN technology; it is simpler and cheaper than token LANs and ATM ? so that is a great advantage and it kept up with speed rates of 10,100, or 1000 Mbps. Ethernet started with 10 Mbps, then went on to 100 Mbps, which is a standard today. People are talking about 1000 Mbps or 1 Giga bit Ethernet, which is also making inroads into the backbones of LANs etc. Maybe in a couple of years? time, it will go into the desktop also, if it has not already reached some desktop. So Ethernet is a very widely used technology. This is (Refer slide time: 20:26 ? 20:54)

a diagram to show you how the original Ethernet looks. You have a cable, which would be terminated on two sides; it would be tapped and then a transceiver will tap into it, to connect it. There will be an interface cable from the transceiver to the interface of the controller. This is basically the NIC, and this is how it will be connected to various machines, which should be tapped into the cable at various points of time, so this is the so-called Ethernet. (Refer slide time: 20:55 ? 21:34)

Now, we go into the details of how exactly this is done. We have discussed the basic technology already. Ethernet uses a bus topology; it assumes a bus topology, carrier sense multiple access. In CSMA/CD, each station has equal access to the network but it can broadcast only when the network is idle. Before transmitting, a station: (1) listens to the network to sense if another workstation is transmitting, which is carrier sense. If the network is still idle after a certain period, (2) the station will transmit. (Refer slide time 21:35 ? 21:42)

It is possible that two stations will listen and sense an idle network at the same time. Each will then transmit a message, which will collide. (Refer slide time 21:43 ? 22:05)

While transmitting, a station must perform collision detection to detect if its message was destroyed. If a collision is detected, the detecting station broadcasts a collision or jam signal to alert other stations that a collision has occurred. Each transmission station then waits a random amount of time ranging from 10 to 90 ?s before attempting the transmission again. (Refer slide time: 22:06 - 22:36)

So this is the same algorithm step by step; adapter gets datagram from the network layer and creates a frame. That means it adds a header to create a frame. If the adapter senses that the channel is idle, it starts to transmit the frame; if it senses that the channel is busy, it waits until the channel is idle and then transmits. If the adapter transmits the entire frame without detecting another transmission, the adapter is done with the frame. If the adapter (Refer slide time: 22:39 ? 25:50)

detects another transmission while transmitting, it aborts the transmission and sends a jam signal. This is an interesting point ? it needs to detect collision only while it is transmitting. After the transmission is over, there is no detection within that frame transmission time; you can forget about collision. That means just some time after that, you may still detect the collision. That collision would be presumed to be due to another station, not your frame ? because if you detect a collision after sending a frame, you have to understand whether it was your frame which collided or your frame went through and somebody nearby started transmitting and they collided. So the idea is that while you are transmitting, that time only should be enough for any collision, any inadvertent collision to happen; we will come back to this point. So if there is a collision, it aborts; after aborting, the adapter enters an exponential back-off algorithm. What is an exponential back-off algorithm? What might happen is that the frame sent might have collided and then it might have backed off for sometime; it may have tried again and it may have collided again; maybe the network is busy. What is the remedy if the network is very busy? Obviously if the network is very busy, what is going to happen is that more and more people will try to send; they will fail and then they will try again. This may bring down the overall throughput of the system to a large degree. So what is done is that if the network is busy, everybody tries to make the network less loaded, which means that they wait for longer before they try to send it. What they do is that they generate this random number over an exponentially increasing period so that the probability of a higher random number becomes more and more if the network is more loaded, which means that you are backing off for a larger amount of time. That is exponential back-off, so after the mth collision, the adapter chooses a k at random from 0,1,2?1. If m is 10, it chooses a random number between 0 to 1023. The adapter waits k = 512 bit times and then returns to step 2, which means, suppose k is 100, it would be 100 ? 512 bit times. It waits 512 bit times and tries again. (Refer slide time: 25:51 - 26:36)

So jam signal makes sure that all other transmitters are aware of the collision. We send a jam when it detects a collision and backs off. We calculate the amount it backs off as follows: Let us say bit time is 0.1 ?s for 10 Mbps Ethernet for k = 1023. If you remember 512 into 0.1, 512 bit time should be about 51.2 ?s and 51.2 ?s into 1023 will be approximately 50 msec, i.e. it waits for 50 msec, if k has to come out as 1023. (Refer slide time: 26:37 - 28:16)

The idea of this exponential back off is to adapt retransmission attempts to estimated current load. If it is heavy, the random wait will be longer. Once again, we do this because if the load is heavy at some particular point of time, then people in general should refrain from sending so that the load comes down. If the load is kept high and everybody starts to transmit, there will be even more collision and even less throughput than what you would achieve by a number of people backing off. This backing off is done exponentially, so first collision, choose k from 01, and delay is k ? 512 bit transmission time, which means that with 50% probability, we try to transmit immediately, and with 50% probability, we would back off for this k ? 512, i.e., 51.2 ?s before listening and trying to send again. After the second collision, we choose k from 0123. The aim is increasing; the number of collisions is increasing. After 10 collisions, it is not allowed to increase more than that. This range of k becomes fixed and you always choose from this 0 to 1023. This is the exponential back-off algorithm; for heavier load presumably a node will choose a larger k and wait for longer, reducing the load on the system and increasing the throughput. This CSMA/CD system can be in (Refer slide time: 28:23 ? 28:40)

one of the three states. There may be contention, there maybe successful transmission, or it may be idle. There may be contention slots or frames or idle period. (Refer slide time: 28:41 ? 29:50)

IEEE 802.3 defines this Ethernet protocol. If you remember 802.2 was LLC and 802.3 was for CSMA/CD and 802.45 etc., like token bus token ring etc. There are a couple of variants of Ethernet: one is IEEE 802.3 and the other is Dixie Ethernet. We will maybe mention it later on, but they are more or less the same, excepting for minor differences. 802.3 is the more dominant one these days; besides, this is the standard. 802.3 is the number of the standard and it describes the format of the frames and the type of encoding used for transmitting frames. It also describes what the frame format is and what kind of encoding is used for transmitting the frame. The minimum (Refer slide time: 29:59 ? 31:01)

length of frames can be varied from network to network. This is important because, depending on the size of the network, the frames must be of a suitable minimum length. Remember that a node has to detect collision only during the transmission time. If transmission is over and there has been no collision detected, it assumes that its own frame has reached whatever destination it was supposed to reach. After that, if there is a collision, it is supposed to be between two other nodes because once there is a collision only the sender would know whether his own frame has collided because you cannot really decipher the address field of the collided frame. The collided frames are gone, they are garbled. The standard also makes some suggestions about the type of cabling that should be used for CSMA/CD, bus LANs, etc. (Refer slide time: 31:02 ? 32:41)

This minimum time may vary from system to system; that means, network to network. This is how it is calculated. First of all, let?s see how the minimum time would come. Suppose this is the shared bus. We put two nodes at the two extreme ends, the packet starts over here at time t = 0, it almost reached B at time (t ? ?). Suppose this propagation time of the packet, from one end to the other, is ?, at (? ? ?), it has almost reached B at that point of time. B starts transmitting because while B has been finding that, the bus has been very quiet; there is no signal on it. B can start transmitting and it starts sending. There is a collision at one end of the medium. The collision bursts, starts traveling back and the jam signal is sent. That starts travelling back, it again takes time ? to reach A. The total time is 2 ?, the collision detection can take as long as 2 ?. The point is that your transmission time has to be greater than this 2 ?. There are a few other issues; we will discuss them later. (Refer slide time: 32:42 ? 35:19) 

This is what we were discussing, minimum frame length to ensure that no node may completely receive a frame before the transmitting node has finished sending it. Ethernet defines a minimum frame size, i.e. no frame may have less than 46 bytes of payload. Remember payload is the packet, which the data link layer receives from the upper layer. What happens if the packet is much smaller? The application itself is such that you have to send maybe 1 character or 2 characters; what happens to it? Ethernet does not allow less than 46 bytes, so you have to pad it up; we will come to that later. The minimum frame size is related to the distance which the network spans, the type of media being used ? because of the time it takes for the signal to travel ? and the number of repeaters which the signal may have to pass through to reach the furthest part of the LAN. Repeater if you remember is something, which enhances the signal. As a signal travels down the transmission line, it tends to get weaker and weaker because of attenuation. At some particular point of time the signal may have to be amplified. A repeater does just that, it takes some incoming signal and amplifies it, maybe some wave shipping or something. There may be a number of repeaters. Each of these repeaters might introduce some delay because the repeater will not go through instantaneously. There will be some delay in this repeater and if there are a number of repeaters, this delay would get added. Remember what we are trying to find out. When the packet being sent from one end of the network, reaches the other end and almost at the other end, there is a collision and that collision comes back. What is this total time? During this whole time, this packet must be transmitting because it will detect the collision only during the packet transmission time. The number of repeaters also is important as are the type of media and distance to the furthest part of the LAN. Together these define a value known as the Ethernet slot time, corresponding to 512 bit times at 10 Mbps. (Refer slide time: 35:20 ? 35:56) 

The longest time between starting to transmit a frame and receiving the first bit of a jam sequence is twice the propagation delay from one end of the cable to the other. This means that a frame must have enough bits to last twice the propagation delay. The 802.3 CSMA/CD bus LAN transmits data at the standard rate of r = 10 Mbps. This was the earlier standard of 10 Mbps. The speed of signal propagation is about V = 2 ? 108 ?s in this coaxial cable. (Refer slide time: 35:57 ? 36:28)

The cable maybe of 400 m length, transmission speed = 10 Mbps, so propagation speed is 2*10**8 ?s. Let us see what the delay is and what the minimum frame size comes out to be. Propagation delay time is tprop; the round-trip propagation delay is twice this number of bits. We can fit into a round-trip propagation delay, the minimum frame length nb. (Refer slide time: 36:29 -37:04)

tprop = d/V = 2 ? 108 and distance is 400 m, which makes it 2 ? 10?6 sec or rather 2 ?s, 2 ? tprop; that means, for the signal to travel all these 400 m and come back, it takes 4 ?s. For the 4 ?s, the transmitting station must go on transmitting and the transmitting station is pumping data at the rate of r = 10 Mbps. (Refer slide time: 37:05 ? 37:52)

A bit time, the time to transmit 1 bit is 1/R; it is 1/10 megabits, which is 10 ? 106 which is 0.1 ?s. The minimum number of bits the frame must have is 2 ? tp/tb. tp is the propagation delay; 2 ? tp for the round trip propagation delay divided by tb = 4/0.1 = 40 bits. This calculation brings us to 40 bits. As I said the minimum size is much bigger than 40 bits, and we will see why. (Refer slide time: 37:53 ? 38:07)

The minimum frame length is thus 40 bits or 5 bytes. A margin of error is usually added to this, often to make it a power of two, so we might use 64 bits or 8 bytes. (Refer slide time: 38:08 ? 38:53)

Let us see another example, calculating something else but it is closely related. Two nodes are communicating using CSMA/CD protocol, and the speed transmission is 100 mbps. This is A fast Ethernet and frame size is 1500 bytes. The propagation speed is 3 ? 108 meters per second ? that is the speed of the light. Calculate the distance between the nodes such that the time to transmit the frame equal the time to recognize that the collision has occurred. We want to calculate the distance. (Refer slide time: 38:54 ? 39:12)

Since frame time is given we calculate the propagation delay from this. So Tround trip = tframe = 2 ? tprop so tprop =  tframe/2, and what is the tframe? (Refer slide time: 39:12 - 39:22)

We have 1500 bytes, which is being pumped at the rate of 100 mbps. (Refer slide time: 39:23 - :) 

If you multiply 1500 bytes into eight, that means how many bits will get 12000 bits which is 1.2 ? 10?4,  i.e., 6 ? 10?5 (Refer slide time: 39:53 ? 40:17)

The maximum distance is calculated using the propagation velocity, tprop = d/V = tprop ? V = (6 ? 10 ?5) ? (3 ? 108) = 18 ? 103 = 18 km. With this kind of minimum frame length, you can go up to 18 km. Remember, the minimum frame size has become 1,500 bytes. This standard (Refer slide time: 40:29 ? 40:48)

frame length is at least 512 bits or 64 bytes long, which is much longer than our minimum requirement of 64 bits which is 8 bytes. We only have to start worrying when the LAN reaches lengths of more 2.5 km. (Refer slide time: 40:49 ? 41:41)

802.3 CSMA/CD bus LANs longer than 500 m are usually composed of multiple segments joined by in-line passive repeaters. As a signal travels down transmission line, what would happen is that it will become weaker, so we will have to put a repeater, and one kind of repeater maybe called a hub. In-line passive repeaters output on one cable the signals received on another cable or we may simply have an amplifier over there. When we work out the minimum frame length for these longer LANs, we also have to take the delays caused by the passive repeater; each passive repeater introduces a delay of about 2.5 ?s each, so we have to take this into account as well. (Refer slide time: 41:42 ? 43:23)

Let us say we have 500 m on each segment, we have 4 repeaters, so 5 segments. That is about the maximum you should go. Nowadays we do not use such coaxial cables. What happens is that 64 bytes sent at 10 Mbps would take 64 ? 8 = 512 bits; 512 bits at 10 mbps is 51.2 ?s and 500 m per segment. Four repeaters between the nodes means there are 5 segments; 2500 m in all. This gives rise to 25 ?s propagation delay, because remember our propagation velocity is about 2 ? 108. If you calculate, it comes to about 12.5 ?s from one end to the other, twice that is about 25 ?s. The frame should be long enough for the sender to detect the collision in 2 ? 25, this is 125 and the other 25 is these 4 repeaters together, about 50 ?s; 64 bytes sent at 10 Mbps is 51.2 ?s. If you have a frame length of 64 bytes, that is quite good. (Refer slide time: 43:24 ? 45:27)

Let us look at the details of the Ethernet frame. All information on an Ethernet network is organized into frames also called packets or may be packet is what the upper layer gives and then this is formed into a frame. contents of an Ethernet frame: Sending adapter encapsulates IP datagram or other network layer protocol packet in Ethernet frame. This is what the Ethernet frame looks like: we have a preamble of 7 bytes, we have a start of frame of 1 byte, we have a destination address of 6 bytes, we have a source address of 6 bytes, we have frame length of 2 bytes and then data may vary from 46 to 1500 bytes and then followed by CRC of 4 bytes. If you remember CRC is the cyclic redundancy code which Ethernet uses for error detection on the other side. So you have actually about 72 bytes, which is even larger than the 64 bytes. We require 46 bytes  the minimum and 1500 bytes is the maximum length of the payload, this is the payload part; to this we have to add this header and trailer part. In the trailer you have the CRC in the header the source address destination address, etc. This frame length is required because you might have padding over here. In some other version of Ethernet, there is a type field over here, which gives the network layer protocol type. Let us look at the (Refer slide time: 45:28 ? 46:08)

details quickly. Sending adapter encapsulates IP datagram Ethernet frame; preamble has 7 bytes with the pattern 10101010. Please note that 1s and 0s are alternating ? and there is a reason for that ? followed by 1 byte with pattern 10101011; so these 7 bytes are called the preamble. This 10101011 is the start of frame delimiter, 7 bytes of pattern. This 1 0 is required; it is used to synchronize receiver and sender clock rates. That is why we alternate 1 and 0 etc. (Refer slide time: 46:09 ? 47:47)

We have an address of 6 bytes. If the adapter receives a frame with matching destination address or with broadcast address, for example an ARP packet, it may be broadcast, it may be multicast or it may be unicast. If actually a frame would be broadcast, only 1 would come up. ARP is the address resolution protocol; we will discuss this in the next lecture. It passes data in the frame to the net layer protocol; otherwise the adapter discards the frame. So this address has to be there. The destination address and source address are also put over there. Type: as I said, in some form the type indicates the higher layer protocol, mostly IP, but others are supported, such as Novell IPX and Apple Talk. What type of network layer protocol is it talking to? The same Ethernet ? as I mentioned before ? the same data link layer protocol may be supporting a number of network layer protocols. In that case, there has to be multiplexing and de-multiplexing. In the receiver side when a packet comes, maybe this data link layer knows that this is for this particular machine because its address is there so it will absorb it but then send it. If there are two different network layer protocols, which are running at the same time, whom will it send it to? That type is mentioned in the field so that it can do a de-multiplexing properly. CRC or cyclic redundancy code is checked at the receiver. If an error is detected, the frame is simply dropped. (Refer slide time: 47:48 ? 49:53)

The other thing to understand is that this ? Ethernet gives you an unreliable connectionless service. It?s connectionless, and there is no handshaking between sending and receiving adapters; so it is connectionless. The sender simply sends some packet and there is no handshaking between the two. The actual reliability, whether there will be error or not, will depend on the quality of the cable, the quality of connections, whether all standards have been followed very meticulously, and the quality of distance and the ambient noise and these kinds of things. That is not what we are talking about, it is unreliable in the sense that whatever physical layer parameters are there, they are given to the data link layer, that is, the Ethernet layer. But the Ethernet does not really try to take any special care to see that the communication is reliable. It has a CRC for sure, so on the other side if the receiver sees that the CRC shows some error, it may drop the packet. But the receiver does not send any acknowledgement to the sender. The sender does not know about it. If that packet is dropped that frame is dropped. If that has to be handled, it can only be handled at a higher layer. So it is unreliable in that sense, receiving adapter does not send ACKs or negative acknowledgements to the sending adapter. The stream of datagrams passed to network layer can have gaps; that means some of the frames may have been dropped. Gaps will be filled if the application is using TCP. This is one kind of transmission protocol, which we will discuss later, which tries to take care of such errors. Otherwise, the application will have to handle these gaps. (Refer slide time: 49:54 ? 52:20)

Just a couple of words about the addresses ? remember there are 6 byte addresses. Every network card in the world has a unique 46-bit serial number, called a MAC address. We are talking about the Ethernet, there is 46 bit; 246 is about 64 trillion, which is a very large number and these numbers or these addresses are distributed by IEEE. A manufacturer who manufactures Ethernet NIC ? that means Ethernet network interface cards ? will apply to IEEE and get a whole chunk of addresses, whole block of addresses and will put these addresses one by one into the network cards that are produced and then sell in the market. What happens is that one of the network cards may have gone to India and the next network card may be used in Beijing. It may so happen that the network cards we have and not the network addresses in the LAN will have very contiguous addresses. They may not; anyway, what is guaranteed is that since this is distributed centrally from IEEE, no two addresses, i.e. no two MAC addresses of two network interface cards or two Ethernet cards are going to be the same. They are all going to be distinct, because we have this pool of 64 trillion addresses being distributed by IEEE. IEEE allocates these numbers to network card manufacturers, who encode them into the firmware on their cards. It is almost in the hardware, encoded in the firmware in the card, the destination and source address fields of the MAC frame have 48 bits set aside. Remember 6 bytes is 48 bits. There is a 2-bit discrepancy. These 2 bits are used for something, the standard also allows for a 16-bit address but they are rarely used. (Refer slide time: 52:21 ? 53:47)

In these 2 bits, the most significant bit is set to 0 to indicate an ordinary address, and 1 is to indicate a group address. This is for multicasting, which means that frames are sent to several hosts. Remember, in the beginning of the lecture we said that although the medium is broadcast, usually there will be 1 sender and 1 receiver, i.e., it is a unicast; that means 1 sender 1 receiver.
Sometimes you may have a broadcast; that means it is meant for everybody, and sometimes you may have a multicast; that means it is meant for a few of the hosts in the network, not for all. How do you indicate the address? You indicate a very special address starting with a 1 for multicasting. If all 48 bits are set to 1, frames are broadcast. So for the destination field if all bits are 1, there is a very special address, not the address of any particular machine, which is meant to match with any Ethernet address on that particular LAN. If 2 most significant bits are both 0, the 46 least significant bits contain the MAC addresses of the source and destination hosts. This is how you make out. (Refer slide time: 53:53 ? 54:49)

Also this is connectionless and unreliable in the sense that it does not take any special care for reliability. This is also non-deterministic network; this means that no host is guaranteed to be able to send its frame within a reasonable time, just a good probability of doing so. Theoretically what might happen is that you may try to go and find it busy. Again you come back and then go; it might collide next time you come back and this might happen an indefinite number of times. Although the probability of its happening in a large number of times becomes lower and lower, with a very good probability, we will be able to send it but there is no guarantee that within this time you will definitely be able to send your frame. When the network is busy, the number of collisions rises dramatically and it may become very difficult for any host to transmit its frames. (Refer slide time: 54:50 ? 55:19)

This makes it not suitable for a real-time computing application. At least this bus type network may not be suitable for a real-time computing application such as an assembly line, which will demand that data be transmitted within a specified time period. Since the 802.3 bus LAN cannot guarantee this, its use for real-time applications may not only be undesirable but sometimes potentially dangerous. There are other ways of very high-speed network when this is low; then the chance of not being able to send at all becomes very low indeed. Just one (Refer slide time: 55:36 ? 56:42)

last word about the CSMA/CD efficiency. We will not go to the details of this discussion. Suppose each station transmits during a contention slot with probability p. The probability A that some station acquires the channel in that slot is A = kp(1 ? p)k?1. So p is the probability that a particular contention slot is taken by one node and 1 ? p is the probability that all the other k ? 1 nodes have not taken and that a particular slot is being used, i.e. kp(1 ? p) k?1. So A is maximized when p = 1/k with a tending to 1/??as k tends to ?. The probability that the contention interval has j slots in it is A(1 ? A)j?1.  (Refer slide time: 56:43 ? 57:02)

The calculation of channel efficiency would be p, where p is the time that a node needs to transmit mean frame and 2 ? is slot duration so p/p + 2?/A means that if p is large, that means if we are sending a large frame, the channel efficiency increases. (Refer slide time: 57:03 -57:15)

As you will see in this slide, with 1024-byte frames, the channel efficiency is very high. If you send in small frames, the channel efficiency tends to be smaller. This plot is with a number of stations trying to send. With this, we come to the end of this lecture. In the next lecture, we will look at the modern versions of Ethernet like switched Ethernet and how different networks are connected together. Thank you.
COMPUTER NETWORKS

Lecture name #20

Prof. Sujoy Ghosh

Dept of computer science and Engineering
I.I.T kharagpur
Modern Internet
(Reference time: 00:46)
Good day! In this lecture we will continue our discussion about Ethernet. In the previous lectures, we have seen the basic data link protocol used in Ethernet. Ethernet is a very popular and nowadays it is almost ubiquitous in the LAN area. It is a very widely used system and it has evolved from whatever was the approach to Ethernet in the earlier days and the basic Ethernet protocol that we had discussed. People are sort of shifting away from it because as the technology develops and as things like switches etc., become cheaper as the speed of network goes up, there is a slight shift in emphasis in modern Ethernet. In this lecture, what we are going to do is that first, we are going to have a look at the physical layer of the Ethernet. There is not much to discuss about that, and then we will see how we are shifting from a shared medium to a switched Ethernet kind of concept, and how speed is increasing. Then, we will discuss a little bit about Ethernet, LAN as a concept is. So, we discuss the modern Ethernet that is how the Ethernet is evolved. (Refer slide time: 02:15 - 02:20)

Before we go into that, (Refer slide time: 02:21 ? 02:47) 

we will look at the topology and some of the limitations of the earlier versions of Ethernet. The topology of a network refers to the pattern of workstations, cables and repeaters. In an Ethernet, there are two primary limitations to the topology: transmission accuracy and end-to-end transmission time. Transmission accuracy means that Ethernet really under a data link layer is an unreliable protocol in sense that it does not do a lot of acknowledgement or negative acknowledgement and things like that. Therefore, essentially the transmission accuracy should be fairly tolerable. What happens is that the maximum distance for accurate transmission is determined by signal attenuation along the cable, and it would depend on the on the quality of the cable, the kind of cable we used and the data rate. The other important point is the end-to-end transmission time as we have seen, because of this collision and other things this end-to-end transmission time is important. It?s not just for the data rate or things to move faster but for other reasons, also this end-to-end transmission time is a factor. (Refer slide time: 03:48 - 04:56)

These are the Ethernet physical layer standards that we have ? some of them, they are actually more. So 10Base5, that is, 10 Mbps base band transmission and 500 m; this 5 here stands for 500 m cable length, this is called thick Ethernet or has thick coaxial cables and this has become obsolete. Then 10Base2 is 10 Mbps base band transmission; once again all of these are base band transmissions; that means we do not modulate them to a higher channel or frequency channel. So, 10 Mbps and 2 for above 200 m cable line, or lines of 180 to 200 m length. Then, 10BaseT; this again is for 10 Mbps base band transmission, which uses UTP cable. There are various versions of UTP cable like categories 3, 4, 5, etc. 100Base-TX is 100 Mbps base band transmission, also uses UTP cable. (Refer slide Time: 04:58 - 06:28) 

So, these are the of maximum segment length. As we can see, 10Base5 is 500 m; 10Base2 is 185 m; 10Base-T is 100 m. Fibre optic cable 10Base-Fl goes up to 2 km. Actually, 10Base-F is not in much use these days, because fibre optic components became cheap and available, and people have moved from this 10 Mbps rate to the 100 Mbps rate. The point is that, the fibre optic cable goes much longer than copper cables and it is best between buildings; that means when it goes through some open space, etc., we avoid copper cable most times because of electric interference and other problems, not to mention the distance. The nodes per segment in 10Base-F could also be more; we will come back to this topic of nodes per segment later. As we have seen previously, 10Base5 and 10Base2 have many limitations; for example, in 10Base2, you see the figure of 30 and a network segment having only 30 nodes is a constraint these days because all kinds of things are being networked. For completeness sake, I will make a mention of 10Base2 and other kinds of (Refer slide Time: 06:51 - 07:18)

Technologies, although they have become obsolete now ? 10Base2 has 10 Mbps, 200 m maximum cable length, is a thin coaxial cable in a bus topology; this is a classical bus topology. Repeaters are used to connect up to multiple segments. Repeater repeats bits; it hears on one interface to the other interface, so this is a physical layer device just for amplification, strengthening the signal. (Refer slide Time: 07:19 - 07:53) 

This is a figure showing how they are connected. 
We have this thin cable, which has terminated on both the ends and these nodes would be connected using some t connectors over here, and this is the network adapter. When a network adapter pushes some signal on to the cable, the transmitted packet travels in both the directions. (Refer slide time: 07:55 - 08:32) 

This is just a mention about 10Base5, because connecting a 10Base5 is more cumbersome and in 10Base2, connection would get loose quite often in the earlier days. Anyway, this is a 10Base5; it has a thick coaxial cable, the transceiver, and transceiver, drop cable and there would be a network interface card here with this MAC unit and protocol control firmware etc. As I said, this has become obsolete now. (Refer slide time: 08:33 - 09:21) 

Let us talk a little about repeaters. Repeater is a physical layer kind of device, which regenerates the signal, and provides more flexibility in network design because it depends on where we want to extend your LAN. So if you want to extend your LAN in some direction, make running into this distance limitation 200 m for 10Base2 etc. You may sort of increase that by just putting another segment on the other side of repeater so that you can go another 200 m. Therefore you can extend the distance over which a signal may travel down a cable; an example of a repeater is Ethernet hub. A hub has two purposes: one is to provide a collision domain and replace the cable. The other function of the hub is the repeater function; that means it regenerates the signal. This connects (Refer slide time: 09:48 - 10:02) 

together or a repeater or a hub. It connects together one or more Ethernet cable segments of any media type. If an internet segment were allowed to exceed the maximum length or the maximum number of attached systems to the segment, the signal quality would deteriorate. (Refer slide time: 10:03 - 10:36)

You can see that you have one cable segment here. This goes up to its maximum distance there is a cable segment too again going up to the maximum distance. But we put this repeater in between so that, we can increase the total distance that may be covered. These are the different computers connected to the cable. It is used between a pair of segments; this is a simple repeater to provide signal amplification and regeneration to restore a good signal level before sending it from one cable segment to another. (Refer slide time: 10:37 - 13:02) 

Hubs are essentially physical layer repeaters; bits come in one link and go out all other links. There is no frame buffering or CSMA/CD at the hub. Adapters detect the collisions and provide net management functionality, which means that this hub, although it is an active device, in the sense that this has some electronics in it, in its action it is more passive. Its active component is restricted to the fact that it regenerates the signal; it amplifies and regenerates it. So far as the other intelligence is concerned, like detecting collisions, it does nothing like that. If two signals coming at two different ports of the hub come to the hub at the same point of time, they will collide. So, a hub is a whole co-axial cable, which has been collapsed into one collision domain inside the hub; we could look at that way. But it does provide some net management functionality. That means, if you have a managed hub, there are two kinds of devices. The point is that a network is usually a geographically distributed, dispersed kind of entity; but this has to be managed. Therefore if there is a problem somewhere, may be some user has put in a complaint, what we will do is that there are so many things which connect from the user to the central network. There are cables, may be switches, hubs, and other things, so it is important that you should be able to remotely tell whether some active device is functioning. We cannot do this on passive things like a cable but there are managed switches and managed hubs etc., whom a central network station may interrogate and find out whether its health is alright or not. So if you have a managed hub, you have net management functionality; of course, the managed hubs are costlier than unmanaged hubs. (Refer slide time: 13:03 - 13:24) 

So this is the picture of hub. This is a co-axial cable coming in here and this is a t joint; the t joint is feeding into the hub. There may be a number of ports and a number of segments may be connected together. (Refer slide time: 13:25 - 14:40) 

Now, we come to 10BaseT; so far as the physical part is concerned, this 10BaseT can go on to become 100BaseT; that means, from 10 Mbps it can go to 100Mbps. But for this UTP, unshielded twisted pair cable of various categories like 3, 4, 5, etc., are the dominant physical medium and the varying technology today for local area networks. We can have category 5 cable data transmission up to 100 Mbps and we can go a little bit more on this depending on the distance. The impedance of a UTP cable is about 100 ohm, give or take another 15 ohm over the useful frequency range of the cable. The transmission speed is 0.6 to 0.65 ? c, which is about 2108 m/s. Time delay is about 5 nanoseconds /m. (Refer slide time: 14:42 - 15:40)

Here, you can see there are some 10BaseT repeaters and then 10Base-Fl segments. This is just to give you an idea about the various delays which are involved. The first segment delay is about 0.57 ?s for this 100-m segment, middle segment delay is about 10 ?s and last segment delay of about 0.57 ?s. The transceiver delay is about 7 ?s, Fl repeater delay is about 1.7 ?s, 10BaseT delay is about 21 ?s and so on. All these delays really add up and you have some limit on this in order for your CSMA/CD to work properly. (Refer slide time: 15:41 - 15:57) 


So this is the same data in tabular form. It has various types of cables and these are all fibre optic kind of cables, which go up to 2 km or 1 km. These are obsolete now ? these are the maximum lengths and these are the delays, the receiver delay and the delay at the repeater and so on. Now, what happened was that the people moved as the demand for bandwidth grew; so at least in the LAN it moved much faster when LAN bandwidth became quite cheap and everybody wanted to move from 10 Mbps to the next speed, which is 100 Mbps, and this was called fast Ethernet. One thing was that, when we come to fast Ethernet, this co-axial cable really was out. So the thing preferred was the unshielded twisted pair or UTP. Category 5 is preferred for 100 Mbps operation, although category 4 would also work. It is possible to transmit it over category 4 cables also; but category 5 cables are preferred. Now one thing about this UTP cables is that because they are used for connection from end to end unlike the coaxial cables ? in coaxial cables, we have one cable and many nodes may be connected at intermediate points ? you put a connector, usually an RJ45 kind of connector on the two ends and then go into two nodes. It is as simple as that. So if there are no nodes in between, just two end point connections, where this part of the medium is not shared, at one end may be the computer, and the other end might go into another machine straight away. These two machines network together. Usually the other end would go to a network device. Let us say, to start with, it might go into a 100 Mbps hub. So, all the nodes which are connecting, that means which are part of the network, all of them are connected by this UTP cable to the central hub. So the central hub is a shared medium at the collision domain. But so far as the UTP part is concerned, there is no collision. 

Just as we had the 10Base-T, which supported 10 Mbps, may be with the same kind of cable with this UTP cable, we could support fast Ethernet, that is, 100Base-T. There was 100Base-TX, But there are some problems with this: one is to increase to 100 Mbps, we have got one order of magnitude jump in the speed. That is not a mean achievement on the same medium. Depending on the quality of the medium, as the speed goes up, its performance tends to decrease. So depending on what kind of medium it is, there is only a certain amount of bandwidth in a certain range that you can support on that medium. So to increase to 100 Mbps, different encoding is used to reduce the bandwidth of the transmission. Encoding data at 100 Mbps using Manchester encoding would create a bit stream of 200 Mbps. So, we cannot straight away use Manchester encoding here, because this is for supporting 200 Mbps, we requires at least 100 MHz of analog bandwidth, which is too much. Category 5 UTP cable is rated up to 100 MHz but for keeping some margin etc., transmitting a signal of
100 MHz bandwidth would be unreliable. So what we do is that instead of Manchester encoding, 

we use a different kind encoding, which is called multiple or multilevel encoding. To reduce the bandwidth, a different encoding scheme is required. 100Base-TX uses a multiple level encoding scheme, that is MLT3, Multiple Level Transition 3. The bandwidth required in this case has come down quite sharply to 31.25 MHz. The encoding is somewhat like this ? 0: there is no transition and 1: there is a transition. And this transition could be of various types from low to 0, high to 0, 0 to low or 0 to high, depending on the context. 

For example, this is a 10110001110 that is being encoded. Suppose with the 1 there is a transition from 0 to low, that is, 0 to low level, then for 0 there is no transition, after 0 there is 1. Now there is a transition; since we go to low now, and the only place to go to is high, we go to a high and then there is another 1 and since we are going high, we go high some more, that is, another 1. Should this go from 0 to high, then three 0s, that is, no transition at all. Then from 0 to 1, now from high since 1 has come, you will have to come to low and again another one has come; we are going in the downward direction. So we continue from 0 to low and then, another 1 from low to high and then 0; there is no transition. So, this is the way an MLT encodes using different levels or multiple levels of signals. This reduces the net analog bandwidth requirement for the cable. So this is how, the ten-fold increase in the speed was achieved on the same physical infrastructure. So bandwidth required for MLT is half the bandwidth required for a two-level scheme.

Half is a rough figure. Long stream of zeros will cause the line to hold a constant voltage and lose clocking, synchronization is necessary to read the signal. This is one problem, which is handled in various ways. 

One way is to encode each 4-bit sequence as a 5-bit pattern 4B5B encoding. In this 4B5B encoding what is done is that instead of a 4-bit sequence we introduce a 5-bit sequence so we get some transition and we can hold on to synchronization. 

This pushes up the bit rate to 125Mbps for100BASE-TX, where data is only 125Mbps/4 = 31.25 MHz. Category 3 or 4 cable is used for a rate of 100 Mbps by using all four twisted pairs in the UTP cable; so this is also possible. Three pairs will transmit data while the fourth pair is used by each station for collisions. As in 10BASE-TX, a three-level signal is transmitted in 100BASE-T4. T4 is when we are using category 4 cable but the data rate must be minimised further. Therefore the signal is broken into 8-bit sequence and each 8-bit sequence is represented as 6 three level signals, or 8/6t. If we are trying to put in a new network today we are never going to use category 4 UTP, we are always use category 5 or better. You know about the increasing requirement of speed in the LAN, but these techniques were developed so that Ethernet could move into some existing physical infrastructure like wiring, where category 4 cable is used for moving from 10 to 100 Mbps; that is why these were developed. 

Fast Ethernet is done with cabling of the following types ? 100BASE-T4 with category 5 100BASE-TX; and 100BASE-FX is quite common. As I said, 10BASE-Fl has become outdated but 100BASE-FX is still there, which uses fibre optics and goes up to 2 km. Category 5 UTP goes only up to 100 m or may be some people are conservative and limit it to 75 m. At full duplex at 100Mbps, fibre is again full duplex at 100 Mbps; these are the advantages. The advantage of T4 is that, it uses existing category 3 UTP. 

These are the repeater delays. 

10BASE-T and 100BASE-TX wiring goes like this ? from the end station a maximum distance of 100 m to the hub. We can use multiple hubs, a maximum of four, to increase the distance between any two stations; a hub may go to another hub; that may go to a station again. Now, there are three segments. We can use a maximum of four hubs and five segments with 100 m from the node to the hub. 

For upgrading from 10BASE-T to 100BASE-TX, we need new hubs or switches. You may have some 10 Mbps ports to handle 10BASE-T; NICs may have auto sensing 10/100 ports that handle either. Auto sensing ports are quite common. As a matter of fact, most of the network interface cards you get today are 10/100; that means they are auto sensing ports. An NIC senses the network device at the other end. If the other end supports a rate of 100 Mbps, this NIC will operate at 100 Mbps. If it senses that the opposite end can handle only 10 Mbps, it will use only 10 Mbps. By the way, if there are problems with the varying connection, etc., it may send 100 Mbps port as 10 Mbps. So, if you want to upgrade from 10BASE-T to 100BASE-TX, you may need new NICs, and only for stations that need more speed and no need to rewire ? that may be a big advantage in some cases. 

We may have 100 multiple hubs, one connecting to the other; you can have multiple hubs for connecting etc. 

In 100BASE-TX there is a limitation that you can have only maximum two hubs and they must be within a few meters of each other. So, the maximum span using these hubs is only 200 m, which is shorter than 10BASE-T. It could be a big problem and that is why 100 Mbps hubs are also becoming obsolete. People have migrated from hubs to switches, and that is what we will be talking about. So if you had used hubs, this would have been a limitation; but nowadays people do not usually use hubs. 

We will come back to the topic about hubs and switches later on. What happened is that this development went on and now we are moving into the era of Gigabit Ethernet. The Gigabit Ethernet in the desktop is still not very common, but it is coming and for servers etc., nowadays we routinely get Gigabit ports and, may be, soon your desktop computer may also be connected with Gigabit ports. So, this is a depiction of a two-station Ethernet ? two computers connected straight away, this is a switch or a hub which operates at Gigabit speed. So, these allow point-to-point links and shared broadcast channels through some hubs if they are there. In shared mode, CSMA/CD is used for the short distance between the nodes to be efficient; but this is not very common.

It uses standard Ethernet frame format 802.3z; the normal mode is full duplex mode with a central switch. This is the most common one connected to computers. In this configuration all lines are buffered. 

On the right-hand side, you see one switch and the nodes are connected straight to the switch. So this is a switch rather than a hub, where each of the ports has a buffering. 

Since each of the port is buffered, contention is impossible; because data can always and gets it on the buffer. So, it makes no sense to the channel to see if it is idle or not because if there is no contention, CSMA/CD protocol is not used at all. And this is the mode, in which Gigabit is usually used, 


although there are buffered distributors also. Now people are talking about moving to 10 gbps, at least in the metropolitan area network. They use copper and fibre with 850 nm or 1300 nm lasers and some kind of encoding. The Ethernet is moving really fast, because it is upgrading quite fast ? it went from 10 mbps to 100 mbps; from 100 mbps we are moving to 1gbps; and from 1gbps, we can see the possibility of moving to 10 gbps, and 10 gbps is already in operation today. From 10 mbps to 10 gbps is a 1000-fold increase in a very short span of time, which is really amazing. For Gigabit (Refer slide time: 32:16 - 32:56) 

Ethernet, we have 1000Base-SX; this SX shows the multimode fibre for 1000Base, so the maximum segment size would be about 550 m. 1000Base-LX uses single or multimode, so this goes upto 5000 m. Shielded twisted pair would only take to 25 m and 1000BaseT can go up to 100 m using category 5, category 5b or, for gigabit Ethernet, category 6 is preferred. We will talk more in detail about the switching, which is very important. So, Ethernet is (Refer slide time: 33:17 - 33:38) 

usually considered as a shared media LAN. Only one station can be transmitted at a time even when you have multiple hubs, these hubs are all shared domain, which means that only one can transmit. All other stations must wait while one station sends. So this is the big limitation of latency and congestion with hubs (Refer slide time: 33:39 - 33:45) 

To improve this, people moved over from hubs to switches. Packet switches are very useful. They give you improved security. Users are less able to tap into other users? data. In a shared medium, we know everything is being broadcast so all packets are going to every other node. That is poor security; this gives you much better security; better management ? we can control who receives what information. That means, even using a switch, even within a LAN, we can create a work group and make users members of different virtual LANs. These virtual LANs may be packet filtering between the virtual LANs, etc., to give a better management and limit the impact of network problems. This switch will also give you full duplex communication, rather than half duplex. One side effect of having a shared medium is that only one sends, and every body else is in the receiving mode. Obviously that channel cannot work as a full duplex mode. Because two of them cannot send simultaneously at the same time, only one may be sending and the other may send. At most, this works as a half duplex channel, whereas over here this can work as full duplex channel. A hub simulates a single (Refer slide time: 35:20 - 36:07) 

shared medium, whereas switch simulates a bridged LAN with one computer per segment. For example, we have only one computer and  many segments, and there is only one computer for each segment. So it does not have any collision and from two different computers you can send data at the same time to the switch. The switch will buffer it and  there will be no collision. So collision detection may be eliminated. Some of these may connect to a hub instead of going to a computer. So a switch may be connected to another switch, that switch may connect to hub, and that hub may connect to so many computers. So there are many possibilities. (Refer slide time: 36:08 - 36:01) 

Ethernet switches are highly scalable. We had 10Base originally; we went from 10BaseT hubs to 10BaseT switches. 10BaseT switches are increasingly being replaced by 100Base-TX switches with auto sensing codes. 100Base-TX switches are very common these days, they have higher performance and their costs have come down. These days we get 100Base-TX switch at the cost of what a 10BaseT hub would cost a few years ago. We can see how the technology and the economy have improved. Gigabit Ethernet switches are still expensive; it will take some more time in order for them to move to the desktop. (Refer slide time: 37:02 - 37:16) 

Other advantages of switches are that there are no limits on the number of Ethernet switches between the farthest stations and no distance limit on size of switched networks. You remember when we were talking about 100Base-TX and when we were using 100 base TX hubs. You can only have a maximum of two hubs and these two hubs have to be close together; they cannot go far long. So, the maximum span of a local area network could almost be 200 m, which was really a limitation, but then people moved to100Base-TX. They also moved from hubs to switches. With these switches, now there is no problem because the distance limitation had to do with the round-trip propagation delay and things like that. Since there is no contention and the switches have buffers, there is no distance limitation at all. You can have any number of switches connected to each other; there is no distance limit on size of switched networks. So often individual hosts, are connected to switches and may be different switches are also connected. This is Ethernet without collisions or CSMA/CD. (Refer slide time: 38:17 - 38:55) 

Here, we have A, B, C, and so on. Suppose A wants to communicate with A' and B wants to communicate with B', both the communications may take place simultaneously. This is a switch. If this had been a hub that would not be possible but since this is a switch, B can communicate to B' and A can communicate to A', just as in a telephone switch, two pairs or distinct pairs of people can communicate at the same time. Similarly, two distinct pairs of machines, through this packet switch, can communicate at the same time. (Refer slide time: 38:56 - 39:22) 

This is essentially a multi-interface bridge; it has a layer 2 frame forwarding, filtering using LAN addresses; LAN addresses mean MAC addresses. We will talk more about MAC addresses presently. It allows switching A to A' and B to B' simultaneously, but there are no collisions. This can have a large number of interfaces. A modern switch may have hundreds of ports, so hundreds of machines can be connected to the same switch. Similarly, a number of such switches can connect to each other. Ethernet switches (Refer slide time: 39:34 - 40:04)

must be arranged in a hierarchy or daisy chain with only one possible path between any two stations. Suppose, from 4 to 3 there is only a single path that is 4, 2, 1, 3; you do not make a loop connecting two ports of a switch because that would really create a lot of problem. Therefore, Ethernet switches must be arranged in a hierarchy or daisy chain. There is only one possible path between any two stations or switches. (Refer slide time: 40:05 - 42:27) 

A station is an Ethernet frame switch plug-in card. It checks to see if it is destined to the same card. If so, the frame is copied there. We mentioned that in a packet switch, usually the ports would be grouped into small groups and each of these groups will go into a particular line card. This line card is similar to the Ethernet. A switch is nothing but a packet switch; there will be line cards over there and may be 4 lines coming to one particular line card and may be there are 8 line cards for a 32-port switch. Now, for those that are coming into the line card, there are variations of how they are handled by the line card. It could be that a frame which has arrived in a switch plug in card is destined to the same card; that means it is destined to some other line in the same card. If so, the frame is simply copied there; if not, the frame is sent. Now these line cards should get connected to each other through the backplane. If the frame is sent over from the high-speed backbone to the destinations card, it will travel to the other distinct destination through the switching fabric. This backplane may be active or passive; they are of various types. Anyway, the backbone typically runs at many gigabits per second. As a matter of fact, nowadays it is possible to have a high-end switch with something of the order of 100 gbps backplane switch. It is a very high-speed switch that can support a lot of machines at the same time. If two machines attached to the same plug in card transmit frames at the same time, what would happen? There are two machines attached to the same plug in a card and they transmit frames at the same time ? what would happen would really depend on the way the card handles it; here there are some variations. For example, let us say, there are 4 line cards ? 1, 2, 3, 4 ? coming into this one particular line card. Line 1 and line 2 frames arrived simultaneously and both of them are destined to line 3; what is going to happen? It depends on how it is handled. One possibility (Refer slide time: 42:54 - 43:41) 

could be that all the ports on the card are wired together to form a local, on-card LAN. This LAN card itself is a local on-card LAN, and on this on-card LAN, there will be a CSMA/CD network. CSMA/CD may operate on that particular card itself; in that case, there will be a collision that will be detected. Only one transmission per card is possible at any instant of time, so if you are doing that, you have made that line card to be a shared medium. All the cards can be transmitting in parallel; the different cards may be transmitting in parallel. Each card forms its own CSMA/CD network independent of the others; that is one possibility. (Refer slide time: 43:42 - 44:04) 

The other possibility is that the plug-in card of each input port is buffered; so incoming frames are stored in the cards on board. This allows all input ports to receive or transmit frames at the same time for a parallel, full duplex operation. This is not possible with CSMA/CD on a single channel. Previously we had talked about buffers in packet switches. We mentioned how buffers could be on input side, how buffers could be on output side and distributed through out the switch fabric. That was from the point of view of handling head of line, blocking, switching, speed etc. Here, it is slightly different in the sense that if there is some small buffer at each of the ports and on input side, full duplex transmission at the same time is possible on all the lines. Instead of making a local CSMA/CD and allowing some kind of collision, this naturally is a better kind of switch. Once a frame has been completely received, the card will then check to see the frame is destined for another port on the same card, or for a distant port. In the A case, it can be directed to the destination; in the B case it must be transmitted over the backbone to the proper card. (Refer slide time: 45:12 - 45:21) 

Each port has a separate CSMA/CD so collisions do not occur. There is now no question of any CSMA/CD because each port has only one machine. (Refer slide time: 45:23- 45:51) 

Then there is another term called cut through switching: frame forwarded from input to output port without waiting for assembly of the entire frame. That means even when the frame has not fully come in, the bits are transmitted and there is a slight reduction in latency. So, combinations of shared dedicated 10/100/1000 mbps interfaces are possible. On the same switch you might get some 10 mbps ports, some 100 mbps ports, also some 1000 mbps or Giga bit ports for uplinking to the main backbone. (Refer slide time: 46:03 - 48:04) 

This is a typical LAN of an IP network. Let us look at this figure ?  some hubs are remaining, but these hubs will give way to switches from 10BaseT to 100BaseT very soon, but let us say, there is a legacy system, through which some 10BaseT hubs and some computers are connected to this. These hubs may be connected to a switch. This switch could be connected to another switch and that switch might connect to some other nodes, etc. This constitutes the LAN ? may be there is a 100 mbps connection from this switch to the mail server, because mail server may be something which everybody is using. This is a server, and you might want to have a higher speed of this one also ? these are 10 mbps ports. You might want to make it 100 mbps. There is a www server, that is, a web server, which is also connected through a 100 mbps port. So from the same switch, you may connect to some hubs, you may connect to some servers, you may connect to some individual desktops, PCs or machines. So this is what the LAN part looks like. And of course, nowadays nobody would want a LAN which is standing by itself. People want to connect their LANs to the internet, which is the great network of networks. There are probably lot of networks, may be millions of networks, in other places; we want to connect to other networks also. So we have to go through what is known as a router. We go through a router to the external internet. We will be talking about this part when we talk about the network layer in more detail; so this is a typical picture of a LAN. (Refer slide time: 48:05 - 48:34)


If we abstract this a little bit, we have a LAN, and different nodes or the different machines are connected to this LAN. And then there is a router, which connects you to the internet. This internet may be a connection between a lot of different networks, may be a lot of different LANs to their own routers etc. This is a slightly more abstract view. (Refer slide time: 46:35 - 48:39)

And then looking into it in detail in the LAN part of it, if we go to the previous picture, (Refer slide time: 48:40 - 48:44)

this is the LAN part of it and these nodes are connected. These nodes are usually connected (Refer slide time: 48:45 - 51:34) 

through adapters. That means the network interface card, NIC, or the adapter. These nodes may communicate with each other in the LAN if it is connected to the internet. Naturally it may communicate with the other machines in the outside world also; but let us for the time being just focus our attention on the nodes which are in the LAN, and they are sort of trying to talk to each other. If they want to talk to each other using either a hub or a switch, or in this case, the Ethernet frame format, what is going to happen is that the frame format has certain fields and those fields have to be filled up. That means the destination address has to be there, and the destination address is the LAN address or the MAC address or the hardware address. Sometimes this is also called an Ethernet address. And this Ethernet address is distributed by IEEE; the manufacturers of these adapters buy blocks of addresses from IEEE. And these are all 6-byte addresses, something like this: 1a23f9, each of then is 1 byte. And if you look at this figure, say 1a23f9cd069b and 88b22a do not have any relationship with each other. They are just 6 bytes, may be 6 arbitrary bytes. This card may have come from some manufacturer and got entirely different blocks from IEEE, the other may have an entirely different address, which has no relationship with the other; but they are all sharing this LAN. There is a small problem ? how does the source node get to know the destination address? One thing is sure ? since one central agency, namely IEEE, gives this Ethernet addresses, each adapter on LAN has a unique LAN address; no two adapters will ever have the same address 
Sometimes, there may be some nodes with a number of LAN addresses, with a number of adapters etc., but one particular adapter will have one unique address, two addresses will never be the same. The point is that, how do get to know this is the source? This is a destination ? how does the source know the destination, LAN address or the MAC address? (Refer slide time: 51:35 - 51:46) 

There are at least two kinds of addresses, which are used in networks. One is the IP address, which is used by the network layer to decide on the total route from the source to the destination. What will happen is that it will know where it will have to go; in order to reach its destination address, it must have some kind of relationship with geography. And then, there are these hardware or MAC addresses, which are at a lower level and are just as unique; they do not have any relationship with each other. So we have this 32-bit IP addresses, (Refer slide time: 52:32 - 52:50) 

network layer addresses used to get datagram to the destination IP. We have LAN, MAC, physical, Ethernet or hardware address used to get datagram from one interface to another physically connected interface in the same network. So this is a 48-bit MAC address for most LANs burned in the adapter ROM. (Refer Slide Time: 52:51 - 53:01)

Analogy of MAC address is like a social security number; IP address is like postal address. The question is how do you get to know the (Refer Slide Time : 53:02 - 53:16 )

other person?s address? Address translation: how does a host know the MAC address of the other host in its local area network? And secondly, how does a router determine the MAC address from the IP address of a host? This are two interrelated questions. (Refer Slide Time : 53:17 - 53:36 )

We need some kind of a translator for IP addresses from MAC addresses. Address translation can be static or dynamic; static translation means you keep some static table. Static translation requires a lot of book-keeping work from system administration and this is not feasible. (Refer slide time: 53:37 - 54:39) 

We have this address resolution protocol or automatic resolution protocol, the so-called ARP protocol, which is a dynamic address translation scheme. If the destination IP address is not in the local ARP cache ? ARP cache is a table, which is stored in the local machine and which gives this IP address to MAC address mapping (the broadcast query for that address) ? at the most one host will reply because the IP address will match with only one of the host addresses. So, the ARP table entries are cached for up to 15 minutes, unless if refreshed. It is firmly at 15 minutes so that a machine can go from one LAN and can also be shifted to another LAN. What will happen is that these old entries must automatically wash out. And if the new entries communicate to their communicating machine, their MAC addresses are going to come into the cache. So this is how it is always kept fresh. ARP uses layer 2 broadcasting (Refer slide time: 54:40 - 54:59) 

Each IP node on LAN has an ARP table. ARP table is IP, MAC address mapping for some LAN nodes. And there is a time to leave, something like 15 minutes or so, it depends on the time after which the address mapping will be forgotten ? it is typically around 15, 20 or 25 minutes. (Refer slide time: 55:00 - 55:36) 

So starting at A, we want to send a datagram addressed to B. We look up the net address of B or link layer. What will happen is that we want we do not need B?s MAC address. That means this 2 23.1.1.1 is an IP address ? we know it is B?s IP address, but we want to know its MAC address. So A?s IP address, B?s IP address and some IP pay load form a frame like this, which is broadcast. (Refer slide time: 55:37 - 56:08) 

A wants to send datagram to B and A knows B?s IP address. Suppose B?s MAC address is not in the ARP table, A broadcasts an ARP query packet containing B?s IP address. All machines on LAN receive the ARP query. B receives the ARP packet; replies to A with B?s MAC address; the frame is sent to A?s MAC address, which is unicast. It is now specifically given as MAC address is known, because A sent the query with a source address. (Refer slide time: 56:09 - 56:34) 

A cache saves IP to MAC address pair in its ARP table until information becomes old, that is, it times out. But if it is being used, then it will not become older. So in a soft state, information that times out, goes away unless refreshed. ARP is plug and play, otherwise it will get washed out and new entries will come in automatically. Nodes create their ARP table without intervention from the node administrator. (Refer slide time: 56: 35 - 56:53) 

All hosts receive the request but only B responds with the IP address; however, other nodes may update their own cache from the transaction. The cache is required since broadcast is a very expensive proposition and more than one packet transfer is very likely between any two stations. (Refer Slide Time:56:54 - 57:12 )

This is an important question, what is a network? We always talk about networks; it is the interconnected nodes in the same broadcast domain. We are doing the broadcast ? this broadcast is limited to that domain. Networks are connected through routers. (Refer slide time: 57:13 - 57:30) 

When we want to route to another LAN, it is more involved. We will talk about going through a router and then we will specifically discuss bridges. Thank you. (Refer Slide Time: 57:45 - 57:47) 


Today we will be talking about local internetworking. (Refer slide time: 57:56 - 58:18)

Our topic of today is local internetworking, what is internetworking? Internetworking is connection of different networks, everybody is aware of the term internet today why are comes to this term internetworking and by internetworking we mean connecting different networks. Just to remind you, (Refer slide time: 58:20 - 58:40)

What is a network? We discussed this in the last lecture that the interconnected nodes in the same broadcast domains. And networks are usually connected through routers but as we will see for local internetworking we may not need a router we may need something called the bridge we will come to that. This is the set of nodes which are in the same broadcast domain, this broadcast may be a good thing to have for some applications. But for the operation of the network there is one very crucial reason, why we require the broadcast that is to discover the MAC addresses of the different computers.




	











COMPUTER NETWORKS
Prof. Sujoy Ghosh
Department of Computer Science and Engineering
IIT, Kharagpur
LECTURE# 21   
LOCAL INTERNETWORKING
(Reference Time: 00:43)
Good day. Today we will be talking about Local Internetworking (Refer Slide Time: 00:51 ? 01:13)

What is Internetworking? Internetworking is the connection of different networks. Everybody is aware of the term internet today. Internet comes from this term Internetworking, and by Internetworking we mean connecting different networks. (Refer Slide Time: 01.13 ? 01:35)


What is a network?  As we discussed in the last lecture, the interconnected nodes in the same broadcast domain form a network, and they are usually connected through routers. But for local internetworking, we may not need a router. We may need something called a bridge. Thus, a network is the set of nodes, which are in the same broadcast domain. This broadcast is advantageous for some applications, but for the operation of the network, a crucial requirement is the broadcast. That is used to discover the MAC addresses of the different computers. You can send an ARP message and get the MAC, which will give the IP address. That means you are basically finding out the MAC address of the machine whose IP address is given. The machine will then reply with its own MAC address. That MAC address has to be put in the destination address of the data link layer frame. This is how it operates in the single network. If you have two networks which are connected to each other, internetworking is required for so many reasons: (Refer Slide Time: 2:47-2:51)




two or more networks can be managed as a single network. That could be one advantage of interconnection. Similar type of computers can communicate with another; that is electronic mail, etc., going across the networks. This is another advantage. (Refer Slide Time: 2.52-04.18)


If you have two networks connected, there may be multiple routes between nodes, which help to create alternative communication routes, when links are either not operating or they are busy. The one important reason for local internetworking is to have the capacity to isolate traffic from other networks. As we have seen, if you want to find the MAC address, you send an ARP broadcast. But if the network is large, many people would be broadcasting and the broadcasting load on the entire network will become very heavy. So as the network grows, as more and more nodes and more and more computers get connected to the network, at some point of time, the performance will be getting degraded because so many nodes are sending broadcast messages. The network has to be broken, although it may be in same building. So the scope of broadcast becomes limited. (Refer Slide Time: 04:19-06:50)


To have the capacity to isolate traffic from other networks ? here we are referring to broadcast traffic ? and to access the information on remote sites are the advantages of connecting networks. Look at the problem, which we had glimpsed earlier. Suppose there is this node A, which wants to send a message to a node B. A knows B?s IP address but they are in two different LANs which are separated. Nevertheless, they are connected through a router to have some physical connection. A router is some kind of network device that enables two different LANs to communicate with each other. We will see what actually this router does and whether the same thing applies to the bridges. When we come to bridges we will discuss the basics of a router and what it contains and other details when we discuss the network layer. But right now, we will just talk about the ARP and the MAC layers. So let us say that there are two ARP tables in router 
 for this LAN and another one for that LAN. So there are two ARP tables in the router, one for each IP network. In routing table at source host, it has to find the router. First of all, the host must find the router. For finding the router it once again must know the MAC address, which is given over here as something like E5 or E9 etc., which does not make too much sense to human beings. It is just a bunch of bytes, 6 bytes actually. So the host, that is, A, has to know the router in order to send it to B; or in order to even know the address of B, A must somehow communicate with the router; and for communicating with the router it must know the router?s IP address. But finally, these two hardware adapters should communicate. So they have to know each other?s hardware address or MAC address. It will find the MAC address in the ARP table at source. If it has already communicated with the router, the address may already be in its ARP table. It will do an ARP and find out the MAC address of the adapter of the router. (Refer Slide Time: 06.51-07.22)


Then A  creates a datagram with source A and destination B. Here source A and destination B mean the IP address of B and IP address of A, i.e., the network layer address. This should all be in the packets, which are coming down from the network layer. A uses ARP to get R?s MAC address for this. A creates link layer frame with R?s MAC address as destination; the frame contains A to B IP datagram; A?s data link layer sends the frame. (Refer Slide Time: 07.23-08.09)


R?s data link layer will receive this frame on that particular adaptor. R removes IP datagram from the Ethernet frame. Now it is stripped of the data link layer header and trailer and the router sees that this is destined to B. By looking at the IP address, the router would know that it is not in LAN 1 but in LAN 2. So R uses ARP to get B?s physical layer address. Now through the other adapter, R must communicate with LAN 2 and specifically to B. For that, it has to know B?s address and for that, it does an ARP in LAN 2. Then R creates a frame containing A to B IP datagram, and sends it to B. (Refer Slide Time: 08.10-09.10)



In this way, A communicates with B and the original packet contains the source IP address as this may be the IP address of A and the IP address of B. This is just for example, and these numbers don?t matter. There is a 4-byte IP address and that datagram is sent with A?s source MAC address to the router. Router finds the MAC address as destination B, does the ARP in the LAN 2 and finds out the MAC address, which is 49B-D. Then R forms the net frame, which contains the original packet which was sent by A?s network layer. But now it has the source as the MAC address of the router and it sends it to the MAC address of B through the LAN. This is how the whole scheme works. (Refer Slide Time: 09.11-10.18)

We will see just a couple more slides for finishing this ARP. When sending an ARP request the sender includes its own binding; that means, its own IP address and MAC address. All machines in the local network can extract the bindings from the ARP traffic and store it in its cache. Remember all other nodes don?t have anything to do with this ARP, but still they listen to this ARP traffic, which is going on and whatever bindings it can, it extracts and puts it in its cache, so that if this particular machine wants to communicate with any one of those, it can bypass the ARP and get it straightaway from the ARP cache, rather than doing a broadcast on the network again. A system can notify others of its address by sending an ARP when it boots, because it includes its own binding in the ARP message. (Refer Slide Time: 10.19-10.56)



ARP is a low-level protocol that hides the underlying network?s physical addressing, permitting one to assign arbitrary IP address to every machine. We think of ARP as a part of the physical network system and not as part of IP. So, ARP has to do more with the data link layer rather than the network layer. The other protocol, which is predominately related to the IP address, will be discussed later. To accommodate various systems, ARP uses variable length packets because these ARPs may be used in different networks (Refer Slide Time: 10.57-12.07)


So this is just a part of ARP message format; there are other parts of it. The format used is, Hardware type: 2  bytes. (For example for Ethernet you will get a value of 1), Protocol type: 2  bytes (0800 for IP), Hardware address length: the ARP may be used in various networks and various networks may have different classes of hardware addresses, which are of varying length. If it is just the Ethernet address, there will be 6 bytes but for other kinds of networks it may be something different. So the hardware address length mentioned here is 1 byte; protocol address length is mentioned in 1 byte; but operation is 2 bytes of ARP; i.e., it could be ARP request/response, RARP request/response, etc. (Refer Slide Time: 12.08-14.50)


What is RARP? RARP stands for reverse ARP. What is ARP?  ARP is used to find the MAC address of the machine having a particular IP address and RARP is the reverse. What is the IP address of the machine having this particular MAC address? RARP is required to know one?s own IP address, when one can?t store one?s IP address locally and specifically. One place where it is quite often used is in diskless machines. Nowadays, there is a concept of diskless machines and they are also known as thin clients, which use the computational power and the disk storage, etc., of a central server. These thin clients, since they have minimal functionality, are easier to maintain. It may be a little cheaper to upgrade them, as it is easier to upgrade one central server rather than a whole bunch of PCs. So, if you have a thin client and a server in a network, they communicate with each other in an IP network. But for communicating, for being an entity in the IP network, you must have an IP address. How is this IP address to be stored in this diskless machine, which does not have any disk, to store anything permanently? Whenever you switch it off, all of it becomes volatile, except for the small ROM. So when it boots up, it has small program in its ROM, which can do some elementary processing. It can find out its own MAC address because it is just local and give that MAC address with an RARP, asking for its own IP address and the server will assign an IP address to it. So RARP is used by diskless machines to get its IP address, which may be in a server. Like ARP, RARP is also sent in the data portion of the frame. The frame type contains 8035 and the data portion contains 28 octets. There may be primary and back-up RARP servers. There are two other protocols like BOOTP and DHCP, which are successors of RARP, and they have to do more with the network layer. We will talk about BOOTP and DHCP later, when we talk about the other TCP/IP protocol suites. (Refer Slide Time: 14.51-16.51)


Till now, we have seen what internetworking is; it means connecting two different networks together. If this internetworking is local, i.e., may be  in the same organization or may be in the same building or in nearby buildings, in the same campus, etc., you may not require the full power of a router for the networking part. You may do it with a data link layer device, called a bridge. We will be discussing about bridges today. Bridges can connect different networks. As a matter of fact, they can connect different networks of different types. For example, a bridge can connect 802.x to 802.y.  These x and y may have the same value; i.e., both of them may be 3, that is, both of them may be Ethernets or one may be an Ethernet and another may be a Token Ring, something like locally connected small LANs. If a LAN is big, bridges can?t handle it any longer. It is a data link layer device and it follows a protocol from IEEE 802.1, which is a spanning tree of bridges. If you remember how this IEEE 802 protocols are organized, 802.1 is put at the top as it gives an overview and a few things which are generally applicable to all the layers. Then we have 802.2 for link layer, etc., and 802.3 is Ethernet 4 for token bus, token ring, etc. So we will look at some more 802 protocols when we discuss wireless networks. A bridge may connect different types of networks and that is why it is put in 802.1; this uses spanning tree protocol. We will discuss the spanning tree protocol presently. (Refer Slide Time: 16.52-17.20)



A local internetwork picture would look something like this. Suppose there are four LANs: LAN 1, LAN 2, LAN 3 and LAN 4, which are connected by two bridges. Bridge  has two ports ? one connecting to LAN 1 and the other to LAN 2. Bridge  has three ports, each one connecting to LAN 2, 3 and 4. Now, A can communicate with H through two bridges. (Refer Slide Time: 17.21-18.18)

Let us see the salient points of a bridge. First of all, it is a link layer device. It stores and forwards Ethernet frames, which means it is related to the MAC address rather than the IP address. These bridges handle the hardware addresses. It examines frame headers and selectively forwards frames, based on MAC destination address. This means that (in the previous diagram) when  gets some frame from here, it will look at the hardware address and decide whether to send it to LAN 4 or to LAN 3. It would selectively send it to one of them. When the frame is to be forwarded on segment, it uses CSMA/CD to access the segment. So that is the protocol. (Refer Slide Time: 18.19-19.46)



Bridges are transparent, that is, hosts are unaware of the presence of bridges. To the host the whole thing might look like one single network. Since it is transparent and plug-and-play device, it will do some self-learning and start operating. In the beginning of the self-learning phase, it may be little inefficient but later on its efficiency will improve as it learns more. Bridges need not be configured, which is another advantage. A bridge separates LANs if we want each group?s traffic to remain within its own LAN. There are other reasons like security issues to maintain the traffic within one?s own LAN. (Refer Slide Time: 19.47-20.12)



At the physical level, the bridge boosts the signal strength like a repeater or completely regenerates the signal. Just like a hub or a switch, before forwarding the signal, it will boost up the signal, i.e., the signal is being regenerated, which is highly advantageous. Bridges usually use the same protocol on either side; for example Ethernet-to-Ethernet or Token Ring-to-Token Ring. (Refer Slide Time: 20.13-20.25)



They also convert between protocols. For example, Ethernet-to-Token Ring Protocol Conversion is possible in bridges. Also bridges are fine for medium-sized organization but are totally inadequate for large installations. They would require routers (Refer Slide Time: 20.26-21.27)


A bridge stores the hardware addresses observed from the frames received by each interface and uses this information to learn which frames need to be forwarded by the bridge. It will maintain a table in itself and for each of these interfaces and in that table it will store all the hardware addresses it has seen in the segment. So it will know the location of the machines in the different segments. If it gets a packet from LAN A, which has a hardware address for the machine in LAN B, it will send it to LAN B. Each bridge has physical interfaces, the data link layer, the address table and the filter table. How is it filtered before being forwarded? Using the information from the address table it learns which frame needs to be forwarded by the bridge. (Refer Slide Time: 21.28-21.31)




This is just an example in the form of a picture (Refer Slide Time: 21.32-22.58)



So bridges forward information only in the form of packets to the segment where the destination host is connected to. Bridges can construct (learn) forwarding table from the source address of the packets which have recently been forwarded to it. So whenever there is an ARP on that side of the LAN, the port of the bridge that is connected to it will also get that ARP request. Then it finds some of the bindings and it quickly learns and fills up its table. What will happen if a host is moved to another segment or if a new host is connected to a segment? If a new host is connected to a segment, this learning becomes a continuous process. If a system is moved from LAN 1 to LAN 2, the network interface card, which is present in it, also goes along with the machine from LAN 1 to LAN 2. Since the particular MAC address has moved from LAN1 to LAN 2 the table entries of the system moved will get erased and the data remain fresh and relevant. (Refer Slide Time: 22.59-23.39)




In the above slide, the green ones are the different LAN segments and ,  till  are seven bridges and they are connecting multiple LANs. You can connect multiple LANs like this; but it has a problem of looping and now we will discuss how to avoid that. (Refer Slide Time: 23.40-24.49)

For increased reliability, it is desirable to have redundant alternative paths from source to destination. With the multiple paths, cycles result and so bridges may multiply and forward frames forever. The frame may go on and on without a frame getting dropped, because in the bridge and in the data link layer there is no concept of a particular frame moving around for a long time. Since there is no way to handle that, we do it in IP layer. We will discuss about that later. Once the frame starts circulating, that means, going in a cycle, it will go on and on and such frames may actually increase in numbers and then bring down the whole network, which is not acceptable. The solution for this is to organize bridges in a spanning tree by disabling subset of interfaces willfully, in the sense that we don?t use them. (Refer Slide Time: 24.50-25.12)


Suppose you have a graph as shown above, there could be cycles over here but if you disable these two interfaces of the bridge, you no longer have a cycle. So with these three paths shown you can?t have a cycle. (Refer Slide Time:  25.13-26.37)




We can think of the extended LAN as a graph. Its nodes are the LAN segments and bridges. Edges are bridge-to-segment links. This is how a graph should be constructed. Now construct a tree from the original graph keeping all segment nodes and removing some bridge nodes and edges. The individual LAN segments are the nodes and for the bridge to the LAN connection, we form an edge. This graph may have cycles. We remove the cycles minimally by still keeping the graph connected. If the original graph is not connected, i.e., if it is not possible to go from one node to another, then in the diagram there is no physical connection and hence you can?t do anything about it. But if the original one was connected and when we disable some of the links to avoid cycles, we must keep it minimally connected and so all the nodes are retained. That is why it is called a Spanning Tree. So, finally what we want is to have a Spanning Tree with some of the edges disabled to avoid cycles. (Refer Slide Time: 26.38-26.59)



The above slide is an example graph with lots of cycles. We remove some of the edges so that the graph still remains connected and it is still possible to go from any node to any other node without a cycle in this graph. This  is the Spanning Tree in which all the  nodes are still connected but there is no cycle. (Refer Slide Time: 27.00-27.34)


For this, we use the spanning tree algorithm from IEEE 802.1. The basic idea is that each bridge decides which ports it should forward packets to, so that the resulting network is acyclic and the resulting network interconnects all segments. Assume each bridge has a unique ID and each one knows its own ID. By the algorithm, we assume that the bridge with the smallest ID is the root bridge and this root bridge forwards packets to all its ports. (Refer Slide Time: 27.35-29.03)




How do non-root bridges compute the shortest path to root? Some algorithms are distributed algorithms, i.e., each node does some computation using the locally available knowledge. But this locally available knowledge may not be consistent with the global picture as the global picture is not known. Since the local things are known, we go for distributed algorithm instead of a centralized algorithm. We do use centralized algorithm, gather all the information together in one place, and then do the computation. Centralized or traditional algorithms are obviously easier but it is more difficult to write distributed algorithms. But you don?t have any option. You have to write a distributed algorithm. A node or a bridge in this particular case can do the computation only based on what it knows locally. It does not have the global picture. It tries to form a global picture and that is the task of the algorithm, to form that global picture. Each LAN has a single designated bridge closest to root and the tie-breaker is the minimum bridge. All packets of a LAN are forwarded only to that LAN?s designated bridge. We will look at this algorithm in more detail. (Refer Slide Time: 29.04-30.04)




Bridges exchange configuration messages to determine spanning tree in a distributed manner. The configuration message (CM) consists of three things, S, R and H, where S is the bridge ID of the message sender, R is the bridge ID for the assumed root. Whoever is sending this CM, assigns the best value for the root; that is R. H is the distance in hops from message sender to the assumed root, so H is the distance from R to S, as is known to this particular bridge. So CM gives these three things: bridge ID, root ID, and the distance from the root to the particular sender. (Refer Slide Time: 30.05-31.26)


When there are two CMs, we say CM-1 is better than CM-2, under three conditions. When CM-1 identifies root with smaller bridge ID; when both the CMs give the same root ID you can?t say which one is better based on this but CM-1 is closer to root; when both CMs identify same root and distance to the root, but CM-1?s sender has smaller bridge ID. In these cases, CM-1 is preferred. Initially all bridges assume that they are roots and generate CMs. As the algorithm starts, all the bridges assume themselves to be root and send initiating CMs to all its neighbor nodes. (Refer Slide Time: 31.27-32.23)



Each bridge remembers the best CM it has received or sent. The best CM is the value of the smallest root it has received. If more than 1 value of the root is same, the best CM is the one in which it can reach the root with the smallest number of hops. Bridges use the best CM to determine true root and to compute the distance to root. A bridge stops generating CMs when it realizes that it is not the root. After that point, it simply forwards all CMs it receives. (Refer Slide Time: 32.24-33.10)



A bridge stops forwarding CMs to a segment when it receives better CM from that segment. Suppose a bridge is sending a CM with some particular root; and through some other segment a CM has come with a better root and the better root and the path to the better root are through the original segment, then that segment will not get CMs forwarded from this particular bridge. Then where does this algorithm converge to? (Refer Slide Time: 33.11-35.37)



Let us say  will send CMs to , ,  and . When  gets a message from , it knows that  is on this side and so it will stop sending on this segment.  will send about  to  but this is two hops away so  will not only know that  is there but it will know the shortest route also.  will get the message from either  or through , whichever comes first. So it will find its hop to the root . Then among  and ,  is smaller assuming , , , , , ,  are in the lexical order. So it will connect to . This is how they all will come to know about the root very quickly. Although each bridge may initially think of itself as a root, at some particular point of time, for example,  originally assumes that it is the root but when it gets a CM from , it will know that there is something smaller than itself and  must be the root. So it will latch on to .  will of course by that time have got the CM from  and knows  is the root. So it will forward this CM with  as the root and with its own distance to  and with it to . Now  will know that  is the root. In this way, all the bridges will come to know about the root very quickly and they will latch on to the path or the root, which goes through the smallest number possible error. For example,  could latch through  or  but it will choose  and in this way we will finally have a tree. (Refer Slide Time: 35.38-36.12) 



Now the algorithm will converge to a tree that connects all segments. What if the root fails or what if the designated bridge of a LAN fails? To identify these you have to run this spanning tree algorithm from time to time. (Refer Slide Time: 36.13-37.04)




One of the main uses of the spanning tree algorithm is to isolate the traffic, specifically broadcast traffic. So bridge installation breaks LAN into LAN segments and bridges filter packets. Same LAN segment frames are not usually forwarded to other LAN segments. Hence segments become separate collision domains and any broadcast over here is just limited to the particular segment. So this is the full LAN 1 IP network divided into LAN segments which are bridged. There may be hubs, nodes, etc. (Refer Slide Time: 37.05-37.19)



How does the bridge know which LAN segment to forward the message? When a bridge gets the frame how does it know where to forward it? (Refer Slide Time: 37.20-38.09)

	
A bridge has a bridge table and entry in the bridge table is of node LAN Address, bridge interface and time stamp. The node LAN address is the MAC address, the bridge interface is the group to which it belongs. If the entry in the table becomes too old and is to be dropped from the table, such stale entries in the table can be dropped after a particular prerequisite time known as time stamp, which can be configured (60 min for TTL). So a configured bridge knows which hosts can be reached through which interface without traffic. When a frame is received, the bridge learns the location of the sender, i.e., incoming LAN segment, and records sender location pair in bridge table. (Refer Slide Time:  38.10-39.57)


When a bridge receives a frame, it forms an index bridge table using MAC destination address. If an entry is found for destination and if the destination is on the segment from which the frame had arrived, it drops the frame, because the frame is already in the particular LAN. If an entry is found for destination and if it has to go to some other LAN, it will forward it. This is known as selective forwarding. If this packet frame is meant for some other LAN, the bridge will forward it to that particular interface. Sometimes a new machine is connected to the LAN, etc. or the bridge is newly connected and its bridge table may not have been constructed fully and so it may not have an entry for this particular MAC address. In such a case it will flood. Flooding means forwarding the frame to all interfaces except the interface on which the frame arrived. When a particular frame arrives from some interface for some destination MAC address and the bridge does not know to which LAN segment this MAC address belongs, it does not have a corresponding entry in the bridge table. So it will simply flood, i.e., put in a copy of the frame to each of the other interfaces. If a bridge is newly put in a network, in the beginning it will be inefficient as we have already seen, flooding many packets to many segments. But as it slowly learns, it will become more and more efficient. (Refer Slide Time: 39.58-41.25)


Suppose C sends a frame to D and D replies back with a frame to C and the bridge has this address table and C is on interface 1. Let A and B be with port 1, E in port 2, H and J in port 3. The bridge doesn?t know about either C or D. So when the bridge receives a frame from C, it notes in bridge table that C is on interface 1. This table will get updated because C is the sender MAC address, which it will put in its MAC table. Still the bridge does not know where D is and it will send the frames into interfaces 2 and 3 and not to 1, as we have seen earlier. The frame gets copied on to other ports, 2 and 3. In interface 3, D is not present. So the host will ignore that frame in 3. But interface 2 has D in it and so D will receive it and will try to reply back to C. (Refer Slide Time: 41.26-41.56)


Now D generates a frame for C and bridge receives the frame. One notes in bridge table that D is on the interface 2. Bridge already knows that C is on interface 1. So it selectively sends the frame to interface 1 and does not give to interface 3 any longer. (Refer Slide Time: 41.57-43.04)




We have LANs and when they get bigger and bigger they can?t really exist as 1 single collision domain. So we have to segment it, i.e. break it up into segments and bridge is 1 way to do it. One way of segmenting some of the LANs is shown above. There are three hubs with each hub having a work group. We can connect them through bridges but this is not recommended for two reasons: First of all single point of failure at computer science hub. If this hub goes the other two can?t communicate with each other and all traffic between EE (electrical engineering) and SE (system engineering) can?t pass through CS, which is not good. (Refer Slide Time: 43.05-43.36)




So the recommended configuration would be something as shown above. A bridge or a switch here connects all the hubs. And this bridge or switch acts as the backbone. If EE wants to communicate with SE, it goes through the backbone. (Refer Slide Time: 43.37-44.25)


Let?s see some of the features of bridges. A bridge has the features similar to a switch. It isolates collision domains resulting in higher total maximum throughput. Because in a collision domain with a lot of broadcast traffic the net throughput of the network will go down. So if you can make it smaller and make the frames to travel from one segment to the other, the overall throughput of the network increases. Also it supports limitless number of nodes and geographical coverage. Bridges can connect different network types. It is transparent (i.e. plug and play) and does ?self-learning?. So no configuration is necessary for its operation. (Refer Slide Time: 44.26-46.15)



Now let us compare  a bridge with a router. Both are store-and-forward devices. A router is a network layer device (examine network layer headers) and is used for connecting two different networks globally. A bridge is a link layer device and is used for connecting two different networks in local internetworking rather than global internetworking. Routers maintain routing tables and implement routing algorithms. But bridges maintain bridge tables and implement filtering, learning and spanning tree algorithms. Bridges maintain bridge tables consisting of MAC addresses but routers maintain routing table consisting of IP addresses. (Refer Slide Time: 46.16-47.09)

These are the layers in the protocol stack of the host and from the higher layer, i.e. layer 5 a frame is coming down to 4,3,2,1. 1 is the physical layer and from the physical layer it travels to the bridge. Since the bridge is the layer 2 device, it goes only up to layer 2. Then it encounters a router in the next hop and router will take it up to layer 3, and again bring it back and then send it to the host. The frame will again go to layer 5. So a bridge is the layer 2 device and router is the layer 3 device. (Refer Slide Time: 47.10-48.19)



Let us see the advantages and disadvantages of a bridge. The advantages are:
- bridge operation is simpler requiring less packet processing
- bridge tables are self-learning
- no configuration is necessary and 
- all traffic confined to spanning tree even when alternative bandwidth is available
The disadvantages of bridges are, 
- As we disable some of the links while we run the spanning tree algorithm, at a time, only some of the links are used while the rest remain idle. So we are not using the total bandwidth that is available to its fullest and bridges do not offer protection from broadcast storms. (Refer Slide Time: 48.20-50.02)



Let us now look into the advantages and disadvantages of routers. The advantages are - arbitrary topologies can be supported. Cycles are supported in bridges, i.e., a frame may go on and on forever. But in the network layer, the packet will have a counter and if a packet starts cycling, the protocol is such that at each hop the counter will be decremented and if some router finds that this count has become 0 it will simply drop the packet. So there is a definite time for a packet to circulate and so it can?t circulate indefinitely. The counters used are TTL counters. It provides good routing protocols such as limiting the cycles etc. The network becomes better. The router is capable of providing protection against broadcast storms. The disadvantages are - it requires IP address configuration which means that it is not a plug-and-play device. It requires some manual configuration and higher packet processing. So it is costlier. (Refer Slide Time: 50.03-50.35)


Bridges work well in small networks, i.e. with few hundreds of hosts while routers are used in large networks, i.e. with thousands of hosts. Also for similar networks like Ethernet modems, switches can be configured to do some bridging functions. As we have seen already, whatever functionality is available in the bridge is available in modem switches also. Now let?s compare switches with bridges and routers respectively. (Refer Slide Time: 50.35-51.56)


Switches are very fast but routers are slow, i.e., switches are doing just switching so they are very fast whereas routers have to do some computation and so they are slow. Switches are inexpensive but routers are expensive. Switches don?t give the benefit of alternative routing whereas benefits of alternative routing are available in routers. There is no hierarchical addressing in bridges, but hierarchical addressing is possible with routers. Hierarchical addressing will be discussed later. When we are connected to the wide area networks, i.e., to the whole wide world, then router is a must because other people can also connect through routers and the routers will talk to each other. But switches can?t talk. If you are trying to do local internetworking, a bridge or a switch which now-a-days gives all the bridging functions, may be a good, cheap and efficient alternative. (Refer Slide Time: 51.57-53.34)


Let us finally summarize the comparison between hubs, bridges, switches and routers. 
* Traffic isolation - Bridges, routers, switches provide traffic isolation where as it is not possible in hubs. Because a hub is just a shared medium that does not give any isolation at all.
* Plug-and-play - Hubs, switches and bridges are plug-and-play devices. But routers need some configuration so it is not a plug-and-play device.
* Optimal routing - Hubs, bridges and switches do not know about routing. But routers can find the optimal route at any particular point of time.
* Cut through - Cut through means you start transmitting as the bits arrive. A hub can cut through because it is a replacement of a passive shared medium, so whatever bit comes, gets transmitted. So cut through is possible in switches also. But bridges and routers have to wait for the whole frame and then inspect it through some routing table or bridging table so they are not cut through. (Refer Slide Time: 53.35-54.58)



Finally let us see about virtual LANs. It is partition of an extended LAN to logically separate LANs (VLANs). Each VLAN is assigned a color identifier and packets are forwarded only to VLANs of the same color. So we can use a bridge or a switch. The different ports of a switch could be different VLANs. The ports belonging to the same particular VLAN may physically be two different LAN segments. But logically they are in the same VLAN. Suppose in one building, computer science is in three different floors and the same floors are also shared by say, the electrical engineering department. Each floor has a switch. The computer science floors 1, 2, 3 are in same VLAN or they are in one logical group. Electrical engineering in these floors forms another VLAN. This needs manual configuration but this is possible. (Refer Slide Time: 54.59)


Why are VLANs so popular today? Scalability is possible because broadcasts are now getting limited. Security and network management is better in VLAN. Network management is decoupling physical topology from the logical topology. As we have seen, two different LAN segments could be in same VLAN. For example, a LAN segment at CCB is to be switched from COC administration to ECE administration. We have finished our discussion on local internetworking. Next, we will see another emerging technology, which is becoming very important in the retailing business. Let us see about Wireless Technology in the next lecture. Thank you. (Refer Time Slide: 55:52 - 55:55)


LECTURE # 22 
CELLULAR NETWORKS



We will start our discussion on terrestrial wireless networks. We have already seen 1 kind of wireless communication which is through satellite. It is a microwave repeater. There are 2 very important and rapidly expanding fields in networking. They are terrestrial wireless networking and wireless LAN. We will have 2 lectures on this. The first lecture is on cellular networks and in the next lecture we will talk about wireless LANs and a little of wireless MANs. Today we will discuss about cellular networks. (Refer Time Slide: 56:52-58:47)







The cell phones have become ubiquitous nowadays.  What is a cell? In the cellular network, the network is organized in the form of some cells and each cell covers a geographical region.  It has base station (BS) analogous to 802.11 AP. AP stands for Access Point.  802.11 is the wireless LAN technology which will be dealt in the next lecture. There is a base station and it will have an antenna and some transmitters and recivers and they are connected to the backbone through a line.  It is a wireless line but usually it would be a fibre optic line. Take this particular base station shown. All the mobile stations or mobile users in the certain geographical location around this base station will communicate with this base station and through this base station to the rest of the network. So mobile users attach to network through BS and air interface is the physical and link layer protocol between mobile and BS. All the base stations are connected to the mobile switching center (MSC). The switching is essentially done here.  The MSC connects cells to Wide Area Network.
COMPUTER NETWORKS
PROF. Sujay Ghosh
Department Of Computer Science and Engineering
IIT Kharagpur
Lecture Name #22
Cellular Networks
(Refer slide time - 0:55)

Good day. So, now we will start our discussion on terrestrial wireless networks. We have already seen one kind of wireless communication, which is through satellites. So it is microwave repeater and we know that. But there are two very important and rapidly expanding field in networking, which is terrestrial wireless networking. We will have two lectures on this. In the first lecture, we will discuss cellular networks. The cell phone, which has become ubiquitous nowadays and in the next lecture, we will talk about Wireless LANs: Wireless LANs are may be little bit of Wireless MANs also. So, today we will discuss about cellular networks. (Refer slide time: 01 :49-04 :29)

So just right away, let us learn some jargons. What is a cell? The cellular network is organized in the form of some cells and it covers a geographical region. It has base station analogous to 802.11 AP. AP is for access point. 802.11 is the wireless LAN technology. We will discuss about this in the next lecture. Anyway, the point is that there is a base station and it will have an antenna, some transmitters and receivers. It will be connected to the backbone through a line. May be this could also be a wireless line, but usually this could be a fibre optic line. In a circluar geographical location around this base station, mobiles will communicate with this base station and through this base station to the rest of the network. So mobile users attach to network through BS. And air interface is the physical and link layer protocol between mobile and BS. That is called air interface between the mobile and base station. Now all these base stations are connected to the mobile switching center (MSC). The switching is essentially done over here. The MSC connects cells to Wide Area Network. The mobile switching center will be referred to as MSC, Base station as BS ,mobile switching center as MSC, MS, by the way, is a mobile station. MSC connects cells to Wide Area Network and manages call setup. More about that later and these MSCs will be connected to each other for a particular service provider. They will also connect to public telephone network and the internet etc. So one service provider, their network would be  connected to another service provider?s network. So somebody from this network can call the other network and so on. So we have the cells; we have the base stations; we have the mobile stations or MSs. We have the mobile switching center, MSC, and of course the PSTN at the back of it all. This part is usually wired.(Refer slide time: 04:30- 06:20)

The first hop ? we are now talking about the air interface between the mobile station and the base station. There are two techniques for sharing mobile to BS radio spectrum. There is certain radio spectrum, which is allocated to the base station and to the particular region that it has to be shared. Now, this is a question of multiple access and two techniques that we talked about earlier are FDMA, if you remember this is frequency division multiple access, and TDMA, time division multiple access. In cellular technology, what we usually do is that we combine FDMA and TDMA. So divide the spectrum in frequency channels and divide each channel into time slots. If you say that these are the different frequency channels and each channel may be divided into number of time slots. We will do FDMA as well as TDMA on this. That is one kind of scheme ? the so-called GSM utilizes this. We will be talking about how FDMA and TDMA are combined. The other technology is CDMA, which was designed by a company called VOLCOM in USA, which uses code division multiple access. We have already seen what code division multiple access is. So we will not get into the details of CDMA systems here. CDMA is another popular way of transporting data to the mobile devices. Both GSM and CDMA are used in many countries. For example, in our country also some service providers provide CDMA services, and few offer GSM services, some provide both, and so on. (Refer slide time: 06:24-07:19)

We will now discuss cellular standards as they stand today. Historically, we had only the cellular system that came from some amps in USA. Previously, there was only one cell from the analog system. From analog system, it came down to digital system in deamps and then we have these two systems of the 2G system ? one is the GSM system and the other is the CDMA version. 2G systems are voice channels: IS-136 is TDMA, combined with FDMA, which is used in North America. GSM, which is more popular of these schemes, is the global system for mobile communications. It has combined FDMA/TDMA, which is most widely deployed. IS -95 is the code for CDMA systems, which use code division multiple access. So these are the 2G or second generation systems. (Refer slide time: 07:20-08:23)

There are 2.5G systems. These were introduced because the 3G was promised quite some time back but the service providers really could not deliver it or crank it up to that extent. But, there was lot of demand for it. Voice was alright with 2G, but then the demand for data and other kind of multimedia services etc., was increasing. So people had to be given some data services. Instead of going all the way to 3G, people went to 2.5G. 2.5G systems have both voice and data channels. So, for those who cannot wait for 3G services, there are 2G extensions. One is GPRS. This is General Packet Radio Services evolved from GSM and the data is sent on multiple channels if available. It has an enhanced data rate for global evolution edge; also evolved from GSM using enhanced modulation data rates up to 384k.(Refer slide time:08 :24-09 :11)

CDMA has its own version called CDMA 2000, that was phase 1; then there was phase 2 also. So data rates up to 144k evolved from IS-95, which is the CDMA system. 3G system includes both voice and data: one service it provides is UMTS. This is the name of the Standard Universal Mobile Telecommunications Service (UMTS). GSM is the next step, but using CDMA 2000. So, all these merge into the 3G systems. How exactly this merging will take place and how it will actually be deployed and become popular remains to be seen; but today you can get these data services on your cell phones, etc. (Refer slide time: 09:12-09:46)

The protocol layering for cells is a little different. We will not go too much in to this. One is of course, the physical layer, which has to do with the physical channels. Then there is MAC, medium access control. We will talk about it, at least for GSM. So, there are these logical channels, transport channels, and then there is Radio Resource Control layer. That is the layer 3 particularly. This you might say is a protocol, but this does not go all the way that the OSI 7 layer. This is just for the cellular systems. (Refer slide time: 09:47-12:04)

 Our idea is that we have some base stations and each base station will cover some geographical area like it has been shown here. Different BS would be connected through a backbone network or through MSCs. We are trying to get the basic idea of the cell. The point is that, nowadays cell phones have become very popular. Its rate of penetration is much faster than the original telephones, and it is much faster than PCs also. So, cellular phones have become very popular, which means a lot of people want to use it and lot of people have cell phones. Many of them would want to talk at the same time. But how do you accommodate all these people talking at the same time? We do multiple access. But then, there is a limit to what you can do using same frequency spectrum. The idea was to do some kind of space division multiple access. In the sense that within one particular geographical area, we use a particular frequency band and then in another geographical area, which is far removed from there, so that these two do not interfere with each other, we use the same set of frequency band at the same time for a different set of users. The point is that these powers have to be controlled because, if they are very powerful, they will start interfering with each other. But if this power is controlled, then within that cell, that power is enough. But, it is not enough to interfere with each other. So, two different groups of users can use the same frequency band at the same time. This is the basic idea of breaking up a region into cells so that you can increase the number of people, who would be using this system. That is the basic concept of a cell. (Refer slide time: 12:06-13:03)

In practice, cells are may be of arbitrary shape. But they will be close to a circle because usually the kind of antenna used in base stations is omni directional antenna, in the sense that it gives the same power on all sides. It has the same sensitivity on all sides. If that is so, the area of influence would be a circle. But when many circles are put together they are pulling and they will intersect with each other. To solve this problem, we can use a tessellation. There are only three types of tessellations, which are possible ? equilateral triangles, squares or regular hexagons. Out of these three, the regular hexagon is the closest to a circle. That is why usually the regular hexagons are used to represent a cellular structure. A hexagonal cell, the closest approximation to a circle, is used traditionally for system design. (Refer slide time: 13: 04-14:27)

This is how a big geographical area may have been divided into a large number of cells ? it looks like a beehive. If you notice carefully some of the cells are dark and these cells are marked as A B C D E F G. So, these are seven. There are seven hexagons like this and these are actually different frequency ranges. These frequencies are again reused. For example, you have another A B C D E F G  over here. This B and this B ? although they use the same frequency ranges ? are far apart. So, different groups of people can use it at the same. Once it gets into the base station, we usually take it to the fiber optic domain, where a large number of calls, simultaneous calls can be handled. This really shows you the frequency reuse. A, the set of frequency bands, which are associated with A, will also be reused here, here, and there and so on. That is how a hexagonal cellular structure is constructed and we do this frequency reuse. (Refer slide time: 14:28-14:54)

Co-channel reuse ratio is given by DL/RL = ?3N, where DL is the distance between co-channel cells, that means those who share the same channel. RL is cell radius; N is the cluster size. The number of cells in a cluster N determines the amount of co-channel interference and the number of frequency channels available per cell. This really comes from geometry. (Refer slide time: 14:55-16:29)

When the number of subscribers in a given area increases, allocation of more channels covered by that cell is necessary. What happens is that in one area, say a small town, one base station could satisfy people, who had these cellular phones or mobile phones. Now what happens is that, the number of people who wanted to use mobile phones kept on increasing and now we cannot serve them any longer. The number of requests, which are denied, keeps on increasing. How can we increase? May be break it up into two cells and then break it up into four cells and break it up into many more cells, depending on the clusters of users and the cells. Now, the same area has been divided into smaller cells. May be in the BS, you decrease the transmitter power so that they do not interfere with each other. So when the number of subscribers in a given area increases, allocation of more channels covered by that cell is necessary. This is done by cell splitting. A single small cell midway between two co-channel cells may be introduced. (Refer slide time: 16:30-16:53)

These are the small adhoc solutions to the problem. For example over here, you had a large number of  cells. We created a small cell over here using A, which uses the same frequency bands as the already existing ones. You cannot use E, F, C, or B. But you can use A with other cells. So that is called cell splitting. These are ad hoc solutions, when a particular area has more number of users. (Refer slide time: 16:54-17:14)

We  now have a cellular hierarchy, the needs of which are: extending the coverage to the areas that are difficult to cover by a large cell; increasing the capacity of the network for those areas that have a high density of users; increasing the number of wireless devices and the communication between them. (Refer slide time: 17:15- 18:04)

So, you have a large number of cellular hierarchies. One set of them are called Femto cells. These are the smallest unit in the hierarchy. So these cells need to cover only a few meters, where all devices are in the physical range of the users. This is also called Personal Area Networking. So I have something in my left pocket, something in my right pocket and something in my hand. These might communicate with each other. So, that is Personal Area Networking. Femtocells are small cells. Then we have Picocells, the size of their network is in the range of a few tens of meters. So, you can think of a small building as Picocell. For example, WLAN (Wireless LAN). (Refer slide time: 18:05-18:34)

Micro cells cover a range of hundreds of meters; for example, in urban areas to support PCS or other technologies. PCS is another kind of mobile technology. Macro cells cover areas in the order of several kilometers, for example, a metropolitan area, or may be a small town. Mega cells cover nationwide areas. So, mega cells possibly are being serviced by a satellite. (Refer slide time: 18:35-19:04)

This is the picture of satellite, which may service a mega cell. Then, we have macro cell from this tower. Then we have pico cells, which have some access points etc inside a building and so on. Microcells for covering communication. So these ways of different kinds of technologies may be deployed for these different ranges of cells. (Refer slide time: 19:05-19:32)

Frequency reuse: We have already talked about this. Radio spectrum is one of the scarcest resources available; because, there is so much demand for it for so many applications. So, employ architectures that can support as many uses as possible (theoretically) with the available spectrum. Same spectrum can support multiple users separated by a distance and thus efficiently be using the spectrum. (Refer slide time: 19:33-20:46)

Frequency reuse has its foundations in the attenuation of the signal strength of EM waves with distance. So, if two points are at a distance from each other, this signal gets attenuated and does not interfere significantly with this one, although they are using the same frequency band. Usually, what will happen is that the service provider will be given some band of frequencies.  Now, he has to use that and cannot stray from there, as that is the license agreement. So, what he will do is that, is depending on where his users are, and what the distribution is, what the density is like, he has to develop or plan a cellular infrastructure in this fashion using and reusing this frequency, the same frequency band, here and there to give the maximum amount of service. The distance separating the transmitters of this frequency reuse should be sufficiently large. Of course, this has to do with a transmitter?s power. Transmit power should be reasonably small. The cellular concept is an intelligent means of employing frequency reuse. (Refer slide time: 20:41-21:21)

So what we have been talking about is something like a fixed channel allocation. That means for a particular cell, the channels, that means, the frequency band associated with the cell is fixed. So, total number of channels is Nc = W/B, where W is the bandwidth of the available for spectrum. B is the bandwidth needed by each channel. The total number of channels per cell is Cc = Nc/N, where N is the cluster size. (Refer slide time: 21:22-21:53)

Adjacent radio frequency bands are assigned to different cells as shown. In analog each channel corresponds to one user while in digital each RF channel carries several time slots or codes. So, you are doing either TDMA or CDMA. So, if you are doing this, the naturally FDMA TDMA combine or CDMA uses spread spectrum technology. So, it?s simple to implement. So, fixed channel allocation is simple to implement if traffic is uniform. (Refer slide time: 21:54-23:24)

But then, sometimes traffics are not uniform ? there may be two cells, which are side by side. So, this has been given one band. one has been given another band of frequencies. They do not interfere with each other. But, we find for each cell.  Let us say, to start with, we are given with equal bandwidth to each of the cells. Now,  I find that in one particular cell, the user density is much higher, whereas in adjacent cell, the user density is lower. So, I could use some more bandwidth in this cell and I could do with a little less bandwidth here. So, what could do is that, a part of this frequency band  can borrow from the adjacent cell. So, that is called Channel Borrowing technique.  High traffic cells borrows channel frequencies from low traffic cells. Temporary channel borrowing and static channel borrowing. This could be a permanent feature or this could be the feature of a day. For example, in the central business district, It might become very busy during the day time. So, it may borrow channels from the side, whereas after the evening the use may fall drastically. In that case, one cell can give out channels to others. Not only sort of giving channels to other people. So, this could be static as well as it could be temporary. (Refer slide time: 23:25-26:26)

This is suitably complex picture of GSM, i.e. the Global System for Mobile communications. So, this just to show you how these TDMA and FDMA are combined. So, you see there are 124 simplex channels in the GSM system. Now, each of the simplex channels actually carries a series of TDMA frames and each of the frames is divided into 8 parts, that is how a large number of channels can be given. There are two parts: one is the uplinking and another is the downlinking, that means, from BS to MS ? Base Station to the Mobile Station or from the Mobile Station to the Base Station. So you have two different frequency bands for these ? one band for this BS to MS communication and another band for MS to BS communication. In each band, there are a number of frequency channels and each frequency channel is again divided into so many slots: eight slots for simultaneous communication. So this one and this one are same channels, but this and this are two different channels for a particular mobile station. So from base station, it may be using this particular time slot in this particular channel, that is, from the base station to the mobile station. The same one, from the mobile station to the base station, will be using another channel and actually another time slot, because there is some technical problem in giving the same time slot in this channel as well as the other channel. So, you give it a different time slot over here. So, in this particular time slot of this particular channel, the mobile station is communicating with its base station. So, that is how it goes. GSM uses 124 frequency channels, each of which uses an 8-slot TDM system. And there is a frequency band at which it operates; this is also fixed. This is in the 959.8 MHZ range. You need not remember these figures, but this is the general scheme of the way TDMA and FDMA are combined together. (Refer slide time: 26:27-27:15)

Suppose this is S1(t) and  this is the signal which comes from source 1 and this is the signal from source n, Sn(t). This is some other source. What is happening is that in the Tm slots, the first slot ? S1 ? gets the first slot and Sn gets the nth slot. So, they are pushed into this in the same frequency band, and as time progresses, they function just like in a TDMA system. So, this is the TDMA part. So, GSM = FDMA 200 KHZ; that is the GSM system (Refer slide time: 27:16- 27:46)

This is a portion of the GSM framing structure. So, how they are framed? Actually, this is a somewhat complicated scheme. Some of these frames are used for control purpose and others for communication: one group for base to mobile and other from mobile to base, etc. So, there is a scheme for this. We will not go into the details of this. (Refer slide time: 27:47-28:17)


A GSM system has 124 pairs of simplex channels. They are in pairs because one goes from BS to MS and the other from BS to MS. Each of these is 200 kilo hertz wide and supports 8 separate connections on it, using TDM. So, each active station is assigned to one time slot on one channel pair. 992 channels can be supported in each cell, but many of them are not available to avoid frequency conflicts with neighboring cells. (Refer slide time: 28:18-30:16)

Transmitting and receiving does not happen in the same time slot because the GSM radios cannot transmit and receive at the same time and it takes time to switch from one to another. That is why different time slots are given. A data frame is transmitted in 547 microseconds, but a transmitter is only allowed to send one data frame every 4.615 milliseconds, since it is sharing the channel with seven other stations. The gross rate of each channel is about 270 or about 271 kbps divided among eight users. This gives about 33 or 34 kbps gross. CC i.e., control channels are used to manage the system if somebody is getting only 33 or 34 kbps. Previously, we have been talking about voice channel requiring 64 kbps. Now, the 64 kbps happens to be if you are doing a plain vanilla PCM. That means we have explained, how it is encoded by sampling it at eight samples and eight levels for each sample ? that gives us 64 kbps. The point is that, it is not the only coding scheme. Actually, there are more advanced coding schemes. We did not find time to discuss those coding schemes. Using those coding schemes, good quality voice transmission can be achieved, using a much lower bandwidth. This 33.854 kbps is actually enough, if you are doing your coding in a smart fashion. (Refer slide time: 30:17-32:19)


As I said, apart from the user channels, there are some control channels. CC is used to manage the system. The Broadcast Control Channel (BCC) is a continuous stream of output from the base station containing the BS?s identity and the channel status. All mobile stations monitor their signal strength to see when they moved into a new cell. The point is that the mobile station, when it gets these broadcasts from BS, by just sensing how much transmitter power it is getting, it can identify whether it is near this particular BCC, what this particular BS is, or what its identity is, or whether it is near some other BS. In some systems like CDMA, this power is very crucial even for decoding purposes. That is one thing which is being broadcast and to listened by all the MS. The dedicated control channel is used for location updating, registration, and call setup; in particular, each BS maintains a database of mobile stations, which are in its area. So, information needed to maintain this database is sent on the dedicated control channel. So, the point is that these mobile stations are moving. They move from one cell to another, from the vicinity of one base station to the vicinity of another base station. So, the set of MS, which are now currently under this, that information in some schemes is collected on the side from time to time and there is a database, which is associated with the BS. This is centrally communicated. That is important for locating a person. We come to that later on. (Refer slide time: 32:20-33:33)

And then there is a common control channel which has got three logical sub channels. That is the paging sub channel, paging channel, in which the BS uses to announce incoming calls. Each MS monitors it continuously to watch for the call it should answer. The point is that, if there is a call,  and MS is in the area of some BS and then somebody wants to call to this MS, that one particular MS has to be alerted. So, there is a paging for that MS from the BS and the MS is always listening to it. So, whenever it hears the page for itself, it gets alerted. So, the other is the  random access channel. This allows users to request a slot on the dedicated control channel. If two requests collide, they are garbled and have to be retried later on. So, this is the part of the call set-up. So, it is the first part of call set-up. It tries to put a request in the random access channel for a slot in the dedicated control channel. When it gets a slot in the dedicated control channel, it can go away with the further steps of call set-up. Next is the access grant channel. (Refer slide time: 33:34-33:51)
 
In GSM the channel multiplexing is FDM + with eight TDM slots. Uplink is this much and channel bandwidth is200 KHz. So, DCS has certain frequency range etc. (Refer slide time: 33:52-34:03)

Channels are broadcast and the channel rate is 13 kbps.  (Refer slide time: 34:04-34:42)

We have already seen what CDMA is ? it is based on DS spread spectrum, that is, the Direct Sequence Spread Spectrum. It has two frequency bands, one for forward channel and one for reverse channel and one frequency band, a wide band actually that is shared. That means it uses orthogonal codes by a number of handsets or number of mobile stations. So, CDMA allows use of same spectrum over all cells. It also gives net capacity improvement. Although which system is better ? CDMA or GSM ? is still not clear. (Refer slide time: 34:43-35:40)

There are certain issues in the cellular infrastructure, which have to be handled.  We will quickly discuss each of them. The most important one is handoff. Because you may be talking on your mobile phone while moving, may be moving in a car. So, what will happen is that, the car eventually will pass out of the range of one base station and move in to the range of another base station. So, you have to hand off. That means previously all communication from this mobile station was being handled by this particular base station, as it moves in to the area of another base station, this call has to be handed off from one BS to the other BS. Handoff changes of radio connection from one base station to another will happen. But this not such a simple scheme and we will see why. There are two types of handoff: hard handoff and soft handoff. (Refer slide time: 35:41-36:33)

This handoff has to be managed. In order to manage the handoff, we have to detect that handoff requirement has arisen because the mobile station has moved and then you have to execute the handoff, in the sense that you have to do the channel assignment and you may have to do some path rerouting and there may be problems in this section also. For example, when you move into a new cell, all the channels over there may be busy and so you may not have any extra channel, which has to be given to this ongoing call. So, there are various schemes for handling ? may be you drop this. That is the simplest thing to do, that you do not allow it or maybe you keep some guard channels specifically for these kinds of cases. But this detection of handoff requirement is a troublesome affair. (Refer slide time: 36:34-37:04)

As I said, there are two types of handoff: hard handoff and soft handoff. Hard handoff is break before make. This is used in GSM system ? that means you break this connection and set up the new connection with the BS in whose area you are moving. MS connects to base station 2 after link with base station 1 breaks, and this is the region where the handoff will take place. (Refer slide time: 37:05-37:57)

The difficulties in handoff detection are the following. The signal strength fluctuates. This is a very challenging area of mobile system design ? the signal area fluctuates due to various reasons: scattering, reflection and diffraction results in fading. There are fast fading and slow fading of the receiving signal. There are false handoff requirements at the boundary; there is a ping pong effect at the boundary. That means what might happen is that it may hand off from BS 1 to BS 2, then again from BS 2 to BS 1, again from BS 1to BS 2. This kind of ping pong might be going on. So, the number of unnecessary handoffs must be reduced because handoffs have a price paid, actually, keeping both channels busy on both sides. There are other kinds of overheads to this. (Refer slide time: 37:59-38:41)

Let us look at a very simple model. Actually the situation is much more complex because, there are number of base stations ? maybe three base stations and you may be equidistant from all the three at a particular point. So, you may have an even more difficult problem. But let us look at a simple problem. Suppose D is the distance between two base stations. So, ideally we would like that the signal strength from BS1 is following like this and the signal strength from BS2 is going like this. As you move from BS1 to BS2, as the MS is moving at the cell boundary, it just switches from BS1 to BS2. But the actual picture is something like this. (Refer slide time: 38:42-39:44)

This is the point is that the signal strength from BS1 is varying, very fast. Why does it vary? I will just tell you. I had just mentioned it, but I will tell it again. This is varying like this. The signal strength from BS2 is also varying like this. So this has been plotted from, let us say,  800 to 1200 region. So, there is a solid region at least from 950 to 1050. There is a region of 100 m, where you really do not know who is stronger. So the signal is varying all the time. What might happen is that you might now decide to move from BS1 to BS2, and then you find that BS1 has become much stronger and BS2 has become much weaker. So, it might switch from BS2 to BS1 and this might go on as a ping pong effect. So, this is a very difficult problem. (Refer slide time: 39:45 -41:03)

And as I said, why is it that it varies in this fashion? There are various reasons for this: one is that you are moving. This mobile station is actually moving.  Now whatever signal it gets, it may get some direct signal, it may get some reflected signal, it may get some scattered signal and all these signals may start interfering with each other. So, actually what might happen is what is called multipath fading. That means, the same signal may have arrived from source to destination through two different paths ? may be through two reflections ? and they may be out of phase because of the different distances, which may be allowed. If they are precisely out of say, 180? out of phase, then you are going to have distractive interference or they may be in phase; they may strengthen each other. So, in a very short span of time, as the mobile is moving, you may find a very largely fluctuating signal. There are other reasons for this fading etc. We will not go into the detail of this. (Refer slide time:41:04-42:21)

So, there is a problem of handoff. So, for the handoff decision there is various algorithms which have been  proposed. I will just mention them. One is relative signal strength, which is the simplest first thing. You will think that whichever is weaker, we leave that, and whichever is stronger it will chose that one. So, choose BS2 if signal from BS2 is greater than the signal from BS 1. But as we saw, with just this, there may be lot of ping pong effect and lot of unnecessary handoffs. You can use this same RSS, that is, received signal strength, and some threshold base. That means we choose BS2 if the signal from BS2 is greater than the signal from BS1 and the signal from BS1 is less than a threshold, which means that although BS2 is stronger, if BS1 is above the threshold, which is still working, then we do not do a handoff. Another thing is RSS plus hysteresis, that is, received signal strength. Just being greater is not enough; it has to be greater by a certain amount of hysteresis. The hysteresis means base1 persists as the BS2 is becoming stronger in base 1 and then there are other kinds of other combinations. People have tried for getting a good handoff decision. (Refer slide time: 42:22-42:50)


As I said, this hard handoff is used in GSM, whereas in CDMA system, they use soft handoff. This is ?make before break? ? that means you make a connection to that coming in the next base station, before you release the connection with the previous base station. So, MS connects to BS2 before connection to BS1 breaks. This is called soft handoff. (Refer slide time: 42:51-43:14)

We will now discuss the merits and demerits of soft handoff. Merits are: mobile station does not loose contact during handoff; the effects of ping pong are reduced; and it is easy to implement for CDMA systems. The demerits are: it is a complex process. So, hardware requirement is more and that means your hardware cost may also go up; and it utilizes extra resource during handoff. (Refer slide time: 43:16-44:22)

[43:16]Now, we come to the question of mobility management.  How do you manage mobility in a local in a MS?  That is because, one of the very fascinating thing about mobile connection.  I have called somebody on the mobile, who I assumed is just local. That is who is in the same area as i am. It just so happens, that he is in a place far away. He is visiting some place.  May be Rajasthan or something. ,as he is very far away. Now  I will expect that the system would somehow locate him in Rajasthan and then allow me to talk to him. So, that is again a non- trivial problem. There are various approaches to this problem. We will just once again touch on this.   So, this is called mobility management. One is location management access point of a mobile station changes as it moves around the network coverage area and important for effective delivery of incoming cells and other is handoff management. We have already talked about it. (Refer slide time: 44:23-45:21)

Now, for location management, one approach through location updates. That means messages are sent by MS that is a mobile station regarding its changing points of access to the fixed network. So, that is to the fixed network, that is sort of time of time it tells that. Ok.  This is where some central database is updated. Each time the MS makes an update to its location a database in the fixed part of the network has to be updated to reflect the new location information. So, that for a particular MS, if you go to the data base and find out what is the last point, where he said, that he was. Of course what he might have done is that he might have switched off his   mobile and then moved to somewhere else and then put it on or something. .  So, put it on again. So,   that is not still solved all the problem 100%.  But this is one approach to solve it. (Refer slide time:45:22-46:26)

The other this thing is the paging. You know what is paging? Paging means that  sort of broadcast it. Well broadcast means broadcast it everywhere. We do not want to broadcast it everywhere. So, you broadcast it only to certain places. So what we do is that, we broadcast that there is a call for such. So, that is what we page and if that paging is being done in a cell, where the MS is actually present and that the MS  will respond, that is what will happen. So, that is another scheme  required to deliver an incoming message to the MS.  Response from the paged terminal enables the network to locate the MS. The other thing about location management is location information dissemination. Procedures to store and distribute the location information related to MS are serviced by the network. that is the issues over. That I am not going detail in any of these, as no time.(Refer slide time:46:27-48:03)

And for the location update, you may do static location update. That means initiation of location update is decided by the topology of the network and location area based location updates which is commonly used.  what a location area is and distance based, which performs location update after crossing certain number of cells or timer- based, performs location update after a certain time has elapsed. So, the question is that how frequently do you update this? Because, if collecting all the data all the time and just updating it all the time, that will consume an enormous amount of resources. You have to optimize somehow.  That this is the point is that if we not doing frequently enough, your data in the database going to stay and then when you want to actually search for somebody, then you might have to search around a large area. Ideally what would you liked is that when a call is there for somebody, we know that exactly in that particular cell that mobile station is there. So, we go and page over there. He responds ok.  That is the idea. But this idea will never work.  Because, you cannot keep it updated on all the time and you cannot collect and keep all the information. So, you have to do some kind of optimization over there, which means that you actually looking for one particular mobile. You have to search not in just one cell, but may be in several cells. So, that is where location area comes in that location. Area is collection of cells, we will come to that. (Refer slide time: 48:04- 49:02)

The other is dynamic location update. It uses mobility of the user and call pattern for location update. So, if you know the mobility of the user and some call pattern etc, you may be able to predict that where this particular user may be. It is more likely that he will be there.  So, it state-based: performs location update, based on the current state information such as distance  travelled, the number of LAs crossed, etc or user profile based, which is more difficult,   not exactly used at the moment maintains. A list of LAs that is location areas that the MS located in at different points of that time usually.  So, usually during office hours I will be found in my office.   In that particular area. So, that may be known and that may be a guess.  But then gathering this information and keeping this information for all users, this is not a mean task. (Refer slide time:49:03-49:58)

So,this is the location area based  location update. So, as you see that a bunch of cells together form a location area.  So, this is the location area 1 containing 1 2 3 4 5 cells. This location area 2 containing 7 cells and so on. So, assign a location area identifier to a group of cells LA 1, LA2.  BS broadcasts periodically LA identifier. So, it is enough to trying to fill a particular cell. You are trying to fill it down to a location area. So, BS broadcast it, whichever location it is in MS is required to listen for LA identifier and make an update to the location if necessary. Drawback is once again,  there may be ping pong effect.  This fellow is moving like this.  So, it going from location area 1 to 2,1 to 2 etc.  That is always the thing. You cannot eliminate this completely. (Refer slide time: 49:59-50:22)

 Location update in GSM. LA identity, that is it takes the location based approach. Identity is used for location updates. LA consists of a group of cells controlled by BSC and MS performs location update under 3 (1).circumstances upon power up, compares previous LA identity with the one currently being broadcast- if different, performs update. (Refer slide time: 50:22-50:52)

(2)When MS crosses LA boundary, performs update. (3)After a predetermined period of time, performs update to ensure MS is available. So that you do all the three things simultaneously. So that would make sort of judgment about, what is this time interval, after which it will go automatically or of course, other two are simple. (Refer slide time: 50:53-51:21))

And then in for paging schemes, you can do blanket paging, that means when you know  the location you just page everything.  All the cells paging in all cells within an LA simultaneously. If the LA update is current, MS responds immediately. Advantage is minimum delay in getting paging response and disadvantage is    it needs paging in all the cells within LA equidistant from the current cells. A timer is used to declare the MS is unreachable. (Refer slide time: 51:22-51:39)

All it could be that closest cells approach. First page the cell where MS was last seen. If not successful, page subsequent rings of cells that are. So, this is all trying to reduce the overhead and give the maximum response time etc.  So,all these different schemes are there. (Refer slide time:51:40-52:52)

And finally we will not go in to the details of these. as i said that, now everybody  want news on their handsets. Not only news they want to access, to the internet through the handsets, which means we will have to give some data service and that is why service providers also move from [Noise]  2G to 2.5G systems wherein from the GSM family.  It,the general packet radio system GPRS and  CDMA to CDMA2000.  So, GPRS is a really ?packet overlay? network that means on the same network, there  is a  packet service, which is going on available frequency bands.  Network on top of the existing GSM digital circuit switched voice based network. So, it is TCP/ IP based. The protocol based  is same as the TCP IP which we will learn later. It allows data packets to be conveyed across the mobile network using packet switching and it is ?Always on?. ? Always connected? type of thing and after initial ?log-on?, user is permanently connected to the IP services, that is the GPRS.   (Refer slide time: 52:54-53:22)

Instant access, no further log on and usually the rates also gives a flat rate. User perceived performance: fluctuates (as GPRS users defer to voice users). So, voice users have a preference. So, because data may delay that is may be acceptable to a maximum of [Noise] 50kbps. Network resources only used when information ready to be exchanged bandwidth on demand.  So, more utilization of air time that is the GPRS. (Refer slide time: 53:23-53:52)

So, this provides high speed frequency.. So, uplink is on the particular frequency band and downlink is on particular frequency band and these are all packet services which provide high speed packet data access. This uses modified GSM hardware (different phones or cards) are required. That is you have particular kind of set that handle GPRS. Several time slots can be dynamically allocated to transmit a block of data. So, if the packet is large, so several time slots may be used for that. (Refer slide time: 53:53-54:25)

The uplink channel is shared by a number of mobiles, and its use is allocated by a BSC, base station stream. The downlink is of course fully controlled by the serving BSC and random access is not needed in the uplink. Of course multiple access will still because, so many people want to send the request for data. The MS requests use of the channel in ?a packet random access message?. The BSC allocates an unused channel to the mobile and sends a ?packet access grant message? etc. (Refer slide time: 54:26-54:49)

CDMA 2000 is once again is the CDMA version of it. Increasing voice capacity. Once again this is always on peak packet data rate of 153 kbps which is quite high. Connectivity to ANSI -41and GSM-MAP, which we need not to bother. Various bands and bandwidths of operation in support of different operator needs. (Refer slide time: 54:50- 55:58)

It is expected that actually that, this  CDMA 2000 1X RTT is backward compatible with CDMA1 system, which was the previous original CDMA system. Improved service multiplexing and QOS management and variable transmission rates and it is expected that in future what is going to happen is that, as data demand is definitely going to grow, so these will sort of move from these interim 2.5G system to the 3G systems. I. CDMA is already being employed.  This is part of our big network architecture.  This converged network architecture, which is slowly emerging and in the next lecture, what we are going to do is that we are going to discuss Wireless networking in the LAN setup.   In the sense purely in this, we are talking about voice and voice added to data. Next, we are going to talk about data and may be data plus wires that is a separate issue. Thank you. 
Computer Networks
Faculty Name
Prof. Sujoy Ghosh
Dept. of Computer Science & Engineering I.I.T, Kharagpur
Lecture Name # 23
Wireless Network
(Refer slide time: 00:39)
Good day. In the last lecture we had discussed about the cellular network and that end of wireless networking; today we will talk more specifically about data networking and wireless LAN, and may be wireless MAN and things like that. Actually there has been an explosive interest in wireless technology in just the last few years, and a number of different systems have come up. It is not known at this moment what will finally stabilize, but the number of systems have come up and some of them of are on drawing board, some of them are on actual deployment. So we will talk about just a few of them, the more important ones today. (Refer slide time: 01:29) 

Today we will talk about wireless networks, (Refer slide time: 01:29 - 03:36)

and specifically if I may say, wireless LAN. A LAN means a local area network that works without wires, which means you do not have to wire up the whole place; you do not have to have a wire coming into your system; you can walk into a room with a laptop and you are already on the network. But this has some peculiar problems; we will discuss them. This is not as easy since signals are of limited range. Unlike wired LAN, if A can hear B and B can hear C, it is not necessarily true that A can hear C. So this is a problem which we have to handle; secondly in many of the cases, these wireless LANs use unlicensed frequencies and low power. Low power is important because you want to have a small-sized cell so that in the another part of the building there may be another cell just giving services to another group of users. As we know that this way, by doing space division multiplexing, we can increase the number of users who are on the network. One of the most important LAN standards today, wireless LAN standard, is 802.11 and there are various versions of 802.11. The speed varies from 2 mbps to 54 mbps.  We will also talk a little bit about Bluetooth, which is a personal area network. We will talk a little bit about wireless MAN, which is 802.16 and just mention of few other emerging technologies. (Refer slide time: 03:07 - 04:28)	   

There are two modes of operation in a LAN ? in the presence of a control module or a CM often called a base station; just as we have a base station in case of a mobile, similarly here we can have a base station  which, in 802.11 parlance, is called an access point or AP. So we can have a base station access point or AP; so we can have a base station or a control module and a number of users. That is one mode of operation. The other mode of operation is a rather ad hoc network; that means, we just have some peers. There is a peer to peer connectivity and there is no central module. So applications could be LAN extensions, cross building interconnect, or nomadic access; that means some body just moves in and gets immediate access to ad hoc networking. (Refer slide time: 04:30 - 04:42) 

These are the two modes of operation ? in one we have a base station, which is controlling them. This is slightly easier to handle than complete peer to peer ad hoc network. (Refer slide time: 04:43 - 05:23) 

What happens is this control module or this access point in the case of 802.11 could be connected to a wired network so that all those stations, which are connected to the control module via wireless link, get connected to the entire network so they may connect straightaway to individual PCs or they may connect to some network hub or switch. They may connect to a server and a number of LANs. So this is the picture of a single cell, wireless LAN single cell ? W LAN ? and we can have multiple cells of W LAN. (Refer slide time: 05:25 - 05:50)

In each of the cells we will have a control module, which will serve some of the user modules. You may note that there may be a region where it is possible to connect to either of the CMs. Also that is something the user module will have to handle.  (Refer slide time: 05:51 - 07:33)    

Now we will look at W LAN requirement. This is some kind of a wish list actually ? what all we would want from wireless LAN. Good use of bandwidth is we want ? high throughput ? every body uses a number of nodes; it should be large, may be in the hundreds. A good connection to LAN backbone is required because nowadays just a local network by itself is of limited utility since every body is getting use to be connected to the entire network meaning the internet even all the time.   so the  backbone connectivity is also important; good service coverage; ok   I mean wherever I am I would like to be  connected so this to be a good service coverage or range; minimal battery power consumption this an important issue in any kind of mobile system because if the battery consumption becomes high, either you have to carry  heavier batteries or you have to  charge them often so that is not good so we want minimal battery power consumption; transmission security and robustness ? this may be an issue in many cases ? because you know  so in a wireless system   the medium is of course open to every body alright including snoopers if any so but you would like your communication to remain somewhat private or protected  and in some cases that may even become crucial  so we want security and robustness ok and some colocated network operation. (Refer slide time: 07:35 - 09:15)  

License free operation: this is another important issue. For example, the ISM band consists of industrial, scientific and medical bands of frequencies which are free; there is no license on it. That means operation with the unlicensed band is important because then whoever can develop a good system can go ahead and compete in it, and that way the world technology improves fast. Then people also get cheaper and better quality service. That is why ISM band is generally preferred; but it is not that in a wireless network, we always stick to ISM band. Cell hand-off and network roaming: this is another thing we would like to have. This is some kind of a wish list; that is, not all of them are achieved 100% today, but these are the kinds of things we would like to have, like cell hand-off and network roaming. Just as in voice network we can roam from one cell to another and our call remains online, similarly in network connection, we would like them to remain online when we move from one cell to another. So we require dynamic management, adaptive MAC address management, dynamic and automated addition, deletion, relocation of end systems without disruption. And then we require a choice of physical solutions; for example infrared spread spectrum, narrow band microwave, etc. (Refer slide time: 09:16 - 10:22)

As I said there are a number of standards that came up; here I am just showing some standards in the 802.11 family. Then there is an 802.15 family; 802.16; and so on. This is just one of them. 802.11 originally was a 2.4 GHz ISM band and used FHSS, which is frequency hopping spread spectrum, with 1?2 mbps speed or direct sequence spread spectrum, DSSS had 1 or 2 mbps, and then slowly it graduated and then it fell over to three standards: 802.11b, which is the most earliest and the most common one; it was followed by 802.11a; and 802.11g. These two are in the 2.4 GHz ISM band, whereas this one is in the 5 GHz ISM band. (Refer slide time: 10:23 - 11:14) 

We will not go in to the details of all these and what exactly are their differences, etc. Today what we are trying to do is that we are just trying to get a general idea, because there are too many standards. 802.11 LAN architecture: by now we know that we will have an access point, which is connected to a hub or a switch or a router. This is in one cell; this is another cell. So cells may be called a basic service set, also known as cell. In infrastructural mode, it contains wireless hosts ? so these are the wireless hosts. It contains an access point. In an ad hoc mode, there will not be any access station, so they will all be connecting to each other in a peer-to-peer module. (Refer slide time: 11:15 - 11:46) 

And in the physical layer in 802.11 family itself you see that there are so many techniques that are used ? FHSS, which is frequency hopping spread spectrum; direct sequence spread spectrum; orthogonal frequency division multiplexing (OFDM); HRDSSS is another one; OFDM and so on. Above this we have the data link layer, that is, the LLC and the MAC sub-layer, and above that we have the upper layers. (Refer slide time: 11:47 - 12:59) 

So we do not have the time to go into the details of the physical layer technologies, like, how exactly the multiplexing and multiple access is done, but this is just a very broad and high level view of the system. We have the input data, which is encoded. So that is a channel encoder; it uses either FSK, that is, frequency shift keying, or phase shift keying, FSK or PSK. There are other variations of this. We will just get a feel of this. This feeds into a modulator and then there is a pseudo random pattern generator on the receiver side. This is on the sender side, similarly there is a pseudo random pattern generator on the receiver side and these two are synchronized. So this modifies the main carrier, and it then goes into the air or whatever the medium; then it arises at the other end, plus some noise is also added to it, where it is demodulated and it is decoded and then we get the output data. (Refer slide time: 13:00 - 14:58)


Now how do we ? glossing over the physical layer ? how do we handle the multiple access of an 802.11? It avoids collisions; that means, we know that when two or more nodes are transmitting at the same time, their signals will	collide, and we will have a collision. So 802.11 tries to avoid collision. It does CSMA ? if you remember CSMA is carrier sense multiple access. So it does some carrier sensing; it senses the channel before transmitting. Of course it does not collide with the ongoing transmissions by other node, but it does not do any collision detection. And the reason it does not do any collision detection is that if it has to do collision detection, first of all what would happen is that not all traffic is apparent to all the nodes in the network. Due to various reasons it could happen. So that is one reason that even if there is a collision and if you are doing collision detection, you may not be able to detect it at all so that is why the stress here is not to do collision detection like you do in a wired LAN like Ethernet, but to avoid the collision. alright so it is difficult to receive sense collision when transmitting due to weak received signal and fading etc and it can not sense all collisions in any case ? hidden terminal fading so goal is to avoid collisions so this is called CSMA CA. instead of CSMA CD we have CSMA CA that is CSMA with collision avoidance.  (Refer slide time: 15:00 - 15:58)

So this is a diagram which shows you this problem about hidden terminal. For example, we have A, B and C. Now B can listen to C; A can listen to B, that means, A B can communicate with each other; B C can communicate with each other; but between A and C there is some kind of an obstacle. So A and C can not communicate with each other. Even if there is no obstacle like this, the situation could be something like this. Suppose this is A; this is B; and this is C. Now at the point B, A and B have fairly high signal, whereas at C, A signal strength is very low; similarly at A, C signal strength is very low. Some of the terminals may be hidden from some other terminals. This is a problem; that is why our MAC protocol is designed to handle (Refer slide time: 16:00 ? 18:29) 

situations like these. This is a MAC overview; we have a number of boxes here. I will not go in to all the details like radio management; power management; management information base; this is for network management; there is an addressing; there is a security part, like shared key and association management; similarly there is a fragmentation of large frames and so on. We will not look into all this; we will just mention that for addressing we use the similar 48-bit MAC address, which is Ethernet compliant. You remember that the Ethernet address or the hardware address or the MAC address that we talked about when we discussed Ethernet, is a 6-byte or 48-bit address and 48 is of course a very large address space; that means 248 is 256 trillion, which is a very large number. So there is no shortage of addresses. So a bunch of addresses may be given to these. The same kind of 48-bit addresses are used for this also. Making it Ethernet compliant has its advantage because Ethernet is just another ubiquitous kind of network. Another point is that we have an acknowledgement request kind of a system, where some frames and some fragments, etc. are acknowledged. So if the acknowledgement does not come we have retransmission. We also have some error correction; and radio link security; data authentication; data encryption; simple scrambling; or peer-to-peer, etc. We will not be discussing these as do not have the time. In the radio link, there is a question of quality of service. There is this CSMA CA ? we will look into this channel access mechanism in some more detail. Dedicated real time support systems are also there; they are with PCF. So there are actually two mechanisms, which may be simultaneously present in the same system ? DCF and PCF ? we will be talking about these. The standard provides two modes of (Refer slide time: 18:36 - 19:33)

Operation: DCF, which is mandatory. That means every 802.11 system has to be following DCF at least. So it is a best effort service that uses CSMA CA; that is, CSMA with collision avoidance. And there is another mode, which may optional, which is PCF. This is a base station. This is a distributed control function and this is a point control function. This base station controls access to the medium and uses a polling mechanism with higher priority access to the medium. So actually, if PCF is there, what PCF can do is that actually PCF can take precision. So for DCF, it can give some guaranteed kind of service or quality of service to some users. There are three different types of frames: data frames, control frames, and management frames. (Refer slide time: 19:35 - 20:09)

So one is the point coordination function which is PCF the other is the distributed coordination function which is the DCF. ok so  and how I mean which one you are using may be you are not using PCF at all so that would be a network administrator?s choice.  so the if you are using PCF that would give you some contention free service whereas if you are using DCF  you are using  a service where there may be contention. of course you can use PCF and DCF at the same time. alright (Refer slide time: 20:11 - 22:10) 

So how does the protocol work? From the sender?s side it senses if the channel is idle for DIFS. DIFS is the period of time which can be configured; it then transmits the entire frame. So it just can not send some thing as soon as the channel is idle; it has to wait for at least DIFS amount of time. And it does not do any collision detection. How does it know that there will not be any collision? Just because you have waited for DIFS amount of time does not mean that there will not be any collision; there may still be collision because somebody else may also be listening to the channel waiting for DIFS amount of time and start transmitting, and you are not doing any collision detection. The point is that you will not get an acknowledgement. Unlike the Ethernet system, where there is no acknowledgement, this is an acknowledgement based system. So you will get an acknowledgement; if you get the acknowledgement you know that there is a collision and if you do not get an acknowledgement you know that there is a problem, so you retransmit. If, on the other hand, you sense the channel to be busy, then you start some random back-off time, similar to Ethernet where you do binary exponential back-off, etc. We start random back-off time. The timer counts down while the channel is idle, transmits when timer expires. If there is no acknowledgement, we increase random back-off interval and repeat step 2. This is the way system works: if there is no acknowledgement, it means that it has not succeeded. So you increase the back-off time and repeat. Another reason why collision detection is not done in wireless network is that for many of the radio systems, it is difficult to do transmission and reception at the same time. So collision detection means you keep on doing collision detection while you are transmitting. You just keep on listening whether it is going through or there is some garbled message in the medium. But that is difficult to do in many systems; so that is another reason why CD is not done. In the receiver it is simple; if it gets the frame then it returns the acknowledgement. (Refer slide time: 22:51 - 23:23) 

SIFS is another time interval, which is defined. So after some time, it will send the acknowledgement. Acknowledgement is needed for the hidden terminal problem. So there is the sender; there is the receiver; and suppose after DIFS amount of time the sender has sent some data, the receiver has received it. After SIFS amount of time, it sends back the acknowledgement. (Refer slide time: 23:26 - 24:48)


There is another scheme which uses this RTS CTS exchange. Suppose A wants to communicate ? this is the AP and this is B ? so A wants to communicate and let say B also wants to communicate. So A sends a request for transmission ? it is just a reservation request. B also sends the reservation request at the same time, and there is a collision. Since there is a collision, none of them will get it. Actually they will get it by some signal from AP as we will see. After some time A may be sending the request again and may be it is has gone through; so once it goes through AP will issue a CTS that now CTS A can be sent. And please note that CTS A not only reaches A it also reaches B. And since now B knows that it has been reserved by A, B will back off or defer for a considerable period of time. B will defer for quite a bit of time and A will send its data, then A will get its acknowledgement. This is an RTS CTS based scheme. (Refer slide time: 24:49 - 25:53) 

This is another example. A wants to send data; so it sends an RTS and gets a CTS from B. A can now send the data. By the way, this RTS and CTS have been detected by C and D also, so what they do is that now they know that somebody is communicating so this NAV or network allocation vector. It automatically puts itself off. This is a very polite kind of system so it automatically puts itself off till it gets the acknowledgement. A wants to talk to B, C is in range of A, but D is in the range of B. That is why the NAV of C starts here, whereas when B sends a CTS meant for A, then D catches it and starts it own NAV. That means it starts its own blocking time; this is called virtual channel sensing. (Refer slide time: 25:55 - 27:17)

Now just to mention how this point coordination function and distributed coordination function work at the same time ? the PFC and DFC ? and why we use DIFS and SIFS ? these two periods of time. In DIFS there are actually three time intervals, which are configured. This is DIFS; this is PIFS for point control function; and this is SIFS. Please note that when the medium is busy, after that if somebody wants to send, he can not send immediately. He has to wait for DIFS amount of time. If PCF is also operating at the same time and PCF wants to send something, PCF has to start doing that within this PIFS amount of time. So somebody wanted to send and is waiting for DIFS; when he gets this, when the PCF grabs the channel, then this other node will defer for a longer time and there is an SIFS, after which acknowledgements are sent. And after this DIFS, there is a contention window where there may be a random back-off and the next frame is sent. (Refer slide time: 27:19 - 28:17)

Suppose this was the previous frame. Within SIFS, the acknowledgement and the control frame and next fragment may be sent here; so either acknowledgement or control frame or next fragment is sent here. PCF: frames may be sent here. That means after PIFS amount of time, the PCF or point coordination function will grab the channel. If PCF has not done that, then after DIFS amount of time, it can be distributed ? that means anybody can try to send anything. There is another time which is called EIFS, for bad frame recovery. So SIFS is for short inter frame spacing; PIFS is for PCF inter frame spacing; DIFS is for DCF inter frame spacing; and EIFS for extended inter frame spacing. These are the different kinds of spacing. This way this PCF and DCF can work at the same time. Another point is that if you have a very large frame, there may be a problem in the sense that  (Refer slide time: 28:38 - 29:45) 

if a large frame becomes garbled, a large frame has a larger window, where it puts off every body. So for better throughput, it may be a good thing to break up a large frame into smaller fragments. After an RTS CTS, it may send as one small fragment; then acknowledgement fragment to an acknowledgement; and so on. The other thing is that a large frame is more likely to beget errors and if you just do the calculation, you will find that if you break it up, there may be orders of magnitude difference between the error probability of a large frame and a small frame. So overall, your throughput may be much better and in an especially noisy situation, your throughput may be much better if you send smaller fragments. For smaller fragments, first of all you can do some error handling locally, and you can handle it ? that is one thing. Secondly, for a large frame, the probability of error is much higher. (Refer slide time: 29:46 - 32:09) 

We will discuss just a little bit about the 802.11 frame addressing. If you remember, in an Ethernet, we had two addresses: the source address and the destination address. Here, actually very surprisingly, we have four addresses. And just to show you why, address 1 is the MAC address of wireless host or AP to receive this frame. So this is the destination, immediate wireless destination, that is, wherever you want to land up on this wireless link. Address 2 is the MAC address or wireless host or AP transmitting this frame. This is the source address, so to say. Now the point is that, after all, quite often what you want to do is that you are not always interested in the technology used for this wireless transmission. You are trying to connect to a network, which is in the outside world. So this is what will happen ? this access point will connect to a router, or it may connect to a LAN and that LAN may be connected to a router. So basically what you have to do for going out of this network altogether ? that means not only this wireless part of the LAN or the wire part of the LAN ? you will have to know the MAC address of that particular port of the router, which you want to reach as a next stop. Then the router will decide to go next so the MAC address of that router must also come from the source itself. So address 3 is for that MAC address of router interface, to which AP is attached and AP may be attached to a LAN and it may have multiple addresses. Address 4 is used only in ad hoc mode; we will not discuss it there. These are the four addresses; yet another thing is the payload. The payload is from 0 to 2 kb. These are all in bytes: all MAC addresses are in 6 bytes; MAC addresses are 18 bytes just for 3 addresses; and for 4 addresses it is 24 bytes. (Refer slide time: 32:10 - 32:33)

So this is the picture ? originally we had a just the routers, MAC address and AP MAC address. These two ? destination and source address ? when you are sending from a wireless host the AP MAC address, the host MAC address, the router MAC address are address 1, address 2 and address 3. (Refer slide time: 32:36 - 33:25) 

Let us now talk about the other fields ? there is a duration of reserved transmission time in the RTS CTS system that we were talking about, and that we showed you. There is a duration of reserved time. Then there is a sequence control ? this is the frame sequence number for reliable ARQ. Since you are doing acknowledgement with retransmission request, you require a 6-sum sequence number for that window. We have discussed this and now we have a sequence control number over here. Then of course, there are other fields. It could be frame type; it could be RTS type; CTS type; or the acknowledgement data subtype. We need not go into all of these. (Refer slide time: 33:27 - 34:23) 

And then we talk a little bit about the mobility within this, because whenever we are in wireless we want to be mobile. If we are going from one from under one AP to under another AP, that means from one BSS to another BSS there is the basic service set that is from one cell to another, so what the mobile host we will do is that it will sense whoever is stronger and he will connect there so there is some chance of confusion in this. But since he handles it, as he moves he will connect from AP 1 to AP 2. But this is assuming that these two APs are in the same network. If these two APs are in different networks, then the situation is a little more difficult and we can not handle it at this layer (Refer slide time: 34:25 - 37:00)

directly. We have quickly covered 802.11, which is the most common kind of wireless LAN that we see today. In many places, we have 802.11; actually in some places they are also called hot spots. That means this is under some AP, so that if a person is in that hot spot he can connect to the network and there are some campuses, at least some places, where a large number of APs have been deployed so that you are continuously ? wherever you are in that whole campus ? always in the network. That is one kind of system, that is, 802.11. Next we come to another kind of wireless systems, namely wireless MAN; that means wireless metropolitan area network. What we want to do is that we want to connect an entire metropolis with this; obviously this 802.11 is no longer sufficient. First of all, the power is low. Actually in 802.11, in order to handle more number of users, we keep the power low so that we have smaller sized cells, etc., but in this metropolitan area network there will be many users in the same cell; working in that 2.4 GHz ISM band will not be sufficient any more. So we have to go for a much wider range of frequencies, and for this, we need to go to a higher frequency in the so-called millimeter wave region. Millimeter wave means when the wavelength is at the millimeter order. There is a standard for this wireless MAN; this is called 802.16. What we might do is that we might have a large tower because these millimeter waves usually travel in straight lines. So what we have to do is that we have to have a line of sight to the base station. We have to have a large tower so that everybody can be on the line of sight, and these different base stations may be connected through a wired network, or this base station would be connected to the general network through may be a fiber optic line or something. (Refer slide time: 37:02 - 37:44)	

This is the 802.16 protocol stack; this is orthogonal phase shift keying or quadrature amplitude modulation QM 16 or QM 64. We are not going into these or the different kinds of modulation techniques, which are used. There is a transmission convergence sub-layer; that means how to handle it from here ? once again we do not bother about this. We will just talk a little bit about the MAC sub-layer common part and the service specific convergence layer. We will just talk a little bit about it. As I said, so many systems are coming up these days that it will not be possible to handle all of them. This is (Refer slide time: 37:46 - 39:03)

just the frame format. There may be a data frame and the control frame. The control frame is the bandwidth request frame.  There is a connection ID; this binds the end points to the system. There is a connection ID, through which any particular system would get a chance to communicate. The first bit defines if it is a data or a control frame; if it is a data frame, the first bit is 0, if it is the control frame, the first bit is 1. Then it says whether the payload is encrypted or not; 1 or 0 type is the type of the frame. There are management frames and things like that; so C1 is the check sum. There is a check sum indication key that is used. Once again, we need not go in to all the details but it uses CRC error correction. So header is the header portion; for the header portion there is a CRC and the data connection ID etc. Basically the access to the medium is controlled through this connection ID. (Refer slide time: 39:04 - 39:00)

There are different service classes, which are defined in this: one is the constant bit rate service for voice real time; variable bit rate service (this is a VBR); RTVBR or a non-real time variable bit rate service, that means, NRTVBR, for high quality data; and for ordinary data, email, http, etc., this is the best efforts service. There are different service classes in 802.16, and all these are possible because here the MAC is simply controlled by the base station. The key architectural principle (Refer slide time: 39:53 - 41:09) 

is traffic control by the base station. The base station controls the traffic totally. It creates frames of time slots and allocates timeslots to connection IDs. So time slots are allocated by service class. This means that if there is a constant bit rate service, what the base station would do is that in every frame it is going to allot one or more slots to this constant bit rate service, so that it gets constant rate updates. So whoever reserves or requests that constant bit rate service, if he is not using it at any particular point of time, then it is going empty. That is why he has to pay higher for this constant bit rate service. Similarly there are variable bit rate services, and finally, with just a slightly higher priority than the non-real time one and just the available bit rate service, whatever else is left may be given to those connection IDs, which are only getting available bit rate service. These are just (Refer slide time: 41:11 - 42:07) 

pictures showing frames and each of the frames will have some slot. There are some guard times between the frames. Some of the slots are reserved for upstream traffic, whereas some of the other slots are for the downstream traffic. Quite often, what happens is that in this metropolitan area networks or in many networks, the downstream traffic turns out to be much higher than the upstream traffic. We all know, for example, if you are surfing the net, which is a very popular activity, you just send one request, which is a very small thing, and in response to your http request, a large page with a lot of graphics, etc., may be downloaded. So the downstream traffic turns to be much larger. There is a lot of asymmetry here; that is why there are a few upstream slots and a lot of downstream slots. (Refer slide time: 42:08 - 42:55) 

So frames and time slots are for time division multiplexing or duplexing. Actually it is for time division duplexing, because both upstream and downstream traffics are given some slots. Duplexing means it is going in both directions. Base station allocates time slots in frames to connections. There are some connection IDs, and to a particular connection ID, the base station may allocate time slots. CBR as I said, constant bit rate services, are of the highest priority. RTVBR, that is, real time variable bit rate has the next highest priority; NRTVBR has the third highest priority. This can be delayed and anything remaining is the best efforts contention based access of unallocated timeslots to other kinds of traffic. This is how it is done. Next we move on to (Refer slide time: 42:56 - 44:55) 

another end of the spectrum. First we talked about  wireless LANs; that means 802.11, and as I mentioned, it has a lot of variations like 802.11 a, b, g, etc. So 802.11 is the LAN side. Then we talked about MAN, metropolitan area network, and now we are going to the other end of the scale, which means very small networks, let us say, personal area networks. That means a small area is covered under a network ? between whatever I have in this pocket and this pocket, and my in my hand. We will talk about one thing, which is quite popular, namely Bluetooth, which is 802.15. So 802.15 is for the personal area network group; 802.11 is for local area network; .16 is for metropolitan area network; and .15 is for personal area network. This is less than 10 m in diameter; so you see this is very small. It has a replacement for cables, mouse, keyboard, headphones, etc. So whatever I am using may be replaced by wireless links ? that was the idea. This is ad hoc; that means there no infrastructure is necessary. This works with the idea of master and slaves; that means slaves request permission to send to master, and master grants the request. So in any such cell or radius of coverage, there will be a master and then there will be some slaves. M is the master device, S are the slave devices, and P are the parked devices. That means these are inactive at the moment, so they are called parked devices in Bluetooth. (Refer slide time: 44:57 - 46:52)

So Bluetooth and 802.15 are almost the same. There are some small differences, but this not very important. It operates in the 2.4 GHz industrial scientific, that is, ISM band, and is unlicensed, packet switched, and 1 milliwatt. This is a very important issue ? that this uses a very small amount of power as opposed to, let?s say, 500 milliwatt for a cell phone. This is low cost; that means up to 10 m to 100 m range and uses frequency hop spread spectrum; so FHSS is used.  We will see what kind of an FHSS, which divides a frequency band into a number of hop channels, is used. During connection, devices hop from one channel to another 1600 times per second; so you see it is hopping the frequencies very fast. So that is one good thing because if some part of the frequency band has noise, it has got better noise immunity because it is hopping such a large number of times. There are a large number of channels ? why a large number of channels? We are talking about a personal area network; but nowadays I may be using so many different gadgets, etc., when I am using my PC, I may be having a cell phone; I may be having a laptop; I may also have a desktop in front of me. Each of them will have a mouse and all these peripherals ? let?s say it has a monitor and all these peripherals may be connected in some way through wireless. So there may be so many things; altogether about 79 channels are possible in Bluetooth and so they go on hopping in the frequency. (Refer slide time: 46:53 - 47:28) 

Bandwidth is 1?2 mbps; we are not looking for a very large bandwidth over here, but this is just more for control and function rather than downloading files. It supports up to eight devices in a piconet. What is a piconet? Two or more bluetooth units sharing a channel is called a piconet. It has some built-in security line of sight transmission through walls and briefcases because of the frequency band which it uses. It uses integration of TCP/IP for networking. (Refer slide time: 47:30 - 47:54)

So piconet is a small area network. It is ad hoc, which means that a network with no predefined structure.  There is no predefined structure; it is based on available nodes and their locations; it is formed and changed in real time. As you can see, these networks are being formed and being changed in real time; so they may be changing all the time. (Refer slide time: 47:56 - 48:38)  

The basic connectivity is point-to-point; that means from the master to the slave. Piconet is point-to-multipoint master multiple slaves; two or more piconets can be connected to form a scatternet. By the way, how does a piconet start? Anybody can start it and claim himself to be a master, and the other devices, which are coming in later, will become the slaves. So anybody can start and become a master. This is one piconet; this is another piconet; and they may be connected. The two piconets may be connected to a scatternet. If you want to have a scatternet, then we have to have a bridge from this piconet to this other piconet. So we have a bridge slave. (Refer slide time: 48:39 - 49:26)

So 802.15 version of the Bluetooth protocol architecture ? these two are slightly different, but we will not bother about it. We have the application profiles; then we have the physical radio base band and the link manager. Link manager means the radio link manager, and then there is a middle layer, which is the service discovery, telephony, RF communication, and so on. Once again, we do not have the time to go into the details of this. The idea is that you can switch from one kind of service to another kind of service, depending on the context and situation. (Refer slide time: 49:27 - 49:38)

This is a more detailed picture of the different kinds of protocol. Let us skip this also. (Refer slide time: 49:39 - 50:29) 

In the physical layer, it has 79 channels, each 1 MHz, using frequency shift keying with 1 bit per symbol. So it comes out to about 1 mbps per channel. Of course, this is 1 mbps. The individual devices finally do not get a 1 mbps throughput for the payload part, because the efficiency is quite low. Much of the 1 mbps is taken up with protocol overheads caused by the frequency hopping. So this takes about 250 to 260 micro seconds needed to stabilize the radio after the hop. So this leaves about 366 bits for actual data, of which 126 bits are headers, leaving only 240 bits for data per slot. So what was supposed to be 1000 bits has become 240 bits. But for small devices, which are getting locally connected to each other, even 240 bits per second kind of speed may be more than enough. But so many different channels are possible; 79 channels are possible. This is just a little bit about the Bluetooth frame structure; we have the access code, and then the header, and then the data. (Refer slide time: 50:49 - 51:16)   

And the header will contain the address type, etc., and some flags and some checksum; also it is an 18-bit header. It is repeated three times for a total of 54 bits. The access code identifies the master to its slaves; one master and upto seven active slaves. Some slaves are in parked mode. (Refer slide time: 51:17 - 51:28)

So these are the system blocks: we have a Bluetooth radio; a Bluetooth link controller; and a Bluetooth link manager.  (Refer slide time: 51:29 - 52:00)

So two physical link types have been defined: one is synchronous connection oriented link ? between the master and a single slave for audio and data ? and the asynchronous connectionless ACL link, point-to-multipoint between the master and all those slaves on the piconet for data only. This is for data and this is used for others. If some voice channel is there, you can get a synchronous connection oriented link there so that you get acceptable quality of service. (Refer slide time: 52:08 - 52:24)

Multiple access scheme is based on FH CDMA, that is, frequency hopping CDMA. High speed of hops and code division multiple access offers the best properties for ad hoc radio systems. As I said, 79 hop carriers have been defined at a 1 MHz spacing. (Refer slide time: 52:26 - 52:58)

The Bluetooth has been designed to allow a large number of independent channels, each channel having only a limited number of participants. Theoretically, the spectrum with 79 carriers can support 79 mbps, but as we have seen, the efficiency may be something of the order of 25%. So it will be much less than that. Different channels have different masters and therefore, they also have different hopping sequences and phases. (Refer slide time: 52:59 - 53:38)

By definition, the unit that establishes the piconet becomes the master. As I said, anybody can start a piconet and become his master. In Bluetooth, the master implements centralized control; once again we do not try to do any distributed control. It is a small system so we do a centralized control by the master. Communication is possible only between the master and one or more slaves, which means that the slaves do not communicate with each other directly. It has to go through by the master. The master unit schedules the traffic in both the uplink and the downlink. (Refer slide time: 53:39 - 54:08)

There are various types of error corrections, which are possible. This includes both FEC and packet retransmission schemes at the base band level. Bluetooth makes use of three types of error correction schemes: one-third rate FEC, sending three copies of each bit with a majority logic; two-third rate FEC, a form of some kind of hamming code; or automatic repeat request or ARQ. So this is the error correction scheme that is used. (Refer slide time: 54:10 - 55:54)

Just now we have talked about three different ends of the spectrum. I will just mention one or two more, just to show that there are all kinds of other possibilities. For example, after this Bluetooth became somewhat popular, there was a group who wondered why not make the radius of operation of this even smaller. But here the main emphasis would be on long battery life, so that you put a small battery in a small device and it will just work till the battery?s shelf life is over. It will have a very low power. We have the 802.15.4. Similarly, there is an 802.15.2 and 802.15.3. We are not covering any of them; this is just to give you a feeling of the kinds of things, which are going on. 802.15.4 is a simple packet data protocol for low rate; it has no quality of service; has wireless personal area network; is a low power, low cost, device. So low power and low cost are the most important things. Naturally you will get low rate also, but for many applications, this may be quite fine. The channel access is via carrier sense multiple access with collision avoidance, and optional time slotting. It has message acknowledgement and an optional beacon structure ? beacon means the signal, which may be sent centrally to synchronize other systems. So three bands and 27 channels are specified: 2.4 GHz and 16 channels; 868.3 MHz and so on. (Refer slide time: 55:56 - 56:37)

It works well for long battery life; it has selectable latency for controllers, sensors, remote monitoring, and portable electronics. For example, a sensor just stays there; it is supposed to do its work, which is sensing, and may be send little bits of data from time to time. So it has a low rate, no quality of service guaranty, etc. is required, but low power and low cost are very important. That is the focus of this particular group. It is configured for maximum battery life; has the potential to last as long as the shelf life of most batteries. (Refer slide time: 56:40 - 56:59)

So MAC uses 64-bit IEEE or 16-bit short addresses, using local addressing. That means, if it is just locally, you can have your own 16-bit or you can use the full 64-bit  IEEE address; it has a simple frame structure, reliable delivery of data, etc.  (Refer slide time: 57:01 - 57:29)

So as I said, there are two channel access mechanisms: one is the non-beacon type, where it uses a standard ALOHA with CSMA CA, that is collision avoidance, and positive acknowledgement; or we can have a beacon enabled network, where it is synchronized. It has a super frame structure for dedicated bandwidth and low latency setup by network coordinator to transmit beacons at predetermined intervals. (Refer slide time: 57:30 - 58:14)

Let us now compare quickly between 802.15.4, which is the low rate and low power one, and 15.1, which is the standard Bluetooth: it transmits smaller packets over large network and larger packets over small network. They are mostly static networks with many infrequently used devices. This is an ad hoc network, which is more dynamic. You can do things like file transfer here, which you do not look forward to doing here. This may be used for home automation, toys, remote control sensing, etc. This may be used for screen graphics, pictures, hands-free, etc. So this is a somewhat different niche of application, which is the two groups, but both use wireless with different emphases. (Refer slide time: 58:15 - 58:55)

As I said, there are many standards ? I just listed some of them. There are many more, which I have not put over here: 802.11 b, which gives 11 mbps; a, which gives 54 mbps; g, which gives 54 mbps, but this is backward compatible with b, because b was the one which was most widely deployed in the beginning. 802.16 is for a MAN; Bluetooth has about 30-feet radius; we have talked about GSM GPRS when we talked about cell phones; it is going to 3G. People also talk about of 4G, but nobody knows when even 3G will actually get widely deployed. We have just seen UWB, and there are so many others. (Refer slide time: 58:59 - 59:06)

The one last point is that if you have a wireless LAN, you would want to have a bridge for connecting the TCP/IP stack, etc. We will talk about TCP/IP later on. To transmit from one to another, we require a bridge in-between. This WLAN may be a plain wireless LAN extension and the application will sit on top of this. We require a seamless support for this bridge; that is very important. There are a large number of such protocols, because there is a lot of interest over there. Some of these protocols, etc., will tie up some of them and naturally become very widely used and this is one of the most important areas of networking today. Thank you. 

 COMPUTER NETWORKS
Prof. Sujoy Ghosh
Dept of Computer Science & Engineering Department
 IIT Kharagpur

Lecture No. 24
ATM: Asynchronous transfer mode
(Refer Slide Time: 00:44)

In this lecture we will start our discussion on another very important technology, namely, Asynchronous Transfer Mode or ATM. (Refer Slide Time: 00:57 - 00:59) 

Slide Time: 02:25 - 03:35
The ATM was originally envisioned as a technology, which will solve all problems. It is present in the LAN, WAN and it gives very good quality of service. When introduced, it was considered a very ambitious plan. Unfortunately, it did not work out that way because the standardisation took a lot of time. When people from computer world and people from communication world start discussing things and come up with a standard, it becomes very complex and also the cost becomes quite high. Although the ATM made its debut in some local area networks, slowly it has been replaced in most of the local area networks. But still it is a very strong and important technology in the wide area network. So ATM remains in operation in a lot of places today and we will look into this ATM (Asynchronous Transfer Mode) now. (Refer Slide Time: 02:25 - 03:35) 

Slide Time: 03:04 - 03:35

Why ATM networks? It is driven by the integration of services, i.e., wires, data, and everything else are integrated into the same kind of network. This is the vision of the performance requirements of both telephony and data networking. This was called broadband integrated service vision or B ISDN. Telephone networks support a single quality of service and are expensive to boot. Internet supports no quality of service; but it is flexible and cheap. The ATM was developed to utilize the best of both. (Refer Slide Time: 03:04 - 03:35)   

Slide Time: 03:37 - 04:45
ATM networks were meant to support a range of service qualities at a reasonable cost and hence intended to subsume both telephone network and the internet. But the cost and complexity turned out to be high and now IP-based technology is going to fill the above role. But as we have seen, in many service providers, ATM is still present and people are deploying ATM networks even today. So ATM will remain in existence for quite some time. (Refer Slide Time: 03:37 - 04:45) 
 

Slide Time: 04:47 - 05:15
Let us see in brief the history of ATM. In the 1980s, a packet research began and in 1986, it adopted the B ISDN approach. In 1989, a 53-byte cell was permitted, which was a rather small value. The communication experts wanted a small value, but computer experts wanted a large value and there was a dispute. In 1991, the ATM forum was set up. In 1992, the ATM forum issued its first spec and added user committees. In 1996, it approved the anchorage that occurred. From then on, death of ATM in the enterprise and rollouts in the carrier networks occurred. But it is still important today. (Refer Slide Time: 04:47 - 05:15)  



Slide Time: 06:27-06:45
The basic points of ATM are that it transmits all information in small, fixed-size packets called cells. Since they are of fixed size, the switch design becomes easier. Cells are transmitted asynchronously at high speeds. This is basically statistical multiplexing. In TDM we saw that one of the advantages of packet networks is that it was more efficient in terms of bandwidth utilization. The circuit switch network was less efficient because when there is no communication, the circuit is remaining idle and bandwidth is wasted. So the ATM tried to address that using statistical multiplexing. That means, the size of cell is fixed and these cells can be pushed in any order by the end stations, but the network will be connection-oriented. This was necessary in order to accommodate the quality of service that everybody wanted. This is asynchronous unlike SDH, which is a synchronous system. Nevertheless, each cell is 53 bytes long with 5 bytes of header and 48 bytes of payload. (Refer Slide Time: 06:27-06:45)  



Slide Time: 07:49 - 08:05
To make an ATM call, a message is to be sent first to set up a connection. Subsequently all cells follow the same path to the destination. So this is just like circuit switching. First of all, you have to set up a connection where the connection is not physical but they are virtual circuits. That means the path should be found first and then along the path all the ATM switches, etc., would reserve some resources for the connection. This reservation of the resources in all the nodes along the way constitutes the virtual circuit. All the cells would flow through this same path. This is a connection-oriented system, but cells are switched for better efficiency. It can handle both constant rate traffic and variable rate traffic. Thus it can carry multiple types of traffic with end-to-end quality of service. (Refer Slide Time: 07:49 - 08:05)




Slide Time: 09:14 - 10:01
ATM is independent of transmission medium; we will see later about the different transmission media that are possible. It does not prescribe any particular rule for transmission medium. They may be sent on a wire or fibre by themselves or they may also be packaged inside the payload of other carrier systems. This is a very interesting situation. For example, when you are carrying just ATM traffic and in-between you have a WAN segment which has SDH, the ATM cells can be pushed into some SDH container and sent along correctly. Reversal of this process is also possible. Suppose there is some SDH traffic and in-between there is an ATM link, the VCs can take some constant bit rate service on an ATM link. So ATM transmitted on SDH is possible and SDH transmitted on ATM is also possible. ATM by itself can be used as a transport network. So the carriers or the service providers have employed a lot of ATM in their backbone.  So ATM is still prevalent, although it is not present in LAN or enterprise network. (Refer Slide Time: 09:14 - 10:01) 




The delivery of packets is not guaranteed but the order is. As this is circuit-oriented or is a set of virtual circuits, the order of the cells will remain the same, so that the higher layer cells need not be changed. For example, IBM suggested 25 Mbps for ATM NICs for taking it to the desktop, but it did not survive. One of the reasons for this was the cost.  The ATM NICs were more costly. The ATM switches are costlier than the ordinary (Ethernet) switches. There were few or no software for ATM as most of the software developed was based on IP. People were also not ready to move all the software into ATM, which would involve high cost and since the market did not expand as expected, the cost remained high for quite some time. So this was the difficulty of this approach.  Much of the ATM devices operated at 155 Mbps (OC 3) or, even higher, 622 Mbps (OC 12) speeds. The standardization process took too long and the resulting technology was too complex (costly) to remain in the cutting edge. The basic ATM concepts are: virtual circuits, fixed-size packets (cells), which allow fast hardware switching, small packet size, statistical multiplexing and integrated services. That means different qualities of services can coexist at the same time with good management and traffic engineering features. Scalability in speed and network sizes is possible. We will look at a few of these in the next lecture. (Refer Slide Time: 11:03 ? 11:35) 

Slide Time: 11:03 ? 11:35
ATM applications could be ATM deployments in frame relay backbones. Frame relay is one kind of wide area network connectivity, which is now slowly going out, but ATMs give connectivity to backbones. It is also deployed in internet backbones and aggregating residential broadband networks (cable, DSL, ISDN, etc). They can feed into an ATM switch and then get transported. Carrier infrastructure for the telephone and private line networks deploys ATM, and this is one area where it is still going strong. (Refer Slide Time: 10:36 - 12:23) 


Slide Time: 10:36 - 12:23

The failed market tests of ATM were the ATM workgroup and campus networks, ATM enterprise network consolidation, and end-to-end ATM. These did not happen because of software and these did not take off because of cost. (Refer Slide Time: 12:25 - 13:36) 

Slide Time: 12:25 - 13:36
Now we will compare the synchronous, i.e., telephone networks and ATM. Telephone networks are synchronous and we know it because 125 ?s is the frame rate, which is very sacrosanct in this world. ATM is asynchronous transfer mode. Phone networks use circuit switching, whereas ATM networks use packet or cell switching with virtual circuits. In a telephone network, cells from a particular source or information or payload from a particular source will come periodically as shown, whereas in ATM they can come any time if the line is free. (Refer Slide Time: 12:48 - 13:26)



Slide Time: 12:48 - 13:26
 

In telephone networks all rates are in multiples of 64 kbps, but with ATM service you can get any rate and you can vary the rate with time by programming for the required rate. If you require a constant bit rate service of 10 kbps in a data service, it is possible to have a virtual circuit where the reservation of resources would be in that fashion. This kind of service is used with current phone networks and all high speed circuits are manually set up. ATM allows dialling at any speed and rapid provisioning since this is done through software and signalling; ATM allows this. As far as telephone networks are concerned, there are lots of advantages of using ATM. (Refer Slide Time: 14:09 - 14:33)


Slide Time: 14:09 - 14:33

Now let us compare ATM with data networks. ATM is ?virtual circuit? based. The path (and optionally resources on the path) is reserved before transmission. IP on the other hand is connectionless and end-to-end resource reservation is not possible directly. Indirectly people are still trying to do that because quality of service remains an important issue. RSVP is a new signalling protocol in this IP domain in the internet, which tries to reserve resources. There are other ways to do this and one way which has become quite popular is MPLS; and we will talk about MPLS in a different lecture. (Refer Slide Time: 14:39 - 15:14)


Slide Time: 14:39 - 15:14
 
ATM cells are fixed and are small in size and there is a trade-off between voice and data. But IP packets are in variable sizes. ATM provides QoS routing coupled to signalling called PNNI. Internet provides ?best-effort? service, aiming only for connectivity. (Refer Slide Time: 15:15-15:41) 


Slide Time: 15:15-15:41
For addressing, ATM uses a 20-byte global NSAP addresses for signalling and 32-bit locally assigned labels in cells. Actually when we are talking about ATM, there are two kinds of addresses that are referred. One is the ATM address, which is 20-byte (160 bits) long. It is a huge address and it requires a large address space; this address space is divided in a particular way, which will be discussed later. There are different schemes of addresses, which people tried to subsume in this ATM addressing; this is one kind of ATM addressing. For setting of a path, this 20-byte address is required but once a path has been set up this address is not required any longer because the path has been set up. The only thing is that since this is a virtual path in-between when a node gets a cell, it must know to which particular virtual circuit this belongs. So some kind of virtual circuit identifier is required and it is a much smaller address. This is another kind of addressing. But IP uses 32-bit global address in all packets. ATM offers sophisticated traffic management and this is one of the strong points of ATM and still remains strong in it. But in TCP/IP, congestion control is packet-loss based. Whether the packet is lost or not, ATM gives much more sophisticated QoS. (Refer Slide Time: 15:43 - 17:26) 



Slide Time: 15:43 - 17:26

Let us see the pros and cons of fixed-size packets. Pros are that it uses simpler buffer hardware, i.e. packet arrival and departure requires us to manage fixed buffer sizes, simpler line scheduling, i.e. each cell takes a constant chunk of bandwidth to transmit, and it is easier to build large parallel packet switches. (Refer Slide Time: 17:29 - 17:55) 


Slide Time: 17:29 - 17:55
The disadvantage is the overhead, i.e. for sending small amounts of data at the same time for each cell you have to have this 5-byte header on only 48 bytes. So 10% is already gone on the header and if a large amount of data (may be several megabytes) is to be sent, you need a lot of cells. Hence overhead becomes important. Large frames, which are to be sent, have to be broken up into small cells. All these cells are to be put together to form the original large frame. This segmentation is on one side and the reassembly is on the other side. The overhead and the cost will also come in and the last unfilled cells, after segmentation, waste the bandwidth. This is not very important. (Refer Slide Time: 17:56 - 18:56) 

Slide Time: 17:56 - 18:56
When the cell is smaller, an endpoint has to wait for lesser time to fill it. So there is low packetization delay. When the packet is smaller, the header overhead is larger. Standard body balances the two to prescribe 48 bytes + 5 bytes, which is equal to 53 bytes.  Therefore the maximal efficiency could be about 90% only. (Refer Slide Time: 18:58 - 19:25) 

Slide Time: 18:58 - 19:25
Now we will discuss ATM layers. It was done by a committee in which there was a dispute and the ultimate result was that the ATM protocol and its layers, etc. were framed. These are quite complex. ATM is the three-dimensional figure and now we will talk about some of the important sub-layers in it. On the control and management side of it, there are number of layers and on data side there are number of them. (Refer Slide Time: 19:26 - 21:18) 

Slide Time: 21:19 - 22:23
The layers are: CS, the convergence sub-layer, SAR, the segmentation and reassembly sub-layer. These two layers put together is the ATM adaptation layer, which is called AAL. There are different kinds of AALs. AAL 1,2,3,4,5, etc. but they all have these two sub-layers. Then we have the ATM layer and this is somewhat in-between the data link layer and the network layer. The transmission convergence sub-layer, in which the transmission will come and will be put back, and the physical medium dependent sub-layer (PMD) together form the physical layer. (Refer Slide Time: 21:19 - 22:23) 


Slide Time: 22:23 - 23:03
In the above slide, there is an ATM adaptation layer, an ATM layer and a physical layer in the end system. This is a very simplistic view of the ATM layers. There are other layers for control and management function, which will be dealt with later. There are two end systems communicating from the ATM adaptation layer. The ATM adaptation layer will readily communicate with other higher layers of the software. It will come through this AAL, physical layer, then go only up to the ATM layer and then go to the other side. This is a simple view of a stack. (Refer Slide Time: 22:23 - 23:03) 


Slide Time: 23:03 - 23:27
ATM layer?s adaptation is mapping of application (e.g.: voice, data, etc.) to ATM cells. The physical layer could be SONET or it could simply be a DWDM system. ATM layer handles transmission/switching/reception, congestion control, cell header processing, sequential delivery, etc. (Refer Slide Time: 23:03 - 23:27)  

Slide Time: 24:40 - 25:03

Now let us discuss about the layers in detail. The top one is the top sub-layer of the AAL (ATM adaptation layer), the convergence sub-layer. It offers different kinds of services to different applications. Here the different types of services are supposed to converge and all of them are supposed to do ATM. So for convergence this ATM adaptation layer is used. For example, for a voice channel, constant bit rate traffic is required; and similarly for data, some other kind of traffic is required. So different classes of services were defined depending on the kind of AAL (AAL 1,2,3,4,5, etc.) and the convergence sub-layer negotiated that and came to a bandwidth contract. So the different services that are offered are the CBR, which is constant bit rate or bandwidth guarantee, and which is suitable for real time traffic. ABR is for available bit rate, suitable for bursty traffic and feedback, bout congestion. UBR, which is unspecified bit rate, is the cheapest of all and suitable for bursty traffic, may be data traffic. It provides a specific AAL service at an AAL network service access point (NSAP). (Refer Slide Time: 24:40 - 25:03) 

Slide Time: 25:34 - 26:16
NSAP refers to network service access point. The other sub-layer of AAL is the segmentation and reassembly sub-layer. It segments higher level user data into 48-byte cells at the sending node and reassembles cells at the receiving node. This sub-layer is usually implemented with ASIC. One of the reasons ATM was envisioned as a system, which will really scale to very high speed, is that cell segmentation and reassembly is to be done very fast. Usually it is done with the help of ASIC. ASIC is an application specific IC for doing the segmentation and reassembling. It tears down messages passed from the upper layer and converts them to cells. Some padding may be necessary to make it a multiple of 48 bytes. At the destination the stream of cells are reassembled. (Refer Slide Time: 25:34 - 26:16) 



Slide Time: 26:30 - 26:50

The different types of AAL are AAL 1,2,3,4, and 5, which give different classes of service, class A, class B, class C and class D. Class A is connection-oriented CBR (e.g. voice) and it is supported by AAL1. Class B is connection-oriented VBR (e.g. packet based video) supported by AAL2. Class C/D may be connection-oriented VBR (e.g. file transfer), connectionless VBR (e.g. LAN data) supported by AAL 3/4, i.e. AAL 3/4 may be connection oriented VBR or connectionless VBR. AAL 5 is a simple and efficient adaptation layer (SEAL) supporting class C/D for bursty error control at higher-layer protocol. These AALs are complex since ATM got into the service providers? backbone. But many of these AALs, etc. were never widely deployed. (Refer Slide Time: 26:30 - 26:50)



Slide Time: 27:03 - 27:53
The convergence sub-layer (CS) interprets the type and format of incoming information based on 1 to 4 classes of service assigned by the application. Class A is constant bit rate (CBR). It is connection-oriented and there is strict timing relationship between source and destination, i.e. voice. If such a very sensitive quality of service like voice is required, Class A service can be used. But Class A service is more costly. (Refer Slide Time: 27:03 - 27:53)

Slide Time: 27:54 - 28:15
Class B is variable bit rate (VBR) service and connection oriented. It has strict timing, e.g. packet mode video for video conferencing. Class C is connection oriented VBR but without a strict timing service. So there is a slight difference between class B and class C; e.g. LAN data transfer applications. Class D is connectionless VBR with no strict timing; e.g. LAN data transfer applications such as IP. For example if it is frame relay, then the person who had taken this frame relay service should have some original negotiation about the speed. So class C is a little better than class D, but not better than class A or B. (Refer Slide Time: 27:54 - 28:15)


Slide Time: 28:17 - 2:32

AAL 5 was introduced for data services. It supports both message mode and stream mode. In the message mode, a packet of length up to 65 kB may be passed to the AAL layer to have it delivered to the destination either on a guaranteed or ?best-effort? basis. (Refer Slide Time: 28:17 - 2:32) 


Slide Time: 29:53 - 30:20


The service categories available are ABR, UBR, CBR and VBR. ABR is Available Bit Rate. In this source bit rate, source follows network feedback and there is maximum throughput with minimum loss. Hence the network gives some feedback. This is just to give you an idea about how quality of service is handled in ATM because a lot of things which were learned in ATM are also employed today in some other guise in MPLS or some kind of very new IP technology. How quality of service can be guaranteed still remains a very important issue in networking today. As people are talking about convergence of voice, data, video, etc., into the same network, we require some guarantee about quality of service. Whatever happens in the pure data network like delay or jitter etc. may not be acceptable for this kind of service. So for convergence, quality of service is important and ATM offered a good quality of service. UBR, Unspecified Bit Rate, is of course the cheapest of all. The user sends anything with no feedback and no guarantee. Cells are dropped during congestion between UBR cells. (Refer Slide Time: 29:53 - 30:20)

Slide Time: 30:20 - 31:00

CBR, Constant Bit Rate, is one in which the user declares the required rate. For this, throughput delay and delay variation are guaranteed. In VBR the average and maximum rates are declared. It has two different types. One is real time VBR for voice, conferencing, etc. with maximum delay guaranteed and the other is non-real time VBR for stored video. (Refer Slide Time: 30:20 - 31:00)



Slide Time: 31:01 - 31:35

Let?s compare ABR and UBR. In ABR, the queue is in the source because the source takes the feedback from the network and adjusts the rate. In UBR, the queue is in the network. In ABR, if the queue gets filled it gets dropped and pushes congestion to the edges. In UBR, there is no backpressure because if there is pressure, the UBR will be dropped. ABR is good if the network is end-to-end ATM. UBR is the same whether it is end-to-end ATM or backbone to ATM. ABR is very fair and good for the provider and UBR is simple for the user. But UBR is generally unfair even though it is simple for the user. (Refer Slide Time: 31:01 - 31:35)  


Slide Time: 31:38 - 31:45

There is also a concept of guaranteed frame rate (GFR). It is a UBR with a minimum cell rate (MCR). It will try to guarantee this minimum cell rate but beyond that, it is UBR. So GFR is a frame-based service or guaranteed frame rate service. In this, complete frames are accepted or discarded in the switch, and traffic shaping is frame-based. All cells of the frame have the same cell loss priority (CLP), whether they are inside the MCR or not. (Refer Slide Time: 31:38 - 31:45)  

Slide Time: 31:47 - 33:10
All frames below MCR are given CLP = 0 service, and all frames above MCR are given best effort CLP, i.e. CLP = 1 service. (Refer Slide Time: 31:47 - 33:10)



Slide Time: 33:10 - 34:01
Having talked about the different types of quality of service that are available in ATM, now let us look at the ATM layer and then we will look at the data link layer of ATM. ATM layer is akin to the network layer of OSI, although it has data link layer characteristics. As seen already, this is somewhere in-between network layer and data link layer. ATM uses globally unique addresses using the NSAP format of ISO. This is used for setting up connections. Path and circuit identifiers are used once a connection is established. (Refer Slide Time: 33:10 - 34:01) 


Slide Time: 34:05 - 34:15
ATM interfaces are different types of interfaces designated in the standards. One is a computer connecting to a private switch. There is a hierarchy of switches like private switches and public switches. Here computer in the LAN may be connecting to a private ATM switch. For this there is a private UNI or private user network interface. Similarly when a private switch is connected to public switch there is a public UNI, that is, a public user network interface. Public switches talk to each other using the interface called NNI (Network Node Interface). (Refer Slide Time: 34:05 - 34:15) 


Slide Time: 34:40 - 35:08
UNI is the user network interface (public and private). NNI is the network node interface (private and public). PNNI is Public NNI. B ICI is broadband inter carrier interface between two carriers. So if there are two different carriers, there is an ISI defined for that.  DXI is data exchange interface with a router, etc. Different interfaces were defined but some were not fully defined because they were not very widely deployed. (Refer Slide Time: 34:40 - 35:08) 

Slide Time: 35:33 - 35:55
There is a hierarchy of switches. In the carrier, there are the carrier backbone switches and carrier edge switches in the service provider frame or central office. Enterprise switches, which stopped at the carrier edge switches, LAN or campus backbone switches and the workgroup switches are in the customer frame. Actually the hierarchy of switches means they are all basically ATM switches with different interfaces; different software and different protocols are given and these links differ depending on where that switch is. (Refer Slide Time: 35:33 - 35:55) 



Slide Time: 35:56 - 36:29

Let?s see the physical layer functions of ATM. It transports ATM cells on communication channels and defines mechanical specs like connectors, etc. There are two sub-layers. One is the PMD or physical medium dependent sub-layer. It has medium dependent functions like bit transfer, bit alignment, optically electrical optical (OEO) functions, etc. (Refer Slide Time: 35:56 - 36:29) 

Slide Time: 36:48 - 37:08
The other sub-layer is the transmission convergence sub-layer. It maps cells into the physical layer frame format like DS 1 or STS 3 on transmit and delineates ATM cells in the received bit stream. It is a wavelength, which generates HEC, i.e. header of cells for transmission. It generates idle cells for cell rate decoupling or speed matching. Based on the kind of transport it is using, all these speed matching, etc., have to be done and hence, this sub-layer is called transmission convergence sub-layer. (Refer Slide Time: 36:48 - 37:08) 



Slide Time: 37:12 - 37:20
Let?s see the physical layers of ATM. In ATM no particular medium was specified and so many media are possible starting from multimode fibre (100 Mbps using 4b/5b), 155 Mbps SONET STS-3c, 155 Mbps using 8 b/10 b, single mode fibre using 155 Mbps STS-3 c, 622 Mbps, plastic optical fibre using 155 Mbps, shielded twisted pair using 155 Mbps 8 b/10 b to coaxial using 45 Mbps, DS3, 155 Mbps. (Refer Slide Time: 37:12 - 37:20) 

Slide Time: 37:23 - 37:40
Other media starting from unshielded twisted pair, UTP 3 (phone wire) at 25.6, 51.84, 155 Mbps, UTP 5 (Data grade UTP) at 155 Mbps to DS1, DS3, etc., are also possible. (Refer Slide Time: 37:23 - 37:40) 



Slide Time: 37:41 - 38:43
Actually a serious attempt was made to inter-operate with several L1, L2 and L3 technologies.  However, since ATM survived only in the service provider?s backbone and at the edge fibre, the single-mode fibre remains the most important medium today. (Refer Slide Time: 37:41 - 38:43) 


Slide Time: 38:45 - 38:56

How is ATM SONET mapping done? ATM SONET mapping is done because ATM may finally get carried by a SONET transport at some point in the WAN in a good and easy manner. In a SONET there is a pointer pointing to the beginning of the payload and this payload can actually be anywhere in the frame. These cells are packed in the frame. So cells are mapped row-wise into the frame. Cells could contain data or be empty. Some empty cells might have been put over there for late matching. (Refer Slide Time: 38:45 - 38:56) 


Slide Time: 41:08 - 41:45

Since ATM is coming at a particular rate, depending on what rate it is, the ATM is connected to the next using an SDH transport. At the user end ATM has to give some guarantee about bandwidth, etc. It has to do some provisioning and for that it carries the provisioning across this SDH link. SDH is not very difficult because, depending on what speed or what rate is required, the next higher-sized container is chosen. SDH, as you know, can accommodate various types of containers like VC 1, VC 2, etc.  So it can have various types of containers, various rates, etc., and tributaries. You take the kind of container that gives guarantee about the rate. This is how the service provider provides the virtual link. The container may be in the infrastructure of the same service provider or it may be in the infrastructure of some other service provider. Then we can negotiate and configure the SDH switch so that the ATM stream will get the kind of bandwidth across the SDH part of the backbone. So this link is a virtual one. The ATM contains a number of virtual paths and each virtual path contains a number of virtual circuits. One particular pair of users is using one particular virtual circuit so that virtual circuit would be identified with a VC number known as a virtual circuit identifier and a VP number or the virtual path identifier. For good management function, ATM is divided into virtual paths and virtual circuits. (Refer Slide Time: 41:08 - 41:45) 

Slide Time: 42:07 - 42:54

ATM cell structure has 48 bytes of payload and 5 bytes of header. The header contains so many fields. There are 16 bits or 2 bytes for the virtual circuit identifier (VCI) and 8 bits or 1 byte for the virtual path identifier (VPI). The virtual path and the virtual circuit together will define a particular stream, a particular ATM stream coming from a particular virtual circuit which is originating from some particular user somewhere. Then there is an 8-bit HEC. (Refer Slide Time: 42:07 - 42:54) 



 Slide Time: 42:55 - 43:35
This structure of the cell header shown is not constant across all interfaces. GFC is present in UNI, but in NNI this GFC has been dropped and VPI is increased. VPI is only 8 bits in UNI but is 12 bits in NNI. When you are doing network node interface, a lot of paths, etc. are coming; you require more bits for identifying the path and so there is 12 bit of path identifier. In UNI there is 12 bit of VCI identifier, but only 8 bit of path identifier is present in NNI. (Refer Slide Time: 42:55 - 43:35) 


Slide Time: 43:35 - 43:56

Now let us discuss about ATM cell format. The first one is generic flow control (GFC).  Once again this was put there with some idea but it was not used much. This has 4 bits and is used for local flow control between the network access point (typically a switch belonging to the network provider), and one or more end stations. This is used for multiple access for more than one station and for reducing the transmission rate for single nodes, etc. This is used to do some local flow control at the LAN end between the switch and the users. But this is usually ignored. (Refer Slide Time: 43:35 - 43:56) 


Slide Time: 45:45 - 46:06
VPI, the virtual path identifier, has 8 bits; but since GFC is irrelevant within the network, 12 bits are used. These may be thought of as the high order bits for the virtual channel identifiers. A virtual path contains a number of virtual channels. The switches store per path parameters so that individual channels do not need any signalling. So, one pair of users is using one channel, one virtual circuit. This virtual circuit has to be identified and then given some kind of bandwidth contract. Some kind of class of service will be negotiated for this. In a big ATM network, thousands or even millions of circuits may be set up and toned down at a very high rate as many people are using millions of switched circuits under a service provider. These virtual circuits may be set up and toned down in a very short time. Theoretically with each of these virtual circuits, some kinds of quality of service parameters have been negotiated. But these are grouped together for the similar kind of services and these virtual channels are put together so that only paths are to be stored. To avoid implications in finding and routing, some of these virtual circuits are put together to form virtual paths. We have the VPI identifier for this. This VPI has the higher order bits specifying the channel, and the lower order bit specifying the VCI. (Refer Slide Time: 45:45 - 46:06) 


Slide Time: 46:12 - 46:42
However, if a virtual path is already set up and is scantily used, then resources are wasted. So, dynamic renegotiation of VP capacity can be used. VCI, the virtual channel identifier, is 16 bits. VC 0 to 15 are reserved for special purpose, and others are used for actual communication. (Refer Slide Time: 46:12 - 46:42) 


Slide Time: 47:05 - 47:10
PT is the payload type, which is of 3 bits. If high order bit is 0, the second bit indicates congestion and third bit indicates end of AAL 5 frame. 100 and 101 are reserved for link management. CLP (cell loss priority) bit is used by the source for making priority.  Intermediate switches may mark it for violating agreements. Depending on whether CLP = 0 or CLP =1, the cells may be dropped or not dropped in an intermediate switch. HEC is an 8-bit header checksum. This is very important in checking whether the header has an error. This has some other function, which will be dealt with in data link functions in ATM. (Refer Slide Time: 47:05 - 47:10) 


Slide Time: 47:18 - 47:38

The next sub-topic is data link layer in ATM. This consists of the transmission control layer. (Refer Slide Time: 47:18 - 47:38) 


Slide Time: 47:40 - 48:06
Each ATM cell has a 5-byte header, in which the last field, HEC, is a checksum for just the header. The TC takes the cells from the ATM layer, adds an HEC to them and sends them as bit streams to the PMD on the transmission side. The bit streams may or may not have a separate transport. (Refer Slide Time: 47:40 - 48:06) 


Slide Time: 49:17 - 49:44
On the receiving side, the incoming bit streams are formed into cells and passed on to the ATM layer. Here the cell boundaries are to be detected. It also discards cells with invalid headers and processes OAM cells for administration management and control. One problem which came up was the ATM forum wanted the ATM to be deployed everywhere from the backbone, provide a switch right down to the user. So nothing much was specified about the physical layer. Since they wanted neutral to the kind of physical medium through which the ATM will pass, another problem came up with receiving the bit streams. Cell starting and cell ending couldn?t be identified on the receiving side. (Refer Slide Time: 49:17 - 49:44) 

Slide Time: 51:00 - 51:12
In some cases, the underlying physical layer helps. For example, when ATM cells are carried over SONET or SDH, the SPE pointer in the SONET header points to the first full cell. So we immediately know where it starts. In other cases every 40-bit sequence is tested for being a valid header. The 8 bits at the extreme right will be valid HEC for the remaining 32 bits. HEC has two functions. One is that it gives you some checking mechanism on whether the header is correct or not and the other is for synchronization function. When a bit stream is coming and if it is 40-bit long, it is taken. Now if these 40 bits happen to be the header, the last 8 bits will be the checksum, assuming that the entire header has come correctly. So the last 8 bits would be a checksum for the other 32 bits in the header. If it is a header with an error, it is neglected. Then, the next 40 bits are taken and tested again, assuming that a header has come correctly. (Refer Slide Time: 51:00 - 51:12) 

Slide Time: 52:40 - 53:27
The TC goes through a HUNT; this is the hunting phase. And once it gets a header it gets into the PRESYNCH stage. It has one header. But, out of 40 bits, 8 bits are the checksum of the other 32 bits. So the probability is that this was not a header but a user payload and was wrongly interpreted as a header because these 8 bits matched the checksum for the other 32 bits. So, now it goes into the PRESYNCH stage. If this is indeed the header, then the first 5 bytes would be the header, the next 48 bytes would be data and the next 5 bytes would again be a header and you continue checking this for some time. This is a PRESYNCH stage. When this synchronization is done for some time and you are reasonably sure that this cannot be a coincidence in the data given by the user, then we are indeed locked on to the header and now TC is synchronized. The TC goes through the HUNT, PRESYNCH and SYNCH stages to detect the cell boundaries. If the number of HECs is found to be incorrect then TC is said to have lost synchronization. Then it has to be resynchronized. This heuristic defies the layered architecture. (Refer Slide Time: 52:40 - 53:27) 


In this simple state diagram, the signal is in the hunting stage. At this stage the signal looks at every 40 bits and tries to figure out whether it could be a possible header or not. When correct HEC is detected it goes to the PRESYNCH stage. If correct HEC is not detected, the signal goes back to the hunting stage. If a few consecutive correct HECs are found, then it is synchronized. If a few consecutive incorrect HECs are found, then the TC will determine that it has lost synchronization and it will go back to the hunting phase. In the next lecture we will talk first about ATM addresses, ATM routing, etc.


Preview of the next lecture
Lecture No. 25
ATM Signalling Routing and LAN Emulation

In the last lecture we discussed ATM technology. We saw, how it handles cells, how it make cells, etc.  Now, in the first half of this lecture we will discuss ATM signalling and routing.  Ethernet networks and other kinds of networks are ubiquitous. In the second part, we will discuss about how an IP network and an ATM network will interoperate when we use ATM as the backbone.  (Refer Slide Time: 55:03 - 55:09)

Slide Time: 55:11 ? 55:36
Next we will discuss ATM signalling, routing and LAN emulation. (Refer Slide Time: 55:11 ? 55:36)

Slide Time: 55:38 - 55:42

ATM connections are of various types, the most predominant being the switched virtual circuit, in which a path is set up and taken to the destination.  The other one is permanent virtual circuit which is pre-coded.  There are also other connection types like simple point-to-point connection, symmetric or asymmetric bandwidth connection (Uni- or Bi-directional), point-to-multi point connection (Uni-directional) and data replicated by the network. (Refer Slide Time: 55:38 - 55:42) 

Slide Time: 55:43 - 56:53

This is an example of a point-to-multi point network.  (Refer Slide Time: 55:43 - 56:53)

Slide Time: 56:54 - 57:28

In an ATM connection set up, a signal is set up where the source and the destination are shown.  There are intermediate switches in between.  From the source there is a set up signal which goes to the intermediate switch.  The switch sends back some kind of an acknowledgement saying that the call is proceeding and sends the set up signal to the next hop and so on.  Each of them will immediately send back some kind of acknowledgement and then when the call is accepted a connect signal will start flowing in the opposite direction. When it reaches the source then a connect acknowledgement will flow and for this connect signal the circuit is set up.  Alternatively the destination may reject the signal and then it will send a Region Release Signal back. (Refer Slide Time: 56:54 - 57:28)

Slide Time: 57:29 - 57:31
On the other hand, when the circuit is as shown above, for taking it down the source will send a release.  When it finishes sending it will send a release till it reaches the destination.  The destination will then send a release all the way back.  The release could be initiated by the sender or the release could be initiated by the destination. (Refer Slide Time: 57:29 - 57:31)

Slide Time: 57:33 ? 58:13
Then connection gets terminated and the release is completed. (Refer Slide Time: 57:33 ? 58:13)

Slide Time: 58:15 -51:20 min
There is another approach to this not using LAN but using classical IP over ATM.  The definitions for implementations of classical IP over ATM are described in RFC 177.  This RFC considers only the application of ATM as a direct replacement of the ?wires?, LAN segments connecting IP end-stations (members) and routers operating in the classical LAN-based paradigm.  Issues raised by MAC level bridging and LAN emulation are not covered here. (Refer Slide Time: 58:15 -51:20 min)


When we look at classical IP over ATM, address resolution and encapsulation are the two issues which are to be considered.  Encapsulation consists of putting appropriate ATM header/trailer to a packet, converting it to a number of cells and then sending it.  This means that when you have an ATM packet and a classical IP is running over ATM and you have got a big packet, you have to break the cells up and put a proper header on each of them and then send them.  ATM features are not utilized and inter network traffic handling is clunky.
COMPUTER NETWORKS
Prof. Sujoy Ghosh
Department of Computer Science and Engineering
IIT, Kharagpur
Lecture-25
ATM Signaling, Routing and LAN Emulation
 (Refer slide time: 00:35)

We have looked at ATM technology in the box, in the sense that how it handles cells, how it makes cells. Today, in the first half of this lecture or first part of this lecture, we will discuss ATM signaling and routing. Since Ethernet networks and other kinds of networks are ubiquitous, everywhere, if ATM is used in a backbone, how they would interoperate between an IP network and an ATM network. We will talk about that in the second part of this lecture. (Refer slide time: 01:23 - 01:30min)

So today we talk about ATM signaling, routing, and LAN emulations.  (Refer slide time: 01:31 - 04:43 min)


The first concept is that ATM uses virtual circuits; that means there are two ways to use packets: carry entire destination address in header, or carry only an identifier, also known as label. Labels have local significance, addresses have global significance. Signaling protocol fundamentally maps global addresses or paths or sequence of addresses to local labels. We will discuss this in much more detail when we discuss routing of IP packets, which we will take up after this lecture.
Usually when you have a packet switching network, then each packet is considered in one extreme, that is, in the IP end. Each packet is considered on its own. That means each packet must contain the destination address at the very least. It also contains the source address that is different; so it must contain the destination address, so that looking at each packet, an intermediate router would know where it should go. That is one end of the spectrum, whereas in the connection-oriented system we know that the physical connection is set up. In ATM we set virtual circuits or virtual paths. In virtual circuits or virtual paths, what happens is that these ATM cells are very small, only 53 bytes long. Each of the cells will contain some local label; so a path is set up. Now setting up a path in ATM means that each of the intermediate switches would know that a flow of cells is going to go through them ? from one source to some destination. They make provision to accommodate this flow; they make provision for the virtual circuit, and then, once they do that, they set up this virtual circuit in the starting phase. After that, each of the cells need not contain any specific identifier or specific address for the destination. It just needs a small address, which tells the intermediate switches which virtual circuit to use. It contains a virtual circuit identifier. Actually it is divided into two parts as we have seen in the cell header: we have the VPI part and the VCI  part. Simply looking at that label, the intermediate switch would know which virtual circuit to use.  (Refer slide time: 04:44 - 05:10 min)


Before this virtual circuit, we see that we have the samples. Suppose we have the data in the ATM cell, the data would be preceded by simply the virtual circuit identifier and this might have two parts: VPI and VCI, whereas in a regular datagram the entire address may have to be there in each packet. (Refer slide time: 05:11 - 06:57 min)


In VPI/VCI assignment used in this case, all packets must follow the same path unlike a datagram, because this virtual circuit is set up before any actual flow of packets begins. There is a time for the circuit set up and this circuit is not a physical circuit we have in a telephone network; but this is a virtual circuit, that means each of the intermediate nodes simply knows that a flow is about to begin. So this has to be set up. Once this is set up, all packets would flow through the same path. Switches store per VCI state, e.g. QOS (quality of service) information about this particular VCI. Signaling implies separation of data and control. When we do ATM signaling, we are talking about setting up the entire path. We will come to that; small ids can be looked up exactly much quickly in hardware ? that is one good thing. If you have a small VCI, that means, virtual circuit identifier, it can be looked up very quickly in a hardware ? this is a bottle neck in a router. This can be handled very fast, it is harder to do this with IP address. That means this longest prefix match ? we will come to that later on. The setup must precede data transfer. This is the other disadvantage of this ? the VPI and VCI must be set up, which means it delays short messages. There are two types of virtual circuits: switched and permanent virtual circuits. (Refer slide time: 06:58 - 07:43 min) 


This is an example, the switch knows in the input ports say 1 and 2, 1/37. If these are the VPI or VCI identifiers, out will be through port 3, 1/35. Similarly port 1 for 34 will go to 4, 2/56 and so on. There is a table which simply quickly matches the VPI/VCI values and puts it on another VCI/VPI pair on the other side. This is how VPI/VCIs are assigned and used. (Refer slide time: 07:44- 10:16 min)






We now come to ATM addresses. This address is different from the VPI/VCI; that is, the virtual circuit identifier/virtual path identifier, which is simply local. But for setting up the path, initially you require an address, and you cannot do with a local address. You have to do with a globally consistent address. This global address of ATM is 20 bytes long. There is a 20-byte long ATM address, which you use for setting up a circuit. I mentioned previously that there are two types of circuits: switched virtual circuits and permanent virtual circuits. In permanent virtual circuits, a path is set up initially manually. The path remains a permanent virtual circuit. Whenever there are two end points between which a lot of traffic will flow, you may skip this overhead of path set up and set up a permanent virtual circuit; this is like a leased line. Whenever there is something to go from this source to that destination, it will simply use the pre-existing permanent virtual circuit or otherwise, we may have a switched virtual circuit. That means this virtual circuit is set up on the fly, as the network is being used. In an ATM switch, hundreds of thousands or may be millions of virtual circuits may be set up or taken down every second. This is a very fast process. For setting up this virtual circuit, we require an ATM format and this is 20-byte long address. Unlike an IP address, which is only 4 bytes, this is 20 bytes. This is a very long address, left to right hierarchical. in this, there are level 1, level 2 level 3 and level 4. The first two levels have a 13-byte prefix, this are the levels of hierarchy. This part is usually used for the actual network addressing inside. Since it is such a long address, (Refer slide time: 10:17- 11:56 min)

it can accommodate various schemes. ATM was conceived as a technology, which will subsume and absorb all pre-existing technologies. The people who designed ATM tried to accommodate different kinds of addressing schemes in one super addressing scheme, which they say is ATM addressing scheme. As you see here, there are different types of addresses possible. Since there are 20 bytes, it is possible to have more number of schemes. These are the schemes 1 byte + 2 byte 3 + 10, 13 ? this is supplied by the network. These 6 bytes are end-system supplied and not used in routing. This may be used inside the host; 1 byte for maybe de-multiplexing inside. These network supplied parts are 1 byte, which indicates the scheme. The three NSAP (network service access point address) formats are DCC, ICD, and E.164 or ICD number or DCC number. This is a data country code, which uses 2 bytes and 10 are used for the other part of the network. (Refer slide time: 11:57 - 13:07 min)
 
Actually here, authority and format identifier is the first thing. So 39 is ISO DCC; 47 is British Standards Institute ICD; and 45 is ITU ISDN, which means that this E.164 is actually an ISDN number. ISDN uses 15 characters, i.e. 15 binary coded decimals, that is, 7? bytes. This entire ISDN number, which again can subsume telephone numbers, can be put over here. ISDN uses E.164 numbers. ATM forum extended E.164 addresses to NSAP format and E.164 number is filled with leading 0s to make 15 digits, that is, AF16 is padded to make 8 bytes instead of 7? bytes. End system identifier is the other part. This is the end system identifier part, and these 6 bytes could be various things, specifically (Refer slide time: 13:08 - 13:42 min)

this 6 byte could be 48-bit IEEE MAC address. Remember the MAC address that we used in the data link layer is 6 bytes, supplied by IEEE. The entire 6 bytes can straightaway be incorporated in the low order bits of the address. Selector is for use inside the host; all ATM addresses are thus 20 bytes long. There are various ways you can route this ATM; given an ATM address there are various schemes possible. (Refer slide time: 13:43 - 14:03 min)

Since various schemes are possible, ATM addresses could be of variable lengths and have an initial domain part and a domain specific part. The initial domain part consists of two fields as we have already mentioned: AFI, that is, the authority and format indicator, (Refer slide time: 14:04 - 14:15 min)

and IDI, that is, initial domain identifier. This identifies the domain within the purview of a given addressing authority.  (Refer slide time:14:16 - 16:18 min)

In a particular format, say ICD (international code designator), all addresses have a unique fixed length prefix. The high order DSP or the high order bits of domain specific parts roughly correspond to the low order part of network number in IP; ESI is the second. This particular scheme can subsume a lot of other schemes, which were possible. The reason for showing this in this fashion is that a similar thing was tried later on in IP version 6. We have a very large address field where a lot of previous schemes could be subsumed in this. One of the difficulties with this IP addressing scheme was that it was done in a very haphazard manner, unlike the telephone number. For example, the telephone numbers are geographically distributed. That means, you put particular first few digits and that will immediately indicate which country and region you are calling. It is easy for the router to just look at the first few digits and send it to the trunk. But that is not possible in the IP version 4 which is used, because it has no geographic correlation. You have to take keep a very long table and look into the table. Once again when people developed this ATM addresses or the IP version 6, they tried to bring back some order into this addressing scheme. (Refer slide time:16:19 - 16:36 min)


 For setting up connections: IP ATM supports both permanent virtual circuits and switched virtual circuits. PVCs are pre-coded in each switch along the way and are always present. They are like leased lines, which do not need any connection setup. (Refer slide time:16:37 - 17:06 min)

For connection setup there is a user network interface or UNI, and the network interface, which is the NNI. There is a part called Q.2931, which is an ITU protocol for setting up paths and this SSCOP means service specific connection oriented protocol. You have the AAL and ATM. These layers have already been talked about earlier. (Refer slide time: 17:07 - 17:49 min)

Unfortunately this whole scheme turned out to be quite a complex one. We will not go into all the details. We have already seen the ATM layer and the AAL layer, which is in the user plane. Today we are going to talk a little bit on this control plane, which has one service AAL and three parts: SSCF, SSCOP and AALCP or AAL common part. For setting up the circuits, etc., we have Q.2931, BISUP and PNNI or public network-to-network interface. (Refer slide time: 17:50 - 18:30 min)

UNI is the user interface of the ATM networks and consists of signaling protocol for setting up circuits of a certain quality and the format of the cells. The NNI deals with the issue of signaling and data transfer as well as routing, data transfer, operations and management. Remember there is also a slight difference between the cells which go through in the UNI part and this NNI part. In the NNI part, the GFC is dropped and we use the whole thing to accommodate more number of virtual circuits; but that is a small technical point. (Refer slide time: 18:31 - 19:01 min)

If you look at the control stack, that is, control plane stack, we have this Q.2931 sitting on this SSCF, this SAAL, that is, service ATM adaptation layer, which sits on the ATM layer, which again is on some physical layer, there is a virtual link between two ATMs, AALCPs and so on between a stack from the source to the destination or from one hop to the other. (Refer slide time: 19:02 - 21:14 min)


Let us see what Q.2931 is. We will not go into this, once again the protocol is quite complex. We will just touch upon some aspects of it. This is an ITU protocol for setting up a connection. First it sends a request in the meta signaling channel 0 to negotiate a quality of service for a signaling channel. What is done is that, for setting up the circuit, you have to do some communication. This communication again will be through ATM. Actually for that, it will require some kind of virtual path and virtual circuit. ATM is quite strong on quality of service. There is a question of quality of service of the service channel also. Although the service channel would be used for a very short time, once the circuit is set up, that service channel may be released. There is a meta signaling channel called 0, where you can negotiate the quality of service for the weak signal channel with the quality of service. Otherwise, there is a default which is VP0, VC5, that means, virtual path number 0. In virtual path number 0, which is a bundle of circuits, take the VC 5, which is a standard channel, where you put your request for signal, for setting up the path. Now if this is successful, then a new VC is assigned for connection setup requests and replies. Then you first make the request in this channel then a new VC would be assigned for this particular connection setup. Q.2931, if you remember, is at the top of the control plane protocol stack, which initiates at the setting up of the circuit and handles setting up of circuits. (Refer slide time: 21:15 - 21:59 min)


Below the Q.2931 we have the signaling AAL; that means, signaling ATM adaptation layer, which again contains three parts: service specific coordination function, which provides interface Q.2931; interface between Q.2931 and the ATM stack; service specific connection-oriented protocol, which is the SSCOP ? this handles error, loss and recovery. All these are communications for setting up circuit for communication for the control purpose and the AAL common part, which handles error detection. This is roughly the stack. (Refer slide time: 22:00 - 23:59 min)


There are various kinds of parameters in the forward direction and in the backward direction. There are various parameters that you can specify for the quality of service. With ATM, when it was introduced, a serious attempt to handle the issue of quality of service was made. It has now become so important that in the IP domain also, which is turning out to be the dominant technology, quality of service has become important. People have thought of various schemes for handling quality of service. Many of the schemes that people had already thought about with ATM have been adopted in various ways. We will talk in detail about quality of service later on. 
Today, we will just touch upon it; there are various parameters like peak cell rate ? that means the peak rate at which you will be pumping inside; sustainable cell rate; maximum burst size; etc. All these different parameters can be negotiated for one particular virtual circuit. When a path is set up along the way, each of the ATM switches on the way makes some provision for supporting that particular new virtual circuit with that kind of service. If it cannot handle that, may be some negotiation about it can be handled. Leaky bucket is some kind of congestion control algorithm. We will discuss it later. (Refer slide time: 24:00 - 24:29 min)


ATM connection types are of various types; the most pre-dominant ones are the switched virtual circuits, where you set up a path and take it down; or permanent virtual circuit, which is pre-coded. There are other connection types, e.g. simple point-to-point connection, symmetric or asymmetric bandwidth connection, point-to-multipoint connection, data flow in one direction only, or data is replicated by the network. (Refer slide time: 24:30 - 24:35 min) 

So this is an example of a point-to-multipoint network. (Refer slide time: 24:36 - 25:45 min)

When you do an ATM connection setup, you set up signal and maybe this is the source and this is the destination. These are the intermediate switches. From the source, there is a set up signal, which goes through the intermediate switch, which sends back an acknowledgement saying that the call is proceeding and sends a set up signal to the next hop. The set up signal is then sent to the next hop and each of them immediately will send back an acknowledgement. When the call is accepted, a connect signal will start flowing in the other direction and when it reaches the source, a connect acknowledgement will flow and each of them will give this connect acknowledgement for this connect signal. The circuit has now been set up. Or alternatively, the destination may reject it; then it simply sends a region release kind of a signal. (Refer slide time: 25:46 - 26:23 min)

For circuit setting up and for taking down a circuit, the source will send a release; that means he has finished sending release and release complete will go on. Then finally the destination will send a release all the way back. A release could be initiated by the sender or the release could be initiated by the destination also.  (Refer slide time: 26:24 - 26:28 min)

The connection gets terminated and release is completed. (Refer slide time: 26:29 - 28:14 min)


PNNI is the private network-to-network interface; that means between the end system and the switch, we have the UNI and we have the NNI. PNNI is a private network-to-network interface and this could be between two switches or two entire networks. PNNI uses link state routing protocol for ATM networks. We will look into the details of link state routing when we deal with OSPF in the IP worlds.
Since IP is the more prevalent technology, we will discuss it in detail when we do it here. Each node will periodically broadcast the state of the link to which it is connected to all parts of the network. This way all the switches get some global picture about the status of the various links, so that they can run some algorithm locally in a centralized fashion and find out all the possible paths. They use this for setting up the path later on. (Refer slide time: 28:15- 30:17 min)

Actually the situation is a little more complex than what I have just said, because there is a hierarchy mechanism that ensures that this protocol scales well for large, world wide ATM networks. A key feature of the PNNI hierarchy mechanism is its ability to automatically configure itself in networks in which the address structure reflects the topology. There are two things here: one is this hierarchy we are talking about, and the same kind of thing is used in OSPF in the IP domain. When we discuss OSPF, we will go into the details of this; but there is a hierarchy over there. Imagine what would happen if all were ATM switches, which they are not, but suppose they all were. That was the vision; and if the ATM switches were communicating with everybody else with their link states, the database and everything would become huge. In order to scale to a very large network, it goes through in a hierarchical fashion, meaning you can have a hierarchy of  networks and at each hierarchy, the peers run some kind of PNNI for routing within that level of hierarchy. And at a lower level, they will again run PNNI for that level and various hierarchy levels are possible. You have seen that the ATM address is given in such a way that we have only shown the broad boundaries but within that boundaries, it can be further divided into a number of hierarchies and again different designated authorities can break it up in a different way. All these flexibilities are possible; PNNI allows that in some plane, in some hierarchy, one kind of addressing scheme is used and in another, a different kind of addressing scheme is used. (Refer slide time: 30:18- 31:36min)

The substance is that it scales to very large networks, supports hierarchical routing, supports quality of service, supports multiple routing metrics and attributes, because when you broadcast the link state, i.e., the state of the link, you may also broadcast all different kinds of parameters about the links that can be handled in that link or how much can be handled by the switches, etc. can be propagated throughout the network. If you have a fair idea about what is possible and what is not possible, then you can plan your route in the source in a particular fashion. Use of source routed connection setup: since the source has the global picture, it will use that global picture to compute the route and set up the connection. Once it decides on the route, it used the Q.2931 to give all the connection setup. That set up request, acknowledgement and release etc. will be used. It operates in the presence of partitioned areas. (Refer slide time: 31:37 - 32:16min)

 PNNI features provide dynamic routing; that means, the link states may change from time to time. Each of the switches are going to broadcast their link states; it is responsive to changes in resource availability; separates the routing protocol used within a peer group from that used among peer groups. Various hierarchies are possible: interoperates with external routing domains, which are not necessarily using PNNI and supports both physical links and tunneling over virtual circuits. (Refer slide time: 32:17 - 33:44min)

This is an example of hierarchy. This big network, which is again a network of networks, is represented by one node in the top level of the hierarchy. Similarly, you use PNNI to plan a route like this. Within the network you might have to again do a planning and each node may again be corresponding to another network at a lower level. At each higher level of the hierarchy, you can use PNNI to plan the route if there is A.1.1. Its view of A.1.1 is something below A.1.1, A.1.2, A.1.3 are explicit. These have abstracted notions of A.1.2 and then you come to B; these are B and C. Although when the call setup is passing through B, it may come all the way down and do the actual path setup. That is how the hierarchy works.  (Refer slide time: 33:45 - 34:10min)

At any level of the hierarchy, A.1.1 will make a source route, which goes through A.1.2, then B and C. So the source specifies the route as a list of all intermediate systems in the route. This was the original idea also in the token ring. (Refer slide time: 34:11 - 34:30min)



For this, it uses a designated transit list (DTL), which is in the form of a stack, as I will show you. Source route is across each level of hierarchy; there is an entry switch for each peer group; it specifies complete route through that group; and set of DTL manipulations is implemented as a stack. (Refer slide time: 34:32- 34:45min)

A and B may be at the bottom of the stack; A.1 is at the top level of the hierarchy and this is how it is put in a stack and the path is completed. (Refer slide time: 34:46- 35:32min)

We now discuss the quality of service parameters. I will just mention some of the metrics, etc. We will not go again very deep into this because we do not have that much time. But it will give you some idea about what we mean when we say quality of service. One such parameter metric is maximum cell transfer delay; that means, the delay from the beginning of the first bit of the first cell to the last bit of the cell. Others include maximum cell delay variation; the variation of this time; maximum cell loss ratio; whether the cells could be lost, and if so, what is the maximum cell loss ratio; administrative weight, etc. (Refer slide time: 35:33 - 35:55min)


The attributes of the parameters are available cell rate or its capacity, whether it is available; cell rate margin is allocated minus actual; variation factor; branching flag; restricted transit flag. These are all different parameters and their attributes ? QOS parameters and their attributes. (Refer slide time: 35:56 - 37:27min)


One way this is handled is the generic call admission control, which happens when you are on the source side, i.e., when you are deciding on the route and you are doing a source routing, at that point whether you can admit a request, which has finally come from the user, from the UNI, etc. But at that particular point of time before making the request, there is an admission control, which is a generic cell admission control run by a switch for choosing a source route. It determines which path can probably support the call; that way it will try to  route the call setup. Actual call admission control is run by each switch. In the beginning we will run a GCAC as well as ACAC and the intermediate switch simply runs an ACAC or the actual call admission control to check whether that request has come to this switch, whether it can handle this request or not; this is the protocol which is running. (Refer slide time: 37:28- 38:06min)

There are traffic management functions; the call admission control is a kind of traffic management.  Traffic shaping means limit burst length, space out cells, etc. for getting a maximum throughput; usage parameter control is to monitor and control traffic at the network entrance of various traffic management systems. Both quality of service management and traffic management are possible in ATM, and there are extensive protocols for negotiating these various parameters and for setting it up. (Refer slide time: 38:07- 38:39 min)


The functions of traffic management are as follows: there is selective cell discard with CLP or cell loss priority. If CLP is 1, the cells may be dropped if the situation warrants. It is something like an unspecified bit rate and it may have a very low priority. Cells from non-compliant connections may be dropped. There is also frame discarding. One example of feed back control is an ABR scheme. (Refer slide time: 38:40- 38:46 min)


We will just quickly look at the peak cell rate. I am not going into the details of these: cell transfer delay, cell delay variation, cell delay variation tolerance, cell loss ratio, etc. are the parameters. (Refer slide time: 38:47- 39:21min)

Explicit forward congestion indicator: we will just have a quick look at this ABR system, which is a binary rate system, which sends an EFCI. This is explicit forward congestion indicator, which is set to 0 at source and congested switch is set to 1. Every nth cell destination sends a resource management cell to the source. (Refer slide time: 39:22- 39:43 min)

What happens is that somewhere in-between if an intermediate source, or EFCI, is congested, this may set it to 1 and then that information may flow back finally to the source and the source may try to restrict its requests. (Refer slide time: 39:44 - 40:16 min)


Sources send 1 RM cell every nth cells, the RM cells contain the explicit rate that has been asked for; the destination returns the RM cell to the source. The switches adjust the rate down; that means if it is congested, it may adjust the rate down and the source adjusts to the specified rate. Whatever rate comes through this, going up and coming down through this negotiation, is what the source finally has to accept. (Refer slide time: 40:17 - 41:38 min)

We will talk a little bit about LAN emulation, which is emulating a local area network when the backbone is of an ATM or when you use an ATM but still want to use the Ethernet and IP, specifically IP, in the nodes. And one very specific reason this is required is as follows. One of the reasons ATM was not successful in the LAN segment, although it was put as a LAN solution also in enterprise LAN solutions, was that there was hardly any software which was developed on ATM. Whereas a huge amount of software has been developed using IP and so much of software has been developed in IP, you cannot throw it up nor can you translate it to an ATM software overnight; that is very difficult. People took a more pragmatic approach and thought the backbone network technology be that of ATM or have an ATM. Let us emulate the LAN so that your software IP based software can still run. (Refer slide time: 41:39 - 42:02 min)


Problem: it needs new networking software for ATM solution. Let ATM network appear as a virtual LAN. How can an ATM network appear as a virtual LAN? LAN emulation is implemented as a device driver below the network layer. These are LAN emulation bridges actually; if there is an ATM, this will look like a LAN. (Refer slide time: 42:03- 42:41 min)

For this, 1 ATM LAN can be n virtual LANs. Many virtual LANs can be there in the same ATM LAN. Only one of them may be sufficient, this is a logical subnet interconnected via routers. This is the abstraction, this is the picture that we want to give to the world. It needs drivers in hosts to support each LAN; so in actual practice, only IEEE 802.3 and 802.5 were supported, although FDDI could also be done. (Refer slide time: 42:42 - 43:21 min)

This is the picture: we have an ATM switch, we have some LANE servers, we have multiple LANs on this, we have a LANE server A and a LANE server B. The logical view would be as if a router is also connected to the ATM switch. The logical view  looks like an IP network. We have A1, A2 connected via this network A and B1, B2 and there is a network B, which is connected. These two IP networks are connected via routers. This is the logical view, although the actual view is emulated here. (Refer slide time: 43:22 - 45:55 min)

It requires several components; one component is that we require a LAN emulation client in each host. Each host must have LEC or LAN emulation client, which is a small software, which can be loaded in each host. LAN emulation configuration server or LECS will be in one central server, it will be taken as the LAN emulation configuration server. Whenever somebody wants to join the LAN or leave the LAN, the LAN emulation client will first ask for the parameters from the LECS. It has to know the address of the LECS and here is a LAN emulation server itself, the LES, and a broadcast and unknown server. If there is something, like the broadcasting in a network, which is done, we know that it is in a particular network for an ARP. We want to do an address resolution protocol. What we do is that we broadcast a request, i.e., the IP address; what is the data link address? Remember that this ATM works on point to point, mostly, as a point-to-point connection. Although point-to-multipoint is possible, but it is not in both directions. Instead of trying to do the broadcast from the host itself, what it does is that if each host in this LAN emulation client has to broadcast anything, it will send it to another server called bus; there is a virtual connection between every host and the bus, and the bus will send the broadcast to each of the hosts. That way in an indirect fashion, the broadcast takes place. Similarly there is an unknown server; that means I do not know the address of the server like ARP. Once again, you send the request to this bus and the bus will find out and finally give you the address. These are the main components of LANE, namely LEC, LECS, LES and bus. (Refer slide time: 46:00 - 46:51 min)

What does the LES do? The basic function of the LE server is to provide directory, multicast, and address resolution services to the LE layers in the work stations. That is what the LAN emulation server does. It also provides a connectionless data transfer service to the LE layers in the workstation if needed. The parameters for setting up a server, etc., will be known to the LECS, which will communicate to the LEC as the LEC joins the network. (Refer slide time: 46:52 - 47:44 min)

Initialization: The client gets the address of LECS from its switch, uses well-known LECS address, or well-known LECS PVC. There has to be a particular PVC, which it starts using automatically. The client gets server?s address from LECS. It also discovers its own ATM address if required for direct VCs. That means if it wants to do some direct communication between two nodes, it has to get the ATM address of the other side if it wants to set up a direct VC instead of going via the servers. In that case it will require its own ATM address also.  (Refer slide time: 47:45 - 48:00 min)


It does a registration; client sends a list of its MAC addresses to the server; declares whether it wants ARP requests. These have to be known to the server so that the server can give the service to the other nodes connected to that network. Client sends ARP request to server. (Refer slide time: 48:01 - 48:19 min)

Address  resolution: client sends ARP request to server; unresolved requests are sent to clients, bridges, servers and the ARP; client sets up a direct connection. This is how a connection is set up. (Refer slide time: 48:20 - 48:40 min)

Broadcast to unknown server or bus: as I said,  it forwards multicast traffic to all members. Clients can also send unicast frames for unknown addresses there. Suppose some address is not known, you send it to bus and then bus will try to find out. (Refer slide time: 48:41 - 49:54 min)

There is a flush protocol. That means clients can send unicast packets via bus while trying to resolve the address. What might happen is that client may try to get the address, then get some address, maybe send it directly. What might happen is that something which was not sent directly here, the packets may come out of order. Remember I mentioned that in ATM, one guarantee is that cells will not come out of order. Cells may get lost, but cells will not come out of order, unlike pure datagram services. But in this particular case this may happen because we are not talking about one particular VC. In one particular virtual circuit, the packets will indeed not go out of order but in this particular case, they might. When direct VCC is set up, client sends a flush message to the destination, destination returns it to source, can then send packets on VC. This is a flush message that this problem is solved. (Refer slide time: 49:55 ? 50:38 min)

There is another approach to this, which does not use LAN but uses classical IP over ATM. What is classical IP of our ATM? The definitions for implementations of classical IP over ATM are described in RFC 1577. All the details are here; once again, we will just simply mention it very quickly. This RFC considers only the application of ATM as a direct replacement of the wires; local LAN segments connecting IP end stations which are the members; and routers operating in the classical LAN based paradigm; issues raised by MAC level bridging and LAN emulation are not covered. (Refer slide time: 50:39 -51:20 min)

If you want to look at classical IP over ATM, you have to do address resolution and encapsulation. These are the two issues to be considered here. Encapsulation consists of putting appropriate ATM header trailer to a packet, converting it to a number of cells and then sending them. This is encapsulation, which means that you have an ATM packet. This is classical IP running over ATM when you have a big packet. You know that cells have to break it up, and put a proper header on each one and then send them. ATM features are not utilized and inter network traffic handling is clunky. (Refer slide time: 51:21 - 52:21 min)

 Each of the IP sub-networks is a logical IP sub-network. All members of a logical IP sub-network are able to communicate via ATM with all other members in the same LIS, which means that if two nodes are in the same logical IP sub-network, i.e., same LIS, you would set up a VC on. There is a VC between every pair within or amongst all nodes in a particular logical IP sub-network. There is a VC mesh; everybody can communicate to everybody else. Communication to hosts outside the LIS, local LIS, is provided via an IP router. This router is an ATM endpoint attached to the ATM network that is configured as a member of one or more LIS. (Refer slide time: 52:22 - 52:49 min)

Naturally, a router may be a member of more than one network. Similarly, the network is configured as a member of one or more LIS. You have to do an address resolution. The valid question is that, if the IP address is this, what is the ATM address? You have to do an ATM ARP: IP address to ATM address translation, address resolution protocol is used. Inverse ATM ARP means VC to IP address; solution: use ATM ARP servers. There are ATM ARP servers there. (Refer slide time: 52:50 - 53:49 min)

This is a diagram, suppose this is a logical IP sub-network number 1 and this is logical IP sub network number 2, each of them has its own ATM ARP server for doing ARP, that means, IP to ATM address translation and vice versa. Nodes are connected; if A1 wants to communicate to B2, naturally at the top level, you give the IP address. Previously we were breaking it up into data link address. Finally what is the route to take if it is in some other list? If it is in the same list, you have a direct VC to it and you have a direct virtual circuit; you take that. Each LIS has an ATM ARP server for resolution; clients are configured with the server?s ATM address and clients register at start up periodically. (Refer slide time: 53:50 - 55:08 min)

In ATM, ARP protocol is used to resolve a host IP address for a known hardware address. It is the inverse ATM ARP. As you can understand, this ATM is a rather complex technology and this is one of the reasons it was not so successful. As you will find later on, many of the ideas which were used in ATM, namely, about this quality of service, about setting up flows and setting up virtual circuits with VCI VPI labels, these ideas were later on adopted in IP and we will discuss it a lecture called MPLS, that is, multi protocol label switching, where the similar ideas have been used to give this kind of services later on. Also, ATM is used quite extensively in big backbone networks because of the various facilities. Although it has moved out of the LAN segment, these days the gigabit Ethernet has replaced it simply because of the host. Thank you.


      Preview of the next lecture (Refer slide time:55:11-55:12)


So today we will start our discussion on routing. Actually we have already talked about routing a little bit in different context specifically in the context of ATM. How the ATM virtual path are set up. Today, we will talk about the major area. We will start our discussion on the major area  of routing which is how and specially with reference to the TCP IP stack. That is how packets are routed in a IP network (Refer slide time: 55:54 -56:04)

we will talk about. Today, we start the introduction and to routing, we will take up the discussion about different routing protocols in next set of lectures (Refer slide time: 56:09 - 56:48)

Let us just recollect what the job of the network layer or what is routing. This is to carry data end to end, i.e. from source to destination perhaps through a number of intermediate subnets depending on whether connection oriented or connectionless services are used. Other functionalities may be incorporated at this layer, we will talk about this later on.  We are talking about IP routing of IP packets and how you can have a virtually connection oriented system on that. We will discuss it later on, the point is unlike data link layer remembering the next hop just the link which is immediately adjacent. That has some advantages in the sense that whatever information you require about it are locally available here. Routing is the major problem, we are talking about routing over multiple networks and towards a very remote system. The packet might have to take many hops maybe 10, 20 even 30 hops to reach the end point and when you take naturally. Let say 20 hops, the area you are sort of serving becomes so large with so many machines connected to it. How to  keep track and naturally switch so many machines with so many links is the problem. Some of the link may go down some of the machines,  may come up and when I say machines it may refer to actual either PCs servers etc. They may also refer to the network boxes like other routers (Refer slide time:58:03-58:29)

Calculate the check sum which is for your connection. As we know can transmit to the next hop and send an ICMP packet if necessary. ICMP is for internet control message protocol if the routers may use ICMP packets for sort of talking to each other and sending various messages if necessary. We will see 1 example, 
COMPUTER NETWORKS
Prof. Sujoy Ghosh
Department of Computer Science and Engineering
IIT, Kharagpur
Lecture-26
Introduction to Routing (Refer slide time: 00:39)

Today we will start our discussion on routing. We have already talked about routing a little bit in different context, specifically in the context of ATM of how the ATM virtual path etc are set up. Today we will start our discussion on the major area of routing and specially with reference to the TCP/IP stack. That is how packets are routed in an IP network. (Refer slide time: 01:22 - 01:33 min)

Today we will start the introduction with routing and then we will take up the discussion about different routing protocols in the next set of lectures. (Refer slide time: 01:34 - 03:46 min)

Let us just recollect, what is the job of the network layer or what is routing? The job of the network layer is to carry data from end-to-end. That is, from the source to destination perhaps through a number of intermediate subnets. Now depending on whether connection-oriented or connectionless services are used other functionalities may be incorporated at this layer. We will talk about this later on. Right now we are talking about routing of IP packets and later on we will discuss on how you can have a virtually connection oriented system on that. unlike data link layer, if you remember, is just the next hop, just the link which is immediately adjacent. So that has some advantages in the sense that whatever information you require about it are locally available. Unlike the data link layer, the major problem with routing is that it happens over multiple networks and towards a very remote system and the packet might have to take many hops, may be 10, 20 or even 30 hops to reach the end point. When you take 20 hops the area you are serving becomes so large with so many machines connected to it. And then how do you keep track and then switch so many machines with so many links? The problem is some of the links may go down, some of the machines may go down, some of the machines may come up And when I may refer it actually says PCs, servers, etc, they may also refer to network boxes like other routers.  So the job of the router is to know which link it should take so that globally the packet will arrive nearer to its destination.  (Refer slide time: 03:47 - 04:12 min)


This is the basic routing problem. At a particular subnet node given a packet with a particular final destination determine the next subnet node or the outgoing link which is appropriate. In the datagram network this is determined for individual packets but in VC networks this is determined only for setup packet for each session. Now we will concentrate on the datagram network.  (Refer slide time: 04:13 - 08:20 min)


Consider a router X. X may not know the topology of the entire internetwork. These days everybody wants to be connected to the internet so we have a giant network of networks. So there is this big network of networks spanning the entire globe and then a particular network once again may be divided into so many sub networks. There are billions of machines connected to this network so some user somewhere wants to connect to another user on the other part of the globe. This is a huge problem that must be solved in a systematic manner. But this is the objective of routing or the network layer. So X needs to determine the next?hop router for every other network in the internet. We are trying to reach some particular machine, some particular server. If you want to keep track of all the machines in the world it becomes really impractical. We somewhat reduce the problem where all these different machines are grouped into different networks or even sub networks. They are grouped into different networks so that a remote router needs to keep track of only the remote network rather than a particular server in that network. Of course, the idea is that, once you reach your destination network finding a particular server within that network will be very easy. Either through ARP or some such protocol you can reach the particular server once you have reached the correct destination network. The entire information that is required is structured as a routing table of router X to keep track of all the other networks. What is a routing table? Given the address of a particular network in a remote location we should be able to find from the routing table that which local hop I must take next. Even that is not easy even if you reduce the scope of the problem from servers or PCs to networks because there are millions of networks in the world. So theoretically if you do it in a very naive fashion you have to keep millions of entries so that you can match and know that this is what the mistake is. Of course, even this is not possible because at the same time you want to go to the correct destination for this particular packet and at the same time you want to process the packets as fast as possible. People want more and more speed so the network traffic and the number of packets are increasing day by day so you have to process the packets very fast. If you have a very large table then it demands time even just to look up to it and you require a lot of memory. Additionally it consumes lot of processing power which will thereby drive up the cost of the router. So, even keeping millions of entries for most routers this is not a feasible option therefore we have to do something about it.  (Refer slide time: 08:21 - 10:04 min).


The other issues in routing are: One is, the topology changes affect convergence, delay and stability. Topology of the network changes all the time because the links may go down or come up, nodes may go down or come up so the topology is changing. Therefore this may the change the path that a particular packet takes and the path the next packet takes in the same stream.  This may affect the delay and the stability. The other problem is scalability to a large number of interconnected networks or routers or links. And even when you come down from nodes to networks this is still a very large problem. Then there are other issues like what is the best path from X to Y. It may be that five different routes are possible from X to Y. Now all these five routes are not equal. First of all they may not be equal in the number of hops, they may not be equal in length, they may not be equal in cost, they may not be equal in the quality, some of the routes may be very unreliable that the packet has a larger probability of getting dropped and so on and so forth. So we would like to have the minimum number of hops or the minimum delay or the maximum capacity.  Therefore if possible we would also like to incorporate the quality of the path when deciding on a route for a particular packet from X to Y.  (Refer slide time:10:05- 11:30 min).


So routing consists of deciding the route for each packet. And in order to do that the router has to have knowledge of the entire network and this knowledge has to be dynamic because the network topology keeps on changing so we have to update the knowledge of the network from time to time. Suppose you have a host or LANs connected to some routers say A, B, C, D, E and then through the links 1, 2, 3, 4, 5 and 6 respectively then for example when the router A gets a packet from a local network which is connected to it which may be destined for the LAN which is connected to router C. So it has to decide whether to take link 1 or link 3. Obviously it does not take this link because this is where it is coming from and obviously it is destined to some other sort of remote so it has to decide. Although in this particular example you may theoretically reach C both by taking link 1 and link 3 but then these links may be of different quality, May be something like A, D, B,C, E is a bit of choice. (Refer slide time: 11:31- 12:12 min)


As mentioned earlier these are the routing tables. We have the routing tables in routers. They look somewhat like this. For example, let us look at the routing from A and this may be just one form of it. It may not be exactly used in this form but let us look at it. Suppose you want to route to the router A which is again local because we are sitting on router A, for B you take link 1, for router C link 1, for router D link 3 and for router E link 1. (Refer slide time:12:13- 12:24 min).

From A to B is link 1, may be C is also link 1 and D is link 3 and E is again link 1 and so on.  .  (Refer slide time:12:25 - 12:59).


Similarly, B will have a routing table and C will have a routing table and so on.  Suppose you take any destination A, (Refer slide time: 13:00 - 15:41).

now for A let us consider a tree with a root at A. Now B connects to A through link 1 and C connects to A through link 2.  Obviously link 2 is not taking C directly to A but link 2 is only taking C to B but that is the preferred path for connection from C to A.  If you think about it this way, for each destination we have got some kind of a tree if the network was very bigger. Here the network is very small so the tree is only two levels deep. But the tree could be very deep may be 10 or 15 levels deep. So for each destination we actually have a tree which is implicit in these routing tables which are distributed. At each node we try to see the local link to be taken for that particular destination. As I said, the local link may not take you directly to the ultimate destination. But the local link will take you somewhere and that particular node will again have a link for the same destination. This way, if everything is working fine, after some hops you will reach the final destination. This means that some of the other nodes may be directly connected, some may be 2 hops away, some may be 3 hops away etc. But overall there is an implicit tree for each destination. And for one movable link of this tree is in one particular link of this routing table just according to the destination. In other words, if there are n nodes and there are n implicit trees and these n implicit trees are distributed over n nodes. So it is one link from each tree in the routing table of a particular node. In that way you have a routing table of size n including the root which is the local link.  And somehow you need to maintain this tree and that is the job of a routing protocol. Later on we will see different routing protocols and also see how these implicit trees may be maintained or rather how these routing tables have to be maintained. (Refer slide time: 15:42- 16:44 min).


When there is a routing table there is a question of forwarding. Routing is the process of building routing tables at each router. Forwarding is the process of looking at the destination address of a packet and sending it to appropriate next hop interface of a router. This means, once through a routing protocol you have a routing table then a packet actually arrives and you have to forward it to the correct interface of that router. So, you look up to this table and the interface to which the packet must be sent and forward it. So this routing or forwarding are two different and distinct parts of the router. Actually sometimes these two are diverged even more. For the time being we assume that routing and forwarding happen at the same place and the routing table itself is being used for forwarding although that may not always be the case. Forwarding requires access to local routing table. Sometimes forwarding table is structured in a different manner than routing tables.  (Refer slide time: 16:45 - 18:05 min).


So forwarding table is optimized for packet look ups. Routing table is optimized for routing changes, topology changes etc. A routing table may look like this: say, for net number 10 the next hop would be this IP number because the routing table is working in the IP address. This IP address is version four addresses and it contains four numbers all less than 256 so it is 171.69.245.10. The link cost may be there. In forwarding, we really do not care what the IP address or the next hop is. We do not even care about the link cost because all these are a part of setting up the routing table. In the forwarding table we just wanted to know the interface as to what is the local interface to which this packet has to be sent and what is the MAC address at the next hop. This is what we are more interested in. Basically you can just add your data-link headers and just send it over to the physical layer. This is a slightly higher level and the abstract view of processing an IP datagram.  (Refer slide time:18:06- 21:09 min).


In this IP module there is a routing table which is the central thing. There is a routing protocol which makes this table. Sometimes we also use static routing, for the time being let us consider that it has been manually configured.  Some of the entries may have come through a routing protocol while some of the entries may have been manually configured. Now, when a packet comes from an upper layer it will come from some transmission layer protocol. Two of the most common transmission layer protocols are TCP and UDP. We will see what they are later on. But let us assume that some packet has come through UDP with some destination IP address in it. You look up at the routing table, The next hop and send the datagram. Or the same thing must have come through TCP also.  When we are discussing the processing of the IP datagram in the IP layer the IP layer is present in two different places. In one place it is the regular host the PC and in the other place it is a router. The jobs of the two places are a little different. If a packet comes from outside to a PC, and is not meant for the PC, the PC is simply going to drop that packet. In the case of a router, if a packet comes from outside, it looks up the destination and then actually forward it. So in a PC the forwarding table may be disabled whereas in a router the forwarding table will be enabled. If the packet originates from machine PC itself then it has to go out on its way. It will send the packet to the router. The IP datagram it is sent to the network layer. This is a high level view of what is happening in the IP datagram processing. (Refer slide time: 21:10 - 21:21 min)


The processing of IP datagrams is very similar on an IP router and a host.  The main difference is IP forwarding is enabled on router and disabled on host.  (Refer slide time: 21:22 - 21:45 min)


Now when the IP forwarding is enabled, if a datagram is received but it is not for the local system, the datagram will be sent to a different system. When IP forwarding is disabled, if a datagram is received it is not for the local system and the datagram will usually be ignored.  (Refer slide time: 21:46 - 22:03 min)


The view at the data link layer is somewhat different. Internetwork is a collection of LANs or point-to-point links or switched networks that are connected by routers. So this is the data link layer view of the IP datagram.  (Refer slide time: 22:04 - 23:02 min)


In this diagram there may be some point-to-point links or some LANs, etc. and they are all connected by some routers R1, R2, R3, R4 etc. A particular host places an IP datagram on the local ethernet that is destined for outside it will eventually reach the local router and the router will decide whether to give it to another network such as the network of Ethernet switches, token ring etc. Through different networks the packet proceeds through the routers to this. When you look at it from a data link layer point of view all these switches become visible whereas at the IP layer only the routers and the networks will be of main concern. (Refer slide time: 23:03 -23:26min)




A view att the IP layer: An IP network is a logical entity with a network number.  We represent an IP network as a cloud. The IP delivery service takes the view of clouds and ignores the data link layer view. That means the details of these actual switches etc are in the data link layer view whereas in the IP layer view it will simply be a cloud.  (Refer slide time: 23:27 - 23:26 min)



In this picture there are routers R1, R2, R3, R4, etc and the connecting networks are shown as clouds.  Each network has some number. (Refer slide time: 23:47 -27:13min)


The following conditions must hold so that an IP datagram can be successfully delivered. The network prefix of an IP destination address must correspond to a unique data link layer network which is equal to LAN or point-to-point link or switched network. The reverse need not be true. This is quite fundamental. You have already seen examples of IP addresses. They are basically four numbers each less than 255 that is something like 144.16.192.53. This may be the IP address of a particular machine. In the network layer it is not possible to handle all the machines individually because that will make the problem really big. So as our first step in our reduction or scaling it, actually in the remote routers we do not usually keep track of the specific IP addresses of specific machines. We just keep track of how to go to that network which contains this IP address. So the IP address usually has two parts: the leading part that is the first few bits or bytes is a address of the network whereas the last few bits or bytes may be reserved for a particular machine within that network. This is how a global IP addressing scheme is. This is not as neat as in telephone numbering which exactly tells you the particular state, area, LECA and the particular exchange finally so it is not that neat. But at least the first few bits or bytes will be associated with all the IP addresses in a particular network. If you just take that prefix part you know that it precisely means that particular network and no other network will have the same exact prefix. The network addresses have to be globally unique because now-a-days our network is really span the entire globe. Therefore this is called as the  network prefix of an IP destination address and this must correspond to a unique data link layer network. But the reverse need not be true which means one data link layer network may have two different network prefixes.  (Refer slide time: 27:14 - 27:37 min)


So routers and hosts that have a common network prefix must be able to exchange IP datagrams using a data link layer protocol such as Ethernet, PPP etc. Every data link layer network must be connected to at least one other data link layer network via a router.  (Refer slide time: 27:38 -28:08 min)



Each router and host keeps a routing table which tells the router how to process an outgoing packet. The main columns as we have already seen are (1) the destination addresses, i.e. where is the IP data gram going to, (2) next hop, or how to send the IP datagram and (3) interface or what is the output port. Next hop and interface columns can often be summarized as one column and routing tables are set so that the datagram gets closer to it is destination.  (Refer slide time: 28:09 -29:39 min)






This is another example of a routing table. IP datagrams can be directly delivered which means these are in the local network. Therefore they go through the interface called Ethernet zero whereas these addresses are outside so this one had to go through a router or to a point-to-point link. Or this can be some other network altogether. Now the prefix has distinctly changed. For example, 10.1.0.0/24 means any number from 0 to 24 can be there. So, for all these IP addresses, you just deliver the datagrams directly. Similarly 10.1.0.2 is also connected directly which means they are all in the same network. And then there is a point-to-point connection through a serial link on this router which is also may be very closely connected to the same set of networks. There may be other networks out in the WAN which really starts from 20. So, for those networks you again have to go to the router and go out through some port of that router.  (Refer slide time: 29:40 -30:22 min)



If you take a more global view these different routers will be having routing tables. We know the details of these routing tables and what they contain and this is how one particular IP packet which originates here will go to a router then to a next router and then to another router and so on.  (Refer slide time: 30:23 -33:41 min)



Processing of an IP datagram at the router: 
First we have to receive an IP datagram then IP header validation. The IP protocol is a network layer protocol. Previously we discussed about a lot of data link layer protocols but now we are talking about IP protocols. And for these IP protocols once again you require some information to be exchanged between peers. This information will be in the header called the IP header because in the network layer we are mainly concerned with IP so it is the IP header. In any case the IP header has to be validated. If there are some options in the IP header they have to be processed. The destination IP address is parsed from this header. Then we do a routing table look up and we decrement TTL i.e. we decrement time to leave. Here is a common example. In this distributed fashion we are trying to capture a good and consistent and correct picture of the entire global connectivity which gives you the best connection. But in practice this may not always be possible because the global picture may change from time to time. So, to get the entire global picture in a very correct fashion is not possible sometimes. You may have inconsistent routing table entries and that may lead to various things such that it may lead to a loop in the routing table. The loop may not be in one routing table but if you take several routing tables together then you can see that the packet will go in a loop. Once a packet starts going in a loop it will continue in that loop because each time it comes to the router the router will see its own routing table locally and send it to the next one which was hopefully in a correct path. But actually now it is in the vicious ring so this packet will just go on circulating ad infinitum. To stop this we put some kind of restriction on the number of hops a packet will take may be 30. Each time an intermediate router forwards a packet it will decrement this time to leave that is from 30 to 29 and from 29 to 28 and so on.  After 30 hops whichever router finds a packet with a TTL zero will simply drop the packet either that packet is going in a loop or the packets have gone very astray because of mistaken entries in the routing table and so on.  (Refer slide time: 33:42 -35:18 min)



Then perform fragmentation if necessary. We will see the details of fragmentation later on. Fragmentation here is, what you are doing is that you are going from one network to another network to another network and so on.  All these different networks are on different administrative controls and different domains and they may even have different data link layer technologies. Some token ring may be connected to some Ethernet and there can be all different kinds of networks in between. Now suppose the source had sent an IP packet which was quite large but inside one particular network it is not possible to handle such a large packet.  It would be unfortunate if you drop the packet all the time because it will never go through. Therefore what is done is that this big packet is broken up into small fragments and the fragments are sent. Later on when you are in a sort of wider area the fragments are again reassembled into a big packet and sent along when it reaches the destination. Then you calculate the check sum which is of error in connection as we note and transmit to the next hop and send an ICMP packet if necessary. ICMP stands for Internet Control Message Protocol. The routers may use ICMP packets for communicating between each other and sending various messages if necessary. We will see one example list now and the rest later.  (Refer slide time: 35:19 -35:41 min)




When a router or host needs to transmit an IP datagram it performs a routing table lookup. So, use the IP destination address as a key to search the routing table. The result of the lookup is the IP address of the next hop of the router and/or the name of the network interface.  (Refer slide time: 35:42 - 37:09 min)


So we have seen that. Therefore either you take the network prefix or host IP address or loopback address or the default route.  Loopback address means this is meant for local consumption so it will go back to the same machine. So it is coming down from a machine and it contains a loopback address then it goes back to the same machine. Why would somebody want to send the packet like this? One process of the packet of the host is sending some packet to another process in the same host and it is using this network operating system part for sending that message.  Default route is very important because you cannot keep the network prefix for all possible networks in the world in this table then this table will then become very large. So you will have fewer entries for the network prefixes and if your network is something else may be there is a bigger router somewhere that knows about all these networks. So there is a router which is likely to know about this particular network prefix which has come so you send it to a default route. And on this side you have the IP address or the name of the network interface.  (Refer slide time: 37:10 - 37:26 min)


So the destination address is a network address, most entries are network routes. For the host route the destination address is an interface address which is used to specify a separate route for certain hosts.  (Refer slide time: 37:27 - 38:09 min)

The default route is used when no network or host route matches. The router that is listed as the next hop of the default route is the default gateway. We are calling it gateway because this is not a packet for a network which is close by. This is a packet for some arbitrary distant destination, this is a smaller router that is connected which will send it to the gateway and the gateway will in turn send it to a bigger router to find its final destination.  (Refer slide time: 38:10 - 38:27 min)


Loopback address:
Routing table for the loopback address is the particular loopback address which is used in IP which is 127.0.0.1 and this is meant for local consumption. The next hop lists and loopback interface as outgoing interface.  (Refer slide time: 38:28 - 40:04 min)


To minimize the size of the routing table we use the longest prefix match, i.e. we search for the routing table entry that has the longest match with the prefix of the destination IP address. It means search for a match on all 32 bits. The IP address contains four integers less than 256 i.e. four bytes which is actually 32 bits. So an IP address in IPv4 address is 32 bit long. So first you try to match all the 32 bits with some entry in the table. If the 32 bit does not match then you take only 31 bits and check whether they match and you keep on doing it and keep on reducing till you get a match so you have identified the first entry that matches with the longest prefix where such a match is possible. The host route loopback entry is a 32 bit prefix match. The default route is represented as all zeros which is a zero bit prefix match. That means there is a zero entry which will give you the next hop as the gateway because now it has not matched with anything. Finally this is a zero bit prefix match. (Refer slide time: 40:05 - 41:14 min)


Suppose the destination address that has come in is 128.143.71.21 then of course the 128 and 143 parts have matched with this but then here this is zero and this is 71 so this is a much better match here with the one shown in red. Similarly 128.143.71 is also matching here but the next number is 21 which will match better with this rather than with this range. This is where the match will take place and you will send to router R4. The default router at the gateway is shown as R5. So the longest prefix match for this is for 24 bits with entry. You can find this out if you actually break up 21 into its binary form and see how many bits are matching but this is matching with the 24 bits etc, so data gram will be sent to R4.  (Refer slide time: 41:15 - 41:32 min)


The longest prefix match algorithm permits to aggregate prefixes with identical next hop address to a single entry. This contributes to significantly reducing the size of the routing tables for internet routers.  (Refer slide time: 41:33 - 42:31 min)


Suppose for 20.2.0.0 to 16 I would have gone to router R2 and for 30.1.1.0 to 28 I would have gone to R2 once again. Now we see that the next hop is the same and what we can do is, instead of 20 and 30 if we make the entry as 20.0.0.0/8 and put it as R2 then because of longest prefix match then whenever something also comes in this range it will have a longest prefix match with this rather than with this because this is taken over here and 30 is naturally closer to 20. So instead of two entries we keep only one entry in the routing table which helps in reducing the size of the routing table.  (Refer slide time: 42:32 - 42:56 min)


 How do routing tables get updated? One way is to add an interface and then configure the same so it adds a routing table entry. This is manual configuration.  We can also add a default gateway that means for the destination that is the default route we can add a gateway. This is once again a manual updating.  (Refer slide time: 42:57 - 43:32 min)


So it is the static configuration of network routes or host routes. If some particular route is forced then I might put in a static configuration of what they are. Routing tables also get updated through routing protocols. There may be ICMP messages from some router which may update the routing tables. So these are the different ways in which a routing table may get updated.  (Refer slide time: 43:33 - 44:31 min)


For example, for this ICMP when a router detects that an IP datagram should have gone to a different router the router (here R2) forwards the IP datagram to the correct router and sends an ICMP redirect message to the host. The host uses the ICMP message to update its routing table. What is happening is that one particular host had sent the IP datagram to one router.  Now this router sees that it need not get it from this host and that host should have sent it to that router. This router will now send it to the that router anyway and send an ICMP message to this host saying that from next time onwards when you have got this destination please send it to that particular router rather than sending it to this destination. This is one example of how ICMP may be used. These are the different kinds of ICMP messages.  (Refer slide time: 44:32 - 45:46 min)


 The other thing is, ICMP router solicitation and ICMP router advertisement. When a router is switched on for the first time how will other routers know that this router has come up? Since the other routers do not know about the existence of the new router which has come up it is was not sending any message to it. So, after bootstrapping a router broadcasts an ICMP router solicitation. It sends an ICMP and advertises itself and solicits ICMP messages from the neighboring routers. In response the router sends an ICMP router advertisement message. Also, routers periodically broadcast ICMP router advertisement. This is sometimes called Router Discovery Protocol. This has to be done periodically because some router may have gone dead in the meanwhile. Therefore by doing things periodically you try to keep it as current as possible.  (Refer slide time: 45:47 - 46:28 min)


We can look at routing as some kind of a graph theory problem where (a) the nodes are the routers of a single administrative domain or different networks, (b) the edges are interconnection links, (c) link costs are related to physical distance, capacity, delay, etc and (d) the objective is to determine minimum cost path. You can formulate it as a graph theory problem and this particular graph theory problem can be handled in different ways. We will see two different ways later on.  (Refer slide time: 46:29 - 46:41 min)


The problem has some constraints, one is to solve the minimum cost path problem in a distributed manner rather than centralized manner and constraint two is to react quickly and robustly to topology changes.  (Refer slide time: 46:42 - 47:26 min)


There are routing protocol requirements. One is to minimize routing table space, i.e. with all these millions of networks working at the same time minimizing the routing table space is always very important. This makes the routers smaller or cheaper or faster, minimizing or controlling messages is also important. Routers should be robust and not misroute packets.  Loops and oscillations must also be avoided. Finally optimal paths must be used. All these are different routing requirements. It is not that we can get 100% of the requirements all the time but we try to do it as best as we can.  (Refer slide time: 47:27 - 48:40 min) 


Now we will quickly go through the different approaches to routing. One is the centralized versus distributed approach. In centralized routing one central processor collects information about the status of each link, computes the routing table for each node and distributes it. This is possible only in a small number of cases and not all the time. And obviously it is not possible over the entire internet because there is no such centralized routing that would handle the scale. In distributed routing, routers cooperate to run a distributed protocol to create mutually consistent routing tables. In distributed routing also there may be two approaches. One is that you distribute the local information only and then globally try to come to a solution. The other is that you distribute the information globally and then locally you simply route some kind of a centralized algorithm to know because you have now got the global picture in each of the places.  (Refer slide time: 48:41 - 49:53 min)


Routing may be source based or versus hop by hop. In source based routing the packet header contains the entire route. If a link or a router along the path goes down a source routed packet will not reach the destination because the route has been fixed by the source. The intermediate routers do not do anything. If the next hop is available it will send the packet otherwise it will drop it. In hop by hop routing the packet contains only the destination address and each router will consult its own routing table and find out what is the next hop and then choose that next hop. But in the source routing the route is fixed from the beginning. Loose source route is something in between, it is an intermediate solution. In loose source route what is done is that instead of specifying the entire path you specify some sort of islands in between. That means you go from one router to the next through several hops then again several hops and so on. So this is something intermediate between strict source routing and pure hop by hop routing.  (Refer slide time: 49:54 - 52:47 min)

Routing may be stochastic or deterministic. In stochastic routing each router maintains more than one next hop for each possible destination. One of these is randomly chosen. So the idea is to distribute the load evenly along the links. On the other hand packets may get out of order because of this. It is because the packets from the same source travels to the same destination and the first packet may be stochastically chosen to go through this path and another may be stochastically chosen to go through this path.  The probability of choosing either this path or that path can be based on some metric like the delay. But in the end the packets may reach out of order.  Please remember, the service the network layer is providing is just to send the packet from one end to the other part of the network. Here this part has been said explicitly but it is also important to understand what has not been said. It has not been said that this service is going to be very reliable which means that in the interim some router may drop a packet so your packet may not reach the destination at all. So reliability is not guaranteed. Another thing that has not been said is that all the packets you sent from destination A to B will reach the other end in the same order in which they were sent. The first packet may reach later than the second packet because may be it came through two different paths or may be due to some other reason. Once again there is no explicit guarantee regarding the ordering of the packets. Obviously towards the end application this may not work at all in many cases. So in such cases where this is very important you have to take precaution against this or you have to put in some corrections for this at some other layer and the network layer is not doing this. This was the idea for breaking it up into layers in the first place. In the physical layer it is the physical sending and in the data link layer sending is from one to one hop only and makes it as reliable as possible through checksum etc. In the network layer it is just reaching the other end of the network layer through several hops. Now if you want to do it reliably you have to go up one level more and then try to do something there. We will see how it is done.  (Refer slide time: 52:48 - 53:16 min)

Single versus multiple paths: Each router maintains one primary and some alternate paths. Single path routing is used in internet to reduce routing table size. Multiple paths such as stochastic etc are not usually used in these routers in the internet because the routing table size is at a premium. Multiple paths are used by telephone networks as routes can easily be deciphered from the address such as telephone numbers.  (Refer slide time: 53:17 - 53:33 min)



Next is state dependent versus state independent routing. State independent or static routing pre-computes the routes ignoring the network state and state dependent or dynamic routing uses the current measured network state (like loading or health of a link) to determine the current route which may change as the packet is proceeding. It requires more overhead but can usually find better routes.  (Refer slide time: 53:34 - 53:42 min)



 In static routing, the next state entry does not change in response to changes in network traffic or topology. In dynamic routing it does change.  (Refer slide time: 53:43 - 54:08 min) 



Routing in the telephone exchange of course is very simple as we have already seen. Under the same exchange there is no routing and under the same Short Distance Charging Area (SDCA) a central switch sets up the connection with the destination exchange. For trunk calls the central switch forwards the setup request to the trunk exchange (TAX) and maintains a primary and alternate path to a Long Distance Charging Area (LDCA). (Refer slide time: 54:09 - 54:20 min)


The possible goals of routing algorithm may be to minimize average end-to-end packet delay (which is desirable from the viewpoint of network user), to maximize throughput (which is desirable from viewpoint of network operator) and to minimize average number of hops (which tends to give both low delay and high throughput.  (Refer slide time: 54:21 - 55:49 min)



Another way we do routing route is by flooding. This is some kind of broadcast, so if a router wants to flood something it will send the same message to all the routers which are connected. That may be a very nice and fast way of reaching somewhere because when you are flooding very soon this message will get replicated at each node and everything is running perfectly and in a synchronous manner it will reach the destination using the shortest possible route. The only problem is that not one copy will reach but multiple copies will reach using different paths. And then other copies will never get anywhere but they will sort of choke other parts of the networks so these overheads are there. Still flooding is used in some special cases. This is one reason we use flooding and the other reason is that when we actually want to broadcast this to everybody then one good thing to do is to flood it. Every incoming packet is sent out through every outgoing line except the one it arrived on. A hop count or keeping track of previously flooded packets may be used to avoid generating infinite number of packets. Flooding gives the shortest route and is very robust but hardly practical otherwise. But in many situations they are also practical.  (Refer slide time: 55:50  - 56:06 min) 

Flow based routing is a static algorithm that uses both topology and load for routing. The traffic matrix and the line capacity matrix and a routing algorithm is assumed to be given. The mean delay time for the entire network is calculated from this. Different routes from different algorithms (or all possible routes) can be evaluated.  (Refer slide time: 56:07  - 56:20 min)


We will come to that later on when we do MPLS. Given a particular set of routing entries, the net average traffic in each link is calculated. So you can try to do some traffic optimization through this.  (Refer slide time: 56:21 - 56:33 min)

Then we have multi path routing.  At the router, for a given packet with particular final destination several choices for next router are enumerated, and then the actual path is chosen in some fashion.  (Refer slide time: 56:34 - 56:42 min)

Multi path routing may yield more stable traffic.  (Refer slide time: 56:43 - 56:46 min)

Alternative routes are also similarly determined.  (Refer slide time: 56:47 - 56:53 min)

We have already talked about dynamic routing versus centralized routing.  (Refer slide time: 56:54 - 56:58 min)

 It has its own disadvantages meaning it lacks some fault tolerance if routing computer goes down.  (Refer slide time: 56:59 - 57:23 min)

Distributed routing is the most usually used. It may use some distributed algorithm like distributed Bellman-Ford or it may use some centralized algorithm with distributed global data. We will look at distributed routing in more detail in the next couple of lectures.   



COMPUTER NETWORKS
Prof. Sujoy Ghosh
Department of Computer Science and Engineering
IIT, Kharagpur
Lecture-27
RIP- Distance Vector Routing

We have seen basic routing. Now we will go to some important and specific routing protocols. The first routing protocol we will be discussing today is Distance Vector Routing or RIP routing information protocol. (Refer time slide from 01:08 to 01:25min)

 RIP is one of the oldest routing protocol actually used in the internet and is still used in many places which is based on distance vector. (Refer time slide from 01:25 to 03:40min)


The distance vector of node X is the minimum distance from X to every other node in the network. For example, the distance vector for A in the following graph is 1, 2, 2, 1, 1 that means for B it is the distance one, for C it is 2, for D it is the distance 2, for E and F it is the distance 1. So from A these are the distance vectors. If you recall from the last lecture, the routing problem can be reduced to a graph theoretic problem so these nodes are the routers but there could be some hosts also. But for our purpose of RIP we will consider these to be routers.  When we are not talking about delay, congestion, link, capacity etc in the simplest case this is the shortest path problem. So the distance here is just the number of hops. These links are also not weighted so we just count the number of links to reach from the source to destination and that is given as the distance vector for the destination node. From the source node we will use these distance vectors to compute the shortest path. We have to run a standard shortest path algorithm and here in this particular case we will be using Bellman Ford algorithm. We have to run them in a distributed fashion. This Bellman Ford algorithm should be running at each node. (Refer time slide from 03:40 to 04:25min)

As per the Bellman Ford algorithm given directed graph with length djk which means this is the distance vector from j to k assigned to each directed link j?k. Assume every cycle has positive total length. This of course is trivial, if all the links are positive and since we are taking the link weight to be 1 this will always be valid. If (j,k) is not included then include it with length d_jk set = ?. The goal is to determine the shortest path from each node to node 1 which is the destination. (Refer time slide from 04:24 to 04:27 min)


So this is the graph and (Refer time slide from 04:27 to 04:51min)



what is this ?? Well, the point is, when you are starting the algorithm j may not know that it is actually connected to k through multiple hops. So to start with j will just set it equal to infinity. (Refer time slide from 04:51 to 07:35 min)

 Let DIh be the length of shortest directed path with at most h arcs from node i to destination. DIh is generated by the iteration DIh+1 = minj {dij + Djh} and Di0 = ?. So you can put all of them as infinity or the direct links you can take note of them and in one iteration that will be taken care of. The D is telling us that this is the best path I know at the moment from myself to this particular node at this iteration, Now, in another iteration when these distance vectors are interchanged we may find that my neighbor knows something which is of a better cost.  So I can go to my neighbor using dij over all j and then djh. If there are n nodes I need to do this iteration where at most there will be n ? 1 arcs. This is the worst case in which all the nodes are connected in a chain.  So, after n ? 1 iterations I will get to know about a node which is at a distance n ? 1 from me because at each iteration this information will filter through and then reach me. This node might have thought that node is not connected at all so DI0 is ?. But after n ? 2 iterations its neighbor would know that it is connected to that with a distance of n ? 2 and then this information will reach this node and this will know that it can go to its neighbor in one hop and take n ? 2 hops to destination so it can reach that node in n ? 1 hops. Since every cycle has positive length no optimal path contains a cycle hence it has at most N ? 1 arcs where N is the total number of nodes. Thus it is at most N iterations including the first one. (Refer time slide from 07:35 to 09:26 min)

 What we do in this distance vector routing is that at regular intervals router j sends packet to each neighbor giving new estimation for djK the current distance to destination k. That means the router j is giving the djk value and not only djk value for all k so that is why this is not just one distance value but this is called a distance vector that is sent to all its neighbors. So the neighbors tell each other about the distances they know to a particular node k and update the shortest one in its table. If node j finds that SP for the node 1 from the neighbor is shorter than it own, it will update the new path. We do some local exchange of information between any router and its neighbors as the algorithm is of course running globally and in a distributed fashion. Router I updates its table via decentralized Bellman Ford iteration. So D(j,k) = minimum {dij + D(j,k)} which gives the minimum distance to a particular destination k. (Refer time slide from 09:27 to 10:55min)

For IP routing, a routing table has to be created and which has to be kept updated. There may be ICMP redirect messages from other routers make changes to this routing table. There is the routing daemon. What the routing daemon will do is that from time to time it will run and send packets to the neighboring routers or run some iterations of this Bellman Ford algorithm and keep the routing table up to date. So, from this routing table we get the IP output and calculate the next hop router and if it is source routing we get it otherwise we get it from the routing table. When a particular packet arrives it sees whether it is for the router and if yes of course it goes up, if it is no then it calculates the next hop. (Refer time slide 10:56 to 13:47min)

 Autonomous systems are not directly used in RIP but it is used in the other algorithm. This is very important for scaling the network. We know that whatever routing protocol we are following this has to scale to the global level for millions of routers which are connected to each other. The router may be connected to a network. The number of routers together may form an autonomous system. Here we are introducing another level of hierarchy. Autonomous system is a region of the internet that is administered by a single entity, Therefore this is not just one network but a collection of networks. Some examples of autonomous regions are service providers, MCI?s with its own backbone system or Regional Internet service providers. The regional internet service providers are giving the services to so many customers and each customer has their own router connecting to this backbone. Each of the customers will have his own network so it is a collection of networks in a way. But this is one autonomous system. The idea is to run the same kind of protocol inside this autonomous system. There are two questions here; one is that, what is the routing protocol you are running inside the autonomous system, and what are the routing protocols you are running between the autonomous systems. Thisis because finally your packet may go to a router to a network to the backbone of the same autonomous system still then it might have to hop across some autonomous systems and then finally enter into another autonomous system and go all the way down to a particular network and then to the intended host. So, autonomous systems introduce another level of hierarchy. Routing is done differently within an autonomous system that is the intra domain routing and between autonomous systems which is the interdomain routing. RIP is suitable for intradomain routing. For the time being we are talking about intradomain routing but will look at interdomain routing later on. (Refer time slide from 13:47 to 14:23 min)

The characteristics of distance vector routing: one thing is, it uses periodic updates. Updates of the routing tables are sent at the end of certain time period, a typically value is 90 seconds. This means every 1? minutes you send your updates which are the distance vectors to your neighbors. There may be triggered updates. If a metric changes on a link a router immediately sends out an update without waiting for the end of the outdated period because if some link has gone down or some thing has become really congested or some parameter has changed then at that point a router may not wait for the lapse of that 1? minutes but send immediate updates triggered by some kind of an event..(Refer time slide from 14:55 to 15:35min)

There may also be full routing table updates. Most distance vector routing protocol send their neighbors the entire routing table. Not only entries we change. In some cases of a triggered update you may send only that data which has changed and then from time to time you send the full which is your distance vector.

Route invalidation timers: Routing table entries are invalid if they are not refreshed. A typical value is to invalidate an entry if no update is received after 3 - 6 update periods.  What might happen is that instead of a link going down a router itself may go down and then that router is not sending anymore updates to anybody. But after 3 ? 6 update periods its neighbors find that they are not getting any thing then it will set that link to infinity. That route or any route through that particular router which has gone dead will become invalid and this information will slowly percolate through the entire network.  So  this (Refer time slide from 16:25 to 18:05 min)

Change is a problem in routing because this change has to traverse globally. Everybody finally have to know but it takes time. We have seen distance vector routing  and now we shall look at the actual protocol which is called RIP or Routing Information Protocol. This is a simple intradomain protocol. That means it is in the same autonomous system. Straightforward implementation of distance vector routing is the distributed Bellman Ford algorithm. Each router advertises its distance vector every 30 seconds or whenever its routing table changes to all its neighbors. RIP always uses 1 as link metric therefore negative cycles are not present. The maximum hop count is 15 with 16 = ? which means it is rather a low value for infinity and the reason we need to keep the hop count is that a packet which has been misdirected may have to be dropped. And secondly what we are talking about here is this 15 that means it is giving you the length of the maximum number of hops within this autonomous system, within this domain. It is not that from the source to the ultimate destination it has to be 16 but within the domain there cannot be more than 15 hops. Routes time out is set to 16 after 3 minutes if they are not updated. (Refer time slide from 18:05 to 18:26 min)

A brief history: In the late 1960?sdistance vectors protocols were used in the Arpanet which is the beginning of this internet era. Mid 1970s: XNS Xerox Network System routing protocol is the precursor of RIP in IP and there were some other versions also. (Refer time slide 18:27 to 19:33 min)

Then this was sort of integrated in BSD distribution of Unix. RIP version one was formalized using this RFC request for comment. This is the standard way of formalizing things in the internet world through RFC?s. So, for any topic in this network you can possibly try to find RFC which will give you the most authentic source of information about that. So, version one was described in RFC 1058 and after encountering certain issues with RIP version one people came out with RIP version 2 in 1993 that was RFC1388. So it adds subnet masks with IP address with each route entry that allows classless routing. The original version is classful routing then classless routing was also possible with current version of RIP 2 is in RFC 22453 which came out in 98. (Refer time slide 19:33 to 21:20 min)

So what is the packet format of RIPv1? A command would be either request or response. So you may either request some information from some other router or you may be giving information in response to some request. The command would therefore be one or two. And the version would be RIP version one and then you have an address family and this is 2 for Ip, from 0 to 0 this is request full routing table. If you put it this way it means that you are requesting the full routing table. And then you have the address of the destination where you are sending this message and then these parts are unused. So this is 20 bytes, this is one route entry. There may be many such route entries up to 24 more routes each of 20 bytes. So this is actually 32 bits of the total. So, 32 bits means 4 bytes so it is 4 + 4 = 8, 12, 16, 20 so one RIP message can have up to 25 routing. Then the cost measured in hops is also mentioned. (Refer time slide from 21:21 to 26:27 min)

RIP version 2 is an extension of RIP version one. Subnet masks are carried in the route information. Now what is a subnet mask? In your full IP address which is 4 bytes for the time being in the IP version 4 you have a 4 byte address, so 4 bytes is 32 bits out of which the first part would be the network prefix and the last part is the particular host in the network prefix. Now the question is, which part of it or how many bits is the first prefix which shows the network and how many bits are there to show the host? Now there were some standards here these are the so called classful days. But there were class a, class b, class c etc these are classful where it is mentioned that for class a address the first byte would be the network prefix and the last three bytes are for the host which means you can have a very large network with a large number of hosts. On the other side of the scale you have a class c where 3 bytes would be the network prefix and one byte would be a small network or limited to less than 255 hosts but a larger number of such networks are possible. But even then what happened was that originally no one realized that the network world will grow into such a manner so only 4 bytes were kept as this address and this address space really became small. Suppose in an organization there are only 30 machines you will network them and then you want to connect them to the internet and for that you have to get some IP addresses. Now, if this network is to be specified completely then it has to be given a unique IP address for the network prefix part. So at best what you can do is that give it a class c address which is say 3 bytes long. So, that 1 byte is left over for up to 255 machines in your organization. But then you are using only 20 of them so a part of the address space is not used. Now, in order to do that people wanted more flexibility in saying that how many bytes contain the network prefix and how many bytes contain the number of hosts and that would be host part of the address. So you have to specify that how many bits are present. So now we are going from classful routing to classless routing. This classless routing has to carry that information called as the subnet mask. So subnet masks are carried in the route information so that we can do classless interdomain routing. Authentication of routing messages was there because routing version one had no security. And if it has no security what could happen is that a router might send all kinds of arbitrary distance vector messages to its neighbors creating problem everywhere. When things become very big then you start thinking of different kinds of possibilities so you try to change so some authentication was included in RIP version 2 although the authentication is not very full proof. Route information carries next hop address and exploits IP multicasting. What is multicasting? Right now we are talking about unique casting. This means there is one source with one message meant for one destination. Sometimes you want a message to be destined to a group of destinations. You do not want to broadcast it, you do not want to send it to everybody. May be there is a special interest group so you all only want to send to members of this group which is called multicasting. So IP multicasting exploits in some sense. So extensions of RIP version two are carried in unused fields of RIP version one messages. (Refer time slide from 26:27 to 26:55 min)

In the RIPv2 packet format as usual the command contains one or two whether it is a request or a response and then in RIPv2 there is the address family, address of destination and then the cost measured in hops. (Refer time slide 27:11 to 27:56 min) 

What happens here is, this field is used for the subnet mask for IP address. So you have an IP address over here, you see the how it is changed and the subnet mask has come here so that now you can do classless routing. Next hop IP address identifies the better next hop address on the same subnet than the advertising router if one exists, so that may come here and these are the changes. And used to carry information from other routing protocols, for example autonomous system, number, etc comes in this part. (Refer time slide from 27:56 to 29:51min)

Now a little bit about RIP messages: This is the operation of RIP in routed. So this routed is actually a command. A dedicated port for RIP is UDP port 520. If you remember we had come across TCP and UDP earlier. These are the transmission level protocols which are at a higher level protocol than these network layers. So UDP is one such which has got very low overhead. So we will talk both about UDP and TCP later on. But this is a higher level protocol and they use ports. Ports means for any message transferred between two nodes so many different transmissions may be going on from the same node to different processes in different hosts. In order to de-multiplex them, unscramble them, when there is an incoming message and you want to know for which machine it is sent to, then with the help of the IP address you can find out that this particular process is for this particular machine designated by a port number. This is a number which you get from the operating system if you ask for it. So, for RIP there is a particular port which is specified which is 520. For standard protocol there are standard ports which are assigned. There are two types of messages: One is a request message used to ask neighboring nodes for an update and response message which contains an update. (Refer time slide 29:55 to 03:28min)

RIP packets are sent using UDP hence may be lost. Actually UDP is one transmission protocol which has a low overhead and the other side of the coin is that it is less reliable than its sister TCP which is more reliable so updates may be lost. Updates are done asynchronously after receipt of new distance vector from any neighbor. If arriving distance vector includes a new destination network routing table is augmented to include this. That means routing table now increases in size. (Refer time slide 30:29 to 31:20 min)

And that is very important because when you power up your routing table may not contain anything. As you get more and more information from your neighbor your routing table grows. Every router normally sends update to each neighbor every 30 seconds. If the current route from node k to network I has a router n as next hop and if k has not received update from n in for 180 seconds which is 3 minutes it assumes failure. That means 6 such update periods then it assumes failure of n or the network connecting k and n. This does not matter because either ways it is the same for k and it marks the route invalid. Invalid routes should be replaced as soon as a valid one is found. Now (Refer time slide from 31:20 to 32:20 min)

how does it initialize? Send a request packet command equal to 1, address family is equal to 0 to 0 that means you are looking for the IP addresses and all interfaces. That means you flood a request at all interfaces. RIPv1 uses broadcast if possible, RIPv2 uses multicast address if possible. The point is that when it is initializing it is asking for information from all possible sources. Of course its sources must come through one of the interfaces so it sends out the request through all the interfaces may be one by one or by some means. Requesting routing tables from neighboring routers means we are trying to get full routing tables from the neighboring routers, that is the initialization phase. (Refer time slide from 32:20 to 32:35 min)

Request received: When routers that receive the above request send their entire routing table and response received update the routing table. Naturally you update the routing table using your distributed Bellman Ford. And then after some time when things are more stable the regular routing updates every 30 seconds send all or part of the routing tables to every neighbor in a response message. And then there are triggered updates. Whenever the metric for a route change something becomes invalid then send the entire routing table. (Refer time slide from 32:59 to 33:12 min)

Issue: One is the security issue, sending bogus routing updates to a router. RIPv1 has no protection. RIPv2 uses simple authentication scheme. We have mentioned about this authentication earlier. (Refer time slide from 33:13 to 33:35 min)

What you may have is a few passwords. Unfortunately these passwords are of course plaintext passwords so it does not give you too much of security but at least it is better than RIPv1 which had no security whatever. (Refer time slide from 33:36 to 34:13 min)

There are some problems with RIP: RIP takes a long time to stabilize. Even for a small network it takes several minutes until the routing tables are settled after a change. So that is one problem with RIP. And RIP has all the problems of distance vector algorithm, for example, count-to-infinity. We will come to this specific problem later on. RIP uses split horizon that means with reverse poison as it is usually said to avoid count-to-infinity. (Refer time slide form 34:13 to 34:36 min)

So this is the count-to-infinity problem. The problem is routers react to good news quickly. The min operator brings down the value in one go. In a sub net with n hops every router knows about it in n exchanges. However it reacts slowly to bad news as other neighbors mislead the routers. Gradually all the routers count their way up to infinity. So this (Refer time slide from 34:37 to 37:17 min)

is the problem. Suppose these are the 3 routers A, B and C with this one, C can go to A via B and the cost will be 2. And A can go to C via B and the cost is 2, B can go to C straight away with a cost one. Now suppose the link C goes down then B immediately finds out about it or after some period may be 180 seconds from then and it does not have any way to go to C so B may set C to infinity. But look at what A does. A had CB 2  so basically its routing table says that it can go to C with 2 because A still does not know about this mishap so it will have this C2 in its routing table which may be sent. So C infinity is being sent from here to here, C2 is being sent from here to here. Now B sees that A can to go C with 2 so for going to C the B will now set A as the next hop and keep a cost of 3. A of course will realize that it cannot take that B next because B is advertising C as infinity so it will make it to infinity. Now we have C infinity here and C3 here, exchange again and now this one becomes C4 and this one becomes C infinity. So this way both of them start counting to infinity.  What was the problem? The problem was A was sending C to, that means A was going to C via B but it was telling to B that it can go there at a distance 2 without realizing that this might mislead B. So one heuristic which was said is that if A is really going to C through B then to B instead of saying C2 it should say C infinity. So this is the (Refer time slide from 37:18 to 37:38 min)

hack so to say which was used. The reason to the count-to-infinity problem is that each node has only next hop view. For example, in the first step A did not realize that its route with cost 2 to C went through node B. How can count-to-infinity problem be solved? (Refer time slide from 37:38 to 38:12 min)

There is a one solution. This is a proper solution but this is not used in RIP. Always advertise the entire path in an update message. That means you give the entire path. If the entire path was given then this would not have happened. But that is not done. So, if routing tables are large the routing messages require substantial bandwidth. There is a interdomain routing protocol called BGP which uses this solution. (Refer time slide from 38:12 to 38:46 min)

This split horizon hack is that a router never advertises the cost of a destination to its neighbor n if n is the next hop to that destination. So A never advertises C to B. Or in split horizon with poisonous reverse it advertises a cost of infinity. This does not prevent three way count-to-infinity. But this is a hack this is not a solution because this takes care of some of the cases but this may not take care of all the cases. (Refer time slide form 38:46 to 40:39 min)

These are the details. Suppose you have this network A B C D E with this kind of cost associated it with it, now consider the router E. So E distance tables from neighbors, from A it may get this (0,7,?,?,1) that means A to A is 0, A to B is 7, A to C is ? to start with and A to B is ?. So distance table E sends to its neighbors so to A it sends A? because it has to go to A itself, actually again it goes directly so to B also it sends ?, what it sends to D you just look at this value it tells that its cost to C is ? although to B it says that its cost to C is 4 because to B it says that I can go with a distance 4 because it can take this route from E to C. Whereas it advertises a cost of ? to D because the path that E thinks it can take with a cost of 4 passes through D so to D it says C ? rather than C4 which is advertised to A and B. So this is the split horizon with reverse poison.  (Refer time slide from 40:40 to 41:05 min)

But look at this case. Suppose you have a network structure like this ABC and D. So these are all routers which are very fine. In order to simplify things let us say all the links have a cost of 1. Now (Refer time slide from 41:05 to 42:42 min)

when the link between C and D fails which is this link then C will set its distance to D as ?. However A will then use B to go to D and B will use A to go to D because there is no reverse poison between them. What is happening is that previously, B knows that it can go to D through C with a distance 2. A knows that it can go from A to D with a distance 2. Now C is saying D is ?. But B is saying D is 2. Now B then sends to A and A sends to B and mutually they start counting to ? not realizing that both of them were actually going through C. But there is no reverse poison between A and B, there is reverse poison between B and C, there is reverse poison between A and C but between A and B there is not reverse poison so they will mutually start counting to ?. After such updates A and B will then report new path to C for D and C will use the path. Now A will tell C that it has got a path to D through B of course and C will not know and now C will also take that. So for any thing from D it will send it to A, A will send it to B and things like this will go on without any packet getting anywhere. We thus have the count-to-infinity problem again. (Refer time slide from 42:43 to 44:07 min)

Other limitations of RIP: The simple metric of hop count does not permit traffic-caused delays to be taken into account in routing. So, when we are just talking about the hop count this is a limited kind of metric. So basically we are seeing whether we can reach there at all. But there are other kinds of metrics which may be important. That means some of the links may be very weak or of low capacity then again some links may be very expensive, somewhere the reliability may be poor etc and all these metrics which may be important in today?s world where the network communication is very important then you cannot handle it in RIP. So that is one problem limitation of RIP. Another limitation is if I6 hops are regarded as ? then RIP cannot be used for networks for which there are distances exceeding 15. If there are distances exceeding 15 then RIP cannot be used. So, by today?s standard that may be a small value. If infinity is regarded as corresponding to a larger number of hops then protocol will be very slow to converge upon initialization or on topology changes. The point is that, if you see a value of 16 or if something has gone up to a value of 16 you know that that is infinity and you can forget about it. Whereas if you simply change that infinity from 16 to 16,000 then you will have to count right up to 16,000 so the thing would be very slow to converge. Convergence rate after topology change or after crash etc becomes an issue in RIP. We will now just look at an example. (Refer time slide from 44:43 to 45:40 min)

Suppose this is the situation and then in the initialization phase A sets itself to 0, D sets itself to 0, E sets itself to 0 and so on then they solicit router advertisements. Then what will happen is that when they get the distance vector D will respond and B will also respond so A would know that these 2 routers B and D are just neighbors so it can put a distance 1 in B and D. And the same thing goes for all the routers so to get their next step. So, direct neighbors come in after one exchange. If there are values over here like 10, 2, 2 as distances etc these values get transmitted there so we get A0, B10, D1 for A. Similarly for D it gets E2 C2 and A1. Therefore the routing table starts getting filled up this way. Then of course after the second step you get neighbors of neighbors and so on so this routing table has now become 0, 10, 3 etc so this is on going. (Refer time slide from 46:20 to 46:49 min)

Then neighbors of neighbors of neighbors so those are basically 3 hops away and slowly these values are converging to the correct values. From A to D you can go to 1 and A to E you can go with a step of 3, A to C is 3 and A to B is 10 and A to A is 0. So now after three iterations it has stabilized. So this is a stable convergence (Refer time slide from 46:50 to 48:36 min)

and no value changes any longer. The ones in red were showing the values which change but now there is no change any longer. Now a new link has come up between B and C with a distance of 1. Now this message will start getting transmitted through the network like this. As soon as there is a communication between B and C both of them know that C has B as neighbor with distance 1 and B has C as neighbor with distance 1. So these are the direct endpoints. Then in the next step the neighbor of E will know the neighbor of C which is namely D will also know that they can reach B very quickly. Previously if you remember they had values like 11 and 13 so E had to go to B through 13 when it did not know because it took the route B to D to A to B so that is 2 + 1 + 10 = 13. But now since it can go to reach C and 2 and C can reach B with distance 1 so now it has updated the distance to B as 3 and distance to B as 3. So from 11 and 13 these values have come down in 1 hop and A of course still does not know. Now A knows that from going to B it need not take this route which is a very high cost path but it can go via C with a distance of 4. So now neighbors know. (Refer time slide from 48:37 to 48:55 min)

Once again we have a very happy and stable network. The point was that this good news traveled as fast as the number of hops because in one go in the main function will bring down the value to anything. But now suppose there is a bad news that (Refer time slide from 48:59 to 49:08 min)

means this link has crashed then what happens? (Refer time slide from 49:08 to 49:34 min)

Direct endpoints will know that it can no longer go in between B and C. Previously B for example thought that it can go to A through 4, there should be another entry that it can go through A through C. Similarly it can go to D through C once again, through E to E through C once again. Now when the link from B to C fails all these entries now become invalid because this link has crashed. So, direct endpoints would know that some of their entries will vanish. In the next step it will now set to A as 10 and when it gets the entire thing then it will set to something else, it will get help from neighbors so go to A to 10 and A says that it can go to C with a distance 3, D with a distance 1 and E with a distance 3 etc so it adds 10 to that so B gets all these values. Unfortunately there is a routing loop now (Refer time slide from 50:35 to 51:12 min)

due to inconsistent state information because now C knows that in order to go to B it can take a path of cost 5, how? It is because there is no way it can reach B through 5. The point is, it has been misled by its neighbors, the same thing that was happening in count-to-infinity. (Refer time slide from 51:15 to 51:37 min)

So they now mutually start counting to infinity for going to B until they reach a value which will force them to go through this link once again. So this count-to-infinity will continue till we will reach the limit and come back to stability once again. These kinds of limitations and problems were there in RIP. So one good thing about RIP was that RIP is a simple scheme, simple algorithm and actually it was conceived in days when networks were smaller but as networks grew bigger and bigger it had problems. this count-to-infinity problem is actually a small problem but other major problems why RIP could not be used when the network really scaled up for very big networks was this problem of having a low value of infinity and making the value larger makes the problem of convergence worse. And then secondly it is that you are really doing this one domain whereas in today?s world with millions of networks you require a hierarchy of domains. You require these autonomous systems and other hierarchy etc. The other limitation you was that we were unable to talk about the quality of the links. Since we were unable to talk about the quality of the links some of the routes which RIP might set up may be much less than optical. One thing that is very sure is because there is a lot of difference between the capacity of the links, you get capacities like 64 kbps on one end of the spectrum, you get capacities like 155 megabits per second may be in another side of the spectrum but the point is that these two values are several orders of magnitude away from each other. But RIP has no way of taking all this into account. So this was also important. So what happened was that people moved from this to another algorithm which is sometimes called link state algorithm just as RIP is sometimes called distance vector algorithm because it exchanges distance vectors similarly there is an algorithm which exchanges the state of links globally.  Since it exchanges the state of links globally then do some local processing and that algorithm is called Open Shortest Path First OSPF so this has got some advantages over RIP. (Refer slide time: 54:43 - 54:45)


Good day, so today we will talk about IP version 4 that is the internet protocol version 4. This internet protocol is really the network protocol of the entire stack and actually is at the heart of data communication. And as it turned out that it became so successful that also other kinds of communication like voice, video etc are also coming over to IP in a big way in many segments. So, today (Refer time slide 55:23 to 55:26 min)

 we will talk about the IP version 4 and (Refer time slide from 55:26 to 56:50 min)

 just a quick review of the stack that IP or the so called Internet Protocol is designed to connect networks that are possibly managed by multiple organizations or people. The internet that we see today is a connection of network of networks. So various networks are naturally owned by different organizations of people and also managed by different people. But if they have to communicate they somehow have to come together namely to one central network layer protocol and IP is that protocol. It may have different physical connections. Naturally if there are different networks having different connections it may have different physical connections. And it may be connected via sequence of arbitrary intermediaries. Arbitrary intermediaries in the sense that when you are communicating from one computer to another computer these two networks also may not be directly connected and they may go through other intermediate networks so there may be a number of hops before your communication reaches its destination. In the very beginning we discussed about layered approach which is used to simplify the application. (Refer time slide from 56:50 to 57:35 min) 

This is just an example. Let us say we have http which is the protocol used in the application layer. So this http protocol may use a TCP connection. This is another layer called the transport layer so we will talk about this later which communicates with the IP and which then may communicate with Ethernet. But you please note that below the IP there may be a multiplicity of different data link layer protocols like the Ethernet is one, token ring is another so they can communicate because of this Integrating Protocol IP which is common to both. (Refer time slide from 57:35 to 57:44 min)

So it is a single protocol at network level that insures packets will get from source to destination while allowing for flexibility. (Refer time slide from 57:44 to 58:18 min)

So we have the so called hourglass design. We have ftp, http, tftp etc and all these different application layer protocols are at the top. Then we have the transport layer and their two common protocols are TCP and UDP. But all of them integrate to one single network layer protocol namely IP and this IP may connect to different networks running different data link layer protocols. (Refer time slide from 58:18 to 59:30 min)

Just to look at how the encapsulation goes, we had discussed this earlier may be in the first or second lecture. So, suppose we have the user data being fed to some application then that particular application will have its own header.


COMPUTER NETWORKS
Prof. S. Ghosh
Dept. of Computer Science and Engineering
I.I.T., Kharagpur
Lecture Number ? 28
IP Version 4
(Refer slide time: 00:46)
Good day, we will talk about IP version 4 that is the internet protocol version 4. This internet protocol is really the network protocol of the entire stack and actually is at the heart of data communication. As it turned out that it became so successful that also other kinds of communication like voice, video, etc were also coming over to IP in a big way in many segments. So today (Refer slide time: 01:23 - 01:26)

 we will talk about the IP version 4 and (Refer time slide: 01:26 - 02:50)

 just a quick review of the stack that IP or the so called Internet Protocol is designed to connect networks that are possibly managed by multiple organizations or people. The internet we see today is a connection of network of networks. So it is of various networks and various networks are naturally owned by different organizations of people, managed by different people. But if they have to communicate they somehow have to come together and agree to one central network layer protocol and IP is that protocol, it may have different physical connections. Naturally if there are different networks having different connections it may be connected via sequence of arbitrary intermediaries. Arbitrary intermediaries mean that when you are communicating from one computer to another these two networks also may not be directly connected they may go through other intermediate networks. So there may be number of hops before your communication reaches its destination. In the beginning we discussed about layered approach which is used to simplify the application. (Refer slide time: 02:50 - 03:35 min) 

This is just an example. Let us say we have HTTP which is the protocol used in the application layer, it may use a TCP connection. TCP is another layer that is called the transport layer. TCP communicates with the IP which then may communicate with Ethernet. But please note that below the IP there may be a multiplicity of different data link layer protocols like Ethernet and token ring that communicates because of this Integrating Protocol IP which is common to both. (Refer slide time: 03:35 - 03:44)

So it is a single protocol at network level that insures packets will get from source to destination while allowing for flexibility. (Refer slide time: 03:44 - 04:18)
We have the hourglass design like FTP, HTTP, TFTP etc, are different application layer protocols at the top. Then we have the transport layer and their two common protocols namely TCP and UDP. Both integrate to one single network layer protocol namely IP and this IP may connect to naturally different networks running different data link layer protocols. (Refer slide time: 04:18 - 06:30)

 Just to look at the how the encapsulation goes once again, we had discussed this earlier. Suppose we have the user data being fed to some application then that particular application will have its own header. This header information is what is used for protocol between peers at the same layers. The application layer of this host will communicate with the application layer of the other host through this application header information and is passed to the transport layer TCP the transport layer protocol which is being used. Here the TCP header gets added and then it is passed to the network layer when it is coming down and when something is being sent where the IP header gets added at the network layer.  As far as this IP protocol is concerned this entire thing containing the TCP header, application header, user data etc is the payload. Similarly, for TCP the application header and user data together is the payload. So, whatever is inside the TCP does not really consider this. Similarly IP considers this entire thing as the payload and then it sends to the next layer which may be Ethernet as an example. Ethernet adds a header as well as the trailer. Some of the overhead we incur in this, for example, this TCP header is 20 bytes, IP header is 20 bytes, Ethernet header is 14 bytes and it has got a 4 byte trailer and then this entire thing is payload for the Ethernet which has a minimum of 46 bytes - 1500 bytes etc. This is how it comes when data is being sent and when data is being received it is in the other way, the first layer will take out these two headers and trailers, then the IP layer will take out the IP header, look at it and pass it up the TCP header and that will get stripped and finally the user data will reach the application layer. (Refer slide time: 06:31 - 07:38)

By looking at it the other way, suppose we have the Ethernet driver, this is really the lowest level in the tree that has been shown upwards. When something comes to Ethernet it has so many protocols in the next layer but these ARP is the address resolution protocol which is that given an IP address then finding the MAC address, this is the reverse of that as given the MAC address finding the IP address. IP addresses are used for communication within a same network but otherwise most of the traffic comes to IP. Above the IP layer this is TCP, UDP, IGMP used for multicasting and ICMP internet control protocol. Mostly the applications are for control and multicasting functions but the major part of the communication comes through this TCP and UDP and they connect to the various applications which may be running at the application layer and this how it is de-multiplexed. (Refer slide time: 07:38 - 09:51)

Just to summarize this IPv4 protocol IP is the best effort connectionless protocol. that means that the intermediate nodes will try to get your packet to the proper destination as best as possible but if it is not possible it will drop the packet. This does not give you any guarantee about the delivery of the packet at the other end but it is understood that all the nodes in between will make its best effort. Suppose you consider a router which is somewhere in the network now it is receiving packets from so many sources and they are destined to many other different networks. It may so happen that because of the pattern of communication the router may get congested. So if it is congested as there are so many packets coming in then it cannot hold them in its buffer any longer so it is force to drop some packets. Although it will make the best effort but then this is not a guarantee. And then secondly this is connectionless. You remember about connection oriented and connectionless protocols? For example, our telephone network was a connection oriented protocol but here this is connectionless. That means from the source to the destination there is no guaranteed physical or virtual connection between the two. And if you are sending a stream of packets from one source to the other some of the packets may take one route and then some take another route where all of them will reach their destination but they may get out of order. These are all part of the connectionless protocol. With a datagram or packet oriented protocol you can get an IP packet from anyone without any setup or connection establishment. Packets are normally routed using destination routing that means the destination is known and how to get to the destination is what is stored in the routing tables. You specify where the packet is to go now and not how it gets there. (Refer slide time: 09:51 - 10:46)

There are some more parts. You can optionally specify source routing. In Internet Protocol there is a provision that in the source itself you specify that this is the route through which it should reach the destination. For example, if you remember our BGP uses source routing. But in general for internet packets it is possible to do the source routing but this is somewhat limited because the number of hops you can specify becomes restricted because of some limitations of IP version 4 protocol structure. Each packet is routed independently. That means they can be delivered out of order or might not be delivered at all. (Refer slide time: 10:46 - 11:19)

Now we come to this important topic of IP addresses. The IP address is a 32 bit address that is 4 bytes which identifies the network and the host on a given network.  It is divided into two parts, first part identifies the network and the second part identifies the host in the network. The form is not the same if the format is not the same for each address. So this is an important point to understand that when you are given an IP address it is a 32 bit address, these 32 bits are actually divided into 4bytes and each byte is usually read out in decimal separately. You can get a IP address like 144.16.19.23 where this 144 is the decimal equivalent of the binary string which is in the first byte of the address. Then we have 145.16 where 16 is the next byte and next byte contains the decimal binary equivalent of 16. So, if you convert this 144161923 separately into  bytes which are the binary strings and put them together that gives you the 32 bit address.  Or in other words the 32 bit address is usually read out this way. Now in this 4 bytes there is some part which specifies which network you are in and there is some part which specifies which is the host in that particular network. This is the same concept when you think of a postal system where you give the name of the town and then the name of the street or the house. But the people outside will not really know about the streets in a distant town. They will just look at the town name or the pin code and just simply send it there. Then those people will figure out as to where is that particular house or street in that particular town. So we have a network part which is usually important for people who are outside who are trying to route into this network, and then there is a host part which is important within the network. So once the packet has reached this particular network then it has to reach a particular host so this is where the host part comes. (Refer slide time: 13:42 - 14:26)

 There are three types of addresses in one sense. There are other classifications also. But one is that, it is for unicast communication. Unicast means it is destined for a single host, So, it is originating somewhere and is destined for a particular host. Otherwise there is broadcast communication which is destined for all hosts on a network. In a particular network if you want to send some message to everybody then you can use this broadcast communication. Or there may be multicast communication which is destined for a set of hosts. This is a subset of hosts which are in that particular network which belong to a particular multicast group. So, that is called multicast communication. (Refer slide time: 14:26 - 15:18)

As we have already seen, the 32 bit number is represented in the following format. It is actually something. something. something. something where xxx is the decimal representation of the binary bit string. For example, 144.16.7.4 so if you see the first byte it has got a place value of 128 + 16 which makes it 144. This is the binary equivalent of 144 or 144 is the decimal equivalent of this. So this is the IP address 4 bytes of it and the corresponding decimal equivalent of each of the bytes is given 144.16.7.4. So this looks like a valid IP address. (Refer slide time: 15:19 to 20:51)

 As for another classification of IP addresses, IP version 4 has five classes A B C D E where all class A addresses start with 0 so it has a prefix of 0, class B has a prefix of 10, class C has a prefix of 110, class D has a prefix of 1110 and class E has a prefix of 1111. So you can see that they all have unique prefixes and by looking at it you can make out what it is. Irrespective of what values are here you can make out which class it is.  An IP address contains two parts, the first part is the network ID and the rest of it is the host ID. Now just consider the class A address, the first byte is given to the network ID and of course of the first byte the first bit is 0 indicating that this is a class A address and then you have 7 bits left for network ID. So you can have at most 127 or 126 networks as a class A because we only have 7 bits to represent it. And in each of this network the host ID has 24 bits for specifying the address of the host in that network. So for 24 you have actually 224 which is about 16 million. If your organization has a class A address which is very unlikely but if your organization does have a class A address, then this is one big network of that 127 networks world wide which has as many hosts as there in the network and how many hosts can be there? For the host address we have kept 24 bits which means that there can be a possibility of 224 or about 16 million hosts so there could be a network with 16 million computers in it. Nobody has such a big network and this is very unlikely to happen. This is how a class A address is specified. Next we go to class B which is in the same way but here instead of 1 byte for specifying the network we have 2 bytes for specifying the network. Out of these 2 bytes the first 2 bits are 10 so that is already gone and the rest 14 bits are for the network ID. So 214 is about 16,000, you can have 16,000 different class B networks. And each network can contain 216 hosts which is about 64,000 hosts so these are also fairly big networks. There are networks which are of that order but you have only 214 or 16,000 such networks but you cannot have more. For class C 3 bytes are given for the network part and only 1 byte. So a class C network can have only 28 that is 256 hosts that is a real small network but you can have a large number of them namely you can have about 2 million such networks 221. Class D is for multicast group and class E is really experimental so they start with 1110. So after 1110 the entire 28 bits is for specifying which group. But this does not work very well so we will come to that later on. So if class A ranges dotted quad so A is 000.0.0.0 of course all 0s is not a valid particular address but anyway we are giving the outer limits of it to 127.255 etc. So by looking at the first decimal number which corresponds to the first byte we can immediately make out whether this is a class A, class B, class C or class D etc address because class A address cannot be more than 127 and if the first digit or the first decimal number is less than 127 then you know that this must be a class A address. If it is from 128 to 191 then you know that this is a class B address and so on. So, by looking at the first decimal number which is the equivalent for the first the first byte is being dictated this way depending on the class of the address. That is how they come to a particular range of numbers. So, by looking at the first number we can know which class of address it is. (Refer slide time: 20:52 - 21:21)

So this class A is obviously used for very small number of networks and large number of hosts.  First byte represents the network address and the last three bytes represent the host address. Class A address have a first bit of 0 and class A network addresses range from 0 to 126 and 127 is actually reserved for something else. (Refer slide time: 21:21 - 21:49)

 Class B provides an equal number of networks and hosts. First 2 bytes are network address and last two bytes are host addresses. First 2 bits of a class B addresses are 1 and 0 so you can only have 16,000 such class B networks and network addresses range from 128 to 191 that is the first decimal number for the first byte. (Refer slide time: 21:29 - 22:03) 

Class C is a greater number of network addresses, fewer host addresses, the first 3 bits are 110. So network addresses range from 192 - 223. (Refer slide time: 22:04 - 22:11)

 Class D is used for special multicast addresses. The first 4 bits of Class D are 1110. (Refer slide time: 22:12 - 22:21)

Class E is used for experimental purposes. The first 4 bits of Class E are 1111. (Refer slide time: 22:21 - 25:42)

There are some special source addresses as part of an initialization procedure for example bootp. This host on this network, the net part is 0 and host ID part is 0. That means when the net part is 0 depending on which class of address it is and suppose it is the class C address then the first 3 bytes are really the network part so it will be 0.0.0 and the host part is 0. So if you say 0000.0.0.0 it means that it is itself. So this is sometimes required for example in bootp protocol you require referring to yourself. Specified host on this particular network: So you keep the network part 0 and the host ID for this particular host that you want to specify, suppose this is a class C address, the host part is given only by the last byte. Suppose the last byte has a decimal value of whatever maybe 130 or something. So if you say 0.0.0.130 that means whatever the network I may be in, in this particular network get me the host number 130. It is a specified host on this particular network wherever it is. Loop back address: The loop back address allows applications on the same host to communicate using TCP IP.  Here the net ID is given as 127 and host ID could be anything which means the first byte is all 1 that is the loop back which is referring to this particular host. Why do you require this? Suppose two different applications are running on the same host and they want to communicate with each other using TCP IP. But why do they require TCP IP if they are on the same host and when they could communicate directly? The point is that these two applications could have been hosted in two different hosts also. If this was the case then they would have to use TCP IP. So instead of writing two different versions, one for the case where both of them are in the same host and the other in two different hosts you write the same program and use the same TCP IP stack. The only thing is that you want to refer to the same host. If you are trying to refer to the same host using the same TCP IP how would you know the network address and so you would not be interested to include a hardware into that particular application. Therefore there is a way to refer to the same host which is by using 127 that is all 1s in the first byte. (Refer slide time: 25:43 - 28:41)

Then we were talking about three different types of addresses for unicast, broadcast and multicast. We have seen about unicast communication that is the network part and the host part, both you indicate and then you are talking to that particular host. We have seen multicast in class C addresses and mostly in class D so that was a multicast group. And then now you talk about broadcast. But here there is a caveat. In the sense that you are not allowed to broadcast to the whole wide world because if everybody or even a very limited fraction of people start broadcasting some message to everybody in the world then the entire network will be swarmed with broadcast because now in this age of internet millions and may billions of people are getting connected to the internet, So if only a small fraction of them want to broadcast things to everybody that cannot be allowed because then the entire network will go down. So broadcasts are always limited and how they are limited and how the broadcast addresses are specified will be discussed now. Limited broadcast typically used for initialization only appears on local cable or collision domain with net ID is ? 1 that is all 1s and host ID is all 1s. This means that if you give an address which is all 1s it means that you want something to be broadcast in the local network wherever you are in. The net directed broadcast means you want to broadcast to a particular network. So this is forwarded via router. Now that particular network has to be mentioned so the network address part is to be specified giving the net ID, the host ID is all 1s so all 1s means to every body. But if you put all 1s in network part as well as all 1s in the host part it cannot mean that all networks and all hosts in all networks, that is not allowed. All 1s in the network part as well as all 1s in the host part means that it is for all the hosts in this particular network. And then there could be subnet directed broadcast. We have not talked about subnet as yet which we will do. There will be a subnet ID which is really carved out of the host part of the address and then host ID part will be all 1s. (Refer slide time: 28:41 - 29:14)

 So all 0s means this host and all 0s in the network part and the host part is something specific which is like a particular host on this particular network. All 1s mean broadcast on the local networks. If the networks part is specified and if host part is all 1s it means that broadcast is on a distant network. If it is 127 then it is all 1s on the first byte and anything in the rest of it is a loop back. (Refer slide time: 29:15 - 34:10)

Now we come to a problem. A few companies got class A like Xerox and some other companies got a class A address when they were actually very closely involved with designing internet in the Arpanet days so people really did not know who designed this network and had no idea that their project is finally going to blossom so wildly and become greatly successful. They had no idea that they had to be careful with all these addresses that they were doling out so the companies which came in got some class A addresses. These class A addresses can have 16 million hosts which is really much bigger than anything that a company might want. Many institutions got class B networks such as 12 institutions. For 14 bits you can have only 16,000 so it is not such a big number when you are talking about in the global scale so 16,000 institutions and may be 126 companies over there is really a small number. But now-a-days there are millions of companies and ripped hundreds of thousands of companies who want to have their own network etc. They are now reduced to accepting only class C address. Class C address is too big because you cannot have 16 million hosts but a class C address can accommodate only 256 hosts but 256 is a very small number. Any institution or many institutions now-a-days have got thousands of computers in their network so this too small for them. For the first one let us think of another problem. Suppose you have a class B network where you have 10,000 nodes now if all these 10,000 nodes is one network, the network in our parlance at this particular moment is one particular broadcast domain, so you can always broadcast in this network. If you are sitting in one particular network and you communicate to some other node in that particular network then you need to know his MAC address which you do not know but you know his IP address so what you will do is you will broadcast the IP address asking for the MAC address. Now whichever machine has got that particular IP address you will get it and answer with his MAC address and that is how the ARP protocol works.  So whenever you try to communicate if you do not know the MAC address of the other side naturally you will send a broadcast to the entire network. Now if all these 10,000 hosts start sending broadcast messages from time to time then the broadcast traffic would be too much. Since the network is so big we have to break it up into smaller parts so that broadcasts are limited to smaller sub networks for which you need some more bits. Previously we were talking about the two parts of the address, one is saying this network and then we are saying this host in this network. Now we have to say three things; it is this network, this particular sub network in that network and then this particular host in that sub network. It is just like instead of a town if you have a city then in a city there will be a large number of post offices in the same city. From outside may be from another country they will send it to that particular city and in the city you will decide this is that particular post office in that city and that particular post office would know that it is for this particular house on this street in this region. So we have a network, a sub network which is the breaking of a big network into smaller sub networks and then we have a host in that particular sub network. So, as we have said that broadcast would enter a network obviously it is impractical for class A networks and even for class B networks. (Refer slide time: 34:10 - 34:29)


So subnets are used to divide a large network into smaller networks. Each address allows for one network address and many hosts that is, all hosts are on the same network. Subnet masks are used to create many subnets within the same network address. (Refer slide time: 34:30 - 35:32)

 So we will look at subnet masks. This is a bit string applied to an address. If the bit is 1 the corresponding bit in the address is considered to be a network bit. The network mask is known only locally. If we have one part which is the network part and the next part which is the host part then we take some bits from the host part and use them for specifying the sub network.  The number of bits of the network which you take for the sub network is given by the subnet mask by placing those particular bits to be 1 and this subnet mask is known only locally. (Refer slide time: 35:32 - 36:55)

 So this is an example. Suppose we have a class B network, in a class B network you know that the first two bytes is the network part and the other two bytes is the host part. Now in this host part these 6 bits will show my sub network address. So you make the corresponding bits in the network mask to be 1 and the rest are all 0s. The host part are all 0s, Now, looking at the address we will know whether it is a class A, B, C just by looking at the first number and we know how much is the network part. If we know the subnet mask we know how much is the network and sub network part. If we take out the network part from that we get the subnet address here. Actually this not the address, the masks only tells you that these are the bits which are used for the subnet address and the rest of it is for the host address. We will see examples of this. (Refer slide time: 36:55 - 39:07)

 Let us say we have an IP address 144.97.16.132 that is the IP address of a particular host. And we are also told that we have a subnet mask of 255.255.255.192,1 etc. These are all for only human communication but of course these are all individual bytes which are to be converted to the corresponding bit strings. The 255 are all 1s in the byte so all 1s and 192 is 128 + 64 which means the first two bits are 1 and the rest is 0 so this is subnet mask. Since this is the class B address we know that if you convert 144.97.16.132 then this is the string you get 144. So the first two bytes is for the network part. Now we also know that the 8 + 10 bits are for the sub network part. So this is the sub network address 0001000010 and the host is 100 part. So the network part is the first two and then the sub network part 000100 so this is the entire network part wherever we have a 1. And beyond that the point where we have a 1 we put all 0s for the network address. So network address is 144.97.16.128, the 144.97 is telling me which particular network it is and 16.128 is telling which particular sub network is within that network. And the host is this 100 so the host is 4. Let us just see another example. (Refer slide time: 39:08 - 39:57)

 So we have this IP address which is 144.97.17.132 and the subnet mask is this. So we see that the first seven bits so the network has been broken down into 127 different networks and then we have 9 bits for hosts so we can have may be 512 hosts in each network.  This is the network part up to this one so we take all the 1s there and the rest are put as 0. So network is 144.97.16 and host is 1.132. (Refer slide time: 39:58 - 40:53)

So how do packets get to the other end? Select router based on the IP address. That is, for class B use the upper 16 bits as a network specification, for class C use the upper 24 bits as network specification and so on and naturally for class A just look at the first 8 bits. Route to that network using the routing tables as we have seen. So depending on whether it is a class A, class B or class C you use 1 byte, 2 bytes or 3 bytes as the network address. Route to that network using routing tables. If your routers, RIP or OSPF etc is working properly then it will reach that particular network. The point is, what happens after that? (Refer slide time: 40:53 - 41:36)

Then the router uses the pre-specified subnet mask to select a subnet because looking at the subnet mask and at the IP address it knows how many bits are there for specifying the subnet. So it finds the subnet mask and it just takes out the subnet part of the address which is looked up in a subnet routing table. A subnet routing table is consulted and traffic is directed to that particular subnet. So this gives you a more hierarchical structure and ARP broadcasts are contained within the subnet. If you reach that particular subnet then the host number is given and you can go to that particular host. (Refer slide time: 41:37 - 46:14)

Now, we come to the other part of the problem. As I mentioned class A is of course huge, class B is also very large so we have to break them up into the subnet. The other side of the problem is that, the class C address is very small. The other thing to note is that the network has grown so much. There is a tremendous demand for IP addresses. This IP addresses have to a global standard. You cannot have your local standard because other people would be looking at your address may be at a different corner of the globe and try to route packets to you. So these addresses have to follow a global knob and that was used to be controlled by one central body. Now, if you have to give the address to somebody then you have to give internetwork address for his particular network usually. And now this creates a problem in the sense that when the demand for these addresses becomes too high you run out of addresses. As a matter of fact we have come to a stage today where for all practical purposes we have run out of addresses. Now you only have a few class C addresses left. If we had been much more careful, if we could have envisaged how the networks would grow and if we had been much more careful earlier in assigning addresses and not waste big addresses like that may be we could have stretched this for by a few more years. But anyway this cannot be helped now. So people are working on various types of solutions to overcome the shortage of IP addresses. And the other side was that quite a number of years back people worked on a new protocol, you must have noticed that this lecture is titled IP version 4 possibly implying that there are other versions available and actually there is IP version 6 which was finalized quite a few years back. Anyway in the IP version 4 we have this problem, what are the various kinds of workarounds? One thing could be, if you take a big address chunk let us say class B address and give it to different organizations may be some parts of it, the one problem is that, first of all you are breaking down these classes class A, B, C at the byte boundary. So the point was that do not have them at the byte boundary so they no longer belong to one particular class, they are called classless. And classless interdomain routing that is the CIDR is the protocol which is there where you specify your starting address and then specify how many hosts you have in your particular network, so this is an example. Suppose some British Universities like Cambridge, suppose the first address is this and the last address is this that means how many nodes they can have? You can have from 0 to 7 so that is 8 over here and of course 256 on the last byte so 8 ? 256 that is about  2048 hosts. So it gives the starting address 194240.0/21. This 21 is a code which really shows that there are 2048 hosts. Now various such numbers are possible, 22 means 1024 hosts, 20 means 4096 hosts. This way it goes down on one side and on the other side you can even have less than 1024 hosts so various numbers are there and there is a table here. Similarly 19 would mean 8000 hosts and so on. (Refer slide time: 46:15 - 50:57)

There is another workaround people have done and actually many organizations are now doing it and in common parlance it is called NATing that is Network Address Translation.  As I said earlier your IP address has to be known globally, it has to be a global standard, globally assigned. Now, if we make an observation that this is true only when we are communicating with somebody or only when somebody else is trying to communicate with me that is when it needs to be global. So you have an entirely private address and if it is a private why not it be a class A address? So whatever be the size of your organization may be you use a class A address inside. This class A address that you have just to use it without permission means that this address is not recognized globally. Assuming that you are in a big organization you do all your internal communication using this private address. The point is that only when you are going out to communicate with somebody you will mask your private address, keep a temporary table and tat table will be dynamic one, for the time being you want to communicate, dynamically there will be your local address which is actually a private address and not a globally legal private address and you will put it and then you will have a pool of legal addresses. And you will use one of them whichever is free and then start communicating. To the outside world it will be as if you are communicating with this particular legal address which you have assumed for a temporary point of time while you are communicating. So that is called Network Address Translation. Suppose we have this company LAN that is the company router etc and then a packet arrives and suppose this has an address 10.0.0.1 then the first number is 10 which immediately tell you that this is a class A address but really this company does not have A class A address but is using it. And there is a convention that when we use private address some how we use 10. But whenever somebody looks at an address that starts with 10 he knows that this is a private IP address. Therefore this is a private address he is using and this is going through may be a NAT box or firewall. Now this NATing could be done at a firewall. The NATing can be done in the router also and so all these boxes usually come with that capability. So, in the NAT box over here or this firewall maintains a table of this IP address it is trying to communicate, it assigns the pool of IP addresses and out of those IP addresses may be this particular IP address 198604212 is free at the moment. So he will take out this particular address, put in this particular address and send it to the outside world. So the outside world will know the source to be this particular address. When the outside world replies back it will come back here to this particular address, it will go through the same box, the box will know this is really an address which is temporarily assigned to him so he will now take this out, put 10.0.0.1 and send the packet to the particular host in that particular network. So the outside world will know that he is communicating with this fellow whereas this is not really a fellow this is just one of a pool of address which is shared by a large number of hosts and the translation is done here. So that is an example of how NATing is done. (Refer slide time: 50:57 - 52:13)

This is not entirely satisfactory but this is used quite often for example in my organization IIT we have got more than 10,000 machines and we do not have a class B address. You cannot help, so we just have a bunch of class C address. So we assign these class C addresses to all these boxes, we do the NATing and that is how we communicate. One problem is that if the NAT box fails all the connections are lost. It violates the OSI layer independency because this is a workaround and we do not have so many addresses that people demand. Some applications insert IP addresses as a part of the message then of course that application will fail because if the IP address is some how hard coated inside the application message somewhere that is not going to work. And NAT changes the content of the IP datagram, this is incompatible with secured data communication. If you want to do the entire thing secured including your IP addresses where you are communicating this NAT will not work or wherever you are doing NATing you can not encrypt that part. (Refer slide time: 52:14 - 52:30)

IP data is laid out in big Endian order. That means byte transmission order is 0, 1, 2, 3. You know big Endian or little Endian or which way you go 0, 1, 2, 3 1 or 3, 2, 1, 0. So, in networks this is the network byte transition. (Refer slide time: 52:31 - 53:23)

 In this IP header we have version, header length etc and the IP header is 20 bytes or more, it is minimum 20 bytes so this is 4 bytes each so that is 32 bits each. The source IP address is given as 4 bytes, the destination IP address is given so that is another 4 bytes so that is 8 bytes gone. These three batches of 4 bytes each which amounts to 12 bytes are used for various things. Later on there may be some options. (Refer slide time: 53:23 - 54:36)

 This is version 4, then version 6 would be there, bit filled specifying the IP version currently 4. Header length specified in 32 bit words and range is from 5 to 15 words or 20 to 60 bytes. So what is the length of the header? Why do we need the length of the header? If you look at the previous one there could be options, so how do you know whether just after destination IP address the data starts or the header goes some more and more options are exercised? The header length has to be given. The type of service: Some kind of quality in service was expected but this did not work out very well and is mostly ignored now. Then the message length is in bytes. The datagram identification field must be unique. So there is a datagram identification field, 16 bit packet identification. We will talk about it when we talk about Fragmentation. (Refer slide time: 54:37 - 56:49)

Time to live field: Upper limit on the number of hops that a message can go before being dropped. Although this is called Time to live, actually this is given by the number of hops and why do you require that? Sometimes the routers work in a distributed fashion as we have seen in this RIP and other protocols that they work in a distributed fashion and there may be some problem somewhere. Now because of that problem you may get a routing loop. That means virtually since this loop is stored in a distributed fashion nobody really detects that there is a loop but actually one particular packet finds that this has gone in a loop. Now this packet if it does not die out naturally it will keep on circulating at infinitum and such packets will get accumulated and it will bring down the whole network. So there is a mechanism and there are other reasons for this of course. But there is a mechanism that if a packet has gone very much astray or if it is just circulating after some number of hops the router will see that it has already crossed so many hops and this is the time to live and the time to live has come down to 0 so drop it. Otherwise you just reduce the time to live by 1 and send it to the next hop. Now there is a protocol which identifies TCP, UDP or ICMP. You remember that this is on the network layer. Now above the network layer there is a transport layer. IP is a hourglass design which sort of concentrates on the IP from the various different types of networks like Ethernet, token ring etc. It also comes to IP, from IP it goes to various different protocols like TCP, UDP and so on.  Now in the network layer how does it decide where to go to, whether to send it to TCP in transport layer or whether to send it to UDP in the transport layer, so that must also be mentioned. So this protocol identifier is there whether it is TCP, UDP, ICMP, IGMP etc. Header checksum: Checksum of just the TCP, IP header that is this IP header and source address which is 4 bytes, 32 bits destination address another IP address this is again 32 bits and options. (Refer slide time: 56:48 - 57:45)

Data starts at total length that is the header length etc and maximum IP datagram size is 64 kilo bytes. Hosts are not required to receive packets greater than 576 bytes. That means at least 576 bytes they have to accepted. Ethernet, MTU is only 1540 bytes. So most implementation allowed is about 8000 bytes for IP datagrams. The point is that when a particular packet has come to a network the packet may be too large for that particular network to handle, so then something has to be done. One thing is of course to drop it but if you drop it then every time he wants to send the packet the packet will get dropped. So what is done Is, this packet is broken down into smaller parts called fragments and these fragments are then sent through the network. In the next lecture we will talk about the IP version 6 and mobile IP.   
PROOF
COMPUTER NETWORKS
Prof. Sujay Ghosh
Department of Computer Science & Engineering
I.I.T Kharagpur
Lecture No: 29
IP Version 6 & Mobile IP
(Refer slide time: 00:00 - 00:40)
Good day, in the last lecture we discussed about IP version - 4 [IPV4]. That is the version of Internet Protocol that is now ubiquitous in the sense almost everywhere it is used. But as this particular version became more popular than its originated thought then some problems about IPV4 came into focus and people started discussing about what is the next generation of Internet Protocol that would be there and after a lot of discussion etc people came up with this IP Version 6 [IPV6]. (Refer slide time: 01:30 ? 01:37)

We will be doing a little discussion on IPV6 today. In the later part of the lecture we will be talking about mobile IP. (Refer slide time: 01:38 ? 01:37)

What was the design goal? As I mentioned, IPV4 was very successful, but the limited addresses posed problems. This was discussed earlier as how people are trying to fight with this problem using NATingnetting etc because so many machines are coming into the network these days and not only machines but in certain cases people are actually deploying all kinds of gadgets which should be connected to the network. If something is connected to the network and accessed from anywhere on the internet then it has to have an IP address. The pool of IP addresses we have in IPV4 is very limited and this is one of the major problems. (Refer slide time: 02:34 ? 02:47)

 And the second problem is, as mentioned earlier, the routing information were not inherent in addresses. For example, in a postal address, we have the Pin Code and in the pin code if the first digit is 7 then immediately we know that it is towards the East. If the first digit is 1 immediately we know that it is towards the North. So just by looking at that you can simply send the material to that direction. But that has not been so because these IP addresses although they were based on networks which are larger chunks than hosts they were distributed but then this could not be maintained at that time. If you could have some means of geographical information inbuilt into it then routing becomes easier and the routing table becomes smaller. Therefore, if the routing table is smaller routing speed becomes faster and so there are many advantages. (Refer slide time: 03:43 ? 04:07)

Thirdly, experience had shown that some aspects of IPv4 were problematic like Option headers and fragments etc were problematic then some type of service [TOS] which people never never used, options also have a very limited utility because of its limited size and fragments was a problem. These were the basic issues. (Refer slide time: 04:08 ? 04:21)

The simplification for IPV6 as mentioned was that to move to a 128-bit address. From 32-bits if you remember that IPV4 has as an address size of 32-bits whereas this is 128-bits. So in IPv4 in a theoretical maximum it is 232 (of course it is less than that but anyway the theoretical maximum is 232) addresses. Whereas here it is 2128 addresses which is a very huge number. Even, if all the devices and computers you can think of are connected and given individual address space then also you will have a huge number of addresses to spare. This was done with the idea that we are not going to run into this problem of limited address space ever. The other point is, if you have so many bits, as I said that even after assigning numbers to all the devices and computers you will be left with some to spare so that can be used more intelligently. (Refer slide time: 05:15 - 05:25)

Second point was to assign a fixed format to all headers. In IPv4 also, the essential part of it, the initial part of it, the compulsory part of it is fixed. But there are options and these options could be of various sizes so that is also removed. (Refer slide time 05:37 ? 06:01)

Remove the header checksum which was not doing much anyway. Use extension header rather than options. Options were removed and we came to the concept of extension header that means headers followed by other headers, we will come to this later on. Remove hop-by-hop segmentation procedure. That means you do not segment it somewhere in between a packet that is traveling and then somewhere in between you try to fragment it. However, that was not a good idea, and because of this fragmentation you have to keep the fragmentation number, the packet identification etc so all these are removed although fragmentation can be handled in some way. We will talk about that later. (Refer slide time: 06:23 ? 06:44)
  
This was the original IPv4 header which we have already discussed like version header, length, type of service etc. This Type Of Service [TOS] was not very useful. Fragments etc came in because we allowed fragmentation which is not done here. Header checksum may go out but the source and destination IP addresses would be there. Let us come to the IPv6 Header. (Refer slide time: 06:45 ? 07:09)

IPv6 Header is actually much simpler than the IPv4 Headers. We have a few fields and then the source address. Assuming that this is 32, previously IPv4 address was only one line but now you have four lines i.e. 128-bits for source address and 128-bits for destination address. Let us look at the fields. (Refer slide time 07:10 ? 07:27)

One is the version number. Previously it was 4 but now it is 6. Class: This is used to assign service class for real time networking. If you are doing some real time networking that can be indicated here. Then, there is a field called Flow: If you  quickly look at it we have version, class, flow level. (Refer slide time: 07:28 ? 07:32)

Flow: Flow means given one particular source and another destination then for this particular source and destination pair there is a flow level. Flow means these two are likely to send large number of packets and all of them would belong to the same flow. This is not a virtual circuit identifier like ATM because in ATM the virtual circuit identifier and intermediate switch would just look at the virtual circuit identifier and switch it that way.  This is not for that purpose at all rather this is for treating the packets with a particular flow level from a particular source and destination in the same way where all packets belonging to the same flow level in the intermediate router. For example, there may be class of service or all kinds of quality of service requirements for one particular flow that may require bandwidth reservation in between. Therefore such things can be handled using the flow level. (Refer slide time: 08:45 ? 09:31)

Payload Length: Only include the payload and not the 20-byte Header. This is 16-bits for that so packets are once again less than or equal to 64 k. Next Header; This gives rise to the possibility that there may be more than one header. If there are not any more IPv6 Headers then, at least the higher layer headers like TCP or UDP Headers could be there. There is a field called Hop Limit. This is really the TTL (Time to Live) which was present earlier in IPv4 but was used to just keep the count of the Hop and this is just renamed as Hop Limit. (Refer slide time: 09:32 ? 10:12)

 Fragments: One of the lessons we  learnt in IPv4 was that the unit of transmission should be the unit of control so no fragments created en-route in IPv6. If message is greater than MTU the Maximum Transferable Unit then you get ICMP message which is an Internet Control Message Protocol. We will talk a little bit more about ICMP later on. But this is some kind of control message which may be sent by a router to host etc. So, an ICMP message should use the path MTU. Let us see what is meant by this MTU and path MTU and how do you avoid transmission, Suppose you are the source and you want to transmit a particular packet it so happens that en route it encountered a link where such a big packet cannot be accommodated. In IPv6 what this router will do is that it will drop the packet and send back an ICMP message saying that this MTU is so much which is for the next link. Now you will reduce your packet size at the source itself and try to send it again. But now it will definitely cross that particular link, it may get struck again in another link so again an ICMP message will come back but finally you will come to size of packet which will go through all the links. Now this is your path MTU.  Now you can go on sending all your communication using this particular packet length and it will not be fragmented in between. (Refer slide time: 11:17 ? 11:36)

This is a way to fragment a datagram but it is done in an end-to-end fashion. It may so happen that for some particular application all these smaller packets we have made should actually be made into bigger packets. So this is fragmentation in some sense so far as the application layer is concerned so there is a way to indicate that, there is a header for that. (Refer slide time: 11:46 ? 12:29)

Finally we have removed the options from the IPv4 Header and we have come to this Extension Header. That means there may be more than one header. We could have this situation that IPv6 Header and next Header is said to be TCP. The Payload is the TCP Header and Payload itself. It could be that IPv6 Header, the Next Header is a Routing Header, which again is an extension header for IPv6 Routing Header and the Next Header is TCP so the TCP header and payload comes here. So there may be more than one IPv6 Headers and Headers are of different types. (Refer slide time: 12:29 ? 13:01)
   
Intermediate routers do not need to look at the Headers unless we tell them to. Specifically it has to look at some Headers but can ignore few other headers. It does not need to process all the information it should be fast. Extension Headers and Protocols, for example, TCP shares the same 256-entry name space i.e. 256-entry name space for the Headers. Hence there are limited number of extensions but this number is a big enough. (Refer slide time: 13:02 ? 13:33)

There is a certain order suggested that these Headers should occur in one particular order. One is, IPv6 Header the main header we talked about, An And the Extension Header called hop-by-hop Header, Destination Options Header, Routing Header, Fragment Header, Authentication Header, Destination Options Header, Upper-layer Headers if any that means TCP or UDP. Let us quickly discuss a few of them. (Refer slide time: 13:34 ? 14:21)

Payload may be encapsulated,payload followed by the Transport layer Header. Then there is a TCP, then a Routing Header, Authentication Header, another two Routing Headers, then IP header and so on. What you do is that you peel them one by one so that one Routing Header is peeled of because the Routing Header gives you information about how to route the packet something like source routing so that is peeled of may be in the next hop and this goes out. The IP Header remains and the routing header authentication header etc remains. You peel out one Header after another and finally you get to the TCP and the payload. (Refer slide time: 14:22 ? 14:38)

Naming: A large part of the address space is unassigned. This means, at this point of time people thought it prudent to keep provision for some future requirement which we cannot envisage at this moment. So a large part of the name space is simply been kept unassigned. (Refer slide time: 14:55 -15:08)
 
There is a way now to move away from provider based routing, based ID?s  the two routing based ID?s although both are possible. Previously what would happen is that the service provider would take a chunk of IP addresses and it is for his network. Now this could be distributed in various places. So, provider wise this loses the destination information. Whereas if you had done it geographically the routing would have been much easier, the routing table will also be smaller. IPv6 keeps the option of both. So you can have provider based addresses and also geographic based addresses. There are various levels of aggregation like top-level aggregation which is essentially a hierarchical organization reflecting the current internet architecture. (Refer slide time: 15:56 ? 16:09)

Then the Next Level Aggregator, then Site Level Aggregator allocated to a link or a link level or site level aggregator that is local. This means, at the link of the site level the rest of it may be common. It does not matter because it is strictly for local use that is something similar to a private IP and not for communication with others. (Refer slide time: 16:15 - 16:20)

The interface ID is based on EUI ID, the extension of the Ethernet MAC address and even that can be embedded. (Refer slide time: 16:29- 16:59)
 
There are some unspecified addresses. We need not bother about all this because IPv6 as of yet is not been deployed much. Only thing I would like to mention is about any cast. We have talked about Unicast, Broadcast and Multicast. Any cast is a concept something similar to multicast but in multicast there is a group where you can send some message to all the members of the group. In any cast you can send any message to any member of the group.  (Refer slide time: 17:15 ? 17:41)

Let us look at some of the Routing Extension Headers. It has the next header. a Header length, a routing type etc. Now we have some address 1 to address n. There are some IP addresses, IPv6 addresses may be listed over here. (Refer slide time: 17:42 ? 17:55)

It plays the same role as source Routing Header. You remember that, in IPv4 options there is a way to give the routing from the source. That means you determine the routing from the source itself.. Such a facility is very important for protocols like BGP because BGP wants to dictate the route through which the packet should be routed. But the problem with IPv4 was that the Header length was very limited so you can go only up to a dozen or so may be 12 to 15 hops in the source routing. If it is beyond 12 to 15 hops you would run out of space in the header so you would not be able to specify that. Here you can have a routing header then you can have more than one routing header and this particular difficulty is obviated. (Refer slide time: 18:42 ? 18:54)

Basic idea is, when a datagram reaches a destination, the destination checks for a Routing Header. If there is at least one segment left, that address is copied from the routing header and the packet is forwarded to that address. (Refer slide time: 18:55 ? 19:17)

Otherwise, the routing header is removed and the next routing header is processed. You can have multiple routing headers if the 8-bit header length causes a problem. There is a Header length of 8-bits so you can go up to a length of 256 but then you can have multiple Routing Headers. You can specify other source routing nodes using type. (Refer slide time: 19:18 ? 19:46)

Fragment Header: Each Fragment routed independently. Identification identifies the original packet that was fragmented. The offset is the offset within the fragment. The M field is a more fragments bit and is set to one for all but last fragment. This is exactly similar to the way fragmentation was handled in IPv4. The difference over here is that the source sends it using the path MTU that means in the in between it is not fragmented and whatever fragmentation is done is done at the source and that information is carried in one header called Fragment Header. And those would need not fragment anything they will not use this header. So, all these extension headers are optional. You have to have the first IPv6 Header but all the extension headers are optional. Therefore, if you are not fragmenting then you will not use this header. (Refer slide time: 20:23 ? 20:39)

There is a Destination Options Header: When a packet reaches its final destination (or at least when all prior routing extensions have been processed), the destination options header is processed. So as an option the unknown options are discarded. (Refer slide time: 20:40 ? 21:23)



Hop-by-Hop Options Header: This is another one. The Destination Extension Header is looked at just at the end at the destination. In the hop-by-hop all these at intermediate hops you need to look at this hop-by-hop options header. They are processed at each hop, For example, the Jumbo payload header. The IP header length is 0 and the jumbo option encodes the true length as a 32-bit value. This is an option that you can have a very big packet traveling down. It is also used to mark spanning trees for multicast and real time protocols etc. There may be things that you need to do at every hop. (Refer slide time: 21:25 -21:56)

Security is another area that was in focus. Security Association: We will talk about network security etc at length later on. There is a way to put authentication and encryption requires that senders and receivers agree on a key for encryption and decryption. And authentication or encryption algorithm, and set of ancillary parameters such as the lifetime etc. This is called security association. (Refer slide time: 21:57 ? 22:17)

Now, you have an Authentication Header where the security parameters may be mentioned namely the sequence number field, next Header, length and reserved. The SPI is selected by the receiver and is used to describe the security association where everything is normally negotiated during the key exchange. (Refer slide time: 22:18 ? 22:53)

There is Encrypted Security Payload. Headers entirely cannot be encrypted because then the intermediate routers will not be able to handle it. The last unencrypted header in the chain, this is an Encrypted Security so there would be encrypted data and authentication data, Also the ESP (Encrypted Security Payload) Header ESP header will be there. ESP Header also includes authentication to prevent tampering with encrypted data. We will talk in details about security in a later lecture. (Refer slide time: 22:54 ? 22:58)

To conclude this discussion about IPv6 this is really one scheme where people will not be running out of IP addresses. Then a funny thing happened in the sense that many of the hardware vendors like routers etc rather modified their design in order handle IPv6. However, actually what happened was that everybody is waiting for all others to switch from IPv4 to IPv6. When you switch you may have problems with some of your software or a lot of your software. If you only switch over to the other version that would not do because the rest of the world will still go with IPv4. You can still operate it through some bridge, through an IPv4, IPv6 etc but then nobody wants to do it unless other people are doing it. That is how everybody is held back for quite a few years. But one thing is that if there are ubiquitous kind of networking, in the sense that, not only your computers but all your devices like refrigerator, TV and Air Conditioner and everything in the house is networked then we will require a huge number of network addresses. Then people will not have any option but to actually make the move. Right now everybody is sort of waiting for other people to make the move. Next, we will come to the topic of mobile IP. What is mobile IP? Mobile IP means, now there are many network attachable devices.  It is not only the laptop computers people are carrying everywhere. Even apart from laptop computers there can be all kinds of devices including hand held devices which can be connected to a network. Now what is the problem if all these mobile devices are connected to the network? There is no problem as such, whenever you go there have to be some way in which a physical connection is made. That connection may be wireless in the case of mobile. The wireless connection is very attractive but otherwise you may go to some other place and actually connect a wire over there, it may be wired also, Although wireless is more dominant but the trouble is what happens to the IP address?. Your device has a particular IP address and that would have worked fine when you were at your home base. But you have moved from your home base to some other place. Now, if somebody wants to talk to you he will be using your IP address and that is what he is familiar with. For example, all the name servers etc will have the IP address corresponding to the URL if you have a URL and that is not going to change. They are going to try to use your old IP address but by using your old IP address they will land in your  home network where you are no longer available. This is the problem of mobile IP. When a particular network attached device moves from one network or one sub network to another network then how would you keep communicating? That is the problem of mobile IP. (Refer slide time: 25:36 ? 26:59)

These are the problems as I just now discussed. Nodes in the Internet are identified by specified IP address. Routing is performed using that same IP address. When a node?s location or attachment changes then routing will not work with the same IP address. That is a simple point (Refer slide time: 27:00 ? 27:18)

What are the alternatives? One is that, the node must change its IP address whenever it changes its point of attachment. It requires upper level protocols to handle address changes, that is one problem. This means, if it is to be made automatic then it has to be automated by a higher-level protocol which really sort of violates this layered architecture, that is one point. More importantly, what would happen is that the others who want to communicate with you know your IP address. They do not know that it has changed in the meanwhile so they would still try to communicate with the old IP address. (Refer slide time: 27:43 ? 28:54)

The other thing was that, Host specific routes must be propagated through the network. This is another possibility because from your IP address if somebody is trying to contact you from outside he first looks at the network part of the address and allows them into your network, then within the network, you have this ARP and other protocols to help you to get the MAC address and reach you directly. So the routing table essentially keeps track of all the networks as many as they can depending on what size the router is. The big routers keep track of many networks, the small routers keep track of only a few network addresses. If these entries were against Host then the routers might dynamically change their entry etc and route it directly to that host. However, even handling so many millions of networks is becoming a problem so handling billions of hosts in the routers is simply out of question. The solution to this is to use another level of indirection, that is what we do in mobile IP as I have just now shown. (Refer slide time: 28:55 ? 29:00)

Mobile IP Design Goals: A mobile node must be able to communicate with other nodes after changing its link layer attachment. Changing its link layer attachment is changing the attachment to the network or sub network to which it was originally attached yet without changing its IP address where its IP address remains the same. This is the problem. A mobile node must be able to communicate with other nodes that do not implement mobile IP. This is the other requirement. It means, you may do something very sophisticated and special in your hand held device but the point is that still it should be able to communicate with millions of other hosts who do not have any special arrangement for communicating with mobile IP. Therefore, you cannot do anything on the other end. (Refer slide time: 29:49 - 31:39)
      
Another point is that, this is a sort of security concerned that mobile IP must use authentication to offer security against Redirectment Attacks. The point is, when you are in your own network you can try to authenticate it apart from any other security arrangement that is present like your password may be at a higher layer. But the point is that it is also possible that you allow communication with that particular host which is in that network, So you will set up your firewall or router policy in such a way that, that particular communication will be allowed, may be communication from others will not be allowed. But the point is, if this fellow has moved to another network then you will not be able to do it using the network address, that is one aspect. The other point is, other people may fake from other places. For example, suppose I want to communicate with Mr X, then Mr Y from some other place may rather try to spoof; in the sense, they may try to show that he is actually Mr X. So I will think that I am communicating with Mr X but actually I am communicating with Mr Y. Therefore, anything might happen and security concern is also an issue. The number of administrative messages should be small to save bandwidth and power. You cannot have a huge overhead for doing this, Mobile IP must impose no additional constraints on the assignment of IP addresses, this is another important issue. (Refer slide time: 29:49 - 32:09)
      
Before describing how this mobile IP is implemented, let us discuss about some Terminology. One is the Mobile node that is a host or router that changes its point of attachment from one network or sub network to another. A mobile node may change its location without changing its IP address. It may continue to communicate with other internet nodes at any location using its own constant IP address. (Refer slide time: 32:09 - 32:42)
			
Home Agent: This is required in order to support mobile IP. Home Agent is a router on a mobile nodes home network that tunnels datagrams to the mobile node when it is away from home. You can immediately get the idea of how it is done. The point is that, this particular mobile device has a home network and that home network has a router and that supports mobile IP. What that home network router would do is, whatever communication is supposed to be received by this particular mobile device will come to its home network. The router will accept that communication on behalf of this mobile host that may now be away somewhere else. Then it would be the job of the router to send that communication back to that particular mobile host. Not only you require a home agent, That means, some router helping you and your home network, then you require a foreign agent. A router on a mobile nodes visited network means the network to which it is currently physically connected provides routing services to the mobile node while it is registered. For getting this service you must register with this foreign agent. (Refer slide time: 32:43 - 34:13)


The mobile node is assigned a care of address. This is a new address. One is the mobile nodes own IP address which is remaining constant that actually belongs to the network in its home base. It also has a care of address on the foreign network. This address is used to deliver the datagrams for the mobile node. This address can either be the foreign agent where the Foreign Agents address may be this care of address or it can be co-located with the mobile node.(Refer slide time: 34:13 to 34:50)


This is the idea you have, this is the home network of the device of A. Now A has moved to another network so this is the visited network of A. In the home network A has a home agent which will help you in this mobile communication. In the visited network it looks for and finds a foreign agent that will help you for this communication. This foreign agent will give that care of address and then both of them will be connected to the internet. (Refer slide time: 34:51 - 35:00)  

Suppose some source wants to send something to A, naturally it will use A?s original IP address so it will be routed to the home network of A. (Refer slide time: 35:01 - 35:15)






What will happen is that then the home network will send it to the home agent. The home agent knows that A is no longer here but it is somewhere else and the home agent also knows the care of address given by the foreign agent. (Refer slide time: 35:16 - 35:25)

He tunnels the communication to the foreign agent using the care of address. (Refer slide time: 35:26 - 35:33)

Then the foreign agent will deliver the message to A because foreign agent knows the A?s current location, MAC address etc where it can communicate. (Refer slide time: 35:34 - 35:52)

Now A replies to C but this can go straight. This need not go in the circuitous manner because he is using the IP address of the source of the original communication so A can send this reply directly back to the source. Hence, this need not go through the entire process. (Refer slide time: 35:52 - 36:05)
      
This is the solution in a nutshell, From the source, it goes to the home agent, to the foreign agent, to the node and from the node it directly goes back to the source for the return communication. (Refer slide time: 36:05 - 37:06)
      
A small overview of the Protocol, you have advertisement. That means the mobile agents the so-called foreign agents and home agents should advertise their services. That means the mobile node comes to know that this foreign agent or home agent is available, that this service is available. Otherwise, a mobile node can also solicit for mobility agents and that is possible. Registration: When a mobile node is away from home it must register its care of address with its home agent. So, not only it must set up some arrangement with the foreign agent to give it an address but also that address has to be sent to the home agent so that, whatever the home agent tunnels it will tunnel it straight to that care of address. (Refer slide time: 37:06 - 37:23)
      
Delivering Datagrams: Datagrams must be forwarded by the home agent to the foreign agent for delivery to the care-of address. The delivery mechanism must handle all packets including broadcast and multicast. A tunnel is used for this analogy. In a little while, let us see what a tunnel means. (Refer slide time: 37:23 - 38:05)
			
Advertisement and Solicitation: The router discovery ICMP protocol was adapted for advertisement and solicitation so not much of a change was required. We will look at the details of ICMP protocol later. The routers broadcast or multicast every few seconds. So it uses limited broadcast or all systems on this link, multicast kind of an address for giving this because they cannot use the IP address directly because it is an advertisement. Mobile nodes also send out solicitation messages that will cause a router to broadcast or multicast their advertisement. (Refer slide time: 38:05 - 38:39)
      
Registration: Request forwarding services when visiting a foreign network. This allocates a local foreign node address. That means a care of address is required. Inform home agent of their current care of address. This creates a binding of the foreign node address to the home address in the home agent. If anything comes destined for the original home address then this can be tunneled to the care of address. (Refer slide time: 38:05 - 39:16)
      
This is one small but important point that this binding has to be renewed from time to time. Bindings have lifetimes. This is important because mobile node may be rude and just go away without informing anybody and that registration will rather last forever, it cannot last forever. It is best that it dies down after sometime. If the mobile agent continues in the same location for more time, it is going to renew this binding from time to time. And of course you have to deregister when they return home. (Refer slide time: 39:16 - 39:26)
			
Tunneling: There are various methods of tunneling. We will just discuss this IP-in-IP encapsulation and minimal encapsulation. (Refer slide time: 39:16 - 39:26)
			
This is IP-in-IP: This was the original message sent from the source and this is what landed in the home network of the destination. If you remember, in the diagram the destination was marked as A. This IP header will contain the actual address of A and this is the datagram. What it does is, when it lands into the home agent the home agent knows that this has to be sent somewhere else. It keeps the inner IP header and datagram intact. This whole thing is considered now as a payload and then you add another IP header with some options if necessary. This IP header will have as its destination the tunnel endpoints, the tunnel destinations which is supposed to be the care of address. In the packet the original packet is still there, this inner IP header and the datagram etc and this whole thing is encapsulated as if this is a payload and sent to the foreign network in the care of address. It will reach the foreign agent and the foreign agent will then send this part to the mobile node who is currently connected and its MAC address is known to the foreign agent. The mobile agent or the mobile node will receive a whole packet including this inner IP header. So you do not require any kind of change in the software which handles it just like a normal packet. It is as if he was in the home network and got this is original packet. (Refer slide time: 41:15 - 41:50)

The outer IP header source and destination address identify the tunnel endpoints. The source would be the home agent and the destination would be the foreign agent. The outer protocol is 4 that is the IP protocol. The inner IP header, the source address and destination address identify the original sender and recipient, this is not changed by the encapsulator except to change the time to live. So for time to live you have to look at the TTL and then make the necessary changes. This whole thing is put in the payload. (Refer slide time: 41:51 - 42:16)

Other headers for authentication might be added to the outer header in order to handle all these security concerns. Some outer IP header fields are copied from the inner IP fields. For example, type of service etc most are recomputed like checksum length etc may change based on the new datagram. (Refer slide time: 42:17 - 43:09)

The other option is the minimal encapsulation. Minimal encapsulation means that you do not keep the entire IP header intact here. So, what you want to do is that, you want to retain the minimal information in the minimal header and then construct an outer IP header. For the outer IP header the tunnel endpoints as the source and destination address would still be there and some of the stuff from the IP header will also come here. The destination address will be there in the minimal header. You have to make some deconstruction and reconstruction at both places. The size is a bit smaller so the overhead may be a bit smaller but it may not be such a big deal. (Refer slide time: 43:10 - 43:30)

In Minimal Encapsulation, we copy inner header. Modify protocol field to be 55 for the minimal encapsulation protocol because on the other side it must know which protocol it is following. If it is following minimal encapsulation then it has to do something. Destination address is replaced by the tunnel exit. (Refer slide time: 43:31 - 43:50)

If encapsulator is not the originator of message, replace source address with address of encapsulator. Then increment total length by the size of the additional header by 12 or 8 octets and then re-compute the checksum. This is called mobile IP in one way in which mobility can be handled and your IP address can be recomputed. There are other possibilities and other ways of handling mobility. For example, this has an overhead that any communication from the source to the intended host that has moved, now has to go through this triangular path. Will it continue to do so or whether after first communication there would be some protocol to exchange their new IP addresses etc? Then, they can communicate directly, that would avoid this triangular path. The other problems with triangular path may be apart from higher overhead. It may exceed the hop limit, as networks are growing it may increase the hop limit and you may never reach whereas if it had gone directly then it would have reached. Other options could be just like you do handoffs in cellular from one base station to another. In the case of cellular networks what is happening is that, you are always in connection with some base station, may be even more than one base station. If you are moving away from one base station when the signal strength drops then it goes to the realm of another base station, and the other base station automatically picks up and does some kind of registration. When this is done, the communication remains direct. But, if you want to change the IP address in such a dynamic fashion then there has to be an integrated system running everywhere which is using this protocol. Mobile IP is a way of handling mobility with minimal change to others and the problem is that this has a significant overhead. In the next class we will be moving into the next higher layer which is the Transport Layer the TCP and UDP, thank you.

LECTURE 30 UDP AND CLIENT SERVER

GoodGood day, today we will start our discussion on Transport Layer Protocols and there are actually two dominant protocols UDP and TCP. We will take them up one by one. Let us look at UDP in this lecture and TCP in the next one. (Refer slide time: 46:56 ? 47:02) 

UDP stands for User Datagram Protocol, (Refer slide time: 47:03 - 47:15)

This is a Transport Layer Protocol and this has got the following responsibilities. First of all It creates a process-to-process communication path. Till now we have talked about the network layer and the job of the network layer is to connect a distant machine to another distant machine. it?s a machine to machine communication. Whereas now we are talking about, process to process communication. In this particular source machine, some application process is running which is trying to connect the other distant machine for some job. This process has to connect to the corresponding process there which may be a particular application server on one side and the application client on the other side, whatever that application may be. So this is a process to process communication path..(Refer slide time: 47:56  - 48:06) 

This also has to provide control mechanisms at the transport level. This control mechanism in the case of UDP is very minimal, as we will presently see. (Refer slide time: 48:07 - 48:19)

UDP is a connectionless, Unreliable Transport Protocol. Immediately, the question that would come in your mind is that, why would we try to have an unreliable protocol? This is not unreliable per say, the point is, it does not do anything extra for reliability making it a very lightweight protocol, the overhead cost is very low. In many cases, this may be a very reasonable thing to have where you do not expect lot of errors or you do not really care if some error occurs from time to time and in such cases, you may use a UDP. (Refer slide time: 48:58 - 49:12)


This is a connectionless protocol. It only adds process-to-process communications to IP. It performs very limited error checking as we have mentioned. It is a very simple protocol having minimal overhead. This is the main point. It forms the payload for the next layer that is the IP layer and the checksum is computed over this entire body. So there is some amount of error checking and error detection  done by UDP and that is the extent to which it will go for providing reliability. Beyond this if the entire packet is lost somewhere the UDP can not do anything about it. (Refer slide time: 49:43 - 49:50)

These are the four fields of the Header, Source Port Number, Destination Port Number, Total Length and the Checksum. (Refer slide time: 49:53 - 50:16)

And regarding the UDP operation this is a connectionless service. This has minimal flow and error control as given by the checksum. It does the Encapsulation and Decapsulation, forming of packets, it uses some queuing and does the multiplexing and demultiplexing. Let us look at the operations one by one. (Refer slide time: 50:17 - 52:12)  

This is a connectionless service. That means each user datagram sent is an independent datagram. It means that, suppose some particular application has sent one UDP and is going to send another one, now the layers below this application may be coming from the same source application process destined for the same destination application process which are the two datagrams. They are going to be treated independently by the rest of the network layers. This means a number of things. First of all It may so happen that these two packets may go in two different directions, may be route differently because there is no connection. This is a completely a Datagram Oriented Service, Connectionless Service so these two datagrams may travel in different paths. Secondly, one of them may get lost. Thirdly, what might happen is, they may go out of order, the datagram that was sent earlier may reach later. The point is that, for all mishaps UDP is not going to take any responsibility. It is taken for granted that whatever application takes place using this UDP is resilient to such events. There is no relationship between different user datagrams. The user datagrams are not numbered, meaning that, the datagram which was sent later if it arises earlier and vice versa then there is no way of knowing unless you have taken some care to identify that in the application layer itself.  (Refer slide time: 52:13 - 52:46)

No connection establishment, since it is completely a connectionless service there is no question of any connection establishment. And since there is no connection establishment there is no connection termination either. These are unregulated which means that up to port number1023 these are reserved and that is also again divided into two parts. One part is for public applications and the other for some vendor specific applications but they are all well-known port numbers. Now, think of the other direction, apart from well-known port numbers you also need a whole lot of other port numbers. Take the previous example that we have made an HTTP request to a web server, now the web server will send you back something. May be it will send you with the content of the first page of its website. This is going to be sent to the requester but to which port? For this, another port number is temporarily assigned. This is assigned from a number range from 1024-65,000. The number is randomly chosen so this is an ephemeral port and not a fixed port. For the duration of this communication this port number is going to be held constant and then it will be released for use by some other process. (Refer slide time: 53:58 - 54:12)  

Source port numbers are dynamically assigned by the originating host, and are usually a number larger than 1023. Port numbers in the range of 0 -1023 are controlled by IANA. (Refer slide time: 54:13 - 55:46)

these are some examples of some well known port numbers. There are a quite a good number of them but I have just mentioned some important protocols. For example, FTP, a File Transfer Protocol uses port number 21. TELNET, a Terminal Connection uses port number 23. There are hundreds of applications that has come up. We cannot talk about all of them but we will talk about a few of them towards the last part of our course. .For the time being, let me just mention them. TELNET is the Terminal Connection which uses the well known port number 23. SMTP is a Simple Mail Transfer Protocol that uses port number 25. TFTP Trivial File Transfer is used when you just have to send a short message that uses 69. HTTP is the Hyper Text Transfer Protocol used for web services that uses the well-known port number 80. POP 3 is a Post Office Protocol that uses a port number 110. What POP 3 does is that, suppose you got some mail in your mailbox in the local mail server then on your desktop you can download all the mails from the local server to your machine through the Post Service Protocol. This is the POP3 Protocol. This type of server is called concurrent. Just to elaborate on the server part a little bit more then what I have already discussed, the client request for a connection has come to the server. Now what is the server in this case? When I mention the term server I mean that software process which is running there and not the hardware box. A hardware box is also called a server in a different context. In our context by server I mean the process which is giving the service, So this is some kind of process which is running in a particular machine. Now, in the non-concurrent case what will happen is that all the user requests will come and they are sort of put in a queue. And now what the server process will do is that, it will take up one from the queue, process the service, then give it back and send the result. Then it will take the next one out of the queue. So there is a queue where all the client requests are waiting. And the server, that means the service process which is giving the service is taking one request at a time out of the queue. This is called a non-concurrent server. Non-Concurrent in the sense that when you are using sock D?gram that is a UDP kind of service, it is one of its kind where you get a request, send a message and may be that is the end of the service. In that case this non-concurrent servers, also called iterative servers are more efficient. But it may also happen that, in a particular service the client server communication is for an extended period of time in which case one particular request may block all other requests for an unnecessarily long time. In that case the concurrent server may be preferred. In concurrent server what happens is, as soon as the server gets a request at the well-known port it immediately spawns or forks.

NOTE: REGARDING VIDEO CONTENT
IN THIS VIDEO AFTER 45 MINIT THE NEXT LECTURE IS STATED AFTER THAT THERE IS A SWITCHING OF SOME OTHER TOPICS WHICH IS NOT RELATED TO EACH OTHER



COMPUTER NETWORKS
Prof. S. Ghosh
Dept. of Computer Science and Engineering
I.I.T. Kharagpur
Lecturer#30
UDP and Client Server

Good day, today we will start our discussion about transport layer protocols. There are two dominant protocols the UDP and TCP. We will take them up one by one. Let us look at UDP in this lecture and TCP in the next one. 


Slide 1
UDP stands for User Datagram Protocol. 

Slide 2

This is a transport layer protocol and has got the following responsibilities: 
First of all, it creates a process to process communication path. Till now we have talked about the network layer and the job of the network layer is to connect a distant machine to another distant machine. So it is a machine to machine communication whereas now we are talking about process to process communication. So in this particular source machine may be some application process is running which is trying to connect to the other distant machine for some job. So this process has to connect to a corresponding process there which may be a particular application server on one side and application client on the other side whatever the applications may be. This is a process to process communication path. 
This also provides control mechanisms at the transport level. The control mechanism in the case of UDP is very minimal. 


Slide 3
UDP is a connectionless unreliable transport protocol. But why would we try to have an unreliable protocol? This protocol is not exactly unreliable but it does not do anything extra for reliability making it a very light weight protocol. So its overhead cost is very low. In many cases it may be a very reasonable thing to have where you do not expect lot of errors or you do not really care if some error occurs from time to time. In such cases you may use UDP. Functions of UDP:
- It only adds process to process communications to IP. 
- Performs very limited error checking. 
- Very simple protocol and has minimal overhead. This is the main advantage. It has very minimal overhead. 


Slide 4
When you are talking about the IP protocol you are talking about the connection from one machine to another. But in this particular machine a number of application programs or processes may be running. This UDP protocol connects one process to another process, so does TCP and that is the job of the transport layer. Although UDP is a connectionless protocol the job of the transport layer is to make some virtual connection or some virtual communication channel available to the corresponding processes. 


Slide 5

So to summarize: 
- IP is responsible for host to host communication. 
- Message still needs to be handed to the correct process. 
- UDP is responsible for delivery of the message to the appropriate process. Actually, not only you need to understand that there is some kind of multiplexing and de-multiplexing going on. In the same server a number of processes may be running and out of all those processes a good number of them may be using the same UDP protocol. So, when sending out a packet it is alright but when receiving a packet the UDP protocol has to determine as to which process it will go to and that is one of its jobs and the other job is to make connections.  
There is a point over here and this is a common task between TCP and the UDP. Our computer network is mostly a packet switched network whereas as far as applications are concerned mostly they do not really bother about packets. They may produce a chunk of data to be communicated to the other side. They may even produce a stream of data to be communicated to the other side so it does not work with packets. So somebody has to take this stream of data from the application layer and chop them into small packets and that is a job of the transport layer. So, both the TCP and UDP do that. 


Slide 6
- UDP is a transport layer protocol within the TCP/IP protocol suite. 
- It is simpler than TCP. TCP has higher overhead, it is more reliable than UDP. If you want to have reliability and still want to use UDP then you have to take care of reliability in some other layer may be application layer or something. 
- UDP lies between the application layer and the IP layer and like TCP serves as the intermediary between the application programs and the network operations. 


Slide 7
If this is your IP datagram, the IP datagram will have an IP header and the payload for the IP would be the entire UDP datagram. The UDP datagram would have the UDP data which it derives from the application and the UDP header. 


Slide 8
When a particular application wants to communicate to another application or when a process is trying to communicate to another process which is in a remote machine, in that case it will want to talk to some particular machine. Up to whatever we have seen, a particular machine means a particular IP address. Now this IP address is handled by the IP layer that means by the network layer on both the machines. Then how does the network layer get this IP address? As such it is supposed to get the IP address, the destination source and destination IP address from the top but there is a transport layer between the IP layer and the application layer. We are talking mostly about the TCP/IP protocol stack so we are not considering talking about OSI protocol stack and presentation layer for the time being. The destination IP address is known to the application but it has to be communicated to the network layer. The transport layer has got nothing to do with the IP addresses therefore the application layer does communicate the source and destination IP addresses to the transport layer and the transport layer passes it on to the network layer and does not do anything with it except considering it for the check sum. So this is known as IP pseudo header because this is not a real header for the UDP protocol or TCP. 


Slide 9
This IP pseudo header contains the 32-bit source IP address, 32-bit destination IP address and then this 16-bit UDP length etc. So this is the header which is passed on to the IP layer and this is in the same stack in the same machine. These headers are actually meant for communication between peers that means the transport layer, the UDP on this machine and the UDP on that machine will communicate via this UDP header so this is the proper header. And the pseudo header just takes that information about IP addresses etc from above and passes it below. 
What the real UDP header uses to communicate with its peer on the other machine contains a 16-bit source port number and 16-bit destination port number. Remember, these were 32-bit source IP address and here these are 32-bit destination IP address. There is also the port number here which we will see later. We have 16-bit source port number, 16-bit destination port number, 16-bit UDP length, optional 16-bit UDP checksum which may be optional and data if any, possible odd bytes and a pad. 


Slide 10
So this is the length of the header and data. Therefore we have the source and destination port numbers, total length and checksum.


Slide 11
Let us go through this in detail. The source port number has 
- 16-bits. 
- Range from 0 to 65,535. 
- Port number used by the process running on the source host. 
Recall the multiplexing and de-multiplexing we discussed earlier. In the same machine number of processes may be communicating and out of all these processes a good number of them may be using the same transport protocol namely UDP while others could be using the transfer protocol TCP. 
When a particular packet comes, you can see whether it is a TCP packet or UDP packet. Once you make it out, you have to decide that out of all the processes using UDP for which process is this packet meant for. All these different processes are associated with different port numbers and looking at the port number the UDP decides the process for which it is meant for and this is the de-multiplexing part. But the multiplexing part comes with the source port number. Hence there is a source port number and the destination port number used by UDP for multiplexing and de-multiplexing at its own level. This is a number which is 16-bits long which will give you from 0 to 65,535. These are the port numbers and port number is used by the process running on the source host. Later on let us see in detail on how this port number is obtained.


Slide 12
Similarly, there is a destination port number: 
- It has got 16-bits. 
- Port number used by the process running on the destination host. 
- In most cases, it is a well known port number. We will what is meant by well known port number.


Slide 13
- The length is 16-bits. 
o It defines the total length of the user datagram, header plus data. 
o Note: Maximum size of data is 65,507 after subtracting 20 bytes for IP header and 8 bytes for UDP header as this is the overhead. 


Slide 14
- Checksum is 16 bits
o Used to detect errors over the entire user datagram. 


Slide 15
- Checksum includes three sections: 
o Pseudoheader UDP header and the data. 
* Part of the header of the IP packet. 
* Ensures that if the IP header is corrupted the user datagram is not delivered to the wrong host. Since the pseudoheader contains the destination IP address, even if the IP header gets corrupted this does not get delivered to the wrong host. The pseudoheader, UDP header and the data taken together forms the payload for the next layer that is the IP layer and the checksum is computed over this entire body. So there is some amount of error checking and error detection done. This is done by UDP and this is the extent to which it will go for providing reliability. Beyond this if the entire packet is lost somewhere, UDP cannot do anything about it. 
o UDP Header - There are the four fields in the header, source port number, destination port number, total length and the checksum.


Slide 16
 UDP operation: 
- It is a connectionless service. 
- Has very minimal flow and error control as given by the checksum. 
- Performs encapsulation and decapsulation and forming of packets. 
- Queuing. 
- Multiplexing and De-multiplexing. 
Let us look at these operations one by one.  


Slide 17
Connectionless service: 
- Each user datagram sent is an independent datagram. If an application sends one UDP and is going to send another then they are handled independently in the layers below this application. These two datagrams may be coming from the same source application process meant for the same destination application process, but they are going to be treated independently by the rest of the network layers. First of all, it may so happen that these two packets may go in two different directions, may be routed differently because there is no connection. This is completely a datagram oriented connectionless service. Therefore these two datagrams may travel in different paths. One of them may get lost or they may get out of order which means the datagram that was sent earlier may reach the destination quite later. UDP is not going to take any responsibility for all these mishaps and it is taken for granted that whatever application takes place using this UDP is resilient to such things.
- There is no relationship between different user datagrams. 
- User datagrams are not numbered. Meaning that if the datagram which was sent later arrives much earlier and the vice versa then there is no way of knowing unless in the application layer itself we have taken some care to identify that. 


Slide 18
- There is no connection establishment. Since this is completely a connectionless service there is no question of any connection establishment, and since there is no connection establishment there is no connection termination either. 
- Each user datagram can travel on a different path. 
Only processes sending short messages should use UDP. Usually UDP is used in a case where you send one packet and that is the end of it. When you are trying to the send a stream of packets or stream of bytes etc usually you do not use UDP.- 

Slide 19
As far as flow and error control is concerned which may be another job of the transport layer the UDP does very little. 
- It is very simple and an unreliable transport protocol. 
- There is no flow control. 
- The receiver may overflow with incoming messages. 
- There is no error control except for the checksum. 
What happens is that, suppose the receiver is getting a large number of UDP packets, and if it wants to just stop or slow down to the senders it cannot do that. If it overflows some of the packets will get lost. And then there is no error control except for the checksum. 


Slide 20
- Sender does not know if a message has been lost or if it has been duplicated. 
- An error in the checksum causes a user datagram to be silently discarded. This is a very simple protocol and this is very efficient and has a very low overhead.


Slide 21
UDP does encapsulation and de-capsulation which is a fundamental job of any protocol in the transport layer. It has to form the packets, the packets form from the streams of data supplied by the application layer. The data is chopped into pieces, a header is added to each piece and then is passed on. To send a message from one process to another the UDP protocol encapsulates and decapsulates messages.   


Slide 22
This is just a standard way as it goes down the stack, the message from the process, so this is the UDP data, this is the UDP header, then the IP header frame header etc and it is decapsulated in a similar fashion.


Slide 23
- Queuing: 
- Each port has an associated incoming queue or incoming and outgoing queue depending on which way the communication is taking place. A port is not just a number. Along with the particular number, inside the OS there will be a queue of data being communicated in the machine. There will be an incoming queue and an outgoing queue. 
- UDP removes message from the outgoing queue one by one. 
- Adds the UDP header. 
- Delivers them to the IP. 


Slide 24
Incoming queue: 
- UDP checks to see if an incoming queue exists. The first thing UDP does is to check whether the incoming queue exists. 
- If there is, UDP sends the user datagram to the end of the queue. The particular application process is going to consume from this incoming queue. The process will consume from this incoming queue which is being fed from the UDP. For the outgoing queue, the application process is going to put things in the queue and UDP will take out the message to be communicated from the queue. 
- If there is no such port then UDP discards the user datagram and asks ICMP to send a port unreachable message. This means that a particular packet has come meant for a particular port and that port does not exist in this machine so it will send them an error message saying that the port is unreachable. In order to generate this ICMP message sometimes a packet is sent to an improvable port number. So, when you get back you know that you have reached the destination but of course there is no application over there. 


Slide 25
It does multiplexing and demultiplexing.
- Several processes may want to use the services of UDP. 
- UDP multiplexes and demultiplexes to handle this. 


Slide 26
Use of UDP:
- This is suitable for processes that require simple request-response communication. Suppose there is a simple request which may be sent by UDP and there is just one message as response to this request then that may also be sent back as an UDP. This also depends on the kind of network you have. Suppose you are communicating only inside the LAN, in that case you may not like to have an overhead since this is not going over the WAN. Therefore this is expected to be much more reliable and you may not like to incur any extra cost for providing reliability etc because you know that the underlying network is quite reliable. And secondly if your process is such that there is a simple message coming and there is a simple response, in that case a simple UDP protocol may be sufficient. 
- This may also be suitable for processes which include internal flow and error control mechanisms. That means, if the application process itself is handling some flow and error control, that means if it is handled at an upper layer then you do not want to duplicate it in the transport layer, in which case you use of simple protocol like UDP which is more efficient because reliability is being handled by somebody else anyway. So that is another case where UDP is a very suitable protocol. 
- UDP may also be suitable for multicasting and broadcasting. Going to some specific examples: 


Slide 27
- This is used for management processes such as SNMP. SNMP is a Simple Network Management Protocol used for managing network. For managing network we have this central network management software which from time to time may probe different network boxes to see if their health is alright, collect all kinds of statistics etc. So this is a simple message response kind of system. It asks for queries, manages a particular device in the network by sending a message and that device responds with some statistics or some alarm or whatever it is. So that is a case where UDP protocol may be quite suitable. 
- This is used for some route updating protocols such as RIP. We have already discussed RIP. For updating the routes the routers have to communicate with each other and that uses UDP. These are just two examples. There are many other examples and user applications which may also use UDP. 


Slide 28
Now let us discuss about port numbers and then I will talk about client server.  Port Numbers:
- Local host and remote host are defined by IP addresses. Here the host refers to the machine. 
- A second identifier called port number is required to identify processes because many processes may be running on the same host. So just identifying the host is not enough but we have to identify the process in this host. Or more specifically you have to talk about this process and this host. So you have to have a port number as well as an IP address. 
- In TCP/IP port numbers are integers between 0 and 65,535. 


Slide 29
- The server process requires a well known port number. 
- The client process defines a port number chosen randomly by UDP which is known as an ephemeral port number. 
We will discuss this in more detail when we discuss more about client server. This is one of the ways you write applications in a network environment by using the client server paradigm. There will be different servers giving different services like you may have a web server giving you web services or mail server etc. Different clients could be using these services. So they use different sets of port numbers. This will be clear when we talk about the client server paradigm in more detail. 


Slide 30
Once again, if you think about the IP address, this selects the host and then the port number selects the process. When you give the destination IP address etc it reaches a particular host and in that host it looks at the port number and then selects that process using that particular port number.


Slide 31
- Port numbers work as source and destination addresses for TCP/UDP segments.
- Ports ensure packets reach appropriate service on the server. 
- The destination port field determines which service the source is requesting. 
- TCP/IP associate ports at the transport layer with certain applications.


Slide 32
- Software developers have agreed on well known ports, for example: 
o A packet bound for an FTP server would use port 21. Just think of some network service, a web service. When you are surfing the net you will use a browser and you may click on a particular URL or network address and you immediately get the opening page of that particular site displayed by your browser. If you think about how did this happen? A particular site is hosted in some remote machine somewhere. You may know only its IP address. Now, by knowing its IP address you have to make a request to that IP address, like requesting it to show the opening page. Now the IP address will help you through this mace of routers who may be running some routing protocol like RIP, OSPF so that your request reaches the destination machine. But in the destination machine how does it know the port number? You do not know about the destination machine, number of processes running etc. Suppose if it is a HTTP request that means it is a request for a web page to a web server then this request has to reach the web server. Now there may be ten or fifty other processes running on that same machine, it should not go to any of these other servers. So, for very standard applications like FTP (File Transfer Protocol), SMTP (Simple Mail Transfer Protocol), HTTP (Hyper Text Transfer Protocol) and so on are all applications. So, for all these protocols the port numbers are fixed. So, if I make an FTP request, as it says FTP server use a well known port 21. That means if I make an FTP request to any machine anywhere in the world I am going to use a port number 21 knowing that this is a well known port number and if it is reaches the destination machine port number 21 would mean the FTP server. 
- Conversations that do not involve applications with well known ports are assigned ports randomly selected from a specific range. 



					Slide 33
There is an organization IANA which handles these port numbers. So port numbers have the following assigned ranges: 
- Below 255 (0 to 255) are reserved for public applications like FTP, SMTP, HTTP etc. 
- From 255 to 1023 these are assigned to companies for marketable applications.
- Above 1023 these are unregulated which means that up to 1024 these are reserved. This is again divided into two parts, one part for the public applications and the other for some vendor specific applications. Apart from well known port numbers you also need a whole lot of other port numbers. Take the previous example where we made an HTTP request to a web server. Now the web server will send you back something, may be the contents of the first page of its website. And that is going to be sent to the requester. For this another port number is temporarily assigned and this is assigned from a number range from 1024 to 65,535 and the number is randomly chosen. This is an ephemeral port, it is not a fixed port and it will be held constant for the duration of this communication and then it will be released for use by some other process.   


Slide 34
- Source port numbers are dynamically assigned by the originating host and are usually a number larger than 1023. 
- Port numbers in the range of 0 to 1023 are controlled by IANA. 


Slide 35
These are some examples of well-known port numbers. There are quite a good number of them but I have just mentioned some important protocols. 
- FTP is a file transfer protocol uses port number 21. 
- TELNET, a terminal connection uses port number 23. I will be talking more about some of these application layer protocols in a later lecture. There are hundreds of applications which have come up and we cannot talk about all of them but we will talk about few of them towards the last part of our course. 
- SMTP is a Simple Mail Transfer Protocol that uses port number 25. 
- TFTP is a Trivial File Transfer, when you just have to send a short message it uses port number 69. 
- HTTP is the Hyper Text Transfer Protocol which is the one used for web services that uses well-known port number 80. 
- POP3 is a Post Office Protocol that uses the port number 110. What POP3 does is that, suppose you got mail in your mail box in the local mail server then on your desktop you can download all the mails from the local server to your machine through the Post Office Protocol. 
- SNMP is used for network management that uses port 161. 
There are a whole lot of others but I have mentioned only a few which are important.  


Slide 36
Now moving on to the full description of client server programming it uses what is known as a socket address. 
Socket Address:
- Socket address is a combination of an IP address and a port number. 
- This uniquely defines each client and server process. So, this process is in this machine. Therefore this process is given by the port number and this machine is given by the IP number. 


Slide 37
These are some of the references for the different RFCS and the UDP is described in RFC 768. Then there is RFC 1122 and IANA port numbers can be seen in the websites mentioned here.


Slide 38
Let us see some more about client-server paradigm which is sort of universally used for network based application. 
- Server application is listener 
o Waits for incoming message 
o Performs service 
o Returns results 
- Client application establishes connection: 
It sends the message to the server and then ito  waits for a return message. 
In this client-server paradigm usually what happens is that, the client would initiate the request. It initiates the request for some service. It has to know the address of the service and if it is a well-known service then it will use the well-known port number. So in that particular machine in that particular port number it will make the request. And the server will accede to the request, in the sense that it will give the service and then return the result to the client and then it goes back to listening. So, the server is always in the listening mode that means it is waiting for a particular request to come in. 
 

Slide 39
- Clients and servers exchange messages through the transport protocols e.g. TCP or UDP. 
- Both client and server must have the same protocol stack and interact with the transport layer. So you may have a client server using both TCP and UDP. 


Slide 40
- Protocols specify general operations and the API specifies exactly how it is done. For using the socket there is a socket API. API stands for Application Program Interface. 
- So the socket API is the de facto standard. 
- Origin is in the BSD UNIX from University of California at Berkeley as part of one of the first versions of TCP/IP. 
- 

Slide 41
Let us see a little bit about the sockets and socket libraries: 
- This is a vendor supplied library of procedures. 
- They have the same name and arguments as one of the socket functions. 
It promotes independent source code and it is - usually easier to use than original sockets. 
The vendors also provide socket libraries with some bells and whistles. 


Slide 42
- Socket software interface is designed to communicate between the user program and TCP/IP protocol stack. Internally that is the socket. All TCP/IP stacks that we talked about right from the bottom one to right up to the transport layer is already in the system. Now you want to develop an application which should be run over the network. In such a case you have to write your application program for any service you would like to give. Now this user program has to communicate with this TCP/IP protocol stack which is already in the machine and the intervening layer is the API for sockets so you go to this TCP/IP layer stack through the sockets. 
- Therefore socket is a data structure inside the program. 
- Both client and server programs communicate via pair of sockets. 


Slide 43
There are several significant socket domain families: 
- Internet domain sockets implemented via IP addresses and port numbers. 
- UNIX domain sockets implemented via filenames. 
- Novell IPX Apple Talk etc. 
All these other vendor specific socket domains are also there. But the first two are the most important. 


Slide 44
There are three types of sockets: 
- Stream: It uses a TCP protocol. Stream socket is a connection oriented service. So, naturally you cannot use stream socket with UDP. Hence the transport layer protocol to use with this stream socket is TCP. 
- Datagram: In the datagram socket there is sock datagram that uses UDP protocol. The type of socket determines the type of protocol it is going to use TCP or UDP. But this uses the UDP protocol. 
- There are the raw sockets which may be used for testing anything internally. 


Slide 45
For creating a socket you have to have a library and for accessing the functions in the library which is specifically that socket you have to include that <sys/types.h> and <sys/socket.h> assuming that you are writing in c language. So you may have a function called something like this:
int socket (int domain, int type, int protocol). 
When you give this socket call, you have to mention its domain, its type and its protocol. 
- Domain is one of the protocol families like PF_INET, PF_UNIX etc so these are the socket domain families we talked about.  


Slide 46
- Type defines the communication protocol semantics. usually it defines either: 
o SOCK_STREAM that means connection oriented stream like TCP or 
o SOCK_DGRAM which is a connectionless unreliable UDP. 
- Protocol specifies a particular protocol, just set this to 0 to accept the default.  


Slide 47
What happens in the client-server is, when the client is initiating a request for a standard kind of a service it knows the destination IP address and the well-known port number And this message has reached the destination machine. And in the destination machine the server program always listens to that particular well-known port number. So the server program is actually in a loop listening whether any request is coming in through that port number. In the case of a UDP server this may be simple. It gets some request, it immediately gives a very short response and goes back to listening. 
But in such a case when the interaction with the user may be prolonged, for example, the user makes an FTP request and then downloading the file is going to take quite a bit of time. Similarly, the user may have made an HTTP request which means it may have asked for a particular webpage. Now, sending that webpage is also going to take some time. Of course it is always with respect to the kind of speed people are accustomed to. 
If the communication is always through that well-known port then during the currency of this particular session between this particular client and the server that well-known port number is going to be blocked and others will not be able to get the service which is not acceptable. 
So usually in the client-server there is a handshake. So the request comes to the well-known port number and of course there is a port address which has been sent by the client. From the server side the response goes back with another ephemeral port number which is not a standard port number. Now, on both sides you have two ephemeral port numbers that means the port number more than 1024 and they can then communicate through these two port numbers with source and destination respectively depending on from which side it is being sent. So these two different port numbers are used. This is happening on one side and on the other side the original server goes back to listening to that same well-known port number ready to service the next incoming request. 


Slide 48
Although we have not discussed TCP as yet but so far as this client-server part is concerned both of them are very similar. 
TCP Server:
- Sock_init() - creates the socket 
- Register port with the system 
- Establish the client connection 
- Accept the client connection 
- Read/write data 
- Close() is for shutting down 
When you establish the client connection and when you accept the client connection in this part all these two ephemeral port numbers are exchanged. This is on the server side. On the client side it creates the socket and tries to setup a connection. Usually in a client-server, the initiation of the connection is always from the client side. Then the write/read goes on and then there is a shut down. 


Slide 49
UDP clients and servers are similar except that it uses SOCK_DGRAM instead of SOCK_STREAM. 
- Connectionless clients and servers create sockets using SOCK_DGRAM. 
- Connectionless servers do not call listen() or accept() and usually do not call connect(). 
This means you may not require any specific handshake and you may not require a special ephemeral port for prolonged communication between the client and the server because in this case usually the service is to just send only one message and ending the service. 


Slide 50
- Since connectionless communications lack a sustained connection several methods are available that allow you to specify a destination address with every call: 
o sendto ( sock, buffer, buflen, flags, to_addr, tolen); 
o recvfrom (sock, buffer, buflen, flags, from_addr, fromlen); 
- There is a way to specify a destination address with every call. 


Slide 51
For UDP server the sequence of calls is the following: 
- Create a socket. 
- Register the port using the bind command. 
- Receive or send data. 
- Shutdown. 
This is somewhat simpler than the TCP server. And then there is the UDP client that you create, send or receive and then you shutdown. 


Slide 52
There are two types of servers. Several clients can request service of the server in the same time. In this case a server can: 
- Service one client at one time. Other client requests must wait. This type of server is called non-concurrent. The second type is,
- Services all client requests are handled simultaneously. This type of server is called concurrent. 
This is what happens, the client?s request for a connection has come to the server. By the term server, we mean that software process which is running there and not the hardware box. The hardware is also called a server but in a different sense. In our context by server we mean the process which is giving the service and which is running in a particular machine. 
If this server is non-concurrent all the user requests come and are sort of put in a queue. Then the server process will take one from the queue, process the service and then send the result and take the next one out of the queue. So there is a queue where all the client requests are waiting and the server process takes one request at a time out of the queue. This is called a non-concurrent server. 
When you are using UDP and when using SOCK_DGRAM that means you get a request, send a message and may be that is the end of the service. In that case this non-concurrent server also called as iterative server is more efficient. But it may also happen that, in a particular service the client server communication is for an extended period of time. In this case, one particular request may block all other requests for an unnecessarily long time. In that case concurrent server may be preferred. 
In concurrent server, as soon as the server gets the request at the well-known port it immediately spawns a new process. When you execute a fork, in say unique, what you get is an exactly similar piece of code to which you make some changes. What you do is that you give them a new port number and let this new process communicate with this particular client?s request and the original process goes back to listening to the well-known port. 
Another request may come from some other client somewhere else so it will again spawn another process and the original process will go back listening to the well-known port. All these child server processes use different ephemeral port numbers to communicate with different clients. In the strictest sense if you have only one particular processor in the server machine then only one program can run, things cannot be concurrent but have to be sequential. But then, that sequentiality is imposed by the way processors are scheduled by the scheduler inside the ?. So it will give some milliseconds for one process and then it gives few more milliseconds for some other process etc so it will look as if all the clients are getting the service simultaneously. So that is what is meant when we say client requests are serviced simultaneously.  


Slide 53
These are the non-concurrent server sequence of calls. The only thing to note is that, there is a while loop and this loop will service each request sequentially.  


Slide 54
The concurrent server uses the fork and in the fork it creates a child process which will now communicate with the client. With this we come to the end of this lecture. In the next lecture we will discuss the TCP protocol. 


Slide 55


Slide 56


Slide 57
Our topic today is TCP, the second most important transport protocol. It is very widely used in many applications. This is a little more complex than UDP but it also has some advantages. We will see what they are and look into the transport layer responsibilities.  


Slide 58
If you remember the transport layer responsibilities, they
- Create packets from byte stream received from the application layer. 
- In order to multiplex and demultiplex amongst various applications it uses port numbers to create process to process communications. 
- Uses a sliding window protocol to achieve flow control. 
- Uses acknowledgement packet, time-out and retransmission to achieve error control. 
So unlike UDP which is unreliable, TCP seeks to provide a reliable communication so that it is error free. It is a connection oriented protocol and it also has some kind of congestion control mechanism. And of course it does the basic thing of making connection between processors amongst two distant nodes possible. Full duplex communication: This means A is communicating to B and B is communicating to A at the same time. Even if predominantly only one side is sending data to the other side the acknowledgement is coming from the other side anyway. 


Slide 59
- The connection can be terminated from both sides but then somebody has to initiate the termination. 
- If connection is terminated in one direction data can continue to be sent in the other direction. 


Slide 60
Four actions are required to close the connection in both directions. Let us assume a bi-directional connection that has to be closed. 
- First host A sends a segment announcing connection termination which means it sends the segment contains FIN. You remember all the SYN, FIN etc, are the flag segments in the TCP segment header. 
- Host B sends a segment acknowledging the request from A. After this the connection is closed in one direction.   


Slide 61
- When host B has finished sending data it sends a segment indicating connection closure. 
- Host A acknowledges the request from B.
- Second and third steps cannot be combined together because although we are sort of allowing the termination of connection from one side but from the other side he may have acknowledgements or other things to send to this side so he will not terminate the connection. So these two cannot be combined together. The third step can be taken only when host B has finished sending data from its side and it sends a segment indicating connection closure.
- Called Four-Way Handshaking.


COMPUTER NETWORKS
Prof. S. Ghosh
Dept. of Computer Science & Engineering
IIT Kharagpur
                                                      Lecture -31
                                                          TCP
                                                (Start time 00.42)
Good day, our topic today is TCP. TCP is the second most important transport protocol.

(slide time 01:13   - 01:44)
TCP is very widely used by many applications. We have already discussed UDP. Actually this is a little more complex then UDP but it also has some advantages. We will see what they are.  So the Transport layer responsibilities are:

( slide time 01:13   - 01:44)

Transport layer creates packets from byte steam received from the application layer. And in order to multiplex and de-multiplex amongst various applications it uses port numbers to create process-to-process communication. It uses a sliding window protocol to achieve flow control and uses acknowledgement packets, time out and retransmission to achieve error control. So, unlike UDP which is unreliable this seeks to provide a reliable communication. That means it is error free, it is a connection oriented protocol and it also has some kind of congestion control mechanism. And it does the basic thing of connecting between processors amongst two distant nodes possible.

( slide time: 02:54-03:02)

TCP is called connection-oriented reliable transport protocol. It adds connection oriented and reliability features to the services of IP. IP as such is best effort kind of service so it does not give you any reliability. Secondly IP is connectionless whereas TCP tries to give some kind of connection-oriented flavor to the connection. And proposal is implemented entirely at the two end nodes. That means the intermediate routers etc do no have any role to play.

(slide time: 03:35-04:07)

The communication abstraction is that, it is reliable and ordered. Ordered means the packets are received in order. In IP it is not guaranteed that the packets will arrive in order because what might happen is that one packet may be routed in one direction and the other packet may be routed in other direction so the packet which was sent first may land up at the destination earlier and vice versa. But TCP makes them ordered. TCP brings an order amongst them.

( slide time: 04:08 ? 04:22)

TCP is point-to-point and unicast. All these features namely it is ordered, point-to-point, reliable and unicast is what gives the connection-oriented flavor to TCP. It takes the byte-stream as an input and gives that as the output. It is a full loop-less connection. That means both the nodes A and B are connected by TCP then A can communicate to B and B can communicate to A and it has flow and congestion control.

(slide time: 04:22-04:32)

TCP (Transmission Control Protocol) is a connection-oriented protocol, reliable, unicast, end-to-end, byte-stream over an unreliable network.

( slide time: 04:58-05:41)

So, before any data transfer TCP establishes a connection. Just like in a connection-oriented network like classical telephones, first there is a connection which is set up by all kinds of control signals. Similarly, like in ATM, first a connection is set up and then the actual data transmission begins. Similarly, in TCP a connection is set up between the two end points before any transmission takes place, so it sets up a connection.

(slide time: 05:42 ? 06:09)

One TCP entity is waiting for a connection that is the server. The other TCP entity-client contacts the server. The actual procedure for setting up connection is more complex. The client first makes a request for a connection and then the server acknowledges the request and accepts it, then the data transfer begins. So there is a connection set up before actual data transfer can take place and at the end of the data transfer there is a disconnect phase also.

( slide time: 07:08 ? 07:34 )

It is reliable. The byte-stream is broken up into chunks which are called segments. Therefore at the TCP level they are called segments. The receiver sends acknowledgements for segments so this is the basic mechanism by which reliability is brought in that each and every segment is acknowledged. TCP maintains a timer. If an ACK is not received in time the segment is retransmitted. So this is the basic mechanism for reliability. The sender sends a packet and it waits for some acknowledgement. And if the acknowledgment does not come through from receiver then the sender assumes that the packet has been lost and sends the packet again hoping that this packet will eventually reach the receiver and the receiver will send the acknowledgement. This might also lead to duplicate packets at the destination. For example, the sender has sent the packet and the receiver has received it and it has sent an acknowledgement but then the acknowledgement got lost. So naturally the original sender did not receive any acknowledgement and after sometime it will send it again so a duplicate segment would be a received by the receiver but then it knows that it is a duplicate and will eliminate that. So, by this way it achieves reliability. 


(slide time: 07:34 ? 07:59)

TCP can detect errors. TCP has checksum for header and data like UDP. Segment with invalid checksums are discarded. Each byte that is transmitted also has a sequence number. So, if some intermediate sequence number is missing the receiver knows that something has been lost.  

( slide time: 07:59 ? 08:32)

To the lower layers TCP handles data in blocks, the segments. This is where the packets actually originate. But to the higher layer TCP handles data as a sequence of bytes and does not identify boundaries between bytes. So, the higher layers do not know about the beginning and end of segments. Hence to the higher layer it is just a stream of bytes. 


( slide time: 08:33 ? 09:02)

For example, the application writes 100 bytes then it writes 20 bytes so all these go into the queue. So the queues are bytes to be transmitted and then TCP transmits them. At the other end also there is a queue of bytes and at the other end also it reads 40 bytes at each go. So, to this application this is the byte-stream and for this application also this is just a stream of bytes coming in such units.  

(slide time: 09:03 ? 09:37)

The unit of data transfer between devices using TCP is a segment. It is 20 to 60 bytes header followed by data. It is a 20 byte header if there are no options and up to 60 bytes if it contains some options. So there can be up to 40 bytes of option. So the header naturally contains some field which allows the TCP protocol to run. Now let us look into the details of this header.

(slide time: 09:37 ? 09:02)
The Header has 16 bit source port address and 16 bit destination port address. It also has a sequence number, acknowledgement number, Header length (HLEN), reserved bits, some flags and then there are some options like window size, checksum, urgent pointer, options and padding. 

(slide time: 10:21- 10:30)
So there is a source port address. In the client-server processes actually communicate through ports. And the port and the IP address together form the socket which uniquely identifies every session. When a TCP session is going on, on both the sides two port numbers are assigned. For standard applications the first communication will start on well-known port number then they will switch to ephemeral port numbers. 

(slide time: 10:31 ? 10:51)

Then there is a source port address which is a 16 bit address that defines the port number of the application program on the host that is sending the segment. 

( slide time: 10:51 ? 11:15)

Similarly there is destination port address which is again 16 bits. The port number is from 1000 to 65000 and those are the ephemeral port numbers. Destination port address defines the port number of the application program on the host that is receiving the segment.

(slide time: 12:50 -12:55 )
There is the sequence number which is a 32 bit number. It defines the number assigned to the first byte of data contained in the segment. During connection establishment a random number generator is used to create an Initial Sequence Number (ISN). The field for the sequence number has been kept quite big, it is a 32 bit and is kept with a reason. If the number of bits for the sequence number was small then what could happen is that, when a particular session starts it sends the number and then it would go back to the beginning. For any finite length sequence number after some point it is going to go back. Now, if it goes back and starts these numbers once again for a very small segment size what might happen is that first of all the two packets may get the same segment number which are on the network at the same time and similarly other kinds of confusion might arise. So what is done is that, actually a large sequence number is generated so that even if it comes back to a low number we can know that which one came earlier and which one came later. If after receiving very high numbers if you start getting some low sequence numbers you know that it has looped back. Actually it may not be a strict loop.

(slide time: 13:27 -14:28)

Therefore this is the range of sequence numbers. So 232 is about 4 {?lian}  which is a large number and random number is used to generate the Initial sequence number. This initial sequence is expected in a large range so it is not going to clash and this ISN is exchanged between the two nodes. 

( slide time: 14:29 ? 14:44 )

There is an acknowledgment number which is 32 bit. Just as the segments which have been sent they have a sequence number. Similarly these segments as they arrive on the other side will get an acknowledgement. So it is one acknowledgment after the other so a stream of acknowledgements will come. Actually in the best of circumstances there will be as many acknowledged numbers as there are segments which are sent. So this acknowledged number will start somewhere and that would be communicated by the receiver to the sender in the connection set up phase which is also a 32 bit number that is large. It defines the byte that the receiver of the segment is expecting to receive form the other party. This is standard norm for the sliding window protocol. 

(slide time: 14:53-14:57)
The next field in the header is the header length which is of 4 bit. This indicated the number of 4 byte word in the TCP header. This is required because the header is of variable length. The header could be anything from 20 bytes to 60 bytes depending on the options. So the Header length is to be specified and then there are some reserved bits for future use

( slide time: 14:58 -15:51)


( slide time: 15:51 ? 16:17)

followed by some flags. It defines 6 controls flags. URG defines the urgent pointer which determines whether the urgent pointer is valid or not. If the urgent pointer is not valid the urgent pointer field itself may contain some garbage. ACK tells whether acknowledgement is valid or not. PSH is for request for push. RST is for resetting the connection. SYN is to synchronize sequence number and FIN is to terminate these connections. These flags are used for setting up and termination of connections.

( slide time: 16:17 -16:35)

There is a window size of 16 bit which defines the size of the window in bytes that the other party must maintain. It is sort of controlled by a receiver. The receiver gives the window size which is of the sliding window protocol. And then there is a Checksum which is of 16 bits. The checksum is like UBP.

( slide time :16:36 ? 16:58)

The urgent pointer is 16 bits and is valid only if the urgent flag is set. It defines the number to be added to the sequence number to determine the last urgent byte in the data section of the segment.

( slide time :16:55- 17:02)

Options are up to 40 bytes. Options could be a single-byte or multiple-byte. Multiple-byte in options may contain maximum segment size, windows scale factor and timestamp etc. In single-byte there are end of options and no operation for padding purpose.  

( slide time :17:03 -17:24 )

End of option is used for padding at the end of the option field and no operation is used as filler between the options. 

( slide time :17:24- 17:51 )

If there are options on the other hand then one option is the maximum segment size which defines the size of the biggest chunk of data that can be received by the destination of the TCP segment. This is not the segment but the size of the data which is taking the header field out. 

( slide time :18:53 -19:00)

The window scale factor defines the size of the slicing window and how it is changed. Timestamp is filled by source when segment leaves and destination returns it in the echo reply field. This allows the source to determine the round trip time. Some estimation about the round trip time is very essential because suppose a particular segment has been sent then the sender is expecting an acknowledgement. If the acknowledgement does not arrive on time, then how long would the sender wait for the acknowledgement? So you have to make an estimate and that estimate is based on the round trip delay, the maximum segment life. So it has to wait that much or may be some more and only then the sender would sort of come to the conclusion that may be the original packet is lost or may be the acknowledgement is lost. But in either case the packet or the segment that was send it has to be retransmitted.


( slide time :19:01 ? 19:24)

There is a Checksum, it has the same calculation as UDP and inclusion is mandatory with TCP.

( slide time :20:21 ? 21:10 )
Setting up connections and resetting of connections: TCP is a connection-oriented protocol. So, a virtual path between the source and the destination has to be established. This is only a virtual path because remember that TCP is working on IP that means an IP is a connectionless service. So the underlying actual service is actually connectionless. But TCP gives a feeling to the upper layer, this application layer that, as if it is a connection-oriented thing. That means all good things about connection orientation like segments, arriving in order and reliability etc is present. But TCP has to work on a connectionless IP network. So this path established from source to the destination is only a virtual path unlike traditional strict connection-oriented service where the connection may be physical.  

( slide time: 21:10 -21:21 )
For connection establishment four actions are required before sending data. And here two of them may be combined so it can even be three actions. First host A sends a segment, if host A is the client and it wants to establish a connection to the server may be a host B, so host A sends a segment to announce its wish for connection and includes its initialization information. And then host B sends a segment to acknowledge the request of A. Here host B sends back an acknowledgement.

( slide time: 21:21-21:24)


Then host B sends a segment that includes the initialization information. So here the second and third steps can be combined.

( slide time: 21:24-  21:47)
That means the host B can acknowledge the request of host A as well as in the same acknowledgement it might send the initialization information meaning the sequence number etc have to be exchanged.

( slide time: 21:47- 24:09)
Then host A sends a segment to acknowledge the request from B. Second and third can be combined which is called as the Three-Way Handshaking.
 

( slide time: 24:09-24:30)
For the first SYN, that is, for synchronization the sender sends a synchronization which is a request to set up a connection to the host. And it also gives initial sequence number. The receiver then sends a SYN, its own SYN the sequence number which is something like 4800 and an acknowledgement of 1200. This segment which is the segment 1 already consumes the first sequence number 1200. So, while acknowledging it sends the value 1201 meaning that 1201 is the next segment or next segment number that the receiver is expecting. What might happen is that, this packet may get lost because once again the TCP is sitting on a best effort kind of IP service so it may get lost. Therefore what would happen to the sender is that, the sender will not get any acknowledgement so after some time it will send the SYN pack again if it is persistent and then finally it will get the SYN and the acknowledgement. Similarly, if this is lost once again this is sent after sometime and finally this is achieved and then the sender sends the sequence number 1201 and acknowledgement 4801. So this sequence number is always the next number that is expected from the other side. When it sends sequence number1200 it replies back saying that next it is expecting 1201 and its starting number is 4800. So he replies in his acknowledgement that the next acknowledgment he is expecting is 4801. So this is the Three-Way Handshake where the SYN and the ACK has been combined and then the data transmission can start. 


( slide time: 24:53-24:58)  

On the other side we have to think about the connection termination. To terminate the connection either party can close the connection. If connection is terminated in one direction data can continue to be sent in the other direction. Remember that this is the full loop-less communication which means A is communicating to B and B is communicating to A at the same time. Even if only one side is sending data to the other side the acknowledgement is coming from the other side anyway. Now, the connection can be terminated from the both sides but then somebody has to initialize the termination.


( slide time: 24:58 ?25:22)

So, if connection is terminated in one direction data can be continued to be sent in the other direction.  

( slide time: 25:27 ? 25:34)
 Four actions are required to close the connection in both directions. First, host A sends a segment announcing connection termination. This means it sends the segments containing FIN. 

( slide time: 25:34-25:44)
Host B sends a segment acknowledging the request from A and after this the connection is closed in one direction.


( slide time: 25:44-25:50)
When host B has finished sending data it sends a segment indicating connection closure.

( slide time: 25:51-25:55)
Here, the second step and the third step cannot be combined together. 


( slide time: 25.55 ? 26:10)

(Slide time: 26:11-26:24)
Although we are sort of allowing the termination of connection for one side but on the other side it may have acknowledgements or other things to send to this side so it will not terminate the connection. Therefore these two cannot be combined together.


(Slide time: 26:24- 29:32)
The third step can be taken only when host B has finished sending data from its side and it sends a segment indicating a connection closure. Host A acknowledges the request from B. So this is called a Four-Way Handshaking.  

( slide time: 29:32-29:36)

This is the diagram showing Four-Way Handshaking. For example, assume that the sender has sent a FIN in a segment 2500 and when it receives the FIN if everything is all right he will send back an ACK and he say he will say that the next one he is expecting from the other side is sequence 2501 and sequence 7000 which is the acknowledgement for this FIN. After sometime when B has finished sending all its acknowledgements and other things it might want to send to the sender then it is sends a FIN from this side. So, this is for closing the connection from the other side. The acknowledgement that is expected once again is 2501 and this is still 2501 because nothing else has arrived from the other side and then it takes the next segment number 7001 from this side. And he acknowledges this second FIN when the acknowledgement reaches here so everything is closed gracefully. Of course things may not run so well because one or more of these packets may get lost on the way because we know that the underlying network is unreliable. If the first FIN is missed he does not get any acknowledgement and after sometime he will send the FIN again. Similarly, if this ACK is not received then he will send this FIN once again anyway. So this FIN may be lost. Since he does not get any acknowledgement he will send the FIN after sometime. The trouble is over here because at this point of time after sending this acknowledgement the sender will assume that everything is fine so he will close. But this last acknowledgement may get lost. As all the FINs have been sent and received and the acknowledgement has been received and sent he will say that it is the end of the story but for him it is not end of the story because this ACK is now lost but if this fellow has already completed he will not send any ACK any longer. So what he will have to do is that after sometime he will have to close the connection.

( slide time :29:36-29:42)
Actually the TCP goes through a state diagram.

( slide time: 29:42-31:11)
This is the state diagram and these are the different states.

( slide time: 31:11-32:21)
One is of course closed that means no connection is active or pending. The other is listening. May be the server is waiting for an incoming call. SYN RCVD means somebody has made a request, a connection request has arrived and is waiting for acknowledgement. SYN SENT: The client has started to open a connection. ESTABLISHED means a connection has been established and that is the normal data transfer state. FIN WAIT 1 means client said it is finished. FIN WAIT 2 means server has agreed to release. TIMED WAIT means wait for pending packets. This is the 2 MSL wait state so for the last pending packet to come in you have to wait this much. This is the maximum segment and this is sort of estimated from the round trip time. CLOSING means both sides have tried to close simultaneously. CLOSE WAIT means server has initiated a release and a LAST ACK is waiting for the pending packets.

( slide time: 32:21-33:05)
When it is closed nothing is happening. Now some SYN or SYN plus ACK it receives and once it receives that then it goes to the SYN RCVD state. As explained earlier this is the connection set up phase and once the connection is set up then it reaches the Established phase and then the two way communication is going on. Then, after this, it goes through a closure either through the closing and then FIN WAIT 1 and FIN WAIT 2, TIME WAIT etc and here it is the close width and the last ACK.

( slide time: 33:05-33:17)
The flow control is implemented by TCP. This defines the amount of data a source can send before receiving an acknowledgment from the destination. TCP uses a window imposed on the buffer of data to limit the amount of data sent before an acknowledgement must be received. This is the traditional sliding window protocol.

( slide time: 33:18 -34:21)
In the Sliding Window Protocol a sliding window is used to make transmission more efficient as well as to control the flow of data so that the destination does not become overwhelmed with data.


( slide time: 34:23-35:01)
In sender buffer there is the occupied part of the buffer and out of this some number or packets or segments have been sent but they have not been acknowledged and these are the next byte to be sent which is available in the buffer. Now, depending on the windows size it can go on sending these bytes. Or if the window size is already over then it has to wait here. Let us assume that the size of the window is three units, in that case the sender has to wait here till some acknowledgment comes. And if the acknowledgment comes suppose 200 is acknowledged then the window will automatically slide and this now can be sent.


( slide time: 35:02-35:22)
At the receiver side it has received 194 to 199. This is the occupied part of the buffer and from this buffer the destination process will keep on consuming the data from these segments as q stream of bytes going out of this segment. Here, this is the empty part of the buffer where new segments can come in. 

(Slide time: 37:48-39:55)
Suppose the size of the receiver window is 7 then after having sent this the sender can go on sending up to the receiver window size. The point is that, if a window size is not fixed what the sender might do is that, the sender may send in a lot of packets. Or may be the channel is absolutely bad, in that case none of the packets have gone to the other side so you will have to send all these packets once again, and that is one point. And secondly the sender may be very fast but the network may be congested. So the sender will push in a lot of data but that will only make the congestion worse. So whatever would have been received on the other side does not reach that side because the intervening network has been congested, or may be some buffer has overflowed or some router has dropped some packets etc so there is a limit on the window size. The window size also has a bearing on the speed at which effective data transfer is taking place. If the window size is very small, suppose 2, then after sending two segments the sender has to wait till the acknowledgment comes back. So the efficiency of the channel goes down because for each acknowledgement the packet will reach the other side and then the acknowledgment will travel all the way back so it is a round trip delay. Now suppose the window size is only 1 so after sending each segment or each byte you wait for the round trip time and then you send the next byte. So now the overhead has become very high and now this is the minimum possible rate at witch the sender is sending. On the other hand if the window is very large a large amount can be sent even without waiting for acknowledgement. 

( slide time: 39:55 ? 39:58)
Now when everything is fine the stream of acknowledgements will also start coming in after this round trip delay and the overall efficiency would be high. Therefore between congestion and the efficiency at which the data can be transferred that is the effective data transfer speed there is a trade-off. 
Suppose if these are the sequence numbers and now the receiver side may be 205 has dropped somewhere and 206, 207, 208, 209 etc has come. So after acknowledging 204 where the acknowledgement packet will tell that the next byte its waiting for is 205 although 206, 207, 208, 209 etc have been received but he will not send any acknowledgement. So after sometime the sender will realize that this acknowledgment for 205 has not come. So he will start pumping 205, 206, 207, 208 and 209 again. Now as soon as 205 arrive what happens is that this gap is closed and the window will slide all the way here and he will say next byte that is expected is 210. Then what will happen is, 206, 207, 208, 209 now becomes duplicated segments on the receiver side so they are simply dropped. Using the segment numbers not only we can put them in order, suppose 206 comes earlier and 205 has come but has come later but then they can be put in proper sequence because sequence numbers are there.  And secondly even this 205 was lost it can still be recovered by this retransmission etc so that is how this protocol is reliable. And when this is acknowledged the window slides.

( slide time: 39:59-40:11)
So, this is the size of the receiving window.


( slide time: 41:17-41:31)
Sometimes what happens is that, if there is actual congestion in the network, you will find that only after sending some packets and seeing that some acknowledgements have not come and the packets have got lost. So, the sender thinks, one thing is, this packet may have got lost and secondly, may be the network is congested. So what will happen is that, there is a mechanism for automatically reducing the window size so that now you are going to transfer data at a much lower rate but then you are sort of trying to control the congestion. So this is the congestion control that is in built into TCP. There are some variations as to how this window size is going to be changed over time etc. So we will come to later on we when will talk in more details about congestion control.


( slide time: 41:32-41:51)
In TCP the sender window size is totally controlled by the receiver window value, the number of empty locations in the receiver buffer. However, the actual window size can be smaller if there is congestion in the network.


( slide time: 41:51- 42:15)

The source does not have to send a full window?s worth of data. When you have limited data then you will send only that data which is available. The size of the window can be increased or decreased by the destination. The destination can send an acknowledgment at any time. 


( slide time: 42:18 -43:50)
Error Control:
The TCP is a reliable transport protocol. TCP delivers the entire stream to the application program on the other end in order without error and without any part lost or duplicated. So TCP provides reliability using error control.

( slide time: 43:51 ? 44:01)
For error control there are the different things that TCP would do. It detects corrupt segments, by detecting lost segments, by detecting out of order segments, by detecting duplicated segments and by correcting errors after they are detected. Detecting corrupt segments is, when checksum would be wrong in that case you can drop it. Or if you can correct the error using the checksum Then you may correct it also. Detecting lost segments is achieved by getting the help of the sequence number. When you have the sequence numbers and when one particular segment in between is lost you know that the segment number is lost because that particular segment number would be missing. Detecting segments that are out of order is, when the segments are in out of order we have got the sequence number and so we put them in order. You can detect and correct it at the same time. Detecting duplicated segments: If a particular segment has come twice, may be its acknowledgement is lost or delayed, in that case once again by the serial number we can see that it is duplicated and we can drop it. 


( slide time: 44:22 ? 44:39)  
Error detection is achieved by three simple tools: Checksum, acknowledgement and time-out. Suppose you had sent the first segment and the acknowledgement never came then after sometime you have to give a time-out and that is how the sender detects that this segment has been lost and it has to be re-transmitted. 

( slide time: 44:38 -44:52)
Checksum, acknowledgement and time-out are the three things together offer reliability and error detection and correction capability of TCP.

( slide time: 44:52-48:05)
For error correction when time-out counter expires segment is considered to be corrupted or lost and the segment is retransmitted.

( slide time: 48:05 ? 49:35)
TCP Timers: TCP has to maintain a number of timers inside. TCP has to do a lot of work to give that reliability which is necessary on this unreliable IP service. That is why this protocol is a little more complex than others. But then at the same time this is necessary. For example, when you are sending a File Transfer Protocol for example, you are transferring a file from one machine to another then even the misplacing of few bits will make the whole file useless. So there are applications and specifically protocols like FTP and ACTP etc use the TCP protocol because there is reliability of the connection and absolute error control, it is error free, and the nature of communication is very high. Then there may be applications where a loss of few bits or bytes here and there does not really matter. For example, suppose we are carrying a voice communication, now if you are carrying voice communication what would happen is that even if a few bits here and there are lost the quality is not impaired that much. Of course voice or other multimedia kind of communication is very sensitive to other kinds of network parameters. But the point is that, even if few bits are lost here and there then the other person will also be able to make out. So in this case, on the other hand if there is a delay and differing rate of transmission because if the window size is large the rate at which you can transmit is high and if the window size is small the rate at which you can transmit is small. So, if the rate at which transmission is taking place keeps on varying or takes a lot of time etc is not acceptable for voice communication. So those are the cases where we will not use TCP. There are cases where we definitely want to use TCP because of its reliability and then there are cases where we do not want to use TCP. So FTP and HTTP etc are examples where TCP is necessary and we use them. TCP uses four timers: Retransmission timer, persistence timer, keep-alive timer and time-waited timer. 

( slide time: 49:35 -51:56)
Retransmission timer: Used to control a lost or discarded segment. Suppose a segment is lost, now, after what time will it be retransmitted? So in this retransmission timer using the timer stamp you can get an idea of the round trip delay and this you will use to set your retransmission timer. And there will be a buffer because it is not constant. By this way you can set up your retransmission timer. As soon as the segment is sent immediately the retransmission timer starts. When the next segment is also sent because your window size happened to be more than 1 then you will need another retransmission timer for this segment. So, for each segment we are maintaining this retransmission timer and as soon as one of these timers is timed-out it means that the packet was sent and the acknowledgement was not received. Therefore, immediately you have to retransmit that packet. So this is the retransmission timer. TCP creates a retransmission timer when it sends a segment. If an acknowledgement is received before the timer goes off the timer is destroyed because this timer is not required.  For each new segment you start a new retransmission timer. 

( slide time :51:57-52:03)
If the timer goes off before the acknowledgement arrives, that means the acknowledgement has not been received before the estimated time then the segment is retransmitted and the timer is reset. Now this timer will be reset because even though you retransmit the retransmitted segment may also get lost so you have to start the retransmission timer once again. And so TCP uses a dynamic retransmission time which is different for each connection and may be changed during the connection. TCP is a protocol and there may be a lot of connections going through this TCP protocol. Just think a simple case of a web server. Now there may be a number of people for very active websites and a number of people may be hitting it at almost the same time. So if you have a concurrent server and you have spawn so many processes on the application side and each of the server is sort of serving one particular client and a number of them. So a number of TCP services are going on at the same time but of course they are using different port numbers. So whenever any acknowledgment or anything comes you will see the port number to determine to which process this belongs to. Secondly, now assume that two clients using the same server at the same time. Now the two clients may be connected in two different networks and the round trip time for each of these clients may be different. Since the roundtrip time is different you cannot use the same kind of retransmission time for each of these connections. This has to be dynamically assessed from the time stamp so it may be different for each connection and may be changed during the connection.

( slide time: 52:03-52:26)


( slide time: 52:40-52:50)
There is a Persistence timer. When TCP receives an acknowledgment with the window size of 0 it starts a persistence timer. Window size of 0 means, it does not accept anything at the moment because either it is overwhelmed or something is sort of wrong in between. So anyway it is very congested and is not willing to receive anything. But now what would the sender do?


( slide time: 53:15- 53:23)
He will start a persistence timer and when the persistence timer goes off the TCP sends a special segment called a probe. What would have happened is that the buffer may have become full. So now, before sending a packet you have to find out whether it has recovered so you have to send a probe segment. 

( slide time: 53:23-53:34)
This prevents a deadlock from occurring in case an acknowledgment is lost.


( slide time: 54:29-54:51)
There is a keep-alive timer that prevents long idle connections between two TCP?s. Suppose two sides the connection has been setup and nobody has closed the connection but both sides are idle for a long time. Now, as soon as the TCP connection is setup and after the last communication this keep-alive timer is automatically started. When the keep-alive timer goes off, that means there has not been any communication between the two for a long time. What might have happened is that the other side might have got switched off. Therefore at times there can be disruption in connection. Hence it is not a graceful connection.

( slide time: 55:22- 55:38)
Each time the server hears from a client it resets the timer and starts the countdown from the beginning. If it does not hear from the client after the timer times out it once again sends a probe segment to see where the other side is alive or not. If the other side is indeed alive and wants to keep the connection on it will reply to the probe segment and then it will know that it still wants to do something but then it is much occupied at the moment and that is why it is not sending. So this is the keep-alive timer. If on the other hand it has been switched off then this probe will go unanswered. So after sometime this side will take some action to close everything. 

( slide time: 55:22- 55:38)
Time-Waited Timer is used during connection termination. When TCP closes a connection it is not really closed until this timer times out. It allows for FIN segments to be received. In the previous case the last acknowledgment has not been received because it was lost. So everything was fine but the other side has actually gracefully terminated the condition and stopped it. So it has to wait for sometime which is more than twice the round trip delay. Therefore it waits for the time-waited timer and once that is over then the connection can be terminated. With this we close our discussion on TCP and there is some more variance of TCP regarding how you control the window sizes etc. We will discuss that when we discuss congestion control.









        Lecture No # 32
              IP Multicasting

( slide time: 56:51 ? 57:01 )
Good day, today?s topic is IP Multicasting. There are three modes of operation. One is Unicast that means one sender is sending something to one receiver. The other is broadcast which means one sender is sending it to all the receivers that means to all the nodes in the networks. Multicasting is, when you want to send it to just a group of hosts and not all the hosts. 

( slide time: 57:26 ? 57:46)
IP Multicasting: Multicast communication refers to one-to-many or many-to-many communication. Unicast refers to the source is one and the destination is one. Broadcast is, when source is one and destination is to all and in multicast destinations are a few.
  
( slide time: 57:46 ? 58:08)
IP multicasting refers to the implementation of multicast communication in the internet. Individual hosts are configured as members of different multicast groups. Multicasting is not connection-oriented. An IP multicast group is identified by a class D address. So these are the general parameters. One particular user may be a member of different multicast groups. For a one particular multicast group there will be a few members in the network and it has to reach those and not others. And the other thing to be understood is that, multicasting is not connection-oriented that means all the packets are sent. That means it is packet by packet and it is not from the source, multiple channels or a prairie set up like that.


 
  
COMPUTER NETWORKS
PROF. S. Ghosh
Dept. of Computer Science & Engineering
IIT Kharagpur
Lecture No # 32 
IP Multicasting
Good day. Today?s topic is IP Multicasting. (Refer slide time: 00:56- 01:08)

Slide time: 00:56- 01:08
Till now, we have seen the three modes of operation: One is unicast where one sender is sending to one receiver. One is broadcast where one sender is sending to all the receivers that means all the nodes in the network and multicasting is when you want to send it to a group of hosts but not all the hosts. So, IP multicasting is the topic for today. (Refer slide time: 01:31- 01:53)


Slide time: 01:31- 01:53
Multicast communications refers to One-to-many and many-to-many communications. For example, this is unicast when the source is one and the destination is one, broadcast is when source is one and destination is all and multicast is when destination are a few. (Refer slide time: 01:53- 02:14) 

Slide time: 01:53- 02:14
- IP multicasting refers to the implementation of multicast communication in the internet. 
- Individual hosts are configured as members of different multicast groups. 
- Multicasting is not connection-oriented. 
- An IP multicast group is identified by class D address. 
 These are the general parameters. One particular user may be member of different multicast groups but for one particular multicast group there will be few members in the network and it has to reach those and not others. Other thing to understand is that multicasting is not connection-oriented that means all the packets are sent. It is packet by packet it is not that from the source multiple channels are prairie setup or anything like that. (Refer slide time: 02:46- 03:15) 

Slide time: 02:46- 03:15
They are identified by class D address which is for multicast. The class D address is seen in the fashion that the beginning four bits are 1110. So the initial value is 224. something and these 28 bits. This is how by looking at an address you can see that this is the multicast address. (Refer slide time: 03:15- 03:26) 

Slide time: 03:15- 03:26
There are many applications like news, sports, stock and weather updates. Let us take the example of stock updates. Now, not everybody would be interested in stock updates, only some group of people would be interested in stock updates. Again, it may be such that one group of stock is of interest to one group of people, another group of stock to another group of people and so these would be different multicast groups and the news feed should reach these people. Then multicasting may be applied in distance learning. (Refer slide time: 03:49 - 04:17)

Slide time: 03:49 - 04:17
When some learning material is distributed to distance learners and just specific group of learners, configuration, routing updates, service location may be the areas where multicast may be applied. PointCast-type push applications where push means when the actual source of information finally on its own sends it to the group of people. For example, stock quotes may be (Refer slide time: 04:28 - 04:52)

Slide time: 04:28 - 04:52
pushed. Teleconferencing, audio, video, shared whiteboard, text editor etc is an interesting and important application of multicasting. That means you may like to have a video conferencing amongst a group of people so this same video stream should reach the entire group of people. (Refer slide time: 04:52 ? 05:18)

Slide time: 04:52 ? 05:18
- Distributed interactive gaming or simulations, some people are participating in some game in a distributed fashion. 
- Email distribution lists. 
- Content distribution; software distribution. 
- Web cache updates, Database replication. 
Multicasting has very large number of applications. But the trouble is, multicasting is a little complex. The technology is not so simple and the fact is there are most of the routers which are in operation today are not configured for multicasting in a proper manner because it takes a toll on the routers capabilities. So we will come to see what a multicast routing is all about. (Refer slide time: 05:42 - 05:59)

Slide time: 05:42 - 05:59
There are three essential components of IP multicast service: 
- IP multicast addressing is, how you address. 
- IP group management. 
- Multicast routing. (Refer slide time ? 06:00- 07:13 )

Slide time ? 06:00- 07:13
If you look at this diagram, suppose you have these three routers and three networks connected to these three routers respectively. Suppose, you take any of these routers and that is connected to its own group of machines and some of these machines would be the members of a multicast group. Of course, new machines can come in and new machine can actually decide to join the multicast group and some of the old group members may choose to leave a particular multicast group. So there is a group membership protocol which goes on between the router and the different machines connected to the network. Then, amongst the routers there is question of multicast routing. So this Internet Group Management Protocol (IGMP) runs between host and their immediately neighboring multicast routers. And within the routers we have the multicast routing protocol running. (Refer slide time: 07:13 - 07:41) 



Slide time: 07:13 - 07:41
This is another picture showing the same thing. This is the service model, suppose, these are the hosts and host to router protocol which is known as the Internet Group Management Protocol (IGMP). And then amongst the routers there is multicast routing protocol and there are various types of multicast routing protocol, we will just discuss a few. (Refer slide time: 07:41 ? 08:07)

Slide time: 07:41 ? 08:07
- IP multicasting only supports UDP as higher layer. 
- There is no multicast TCP. UDP is a connectionless datagram oriented protocol and TCP is connection-oriented. Since, IP multicasting is essentially connectionless that is why it chooses UDP as the transport layer protocol. (Refer slide time: 08:07 - 09:13)

Slide time: 08:07 - 09:13
If you look at the details of this part of the protocol stack here we have the network interface, IP and IP multicast layers. The IP part of this network layer takes part in the normal routing protocol whereas IP multicast part takes care of the multicasting routing protocol. Then above this in the TCP IP stack we have the TCP protocol and the UDP protocol. So, for TCP we use the stream sockets whereas for UDP we use the datagram sockets as well as multicast sockets. So, multicast sockets also use UDP. This is the socket layer and above this we have the user layer. So that is the application layer which uses these multicast sockets and uses UDP to send multicast messages which are routed by the multicast supporting routers. (Refer slide time: 09:13 ? 09:51)

Slide time: 09:13 ? 09:51
IP multicast works as follows: 
- Multicast groups are identified by IP addresses in the range 224.0.0.0 to 239.255.255.255. These are the class D addresses. 
- Every host (more precisely every network interface card) can join or leave multicast group dynamically. 
- At present the way it has been done has no access control. So, if there is a multicast group which requires access control then this has to be implemented in the application layer. As the protocol stands today this has not been included. One of the reasons is that IP multicasting in actual practice constitutes a very small amount of traffic compared to its potential true multicasting. Later on let us see what is true multicasting and simulated multicasting. But actual true multicasting traffic is really small compared to its potential mainly because most of the routers may not support, they are not more precisely configured to support multicasting because of the cost involved. (Refer slide time: 10:46 ? 11:06 )

Slide time: 10:46 ? 11:06
Since there is no access control every IP datagram sent to a multicast group is transmitted to all members of the group. 
There is no security, no floor control. 
Moreover since it uses UDP, IP multicast service is essentially unreliable. (Refer slide time: 11:06 ? 11:46)

Slide time: 11:06 ? 11:46
More detail about the multicast addresses: 
- The range of addresses between 224.0.0.0 and 224.0.0.255 means the last byte for the first range inclusive is reserved for the use of routing protocols and other low level topology discovery or maintenance protocols. 
- Multicast routers should not forward any multicast datagram with destination address in this range. So they are reserved addresses and other addresses can be distributed to different multicast routes. (Refer slide time: 11:46 - 12:21)

Slide time: 11:46 - 12:21
Examples of special reserved class D address: 
- 224.0.0.1 really means all systems on this subnet. 
- 224.0.0.2 means all routers on this subnet. 
- 224.0.1.1 is for NTP (Network Time Protocol) used for synchronizing machines. 
- 224.0.0.9 is for RIP-2 (a routing protocol). 
So these are some special addresses and there are others. (Refer slide time: 12:21 - 12:28)

Slide time: 12:21 - 12:28
Now there is a question of multicast address translation. You remember what happens in the case of unicast is, from the IP address, for transmitting packets in the local network we need to go down to the data link layer and we need to find out the hardware address. Let us say if we are using Ethernet then we need to find out the Ethernet or MAC address and then data is actually sent as an Ethernet frame so the Ethernet Address is put over there. Now, that is the case when we are handling unicast. Now, what happens in multicast? For this particular IP address it is actually representing a particular multicast group and there will be a number of machines in that group. How do you handle it in the Ethernet level? (Refer slide time: 13:23 - 14:19) 

Slide time: 13:23 - 14:19
- In Ethernet Mac addresses a multicast address is identified by setting the lowest bit of the most left byte. That is this byte. Suppose you want 1, 2, 3, 4, 5, 6, if you remember, Ethernet address is 6 bytes long, now, of the first byte the most significant byte if you want and the last bit of that is set to 1 in Ethernet Mac addresses to indicate that this is the multicast address. 
- Unfortunately not all Ethernet cards can filter multicast addresses in hardware. So, if it cannot be done in hardware then filtering is to be done in software by the device driver. So, you accept the packet and then do the filtering if it is multicast and if you are a member of the group and then accept it. (Refer slide time: 14:19 ? 15:49 )


Slide time: 14:19 ? 15:49
This is how the mapping is done. Suppose in this 1110, the first 4 bits and suppose this is the class D address and we are looking at the first byte of that address and the first four bits 1110 identifies that this is a class D address. Then this bit is actually ignored and then we have a 23 bit address. This 23 bit address comes straight to the Ethernet address. So these 7 bits, these 8 bits and these 8 bits are matched straight to the last 3 bytes of the Ethernet address. For the first three bytes of the Ethernet address we have a one here showing that this is multicast. Actually the Ethernet address with 01, 00, 5e in the first 3 bytes are reserved for IP multicast. So 01, 00 and this is 101 is 5e and 1110 is e. So, this is 01, 00, 5e and this is first 3 bytes, this is reserved for multicast and this part comes straight away. (Refer slide time: 15:49 ? 16:12)



Slide time: 15:49 ? 16:12
- Now let us move on to IGMP which is the Internet Group Management Protocol. This is a very simple protocol for the support of IP multicast. 
- IGMP is defined in RFC 1112. 
- IGMP operates on a physical network that is the single Ethernet segment. (Refer slide time: 16:12 ? 17:08)

Slide time: 16:12 ? 17:08
If you remember, in the previous diagram we saw that one particular router is connected to one Ethernet segment. So IGMP is between this router and the host which are there. So IGMP is used by multicast router to keep track of the membership. Now, who all amongst these members, who has ceased to be a member, who is the new person who wants to join as a member, so this has to be kept track by the local multicasting router and that is what IGMP supports. So, it supports joining a multicast group, query membership and send membership reports. So the multicasting router will send queries from time to time and the host will respond or not respond depending on whether or not they are members of the group. (Refer slide time: 17:08 ? 17:27) 

Slide time: 17:08 ? 17:27
So, we have this multicasting router over here and one single Ethernet segment and number of machines connected there. So IGMP query comes from this multicasting router and IGMP report goes to the router from the host. (Refer slide time: 17:27 - 18:30)

Slide time: 17:27 - 18:30
There may be an IGMP general query that IGMP group address is set to be equal to 0. It means that this query is for all the hosts, for all the groups and destination IP address is broadcast in this subnet. You remember that 224.0.0.1 is broadcast in this subnet and source IP address is the routers IP address. There may also be groups specific query in which case the IGMP group address is the group address, destination IP address is again the group address because now I want to give this query only to the members of one particular group and source address is the routers IP address. And this individual host sends the reports so it is the IGMP membership report. Therefore IGMP group address is the group address, destination IP address is also the group address and source IP address is equal to host?s IP address. (Refer slide time: 18:30 - 19:02)

Slide time: 18:30 - 19:02
In the IGMP message format there is a version and type. Type may be 1 or 2, version is usually 1 and then a 16-bit checksum and 32-bit group address. Type: 1 for the query sent by multicast router and 2 is a response sent by a host. Group address is a class D IP address. On query it is 0 and on response it is the group address being reported. (Refer slide time: 19:02 - 20:27)

Slide time: 19:02 - 20:27
- A host sends an IGMP report when it joins a multicast group. (Note: Multiple processes on a host can join. A report is sent only for the first process). This means that when a host wants to join one particular multicast group then it sends an IGMP report to the router that it wants to join. 
- On the other side when a particular host wants to leave a group, it does not want this multicast traffic any longer so when it wants to leave that group it does not do anything at all. Only thing is that when the next query comes for this particular group, for which it was a member then it will not respond. So this means that there is some kind of aging in the group membership list that the router would maintain. So, no report is sent when a process leaves a group. 
- A multicast router regularly multicasts an IGMP query to all the hosts (group address is set to 0). 
- A host responds to an IGMP query with an IGMP report. If somebody fails to respond then it is taken that he has left the group. (Refer slide time: 20:27 - 20:51)

Slide time: 20:27 - 20:51
What does the IGMP host reports look like? 
- Host sends a report when it joins a group. 
- It does not report when it leaves the group but does not respond to the next query. So this is the IGMP report, the time to leave is 1, the IGMP group address is group address, destination IP address is group address and source IP address is host IP address. (Refer slide time: 20:51 ? 21:17)

Slide time: 20:51 ? 21:17
For general query, group address is 0, destination IP address is naturally broadcast and source is the routers IP address. 
- So routers sends query at regular intervals to see if anyone still belongs to any group. Queries send out each interface. 
- Host responds by sending one response for each group to which it belongs. (Refer slide time: 21:17 - 21:36)

Slide time: 21:17 - 21:36
IGMP messages are only 8 bytes long. We have Ethernet header, IP header and the IGMP message which is version, type and some part is unused and the checksum and a 32-bit class D address. (Refer slide time: 21:36 ? 22:04)

Slide time: 21:36 ? 22:04
Suppose you have a network with multiple multicast routers. That means the same network but it has got multiple multicast routers. Only one router responds to IGMP queries so this is the Query. So the router with the smallest IP address becomes the Querier on a network. One router forwards multicast packets to the network, so it is the forwarder. If a network happens to be so constituted that there are two routers connected to it and both of them support multicasting in that case, only one router will actually do the querying so out of these two routers whichever has the smaller IP address will be the one which does the querying. (Refer slide time: 22:27 ? 22:38)

Slide time: 22:27 ? 22:38
Now we come to the topic of Multicast Routing. So, what is special in Multicast Routing? (Refer slide time: 22:38 ? 22:52)

Slide time: 22:38 ? 22:52
Let us see this diagram. Suppose there is no support for multicast at the network layer which is the case in many practical situations, even then you could sort of simulate multicasting by doing repeated unicast. So your original source has the list of all the members meaning all their IP addresses so it sends the message to all of them one by one, one by one, one after the other. So this is just a successive unicast done. Therefore this is as if a multicast. Obviously you are more packets are packed into hops if you take some measure like that and of course you are doing much more work than what is strictly necessary but this is all you can do. This has an advantage that the source can closely control that who could be a member of the group and who would not be a member of the group. Therefore you can impose some kind of access control by having an access list. (Refer slide time: 24:05 ? 26:37)



Slide time: 24:05 ? 26:37
Now, if there is support for multicasting then what would happen is that, let us say there is one packet being sent to all three members of the group and say this is the source therefore the source will send only one packet to the next router. This router is going to duplicate the packet as to one on this link and the other on this link. This packet is ultimately destined for this host whereas this packet again gets multiplied and now this packet goes to this host and this packet goes to this host. So, the number of packets traveling down is minimized a lot, of course the final number of packets is the number of users but if you just consider how much each packet travels, that means if you take the number of packets into the hop count kind of a weighted measure then that could be much lower overhead in some sense for packets that are traveling. So it is much lower overhead on the links but may be more overhead on the routers. Now, if you have to have this multicasting capability for the routers this specifically requires two things. One is, packet forwarding that can send multiple copies of the same packet. For example, consider this router, this router is receiving only one packet but it has in its list that there are two sort of users for which the packet is supposed to go through this router. So it has to duplicate the packet one for this link and other for this link. So it forwards multiple copies of the packet. This capability has to be there in the router. Secondly, multicast routing algorithm builds a spanning tree dynamically. But how you found this tree? This looks somewhat similar to forming a routing tree for unicast cases but there are some differences. This tree has to be built up dynamically and in a distributed fashion by the routers. For that they have to run IP multicasting protocol between themselves. That is what we mean when we say routers are supporting multicasting. (Refer slide time: 26:37 - 26:52)
  
Slide time: 26:37 - 26:52
Goal: The main goal of multicast routing protocol is to build a spanning tree between all members of a multicast group. So these are the members of the multicast group and we have to somehow get this tree. (Refer slide time: 26:52 - 27:57)
 
Slide time: 26:52 - 27:57
This can be looked upon as a graph theoretical problem. In whatever graph you have you have to embed the tree such that all multicast group members are connected by the tree. Suppose these are the three members of the multicast group seen in the previous graph and you want to form a tree like this then the only solution is to have a shortest path tree or source-based tree. That is, build a tree that minimizes the path cost from the source to each receiver. So this is the so called source-based tree and you can form this. Hence this is one kind of a solution. (Refer slide time: 27:57 ? 30:24)

Slide time: 27:57 ? 30:24
This is called a Source-Based Tree. 
- This is a good tree if there is single sender. So sometimes multicast group is such that there is a single sender. Take the previous example we were talking about, a central news service or some kind of financial advisory service has some members and these are the members of this multicast group and there are some stock codes. So this particular group may be interested in the quotation of a particular group of stocks and for this particular group of stocks this company collects all the information, the stock value, etc in a regular fashion and keeps on pushing it to the members of this group. Now, this is multicasting where there is a single sender. And in such cases making a Source-Based Tree makes quite lot of sense. But of course, if there are multiple senders you need one tree per sender. Now that becomes really difficult where it becomes more democratic where all the group members are interacting and any of them can send messages to any member in the group. Therefore you have to have a tree for each sender. Having a single sender is easy to compute. For this multicasting router we will always assume that whatever unicast routing is happening through OSP etc is always present here but this multicast is sitting as an additional service by the router which essentially means that the unicast routing table is available for building up your Source-Based Tree. So in such a case this is easy to compute. The tree is built from receiver to the sender. This is called reverse shortest path or Reverse Path Forwarding (RPF). (Refer slide time: 30:24 ? 31:20)

Slide time: 30:24 ? 31:20
A second solution to the same problem is that, if you remember the other graph, this graph looks different because this is the graph where it minimizes the total cost of the edges. If you do not know who your sender is going to be, that means if any member of the group can be a sender then it makes sense to make the tree in such a manner, suppose, if you assume that all of them send packets frequently then in that case having the tree with the minimum total cost of edges would be the optimum solution. So this is the second kind of solution. (Refer slide time: 31:20 ? 32:18)

Slide time: 31:20 ? 32:18
This is very difficult to compute, this is called the Core-Based Tree. 
- This is a good solution if there are multiple senders. Instead of keeping one Source-Based Tree for each potential source we keep one Core-Based Tree. 
- Very expensive to compute, not practical for more than 30 nodes for a very good solution.
- Selects one router as core (also called ?rendezvous point?). 
- All receivers build a shortest path to the core using the reverse shortest path or reverse path forwarding. 
But who would be the core of the rendezvous point depends on how good your core based tree is thereby depending on how you choose the core. If you have chosen the core towards the center of the potential graph then that is good. (Refer slide time: 32:19-34:26)


Slide time: 32:19-34:26
 Let us see the details of Reverse Path Forwarding (RPF). This is the way to build the tree.
- RPF builds a shortest path tree in a distributed fashion by taking advantage of unicast routing tables. 
- Main idea: Given the address of the root of the tree, you know the source. This tree is being formed from the sources. Each of the destinations, that means each of these potential recipients are trying to reach to the source and for that they use the unicast routing table which is already there in the router. So given the address of the root of the tree, a router selects its upstream neighbor in the tree, the router which is the next-hop neighbor for forwarding unicast packets to the root. So, what you do is, for each of the potential recipient you are trying to minimize the path cost from this recipient to the one single source. Right now for the Source-Based Tree you have one single source. Whatever you do for unicasting, while sending a message from this node to that node is what you have to follow. What the routers have to do is that, on the way suppose two different potential routes from two different recipients go through the same router then from this point onwards it is expected that this router to the final destination is actually the source of multicast communication and there is only one path and the tree would be automatically formed. This is the basic idea of Reverse Path Forwarding. (Refer Slide time: 34:27-34:48).

Slide time: 34:27-34:48
- How can this be used to build a tree? 
- RPF Forwarding: Forward a packet only if it is received from an RPF neighbor. 
- Set up multicast routing table in accordance from receiver to sender along the reverse shortest path tree. (Refer slide time:34:49-35:51)

Slide time: 34:49 - 35:51
This is an example. Suppose H1 is the source and RPF neighbor of R3, this is R3 and this is R1, R4 and R5. From these when they try to reach H1 they go from R3 to R2 to some path to H1. So R2 is the RPF neighbor of R3. The destination is H1 and the next hop is R2. This is the unicast routing table. So R3 knows that R2 is the RPF neighbor of itself. (Refer slide time: 35:52 - 36:09)

Slide time: 35:52 - 36:09
- Routing table entries for Source-Based Trees and for Core-Based Trees are different. 
- Source-Based Tree: For Source-Based Tree it is (Source, Group) or (S, G) entry. 
- Core-Based Tree: Naturally anybody can be communicating. So it is (*, G) entry. (Refer slide time: 36:10 - 36:49)

Slide time: 36:10 - 36:49
The Source IP address, Multicast group, Incoming interface (RPF interface) and Outgoing interface are the L2, L3 etc, this is a list. And finally when a packet arrives the router has to forward one copy of the packet along each of these outgoing links which eventually reach some members of this particular multicast group. (Refer slide time: 36:50 - 37:08)

Slide time: 36:50 - 37:08
For building a Source-Based Tree in a network like this set routing tables according to RPF forwarding and then use Flood-and-Prune. (Refer slide time: 37:09 - 39:43)

Slide time: 37:09 - 39:43
- Set routing tables according to RPF as we have already discussed. 
- Flood-and-Prune. 
What is flood? Forward packets that arrive on RPF interface on all non-RPF interfaces. Receiver drops packets not received on RPF interfaces. These routers require the capability of forwarding multiple copies of the same packet. So, if a packet has come from its RPF neighbor, (RPF neighbor is with respect to a particular group) bearing this address means that it is actually coming from the source. Now it has to be forwarded to each of the outgoing interfaces. Of course it is a non-RPF. This means, when I say RPF interface it means that the RPF neighbor is coming from the source side to all others which lead to different members of this Multicast group. What happens if you happened to get a packet from a link for this particular group who is not your RPF member? First of all, how did it happen? You must remember that we are doing this in a distributed and dynamic system so things can come up and go down. So that way a packet can come in. But obviously so far as this particular router is concerned if a packet comes from non-RPF link for this group then this is not coming from the source so that packet is dropped. And naturally it also does pruning. Pruning is sending a prune message when a packet is received on a non RPF interface. This is one case when you prune. Or when there are no group members in its local network and no connection to other routers. Suppose, it so happens that this particular router, you remember that this router is also connected to its local network and with the host in the local network it is running IGMP always finding out who are the members of group etc. It could happen that this local member has retired. It no longer wants to remain in the group that means it is no longer sending your IGMP reports. So it has nobody to send it to nor is it connected to neither any router nor a part of a link from a distant source to a distant destination, nor a transit link like that. So it is not connected in that case also. Now as the local contributor member of the prune has retired then this of course may not be known to others. So it may still get a packet but then what it will do is that, it will send a prune message stating not to send anymore packets to it any longer. So it will send the prune message along the route because the neighboring router has sent in a packet and it has nobody to distribute it to nor is it a transit router. So, naturally it drops this packet because it has no use for this packet. And along this link whoever has sent this packet to it the local member sends a prune message. It means do not send the local member any further packets because it has nothing to do with this group anymore. So that is one case, (Refer slide time: 41:23 - 42:24)

Slide time: 41:23 - 42:24
or, has received a prune message on all non RPF interfaces. That means, it was a member earlier with all the non RPF interfaces and just like the group member may have retired similarly this may also have been a transit router on a link from some distant source to distant destination this may be an intermediate router on the way. But then it has got a prune message on all its non RPF interfaces. That means it was a member of a transit link earlier but now it is no longer a member. So once again whoever had sent it a packet, it will send back a prune message to that destination that prunes this link as well. So this is no longer interested. (Refer slide time: 42:25 - 42:51)

Slide time: 42:25 - 42:51
- Prune message temporarily disables a routing table entry. 
- Effect: Removes a link from the multicast tree. 
- No multicast messages are sent on a pruned link. 
- Prune messages is sent in response to a multicast packet, which has come and which satisfies any of these conditions. (Refer slide time: 42:52 - 43:07)

Slide time: 42:52 - 43:07
Once again that it has received on the non-RPF interface or when there are no group members in its local network and no connection to other routers or it has received a prune message on all non RPF interfaces. So in such cases the prune message is sent. (Refer slide time: 43:08-43:19)

Slide time: 43:08-43:19

The prune message has the effect of temporarily disabling a routing table entry. The question is, why temporary? (Refer slide time: 43:20 - 44:16)


Slide time: 43:20 - 44:16
- Why the routing table is only temporarily disabled? 
- What happens is that, you may have a receiver who may again like to join. So one needs to reactivate a pruned routing table entry in that case. So what happens is that, this group member may have gone away somewhere and now has come back and wants to be a member of the group once again. So, it gets the IGMP report saying that it is a member of this group. Now, it is aware of who knows the source, this multicasting router. So what it will do is that it will try to reactivate this link and then the rest of it will work, so this is called Grafting. 
- Sending a Graft message disables prune and reactivates the routing table entry. So this pruning and grafting are complimentary to each other. You prune to disable and you graft to enable again. (Refer slide time: 44:17 - 45:50)

Slide time: 44:17 - 45:50
Next is the Core-Based Tree. This was a Source-Based Tree when you have one source and many receivers. Now you have many to many kind of situation. That was one to many communications, not one to all (not broadcast). But now we have many to many communications. That means there are many members of the group who might like to communicate with other members of that group. In this case we would like to have what is known as a Core-Based Tree. 
- One router is the core. 
- Receiver sends a join message to RPF neighbor with respect to core. Now every receiver actually wants to join to the core. 
- Join messages creates a (*, G) routing table entry. 
- Source sends data to the core. 
- Core forwards data according to routing table entry. 
Now, since there is no source or that anybody could be a source we put a star in place of s. For a particular router all the links get messages like this so a message may come in through any such link and it has to be forwarded to the other links. (Refer slide time: 45:59 - 46:47)

Slide time: 45:59 - 46:47
We just mentioned about multicast routing protocols which is actually implemented in most of the routers and many of the level 3 switches. This is called DVMRP. So you find them actually but unfortunately I have seen it very rarely being used. But this is there in most of the routers of today as well as in many of the Level 3 switches DVMRP is there 
- DVMRP is a Distance Vector Multicast Routing Protocol. 
- This is the first multicasting routing protocol. 
- It implements flood and prune. 
Distance vector routing, if you remember, the distance vector routing uses the distributed Bellman Ford algorithm which is implemented by RIP. The centralized diesters algorithm, the link state algorithm is implemented in the routing protocol called OSPF. (Refer slide time: 46:48 - 47:58)

Slide time: 46:48 - 47:58
Open Shortest Path First: Recall our discussion about routing protocols and this OSPF is currently the most acceptable routing protocol. There is a multicast extension of OSPF which is known as MOSPF (Multicast Open Shortest Path First). 
- Multicast extensions to OSPF: Each router calculates a shortest path tree based on link state database. 
- It is not very widely used. 
- PIM-SM builds Core-Based Trees but they are not widely used. (Refer slide time: 47:59 - 48:18)

Slide time: 47:59 - 48:18
- Distance-Vector Multicasting Routing (DVMRP) consists of two major components: 
- A conventional distance-vector routing protocol (like RIP). 
- A protocol for determining how to forward multicast packets based on the unicast routing table. (Refer slide time: 48:20 - 49:27)

Slide time: 48:20 - 49:27
- DVMRP (Distance-Vector Multicasting Routing) routers forward a packet if: 
- The packet arrived from the link used to reach the source packet. This is the reverse path forwarding. That means if from this router I want to reach the source then I have to go to that next hop and that is my RPF neighbor. So, if the packet arrives from this link from my RPF neighbor link, arrived from the link used to reach the source of the packet then this is the RPF chain. 
- Packet forwarded only to the child links not in the direction from which it came but other child links. 
- If provided the downstream links have not sent a prune message. (Refer slide time: 49:28 - 52:45)

Slide time: 49:28 - 52:45
But DVMRP has limitations: 
- Like distance vector protocols, affected by count-to-infinity and transient looping. In the count-to-infinity problem some link has failed but nobody could make out that the link has actually failed so it is going round and round known as the count-to-infinity problem where exactly the counting of potential distance to that link comes to infinity. Since we are using the same distance vector routing. Transient looping means sometimes a routing loop may form and the packet goes round and round. 
- Multicast trees are more vulnerable than unicast for these problems. 
- This shares the scaling limitations of RIP. And this scaling limitation essentially comes from what I have written in the last.
- No hierarchy: Flat routing domain. One of the advantages of OSPF over RIP was that in OSPF we break up the network into a hierarchy. There are these autonomous regions or autonomous domains and then further down it can be broken up so that the routing problem remains simpler and you can scale to bigger and bigger networks. Since DVMRP is based on RIP or essentially on the ideas of RIP it is again a problem in DVMRP also. Here you cannot scale and then you have further problem because of multicasting. 
- You may have (S, G) state in routers: even in pruned parts. 
- Broadcast-and-prune has an initial broadcast. When I say flood-and-prune, actually you are flooding the network so there is some kind of broadcast going on. If the network size is small, this broadcast may be acceptable but when the network grows bigger and bigger broadcast becomes unacceptable. So that is again another problem in scaling. 
- This is limited to few senders. Many small groups also undesired. Since this essentially forms a Source-Based Tree you can have only a few of them, just a few senders. Many small groups are also undesired. If you have large number of groups, once again you have the same problem of maintaining so many trees and that also becomes a limitation for scaling. (Refer slide time: 52:46 - 53:29)

Slide time: 52:46 - 53:29
Let us discuss about an effort to implement multicasting. As I told you, most of the routers are not configured to use the multicast in every manner but still some people want to use multicasting. So they built up this Multicast backbone (MBone) which is essentially an overlay network of IP multicast-capable routers using DVMRP. So it uses DVMRP and it is an overlay network of IP multicast-capable routers. What does that mean? (Refer slide time: 53:30 ? 55:05)

Slide time: 53:30 ? 55:05
That means, some of the routers in the network in some places are multicast-capable. And what happens is that they are going to support multicasting in its own locality. That means it will support multicasting amongst the network to which they are directly connected. These routers are going to run a multicasting protocol between themselves. But then in between there are whole lot of other routers. In between there is a cloud of routers which are not supporting multicasting so what it will do is that it will tunnel through this cloud to the next multicast supporting router. So this is the picture of the MBone. You have R which is the host or the router and this R and this H are the MBone routers. They support and the part of the MBone is shown in light blue where the multicasting is directly supported whereas when they try to communicate to another multicast supporting node over a cloud which does not support multicasting they tunnel through it. (Refer slide time: 55:08 - 55:18)

Slide time: 55:08 - 55:18
- MBone tunnel is a method for sending multicast packets through multicast-ignorant routers. 
- IP multicast packet is encapsulated in a unicast IP packet (IP-in-IP) addressed to far end of the tunnel. (Refer slide time: 55:21 - 56:07)

Slide time: 55:21 - 56:07
You have the IP header destination which is unicast and then you have another IP header destination that is multicast and then the transport header. What happens is that, the intervening routers which are not multicast-enabled are going to see this destination and this destination would then actually the next multicast router. And here this part will be the pay load so the network nodes would not look into this. When it reaches the next multicast supporting router it will get this and then discard this and then look at this multicasting header. So this is the IP-in-IP encapsulation and tunneling. (Refer slide time: 56:07 - 57:23)

Slide time: 56:07 - 57:23
- Tunnels act like virtual point-to-point link. 
- Intermediate routers see only router header. That means the unicast routing header. 
- Tunnel endpoint recognizes IP-in-IP (protocol type = 4) and de-capsulate the datagram for processing. 
- Each end of the tunnel is manually configured with unicast address of the other end. So, this is what you have to do. This was done to implement multicasting in an environment and try it out. If there are problems about the one which is actually implemented in most of the routers namely DMRP that does not scale well and if there are many groups, in today?s world when everybody in sort of networked and people have all their special interest etc it is quite considerable that the number of groups will explode if it could really do multicasting in a very easy fashion and that is very difficult for routers to handle. That is why most of them do not use it at the moment. But potentially this is a very useful kind of technology. Thank You. (Refer slide time: 57:27 ? 57:28 )

Slide time: 57:27 ? 57:28
Good day. Today we will talk about some protocols which are useful for controlling the network and making the machines connected to the network. Specifically, under the broadcast we will talk about DHCP and ICMP. There are some protocols associated with this and we will talk about this. (Refer slide time: 57:59 - 58:02)

Slide time: 57:59 - 58:02
(Refer slide time: 58:03 - 58:20)


Slide time: 58:03 - 58:20
DHCP is the Dynamic Host Configuration Protocol. It is about configuring a host, configuring a machine, configuring a may be a PC or some computer which is connected to the network. (Refer slide time: 58:20 - 58:44)

Slide time: 58:20 - 58:44
Its chief utility: There are other utility are DHCP we will be discussing later. The chief motivation came from dynamic assignment of IP addresses. Now, dynamic assignment of IP addresses is desirable for several reasons.

COMPUTER NETWORKS
Prof. S.Ghosh
Dept of Computer Science & Engineering 
IIT Kharagpur 
Lecture 33
DHCP AND ICMP
(Refer slide time: 00:47)

Good day. Today we shall talk about some protocols which are useful for controlling the network and also in keeping machines connected to the network.  Specifically under the broadcast we will talk about DHCP and ICMP. There are some protocols associated with this so we shall discuss about it.   

(Refer slide time: 1:14-1:16)



(Refer slide time: 1:18-1:38)

 DHCP is Dynamic Host Configuration Protocol. It is about configuring a host, configuring a machine may be a PC or some computers connected to the network. 
 

(Refer slide time: 1:35-3:29)
The chief motivation came from the dynamic assignment of IP addresses.  The dynamic assignment of IP addresses is desirable for several reasons:
IP addresses can be assigned on demand.  
For example, when you have a scarcity for real IP addresses then you keep a central pool of IP addresses and then as some computer comes on line it assigns an IP address from the pool and when it goes out those IP addresses are withdrawn and are given to some other machines.  
Another place where it may be required is, suppose you have some kind of a RAS or Remote Access Server to which a number of machines should be connected via dial up connections then in that case you give them a temporary IP address for the connection. Now if somebody wants to visit some network with a laptop then they have to get a network address of that particular network therefore that network address has to be dynamically assigned. 

IP addresses are assigned on demand.
It avoids manual IP configuration which is prone to errors. 
It also supports mobility of laptops. 


(Refer slide time: 3:30-3:59)
Dynamic assignment of IP addresses is done using three different protocols.  
1.   RARP: It was widely used up to 1985 and even beyond this period people kept using it. 
2.   BOOTP: Bootstrap protocol was used until 1993
3.   DHCP: Is used after 1993 and currently this is in wide usage.  
A  Bootp client can also use DHCP server.   
 

(Refer slide time: 4:08-5:33) 

RARP is actually the reverse ARP Address Resolution Protocol. The problem is, given an IP address what is the MAC address. This is for mapping between the IP addresses and the MAC addresses. Finding the MAC address for the IP address is useful when you want to communicate over a LAN. RARP is the reverse of this. Given a MAC address, RARP finds IP address. This would be necessary in case you have something like disclosed work station which boosts the signal over the network. A disclosed work station has its own MAC address and it wants to get an IP address assigned. This is where RARP is typically used. RARP is used to broadcast a request for IP address associated with a given MAC address. RAP server responds with an IP address. It only assigns IP address and not the default router, subnet masks etc that are required and they are not a part of this server.  
Refer slide time: 5:34-5:46)

(Refer slide time: 5:34-5:46)
So IP address to MAC address is the ARP and Ethernet MAC address to IP address is the RRAP.
 

(Refer slide time: 5:45-6:39)
Let us see the improved version of RARP i.e. BOOTP.
BOOTP not only assigns IP addresses dynamically but also has some more functions. Host can configure IP parameters at boot time. Basically there are three services: 
 IP address assignment 
Detection of the IP address for a serving machine
The name of the file to be loaded and executed by the client machine i.e. the boot file name  
This is the source from which it gets the name bootstrap protocol i.e. when the machine is booting up it not only gets an IP address but also gets the name of the file which can be loaded and executed. This is the bootstrap protocol or BOOTP.

(Refer slide time: 6:40-7:37)

 BOOTP not only assigns IP address but also default router, network mask, etc.   Therefore whatever that particular machine requires for communication namely the addresses such as network, subnet mask, gateway etc are given by the BOOTP protocol. This is sent as an UDP message. So UDP port 67 is for server and UDP port 68 is for the host. Port 68 for host is required when you may want to find out a machine from bootstrap protocol which is already available on the network. 
And use limited broadcast address that is 255. 255. 255. 255. If you recall from our discussion about addresses this is a broadcast address where the broadcast is limited to this particular subnet or network.

 
 
(Refer slide time: 7:51-8:14)
BOOTP can be used for downloading memory image for diskless workstations. So whatever was the motivation for RARP the same thing can be done through BOOTP also. But assignment of IP address to hosts is static. This is one sort of drawback of BOOTP 


(Refer slide time: 8:16-8:52)

To make it dynamic we go to the dynamic host configuration protocol which is standard now and more versatile than RARP and BOOTP. It can do a lot of things apart from just giving the IP address. This was designed in 1993 as an extension of BOOTP with many similarities to BOOTP and same port numbers as BOOTP.  That is why DHCP server can handle a few BOOTP clients.  

(Refer slide time: 8:52-10:05)

Extensions: There are lots of extensions especially with options. But these extensions support temporary allocation or leases of IP addresses. Leasing of IP address, suppose when we have a remote access server and when people are dialing what would happen is that, it would be given a particular IP address for a fixed amount of time.  When its lease expires then that IP address may be withdrawn. And half way down the lease period if there is no great demand for IP address then the lease may be automatically extended or if there is a great demand the lease may be withdrawn also. This is for a temporary period of time and that is how it is dynamic. DHCP client can acquire all IP configuration parameters. Not only subnet mask and gateway addresses which are there in BOOTP but also other kinds of parameters can be downloaded from a DHCP server.
  

(Refer slide time: 10:16-10:23)


So DHCP is the preferred mechanism for dynamic assignment of IP addresses and DHCP can interoperate with BOOTP clients.

(Refer slide time: 10:24-12:17)

DHCP has a number of options. It is not possible to mention all the available options here. Other DHCP information is sent as an option so the number of options is actually greater than 100 which include things like subnet mask, name server, host name, domain name, forward on/off, default IP time to leave, broadcast address, static route, Ethernet encapsulation, x window manager, x window font, DHCP message type, DHCP renewal time, DHCP Rebinding, time server SMTP server, client FQDN, printer name etc. As the number of services given over a network grew it became important to give more information to the machines. Originally the machine was used just for communicating between two computers. Suppose there may be a centralized print service in the network and whenever you want to print something it can be done in the network. Similarly all other kinds of services became available in the local network as well as over wider networks. So all these would require some kind of configuration on the host end therefore such information can be transferred via this DHCP.   
 (Refer slide time: 12:18-12:50)

(slide time: 12:18-12:50)

There are a number of DHCP operations. Let us discuss a few of them.  
 DHCP DISCOVER: 
At this time the DHCP client can start to use the IP address.
Renewing a lease: 
It is sent when 50% of the lease has expired. If DHCP server sends DHCPNACK then the address is released. Then you know your lease is not going to be renewed.


                                     ( slide time: 12:49-12:55)

DHCP RELEASE: At this time the DHCP client has released the IP address, so the client has given it up.
 (Refer slide time: 12:56-13:40)

                                       (slide time: 12:56-13:40)

DHCP message header fields:  In some fields there is an opcode. It may be a DHCP request from the client or it may be DHCP reply from the server. The DHCP message type is sent as an option. The hardware type of message is 1 for Ethernet and hardware address length is 6 for Ethernet. Hop count is set to 0 by client and transaction ID is an integer used to match reply to response if there is more then one request.
(Refer slide time: 13:38-14:30) 

(Refer slide time: 13:38-14:30)

Seconds: It is the number of seconds since the client started to boot.  
Client IP address, your IP address, server IP address, gateway IP address, client hardware address, server host name, boot file name, etc. All these fields are available so when the client sends the request it would fill in whatever is known to it, maybe the MAC address is known to it. So it puts in the MAC address and all other fields are left blank.  
 DHCP server will pick up the message that we broadcast and then fill up all the other necessary fields and then broadcast it back.
 

 (Refer slide time: 14:29-14:50) 

The following are the DHCP message types sent as an option: 
DHCPDISCOVER
DHCPOFFER
DHCPREQUEST
DHCPDECLINE
DHCP acknowledge
DHCP not acknowledge
DHCPRELEASE
DHCPINFORM and so on  
 

(Refer slide time: 14:48-14:58

Our next topic is ICMP Internet Control Message Protocol.   
(Refer slide time: 15:05-15:30)
Let us see IP protocol and its deficiencies before that.  



(slide time: 15:05-15:30)

The internet is of course based on the Internet Protocol. IP protocol has some drawbacks. Though it is a best effort delivery service it lacks error control and lack of assistance mechanisms. Since IP is a best effort delivery at some point of time the effort may not be enough and routers ore other nodes on the network may have to drop packets and packets may not reach its destination on time and in proper order.. First of all there is no error control and secondly if such errors do occur there is no message to the sources. Secondly, if you want to control the network for some reason, for example, may be the network is getting congested and so you want to do something about it, but IP does not have the mechanism. So, for all these purposes ICMP was brought in.
 

                                     ( slide time: 16:36-16:55)

Therefore what happens if a router must discard a datagram because it cannot find a route to the final destination? What if the time to live field has zero value? What if it has to discard all the fragments because not all were received in a predetermined time limit? In all these cases IP has to discard a packet.  
 

                                       (slide time: 17:05-17:24)

And similarly there are other situations. For example, may be it has reached the destination but the port is not available. So IP protocol also lacks a mechanism for host and management queries. So ICMP was designed to compensate for these deficiencies. 

                                          (slide time: 17:25-18:01)

 ICMP is a type field that indicates the type of ICMP message being sent and the type may be queries or errors. Code field gives further information specific to the ICMP message. For example, when an error occurs it tells what kind of error it is.   Checksum field is used to verify the integrity of the ICMP data. So once again the checksum is included to control the error.  


(Refer slide time: 18:00-18:24)
There are two types of ICMP messages. One is error reporting and the other is query response. If there is some error then the error reporting type of ICMP message would be generated and if there is a query another type of ICMP message would be generated.  
 

(Refer slide time: 18:23-18:29)
There is no effort in ICMP to correct the errors. This is the job of some other layer. So it does not really try to correct the errors but nearly reports the errors. The error messages are sent to the source. Suppose the datagram has been sent and something has happened to it and due to that there is an error, and now whoever drops that packet send an ICMP message back to the source. It may be a router on the way or may even be the final destination.  
 

(Refer slide time: 19:23-19:50)
These are the various types of errors in error reporting. There may be a destination unreachable, there may be a source quench sending to first. Some of the important ones may be time exceeding, some may be parameters problem or redirection, etc.   


(Refer slide time: 19:51-21:29)
Please note that, no ICMP error message would be generated in response to a datagram carrying an ICMP error message. That means, somebody has generated an ICMP error message and it is traveling back to the source, and that error message itself gets an error and may have to be dropped on the way, then in such cases we do not generate another ICMP message. A little bit of problem happens due to congestion of networks. So if the network is very congested many packets may get dropped. And then if in response to dropping many packets you generate more packets then the congestion is not going to go away. So, ICMP error messages do not trigger other ICMP error messages for a fragment datagram that is not the first set of fragment. For example, the datagram may have been fragmented into a number of parts, may be fifty parts, now for each of them you generate an ICMP message. Then the ICMP message would be too many so it is only generated for the first fragment. For a datagram having a multicast address, once again we cannot send an ICMP messages to all members of the group and for a datagram with a special address such as 127. 0000 or with some address like 0.0.0.0.0 also no ICMP error messages are generated for these.  
 (Refer slide time: 21:30-23:15)

All error messages contain a data section that includes the IP header of the original datagram plus the first 8 bytes of data in that datagram. This information is required so that the source can inform the protocols about the error. From the original packet that was dropped the IP header of that original packet is sent back. First of all you need to know the source and know where you want to send back this ICMP message.  Secondly, even after this ICMP message gets back to the specific machine from which the original packet was generated. At this point it may have some error messages due to network intervening or this may have to do something with some process or application which is running on the source machine. So, after getting the message the host must know to which process it relates to. After the IP header what comes is the transport layer header so, a part of the transport layer header also goes back along with the ICMP message .This information is required so that the source can inform the protocols about the error.  
  

(Refer slide time: 23:16-23:50) 

Destination Unreachable: This is one type of an error message. When a router cannot route a datagram or a host cannot deliver a datagram, then in that case the destination is unreachable. A router cannot detect all problems that are preventing the delivery of a packet. So it is not always possible to exactly know why the destination is unreachable. But at least this information that the destination is unreachable, goes back to the source.  
 

(Refer slide time: 23:51-25:30) 
Source Quench: This is a crude attempt to implement some kind of flow control. IP protocol has got no flow control. Routers and hosts have limited size queues. So what happens is that, may be in an intermediate router and a number of packets have come up and certainly there is a flood of packets into one intermediate router from various directions. So what would happen is that its buffer is going to overflow and it will not be able to process because there is a limit depending on the speed of the router etc, there is a limit as to how fast packets can be processed and forwarded by an intermediate router and if other packets keep coming in, within that time they are going to be stored in the buffer where in the buffer might overflow. In that case the router cannot do anything else but to drop those packets. This router desperately wants to tell other people in the network to slow down on sending packets and that it cannot handle it because of overload. Basically it tries to slow down the flow of packets into itself. So it sends the source quench ICMP message towards the sources. If datagram is received faster than they can be processed the queue may overflow and in that case it asks the network to slow down.  

(Refer slide time: 25:32-25:43) 

If a router or host discards a datagram due to congestion it sends a source quench message to the sender. The source must slow down the sending of datagram until the congestion is relieved.  
 

(Refer slide time: 25:44-26:03) 
This may be used when bottlenecks occur. For example, on a WAN link with too much congestion it is used to reduce the amount of data lost. But a warning is, source quench message will in turn generate network congestion. There were already too many packets in the network but you have sent a source quench packet towards the source which is just one hop towards the source was already getting packets from the source but will also get an ICMP message from the router just one hop down so it is having more packets now. So by this way congestion might travel towards the source but anyway finally it reaches the source and the source will hopefully slow down and all these will die.  

(Refer slide time: 26:48-27:18) 

Time Exceeded:  Whenever a router receives a datagram with a time-to-live value of zero that means it has been going round the network it discards the datagram and sends a time exceeded message to the source. When the final destination does not receive all of the fragments in a set time it discards the received fragments and sends a time exceeded message to the source. These are two different cases: One is that, in the destination all the fragments did not reach so there was a specified time after which it has to drop all the fragments and send a time exceeded message to the original source.  
The other thing is that, when a router receives a datagram with the time-to-leave field which is zero. If you remember, keeping a time-to-leave field and decrementing it at every hop is quite important because suppose there were some packets which were floating around in the network and due to some trouble with the routing tables a loop has been formed, so, if you do not have this time-to-leave field it will go round and round at infinite term where they slowly burden the network. So, the solution to that was, after a certain number of hops the packet is dropped and when a packet is dropped a time exceeded message is sent to the source.  
There may be parameter problems.
 

(Refer slide time: 28:35-28:48) 
If an ambiguity is found in the header of a datagram the datagram is discarded and a parameter problem message is sent back to the source.  


(Refer slide time: 28:47-28:59) 

Redirection: A host usually starts with a small routing table that is gradually augmented and updated. One of the tools to accomplish this is the redirection message, so, actually this helps in routing.  
 

(Refer slide time: 29:00-29:18) 

Now let us come to queries. ICMP can also diagnose some network problems. For example, echo request and reply, time stamp, address mask, router solicitation and advertisements, these are example of queries. We will just see a few of these also. 


(Refer slide time: 29:22-30:01) 
Echo request and reply: Is used very often when you want to find out whether the network is up and running or not. An echo request message can be sent by a host or router. An echo reply message is sent by the host or router which receives an echo request message. The echo request and echo reply message can be used by network managers to check the operation of the IP protocol.  
   Echo request and echo reply message can test the reachability of a host. This is usually done by invoking the ping command. Later on we will get into more details of ping because that is one kind of command which even users quite often require. For example, if you are logged on and you find that you cannot reach your destination anywhere in the network then you have to find where the problem lies, is it in your local network or in the local subnet. Therefore in the local subnet you might ping that gateway to see whether you can reach up to the gateway. If your ping message goes up to the gateway and you get an echo reply then you know that up to that much the network is ok. And if you are ok up to the router you may want to ping the router in the entire network. Now the problem may be somewhere in the link outside. The problem may even be in the destination which you are trying to reach. So one way is to go probing the network, even by users is to use ping.    
 (Refer slide time: 31:16-31:47)

(Refer slide time: 31:16-31:47)
Timestamp Request and Reply: Timestamp request and timestamp reply messages can be used to calculate the round-trip time between a source and a destination machine even if their clocks are not synchronized.  So, sending time is equal to the value of receiving time stamp minus value of original time stamp.  So this way you can get some idea about the round-trip time. There are other ways also.  


(Refer slide time: 31:49-32:12)

So receiving time = time the packet returned ? value of transmit timestamp. Round-trip time = sending time + receiving time.  So the timestamp request and timestamp reply message can be used to synchronize two clocks in two machines if the exact one-way time duration is known.  
 

(Refer slide time: 32:14-32:21)

Address-Mask Request and Reply: Enables a host to request and receive the network or subnetwork mask. It is useful for diskless stations at start up. But we have seen the DHCP is another way of handling this.  

(Refer slide time: 32:23-32:43)

Router Solicitation and Advertisement: Allows request of routing information and the reply of this information. Routers can periodically send router advertisements without being solicited. Suppose a router has just been connected to the network, anyway the routers have to run the routing protocol like the RIP or BGP, OSP etc, this means it needs to communicate to the neighboring routers, but how do the other routers know there is a new router in the group. So, one way is, as soon as the router gets connected it does some router solicitation and it advertises itself so that other routers get to know that and slowly the entire network becomes aware of this new router which is connected. Similarly, a link may go down and all kinds of other things may happen. So, the exchange of router information has to happen through some mechanism.  

(Refer slide time: 33:44-34:15)
Router Discovery Message: Host can learn about available gateways to other networks. Host send the router solicitation message to begin the process using the multicast address of 224.0.0.2 as the destination. It can also be a broadcast message in case a router does not accept multicast messages. When a router receives the message it will advertise its available gateway.  
 (Refer slide time: 34:16-34:28)

(Refer slide time: 34:16-34:28)
The checksum of the ICMP message: In ICMP the checksum is calculated over the entire message, that is the header and data combined. This is just to keep some control over errors.   

(Refer slide time: 34:26-34:48)
Clock Synchronization: Software may require time synchronization. So ICMP time stamp message combats this problem. It allows local host to ask for the current time from a remote host using ICMP timestamp request. So it is type 13.  
 

(Refer slide time: 34:47-35:04)
Remote host uses ICMP timestamp reply which is type 14. So, the better way of synchronizing the clocks is to use the network time protocol. The time is the UT Universal Time.  


(Refer slide time: 35:05-35:08)

Ping and Traceroute:    

(Refer side time: 35:15-35:55)
This is an overview. This is part of the ICMP messages. Ping sends an ICMP message to a remote host and lets you determine if that host is responding.  Actually ping uses echo and echo reply for the ICMP message. Traceroute uses TTL fields to query all hosts enroute to a specific destination. you can use traceroute to map a network. That means, if you want to know which is the route you are tracing then this helps you.   

(Refer slide time: 35:58-36:46)
Ping is named after sonar. In sonar if you want to probe some place you send an ultra sound signal just like you do in radar and if it bounces of something you get a ping, so that is where the name comes from. If you want to send an echo request you expect an echo reply and that is your ping. So server normally implemented in kernel uses ICMP echo and echo reply messages. On UNIX the identifier field is set to UNIX PID or sending process. Sequence numbers starts at 0 incremented every time a new echo message is sent. Actually, when you ping a machine not just one request is sent. The machine you are trying to ping or the channel may be noisy and if that happen then your echo request or the reply may get dropped in between. So, sending one request is not sufficient and may be three times or five times etc you can configure it, it sends echo request and it expects all the three or all the five replies. And if it receives none of them, then in that case it will say that hundred percent of packet loss or it may get two out of five so it will say sixty percent of packet loss or forty percent.  
 (Refer slide time: 36:37-39:46)


(Refer slide time: 36:37-39:46)
Let us see one example of ping. Suppose we ping a machine
144. 16.182.1, we have pinged this machine and then give the IP address over here. By the way if you have a name server on the network you could also put the name over there. Ping 144. 16.182.1, 56 data bytes is your data plus it will have some thing. So you may get a result like this: 64 bytes, this is what you are getting from the echo reply, 64 bytes from 144 16 182 1. ICMP sequence = 0 time-to-leave is 240 and time = 37 milliseconds. So it gives you some idea about how much time it takes. Then another packet has came back as an echo reply 64 bytes from the same machine, sequence number 1 and time is so much. For each packet it receives back as reply it is going to print a line like this and then finally it will give you a statistics as something like this: 13 packets transmitted, 11 packets received which means that it had originally sent 13 packets and it got only 11 packets back so 2 packets must have got lost. Therefore it is a 15% packet loss. And the round-trip time you may calculate the mean, average, max.  So, from the ping you can get an idea about the round-trip time.   


(Refer slide time: 39:51-40:12)

Some details on the output sequence number are shown for each message. In our example message returned in order but we lost some packets. They may be returned due to out of order. Also TTL field of return message is displayed and round-trip time is calculated at the host based on the sequence number.  
 (Refer slide time: 40:14-41:45)

We can estimate not only the round-trip time but also the bandwidth using ping.  But this works only for few hops. If it is beyond a number of hops your ping will not work. The ping packet can estimate the bandwidth in this way: 20 byte IP header, 8 byte ICMP header, 56 byte message this can be set by the user, so the total datagram size is that + 76 +8 = 84 bytes so 84 bytes were sent. Now, if it was sent through PPP it will add about 8 bytes so the total size will be 92 bytes.   So this connection looks like 92/.180/2 that is about 1069 bytes per second.  What is this .180? It is 92 bytes so this is time. This gives you some idea about what kind of bandwidth you have. In this particular case the bandwidth is not that much but it is only about 1069 bytes per second. This is a very crude estimate but you can get some kind of feel about your immediate locality.  

(Refer slide time: 41:56-42:23)
Record Route Option: Most ping implementations provide record route which is ? R option on linux, ? r option on windows.  Each router stores its address in the IP options field, only 9 addresses are possible. Thus round-trip is only possible for 4 routing hops. So you can take only 4 hops and within those 4 hops you can find out that how your message went and how it came back. That is, may be it came back through different paths or it could have returned in the same path etc.  You can actually trace the route and because of the limitation on the size that is on the number of addresses you can store you can only route or map the network in your immediate locality. But if you want to go beyond this then you have to use something else called Traceroute.  

(Refer slide time: 43:12-46:16)

Traceroute uses a sequence of ICMP messages to determine the current route to a particular destination. This is actually done in an iterative fashion. Suppose I want to traceroute to a distant machine whose IP address is known. Then I will send a message to that machine but with a very small number for the time-to?live. Therefore what will happen is that my message will take so many hops but then it has not reached the destination so may be it must have just started and it will be somewhere in the beginning so its time-to-leave is going to become 0.  As soon as the time-to-live becomes 0 the intermediate node may be that router will have to drop the packet and it sends an ICMP message back to the source. Now my program gets this ICMP message and now it sends the same dummy message to the destination after increasing the time-to-live by one unit. Now it is going to pass that router that had dropped the packet in the previous instant and so it will go one more hop and then the packet will get dropped so that router is now going to send an ICMP message back to the source. Now we will know which router is on the way. Therefore by this way iteratively you keep on increasing the time-to-leave one by one and you trace the entire route, that is, you map it out.  
      But let us see what happens when it reaches the destination? When it reaches the destination what happens is that this message is sent to a very unlikely port, a randomly selected port. Most probably the destination machine will not know about this port so it will say that the port is unreachable and then that ICMP message will come back. Now we know that we have reached the destination. Hence this way we have traced the entire route one by one from the source to the destination.  Traceroute uses a sequence of ICMP messages to determine the current route to a particular destination. The TTL specifies the number of hops a message can travel. Trace route sends UDP datagrams while varying the TTL. The router that drops the UDP packet now replies with a time exceeded ICMP message.  
 

(Refer slide time: 46:20-46:40)

The end point will not reply with that ICMP message because it has already reached there. So traceroute sends to an unlikely UDP port. Eventually get a no such port ICMP message. It knows that it has reached the end.  

(Refer slide time: 46:41-46:46)

So this is the reference about ICMP messages. Actually these are not the only internet control message protocols but there are a number of others which we did not discuss. We just discussed a few of them. There are other protocols like DHCP, BOOTP, RARP, ARP.  For example, they help in running the network in a better fashion. ARP protocol is a low level protocol. Then we have this RARP, BOOTP and DHCP for assigning a network. This ICMP helps in controlling the network operation and giving error messages. Then there is another side protocol which we will discuss in the next class namely IGMP which is internet group management protocol. So, that is another part of routing that we have not discussed as yet.    
Preview of the next lecture
Lecture ? 34
DNS & Directory

Good day, so today we will take up two topics and they are DNS and directory.  First let us talk about DNS.

(Refer slide time: 48:29-48:33)



(Refer slide time: 48:35-50:00)

The DNS is a short form for Domain Name System.  Until now we have seen two kinds of addresses. One is MAC address or the so called hardware address and in case of Ethernet they are also called Ethernet addresses which is used in the data link layer for direct communication.  
             Then we have seen IP addresses which are used for communication  between two end points and these two end points may be anywhere in the network. So IP address includes information used for routing and there is a network part and post part etc.  
             But unfortunately these IP addresses are tough for humans to remember. You can remember only a small part of address which you basically require for your own configuration like your own IP address, address of your gateway, address of your mail server etc. But beyond that if you have to remember IP addresses of other people it becomes very difficult for human beings to remember.  
          And of course they are impossible to guess. Humans find it much easier to remember and use the domain names. Domain names are what people use in surfing the web and for a www site sometimes you do not know the name exactly but you make out some guesses by some combination of .com or .net etc. We not only can guess but it is much easier to remember. We remember so many site names.  But now, what is the name? It is used to map some particular machine or site.  Therefore this is also some kind of an address.
          So we have a third layer which are the domain names. Today we will see how we use these domain names. And of course just as in the local area network you require mapping from IP addresses to MAC addresses which is done by the ARP protocol and in the reverse MAC address to IP address done by RARP. Here you need a mechanism for mapping the domain names to IP address.  

(Refer slide time: 51:51-52:36)

We have discussed this, why not centralized DNS>? Single point of failure, traffic volume, distant centralized database would not work, maintenance would be a problem, does not scale so it is distributed. So, no server has the entire name to IP address mapping. Local name servers: Each ISP Company has local name or the default name server and host query first goes to the local name server. Authoritative name server:  For a host, towards that hosts IP address names can perform name address translation for that host name.    


(Refer side time: 52:37-53:10) 
   
There are some root name servers which are [[?.centric]] and some of the biggest name servers are in USA but of course it depends on what root it is and they could be distributed also.  
 

(Refer slide time: 53:10-53:36)

NSLOOKUP is an interactive resolver that allows the user to communicate directly with a DNS server.  So, from the OS you can use this NSLOOKUP and give a name query.  This is actually a name server look up and that is how the term NSLOOKUP comes. NSLOOKUP is usually available on UNIX workstations.   

.   (Refer slide time: 53:38-53:55)

Servers handle requests for their domain directly. Servers handle request for other domains by contacting remote DNS servers. Servers cache external mappings.
 

(Refer slide time: 53:56-54:21)
If a server has no clue about where to find the address for a hostname it asks the root server. The root server will tell you what name server to contact. A request may get forwarded a few times.  For example, if the IITKGP has a name server.  Now the IITKGP has a name server and request for a particular domain name translation has come to it and it does not know to whom to connect the name server so it can always transfer it to the next higher level namely ERNET and if the ERNET also does not know where this ERNET is there then it can contact the IN. IN will definitely know all the sub domains under it. IN has to know because it is administering that domain.  
      Similarly if it is for some address which is from outside you can send it directly to the root of that particular domain, suppose from India you are trying to contact something for Japan then you can send it to the JP root name server and then JP root name server would know which name sever to contact so that will come back.  So this way the DNS queries will go back and forth few times and finally the name will be resolved.  

(Refer slide time: 55:30-56:39)

Now we come to LDAP which is the Lightweight Directory Access Protocol.  Since this was designed by the same people who has designed OSI this X.500 actually tends to be a little complex. It is heavy and it uses the all the seven OSI layer for the internet purpose. Actually it uses the TCP IP stack rather than the seven layer OSI layer stack. There was a lightweight directory access protocol which can interoperate at least on one side. So the LDAP can use that X.500 directory service but this is much simpler than X.500 and LDAP is used in many places. These are Lightweight Directory Access Protocol. It supports X.500 interface, it does not require the OSI protocol. It uses the TCP IP protocol so this is X.500 for the internet crowd. It is useful as a generic addressing interface like Netscape, address book and so on.  

(Refer slide time: 56:41-56:57)

The LDAP or Lightweight Directory Access Protocol is a networking protocol for querying and modifying directory services running over TCP IP. An LDAP directory usually follows the same X.500 model which we have discussed.  


(Refer slide time: 56:58-57:24)
Now it is a tree of entries, each of which consists of a set of named attributes with values. An LDAP directory often reflects various political geographic and or organizational boundaries depending on the model chosen. When you do that you can also define your security policies based on this directory and based on these boundaries. These are especially authentication services.  
 

(Refer slide time: 57:33-57:40)
So directory is a tree of directory entries.  An entry consists of a set of attributes an attribute values.  The attributes are defined in the schema.  

(Refer slide time: 57:43-59:00)

So this is the protocol stack for LDAP. Suppose you have a directory based application or some authorization service or may be access to some information which may be there for the organization which uses LDAP may use TLS. This TLS is Transport Level Security. You can also use SSL.  Here we are talking a lot about security. In future we will give one lecture to security because this has become so important. Now, in an organizational context a directory may be an important component of the entire security arrangement. Security is a complex issue. But anyway for this LDAP we need to communicate securely in many cases and many LDAP implementations support this TLS the Transport Level Security or you can use SSL or SASL also.  
	






COMPUTERS NETWORKS
Prof. Sujoy Ghosh
INDTAN INSTITUTE OF TECHNOLOGY
IIT KHARAGPUR
Lecture - 34
DNS & Directory 
(Refer slide time: 00:42)

Good day. Today we will take up two topics, DNS and directory. First let us talk about DNS. (Refer slide time: 00:54-00:56)

Slide time: 00:54-00:56
DNS is the short form for Domain Name System. (Refer slide time :00:57-4:19)


Slide time: 00:57-4:19
 We have seen two kinds of addresses. One kind is MAC address or the so called hardware address. In the case of Ethernet it is also called as Ethernet addressed which is used in the data link layer for direct communication. Then we have seen IP addresses used for communication between two end points. The end points can be anywhere in the network. So, IP address includes information used for routing. IP is used for routing where there is a network part and post part etc. But unfortunately, this IP address is tough for humans to remember. We can remember only a few addresses that we really require for our own configuration, like our own IP address, address of gateway, address of mail server etc. Beyond that it is very difficult for human beings to remember IP addresses of other people. They are impossible to guess. What human beings find is, it much easier to remember and use the domain names. Domain names are used most of time by the people for surfing the web (www site name used for surfing). While surfing the web for a www site, sometimes you may not know the name exactly but you can make it out in 3 or 4 guesses, may be make some combination of .com, .net etc. The most important thing here is that it is much easier for humans to remember site names. The name is used to some particular machine, site etc. This is also some kind of an address and we have the third layer namely the domain names. We will see how we use these domain names. Just as in the local area network, you require a mapping from IP addresses to MAC addresses done by the ARP protocol and the reverse MAC addresses to IP addresses using RARP. You need a mechanism for mapping the domain names to IP addresses. That is what this whole scheme of DNS is all about. May be sometimes we will look into the reverse query also. (Refer slide time: 4:20-5:58)


Slide time: 4:20-5:58
- The domain name system is usually used to translate a host name into an IP address. 
- Domain names comprise a hierarchy so that names are unique, yet easy to remember. 
It is important to remember that, if it is an address it needs to be unique. In IP v4 we ran out of IP addresses because we had limited length. Here, we can go easily with the length because these are not to be used in high speed computations but may be used just once or twice in a session. This means you can allow longer names, that is, rather than 4 bytes you can use many bytes but then this also has to be unique. What people thought was, if we make a hierarchy of names which is a logical hierarchy which corresponds with the external world then not only they will be remembered easily but can be easily administered and we can make them unique. So, domain names comprise a hierarchy so that names are unique yet easy to remember. (Refer slide time: 5:59-6:48)
  

Slide time: 5:59-6:48
This is what the hierarchy looks like. There is a root and from the root we have the top level domain. Here we have edu, com, org and so on. And finally these are for some nations. Under edu, we have mit, Albany and all other kinds of organizations. Under in, there may be ernet and all that. Under mit, also there may be cs or something. So there is a tree and a hierarchy. This is sort of a global hierarchy and is the DNS hierarchy. (Refer slide time: 6:49-8:34)


Slide time: 6:49-8:34
- Each host name is made up of a sequence of labels separated by periods. 
o Each label can be up to 63 characters long
o The total name can be at most 255 characters. So, you have up to 255 bytes to code these names so you can go easy on the length 
- Examples: Whitehouse.gov, .gov is for government and since it started in USA, it is US government and whitehouse.gov is a domain. Similarly let us say csc.iitkgp.ernet.in, .in is a top level domain standing for India and ernet is an organization which comes in the next level in in. Under ernet, there is iitkgp which comes in the next level and under iitkgp there is csc which again comes in the next layer. So, starting from here you can go from the leaf of the tree right up to the root of the tree that is the DNS hierarchy tree. And, after giving a label at each level we put a dot and then go up to the next level. This is how we name hosts. (Refer slide time:8:35-9:04)


Slide time: 8:35-9:04
- The domain name for a host is the sequence of labels that lead from the host to the top of the worldwide naming tree.
- A domain is a subtree of the worldwide naming tree 
So, mit is a domain, ernet is a domain and so on. There is a subtree under ernet and there will be many sub domains. So any domain is a subtree of the worldwide naming tree. (Refer slide time: 9:05-9:25)


Slide time: 9:05-9:25
- A host has a domain name specified using a sequence of names, each of which may be up to 63 characters long, separated by periods. 
- Names are case sensitive.
- A domain is an absolute domain name or a fully qualified domain name (FQDN), if it ends with a period. (Refer slide time:9:26-10:41)


Slide time: 9:26-10:41
- Most generic domains (.com, .edu, etc) are international, but there are some like .gov for government and .mil for military are US-specifications. So when you say .gov, it actually means US government and .mil is for US military. If you remember, the history of internet started in USA from the ARPA net and it evolved. At that time, it was very US centric but now the whole world has embraced it. 
- New top level domains recently been proposed, though they are not very popular yet.
- Countries each have a top level domain (2 letter domain name). For example: in for India, jp for Japan, uk for UK and so on.
- A system is required to map the domain names to IP addresses. Just like we have ARP for IP address to MAC or RARP for MAC to IP address, we need a system to map domain name to IP address that is the chief one and may be the reverse also. (Refer slide time:10:42- 12:45)
 

Slide time: 10:42- 12:45
Implementing DNS: 
- Distributed database implemented in hierarchy of many name servers. This is distributed because this is widely used and you cannot have a centralized database. Centralized database will be much more difficult to administer, Contribute to a single point of failure and the network traffic at that node will be tremendous. You will not be able to handle the network traffic if everybody tries to login to the same central name server. That will not scale and it needs to be distributed. Later on we will study the way it is distributed and the way it is administered. What are distributed are the name servers (servers with gives you the mapping from the domain name system to the IP address). 
- There is an application-layer protocol host, routers, name servers, which sort of all combine and communicate to resolve names (provide this address/name translation). 
o Note: Core internet function is implemented as application-layer protocol
o Complexity at networks edge so that at the networks core where the traffic is very high and very heavy this is not present. If you had to do the domain name servers at the core routers then it will be of much strain on the routers so they have been put to the edge of the network (Refer slide time:12:46-14:29)


Slide time: 12:46-14:29
Full Resolver: For resolving we use resolvers, and there are full resolvers and stub resolvers.
- The client for this naming system is called resolver. This is transparent to the user and is called by an application to resolve names into real IP-addresses or vice versa. When you type a domain name in your browser, the browser is a client application program for http and so its calls the resolver to translate these domain names to corresponding IP addresses. The browser will send the http request to the web server using that IP address. So it gets it from the resolver.
- A full resolver is a program distinct from the user program, which forwards all queries to a name server for processing. It knows about the name server. 
- Responses are cached by the name server for future use, and often by the name server. The local full resolver in your host may connect to the local name server. If the local names server does not have the resolution, it is going to contact other name servers and finally will get the resolution. We will look into the details later. (Refer slide time:14:30-15:19)


Slide time: 14:30-15:19
In this diagram, the user program gives a user query to the full resolver. The full resolver gives this query to the name server. This name server has its own database, so it may look up in its own database. If it is not there then it may send the request to foreign name servers and finally it will give a response. The full resolver also maintains a cache and it will cache this response so that if it gets another request to resolve the same name then it can find from the cache and give the response to the User. Name server also maintains its own cache. (Refer slide time: 15:20-16:16)


Slide time: 15:20-16:16
- A stub resolver is a routine linked with the user program which forwards the queries to a name server for processing. Responses are cached by the name server but not usually by the resolver. There are two differences between a stub resolver and a full resolver. One is that, full resolver has a cache but stub resolver usually does not, and secondly, stub resolver has to be linked with the user program where as the full resolver runs by itself. 
- On UNIX, the stub resolver is implemented by two library routines: gethostbyname() and gethostbyaddr() for converting host names to IP addresses and vice versa. (Refer slide time:16:17-16:34)


Slide time: 16:17-16:34
Stub resolver is a part of the user program which gets linked and this routine sends the query to the local name server and gets the response and passes it on. Naturally the user program gets it. (Refer slide time: 16:35-20:04)


Slide time: 16:35-20:04
DNS Organization: Is a distributed organization.
- Distributed Database 
o The organization that owns a domain name is responsible for running DNS server that can provide the mapping between hostnames within the domain to IP addresses. 
o So some machine run by say an organization like RPI is responsible for everything below within the rpi.edu domain. 
o There is one primary server for a domain, and typically a number of secondary severs containing replicated databases. 
So, it is important to understand how this is organized. This is a global system, which means this is very big. So any effort to control this centrally would become quite difficult. Let us say csc.iitkgp.ernet.in is a domain. Now .in is a domain, which is for whole of India. There is one specific organization (in India) which looks after this domain. So whatever sub domains are there under this domain, it is the responsibility of that organization to keep track of who all can be given sub domains. That is, who all can be given names within these sub domains.in. Also, any name resolution query coming from anywhere else in the world will direct their query first to this organization who is maintaining the domain .in. 
Also there are so many organizations maintaining their own sub domains under the domain .in. Either that organization will have it in its cache or it will forward the query to the particular domain. Suppose if it gives sub domain ernet, ernet again gives sub domains to iitkgp And again under iitkgp there may be many sub domains but ernet does not really bother about whatever sub domains are there under iitkgp. It only has to know that iitkgp is an organization under ernet which has got a name and a separate domain. It is for iitkgp to decide how it is going to break this domain into further sub domains or it may not break at all. This way the entire domain administration is decentralized and it is easy, and also these names are some kind of addresses that has been made unique. Since the names are broken into names which stand for actual organization at any level, naturally any particular organization administering a domain will not have two organizations having the same sub domain name under it. This way all the names automatically become distinct which is another good advantage. (Refer slide time: 20:05-20:54)


Slide time: 20:05-20:54
Why not centralize DNS? 
- Single point of failure. 
- Traffic volume. 
- Distant centralized database should not work 
- Maintenance should be a problem 
- Does not scale so new server so its distributed server
- No server has all names to IP address mappings 
Local Name Servers: 
- Each ISP Company has Local Name Server (Default Name server) 
- Host DNS query first goes to the local name server 
Authoritative name server: 
- Sometimes we may come across something that has been given by authoritative server 
- For a host stores that hosts IP address, Name 
- Can perform name/address translation for that hosts name
(Refer slide time: 20:55-21:27)


Slide time: 20:55-21:27
There are some root name servers. The root name servers are actually still mostly US centric and have some of the biggest name servers. So, root name servers are all in USA but of course that depends on what root it is and therefore they could also be distributed.   (Refer slide time: 21:28-21:54)


Slide time: 21:28-21:54
nslookup: 
- nslookup is an interactive resolver that allows the user to communicate directly with a DNS server.
- From the OS, you can use this nslookup and give a name query so this is actually name/server look up. This is why it is called nslookup
- nslookup is usually available on UNIX workstations.  (Refer slide time:21:55-22:12)


Slide time: 21:55-22:12
- Servers handle request for their domain directly 
- Servers handle request for other domains by contacting remote DNS server(s) 
- Servers cache external mappings (Refer slide time:21:13-23:43)


Slide time: 22:13-23:43
Server Operation:
- If a server has no clue about where to find the address for a hostname, it asks the root server 
- The root server will tell you what name server to contact 
- A request may get forwarded a few times 
Let us say the iitkgp has a name server. A request for a particular domain name translation has come to it. It does not know to whom to connect the name server.. So it can always transfer it to the next higher level, namely, ernet. If the ernet also does not know where in .in it is there, then it can contact in. in will definitely know all the sub domains under it. It has to know because it is administering that domain. If it is for some address which is from outside you can send it directly to the root of that particular domain. Suppose from India you trying to contact something for Japan, you can send it the jp root name server and then jp root name server would know which name server to contact. DNS queries will go back and forth few times and finally the name will be resolved. (Refer slide time: 23:44-24:03)


Slide time: 23:44-24:03
Server  Server Communication:
- If a server is asked to provide the mapping for a host outside its domain (and the mapping is not in the server cache): 
o The server finds a name server for the target domain 
o The server asks the nameserver to provide the host name to IP translation 
- To find the right nameserver, once again it can use DNS  (Refer slide time: 24:04-25:20)


Slide time: 24:04-25:20
Recursion:
- A request can indicate that recursion is desired - this tells the server to find out the answer (possibly by contacting other servers) 
- If recursion is not requested - the response may be a list of other name servers to contact 
So there are two versions of this DNS server, the recursive server and iterative server. If the recursion is requested and that is honored, suppose I am a name server, I will send the request to a name server whom I think may have a clue about how to do this request recursion. Now, he will make all the other contacts, the next level or may be another level contact as necessary and finally send me the answer. So this is the recursive version. In the iterative version what happens is that, he will simply give me a list of name servers to whom I can possibly contact directly to find out more. Then I make some more requests and finally get the name resolved. (Refer slide time: 25:21-25:48)


Slide time: 25:21-25:48
For example, host1 makes a request to the local names server. Then the local name server may send the request for recursion to another name server. So, it will make the necessary request, get the response and then give the response. (Refer slide time: 25:49-26:13)

Slide time: 25:49-26:13
In the iterative case, it knew in one shot but it may not be in one shot. When recursion is requested this may go deep down and finally find out and then send the request. (Refer slide time: 26:14-26:37)

Slide time: 26:14-26:37
Whereas in the iterated queries, he gives a query and gets some list, he gives it to the next name server and gets some more response. Then finally he will get it and send it for you and then it will be resolved. This is an iterated query. (Refer slide time: 26:38-26:59)

Slide time: 26:38-26:59
DNS caching and updating of records: 
- Once (any) name server learns mapping, it caches mapping 
- Cache entries timeout (disappear) after some time 
- Update/notify mechanisms under design by IETF (how the cache is to be managed)  (Refer slide time:27:00-27:46)

Slide time: 27:00-27:46
We will not going into all the details, although they are given in the slide. These are the so called resource records or RR. The RR (resource records) have a format Type = A where the name is hostname and value is IP address. This is the most common one. Type= NS where name is domain and value is IP address of authoritative name server for this domain. Type = CNAME where name is an alias name for some canonical (the real) name and value is canonical. Type = MX is for mail server records, so these are the various resource records which are handled by the DNS. (Refer slide time: 27:27-28:02)

Slide time: 27:27-28:02
This is the DNS message format. We have a header followed by some queries, followed by some resource records and some authority records if it is from alternative name server and may be some additional information. This is the DNS message format. (Refer slide time: 28:03-28:18)

Slide time: 28:03-28:18
We have in the message header, a 16-bit # for identification, 16-bit # for query, and reply to query uses the same # and then we have a number of flags (Refer slide time:28:19-28:44)

Slide time: 28:19-28:44
- QR flag identifying a query (0) or a response (1) 
- Op code is a 4-bit field specifying the kind of query: 
           0 for standard query (QUERY); 1 for inverse query (QUERY) 
- Server status (STATUS). Other values are reserved for future use (Refer slide time:28:45-29:10)

Slide time: 28:45-29:10
- AA: is authoritative answer that means this it is coming from an Authoritative Name Server
- TC: to see if the response has been truncated. Actually if it is truncated, it switches from UDP to TCP
- RD: recursion desired
- RA: recursion available etc 
- rcode: is return code (Refer slide time:29:11-30:19)


Slide time: 29:11-30:19
Both UDP and TCP are used by name server:  
- TCP for transfers of entire database to secondary servers 
- UDP for lookups. The lookups that we discussed about, usually use the UDP protocol. UDP protocol is used as it is a very simple protocol with no extra over head. But there is a limit to that UDP and in response to UDP you will just get one packet which is just 512 bytes 
- If the response requires more than 512 bytes, then the requester resubmits request using TCP. If the message is truncated the flag is set and when the client sees the flag it opens a TCP connection with the corresponding name server and then resubmits the request so that it can get a longer request (Refer slide time:30:20-30:46)


Slide time: 30:20-30:46
We have already discussed the administration zones. 
- A zone is a sub tree of the DNS tree that is independently managed. 
o Second level domains (ernet.in) are usually an independent zone 
o Most sub domains (iitkgp.ernet.in) are also independent. Independent means that what happens under this sub-domain is their business 
o Eg: Most universities have departmental domains that are then independently administered  (Refer slide time:30:47-32:24)


Slide time: 30:47-32:24
- A zone must provide multiple name servers. This server records the members in the domain 
- You typically need a primary name server and one or more secondary name servers 
- Secondary retrieves information from primary using a zone transfer, using a TCP connection 
The reason why the secondary name server is required is sometimes more than one secondary server is kept and the reason is that it is very vital for everybody. Suppose if you are administering a domain and you want to be independent then you can create extra sub domains under you where you have to maintain your own name server. You cannot maintain just one name sever but you have to maintain multiple name servers so that when the primary name server fails the secondary can immediately take over. So, from time to time the primary will cache the data and from time to time or the database will be shifted to the secondary one so that they remain more or less in sync. Therefore as soon as primary goes down the secondary can start acting as a primary and be the authoritative name server for this particular domain. There will be requests from other people to you for IP addresses in your domain and you are bound to give that. That is the reason you need to have these different name servers and good network connection to handle all these requests. (Refer slide time: 32:25-33:02)





Slide time: 32:25-33:02
There is a reverse query which is sometimes used as a sort of weak protection against spoofing. 
- Set q = ptr i.e. ptr is the reverse query for lookup. So here what is done is that, an IP address is given and the domain name is found out. Just as we have ARP and RARP similarly we have query and reverse query. (Refer slide time:33:03-33:42)


Slide time: 33:03-33:42
Reverse queries are used as a weak mechanism to avoid spoofing. This is weak because those machines may have a name but if it connects to the network through a modem and is given a dynamic IP address then sometimes it may not get reflected. So sendmail uses an identity message to identify the sender and receiver but this can easily be spoofed. ( Refer slide time:33:43-35:18)


Slide time: 33:43-35:18
- Some mail servers do not forward mail if you are not in the domain. This is for anti-spam. Spams are mails which are spuriously generated mostly by programs/machines which are automatically generated and sent to millions of people. It is estimated that the majority of the mails which go through are actually spam. If the mail is not for my domain then I am not going to relay this to other mail servers. Sometimes you can do that but sometimes you have to relay because it is your responsibility to relay. And does not accept mail if it can reverse query your IP address. As soon as a mail comes it will find out from which IP address this mail has come from and send a reverse query. By reverse query the name looks responsible may be it is coming from some known University etc then accept the mail. If you cannot reverse query it then maybe it is coming from some spurious source and it does not accept. 
- This is not totally secure because hosts on same physical network can spoof IP headers of domains which are sort of respectable, but anyway this maybe of some use. ( Refer slide time:35:19-35:35)


Slide time: 35:19-35:35
Who manages the in-addr arpa space? 
- When you get a portion of the IP space you also become responsible for handling the in-addr.arpa queries for that space. 
- This is why queries are in reverse order. ( Refer slide time:35:36-35:57)


Slide time: 35:36-35:57
Dynamic DNS: 
- DNS maps domain names to specific IP addresses. 
o This requires that each domain name is statically assigned, since the zone table is typically stored on disk. 
- This implies that a host using a dynamically assigned IP addresses means you connect to that host. ( Refer slide time:35:58-36:34)


Slide time: 35:58-36:34
This is what dynamic DNS is for. This is not widely implemented but there is an RFC for this and it uses a secure connection for doing a dynamic DNS update. You have to sort of update the name server database and anybody cannot use this name server and so you have to use a secure connection to update the DNS database. ( Refer slide time:36:35-37:58)


Slide time: 36:35-37:58
Next, we take up the topic of directories. This is similar to DNS but more general. Actually the directories sometimes uses DNS,. Many of you have used mails and email clients where you have your own address book from where you can look up the email address of those who have sent you mails. One example of a directory service could be some kind of a global address book where we can find the email address of anybody. This was the original intention for creating the directory. But of course people who produce spam mails have sort of killed this idea. So nobody wants his email address to be in a directory which is universally available so that everybody can send junk mails. Anyway, directory can be used for many other purposes as well. ( Refer slide time:37:59-38:48)


Slide time: 37:59-38:48
- What directories are...? 
o They are object repositories 
o Typically read more than written 
o Have explicit access protocols 
o Support relatively complex queries 
DNS queries are simple but it supports relatively complex queries. For example, you can have something like give me the email addresses (assuming that email addresses are still available) of all people who live in Delhi whose name contains ram etc. You may be looking for somebody so you can have more complex queries than you have in a DNS. ( Refer slide time:38:49-39:03)

Slide time: 38:49-39:03
But directories are not meant to be RDBMSs, they are just for looking up. So, lack notions of tabular views, join operations, stored procedures etc. They are not regular RDBMS but they are just for this particular service. ( Refer slide time:39:04-39:41)


Slide time: 39:04-39:41
X.500 was originally how directory was envisaged by the telecom industry. 
- The goal was to have global white pages 
o Lookup anyone anywhere 
o Developed by telecom industry 
o ISO standard for OSI networks 
- Idea was distributed directory 
o Application uses distributed directory structure 
o Application uses directory user agent to access a directory access point ( Refer slide time:39:42-39:53)


Slide time: 39:42-39:53
The picture is something like this; you have a directory user here who uses a directory user agent which in turn connects an access point to the directory. ( Refer slide time:39:54-40:39)


Slide time: 39:54-40:39
- How is the name used? 
o Access resource given the name 
o Build a name to find a resource
o Information about resource.
o These are the different uses of names 
- Do only programs look at these names? Sometimes humans also need to use the names for constructing the names recalling names. 
- Is resource static? 
o Sometimes resource may move 
o Change in location may change the name of a particular resource. 
- Performance requirements 
o Human scale ( Refer slide time:40:40-41:12)


Slide time: 40:40-41:12
Directory information base which is defined in X.501 is given as a tree structure.  
o Root is the entire directory 
o Levels are groups. For ex: country, organization, individual. 
- Entry structure 
o Unique name build from tree 
o Attributes: Type/Value pairs 
o Schema enforces type rules 
- There may be alias entries also. ( Refer slide time:41:13-41:53)


Slide time: 41:13-41:53
Directory structure may look something like this. You have these different levels, which starting from the top may represent some organizations and then some sub organizations and finally you have the objects. Now, in an object entry you will have some names and each of these names should have a series of type value pairs. There may be more than one particular name; it may have a number of attribute and each attribute will have a type/value pair. This is the general structure of a X.501 tree structure. ( Refer slide time:41:54-42:28)


Slide time: 41:54-42:28
- Query is to this system defined in X.511 
o Query is a read, get selected attributes of an entry 
o Compare does an entry match a set of attributes? 
o List children of an entry 
o Search, Abandon request etc 
o These are all kinds of queries are possible 
- Modification you can modify these records  add, remove, modify entry 
o Modify distinguished names etc ( Refer slide time:42:29-43:14)


Slide time: 42:29-43:14
- There is a directory system agent. 
o It may have some local data 
o Can forward request to other system agents 
o Can process requests from user agents and other system agents 
So, these are like the name server system with dissolvers and also this is the system using directory system agents which can do the query processing. It may get the data locally or it may forward the request to other systems. 
- Referrals: 
o If DSA cannot handle the request it can make request to other DSA just as you can make iterative and recursive queries in DNS. 
o Or tell DUA to ask other DSA, this is the iterative process. ( Refer slide time:43:15-44:14)


Slide time: 43:15-44:14
- Directory information can be protected. Actually they are usually protected. 
- There are two issues: 
o Authentication defined in X.509 
o Access control defined in X.501 
- This directory by itself does not give you security. So you have to have other components in order to ensure security. But a directory can be used for some authentication services, some security purposes, etc directory can very well be used. They are actually used that way. 
- Standards specify basic access control and individual DSAs can define their own access control. They can specify to whom they are going to allow access to their local databases. ( Refer slide time:44:15-44:59)


Slide time: 44:15-44:59
Replication: This is defined in X.525. 
- Single entries can be replicated to multiple DSAs. Just like you have a primary name server and a number of secondary name servers, similarly you can have a directory in a primary or master and then you can have replication. 
- Two replication schemes: 
o Cache copies - On demand 
o Shadow copies - Agreed in advance from time to time 
- There is a transfer, copy required to enforce access control. 
o When entry sent, policy must be sent as well 
- Modification is done at the master only 
- Copy can be out of date 
o How to handle that is defined in X.525 ( Refer slide time:45:00-45:30)

Slide time: 45:00-45:30
There are a number of protocols which are defined in X.519. 
- Directory access protocol means the structure of the query and other things would be defined. 
- Directory system protocol 
o Request/response between DSAs 
- Directory information shadowing protocol
o DSA-DSA with shadowing agreement 
- Directory operational binding management protocol. There are a number of protocols.. ( Refer slide time:45:31-46:07)


Slide time: 45:31-46:07
- Uses are of course for Look-up 
o Attributes, not just distinguished name 
o Context 
- Humans can construct likely names 
- Browsing 
- Yellow pages 
o Aliases also may be given 
- Search restriction/relaxation may be there 
- Groups may be defined that means having a number of members who will be having they are own attributes 
- Authentication information that may be contained in the directory and so on 
o Directory may be used for various purposes ( Refer slide time:46:08-47:20)


Slide time: 46:08-47:20
We will look at LDAP, which is: 
- The lightweight directory access protocol designed by the same people who designed OSI. X.500. It actually tends to be a little complex, it is heavy and intuits the OSI all the seven layers. Now for the internet purpose which actually uses the TCP/IP stack rather than the seven layer OSI stack there was a lightweight directory access protocol which can interoperate at least on one side, that LDAP can use that X.500 directory service. but this is much simpler than X.500 and LDAP is used in many places. 
- This is a lightweight directory access protocol. 
o Supports X.500 interface 
o Doesnt require the OSI protocol. This uses the TCP/IP protocol 
o So this is X.500 for the internet crowd 
- Useful as generic addressing interface 
- Like Netscape address book, etc ( Refer slide time:47:21-47:35)


Slide time: 47:21-47:35
- The LDAP or lightweight directory access protocol is a networking protocol for querying and modifying directory services running over TCP/IP. 
- An LDAP directory usually follows the X.500 model (Refer slide time:47:36-48:11)


Slide time: 47:36-48:11
- It is a tree of entries, each of which consists of a set named attributes with values 
- An LDAP directory often reflects various political, geographic, and/or organizational boundaries depending on the model chosen. When you do that you can also define your security policies based on this directory and based on this boundary, especially authentication service. (Refer slide time:48:12-48:21) 


Slide time: 48:12-48:21
- A directory is a tree of directory entries 
- An entry consists of a set of attributes
- An attribute values pair 
- The attributes are defined in the schema. (Refer slide time:48:22-49:58)


Slide time: 48:22-49:58
This would be the protocol stack for LDAP. You have a directory based application, some authorization service or access to some information which may be there for the organization which uses LDAP. LDAP may use TLS (Transport Level Security). Actually you could use SSL also. In future we will give one lecture to security because it has become so important. Now, in an organizational context, a directory may be an important component of the entire security arrangement. Security is the complex issue. But for LDAP we require that we communicate securely in many cases. And many LDAP implementations support this TLS Transport Level Security. We can also use SSL or SASL and these uses TCP. TCP sits on the IP which sits on the other layer etc. So it comes in between the directory based application and the TCP layer. This is where it stands in the protocol stack. (Refer slide time: 49:59-50:50) 


Slide time: 49:59-50:50
This mentions some of the operations which are in LDAP,: 
- Bind - authenticate and specify LDAP protocol version. This actually starts the process. 
- Start TLS - protect the connection with Transport Layer Security to have a more secure connection. Since you are giving some access to information, you need to put some security feature in that. So that is the start TLS. 
- Search - Search for and/or retrieve directory entries. If you go through the access controls then you can search for some records. (Refer slide time: 50:51-51:08)


Slide time: 50:51-51:08
- Compare - test if a named entry contains a given attribute value 
- You can add a new entry 
- You can delete an entry 
- You can modify an entry. These are various LDAP operations. (Refer slide time:51:09-51:37)


Slide time: 51:09-51:37
- Modify DN - move or rename an entry 
- Abandon - abort a previous request 
- Extended operation - generic operation used to define other operations 
- Unbind - Close the connection, not the inverse of bind.  But anyway this is to close the connection, so these are roughly some operations in lightweight directory access protocol. (Refer slide time:51:38-53:18)

Slide time: 51:38-53:18
We will just touch on this security issue: 
- Notion of security for a network protocol is comprised of at least these axes: 
- Identity and Authentication: 
o Who are you and who says so? So identify yourself and then it should have some protection against spoofing that somebody is claiming to identity which is false. 
- Confidentiality: Whatever information is being passed around, other people should not be able to snoop into it so confidentiality is important. You might use some kind encryption for this confidentiality purpose. 
- Integrity - Did anyone muck with this data? This means, did any one change the data? If there is a change which has done by some person who is authorized to do that you might want to keep a log or audit trail for such changes otherwise you want to be sure that on the way somebody has not changed this data. It may be necessary for some applications to maintain some signature kind of a thing to see that the data has not been changed. 
- Authorization: Yes, you can do that, but no, you cannot do that other thing. This means there some organizations in access control user. These identity, confidentiality, nativity and authorization kind of accesses are definitely there. ( Refer slide time:53:19-53:44)


Slide time: 53:19-53:44
- One needs to separately consider each of the four security axes in the context of anticipated threats. 
- Also need to consider security from the perspectives of 
o the information stored in the directory, and 
o Attributes of the requesters 
- Data security is not equal to access security. ( Refer slide time:53:45-55:06)


Slide time: 53:45-55:06
- Some typical security features of LDAP implementations: 
- Simple password based Authentication. 
- SSL on a particular port 636. 
- SSL is secure socket layer and TLS is transport layer security. This is on port 389 
- There is some Access control
- There is some Configurability. There are other things that you can do with the directory especially in the context of an organization. Therefore because of spams and other issues the idea of actually generating a very global white pages for everything under sun, such a grand idea did not really work out but in the context of specific organization with its own security boundary and its own needs, LDAP is a well defined is a protocol which can be used. 

Good Day. The topic for today is Congestion Control. (Refer slide time 55:07-56:18)


Slide time: 55:07-56:18

The performance of computer networks on a large extent depends on the kind of congestion present in the network. So, actually this is a large topic and we will just touch upon some aspects of them. One thing we know by now is that, in general network namely data network or internet in particular, although multimedia and other content are coming in, first of all this is a packet based network and a large data network where we make only best effort of delivering a packet. What exactly do you mean by best effort? Most of these best efforts have to do with how we handle the congestion? (Refer slide time 56:19-56:52)


Slide time: 56:19-56:52
What is congestion? When too many packets are pumped into the system congestions occurs leading into degradation of performance. Here you can give selective acknowledgement that you have got something in particular but have not got that. Issue: Reno and new reno retransmit at most one lost packet per round-trip time.
Selective acknowledgement: The receiver can acknowledge non continuous blocks of data which means SACK Selective Acknowledgement of 0 to 1023, 1024 to 2047 and so on. (Refer slide time 56:53-57:17)


Slide time: 56:53-57:17
- Multiple blocks can be sent in a single segment 
- TCP SACK: 
o Enters fast recovery upon three duplicate ACKs 
o Sender keeps track of SACKs and infers if segments are lost Sender retransmits the next segment from the list of segments that are deemed to be lost, like fast retransmit (Refer slide time 57:18-58:22)


Slide time: 57:18-58:22
Many people have tried various kinds of heuristics to improve the performance of TCP. There are two competing demands. One is that we have to maximize the throughput and if you can maximize the throughput, naturally the overall delay, congestion will be small and at the same time you will get your job done faster. But in order to push this maximum throughput we should not get into congestion or absolutely no congestion collapse so we try to guard against that. These are the various versions or various flavors of TCP for doing that. So, we are going to look at some topics associated with this congestion control and one is traffic engineering. This means, can you shape or handle your traffic in a particular way so that congestion is less likely to occur. (Refer slide time 58:23-58:53)


Slide time: 58:23-58:53
All these build to give quality of service in a network. At routers, these may depend on Packet Classification and Packet Scheduling. At network entrance it may have to do with Traffic Conditioning. At routers or somewhere in the network you may do admission control. Between hosts and routers you may do signaling. Therefore these are the different components of QoS network. 



COMPUTER NETWORKS
Prof.Sujoy Ghosh
Department of Computer Science and Engineering
IIT, Kharagpur
Lecture-35
(Refer start time 00:42-57:33)

Good day. The topic for today is congestion control. 
(refer slide time 00:48-02:00)

SLIDE 00:48-02:00
The performance of a computer network depends on a large extent on the kind of congestion that is there in the network. Once again this is a large topic. We will just touch upon some aspects of them. One thing we know by now is, in general network, may be data network or inter network in particular although multimedia and other contents are coming in, this is a packet based network and a large data network that is announced where we make only the best effort of delivering a packet. Now, what exactly do you mean by best effort? Most of these best efforts depend on how you handle congestion. 
(refer slide time 02:01-02:38)

SLIDE 02:39-04:18
So, we know what congestion is. When too many packets are pumped into the system, congestion occur leading into the degradation of the performance. Congestion tends to feed upon itself and backs up. Congestion shows lack of balance between various network components. Moreover it is a global issue because the congestion may happen in some intermediate router because of packets being pumped from various sources, so in that sense it is a global issue. (refer slide time 02:39-04:18)

SLIDE 04:19-04:45
We have this intermediate node or channel etc and the demand is in the form of various sources pumping in packets at various rates namely ?1 to ?n and this is being serviced by the channel capacity or the router capacity at the rate ? and going to the various destinations. The problem is, the demand outstrips available capacity. So this is basically the congestion problem and added dimension to this problem comes from the fact that although these ?, ? etc are coming from the general queuing theory and for simple queuing theory these are the arrival rate and service rates etc. Although queues are there, but in general the statistics which the data networks follow are rather complicated. So traditionally telecom networks would follow some Poisson distribution with exponential interval time. But in data network it is seen that it follows something called as a self similar traffic or heavy tail distribution and that is a complex distribution. One of its key features is bustiness. That means data tends to come in busts and then there is comparatively long quotient period and again another burst comes. So, that is the problem which has to be handled and when several bust arrives at a node at the same time that particular node may get overloaded.  (refer slide time 04:19-04:45)

SLIDE 04:46-05:43
So, if information about ?1, ?? etc is known in a central location where control of ?n and ? effected instantaneously with zero time delays then the congestion problem is solved. Unfortunately we cannot do that because we have incomplete information and we require a distributed solution with time varying time delays. This is what makes the problem a little difficult. (refer slide time 04:46-05:43)

SLIDE 05:44-06:12
Already we have seen this kind of throughput versus load curve we have seen already when we saw alocha networks, ccna cd etc. But, in general this is what happens. As the load increases the throughput also keeps on increasing at the same rate and then it starts sort of going down and it keeps going down because of the intermediate delays and other bottlenecks coming into the picture. And then there is an area where the throughput does not increase any longer. If you increase the load beyond that there is a catastrophic fall in the throughput. So this part is known as the knee and this part is known as the cliff and this catastrophic fall is called congestion collapse. (refer slide time 05:44-06:12) 


SLIDE 06:13-06:42
So, knee is the point after which throughput increases very slowly and delay increases fast. Cliff: Point after which throughput starts to decrease very fast to zero and this is congestion collapse and delay approaches infinity.
 Note in an M/M/1 queue delay =1/(1- utilization).
It does not follow this kind of a simple formulation. 
(refer slide time 06:13-06:42)

SLIDE 06:43-07:03
If this was the previous curve and as you plot the delay, the delay is low in the beginning and starts increasing. And in this area where there are lots of packet losses and there is a congestion collapse and there are throughput collapses the delay also becomes very high and it becomes a hyperbolic curve. So obviously you have to take all precautions not to fall into this area. 
(refer slide time 06:43-07:03)

SLIDE time 07:04-07:45
So, we talk about congestion control whose goal is to stay on the left of cliff that means not to go into the congestion collapse. Congestion avoidance: Goal is to stay left of the knee and right of cliff is of course the congestion collapse region.
(refer slide time 07:04-07:45)



SLIDE time 07:46-09:15

So, the goal of congestion control is to guarantee stable operation of packet networks and a sub goal is to avoid congestion collapse. To keep networks working in an efficient manner, for example: high throughput, low loss, low delay high utilization are the goals not always achievable especially because we have distributed systems with insufficient information about the global picture but anyway that is there.
 (refer slide time 07:46-09:15).


SLIDE time 09:16-10:47

To provide fair allocation of network bandwidth among competing flows in steady state. So, there has to be some kind of fairness, Sometimes all are not taken as equal. First of all you must see, if there is congestion at an intermediate node, what would happen is that there would be lot of packet loss over there. So, various packets from various sources would be lost and the delay would become high. Many of them were running a TCP protocol. Therefore the question is who would again start retransmitting. Hence what would happen is that, more packets would be lost and more and more packets will keep on getting pumped. This is just like a traffic jam, it starts at one place and then if the jam does not resolve soon then it becomes bigger and bigger and it starts getting pushed towards the source. So the overall network throughput goes down and people tend to push more packets. These are the kinds of scenario we would like to avoid.  (refer slide time 09:16-10:47)

slide time 10:48-13:18
Now, there are various policies at various levels that we can take for congestion control. Let us look at the data link layer, the open loop policies. One is retransmission policy. How would you retransmit? One example of this re transmission policy is, suppose you have a Ethernet network with csmacd going on and then you have detected a collision, the question is, are you becoming persistent or non persistent or you do a random back off or exponential back off  or what is your retransmission policy. Hence you will try again.
Similarly there are other things like out of order policy. Out of order policy is when you receive a packet when it is out of order. Acknowledgement policy is, do you acknowledge or do not acknowledge it. For example, if you have an acknowledgement then the acknowledgement for each packet also takes up resources. So, if you acknowledge every packet then there is going to be as many packets sent as many acknowledgements. Therefore this is a lot of acknowledgement for the network and there is a high overhead. May be you take a policy of not acknowledging all the packets. It could be some kind of a flow control policy. Therefore we have seen some kind of flow control in TCP. We will look into more details and variations of it. (refer slide time 10:48-13:18)

SLIDE time 13:19-14:25
In the network layer you can have the virtual circuit versus datagram. This is an important issue and we will look at it in more detail when we discuss QOS and multimedia communication. Suppose there is a very important communication going on between two hosts, mission critical or whatever it may be, now they will be exchanging a lot of packets. Let us say the packets are flowing in only one direction so lot of packets will be sent and we want some premium service for this. If you want to give a premium service to this particular pair of nodes, may be they pay more or something then in that case you have to distinguish among the packets and assume that they form some kind of a flow. So you need to have a kind of virtual circuit between these two points in order to distinguish them. If all packets are on their own then that is a different kind of a situation where it will be more difficult to distinguish the flow between the two specific nodes. Virtual circuit versus datagram may be an important issue. Once again we will see more details of this when we look at RSVP, diffsservent in the QOS when we discuss QoS a little bit more in detail. 
Packet Queuing and Service policy means, in the router a number of packets may come and they are going to be serviced one by one and they are going to be put in a queue. Now the question is, do you put them in one queue or do you put them in several queues? Do you have the same priorities for all the queues or do different priorities for different queues and so on. Packet discard policy has to do with the buffer management of the router. If the buffer becomes full, which packet do you drop?
Routing algorithm: What kind of routing algorithm will you use?
 Packet lifetime management: This means the lifetime of the packet is over and you drop the packets. These are important in the network layer. So far we were discussing about open loop polices. (refer slide time 13:19-14:25)

SLIDE time 14:26-15:26
Now let us take a look at closed loop control. This means monitoring the system to detect when and where congestion is occurring. Pass the information to places where actions can be taken. Adjust system operation to correct the problem. This is more sophisticated and better. The point is, if one router in between is congested and now if it can identify the chief sources of trouble where it cannot handle a lot of packets, therefore if you could send a feedback back to the source so that he can control this behavior by sending less number of packets then the situation can be handled. Or you can adjust some system parameters, may be the window size in TCP etc. These are the examples of the kind of thing you can do with close loop control.
(refer slide time 14:26-15:26)

SLIDE time 15:27-16:07
If you say that there is some congestion then we need to have some metrics for measuring congestion. Some examples are percentage of all packets discarded due to lack of buffer space. This may be one measure.
 Average queue length in the buffer
 No of packets that time out and are retransmitted
 Average and standard deviation of packet delay
 These may be metrics with which you measure congestion. So, if these metrics go beyond a certain level then you might decide that some congestion is taking place and you need to take some action in order to prevent the performance degradation in a sharp manner. (refer slide time 15:27-16:07)

SLIDE time 16:08-16:44
Feedback mechanisms: It can be many. As we have mentioned, router on sensing congestion sends a control packet to the source. A bit in every packet can be reserved to announce congestion. Explicit probe packets can be sent to ask about congestion. Implicit algorithms make only local observations. 
(refer slide time 16:08-16:44)

SLIDE time 16:45-17:56
Then you can try adjusting system operations. Adjust time constants to a near optimal value. Decrease the load selectively if possible. May be if one source somehow can decrease then all the others can be served very well because it goes below a threshold. Increase resources if possible, this is usually difficult. .(refer slide time 16:45-17:56)

SLIDE time 17:57-18:29
Now, let us look at the one aspect of congestion control which is very important and which is done by TCP all the time. TCP congestion control, if you remember uses a sliding window protocol. We have a window and a sender can send right up to the window size to the other side and it will wait for acknowledgements and he will keep on acknowledging and once he gets the acknowledgement the window will slide. This is about the basic TCP. What we are going to see now is some variance of TCP. Since TCP is a very important protocol and application protocols like FTP, STP, etc use TCP, a lot of important traffic on the net is actually carried on TCP and that is why whatever we do at the TCP level is very much important. And one of the chief tool for doing any congestion control by TCP is by adjusting the window size. So, there are various variants. 
(refer slide time 17:57-18:29) 

SLIDE time 18:30-19:16
TCP has a mechanism for congestion control. The mechanism is implemented at the sender. The sender has two parameters, congestion window with a variable called cwnd and slow start threshold value with a variable called ssthresh. So, initial value is the advertised window size. So, with a TCP connection there is an advertisement of window size and this window size is taken as the initial ssthresh value. (refer slide time 18:30-19:16)

SLIDE time 19:17-20:06
Congestion control works in two modes. One is slow start and the phase is slow at start when the cwnd value is less than ssthresh and congestion avoidance means that cwnd value is greater than equal to ssthresh. So, basically we are trying to figure out whether we are on the left of knee or in the right of the knee. So, if you are on the right of the knee but left of the cliff we are going to be careful. If you are on the left of the knee and if things are going fine then we can try to increase the load stress to increase the overall throughput. This is the basic idea. (refer slide time 19:17-20:06)

SLIDE slide time 20:08-21:17
Knowing initial values in a slow start i.e. set cwnd = 1.
Naturally if the window size is small i.e. one so one unit will go and the acknowledgement will come back and then only something else will go from this side. That is why we are being very conservative and we are sending only a small bit of information. 
 Note: the unit is a segment size i.e. one of a second. TCP is actually based on bytes and increments by 1 MSS (Maximum Segment Size).
 The receiver sends an acknowledgement (ACK) for each packet. So this is the slow start. So, the receiver must acknowledge every packet, so the first packet it receives it can send an acknowledgement. 
Note: Generally a TCP receiver sends an acknowledgement (ACK) for every other segment.
 (refer slide time 20:08-21:17)

SLIDE time 21:18-21:35
 Each time an ACK is received by the sender, the congestion window is increased by 1 segment. So what happens is that, the sender has sent one packet so it has got the acknowledgement, so actually the sender decides that things are fine and may do better. That means it is the increase in congestion window size (cwnd). So we increase cwnd by 1 i.e. cwnd = cwnd + 1.
 We make cwnd = 2. If an ACK acknowledges two segments cwnd is still increased by only 1 segment. That means for every ACK it increases by 1. If it acknopwledges onlys one segment or two segments then cwnd is increased by one only. Actually the reason to acknowledge every other segment is to decrease the number of acknowledgements. Now, even if ACK acknowledges a segment that is smaller than MSS bytes long cwnd is still increased by one. So, at anytime you get an ACK you increase cwnd by one when you are in the slow start phase. Although it starts slowly does it increment slowly? Not really. In fact, the increase of cwnd is exponential.
(refer slide time 21:18-21:35)

SLIDE time 21:36-22:24
the congestion window size grows very rapidly, cwnd rises very rapidly. For every ACK we increase cwnd by 1 irrespective of the number of segments ACK?ed.
 The TCP slows down the increase of cwnd when cwnd > ssthresh .
(refer slide time 21:36-22:24)

SLIDE time 22:25-23:00
As you can see, suppose if it sends one segment it receives one acknowledgement, the cwnd is increased from one to two. Now you can send two segments, segment two and segment three. It will get back the acknowledgement for segment two and acknowledgement for segment three. Now cwnd has become 4. It will send 4, 5, 6, etc, so three of them it has sent and the acknowledgement for 4, 5, 6 will come. Now cwnd has become 7 and it will send more. You can see here, 1, 2, 4, 7 is increasing quiet fast because for each acknowledgement it is increased by one and when you are sending so many segments at a group you will get many acknowledgements. Therefore cwnd is increasing exponentially. (refer slide time 22:25-23:00)

SLIDE time 23:01-23:21
Congestion avoidance phase is started if cwnd has reached the slow start threshold value (ssthresh). 
If cwnd>=ssthresh. Then each time an ACK is received, increment cwnd as follows: i.e. cwnd=cwnd+1/[cwnd] where [cwnd] is the minimum or the larger integer and is smaller than cwnd. So this is only increased by a fraction while sending. Of course you will not send a fraction and whatever be the current cwnd value that is floured that many segments you can send. (refer slide time 23:01-23:21)

SLIDE time 23:22-23:42
So, cwnd is increased by one only if all cwnd segments have been acknowledged. That means, if all the cwnd have been sent or acknowledged, then cwnd increases only by one. So we are very cautious while we move to the right of the knee. 
(refer slide time 23:22-23:42)

SLIDE time 23:43-24:26
So, assume that ssthresh is 8 therefore what will happen is, round-trip time = 2, 4, 6, etc. As time is going so cwnd is first increased exponentially. It reaches the ssthresh value and then it increases slowly. (refer slide time 23:43-24:26)

SLIDE time 24:27-25:02
TCP assumes there is congestion if it detects a packet loss. Now, what is the response to congestion? TCP assumes that if congestion detects a packet loss a TCP sender can detect a lost packet via timeout of a retransmission timer or receipt of a duplicate ACK. Duplicate ACK has been received which means that previously may be some acknowledgement has been dropped and there is a duplicate ACK. So, when something is dropped it means that there may be congestion. Now there are different ways to respond to this congestion. (refer slide time 24:27-25:02)

SLIDE slide time 25:03-25:20
One is, TCP interprets a time-out as a binary congestion signal which means there is congestion as soon as there is a timeout. Therefore when the sender performs cwnd is now reset to one
i.e. cwnd = 1 so once again it becomes very conservative.
 ssthresh is set to half the current size of the congestion window Ssthresh =cwnd/2.
Before sending it to one whatever be the cwnd value you divide it by two and make it the new threshold value and enter the slow start again. 
(refer slide time 25:03-25:20)

SLIDE time 25:21-25:36
so initially: cwnd is equal to one 
i.e. cwnd =1 and ssthresh = advertised window size.
 New acknowledgement (ACK) is received: If (cwnd < ssthresh)
/* Slow Start*/ cwnd = cwnd + 1; else 
(refer slide time 25:21-25:36)

SLIDE time 25:37-25:58
Congestion avoidance cwnd = 1/cwnd + 1/cwnd
Cwnd = cwnd + 1/cwnd.
 If there is timeout it is multiplicative decrease 
 i.e. ssthresh = cwnd /2 and cwnd = 1.
 (refer slide time 25:37-25:58)

SLIDE time 25:59-26:32
this is the typical plot of cwnd for a TCP connection(MSS=1500 bytes) with TCP Tahoe: TCP Tahoe is one flavor of TCP we have been discussing. We will discuss about some other flavor also. So, if cwnd goes on increasing, decreasing and then after sometime again increasing while things are good then this may be a typical plot. (refer slide time 25:59-26:32)

SLIDE time 26:33-27:36
TCP Tahoe uses one flavor, 
slowstart for every acknowledgement
Congestion avoidance that means only beyond the ssthresh it increases slowly
Fast retransmit.
In TCP Reno there is also another version of TCP   
Uses fast recovery
 And then there are some versions like New Reno, SACK,RED, etc. (refer slide time 26:33-27:36)

SLIDE time 27:38-28:42
Acknowledgements in TCP: Receiver sends acknowledgement (ACK) to sender
Acknowledgement is used for flow control, error control and congestion control. In error control if the acknowledgement is not received then you send a retransmit. In congestion control we find that ACK is used for controlling this. ACK number sent is the next sequence number expected. 
Delayed ACK: TCP receiver normally delays transmissions of an ACK for about 200 ms because it allows the packets to arrive thinking that it can send less number of acknowledgements this way, and ACKs are not delayed when packets are received out of sequence i.e. a little out of ordinary, may be they came from two different paths, so you do not delay the ACK but send it immediately. (refer slide time 27:38-28:42)

SLIDE time 28:43-29:50
Now fast retransmit, if you remember that the TCP RENO uses fast retransmit. If three or more duplicate ACKs are received in a row, the TCP sender believes that a segment has been lost. This means ACKs have come meaning some earlier packets are gone. This means, possibly the later packets or segments may be lost. So what it does is, without waiting for the timeout to occur for this particular segment which has been sent it assumes that it has been lost and it sends one more again. TCP performs a retransmission of what seems to be the missing segment without waiting for a timeout to happen and then it enters slow start. That means it brings down the multiplicative decrease of ssthresh and sets the cwnd to one.
i.e. ssthresh = cwnd/2 cwnd = 1. This is fast retransmit.
 (refer slide time 28:43-29:50)

SLIDE time 29:51-30:18
In Fast Recovery the slow start is avoided after a fast retransmit. That means after a fast retransmit intuition Duplicate ACKs indicate that data is still getting through or at least the duplicate ACKs are through. After three duplicate ACKs set retransmit lost packet, i.e. decrease ssthresh to half so ssthresh = cwnd/2 but
cwnd= cwnd + 3 and then you enter congestion avoidance. So increment cwnd by one for each additional duplicate ack. This is a fast recovery but then after this you enter the congestion avoidance. That means, basically this is trying to tune the performance of TCP to get the maximum throughput without causing any congestion. When ACK arrives that acknowledges new data cwnd = ssthresh
  After that we enter the congestion avoidance. So this is fast recovery. 
(refer slide time 29:51-30:18)

SLIDE time 30:19-30:42 
TCP RENO: For Duplicate ACKs it does fast retransmit and fast recovery. Fast recovery avoids slow start. And if there is a time-out you retransmit and go to slow start. TCP RENO improves upon TCP Tahoe when a single packet is dropped in a round-trip time. But if multiple packets are dropped then of course the TCP RENO cannot handle that and for that we have a TCP NEW RENO.
(refer slide time 30:19-30:42)

SLIDE time 30:43-30:58
 When multiple packets are dropped RENO has problems.
 Partial ACK: Occurs when multiple packets are lost. A partial ACK acknowledges some but not all packets that are outstanding at the start of a fast recovery, takes sender out of fast recovery. The sender has to wait until time-out occurs. 
(refer slide time 30:43-30:58)

SLIDE time 30:59-31:42
In new RENO partial ACK does not take sender out of fast recovery. Partial ACK causes retransmission of the segment following the acknowledged segment. New RENO can deal with multiple lost segments without going to slow start. (refer slide time 30:59-31:42)

SLIDE time 31:43-32:07
There is a selective acknowledgement (SACK). Here you can selectively acknowledge. In an original TCP when you give an acknowledgement, that is the next segment you are expecting and all segments before that are acknowledged. Here you can give selective acknowledgement stating that you have got all these but not that particular one.  
Issue: Reno and new Reno retransmit at most one lost packet per round-trip time. Selective acknowledgement: The receiver can acknowledge non continuous blocks of data. That means SACK selective acknowledgement of 0 to 1023, 1024-2047 and so on 
(refer slide time 31:43-32:07);

SLIDE time 32:08-33:12


Multiple blocks can be sent in a single segment.
TCP SACK enters fast recovery upon three duplicate ACKs. Sender keeps track of SACKs and infers if segments are lost. Sender retransmits the next segment from the list of segment that is deemed to be lost like fast retransmit. (refer slide time 32:08-33:12)

SLIDE time 33:13-33:43
To improve the performance of TCP, there are two competing demands here.
 One is that we have to maximize the throughput and if you can maximize the throughput, naturally the overall delay, congestion etc will be small and at the same time you will get your job done faster. But in order to push this maximum throughput we should not get into congestion, a collapse so we try to guard against that. These are the versions or various flavors of TCP for doing that. So, we have looked at TCP. Now we are going to look at some other topic once again associated with congestion control and the other one is traffic engineering. That means, can you shape or can you handle your traffic in a particular way so that congestion is less likely to occur.
(refer slide time 33:13-33:43)

SLIDE time 33:44-34:00
All these build to give a quality of service in a network. And at routers these may depend on Packet Classification and Packet Scheduling. At network entrance it may depend on traffic conditioning. At routers or somewhere in the network you may do admission control. Between hosts and routers you may do signaling. So these are the different components of QoS of a network. 
(refer slide time 33:44-34:00)

SLIDE time 34:01-35:02
So, let us say you have a sender and receiver here, these are the intermediate routers then you can do the traffic conditioning at the edge of the network. You can also do admission control here or somewhere else. So, these are the different components.
(refer slide time 34:01-35:02)

SLIDE time 35:03-36:56
Traffic conditioning mechanisms at the network boundary need to enforce that traffic from a flow does not exceed specification.  
So, we will look later at what kind of specifications we are talking about, what kinds of things people may agree on, or negotiate about that what are the parameters. But suppose from some source we had negotiated certain parameters and we find that the source is not sticking to that parameters and it is going out of that then we have to do some policing. So, policing is a drop traffic that violates the specifications. The specification as was agreed between the service provider and the sender. Shaping means the buffer traffic that violates specifications. Marking means mark packets with a lower priority or as best effort, if the traffic specification is violated. (refer slide time 35:03-36:56)

SLIDE time 36:57-38:10
Let us look at Traffic shaping first. Regulating the average rate of data transmission allows control algorithms to work better. So this is to be understood. As I mentioned earlier, in computer network specifically data networks or internet traffic etc they are inherently very bursty in nature. When it comes it comes in one big bunch and then for long periods there may be no traffic. Now the trouble is, if the burst peak to the average ratio may be as much as 1:1000, we have to accordingly design your buffer and other network provisioning. So, we have to decide upon whether we are doing it for the peak or doing it for the average or may be doing something in between as designing for the peak. If you design it for the peak everything works fine but that becomes very expensive and not practical in many cases. You cannot do it for the average also and that may be to lower so it may be somewhere in between. Therefore one inherent problem is the burstiness of the traffic. Now, if you could somehow make the burstiness smooth, then all your system will work much better. One way of doing is to buffer it somewhere.The shape of the traffic is related to some statistics about data transfer rates as well as its sensitivity to error, delay jitter etc. 
(refer slide time 36:57-38:10)

				       
      SLIDE time 38:11-
One famous algorithm is the Leaky Bucket Algorithm. It is a single server queue with a constant service rate. If you have a bucket which is leaking drop by drop that means water will come out at a constant rate. Therefore the same thing happens here. If you have a single queue and then you service it at a constant rate this is the rate at which you are pumping the data into the network. So, if there is a burst then it will get absorbed in your buffer at the edge so that in the core of the network the burst will not come and it will be more of a steady kind of a flow. A steady average kind of flow is also something beyond the capacity of the intermediate nodes. Then of course the capacity of the intermediate nodes has to be increased. That is the leaky bucket algorithm in short. So the input buffer allows a bursty flow to be smoothed out to an even flow onto the network. It may be implemented in hardware or the OS Operating System. It may be implemented either in hardware or software. (refer slide time 38:11-)

SLIDE time 39:04-39:58
Underutilized slots are written off. By this what we mean is, the packets are being serviced by this network at a particular ray, let us say once every T unit of time. Now after another T unit of time it will try to service and finds that the buffer is empty so it will not send anything. Again after T unit of time and if something has arrived by that time it will send one packet. The algorithm can work on the volume of the traffic rather than number of packets. Only problem here is, a somewhat slow response time for inherently bursty traffic which is quiet often in the node.
(refer slide time 39:04-39:58)

SLIDE time 39:59-41:19
One way to handle a little bit of burstiness is by a token bucket. This again improves the throughput a little bit and it can accommodate burstiness to a certain degree. We cannot allow all kinds of burstiness because then the burstiness will flow into the core of the network where it will be more difficult to handle. So this is the token bucket, it limits the input to specified burst size (b) and average rate (r). 
So traffic sent over any time T<= r*T+b, also know as linear bounded arrival process (LBAP). So there is bound on an arrival process. Excess traffic may be queued, marked or simply dropped. (refer slide time 39:59-41:19)

      SLIDE time 41:20-41:44
So, Tokens are generated for the buffer at a fixed rate which can be accumulated. So this is the main point where is the token differs from the leaky bucket. In the leaky bucket the underutilized slots were written off. But here if your time comes you can get a token and you can collect and accumulate so many tokens. And then when a burst comes up to that many tokens can be sent. .The longer time average is helpless because there is a limit to the number of tokens you can really accumulate because after that you cannot accumulate tokens anymore. And at the same time a little bit of burstiness is allowed if your source is inherently bursty, and if you can allow some amount of burstiness that will improve the throughput. So, for each token only one packet can be sent but tokens can be accumulated up to a certain maximum.. A variant is to allow k bytes per token. Essentially it allows bursts up to a regulated maximum length that is maximum number of tokens. A leaky bucket may follow a token bucket also in order to make it absolutely smooth.
(refer slide time 41:20-41:44)

SLIDE 41:45-42:18
So this is the diagram, the bucket holds up to b tokens and there are so many tokens per second accumulating there. And when a packet burst comes then it waits for the tokens. If the tokens are not there then it cannot send. But if the tokens are there depending upon as many tokens that are available the tokens are removed and the packets are sent into the network. (refer slide time 41:45-42:18)

SLIDE time 42:19-43:08
Now having talked about this, let us just mention what are the kinds of traffic parameters that are important or that may be negotiated between the sender and the network service provider. One could be maximum packet size that defines how big the packet is. The token bucket rate: Defines what the average rate is. Token bucket size defines how burst it will be. Maximum transmission rate tells us the exact maximum transmission rate. 
(refer slide time 42:19-43:08)

SLIDE time 43:09-43:39
Loss sensitivity: Is this flow very sensitive to losses.. If you are just doing some file transfer it will be sensitive to losses, but if you are sending some voice it may not be that sensitive. Loss Interval: At what interval it is a loss, if the loss is very bursty or if the loss has to be averaged out, etc. Burst loss sensitivity (packets): in terms of the number of packets. Minimum delay noticed, and maximum delay variation which are allowed. These are again very important for multimedia traffic. Quality of the guarantee: Is it just a best effort or better than the best effort is what it tells about. These are the flow specifications of services. (refer slide time 43:09-43:39)

SLIDE time 43:40-44:06
We will come to admission control and signaling in more detail when we discuss RSVP in the next lecture when we disscuss QoS and Multimedia traffic. But just to mention it here, Admission Control is a function that decides if the network has enough resources. Admit new flow if enough resources are available. Reject the flow otherwise. 
(refer slide time 43:40-44:06)

SLIDE time 44:07-44:20
You do some reservation of capacity through some protocol like RSVP which we will discuss later. And if you find that you can reserve the capacity for this kind of flow that is the flow with these kinds of parameters then you admit it but otherwise you do not admit it. This assumes that we have some kind of a virtual circuit. (refer slide time 44:07-44:20)

SLIDE time 44:21-44:48
There may be Distributed Admission Control instead of central admission control at the beginning. For example, it may be end to end delay which must be less then than a delay bound D. So calculate d1, d2, etc and you reserve resources. (refer slide time 44:21-44:48)

SLIDE slide time 44:49-45:12
And what would you do is, the D is specified by the source and as it travels some reservation signal it calculates the delay d1, d2, d3 etc and
if D < d1+d2+d3 then you reject the flow and if it is greater then you accept it. Send reject message to sender and release resources.
 Therefore if D>d1+d2+d3 accept flow, commit resource reservation and notify sender. (refer slide time 44:49-45:12)

SLIDE time 45:13-45:39
Some signaling protocol is used to reserve and release resources and to do admission control. So you reserve one mbps that the request goes through. (refer slide time 45:13-45:39)

SLIDE time 45:40-47:01
So, this is a Congestion Control in virtual circuits. One approach is admission control, not allow new VC till congestion goes away or route new ones around problem areas. Other is, negotiate flow specification when new VCs are set up. This requires resource like buffer space, bandwidth etc, and reservation along the way. This may waste resources. (refer slide time 45:40-47:01) 

SLIDE time 47:02-47:59
One topic we mentioned earlier is the TCP IP source quench or sometimes called as choke packets. This may be used as a crude mechanism for handling congestion. Each router monitors each output line and calculates the utilization as a weighted sum of current and past utilization. Above a certain threshold a choke packet with the destination is sent to the source and the original packet is tagged and sent along. On receiving a source choke packet the source is supposed to reduce the traffic to that destination by some percentage. If that happens and if it works then that is very fine. When congestion is detected the source is sort of distributed and they are remote to each other. So, if you could send this feedback instantaneously then you could control the congestion much better but that is not possible. you have a distributed algorithm where you work  only with some local information and something that might come along with some particular packet .(refer  slide time 47:02-47:59)

SLIDE time 48:00-48:16
The source waits for some time before acting again on the next choke packet because there may be multiple choke packets coming. Therefore for the same burst it has created ripples of congestion along the way and all the routers sending choke packets so multiple choke packets does not necessarily mean these are independent but they may have come because of the same source so it waits for sometime before acting on the next choke packet. For high speed lines with a lot of hops, choke packets to the source is too slow. So, choke packets may operate hop by hop thus by distributing the pressure on buffers.  These choke packets add to the network traffic and it operates hop by hop thus distributing the pressure on buffers. 
(refer slide time 48:00-48:16)

SLIDE time 48:17-49:42
Scheduling: This is another way to handle congestion. (refer slide time 48:17-49:42)

SLIDE time 49:43-51:17
Packet scheduling has to be done by deciding when and what packet to send on output link, usually implemented at the output interface. Suppose you have some switch or some router or network node and a number of packets are coming out so what you might want to do is to classify these packets. In this context this could be the worst effort. There are premium services and other kind of services etc and the rest are the best effort for with the rest. What you may have is that you may have various classifications for these flows which may go into different queues and then there is a scheduler which schedules as to which queue to be serviced next. A scheduler has a vital role to play on the kind of services on each of the packets at the micro level and each of the flows in general at a higher level they get.
(refer slide time 49:43-51:17)

SLIDE time 51:18-52:33
Typical Internet Queuing: In Internet queuing what we do is, we use FIFO + drop tail. What is FIFO? FIFO is, First In First Out. It is a simplest choice and is used widely in internet. This First In First Out implies single class of traffic which is essentially means that we have a single queue. So whoever comes in first, he is the one who would be attended first for servicing. This FIFO has to do with scheduling. And there is a drop tail which means the arriving packets get dropped when queue is full regardless of which flow it belongs or regardless of its importance. So, if the buffer is full whoever comes next will be dropped. So FIFO has to do with the scheduling discipline and drop tail that is the drop policy has to do with the buffer management. This means how much of your buffer is kept empty or whether you allow the buffer to get full or whom you drop out when the buffer gets full etc are some of the buffer management policies. Now let us look at scheduling. Actually the scheduling policy and the buffer management policy always go hand with hand and the buffer management policy always come in pair.
(refer slide time 51:18-52:33)

SLIDE time 52:34-53:35
FIFO Issues: In a FIFO discipline, the service seen by a flow is convoluted with the arrivals of packets from all other flows. So there is no isolation between flows and no policing. Send more packets and get more services. We have one single queue so whoever is pumping in more packets into it he is more likely to be serviced. Of course, you will lose some packets also but other people will also lose some packets but in some sense it favors somebody who is pumping data at a higher rate. So he gets more service or may be he requires it or it is just some kind of a row node. You do not differentiate between different flows at all. There is no isolation between the flows. If there is a flow which is very important but sends less number of packets he will get much lesser service compared to the one pumping lots of packets. So that is the Issue with FIFO. (refer slide time 52:34-53:35)

SLIDE time 53:36-54:02
Drop-tail issues:
Routers are forced to have large queues to maintain high utilizations, that is a problem. Larger buffer implies larger steady state queues or delays so the delay is more. Synchronization: End hosts react to same events because packets tend to be lost in bursts. So what happens is, when the buffer gets full, different packets coming from different sources would be dropped. So, all the sources would know that the packets may be timed-out or something so they would act again in unition and this acting in unition is always bad, then you take another step which again is wrong in some way, this makes it become more bursty 
 Lock out: A side effect of burstiness and synchronization is that a few flows can monopolize the queue space. So these are the drop-tail issues
(refer slide time 53:36-54:02)

SLIDE time 54:03-54:11
Priority Queuing: classes have different priorities, and class may depend on explicit marking or other header info, for example IP source or destination, TCP port numbers, etc. Transmit a packet from the highest priority class with a non empty queue. This has preemptive and non preemptive versions. This is the kind of scheduling policies we have. (refer slide time 54:03-54:11)

SLIDE time 54:12-54:27
So, Routers must be able to 
classify arriving packets according to QoS requirements. This is known as packet classification and packets are transmitted in order to meet the QoS requirements which are known as packet scheduling. 
(refer slide time 54:12-54:27)

SLIDE time 54:28-54:41
You have Class A service which is very premium. Class B services and Class C services are also there. You might attach different priorities to different queues and serve them that way.
(refer slide time 54:28-54:41)

SLIDE time 54:42-55:22
So, each router must implement some queuing discipline. Queuing allocates bandwidth and buffer space, so bandwidth tells which packet to serve next (scheduling) and buffer space tells which packet to drop next (buff management). Queuing also affects latency. (refer slide time 54:42-55:22)


SLIDE time 55:23-57:33
 One thing which is very widely used is Weighted Fair Queuing. Router maintains multiple queues for each output line one for each source. The queues are serviced in a round robin fashion. Instead of packets the volume can also be examined and packets sent in order of their finishing. Some sources can be given a greater weight than others. The point is, even if you do not give a greater weight, then the service which is premium, may be much less number of people might be there. So, since the round robin is between the queues, automatically those which have the premium class get a better service since the population is low.. 
(refer slide time 55:23-57:33)

SLIDE time 57:43-57:51
Use a few bits in header to indicate which queue (class) a packet goes into (also branded as CoS. Lower delay and low likelihood of packet drop for high end users.  Priority, round robin, classification, aggregation etc are the different mechanism which we use. With this we come to a sort of the end of short handling of this congestion control issue, it is not a very easy issue because as we know this is a global problem but you have to take some local action so that it works fine. 
In the next lecture we will take up Quality of Service, Quality of service as we have already mentioned today, we have different kinds of quality requirements for different sets of people. As I mentioned that if you are transferring a file, you do not want any bit to be lost because it may be a very vital bit so it may be a binary or a source or something that the whole thing may become junk. If you lose one bit, it is very difficult and if it goes in a jerky fashion or takes longer time then you may not mind. So that is one kind of quality you require. Another kind of quality you might require is, when you are doing some kind of multimedia transmission like audio, video, etc where I may sort of be insensitive to a few packets or a few bits being lost here and there but if the delay is too large or keeps on varying too much then I have a problem with the quality of reception. So that is a different kind of quality. So how to handle different kinds of quality and how multimedia transmission etc can take place in a network etc would be the content of our next lecture.
 Thank you. 
refer start 57:34
Good day, so our topic for today is QoS and Multimedia, that is (refer slide time 57:43-57:51)

SLIDE time 57:52-58:43
Quality of Service and Multimedia: We will just look at these one by one. 
(refer slide time 57:52-58:43)
 
Quality of Service:
What is Quality of service?
 QoS refers to traffic control mechanisms that seek to either differentiate performance based on application or network operator requirements, or provide predictable or guaranteed performance to applications, sessions, or traffic aggregates. It talks about a lot of things. The basic notion is that there are some applications which require one kind of quality of service. The quality of service may mean different things but the most important of them are the network delay and packet loss, so it is delay and various ways of delays. 
COMPUTER NETWORKS
Prof.Sujoy Ghosh
Department of Computer Science and Engineering
IIT, Kharagpur
Lecture - 36
 (Refer slide time:00:47)

Our topic for today is QOSQOS and multimedia, that is quality of service and multimedia. (Refer slide time : 00:57:00:58)

We will just look at these one by one. (Refer slide time : 01:07 - 01:37)

Quality of service: What is quality of service? QOS refers to traffic control mechanisms that seek to either differentiate performance based on application or network operator requirements or provide predictable or guaranteed performance to applications, sessions or traffic aggregates. It talks about lot of things, the basic notion is that there are some applications which require one kind of quality of service. By the way the quality of service may mean so many different things but the most important of them are the network delay and the packet loss. These 2 parameters may sort of come in various ways, they affect differently when you talk about different applications. And by multimedia we mean audio, video, etc going through the net which has got a different kind of QOS characteristics, a different set of requirements than a file transfer. We will see how we can handle QOS and how multimedia traffic is handled in the network. Sometimes it may so happen that some applications are more important than others, so they need a better guarantee of performance. Such things also come under quality of service. (Refer slide time : 02:46 - 04:24)

 Take examples: Video and audio conference requires bounded delay and loss rate. That means the delay has to be bounded and also the variability of the delay should also be within the bounds and the loss rate of packets should again be bounded. As you can see, this is quiet different from a file transfer where the loss of some packets cannot be tolerated at all. It is somewhat tolerant of losses, but more sensitive about delays. When you are downloading a file, they are usually not that sensitive to trace. Video and audio streaming means maybe from video server or some internet radio, etc; audio or video is being streamed to different clients so this again requires a bounded packet loss rate. It may not be so sensitive to delay. There might be some time critical applications like real time control. Here again, bounded delay is important. There are some valuable called premium applications. Maybe those who run these premium applications are ready to pay more for this and they expect better guaranteed service than a low premium or less valuable applications. (Refer slide time : 04:25 - 04:47)

QOS requirements can be specified as delay, delay variation (jitter), throughput which is the rate at which you can send data and error rate. Error rate is something where some packets get lost. (Refer slide time : 04:48 - 08:10)
 
There are two approaches to this, stateless and stateful QOS solutions. Stateless solution: Router maintains no fine grained state about traffic. This has got a positive point that this is very scalable. Actually the kind of thing we have talked about till now is that routers do not maintain any states. It is simply the raw packet forwarding capacity that comes to this. In this way it is very scalable, this is also quite robust but it has weak services because there is no guarantee about the kind of delay or performance a particular application you will have to encounter. On the other hand, we have stateful solutions. Routers maintain per-flow state. This flow is very important while guaranteeing quality of service. Suppose there is some audio conferencing going on between some people, the audio signal nature is digitized and packetized and then these packets are going on. This is a packet switched network, essentially at the heart of it this is a packet switched network. but at the same time we not only talk about a single packet in entity in itself which is how we have dealt with packets so far but the end users are interested in the flow, that means the flow of packet. All these streams of packets that are flowing which contain these audio signals constitute a flow and the users are actually interested in the flow. Suppose somebody wants some guarantee on this service, he wants a guarantee on this service of this flow. Merely looking at packets is not enough; you have to go from a purely stateless situation to a stateful situation. Actually you have to make out that this is a part of particular flow. Naturally if you are stateful, if you go into the packet and look at what kind of packet it is and maintain a state about it in the router then you can give powerful services like guaranteed services plus high resource utilization, fine grain differentiation between different flows, protection where here there is a problem that this is much less scalable that means when the number of flows through a router starts increasing, and that is what will happen in any of the core routers, then the core router will not be able to handle this. So it is much less scalable and much less robust. (Refer slide time : 08:11 - 10:25)

Let us look at the Integrated Services. This is a fully stateful system and is also known as intserv. This is an intserv architecture or isa. It is the architecture for providing QOS guarantees in IP networks for individual application sessions. We are talking about an entire session and the flow that has gone on during that session of flow consisting of so many packets. It relies on resource reservation and routers need to maintain state information of allocated resources (for example g) and respond to new call setup requests. The point is, in this situation how will you guarantee that this particular flow which is starting will get the requested kind of guarantee of service that means its delay would be bounded etc? For this we have to specially reserve resources like buffers, resources like CPU time and so on at the intermediate points. So, you have to reserve these resources for this particular flow so long as this flow continues. That is how you give better service to this particular flow than to any other ordinary packet. It will have to maintain the state in the form of these reservations and respond to new call setup request. For example, as soon as you take out some resources, then naturally it goes out of the available resource pool and if a new call setup request comes for another session some intermediate router may not be able to handle that so it will not be admitted. Network decides whether to admit or deny a new call setup request. (Refer slide time : 10:26 - 10:55)

This is the basic idea of intserv. Therefore in this case since you have to reserve resources there is a call setup phase. This is almost like a virtual circuit which is being setup. There is a call setup request which goes from the source to the destination and back and on the way it reserves resources at each of the intermediate nodes. And if each of the intermediate nodes, that is, each of the intermediate routers are ready to provide these resources then the call will be admitted. (Refer slide time : 10:54 - 12:20) 

For resource reservation there is a particular protocol called reservation protocol or RSVP. RSVP constitutes of call setup and signaling. Then there is a traffic QOS declaration and per element admission control which means whether the call could be admitted or not. Then there will be QOS sensitive scheduling. That means, when a packet arrives first it may go into the buffer of a router and there maybe various buffers. Each of these packets will have to be scheduled for processing and then forwarding. This processing may not be done in a uniform manner but it may be done in a QOS sensitive manner. Similarly routing algorithm may also be QOS sensitive, packet discard strategies etc come under the intserv QOS components. We have talked about scheduling and some of the other things. Now let us discuss about RSVP. (Refer slide time : 12:21 - 13:44)

RSVP is internet signaling. It creates and maintains distributed reservation state. This is decoupled from routing. One thing is to use routing for setting up paths but then this has to reserved resources therefore it is decoupled from routing. It uses multiple trees setup by routing protocols, not RSVP. Multicast tree is setup by routing protocols. RSVP is for reserving protocols there unlike ATM or telephony signaling. While doing the call setup you actually assign the virtual setup. This is initiated by the receiver, this scales for a multicast and it may use soft state. That means reservation times out unless refreshed. so you have to refresh reservation from time to time, otherwise one particular flow may be reserved and never be used so that part will go away. That is why reservation will be timed-out. And latest paths discovered through path messages (forward direction) and used by reservation messages in the reverse direction. (Refer slide time : 13:45 - 14:08)

Signaling Semantics: In signaling semantics you have a setup acknowledgement, setup response, admission control, tentative resource reservation and confirmation. This can be done from both directions, simplex and duplex setup and with no multicast support. These are the signaling semantics in brief. (Refer slide time : 14:09 - 15:02)

 Suppose this is the source, the source would initiate the setup. It will send a setup call to the first switch controller, router to another one and finally to the destination. Each of the intermediate nodes send a setup ACK so that the source knows that this is the way it is going to go and the path is being setup. And then a setup response will come and the acknowledgement will go to the destination and another setup response and acknowledgement will keep coming in. What we will have to see is that, in this fashion, after this, the resource has to be reserved at each of the node for whatever service that is being asked for. (Refer slide time : 15:03 - 16:19) 

Session must first declare its QOS requirements and characterize the traffic it will send through the network. There are two specifications r spec and t spec: The r spec defines the QOS being requested, what kind of bound we want on the delay, what kind of bound we want on the jitter, what kind of packet loss is acceptable to me and depending on this the resource is going to be reserved. That means maybe a special queue or buffer will be setup just for this flow or something will be done with scheduling. There will be some weight given to it for processor time and so on in the intermediate nodes. And then there is a t spec that is, traffic characteristics like what kind of traffic, what is the burstiness of the traffic etc? Such traffic characteristic will also have to be sent. Therefore depending on the t spec and the r spec the intermediate nodes will compute what kind of resource is visibly required. So signaling protocol is needed to carry the r spec and t spec to the routers where reservation is required. RSVP is a leading candidate for such signaling protocol. (Refer slide time : 16:20 - 17:00)

As far as call admission is concerned, routers will admit calls based on their r spec and t spec that whether they can handle or they can manage to give so much resources for this. There is the financial side also, that whoever has initiated whether he has an agreement to pay for such premium service and based on current resources allocated at the routers to other calls. If other senders have already reserved most of the resources in the intermediate routers then a new call may not be admitted. (Refer slide time : 17:01 - 17:30)

This is the router, the request specifies the traffic the t spec and the guarantees of service which is required which is the r spec. Now this reply to the element considers unreserved resources and the required resources for this new request. If it can handle that then it replies whether or not request can be satisfied. (Refer slide time : 17:31 - 17:46)

 This achieves per-flow bandwidth and delay guarantees. For example, suppose we want a guarantee 1 MB/sec and < 100 ms delay to a flow and if we have a network like this, (Refer slide time : 17:47 - 18:06)

first the receiver will just allocate resources, that means perform per-flow admission control. First the path has to be setup, that has not been shown and this is in the response path, so perform per-flow admission control. (Refer slide time : 18:07 - 18:14) 

Install per-flow state, so this flow has to be recognized, for that the router has to have a per-flow state. (Refer slide time : 18:15 - 18:25)
 
 Challenge: maintain per-flow state in a consistent manner right up to the sender. If all the routers agree then only you can give it. (Refer slide time : 18:26 - 18:36)

When sender starts sending first of all it has to be classified that this is the integral part of this flow, so per-flow classification is necessary. (Refer slide time : 18:37 - 18:48)

Depending on how it has been classified, it will be put in the appropriate buffer and the corresponding buffer has to be managed, so per-flow buffer management is also necessary. (Refer slide time : 18:49 - 19:03)

And this way it will go through all the routers, and per flow scheduling is necessary for CPU resources and the flow will go through. (Refer slide time : 19:04 - 19:56)

In the data path, we have per-flow classification, per-flow buffer management, per-flow scheduling whereas in the control path we have the installed and maintained per-flow state for data and control paths. For each of the flow the router has to do a lot of work so this does not scale unfortunately. This is very good at giving services but if you are trying to maintain a per-flow kind of classification and management at each of the nodes, this would not scale because if the number of flows goes beyond a certain level then it will be very difficult to maintain. It will be very difficult to properly give it buffers and queues etc in the intermediate nodes so that is why it does not scale so well. (Refer slide time : 19:57 - 21:00)

Stateless solutions are more scalable and they are more robust. Stateful solutions provide more powerful and flexible services. It gives guaranteed services plus high resource utilization. It can make fine grained differentiation between different flows and it also receives protection on the way since we are short of demarcating each of the flows. Demarcating the flow is somewhat similar, either to setting up ATM virtual circuits, when you do that, you also reserve request but that is done while call setup itself, that is not done by a separate reservation protocol. Hence it is similar to that or it maybe similar to MPLS where a particular flow can be differentiated from other flows and it maybe treated specially. (Refer slide time : 21:01 - 22:56)   

Then on one hand you have, the raw internet just takes a packet by itself and tries to route it and if it cannot route, it is the best effort and it goes off. That means it is discarded, that is one end of the spectrum. The other end of the spectrum is that you have a full power flow classification and management for each of the flows where you can have a very good control etc but it does not scale. So there is a question, can we do something in between and that is well the next model comes which is known as a diff-serv, differentiated service. This is also stateful but here each distinct flow does not necessarily mean a distinct state. These are sort of soft states which can accommodate a number of flows together so that the pressure on the routers in the intermediate nodes or in the backbone of the network becomes less..
Scalability: Maintaining states by routers in high speed networks is difficult due to a very large number of flows. So diffserv provides reduced state services, that is maintain state only for large granular flows rather than end to end flows, tries to achieve best of both worlds. So diffserv intends to address the following difficulties with intserv and RSVP. Flexible service models: intserv has only two classes, wants to provide more qualitative service classes, wants to provide relative service distinction like platinum, gold, silver etc and simpler signaling than RSVP. Many applications and users may only want to specify a more qualitative notion of service rather than a quantitative notion of service. (Refer slide time: 22:57 - 24:00)


This is a diffserv model, you have an ingress edge router and you have the interior routers, it goes to the egress edge router. 
Edge routers: Traffic conditioning that means, policing, marking, dropping etc are done at the edge. The reason we want to do them at the edge of the network is that, there the number of flows etc is much lower and you can handle it over there and when you come to the core the core routers are very busy so we do not want to do that over there. Therefore set values in DS-byte in IP header based on negotiated services and observed traffic.
Interior routers: Traffic classification looking at this byte and forwarding near stateless core. This is almost like MPLS or ATM. Use DS-byte as index into a forwarding table. (Refer slide time : 24:01 - 24:45)

Edge router: It does per-flow traffic management, marks packets as in profile and out profile. 
Core router: It does per class TM, buffering and scheduling based on marking at edge. Preference is given to in-profile packets. In-profile means some traffic shape has been negotiated or is expected and then something comes which is out of the shape. That means somebody is sending more packets per unit time than that was negotiated, so those are out of profile packets. 
Assured forwarding: This is what the core router does. (Refer slide time : 24:46 - 25:34)  

Marking and power flow classification etc is done at the edge but in between the scheduling is done. The packet is marked in the type of service in IPv4 and traffic class in IPv6 renamed as DS. Then 6 bits used for differentiated service code point DSCP and determine the PHB or Per Hop Behavior that the packet will receive, 2 bits are currently unused. And this service code will dictate how this particular packet will be treated. (Refer slide time : 25:35 - 26:47)   

For forwarding Per Hop Behavior or PHB carried out in the interior routers is simplified process based on class and resources specified when SLA was created. PHB has been defined in a certain RFC. For example for premium service, it should have low loss, low delay, low jitter, assigned bandwidth is equivalent to point to point leased line etc is the kind of service we want, although this is not leased, this is the plain packet switch network. It is guaranteed through policing and shaping in order to stay within the departure rate of leaky bucket and WFQ. Since all the intermediate nodes guarantee that it will get premium service. Traffic conditioning: It maybe desirable to limit traffic injection rate of some class, users declare traffic profile, example rate and burst size, traffic is metered and shaped if non-conformed. We have seen this already when we discussed congestion control. (Refer slide time : 26:48 - 30:34)

The packets are classified and then they are marked. When they are classified and marked naturally for the session, some kind of t spec that is traffic spec has been already negotiated so it meters to see whether it conforms or not and then the traffic maybe shaped. If by chance some burst comes and if the system can handle it then it will keep it for some time and then forward it only at an appropriate rate. But if it goes too much then it drops it here itself.
This QOI is absolutely important especially for multimedia traffic as was explained. for ordinary data traffic like web traffic or FTP, file download etc, those usually are not very sensitive to delay and variation in the delay that means you may get a lot of packets in one bunch then packets maybe slowing and coming extra and these things really do not matter much if it is within some bounds. But in multimedia, these things are very important. For example, when we talk, if the delay is beyond a certain level then we feel very odd.  For example, few years back if you had made an international call which is routed in a very peculiar way, you would find a lot of delay if you are speaking to somebody in USA, which is quiet annoying. Additionally what is annoying is that if the delay remains constant, some other people can manage it. But if the delay keeps varying a lot, that is, if there is a lot of jitter then that is very irritating for people. On the other hand unlike data, maybe a few bits are dropped here and there; it will not matter too much. Maybe if you analyze it using machines you will find some difference in quality but as such to human perception most of the time it does not matter too much. The multimedia traffic is quite different from ordinary data traffic. And as network is spreading and the way it is going, people find it cheaper and most convenient to use this internet or use this computer network for what was traditionally telecoms domain in the sense for audio or video conferencing. We have discussed about inserv, diffserv, RSVP etc, but the trouble is, many of the routers on the way are not going to support this because you need a consistent support throughout the path. So people still try to use the raw internet which is just handling it at the packet level not really taking note of the flows and still try to handle multimedia and for that a set of protocols has evolved. Let us just look at those protocols now. (Refer slide time : 30:35 - 31:17)

Principles: Classify multimedia applications; identify the network services that the applications need. Making the best of best effort service, you already have a best effort service and how can you make the best of it is what we will see in RTP, etc and there are some mechanisms for providing QOS. These are specific protocols for best effort and architecture for QOS. At the network level if you have supported QOS these would have gone through much better. (Refer slide time : 31:18 - 33:24)

There are various classes of MM applications and not all are the same. There is streaming of stored audio and video, that is one kind of application, for example, you download a song, movie that is a streaming, the stored audio and video is being streamed. Then there is streaming of live audio and video, then there are real time interactive audio and video and by this way you can generally classify. By multimedia application you mostly need audio and video so by this way they can be classified. 
Fundamental characteristics: They are typically delay sensitive. They are also sensitive to delay jitter which is a variation in delay. 
Loss tolerant: Infrequent losses cause minor glitches, so this is the antithesis of data which are loss intolerant but delay tolerant. Sometimes we like VCR like functionality. suppose we are seeing a video, so we try to equate it with the kind of functionality like in a VCR so client can pause, rewind, fast forward, push slider bar etc. And at the same time when a multimedia session starts maybe an initial delay of 5, 10 seconds is acceptable, 1 to 10 seconds until command effect is also acceptable but we need a separate control protocol because we really have no other way of getting any kind of control, feedback, etc on these streams as we will see. Timing constraint for still?to-be transmitted data: In time for play out.  (Refer slide time : 33:25 - 34:42)   

Suppose we have, streaming of stored multimedia. Suppose this is the recorded video which is being sent, this is the cumulative data that has been sent, this is like a staircase because they have been sent as packets so chunks of data is going together so they are being sent. First of all you record the video and the video is sent and in the third position the video is received and played at the client. This is streaming of stored multimedia. There is a network delay in between when the video is sent and the video is received. As soon as you send it, you cannot start seeing it, 
Streaming: At this time the client is playing a part of the video while the server is still sending the later part of the video. Maybe in this particular diagram the client has already started seeing something although some of the data has still not been sent. (Refer slide time : 34:43 - 38:05)

Therefore the streaming of stored multimedia is different from streaming of live multimedia. For example, internet radio talk show is a live multimedia, a live sporting event which is being web cast. Streaming has a playback buffer, so playback can lag 10?s of seconds after transmission but it still has some timing constraint. The interactivity is limited, for example, obviously if it is live you cannot fast forward it but rewind and pause maybe possible. It solely depends on how you handle it. So this is unlike a stored multimedia where fast forward would also be possible. 
Interactive real time multimedia: Is one of the most important applications like IP telephony, video conference of the web, distributed interactive worlds etc are the applications of interactive real time multimedia which has the most stringent QOS requirements.
End-to-end delay requirements: Different applications have different kinds of requirements. for example if it is audio, if the end-to-end delay is less than 150ms it is good and less than 400ms is acceptable but beyond that people seem to be bothered a lot. It includes application level that is packetization and the application level delay and the network delays. Higher delays are noticeable and they impair the interactivity. 
Session initialization: How does callee advertise its IP address port number, encoding algorithms etc because like in an IP telephony call all these issues are there. If you are using a standard TCP or UDP IP this is what you would have to do actually but there are no guarantees on delay or loss. But multimedia application requires some QOS and level of performance to be effective.
Today?s internet multimedia applications use application level techniques to mitigate as best as possible. This is what we mean by best efforts, this means we cannot do anything, maybe at the network layer etc it would implement an inserv or diffserv, but in most of the places that is not practical at the moment so we try to do something at the end points i.e. at the application level. The application level necessarily means the end point because it is the intermediate, it will not go right up to the application layer but maybe it will penetrate a little deeper. This is mostly done from end to end. (Refer slide time : 38:06 - 40:18)

In this pseudo protocol, the number of protocols, first we look at real time protocols RTP that is RFC 1889. RTP specifies a packet for packets carrying audio and video data. So RTP packet provides payload type identification such as what type of payload it is carrying, packet sequence numbering, time stamping that is important for giving some feedback. RTP runs in the end systems. RTP packets are encapsulated in UDP segments.
Interoperability: If two internet phone applications run RTP then they maybe able to work together and may communicate to each other through the network. RTP runs on top of UDP. We have seen in both TCP and UDP, the UDP is a very lightweight protocol so it is also expected to be much more efficient than TCP. TCP is a heavy weight protocol so it has a lot of overhead and all these ACKs keep on flowing. RTP libraries provide some kind of support over an above the transport layer library, that is how a RTP runs on UDP so this is a provider transport layer interface that extend a UDP. So port numbers, IP addresses are there in UDP itself and then payload type identification, packet sequence numbering and time stamping is what RTP adds to UDP. And then, the application is here, it moves a little bit into the application domain. Maybe just look at the RTP header to do some what better service. (Refer slide time : 40:19 - 41:43) 

RTP by itself does not provide any mechanism to ensure timely delivery of data or provide other quality of service guarantees, because that really depends upon what is happening on the way. An RTP is running just between the two end points. So RTP encapsulation is only seen at the end system which is not seen by intermediate routers. So routers providing best effort service do not make any special effort to ensure that RTP packets arrive at the destination in a timely manner. The services in between is still a best effort. Since we look at it at the end points there we can effect on some kind of control. If you have noted, this RTP header goes in front of the payload for the data. For example, it is the voice which is traveling then the voice signal is digitized and packetized before each packet produced by your application layer, the RTP header will come which is available in the data. There is a separate protocol control called Real Time Control Protocol or RTCP. (Refer slide time : 41:44 - 43:25)

This is used in conjunction with RTP for multicast transmissions. Each participants in RTP session periodically transmits RTCP control packets to all the other participants. Each RTCP packet contains sender and receiver reports. So reports statistics etc are useful to applications and this is what you might use to improve the performance of your session. Inside the network, inside the backbone nobody really knows about your session so it is only the end points. But they exchange this kind of a statistics through the RTCP packets and then the application can control the rate at which things can be done, etc. Therefore some kind of feedback mechanism is present. The statistics include number of packets sent, number of packets lost, inter arrival jitter etc. Feedback can be used to control performance, sender may modify its transmissions based on feedback. This is the main point, you get some kind of feed back as this is running on UDP. There is no ACKs, packets coming on which you can send these statistics. Therefore what is done is that, this RTCP is actually in band control so a part of the bandwidth is kept for RTCP and through this bandwidth they exchange this kind of information and the time stamp, so you know that how much delay is being experienced at the other end. And after having all these  statistics you may do what you can at the application layer to modify the way you are transmitting in order to get better service. (Refer slide time: 43:26 - 44:14)

There are receiver reports, fraction of packets lost, last sequence number, average inter arrival jitter, etc are a part of the receiver report. Sender report: SSRC of the RTP stream, the current time, the number of packets sent, and the number of bytes sent. Source Description: E-mail address of sender, sender?s name, SSRC of associated RTP stream etc. Provide mapping between SSRC and the user or host name. (Refer slide time : 44:15 - 45:46)

Another use of RTCP is for synchronization. RTCP can synchronize different media streams within a RTP session. So consider video conferencing applications for which each sender generates one RTP stream for video and one for audio. The point is, you have got two streams of packets for the video streams and then the audio has to be synchronized with it. The lips are moving in some other way and sound is coming in some other way so that will not help in anyway hence they have to be synchronized. In order to synchronize this, RTCP uses the time stamp which I referred to earlier and tries to synchronize the two different streams. This is another important use of RTCP. So time stamps in the RTCP packets are tied to the video and audio sampling clocks. They are not tied to the wall clock time. The wall clock times are also exchanged and this is to get an idea about the delay. Each RTCP sender report packet contains for the most recently generated packet in the associated RTP stream, timestamp of the RTP packet and wall clock time for when packet was created. This gives you an idea about the delay. Receivers can use this association to synchronize the play out of audio and video. (Refer slide time : 45:47 - 48:02)

RTCP Bandwidth Scaling: RTCP attempts to limit its traffic to 5% of the session bandwidth. Remember, that this is in band control that means whatever bandwidth is given for the session it is within that only. So whatever the RTP actual data packets are using, maybe 5% of that, the RTCP tries to use and in this 5% it is kind of divided, like 25% for the sender, and 75% for the receiver because there may be many receivers and this 75% will again be distributed amongst the receivers. Suppose one sender sending video at a rate of 2 mbps then RTCP attempts to limit its traffic. That means to the RTCP traffic to 100 kbps. RTCP gives 75% percent of this rate to the receivers, remaining 25% to the sender. We come to another protocol called RTSP known as the Real Time Streaming Protocol. Earlier we were talking about interactive live audio or video conferencing, but now let us talk about streaming. User interactive control is provided, for example, the real time streaming protocol where a helper application displays content which is typically requested via a web browser, for example a real player, then we have the typical functions namely decompression if it is compressed in audio or video, jitter removal which means you play them back at a constant rate, error correction: use redundant packets to be used for reconstruction of original stream, some kind of Graphical User Interface for user controls like rewind, fast forward, etc are given by RTSP. What it does not do is, it does not define how audio and video is encapsulated for streaming over network, it does not restrict how streamed media is transported, whether TCP or UDP does not specify how the media player buffers audio and video so these are more at the application layer. (Refer slide time : 48:03 - 49:08)

This is the simple approach for steaming audio or video. Audio or video is stored in file. Files are transferred as http object that is received in entirety at client then pass to player. So audio or video are not streamed and so no pipelining, long delays until play out. Here the problem is that if it is some kind of a movie and if it is downloaded then the entire file is downloaded, so downloading that entire file will take a lot of time, this is a very simple approach and then there is no problem, you can download the entire file and then play it use options like fast forward or reverse or speed up etc. But streaming in the media means, as it starts streaming the audio or video file then after some delay the play back also starts. (Refer slide time : 49:09 - 50:43)   

Simple situation is, we have a web browser, web server with audio files and a media player and that is how we do it. The ACTP request, so browser gets a meta file then the browser launches a player passing the metafile to the player and the player contacts the server and the meta file contain all the necessary information, this is called progressive download. Let us look at the streaming multimedia client buffering. Suppose as before this is the constant bit rate video that is being transmitted and now because of the network delay and the availability of the network delay this is what the profile looks like of how it is received. But if you do this after some sufficient delay and you buffer it, the main point is, these two lines are never crossing. So now you can play it at a constant rate so all these delay variation etc that will sort of go and you would get a nice streaming audio or video. This is the basic idea of streaming multimedia. In this part, the difference between these two where this is the buffered part, so the client side buffering play out delay compensate for network added delay or delay jitter etc. (Refer slide time : 50:44 - 51:29)

Streaming multimedia: Is where UDP is usually preferred. We will also look at how TCP is done. The server sends at rate appropriate for client oblivious to network congestion. So, often sent rate is equal to encoding rate that is constant rate, then there is a fill rate for constant rate minus packet loss. Short play out delay 2-5 seconds compensate for the network delay jitter. Somebody does not really mind so much, that if the play back starts after 5 seconds but in this 5 seconds the entire thing will be buffered and that will take care of the jitter. Error recovery takes place if time permits. (Refer slide time : 51:30 - 52:07)

TCP: it sends at maximum possible rate under TCP and fill rate fluctuates due to TCP congestion control because in TCP congestion control the windows size can keep on varying so larger play out delay is smooth TCP delivery rate. If you start playing it after quite some time so that you can handle this TCP variation in rates etc then you can get a smooth rating. Then http TCP passes more easily through firewalls rather than UDP. (Refer slide time : 52:08 - 52:24)

RTSP example: Meta file is created and communicated to the web browser. Browser launches player as we have seen and player sets up an RTSP control connection data connection to streaming server. (Refer slide time : 52:25 - 52 - 47)

Web browser gets an http GET and web server presentation description. Then the media player is setup a connection with the media server and it starts getting it, buffers it and starts playing. The media stream comes, then you may pause, you may teardown, etc.  (Refer slide time : 52:48 - 53:09)

Finally I will just mention about two protocols namely: H.323 which is an ITU standard which is the old protocol for setting up telephone calls. H.323 is aging a little bit and there is a (Refer slide time : 53:10 - 53:28)

newer protocol namely session initiation protocol. All telephone calls and video conferencing calls take place over the internet. That is the vision, you can reach the callee no matter where the callee roams, no matter what IP devices the callee is currently using. Therefore (Refer slide time : 53:29 - 53:58)

for setting up calls there is a large protocol. 
Setting up a call: Provides mechanisms for caller to let callee know she wants to establish a call. And provides mechanism so that caller and callee can agree on media type and encoding because that has to agree and it provides a mechanism to end the call. The first one is called an invite and there are all kinds of  (Refer slide time : 53:59 - 54:29)

commands for this. It also has the facility to determine the current IP address of callee, maps mnemonic identifier to current IP address. There are other components of chip like, chip register and so on. Then it has something for call management, that means add new media streams during call, change encoding during calls, invite others for a conference, transfer and hold calls etc. (Refer slide time : 54:30 - 54:38)

 There are all kinds of details about codec negotiation etc. (Refer slide time : 54:39 - 56:15)

And then you may reject a call also. The audio, video and multimedia over the internet is going very strongly. At one point of time network performance was so poor that there was no question of accommodating this. But as the network performance has increased exponentially over the past years now it is becoming much possible to handle lot of multimedia traffic through network. This is getting more popular.. We talked about inserv, gifserv but the actual strategy that people actually deploy is when they feel this is not sufficient then they put in more bandwidth so put another fiber  and take up your multiplexer weight etc so that you solve the problem because the demand for multimedia is so great. 

Good day, today we will talk about network management. As the network becomes very large we need support for managing these networks, we need support for keeping track of these networks and so, we are going to discuss on how that is done.  (Refer slide time :56.16 - 56.18)

Today?s topic is network management. (Refer slide time :56.19 - 56.42)

Network management is the process of controlling a complex data network to maximize its efficiency and productivity. The overall goal of network management is to help with the complexity of a data network and to ensure that data can go across it with maximum efficiency and transparency to the users.  (Refer slide time :56.43 - 57.18)

The ISO, that is the International Organization for Standards in network management forum divided network management into five functional areas: Fault management, configuration management, security management, performance management and accounting management. Out of this, actually it is quite often you find that there are two systems which was deployed. (Refer slide time :57.19 - 57.41)

In that case the trap will never come. The only thing is that when the network management station polls that particular device so it will fail to respond. That way you will find out that something is wrong. So you should use a mixture of trap and polling to do your management.  (Refer slide time :57.42 - 58:30)

Let us quickly go through the types of SNMP packets: One is the get request, it retrieves the value of a variable or a set of variables, and get next request used to retrieve values of entries in a table, so the next entry. Get bulk request retrieves a large amount of data used instead of multiple get request and get next request so that in one go you can get a lot of information as far as the bandwidth is concerned. Set request: set or store a value in a variable.
Response: response to get request or get next request contains values and or variables requested.
Trap: sent from agent to manager to report an event.  (Refer slide time :58:31 - 58:48)

Inform request: sent from one manager to another remote manager to get a value of some value from agents under the control of the remote manager. 
Report: designed to report some types of errors between managers but not very widely used. (Refer slide time : 58:49 - 58.58)


Now we come to the SNMP data types.




	

COMPUTER NETWORKS
Prof : Sujoy Ghosh
Department of Computer Science and Engineering
IIT, Kharagpur
Lecture#37
Network management
(Refer slide time:00-57 min)



Good day, so today we will talk about network management. You know that as the network becomes very large we need support for managing these networks, we need support for keeping track of these networks and how these are done is what we are going to discuss. 

 (Refer slide time-1:13- 1:16min)

So the topic for today is network management. (Refer slide time: 1:17- 1:40 min)

Network management is the process of controlling a complex data network to maximize its efficiency and productivity. The overall goal of network management is to help with the complexity of a data network and to ensure that data can go across it with maximum efficiency and transparency to the users. (Refer slide time: 1:41- 2:54 min)

The ISO that is the International Organization for Standards in network management forum divided network management into five functional areas: Fault management, configuration management, security management, performance management, and accounting management. Out of these often you find that there are two systems which has deployed in large network. One is the Network Management System and another is the Enterprise Management System. Enterprise Management System usually takes care of the configurations of individual machines etc, and the Network Management does the fault management and performance management. These are two aspects which network management always has to take place and it can do some configuration management for some network devices also. So we will just quickly run through each one of these. (Refer slide time: 2:56- 5:48 min)

Fault management is the process of locating problems or faults on the data networks. It involves the following steps: Discover the problem, isolate the problem, and fix the problem (if possible). Sometimes it is possible to fix the problem remotely and sometimes it is not possible. First of all discovering the problem is important because usually in a large network there are so many hundreds of components which have been deployed over a considerable area. You need to have a good support for discovering the fault. A fault in one part of a network can lead to impairment of services in another part of a network. For example, suppose some switch has become faulty and it is generating a lot of spurious traffic, now the first thing is to locate where all these traffic is coming from, and then you have to isolate it, and then fix it. Sometimes you need to physically go there and fix it but sometimes you can do it remotely. But before you can fix it, you have to discover that there is a problem and this is where the problem lies, this is the central part of fault management. Another thing to note is that, the network devices may come up first and be switched off. Therefore the topology of the entire network is not fixed, some of them may be off, some may be switched on but it is not working. So anyway the topology of the network is not fixed therefore for fault management, first you have to discover which are the areas or which are the elements that are currently on the network and are put on and are working correctly and then which others are not working. That is, they are switched off, or may be they are faulty etc. Continuously you have to be in a mode of discovering the topology of the network, as to find out which of these are switched on and which is doing what. Then we need to locate the faults as it occurs. This is fault management. Then you have configuration management. (Refer slide time: 5:45- 6:02 min)

The configuration of certain data network devices controls the behavior of the data network. Configuration management is the process of finding and setting up that is (configuring) these critical devices. (Refer slide time: 6:03- 6:36 min)

Security management: it is the process of controlling access to information on the data network. It provides a way to monitor access points, and records information on a periodic basis, and provides audit trails, and sounds alarms for security breaches. We will be talking more about security in the next lecture which is a very big topic by itself but we will devote only one lecture to it. There we will talk little bit more about the security management and securities in network. (Refer slide time: 6:39- 7:48 min)

Performance management: Involves measuring the performance of the network, hardware, software and media. Examples of measured activities are: Overall throughput percentage utilization, error rates, response time. Quite often a fault in a network does not always be faulty but there may be degradation in performance. For example, in one particular network node you may find that a lot of packets getting dropped. So you have to find out periodically that, who is doing what, and how, in a sense that at what percentage? How many packets it has handled? How many it has handled successfully? How many it has dropped and so on. And from such statistics you have to come to a conclusion about whether this is really malfunctioning or not. Error rates are important, response time, percentage and utilization. Once again there are ways to find out these things and we will talk about them. (Refer slide time: 7:51- 8:11min)

Accounting Management: It involves tracking individual?s utilization and grouping of network resources to ensure that users have sufficient resources. This involves granting or removing permission for access to the network etc so this is also important in many cases. (Refer slide time: 8:13 - 10:50 min)

Now the protocol that is used for network management is known as SNMP Simple Network Management Protocol. Objectives are following: It is a framework for management devices in an internet using TCP/IP. It provides a set of fundamental operations for monitoring and maintaining an internet, and an application level protocol allows it to monitor devices made by different manufacturers installed on different physical networks. In a particular network all the network devices will not come from the same vendor. You may have procured some of the devices at some point of time from some vendors and some other devices from some other vendor and all of them form a part of the same network. When you are talking about network management in that place, you have to talk about all these heterogeneous platforms. There has to be a standard protocol by which the network manager can talk to each of these devices. When you look at the network devices which are available in the market, you find that there are broadly two types of devices: managed device and unmanaged device. For example, for a switch there is a managed switch and an unmanaged switch. Obviously the switch which is unmanaged does not support this SNMP and do not have the so called management agent in it. In order to manage and get the information about it from a central place the devices have to be managed. Therefore at the very edge of the network for cost consideration you may put some unmanaged devices but then management of fault etc, for those devices have to be handled separately. You cannot monitor them centrally. For monitoring them centrally you need to have managed devices. Managed devices would contain some kind of software and some hardware support inside each of the device to talk to this network management station wherever it is broke. Usually this costs money but for a large network this is important. (Refer slide time: 10:51 - 11:12 min)

This is a very simplified picture, we have the network manager here and through the TCP/IP protocol it talks to an agent. This agent resides in a managed device and these agents will have some agent variables which should be available to this manager for monitoring. That is the basic idea. (Refer slide time: 11:14 - 11:48 min)

Agent: is a router or host that runs the SNMP server program. By the way, apart from managing network devices you can also have management agents in software etc to instrument it in some fashion. The agent keeps performance information in a database, can send a trap to the manager if something unusual occurs. So, trap is a sort of alert to the manager. (Refer slide time: 11:50 - 11:56 min)

Manager is a host that runs the SNMP client program and has access to values in the agent?s database. (Refer slide time: 11:57 - 12:28 min)

Based on three basic ideas, a manager checks an agent by requesting information that reflects the behavior of the agent. A manager forces an agent to perform a task by resetting values in the agent database. An agent contributes to the management process by warning the manager of an unusual situation. These are the three basic approaches and we have specific commands for doing all these. (Refer slide time: 12:29 - 12:46 min)

SNMP uses two other protocols: SMI: Structure of Management Information, and MIB: Management Information Base. So, one is used for naming objects and the other is the basic management information base accessed by the manager. (Refer slide time: 12:47 - 12:57 min)

SNMP defines the format of packets exchanged between a manager and an agent. It reads and changes the status that is the values of objects, variables in SNMP packets. (Refer slide time: 12:58 - 13:52 min)


SMI defines the general rules for naming objects, defining object types including range and length and showing how to encode objects and values. SMI defines neither the number of objects an entity should manage nor the names the objects to be managed nor defines the association between the objects and their values. Instead, what SMI does is to have some scheme for giving names to the object. The names are not human friendly but they look as a series of integers, but there is a way of encoding the names of objects so that we are very precise and distinct about what we are talking taking into consideration we can have a heterogeneous network. (Refer slide time: 13:54 - 14:04 min)

The role of MIB: MIB creates a collection of named objects, their types, and 
their relationships to each other in an entity to be managed. (Refer slide 
time: 14:06 - 14:30 min)

SNMP: A group was formed and their efforts were completed in early 1993. There are 12 documents describing SNMP version 2, there is an SNMP version 3 also. There are three basic commands that are used with SNMP: get, set, get next. These are the basic operations then you can have a bulk, get, etc. (Refer slide time : 14:31 - 15:35 min)

This is the general situation we have. Network management station: There may be more than one network management station. In a very critical network you may have two network management stations, etc. Therefore we have some network management station which runs some network management application, NMA. This software talks in SNMP which manage devices. These are network elements which are managed. This NMA Network Management Application uses the SNMP protocol to talk to the management agent which resides in every managed object. This managed object maintains the MIB Management Information Base and from this the information passes back and forth. Hence this is the general scheme. (Refer slide time: 15:36 - 15:40 min)

There are two approaches for the management system to obtain information from SNMP: One is the traps and other is polling. (Refer slide time: 15:41 - 16:14 min)

First let us talk about traps. Traps are basically the information which is being pushed from the managed device to the network management station. Traps are un-requested event reports that are sent to a management system by an SNMP agent process A. A trap will contain the network device name, time the event happened, and the type of event. (Refer slide time: 16:15 - 16:51 min)

When a trappable event occurs, a trap message is generated by the agent and is send to a trap destination - a specific configured network address which is the network address for the management station. Many events can be configured to signal a trap like a network, cable fault, failing NIC, or hardware, a general protection fault, or a power supply failure. Although many events can be configured to signal a trap but there are pros and cons of what event is being configured a trap, there are trade offs. (Refer slide time: 16:52 - 18:15 min)

Traps can also be throttled. You can limit the number of traps sent per second from the agent, this is important. For example, a faulty node may keep on generating a very large number of traps. So that it will overwhelm the network and it will not carry any extra information in helping to set right the fault. You may have to constrain the number of traps that can be generated per minute time or something. So traps have a priority associated with them. Critical, major, minor, warning, marginal, informational, normal, unknown etc. Actually it may not be a good idea to generate traps. For low clarity may be you should not be generating any trap at all because if you do that, remember that a network may consist something of the order of a few hundred or a few thousand network devices and if all of them generate routine reports and keep on sending traps then that may not be a very effective way of managing a network. (Refer slide time: 18:16 - 18:43 min)

Resources are required on the network devices to generate a trap. When a lot of events occur the network bandwidth may be tied up with traps. So, thresholds can be used to help because the network device has a limited view. It is possible that the management system has already received the information and the trap is redundant. If something has happened somewhere else and somebody else has detected it and generated a trap then that is not generating any extra or relevant information.  (Refer slide time: 18:49 - 19:26 min)

The network management system periodically queries the network device for information. So this is the other mode which is SNMP polling. From time-to-time the network management agent will request for some information from the management agent and this might do in a round robin fashion, in a periodic fashion using some period which is configurable. The advantage is, the network management system is in control and knows the big picture. So this is the other way; one is trap, the other is polling. (Refer slide time: 19:27 - 20:25 min)

The disadvantage is the amount of delay from when an event occurs to when it is noticed. For short interval the network bandwidth is wasted and for long interval the response to events is too slow. The point is that, if you are depending only on polling and if you are polling at a very low frequency then after your last poll immediately some fault may have occurred but you will discover that only when you poll next. So, in a long interval the response to events is quite slow. On the other hand if you poll very fast then most of the network bandwidth and network resources are used for this network management so that is not very efficient and bandwidth is wasted. So you have to come to some kind of trade-off between the two. (Refer slide time: 20:26 - 21:23 min)

One good way to approach this is to use both traps and polling. When an event occurs the network device generates a simple trap. The management system then polls the network device to get the necessary information. The management system also does the low frequency polling as a backup to the trap. What might have happened is, you cannot depend purely on trap for one reason that is, the network device may have gone down in such a way that it is not even able to generate the trap. In that case the trap will never come, only thing is that, when the network management station polls that particular device it will fail to respond and that way it will find out that something is wrong. So you should use a mixture of trap and polling to do your management. (Refer slide time: 21:24 - 21:26 min)

Let us quickly go through the types of SNMP packets: GetRequest - It retrieves the value of a variable or a set of variables; GetNextRequest ? Is used to retrieve values of entries in a table; GetBulkRequest- Retrieves a large amount of data used instead of multiple get request and get next request so that in one go you can get a lot of information as far as bandwidth is concerned. (Refer slide time: 21:57 - 2:11 min)

SetRequest - Set or store a value in a variable; Response ? Response to get request or get next request contains values of variables requested; Trap - Sent from agent to manager to report an event. (Refer slide time: 22:15 - 22:31 min)

Inform Request - Sent from one manager to another remote manager to get some value from agents under control of the remote manager; Report - designed to report some types of errors between managers but is not very widely used. (Refer slide time: 22:32 - 24:01 min)

Now we come to the SNMP data types, These data types have to be, since network may contain devices from different vendors, each vendor of course will design his own agent so he has to agree to some kind of standard about what will be there. But at the same time we do not want to standardize things in such a way so that people cannot innovate and come up with new kinds of devices. What is defined are the different data types, different types of variables, one is the integer - it is a signed 32-bit integer, Octet string that is the byte string, Object identifier - is one of the most important data types and we will look into the detailed of object identifier when we talk about SMI; NULL: Is not actually a data type but a data value, IP address is a special data type, it is an octet string of size 4 bytes. We are using 4 byte of addresses in network byte order. (Refer slide time: 24:00 - 25:02 min)

Then you have a counter: It is an unsigned 32 bit integer which rolls over. The 32 bit will count to about 4 trillion. After the end of the counter the value is rescheduled which means it will go back to zero and start counting from zero again. This counter is more important because that is what will give you the statistics about the network. For example, you want to know that in the last may be ten minutes or so how many packets this has been sent, and how many packets has been dropped, and how many packets has been successfully processed etc. So the management station may want to know such information. So these are the kind of statistics you collect. Therefore you need to have these counters for these different values. Now a counter is going to go back to zero after sometime so you need to take care of that. (Refer slide time: 25:02 - 26:05 min)

If this going back to zero then after a poll, when you have got a value and when you see that the counter value is sufficiently high, you may like to set it to zero deliberately so that you get the right count next time. Gauge: unsigned 32 bit integer, once again it is the same as counter but it will top out and stay there. That means, it is going to top out and the next time you come it will not show a very low value but it will show a high value. And if you see that the gauge has topped out may be you can reset it after that but you get some estimate of what has happened rather than a rolled over value. Time ticks: Unsigned 32 bit integer so 32 bit integer rolls over after 497 days. But 497 days is very long in this network world. (Refer slide time: 26:06 - 26:36 min)

Opaque: Used to create new data type not in SNMPv1. And then there are some other data types for specific purpose like date and time, display string, MAC address, physical address, time interval, time stamp, truth value, variable pointer, textual conventions used as types. These are the different SNMP data types. (Refer slide time: 26:37 - 27:43 min)

Let us just have a look at the SNMP MIBS. Management Information Base is a collection of related managed objects used to define what information you can get back from the network device. These are standard and enterprise specific MIBS. The point is that, some things may be standard, and there are some information about the device. For example, if we have a device designed by CISCO then that may have its own specialty which is not found in other vendors. Therefore that specialty will require some information which is specific to the particular vendor. Hence we have vendor specific information also, and we have general information also. There are standard and enterprise specific MIBS. And, if you later on mix up between the two, that means, something that is specific to vendor 1 and something specific to vendor 2 it will not work out and so these two need to be separated. So naming of objects is very important. Thus there is standard and enterprise specific MIBS. (Refer slide time: 27:45 - 27:55 min)

Types of MIB modules: Standard: These are the standard MIBS currently designed to capture the core aspects of the particular technology. (Refer slide time: 27:56 - 28:10 min)

Experimental: Temporary and if achieves standardization then it is placed in the standard module. Then Enterprise-specific: Vendor specific MIBS that provide additional management capabilities for those features that require it.  (Refer slide time: 28:11 - 28:56min)

If you are using the MIB you also require the MIB tools, you require a MIB compiler, you require a MIB browser, a MIB alias tool, a MIB query tool, etc. A MIB browser allows you to browse through the management information base in a particular device directly. And MIB alias is required because, as we see the naming is very complex. So once you have given a name you do not have to repeat that name in every query so you have an aliasing tool and a MIB query tool. (Refer slide time: 28:57 - 29:10min)

SMI: Structure and identification of management information. The SMI defines the rules for how managed objects are described and how management protocols may access these objects. (Refer slide time: 29:11 - 29:24 min)

Functions: Are to name objects, to define the type of data that can be stored in an object, to show how to encode data for transmission over the network. (Refer slide time: 29:25 - 29:52 min)

Name: SMI requires each managed object that is router, variable in a router, etc. which have a unique name. So router will have a name, a variable in a router will also have a name, and a specific router coming from location should also have some way of describing that. (Refer slide time: 29:53 - 32:24min)

The naming convention really starts from a high level and it uses the integer.representation. There is a name.notation. Actually ISO has defined a very global naming tree. It starts from ISO itself, and under ISO there are so many things and under each of these there are so many things. For example, ISO.org.dod.internet.Management.MIB 2 and when I say this much, actually this ISO has got the number 1 and under ISO there are so many entities, one of them is called organization and this is an entity, a number 3 and there is a department of defense of US, that is an entity with a value 6. ISO is a European organization and all these are decided by ISO. Therefore the Department of Defense did not take a note of it. But when the internet started on the Department of Defense they saw that nobody has used anything of the naming convention and they decided to take one. And then management.2, then MIB is .2 and up to this much we have not even come anywhere close to the particular device that we are trying to manage. All objects managed by SNMP are given an object identifier. The object identifier always starts with 1.3.6.1.2.1 because we are talking about network management, and in network management naturally it has to do with network, then may be it has to do with MIB.2 and then internet and since it is the internet it has to do with DODN and all the way up to ISO. So that 1.3.6.1.2.1 etc is always the prefix of any name and there are many other integers with many more DODs in them. (Refer slide time: 32:25 - 32:52 min)

Apart from this an object in SMI has a textual name which is there in SMI termed as the object descriptor for the object type along with its corresponding object identifier which is defined. Syntax: the abstract syntax for the object type. It can be a choice of simple syntax that is integer, octet string, etc or application syntax. (Refer slide time: 32:54 - 33:08 min)

Definition is a textual description of the semantics of the object type. What type of object it is? Access: one of the read only, read write, write only, or not accessible kind of access is defined. And status: One of mandatory, optional, or obsolete. (Refer slide time: 33:10 - 33:30 min)

This object identifier is also a part of that, it is like a telephone number and it always starts with 1.3.6.1.2 and SMI uses it as the base for defining new objects. (Refer slide time: 33:31 - 34:25 min)

In the first group, ISO was 1, CCITT was 2, and for the joint ISO ? CCITT there was a number 3. Since this is under ISO that is how the first one comes. The second group for the ISO Node administrator defines 3 for use by other organizations. So in 1.3 the 3 is the other organization. Actually there are a large number of things in that particular level, we are having a global naming tree for naming anything under the sun. So there is a thing called other organization and that other organization is 3 and the third group defines 6 for the use of U.S. Department of Defense, so it is 1.3.6. (Refer slide time: 34:26 - 34:45 min)

In the fourth group the DOD has not indicated how it will manage so internet is 1. The fifth group was approved by IAB to be 1 for the use of OSI directory, 2 for object identification for management purpose. So this is the one that we are interested in. (Refer slide time: 34:46 - 36:22min)

Therefore we have 1.3.6.1.2.1.1.1 and since it is a global naming tree we have to come quite deep down to any particular level. Finally it is possible that not only you have some network devices etc somewhere down in the tree but you also have some sub trees for specific vendors. So this naming can go down that sub tree to talk about something specific to a particular vendor. Or even if we have come down to the tree somewhere and now you have got into a router, now in the router also we want to talk about interfaces and about other things so the tree also goes down from that point. So the point is, through this naming procedure we can exactly specify anything that is being talked about in this whole management scenario. Only trouble is, it is almost impossible for human beings to remember any of these names or any of these things. But the network management application makes this query to MIBS etc., so you are usually spared the trouble of looking into this. But if you look into this you will find such object identifiers in the MIB. (Refer slide time: 36:23 - 36:53 min)

The MIB-2 group: This was divided into ten groups: One for system, one for interfaces, one for address translation, one for internet protocol and so on. And the number of objects in each of these groups is given over here. (Refer slide time: 36:54 - 37:32 min)

We have a system group which has a system descriptor system, object ID system up time, system contact. The system up time is the time since last re initialization, and system object ID is vendor object identification and so on. This makes the scheme very general so that even devices from heterogeneous vendors can also be accommodated and you can talk about very standard things and that is why it was done this way. (Refer slide time: 37:33 - 38:03min)

Now let us look at the network management platform. Historically, network management revolved around multiple systems each managing one specific set of components on the data network. Restrictions of money, physical space, and technical expertise led to the desire to have the components managed by a single system that would show their inter connections on a network map. (Refer slide time: 38:04 - 39:20 min)

This is how a network management platform evolved. It is a software package that provides the basic functionality of network management for different network components. So, a network management platform has to be a general kind of software. For example, on one side you have specific vendors for managing a specific set of devices, you may have some network management application etc which is there. Hence you have this particular application from this vendor for managing his devices, and then you have some other network management application for some other devices from some other vendor etc. But then finally you would like to sit in one place and have the big picture. So there has to be a platform which can couple with these different network management applications known as the network management platform, i.e. the general software which is running which can integrate all these different components.  (Refer slide time: 39:21 - 39:57 min)

The goal of the platform is to provide generic functionality for managing a variety of network devices. There are some functionality and some requirements for network management which is very general. Those are collected in the network management platform and if there are specific things you have to talk about for a particular type of device may be from that particular vendor then that is out of the platform although that software can integrate with this network management platform. That is the idea of having a network management platform. (Refer slide time: 39:58 - 41:11min)

Basic features for any platform to include are Graphical User Interface GUI and network map. The first thing that a network management platform should do or is expected to do is to go on discovering the different network devices which are right now on, which are accessible etc, and those which are not accessible that means which were supposed to be previously there. And it would be nice if you can show it graphically on a screen like showing different colors for different types, and different icons for different types of nodes, and different colors showing whether it is working, not working, switched off etc, and if you can see that picture then you get the big picture in one go. Therefore this network map is important. A DBMS naturally wants to keep this management data and then query etc so it needs some DBMS. You need a standard method to query devices and a customizable menu system, event log, etc. These are the basic features for any platform. (Refer slide time: 41:12 - 42:19 min)

Additional features for a platform using graphing tools which graphically shows with some kind of plots that how each device is performing. This should be able to show you the plot very easily. So this graphing tool is also an integral part. Then the application programming interface API is important because after you have finished talking about the generic network management aspects then you also have these special aspects for which the different vendors are going to give you special tools. These special tools now have to integrate with this network management platform. Therefore the API Application Programming Interface has to be very clearly defined. Finally it is the system security which is always present. (Refer slide time: 42:16 - 42:33min)

Here are some examples of network management platforms: Sun?s SunNet Manager, IBM Netview, HP?s OpenView by the way is a very widely used network management platform in this category. (Refer slide time: 42:34 - 43:02 min)

If you remember, one diagram in which we showed two network management stations etc which has to do with the network management architecture and the network management platform can use various architectures to provide functionality. The three most common are: Centralized, Hierarchical and Distributed. (Refer slide time: 43:04 - 44:22 min)

In the centralized architecture as the name suggest you have one central network management station or network management platform besides on a single computer system that is a centralized system. In that case that particular station becomes absolutely critical because in a large network network management software is used all the time. Hence if you have them centrally it becomes a single point of failure. So what you may do is that, you may duplicate the same thing on another machine. If one of them fails, it will not want to have single point of failure in a network if you want to at least make the critical components of it; and network management is a critical component of a large network. So you may like to have a fall back mechanism where in the primary one fails the other one can immediately take over. Although we have put two network management stations this is a centralized architecture. Therefore for full redundancy the computer system is backed up by another system, can allow access and forward events to other consoles on network. So this is a centralized architecture. (Refer slide time: 44:23 - 44:38 min)

This is used for all network alerts and events, all network information, access all management applications etc. Hence one central architecture is used. (refer slide time: 44:38 - 45:20min)

And the pros are the following: A single location to view events and alerts, a single place to access network management applications and information, security is easier to maintain. This is an advantage, so in one place you get all that you want to view and it is easier to make it secure, and a single place to access the NMI because using the NMI, set etc you can also do things to the network, and you do not want any unauthorized person to have access to the network management station. (Refer slide time: 45:21 - 47:08 min)

In the cons of the centralized architecture it has disadvantages. Single system is not redundant or fault tolerant, but you can make it somewhat fault tolerant by keeping another machine but if the connection to this room is somehow cut then the entire network becomes a black box. As network elements are added, may be it is difficult or expensive to scale the system to handle the load, having to query all devices from a single location. As a matter of fact think of an enterprise of today. An enterprise of today that may be across various locations, it may be over a WAN, and many of these locations may be quite big hence a network that an enterprise may want to control centrally may become too big. So there is a question of whether a central server can scale it to that extent. Actually there are problems, for example, if WAN bandwidth server or some part of it is out of WAN and WAN bandwidth is also a problem because if all the network management traffic is also coming over a WAN then that becomes quite an expensive scenario. But at the same time you have this advantage of this centralized architecture that in one place you can view all the things. These are the pros and cons, and so having to query all devices from a single location will not be good. (Refer slide time: 47:09 - 49:14 min)

Therefore we go to the next step which is the hierarchical architecture. It uses multiple computer systems where one system acting as a central server and other systems working as clients. Central server requires backups for redundancy, this is the situation. Once again think of the same enterprise which is distributed geographically which may be five different locations and in each of these locations you have a big local area network to manage. So what you might do is, for each of these locations you may put its own management station who will manage the local network. You can even put a central server and all these management stations in different locations would be the client to the central server so only the important information or only the condensed and summarized information is coming to the central location via the WAN. Therefore by this you conserve WAN bandwidth and at the same time you have a central location where you can get the entire picture and if you want you can talk to one of the clients and drill down and look at the specifics if there are problems somewhere, so this has all kinds of other advantages. For example, you can keep very good and expensive network experts in the central location as he can detect a problem over there and then over the network itself he can actually look into the issue and offer a genuine advice as to what needs to be done at that juncture. This is an advantage and this is a hierarchical architecture. The central server can also be backed up for redundancy. (Refer slide time: 49:15 - 49:36 min)

Key features: Not dependent on a single system. So, even if a single system or a single link is cut still some of the network is being managed. Distribution of network management tasks is distributed now. Network monitoring distributed throughout the network, and centralized information storage is also there. These are the advantages of hierarchical architecture. (Refer slide time: 49:38 - 50:02 min)

Pros: Multiple systems to manage the network.
Cons, the disadvantages: Information gathering is more difficult and time consuming because it is coming through two layers. And the list of managed devices managed by each client needs to be predetermined and manually configured. (Refer slide time: 50:03 - 50:56 min)

Then we have a distributed architecture. It combines the centralized and hierarchical architecture. It uses multiple peer network management systems. Each peer can have a complete database. Each peer can perform various tasks and report back to a central system. Previously there was only one station that was central and the others were in a sort of master-slave kind of situation but now there are all peers so each of them can keep an entire database if it wants to. Therefore each of them can work as a central management network station and they are distributed. (Refer slide time: 50:57 - 51:18 min)

It contains advantages from central and hierarchical architectures, single location for all network information alerts and events, single location to access all management applications, not dependent on a single system, distribution of network management tasks, distribution of network monitoring throughout the network. (Refer slide time: 51:19 - 52:03 min)

We already discussed about the network management platform. Now let us talk about specific network management applications. The goals of specific network management applications are to effectively manage a specific set of devices, avoid functionality overlap with the platform which means the network management platform, integrate with a platform through the API and menu system, and reside on multiple platforms. These are the goals. So, applications do not share information because these two applications may have come from two different vendors. (Refer slide time: 52:04 - 52:09 min)

Some examples are; Cisco works or 3com?s Transcend etc. (Refer slide time: 52:10 - 52:30 min)

Choosing a Network Management System: It is built from two major components: the platform and applications. A practical approach follows these steps: Perform device inventory, prioritize the functional areas of network management, survey network management applications choose the network management platform. (Refer slide time: 52:31 - 54:45 min)

We will just mention about another term which comes quite often. RMON: Remote Monitoring MIB. These have agents and probes. So this is actually used for monitoring MIB remotely and there are specific groups for this RMON, that is statistics group, history group, alarm group, host group, host top N, etc. These are standardized to operate on Ethernet segments so that apart from network management stations you can monitor it from other places also. I think I am going to stop here, as you can see what I have tried is, to give you a broad overview of this network management system and as it is growing it. Network management is something which in today?s world you cannot do meaningful network management without machine support, i.e. you cannot do it manually and those days are actually gone. And in large network the number of events that are happening is really tremendous and it is not also possible to keep a log, the machine of course keeps a log and after a month or so the log has become so big, may be you have to delete it from your machine and then the log starts accumulating at a very high rate because the network traffic is flowing back and forth at a tremendous pace and you have to somehow keep track of those and at the same time keep a balance. That is a major part in designing a network management system, keep a balance among trap and polling, keep a balance about what information you will take and what information you will not take, keep a balance about how much you are going to manage. 

Good day, the topic for today is network security. As the network is becoming ubiquitous, in the sense that it is becoming widely available to people, many people have started giving services through network. These services are crucially dependent on security. For example, you want to do a bank transaction through the network so that anywhere you can log onto the network and do your banking. Now this banking service is of course very sensitive so that if security is breached then the whole thing falls down. Therefore security once again is a very big topic, so let us discuss about the overview of security features in network. 
 So what is network security? What is the purpose of a network? It moves bit from A to B. And we want this movement to take place securely. And what exactly we mean by securely? Of course, the bits from A must reach B but we also want confidentiality in the sense that only A and B see the bits. That means, if there is somebody in between then he cannot find out the content of the communication from A to B because A may be a client and B may be a bank. Therefore this has to be secured. 
Integrity: The message must reach from A to B intact. You have the original document; you use the one way function or a hash function and the hashed result is now encrypted with the secret key the private key of the assigner which gives the digital signature. Now the receiver should get the original document plus the digital signature. Now if you want to ensure that the original document also should not be seen by anybody then this entire thing i.e. the original document plus the digital signature you can encrypt using the public key of the receiver. Therefore what the receiver will do is that the receiver will decrypt it using his own private key so that he gets back this. Now using the public key anybody could have sent this document. Now how do I know that this actually came from whomever it purports to be? This is where the digital signature plays its role. Now the receiver can verify by applying one way hash function to the received document. Therefore he will apply the same hash function to this document so he is supposed to get back whatever was here. Now he will decrypt the signature using the sender?s public key. He has the sender?s public key with him and since it was encrypted with the sender?s private key now he can use the private key of the sender to get back the same message. Now, comparing the two results equality means document is unmodified.  
COMPUTER NETWORKS
Prof. S.Ghosh
Dept. of Computer Science & Engineering
I I T, Kharagpur
Lecture#38: Security
(Refer Start Time: 00.28)
Good day, so the topic for today is network security. As the network is becoming ubiquitous, many people have started giving services through network. These services are crucially dependent on security. For example, if you want to do a bank transaction through the network so that from anywhere you can log on to the network and do your banking. This banking service of course is very sensitive so that if security is breached then the whole thing falls down.  Therefore security once again is a very big topic, so let us discuss about the overview of security features in network. 
 (Refer slide time: 01.31 - 01.32)

So what is network security? What is the purpose of a network? It moves bit from A to B. And we want this movement to take place securely. (Refer slide time: 01.33 - 01.50)

(Refer slide time: 01.51 - 03.32)

And what exactly we mean by securely? Of course, the bits from A must reach B but we also want confidentiality in the sense that only A and B see the bits. That means, if there is somebody in between then he cannot find out the content of the communication from A to B because A may be a client and B may be a bank. Therefore this has to be secured. 
 Integrity: The message must reach from A to B intact which means without any modification on the way. This is again very important for many kinds of services. Of course sitting at B you are getting some message down the network but may be you want to make sure that this is really from A and whether the messages are coming in order. These are integrity issues and then finally the availability that B gets it in time. When B wants something from A it is able to get it. Even if all other lower layers are working perfectly B may not get the service from A or vice versa if network security is breached. So these are the three main issues: Confidentiality, integrity and availability. Let us look into these and how they are handled. (Refer slide time: 03.33 - 04.56)

Network security: There is a lot of work on security and it is a very active research area at present. People have come up with lots of solutions but as solutions come up new problems also get generated as we will see. The confidentiality can be handled through encryption. Instead of sending the message as it is, you encrypt it in some fashion so that even if somebody intercepts this message and gets a copy of that he will not be able to decrypt it. We will see various encryption techniques. Encryption basically is the fundamental way in which we handle confidentiality. Integrity may be handled through what is known as the digital signatures. And retransmission and getting the messages in order is a part of the TCP protocol. Then comes availability which is something to do with the quality of service. (Refer slide time: 04.57 - 05.57)

Security environment: this is closely linked not only with the network nodes but also with the operating systems of the systems. Mostly in the application layer although security has become more important that nowadays there are some network nodes also like routers which can encrypt messages almost at a good speed. But a lot of these security issues are handled near the application layer where the operating systems have a role to play. 
Goals: Someone attempts to subvert the goals, it may be for fun, it may be for commercial gain, it may be with some malicious intent so (Refer slide time : 05.58 - 06.27)

It is the exposure of data. If you just make a matrix of Goals vs. Threats then for goals it is the data confidentiality and for threats it is the exposure of data. For data integrity in goals it is the tampering with data as the threat. For System availability there may be a denial of service kind of an attack by some malicious people or either for fun that serves as a threat to the systems availability. (Refer slide time: 06.27 - 08.24)

Now what kinds of intruders are there? There may be casual prying by non technical users out of curiosity. There may be snooping by insiders often motivated by curiosity or money. As a matter of fact it has been estimated that for most of the serious security breaches some insider is involved in majority of the cases. And when that happens it becomes more difficult to handle it. Determined attempt to make money may not even be an insider; it could be an insider or someone who wants to just make money. Then there is the commercial and military espionage. This of course is a very big business. Military communication of course traditionally has been confidential so they were encrypted. So encryption basically derives its history for the military purpose. And now-a-days in the age of information the information technology is a very core part of the military machine of any country. So, security breach in these cases is of course a very serious issue and since most of the large commercial houses are now networked and as they have networked on one hand of course you deal with your business process in a much more efficient manner but at the same time you expose yourself to this kind of new threats which needs to be handled. (Refer slide time: 08.25 - 10.07)

As I mentioned, for confidentiality cryptography is a time honored technique. Now it has become more and more sophisticated. Now-a-days with the help of computers if you are using the older techniques it is possible to crack those encrypted messages so now people have come up with better encryption algorithms and better encryption techniques. The goal is to keep information away from those who are not supposed to see it.  You can do it by scrambling the data and use a well known algorithm to scramble data. The algorithm usually has two inputs; the data and key. Key is known only to authorized users. Now, relying upon the secrecy of the algorithm is a very bad idea. If you just have an algorithm and if you think that it is a secret algorithm then it becomes very vulnerable because all your communication is going to use that same algorithm so it becomes very difficult to ensure that the algorithm will indeed remain secret. So the modern cryptography depends on the secrecy of the key as its mainstay. Cracking codes is very difficult, with the modern encryption techniques as we will see. (Refer slide time: 10.08 - 12.53)

First let us see some basics of cryptography. Actually we have two algorithms in general. You have two algorithms E and D.  We will assume that E and D are known. This means they are not only known to the sender and the receiver but whoever wants to hack into it or whoever wants to intercept it may also know E and D. Let us assume that E and D are known to everybody. There are two keys ke and kd, these are kept secret. So keeping these keys a secret is a very important part of any crypto system and so how to distribute the keys by themselves is the topic for our discussion, Let us assume that these keys ke and kd, ke is for encrypting the message and kd is used for decrypting the message. In general ke and kd may be different. In some cases they are same but in some cases they are different. For this to be effective the cipher text should be the only information that is available to the world and the plain text is known only to the people with keys. In an ideal world, this is what happens. For example, we have got some plain text here which we want to send in a very secret fashion over the network.  What we do is, first with the encryption key ke and the encryption algorithm E we encrypt this. So, as we encrypt the plain text we get what is known as the cipher text c, so c = E(p, ke) that is the encryption algorithm and then the cipher text is sent. The idea is that, even if somebody intercepts the cipher text since he would not be having ke it will not be possible for him to produce P or it will be very difficult for him to produce P. And on the other side there is a decryption key kd and using the algorithm D and kd we decrypt the cipher text to get the plain text back.. (Refer slide time: 12.54 - 13.05)

Therefore this is the encryption part and this is the decryption part. It should be very hard or impossible to find out the message without knowing the key and it should be very easy and fast to find out the message knowing the key. It is also difficult to find out the message even if you know the key or if it takes long time then it becomes very inefficient. So, in order to handle efficiency this should be fast if the key is known. (refer slide time: 13.23 - 13.58)

Before looking at modern encryption techniques let us look at some classical encryption techniques. There are actually two approaches; one is the substitution techniques. The letters of the message are replaced by other letters or by numbers or symbols. And transposition techniques that performs some sort of permutation on the message letters. These two of course are very basic techniques and they are still used today but in a much more sophisticated fashion. You may have come across (Refer slide time: 13.59 - 15.09)

this kind of encryption in some detective stories, at least in some of the classical detective stories where a message may have been encrypted in this fashion. One of the earliest known uses of the substitution cipher was by Julius Caesar; this is known as Caesar cipher. Suppose the message is ?meet me after the party? and your c = m+3 mod 26 in this particular case then m becomes m n o p, e becomes h so ?meet me after the party? becomes phhw ph diwhu etc which looks absolutely gibberish to anybody. But of course with the help of modern computers trying out various possibilities it is very easy to crack this code. This is of course no longer used so it is just a kind of history. (Refer slide time: 15.10 - 17.44)

The other is, use any permutation of the 26 alphabetic characters. Caesar?s cipher all of them are being displaced by the same amount by three letters. In mono alphabetic cipher this may not be so but you have basically a b c d e f g h etc and it may be any permutation of the same set of letters. Now, for encryption as well as decryption you must have this mapping from a to q b to e c to r and etc. Now under attack u becomes c, n becomes w, d becomes y so c w y u l q etc becomes something like this. You may have noted that we have not considered the blank over here and all of them and that is why it has come as just one string. If you get the reverse mapping then using a dictionary breaking up with these blanks is not difficult. This is mono alphabetic cipher. Once again it is not difficult to break this, for example, if you have done something like this and if you have got a fairly long text there is a statistical table which is available on the internet also about the frequency of occurrences of different letters, for example, e, a, etc, there are some letters like this which occur many times more than letters like j, q, etc so this is available. So using this kind of a table you may even have a mono alphabetic cipher like this. Using this distribution it is quite possible especially with the help of computers, in those detective stories it was done by detectives but now-a-days a computer will do it. It is possible to guess some of the letters and as you guess more and more it becomes faster and finally you crack the message. This is also not such a great way of encrypting things. Once again we will look at this (Refer slide time: 17.45 - 18.33) 

 for historical purpose. The other approach to encryption is transposition. It is performing a sort of permutation on the message letters.  For example, if the message says ?meet me after the yoga party?, then let us say the algorithm is simply this M e then et M e e t, the point is that alternately you write them in one line or in the other line and then you read the top line first M e M a t r etc and then followed by the bottom line e t e f etc. Once again this is quite easy to decipher. (Refer slide time: 18.34 - 19.23)

 Apart from this there are certain practical problems which you have to consider. One is, generating a fully random key is practically very hard, sometimes impossible. And more over to ensure the security of the system the key size should not be less than the message size. And sending a not repeated key in the same size of the message through a secured channel to the receiver is impossible, things like how to distribute this key amongst the potential senders and receivers needs to be understood. The previous one?s possibly if we have a very long key with a length which is comparable to the length of the message then you can have more and more secured communication but having such long key which is almost equal to the size of the message then distributing is a problem. (Refer slide time: 19.43 - 20.34)

Today let us see what we mean by computational security. An encryption scheme is secure if it takes very long time to break the cipher text. If you are using brute force may be you can break all text but then breaking the cipher text takes a very long time then it becomes infeasible. Lifetime is defined in each application. For example, military orders may be one hour to three years, check transaction may be it is for only one year, business agreement 10 to 15 years. So, if decrypting a cipher text takes longer than this kind of time then may be you are doing fine. (Refer slide time : 20.35 - 21.00)

So the bottom line is that if somebody does not know anything about the key it must take more than several years to decrypt a message. With enough number of substitution and transposition modules we can make a strong encryption scheme and this is an algorithm (Refer slide time: 21.01 - 22.11)

called DES or Data Encryption Standard. The basic DES module uses a 56 bit keys. The same key is used to encrypt and decrypt. Keys used to be difficult to guess, needed to try 255 on average, so modern computers can try millions of keys per second with special hardware. Actually you can make a very special machine which can break DES. But it is not easy as you can see that you have to make a very special hardware for trying out millions or even billions keys per second and then it is possible to break this and put a number of machines working in parallel etc. It is possible to break it in some time frame but even then DES is a very good encryption technique and the modern and much more difficult encryption techniques are based on DES. (Refer slide time: 22.12 - 22.56)

Let us just take a quick look on DES. the current algorithm is Advanced Encryption Standard AES which uses 128 bit keys. Adding one bit to the key makes it twice as hard to guess so must try 2127 keys on an average to find the right one. At 1015 keys per second this would require 1021 seconds or may be thousand billion years. This of course is very good and modern encryption technique. It is not usually broken by brute force so what people try to do is they try to get more information. We will not get into attack crypt analysis but discuss just a part of it because we do not have time. (Refer slide time: 22.57 - 23.44)

This is a basic DES scheme suppose, the input is 2w bits, you divide it into two parts, these w bits you do an xor with the output of this function and this function takes other w bits and a key for this round and these two feed into a non linear function which produces an output. And this output is xor with the first w bits. Now this xor is transposed to the second half and this part comes to the first half. So this is one round of DES. (Refer slide time: 23.45 - 24.29)

Actually DES goes in various rounds so for each round there are keys. Actually this is generated from 156 bit am not going into the details and each time you go through this kind of operation with that particular round key from k1 to k16 in 16rounds then in 16 rounds you get the final  encrypted text. The decryption also uses the same set of keys only in the reverse order and if you do that you will finally get the plain text. So the original key size is 56 bits from which all these keys are generated. (Refer slide time: 24.30 - 25.13)

Now-a-days as I mentioned, since DES can be broken sometimes these days people use triple DES. Triple DES is a modern encryption standard which is very difficult to break so you require three keys or sometimes only two keys are used; ka, kb and ka once again. So you take the message, make it go through the first DES block ka then this encrypted message you encrypt again using kb and this encrypted message you encrypt again using ka and this gives you the cipher text. So like AES this is also very difficult to break. (Refer slide time: 25.14 - 27.42) 

Another very important kind of cryptographic technique is the public key cryptography, this is asymmetric key. Till now we have been talking about the symmetric key cryptography. In symmetric key for the encryption and decryption you use the same key. so if you use the same key, if you are doing the same something like a mono alphabetic substitution that table is available to both the parties, it is the same table so that is the same key basically. Similarly in DES or triple DES you use the same key or sets of keys in both the cases for both encryption purposes as well as for decryption purpose. Now, in the public key cryptography, this is another class of algorithms where the key that is used for encryption is not the key which is used for decryption. And knowing the key for decryption it is impossible to guess the key which was used for encryption. So this is the so called asymmetric key or public key cryptography. It uses two keys; one is known as the public key mainly for encryption and private key which is for decryption. Now these keys come in pairs.  Suppose A wants to send some message to B, now what will be available to A will be the public key of B. So what it is going to do is that it will encrypt it or and of course it will have his own private key. So you can do it either way. You can encrypt using your own private key and then send it. But then that can be decrypted with your public key. If somebody else sends it using your public key then I can decipher it using my private key. And since my private key is not known to anybody then nobody else can decipher it, and this is the basic idea. Another kind of function which is used is a trap door or the one way function. 
 (Refer slide time: 27.43 - 29.17)

Let us look at public key cryptography quickly. 
Public-key cryptography: instead of using a single shared secret, keys come in pairs. One key of each pair distributed widely which is called the public key and one key of each pair kept secret, i.e. private or secret. Two keys are inverses of one another but not identical. Encryption and decryption are the same algorithm. If you encrypt a message M using the secret key then using the public key if you run the same algorithm then you will get the deciphered message M. Similarly if you encrypt using the public key and then decipher it that means run the same algorithm again with the secret key then once again you get M. So, currently the most popular method involves primes and exponentiation. This basically is based on the fact that this is difficult to crack the encryption unless large numbers can be factored easily and there is no known method for factoring very large numbers. Large number means numbers or integers with large number of places. The difficultly with public key cryptography is that it is very slow for large messages. (Refer slide time: 29.18 - 30.00)

 Trap door one-way functions: It is computationally impossible to find out what are K and M when knowing the Ek (M). So knowing M you can of course use the trap door function with this ke to give Eke(M). Therefore you can go in only one direction but from this side it is impossible to come on the other side. This is also sometimes used for authentication in digital signature as we will see. (Refer slide time: 30.01 - 30.41)

 So one way functions are functions which given a formula for f(x) it is easy to evaluate y = f(x). Given y computationally infeasible to find any x such that y = f(x). Often operate similar to encryption algorithms; it produces fixed length output rather than variable length output. It is similar to xor in blocks of cipher text together. Common algorithms include MD5, so 128 bit result or SHA 1 which gives you a 160 bit result from a text. (Refer slide time: 30.42 - 32.11)

 Now let us see one important application of all these; public key and one-way functions etc which is in digital signatures. A hand written signature is a function of the signer only and not the message. That means suppose I sign some document I can sign any number of documents, my signature will remain the same and it is very characteristic of my way of signing, my handwriting etc so it is difficult for ordinary people to replicate that although there are good forgers who can replicate many signatures. Now, the digital equivalent of handwritten signatures would be useless in ecommerce. So we must be able to compare it with the real signature and must be sure it is not copied or forged. Now how can A prove his identity over the internet that is the basic idea? When I sign a document I am basically saying that it is me alone who has sort of written this so that is why I signed the document or agreed to do this. Now how can I do this over a network? (Refer slide time: 32.12 - 33.09)

This is the basic scheme of a digital signature. Digital signature is a function of both the signer and the message. A digital signature is a digest of the message encrypted with the signer?s private key. So, we have a message M and it may be a very long message so use some secure hash algorithm to produce hash that is the message digest. That means you make it run through some hashing function which produces an output of its length which is a product of this particular message.  Now you encrypt this hash using the signer?s private key and this produces the digital signature. Now this is the digital signature of Mr. A on message M. (Refer slide time: 33.10 - 35.16)

You have the original document, you use a one-way function or a hash function and the has result is encrypted with a secret key or the private key of the signer and this gives the digital signature, Now the receiver should get the original document plus the digital signature. Of course, if you want to ensure that the original document also should not be seen by anybody then this entire thing that is the original document plus the digital signature you can now encrypt using the public key of the receiver. What the receiver will do is that the receiver will decrypt it using his own private key so that he gets back this. Since public key may be public so anybody could have sent this document, how do I know that this actually came from whomever it purports to be and this is where the digital signature part come in. So the receiver can verify by applying one way hash function to the received document so he will apply the same hash function to this document therefore he is supposed to get back whatever was here. Now he will decrypt the signature using the sender?s public key. He has the sender?s public key with him and since it was encrypted with the sender?s private key now I can use the private key of the sender to get back the same message. So comparing the two results equality means document is unmodified because this digital signature could not have been produced by somebody else because this is a product of this document as well as the secret key of the sender and it is visually known only to him. (Refer slide time: 35.17 - 35.25)

Therefore if the two hashes are equal then the message is authentic. (Refer slide time: 35.26 - 36.42)

This does not solve the entire problem because sometimes you require some identity documents. What is an identity document? Identity documents are passport, birth certificate, driver?s license, etc. This is basically a piece of paper issued by a trusted third party with information verifying the identity of the holder. Now, this has to do with the following: For example, even if you have a digital signature, the digital signature will only tell you that it will verify that this message has been sent by somebody who has this particular secret key. That means secret key corresponding to the public key that the receiver is holding. Now how do you absolutely guarantee that the secret key is held by the right person? That is where the identification comes in. So, a challenge is a protocol for holder to prove he is the person named in the document. In the non electronic world we do it with photograph, signature, fingerprint, etc. (Refer slide time: 36.43 - 37.15)

 A digital certificate is a digital identity document binding a public private key pair to a specific person or organization. Verifying a digital signature only proves that the signer had the private key corresponding to the public key used to decrypt the signature. It does not prove that the public private key pair belonged to the claimed individual. We need an independent third party to verify the person?s identity through non-electronic means and issue a digital certificate. (Refer slide time: 37.16 - 38.54)

 And a digital certificate contains all these things; the serial number, name of the holder etc. This has the public key of the holder, name of the trusted third party, then the digital signature of a certificate authority. Data on which hash and public key algorithms have been used and other business or personal information. The point is that, the digital signature of the certificate authority since the certificate authority is the third party who is trusted. This means, if his digital signature is there, because since the public key of certificate authority is known then his digital signature can be verified. And in this particular case this authority is trusted. That means we know that something bearing the digital signature is indeed coming from that physical trusted authority. And now this trusted authority is basically certifying that the name of the holder is the holder of this particular public key. Of course the holder has his own private key. This private key would not be known to anybody else but then he is the holder of this public key. (Refer slide time: 38.55 - 39.29) 

We have a version of certificate standard. So these digital certificates can be checked by a machine that is the whole idea. You have a hash algorithm and then the message digest, issuer?s private key and then put the signature of the issuer. The subject?s public key is what is being certified. So, it is this algorithm plus the public key value. Then there is period of validity for this. (Refer slide time: 39.30 - 40.26)

This public key cryptography is quite an elegant system. The only trouble is that, this is rather a computationally expensive process, it is quite slow. Pretty good privacy: It uses both public and private key to chart a middle course. It uses public key encryption to facilitate key distribution. It allows messages to be sent encrypted to a person that is encrypting with persons public key. It allows person to send message that must have come from her. That is, encrypt with persons private key.  (Refer slide time: 40.27 - 41.31)

The problem is, public key encryption is very slow. The solution is to use public key encryption to exchange a shared key. A shared key is relatively short which is about 128 bits or so. Therefore message is encrypted using the symmetric key encryption. Now what you do is that in the first phase you use the public key cryptography system to exchange just the shared key. This key may be shared just for this very session and then it may go. This is shared for particular session. Since this is 128 bit you can do it once even though it is expensive or this is a bit inefficient. And then, once both the parties are sharing the same key, this is the symmetric key, so this works much faster and the message is encrypted using symmetric key encryption.  PGP can also be used to authenticate sender, use digital signature and send message as plaintext. There are various ways in which PGP can be used. (Refer slide time: 41.32 - 43.32)

With this we come to the end of our discussion about some security issues which are very general and with which most of the service organizations are very much concerned. Security is the foundation on which all these so called e activities dependant upon. Now we will focus more on the network technology part of it that what we mean by the security of a network. This is security of messages in the network and now I am talking about security of the network itself. Some terms are quite commonly used here and one such device commonly used is the firewall. It solves poor internal security measures using the network. Now all these have to do with securing the functioning of the network. The other is an intrusion detection system. Sometimes an intrusion detection or intrusion prevention system is integrated with a firewall and sometimes they are two different boxes. So intrusion detection is that, I want to detect whether any unwanted or unauthorized person has some how intruded into my secured zone and once he intrudes into my zone he will have access to data and messages from my network which I do not want. It detects non network security breaches accomplished via the network. Therefore firewalls and intrusion detection, stock prevention are all very important and most of the network vendors have got separate boxes for doing this. (Refer slide time: 43.33 - 43.58)

Therefore here let us see how they are being used. By the way there is also a question of authentication of users and what is used. Sometimes you may use distributed authentication, a centralized authentication, LDAP or AAA server for that authentication and you can do a distributed co-operation also. (Refer slide time: 43.59 - 48.23)

 This is a typical corporate network. A typical corporate network is meant to be very secure. We start from the left hand corner. Suppose these are the user machines and then we have an internal domain name server or internal mail server, internal web server, internal file server, etc. This is the private network of the corporation, i.e. the intranet of the corporation. This term intranet really means that we use the same technology which we use in internet and which you use in your internal network also. So you are using things like web browser, DNS, mail server, etc which work in the internet, the technology being deployed only for private purpose. So, this left hand part is only for the private purpose and only these authentic users should be able to use this intranet. This is connected to the internet as you can see here on the right hand corner. Although this is connected to the internet other people will generally not have access to this part. So what we do is, we try to make this internal network secure using a firewall and this is an internal firewall. And then we have another machines also belonging to the same corporation like this web server which is used for helping people by providing servicing in the internet, we have mail forwarding and DNS etc. These are put in a zone which is known as demilitarized zone. Then this connects to the external firewall and then the external firewall finally connects to the internet. Actually what happens is that, what a firewall will do is that, a firewall basically inspects the packets and decides on whether these packets are ok and secured or not. It follows a set of rules.. Depending on how you configure it is possible to have different levels of security at various ports of the firewall. A firewall may have 2 3 4 5 ports.   Suppose it has got 4 ports, it is possible to port-wise configure the security level of each of the port. Now what you want to do is that, you want to make your internal network absolutely secure. So somebody who wants to access through this firewall into something which is staying in the highly secured zone, that is absolutely the internal code network then he has to go through lot of checks, so that is a high security zone. But again what may happen is that you also have some machine like web server which you want to be open to the public. Similarly your DNS or your mail machine etc have to have a public dealing, a public face. So we want to keep some kind of medium level security for these kinds of machines. So they are put in a network area which is known as the demilitarized zone DMZ and then a firewall may connect directly to the internet also so that leg of the firewall would be of very low security termed as the low security zone. This may be one way of using two firewalls. Sometimes people will use only one firewall but this is a deployment using two firewalls such as one internal firewall and one external firewall. (Refer slide time: 48.24 - 48.59)

Network regions are these internet that is not secure at all, intranet which is highly secure and DMZ which is the middle level security. And the network boundaries for all these different regions are at the firewall. So there may be filtering firewall based on packet headers, there may be audit mechanisms. A proxy may also be used as some kind of a security device. (Refer slide time: 49.00 - 51.42)

For example, you can use IP addresses for some low level of security. The network address translation came up as the IPv4 addresses have virtually disappeared and only some class C addresses are available. So, in order to obviate that what we did was that in a internal network we used some private IP addresses and when we use a private IP address in my network, and in order to communicate we have to go through a proxy and the proxy will remember that a machine with this private address is wanting to communicate with the outside world, so it puts a valid IP address from a pool in place of the private address which is not really valid outside and then sends out the request gets the message and then sends it to this private IP. It is conventional to number the private IP addresses starting with 10 which is 10.x.x.x so that is one full class A address which is a huge address space for one organization. And the point is, the routers outside, are usually not going to route any address which starts with 10. because it knows that 10. is used for private IP. So if you use private IP addresses it is difficult for outsiders from the internet to log on to your machine unless he/she is going through some proxy and getting into your network in the first place. Therefore network address translation protocol maps internal to assigned address. 
Mail forwarding: Hide internal addresses, map incoming mail to real server, and additional incoming and outgoing checks may be performed. Therefore all these are different things you can do to enhance the security of your network. It is not possible to make your network hundred percent safe but you can increase the level of safety through various measures, this is one kind of such measure apart from any firewall that you might have put. (Refer slide time: 51.43 - 53.24)

For firewall configuration, in the external firewall, what you can configure is, find out what traffic is allowed. External source: You may put IP restriction so external source may only be from this or from these IP addresses, we will specify that. What type of traffic: Ports for example SMTP for mail or http for web etc, what kind of traffic I will allow and what kind of traffic I will not allow. May be I will not allow telnet in the secured zone at all from the outside. 
Proxy between DMZ servers and internet and proxy between inner and outer firewall are the things which you can configure in an external firewall. the network has become such a burning issue these days, so more and more development is taking place in this region and especially in firewall design etc so in modern firewall it not only works very fast but it can look at lots of things. It can counter peer to peer messages and all kind of things it can handle.. For internal firewall you can put traffic restrictions on ports from or to IP address and you can proxy between intranet and outside. These are the things which do with the firewall.  (Refer slide time: 53.25 - 54.00)

For DMZ administration whether a direct console access is required? If a direct console access is required then naturally this is another place, this is somewhat troublesome. Or you can use special access using SSH Secured Shell Connections allowed from internal to DMZ administration connections or only from some specific internal IPs or only through internal firewall etc you can administer the DMZ. This reduces the security risk to only one or a few machines. (Refer slide time: 54.01 - 54.23)

You can authenticate in various ways. One question always comes up is that whether your authentication will scale? You can repeat the authentication, have multiple administration, but if you have distributed authentication scheme then that is always good. (Refer slide time: 54.25 - 54.26)

(Refer slide time: 54.27 - 54.40)

Let us look at some attacks and defenses. One kind of attack which is common is denial of service attack, routing attack, spoofing attack and may be there are other kinds of attacks people are thinking about. (Refer slide time: 54.41 - 55.10)

As people are thinking in a more secured way, the hackers and all other peoples are also thinking of many ways of compromising security. Confidentiality on the network is manageable such as encryption to protect transmission, public key cryptography, key management, etc. Integrity is reducible to single system: digital signatures verify source and commit protocols handle network failure. What about availability? This is where the attacks on the network come into picture. (Refer slide time: 55.11 - 55.40)

 One is flooding: this is a denial of service kind of attack. That means overwhelm TCP stack on target machine which prevents legitimate connections. Routing attack is misdirecting traffic. Spoofing: when somebody is entering your network claiming to be a false identity then spoofing imitates a legitimate source. (Refer slide time: 55.41 - 56.02)

What is a flood attack? Limit the availability by overwhelming service by following service?s protocol to an extent. Example is a SYN flood; it overwhelms the TCP stack or may be a large number of emails being generated by a script. (Refer slide time: 56.03 - 56.39)

Let us look at SYN flood. In TCP protocol when a client initiates a SYN then it waits and then the server will acknowledge it and send a sequence number. The server will also send the SYN and then it will wait for an acknowledgement from the client. This is a multi-step process. Sequence numbers are incremented for a future message. It ensures message order and retransmits if lost. (Refer slide time: 56.40 - 57.35)

What some malicious person might do is that receive the SYN, the server receives the SYN, allocates connection, acknowledges and waits for response from the other side, waits for this response. So what this is doing is that it is sending one SYN after the other in rapid succession. So, on the server side you will be opening so many connections and you will send acknowledgements and wait. Therefore what will happen is that the entire space for connections becomes allocated. And this is of course is done by a malicious user but a legitimate user can no longer get any access to this server because the server has become absolutely overwhelmed. Hence this is a denial of service. (Refer slide time: 57.36 - 58.04)

So you can limit connections from one source, ignore connections from illegitimate sources, drop oldest connection attempts etc. What the firewall will do is, the intrusion detection system will also try to detect that and block all the flood of SYNs etc which is coming from one source. But that does not solve the distributed denial of service attack which is coming from many sources. (Refer slide time: 58.05 - 58.29)

Network solutions: TCP intercept: Router establishes connection to client, when connected establishes with server. So router comes in between server and the client. 
SYNkill: Monitor machine as a firewall, good addresses: allows history of successful connections otherwise kills it. These are the intrusion prevention kind of situation. You can try to encrypt your SYN (Refer slide time: 58.40 - 58.47)

or make changes in protocol also but that is usually more difficult and cumbersome to implement unless absolutely it is your own and private network. (Refer slide time: 58.48 - 59.12)

 Service-Level Flooding: Overload the server: Looks into its processing, storage, etc. Typically garbage requests using legitimate protocol: Large emails to victim, many http connections, Heavy use of scripts. These overwhelm the server in some other way so that it cannot give legitimate service. (Refer slide time: 59.13 - 59.15)

(Refer slide time: 59.16 - 59.18)


IP Spoofing: What somebody can do is that, instead of overwhelming server he can overwhelm the client and then take on the role of the client. So, there are various ways in which these network attacks are coming and there are ways to handle this. This is a developing and important field. We have just given a overview today, thank you.
Computer Networks
Prof.S.Ghosh
Dept of Computer Science and Engineering
I.I.T kharagpur
Lecture # 39
FTP-SMTP
Good day, so today we are going to take up two important application layer protocols namely FTP and SMTP. So, first (Refer slide time: 01:00 - 01:01)

let us look at FTP.  (Refer slide time: 01:07 - 01:33)

This is called File Transfer Protocol and it is a very widely used protocol for reliable transfer of files. It uses TCP as transport for reliability, it uses out of band control and this is stateful. So we will go into these one by one. First thing is that, we want this file transfer to be very reliable, in the sense that even missing one bit in between may actually corrupt the entire file. So we want a good degree of reliability and as we have seen in our discussion on TCP that one way to achieve this reliability is to use TCP. You use FTP for downloading files, now, as soon as you start an FTP to download a file it should start immediately unlike mail which may be delivered after sometime i.e. 10 or 15 minutes or so. That is one requirement of FTP. The other requirement is that the speed is not constant. The speed should remain constant but that is not a crucial requirement for FTP. A file may be transferred fast in the beginning, then if the network gets congested it may slowdown but then that is alright, in all these respects TCP is a very ideal transport protocol for running FTP. Let us look into the (Refer slide time: 02:56 - 04:35) 

general scheme. First of all, suppose this is an FTP client trying to contact an FTP server there is a TCP control connection at port 21. On the above we have mentioned the control connection and below we have mentioned a data connection. These two connections are different, that is why it says that its control is out of band. Actually, out of band control means, whenever two nodes are communicating this is a general term that if there is some control information going between the two, for example, in a telephone the connection setup there is control information, ringing etc there is a control information and when this control information and the actual data occupy two distinct channels then it is called out of band communication. That means the control is out of band. For control connection TCP uses different port and for data connection it uses a different port. One is a port number 21, the other is a port number 20 and these are basically well known ports. The request comes from the client through this port number 21. So the server opens a port number 20 with the client and starts communicating. (Refer slide time: 04:35 - 06:37)

Now, FTP client contacts FTP server at port 21 specifying TCP as transport protocol. Client obtains authorization over control connection. Now this is important, when you want to download a file it is important to check whether whoever wants to download has a proper authorization or not because not all files and all information should be available to everybody in the world. There are questions of privacy and security etc so that is why some authorization is necessary. Of course in this case what FTP does is, the authorization is rather at a rudimentary level in the sense that it is through users and passwords only but anyway that much of authorization is there and this authorization process goes through the control channel. Therefore client obtains authorization over control connection. A client browses remote directory by sending commands over control connection which means that, if the authorization phase goes through without any trouble then the current directory at the server end can be seen by the client from the other end by giving some commands. Now, in raw FTP, the commands are a little bit cryptic whereas there are FTP clients with good graphical user interface where you can actually graphically see the directory etc, this is the remote directory we are talking about it. Therefore he can look into the remote directory and select whatever file he wants to download. (Refer slide time: 06:38 - 07:27)

When server receives a command for a file transfer the server opens a TCP data connection to client and this is again done on a well known port 20 on the client so a new data connection is setup. After transferring one file server closes the connection. Now, what might happen is, a person may want to transfer a number of files from the server, so if again there is a request for another connection the server will open another connection again on the same port and the second TCP data connection is setup to transfer another file, so this is how it goes. (Refer slide time: 07:27 - 08:41) 

So control connection is out of band and FTP server maintains state. And what is the meaning of state here? It is the current directory and earlier authentication. That means that once a client connects to a server and gives his authorization then for that entire session till such time he does not log out that authorization is kept valid. That means he can go on accessing whatever is available on the other side, then the current directory  means the directory which he is browsing at the moment, the directory from which he is downloading the present file etc. The current directory for this particular client is also maintained at the FTP server. Therefore it is called a stateful protocol in the sense that it maintains the state. Even if one file transfer is over and it closes but the authorization in the current directory remains, hence this is a stateful protocol. (Refer slide time: 08:42 - 09:57) 

FTP Sample Commands: These commands are sent as ASCII text over a control channel. This is a simple ASCII text going over the control channel. It expects the user?s username and the password. These are commands you can give and these commands go to from the client to the server through the control channel. So, you can do a return list of file in current directory. This is the basic FTP command. You may also have the user interface which shows it very attractively in a graphical fashion. RETR is a file name, it retrieves or gets a file, you can use it to get and retrieve a filename. STOR filename stores or puts file onto the remote host. You can both get a file from the remote host as well as you can upload a file on the host and for doing that you have to give the particular filename. These are some sample commands for FTP which you can use. (Refer slide time: 09:58 - 11:07) 

And then once again we are talking about the control channel only. The FTP sample responses give a status code and phase as in HTTP. These are basically codes 331 user name OK, password required. When you have just entered the user name and if that user name is in his list he will sent an ok for the user name but then you still have to give the password. And 125 data connection already open transfer starting, so, in the control channel these are the kinds of responses from the server side that may come. And 425 cannot open the data connection, 452 error writing file. So, all these errors and other kinds of responses come through the control channel. These are some of the FTP sample responses. (Refer slide time: 11:08 - 13:23)

 And that is all about FTP as far as it is concerned. It is a very simple protocol but at the same time it is an extremely important protocol. People always want to download files and these files could be of any kind, there is no restriction on the kind of file. If it is a binary file you have to set the mode that way. But the file could be a text file, it could be a binary file, it could be images, multimedia or anything. If it is a binary file you have to set a mode.. And the next important application we are going to discuss today is SMTP which is at the route of all the emails or the so called electronic mails. Electronic mail has been a very important application of the network. As a matter of fact these two things taken together that one is the web which is the HTTP protocol which we are going to discuss in the next lecture. HTTP and email i.e. web and email have been the killer applications so as to say for computer networks which made computer network immensely popular, contributed a lot to the growth of a network in general. As you grow you get all the economies of scale so naturally money is invested and a lot of money comes in which sparse further growth and further innovation etc. HTTP and SMTP have been the so called killer apps of computer network. Let us now look at the details of SMTP. SMTP stands for (Refer slide time: 13:24 - 14:17)

Simple Mail Transfer Protocol. It is a simple protocol. Like FTP it is quite simple. But for sending mails etc there are some more intricacies which we will see. This is one of the most popular network services, email, is supported by TCP IP protocol SMTP. System for sending message to other computer users and provides a mail exchange between users. Actually, many of you must have used emails and you know that you can send a mail to a number of people and there are lots of advantages of this email. The chief advantage is being the speed at which your mail reaches the other end. Actually once email came into vogue the classical mail which goes into the post office have been renamed as snail mail that it goes at a snail space whereas email goes extremely fast. It may not be instantaneous depending on how your mail servers etc are configured but it goes very fast and it may be almost instantaneous. So, in a few minutes the mail reaches the other side. And the other thing is that it is so cheap and you can attach any kind file to it. Therefore all these make the email very attractive and the volume of email has grown tremendously dwarfing the number of ordinary mails that had been going on before that by orders of magnitude. (Refer slide time: 15:15 - 15:19)

So that is what SMTP (Refer slide time: 15:20 - 17:08)

does. It supports sending a single message to one or more recipients. Sending messages include text, voice, video or graphics. Actually as the SMTP was originally envisaged it can carry only text messages but then people found a way around for encoding other kinds of a data because as soon as text messages become per say people wanted to send pictures, voice, video and all kinds of graphics etc through the email so there was a way of encoding it. So we can send messages to users on networks outside the internet which is also possible. Actually this is becoming less important by the day because what is happening is that internet is swamping all other types of network but when SMTP came into existence not only the TCP IP networks but other types of networks were also in vogue and so a way had to be found to exchange mails to users who were on different networks. It was in the heterogeneous kind of network and that has actually introduced a lot of complexity into the SMTP gateways. This is also possible although this is becoming less important now because everybody seems to almost switch over to this internet and TCP IP protocol. (Refer slide time: 17:09 - 17:59)

An electronic mail system has three major components: One is the user agents. Here are some examples of user agents like Eudora, Outlook, Pine, Netscape messenger. So all kinds of user agents are there which provides the user with a graphical interface and it makes it sort of simpler even for late people to use this mail system. And then there are mail servers which handles the incoming and outgoing messages and they use the simple mail transfer protocol between the mail transfer to reach the mail from one end of the network to another. (Refer slide time: 18:00 - 19:21) 

This is just a diagram. We have these user agents who are connected to mail servers. The mail server maintains an outgoing message queue so any of the user messages may keep on sending mails so they are in the outgoing message queue. Similarly, whatever incoming messages are coming they also come into this mail server and by looking at the users address they go into the individual users mail boxes. So, if these yellow boxes represent the users mail boxes then the user agent will connect to these mail boxes. Mail boxes are basically directories where all your mails are kept and it can access its own mail i.e. the mails which are incoming so they go into these mail boxes and mails which are outgoing they go into a queue and from this the mail server sends it to different other mail servers and these different mail servers talk to each other via this SMTP protocol so it may reach another mail server to the mail box to a particular user agent. That is how the system works. (Refer slide time: 19:22 - 21:11) 

One important component of the system is the user agent. It prepares the message and creates an envelope. This envelope is nothing but, the term obviously has been brought in from classical mail just as you write a letter and then put it in an envelope similarly you put your message into an envelope which has got a certain format. It is the job of the user agent to create the message, the user has to type in the message or upload it from somewhere and then it is put in the envelope in the proper format. This would be the job of the user agent. Then he puts the message in the envelope and sends it over to the mail transfer agent, which is one job of user agent. The other job is that, for the incoming mails it has to show the incoming mails and then depending on what kind of user agent you have you may be able to create directories for classifying the mails. You may have mails from friends in some directory and mails for business purposes in other directory and some user agents also allow you to define rules so that whenever an incoming mail has come to the user agent it will automatically examine the mail and put it in its proper directory. All these are done by the user agent and then there is this mail transfer agent. So from the user agent the envelope goes to the mail transfer agent who transfers the mail across the internet. (Refer slide time: 21:12 - 22:04)

You have these different machines who uses the user agent to create the envelope which are then sent to the message transfer agent or mail transfer agent. Now it travels through the internet, for the time being let us consider only internet as a homogenous kind of a networking system and then the envelope is delivered to the proper user agent and then the user accesses his mail through the user agent into his own machine. So this is the overall email architecture. We will look at another overall architecture a little later. (Refer slide time: 22:05 - 22:32)

There is some relay MTAs. So mail may be relayed through a number of MTAs, allows systems not using TCP IP to send email to users on other sites, is accomplished through a mail gateway. A relay MTA that can receive and send a mail prepared by a protocol other than SMTP. So this relay MTAs are very important. First of all, by looking at the address remotely it may not be possible to disambiguate the address of the final mail server who is going to give directly to the user agent, it may not be possible to disambiguate it. So maybe there are mail relays before that are bigger entities who are known and the mail can be sent to him and he will send it to the proper mail server who will directly give it to the user agent, that is one job of mail relay. The other job of mail relay is that, suppose it is possible to transfer mails between heterogeneous networks, so you have one kind of network here and another kind of network here, so in between there has to be a mail gateway. This mail gateway is also some kind of relay MTA so the message will first come to this relay MTA who will then actually store it and then the format etc of the envelope and other things may not be the same in the two networks. So he will make those proper changes and then send it over to the second type of network. This is the job of a mail gateway. (Refer slide time: 24:08 - 24:47)

Here is an example, suppose a user agent has sent a mail through its own MTA via the internet, now this MTA is actually working as a mail gateway and this mail gateway then can push it to another kind of network which is a private network to the corresponding mail system. So this way it makes it possible for user A and user B to communicate, exchange mails although they are on different kinds of networks. This is becoming less important by the day because other kind of network is giving way to TCP IP networks. (Refer slide time: 24:48 - 26:47)

Now, for delivering mails of course you need the addresses so there is a unique addressing system. It consists of two parts; one is the local part followed by ACK followed by a domain name. You must have come across mail addresses and seen ACK something dot something dot something. So this is what it looks like. Now the part before the ACK is the local part and this local part is supposed to specify the mail box within that local network where this mail is finally to be deposited. Suppose that is the address of the person to whom you are sending the mail, the destination of that particular mail box to which this mail is to be deposited is given by the local part, whereas the domain name is what allows him to send a mail to the proper destination. By looking at the domain name he knows the destination so he will send it to the corresponding mail server in that domain. Sometimes this may not be done in a single hop so it may go through a few mail relays.. So this is the address part. Once the domain name reaches its destination then by looking at the local part finding out the address of the mail box is easy. (Refer slide time: 26:48 - 26:59) 

The local part defines the user mailbox where mail is stored for the user and domain name is the name of the host used as the mail exchanger. .(Refer slide time: 27:00 - 28:28)


Unlike FTP or HTTP for mails messages do not necessarily have to be delivered immediately. It can be delayed at the sender site or receiver site or intermediate servers. During this time what happens to the mail is that it is waiting in some queue and the mail servers may be configured to pole the different machines connected to it for outgoing mail from time to time. That is the time when it delivers the mail so it sends and receives messages during that time and that time may be set, that time may be a few minutes or may be more than that. And then it goes and waits in the outgoing message queue before it is processed etc. Therefore the delivery is not instantaneous it may be delayed. But usually nowadays mail is so important so people put powerful machines for doing these jobs and the mail goes really fast compared to snail mail but then compared to other protocols like FTTP or HTTP this SMTP, the messages may have to wait in some queues in some servers either on the sending site or receiving site or intermediate servers. (Refer slide time: 28:29 - 29:34) 

Aliases mean different names for the same person. SMTP allows one name an alias to represent several email addresses. This is one-to-many alias expansion. A single user can be defined by several email addresses so this is many-to-one email expansion, so both are possible. For example, you know that it is possible to send mails to a group of people. Suppose you regularly communicate to a group and when you send a message you want that message to reach to every member of the group, something like multicasting, so what you can do is that, you can define the group giving the email addresses of all the members in the group and then when you actually compose the mail you can simply send it to the group so the machine will automatically send one copy to each member of the group. That is one-to-many alias expansion. Similarly many-to-one is also possible. (Refer slide time: 29:35 - 30:11) 

this is a general kind of picture, you have the user who goes through the user interface to the user agent which puts it in this spoon for the outgoing mails and it collects the mails from the mail boxes from the corresponding mail boxes. User A will have one mail box here and then there will be alias expansion. Then it will go through the MT?s through the internet to other MT?s to reach its destination. (Refer slide time: 30:13 ? 30:57)

Now, looking at the mail message format, this is the SMTP protocol for exchanging email messages and RFC 822 gives the standard for text message format. The header lines contain two things namely to whom it is addressed from, i.e. where the mail is coming from and then there is a subject line. So these are members of the header. Then there is one single blank line and then there is the body that is the message and the message as you know is the ASCII characters only. (Refer slide time: 30:59 - 31:12)


Between the MTA client and MTA servers SMTP uses commands and responses to transfer mail between an MTP client and MTP server. Therefore this is the command and response mode like other (Refer slide time: 31:14 - 31:29)


application protocols. Mail transfer has the following phases: The process of transferring a mail message occurs in three phases: connection establishment, mail transfer, connection termination. (Refer slide time: 31:30 - 32:12)

So, for connection establishment client makes a TCP connection to the well known port 25. The well known port 25 is used for SMTP. The SMTP server starts the connection phase. So the server will reply with the 220 service ready. As soon as MTA client sends a request to this port 25 if the sever is ready or 220 service ready will come back automatically then there will be a hello message from client to the server side and server side will respond with a 250 OK. These are the commands and responses which are going on. We will (Refer slide time: 32:13 - 32:33)

look into more details of an actual SMTP session later. When the connection is ready a single message between the sender and one or more recipient can be exchanged, this is the message transfer phase. (Refer slide time: 32:34 - 32:42)

And finally there is a connection termination when the client sends the quit and the server responds saying that the 221 service is closed. (Refer slide time: 3:45 - 33:17)

Actually, this protocol is so simple. You can just try it out yourself to send mails by directly doing a telnet on a particular port suppose you know the server name. So you just telnet the sever name 25 and you will get a 220 reply from server, then you enter your commands hello, mail, from, receipt, etc and then finally you quit. The above lets you send mail without using email client or a reader. (Refer slide time: 33:18 - 34:50)

This is of course also used by people who do not want to be identified that who is sending the mail etc, so all other misapplications and misuse of these facilities are also there. Here is one sample SMTP interaction, after you have telneted it the server comes with a response that 220 iitkgp ernet.in if suppose that is the name of the server. Then the client sends a hello cse iitkgp ernet.in, so you receive a helo from this client. Then the server will say helo cse iitkgp ernet.in pleased to meet you, then it will say mail from then maybe you will give an address, then 250 so the sender,OK. So, this is the client and server, the client is sending one command and the server is responding saying they are giving the feedback all the time that it is ok. Then you have receipt TO, this mail is to be sent to such and such, so he will say that recipient ok. Then he will say data, meaning that, what follows would be the data which is the body of the message that is to be sent. (Refer slide time: 34:52 - 35:45)

In response to this data command what the server will say is, give 354 enter mail, end with dot, there is a full stop on a line by itself. So, suppose this is the body of the mail, this is a test mail checking protocol etc then you end the mail with a dot by itself. So carries return nine feeds, CRLF followed by dot followed by CRLF that is the SMTPs way of checking that the mail body is ended. So the message 250 means message accepted for delivery. Now, when the message has been accepted you can quit and then it quits. (Refer slide time: 35:47 - 38:06)

Here are some final words about SMTP. First of all it uses persistent connections. That means, so as long as you do not quit you can go on sending mails one after the other. Actually that is what the machines will do. The session I showed you is just an example, usually you would not send mail by directly using SMTP. as a matter of fact there is no point doing that unless you have some other intention. So what you will do is that you will use a standard mail agent for doing that. When the machine opens an SMTP connection this is going to be persistent till such time you quit. So, a number of messages can be sent in the same session. I should also mention that, apart from these standard mail agents like using SMTP etc, one service which is becoming quite popular now-a-days are the web mails. Actually for the web mail you connect to the web mail server i.e. whoever gives you that service through HTTP. From that point onwards it works the same way. So SMTP uses persistent connections. SMTP requires message that is header and body to be in 7-bit ASCII. SMTP server uses CRLF. CRLF that means carriage return line feed. carriage return line to determine end of message. And as we have seen that SMTP is a chatty protocol in the sense that you say something immediately it tells you something back so this back and forth command and then the response goes on for this CRLF. CRLF sometimes some flexibility is also given when some part of it is missing. (Refer slide time: 38:08 - 38:53)

And comparing it with HTTP one thing is that HTTP is a pull kind of a protocol that means the user rather makes a request and pulls the content whereas here the content is pushed from the sender to the receiver. Both have ASCII command response interaction and status codes. In HTTP each object is encapsulated in its own response message. SMTP multiple objects sent in multipart message. Actually you can send a multipart message, this is where all the attachments etc to a mail come in. (Refer slide time: 38:55 - 39:59)

Now, of course as we know that now-a-days we can send not just text messages but other kinds of things namely pictures, graphics, voice recording etc we can send through the mail. This means it is possible to send binary files through the mail. And the way it is done is by encoding this binary file into a textual form. There are various ways to do this encoding. SMTP is the Simple Mail Transfer Protocol, it can send in NVT 7-bit ASCII format only. It cannot directly send binary files, for example, video or audio or images.
MIME: Is a Multipurpose Internet Mail Extension. So MIME is an extension to SMTP which allows non ASCII data to be sent through SMTP. This is so important and useful and very common now-a-days. (Refer slide time: 40:01 - 40:15)

It transforms non ASCII data into NVT ASCII at the sending end which can be transformed back at the other end. What it does is that it adds additional lines in message header to declare MIME content type. (Refer slide time: 40:17 - 42:05)

This is an example of some message which actually contains a binary file. The initial part of the header from sujoy etc to etc etc, subject: picture of car where maybe we are sending a picture through the message. So the from to subject is like a standard message. Then since this is using MIME it says MIME version 1.0 so MIME version has to be mentioned. This is important because whatever you are sending has to be decoded on that. Then content: The method used to encode data, content transfer encoding is base 64. So base 64 is one way of encoding a binary file into a text file but you have to mention that. There are different ways of encoding binary file into text and base 64 is just one of them. So the sender has to mention or sender has to clearly specify that this is the standard encoding scheme that he has used for encoding the file so that the receiver on the other end can actually decode it. So you have to mention the method used to encode data. Then you say the content type which is image or JPEG and then you give the base 64 encoded data. (Refer slide time: 42:06 - 43:42)

Base 64 encoding is actually quite simple. Suppose you have a binary file, you take 6-bits from a binary file at a time from groups of 24. So you take 3 bytes and make four 6-bit sequences from this so 3 ? 8 = 24 and 4 ? 6 = 24 so from three bytes you can get four groups of 6. So you interpret these 6-bit binary strings as a binary number and then depending on that number you encode it into a textual form. So 6-bit will give you 26 which is 64 different numbers which is 0 to 63 and the encoding is the following: Encoding is A B for 0, 1 etc followed by 26 lower case letters so that makes 26 upper case plus 26 lower case that is 52 letters followed by ten digits (0 - 9) so that makes it 62 followed by ?+? and ?/? for number 62 and 63. This way from (0 - 63) you get A to B upper case, a to b lower case, digits, and ?+?and ?/? that gives you an encoding so that is a text and you encode it as a text and then send it as a text. (Refer slide time: 43:43 - 45:00)

Now, we will come to the user end so this is POP3 which is the Post Office Protocol. POP3 is a very simple protocol for interacting with your mail box which is in the server. This is very widely used. What this is used for is that, actually your incoming mails will be deposited in your mail box, now POP3 is a protocol for getting that content of your mail box or downloading it on to your desktop machine. that has advantages for example, the server gets less loaded because with people getting so many mails now-a-days, very soon if you have a departmental server giving the mail services to all the members of the department and if all the mails get accumulated there then it will load the mail server variable, it is good practice to download it to your machine and POP3 is a protocol for doing this download. (Refer slide time: 45:01 ? 45:42)

Mail Access Protocols: there are other protocols apart from POP3. SMTP is a push protocol. Now, if somebody has pushed a message for you how will you access that email. So, this Mail Access Protocol is retrieval from the server, it allows mail stored in mail boxes to be accessed by the recipient. (Refer slide time: 45:45 ? 46:36)


POP is the Post Office Protocol where 3 is the version 3 RFC 1939. Users can not create folders on mail server usually. POP is a protocol for doing this mail access. Another one is IMAP which is a Internet Mail Access Protocol. This has more features and you can manipulate messages on the server. There are web mail services also which uses HTTP that means it may be hot mail, yahoo mail etc., these are web mail services which are accessed through HTTP. These are three major protocols which people use for accessing mails. (Refer slide time: 46:37 ? 48:29)

Post Office Protocol version three is simple and limited in functionality. It consists of client software and server software. The server performs password authentication, the server software allows the client software to access the recipients mail box. So what the POP3 client will do is, it will connect to the POP3 server on the main mail server of a work group or department or organization. So, the POP3 server would do a user authentication once again through user and password as this is simply a password check. And when the user is authenticated he is given access to his particular mail box. And what POP3 would usually do is, download all the mails from the mail server to the desktop machine. For example, it is possible to configure it so that only a copy comes to your desktop and another copy remains in the mail server. But such practice leads to overloading of the mail server and very soon its buffer starts getting full, its memory space starts getting used up. To POP3 you are not supposed to do much here and simply get the machine on your desktop. You can organize your local mail folders in anyway you like by creating directories, subdirectories, etc. (Refer slide time: 48:31 ? 53:06)

POP3 has a problem for some users in the sense that, if all the mails are actually downloaded onto your desktop machine then when you are away from your desktop, if it is a desktop and not a laptop, so if all the mails have been downloaded on your desktop then when you are away from office you have no access to the email folders. Now-a-days, the mail has become a common mode of not only exchanging information as well as storing old information etc., so people need access to pass mails almost all the time. One way to do it, instead of downloading it using POP3 onto your desktop you can download it onto your laptop and carry that laptop around wherever you go. The other alternative way is to keep it in the server. If you can keep it in the server what would happen is that, naturally you can get access to the server from other places and you can see your mails. But the same advantage is there for web mail also. If you are using a web mail service your mail is stored in the server whoever is giving you the service. Both web mail and IMAP4 has this facility of storing the mail in your server. What facility you require? They are in the server, mail box, and anywhere. You can have access to it and if you directly access the mail box the interface is not so good so these access protocols gives you somewhat better interface. Specifically, IMAP4 is more complicated than POP3 and has got more features. IMAP4 means Internet Mail Access Protocol version 4. It stores user?s mail in the server so that it can be access from multiple locations. It is able to address mails not just by arrival numbers but by attributes. So you can define attributes on mails and you can say something like give me the last mail from Steve so it is going to do a search on your directory and access that particular mail. But if you have it on your desktop you can organize it anyway you like. here of course you are not taking it to your desktop it is in the server so this protocol has got more features so that you can access your mails in a more flexible manner and that is important because now-a-days it is all the old mails you have, there are many people who have thousands of mails and many of them are useful of course it keeps accumulating junk also somehow we fail to delete. So, anyway some kind of organization and some kind of searching this is necessary in order to operate with mails in an easy manner. So it is able to address mails not just by arrival numbers but by attributes also. It has more features than POP3 as I mentioned. Can check email header prior to downloading which is another feature. What POP3 would do is, download all mails that are in your mail box in a blind fashion. For accessing through IMAP4, before accessing you can do some kind of filtering by checking the email header. It can search contents of email prior to download so contents can also be searched before you download. (Refer slide time: 53:07 ? 53:26) 

It can partially download email, it can create delete or rename mail boxes on the mail server and can create a hierarchy of mailboxes in a folder for email storage. This is like creating a hierarchy of directories for sorting the mail into its proper category. These are the references. (Refer slide time: 53:43 ? 53:56)

For example, RFC:821 for mail transfer protocol, RFC:2821 once again for SMTP, RFC:2045 for MIME and there are other RFC?s associated with mail. RFC?s contain all the information that is necessary for these protocols. With this we come to the end of our discussion about these two important protocols namely FTP and mails that means file download upload and mail exchange. Another protocol we have to talk about is naturally the HTTP or the web which we will take up in the next class, thank you. 
Preview of the next lecture 
Lecture ? 40
HTTP 
We will discuss about HTTP which is the most widely used application layer protocol today. This is of course at the base of your world wide web, this (Refer slide time: 55:02)

HTTP stands for (Refer slide time: 55:04 ? 56:47)

Hypertext Transfer Protocol. It is the network protocol used to deliver virtually all files and other data collectively called resources on the World Wide Web whether they are HTML files, image files, query results or anything else. Usually HTTP takes place through TCP IP sockets which means it uses TCP. World Wide means majority of them are some files which are usually HTML files, there may be images, but also increasing the we have a lot of multimedia content on the web may be audio files, video files which are again termed as files and large number of applications are also being developed based on web because the spread of web has been so fast that many of the service providers are beginning to give their services through the web so that the service becomes available may be 24 hours a day anywhere in the world. So, apart from files there may be outputs of other programs like hyper text transfer protocol which are also passed through this protocol. In hypertext, the word text actually came from the original content which was most probably textually based with hyperlinks. But then, this cache has to be updated regularly. Last-Modified: This means if there is a modification on the other side that means if the lastly modified value has changed then you should get a new copy of that so you should refresh the current page. (Refer slide time: 57:11 ? 57:17)

Now, the request line consists of the request type, URL and version. (Refer slide time: 57:19 ? 57:22)

Request type: Categorize request message into several methods. (Refer slide time: 57:23 ? 57:40)

URL: Uniform Resource Locator which is the web page or document address. It indicates a method, the host, the port number and the path. (Refer slide time: 57:41 ? 58:06)

Let us look at this figure. First is the method used and it could be something like HTTP or even FTP etc, :// then the host. We usually prefer naming the hosts with domain names because it is much easier for us to remember the domain name that is one thing, Secondly, in some cases the domain name remains the same but the underlying IP addresses may have changed. So, if you give the domain name what will happen is that, first it will go through the resolver and get the IP address. 
COMPUTER NETWORKS
Prof. Sujoy Ghosh
Department of Computer Science and Engineering
IIT, Kharagpur
Lecture No. # 40
HTTP
Good day! This is the last lecture and here we will discuss about the most widely used application layer protocol HTTP. This protocol is at the base of your world wide web. (Refer Slide time: 01:04 - 01:07)

HTTP stands for (Refer Slide time: 01:08 - 02:55)

Hypertext Transfer Protocol. It is the network protocol used to deliver virtually all files and other data (collectively called resources) on the world wide web whether they are HTML files, image files, query results or anything else. Usually HTTP takes place through TCP/IP sockets. So the first thing is that the majority of resources available today on world wide web is of course some files which are usually HTML files. There may be images also. But also increasingly we have a lot of multimedia content on the web which may be audio files, video files etc. And a large number of applications are also being developed based on web because the web is vastly spread and many service providers are beginning to give their services through the web so that the service becomes available 24 hours a day anywhere in the world. So, apart from files there may also be an output of other programs which are also passed through this protocol namely the hyper text transfer protocol. So in this hypertext the word text actually came from the original content which was mostly textually based with hyperlinks but now-a-days a lot of it has changed. So, this is the HTTP (Refer Slide time: 02:55 - 03:29 )

hyper text transfer protocol. The HTTP client is our browser therefore this internet explorer, netscape, Mozilla are all HTTP clients. It sends a request to an HTTP server also known as web server which then sends responses back to the client, the standard and default port for HTTP server to listen on is 80 though they can use any port. So, the well known port for HTTP is 80. So the first request which the client is requesting for in any HTTP server or any web server goes to the port number 80. Later on they may negotiate another port number. (Refer Slide time: 03:46 - 04:47)

HTTP is used to transmit resources not just files, that is another point because require we are talking about resources. A resource is some chunk of information that can be identified by a URL. This is R in URL and URL is Universal Resource Locator, it is some kind of an address for these resources which is universally valid. Any resource which has an URL can be accessed through the web and of course it has to be served by the web server. The most common kind of resource is a file. But a resource may also be a dynamically generated query result, the output of a CGI script, a document that is available in several languages or something else. Now-a-days we have dynamic web pages or a web page may change and somehow they are sort of outputs of some programs and they are also resources or rather static files. (Refer slide time: 04:48 - 05:11)

While learning HTTP it may help to think of a resource similar to a file, but more general. As a practical matter, almost all HTTP resources are currently either files or sever side script output that means some script running on the server. What you get to see when you surfing web, sometimes some of the effects may also be the output of programs which have been downloaded along with the page which is running on your machine that means which is running on the client machine through a java, applets etc.. But anyway that program actually is coming from the server. (Refer Slide time: 05:36 - 06:42)

Used to access data on the www plain text, hyper text, and hyper text means text with hyperlinks that means links to other pages embedded in them. Actually these hyperlinks are so called links to other pages, as soon as you click on a link what happens is that a request is automatically sent to a particular page which the URL is pointing. Therefore it may be a text, it may be an image which can be clicked and whatever be the manifestation of this hyper link there is a URL embedded in it and when you click on it a request is automatically generated. When such hyperlinks are embedded in a text it is called a hypertext. Apart from plain text and hyper text we have audio, video or other things. (Refer slide time: 06:43 - 07:32 )

HTTP functions like a combination of FTP and SMTP in some way. FTP as it transfers files and uses the services of TCP. Just like FTP this is also using TCP and has only one TCP connection unlike FTP which uses two connections; one for control and one for data but here only one connection is used. And it is like SMTP. As data transfers between client and server it looks like SMTP messages. So they have format of message controlled by MIME like headers. HTTP messages are delivered immediately unlike mail, unlike SMTP. Like FTP, in HTTP the messages are also delivered immediately. (Refer slide time: 07:32 - 07:49)

HTTP is actually very simple. Client sends a request and server sends a response, similar to mail request and reply, data in the form of a letter with MIME like format. Refer slide time: 07:50 - 08:06)

Like most network protocols HTTP uses the client server model. An HTTP client opens a connection and sends a request message to an HTTP server. The server then returns a response message usually containing the resource that was requested. (Refer slide time: 08:07 - 08:25)

After delivering the response, the server closes the connection making HTTP a stateless protocol. That is not maintaining any connection information between transactions. This is one important point to remember that basic HTTP is a stateless protocol. A request has come, it responds and forgets and closes the connection and it forgets everything about it immediately. Sometimes some web servers would really like to keep some information and there are various things like access logs etc which may be maintain on the server side. Another common way in which some state information is kept are through cookies. So, cookies are a simple text file kind of a thing which is stored at the client?s machine. What the server does is, it writes something about this clients request in a text file in the client machine itself so that next time if the that same client connects again then the HTTP server can leave that file and can find out what is the last visit or some information about it. Cookies too sometimes breaks down the privacy in some sense and somebody else is writing things in my machine so that may be a source of breach of privacy or security so some people would like to block the cookies also. It is also possible to block the cookies. But anyway the point is, basic HTTP protocol is stateless. that means when one a request comes and once it is serviced the machine forgets about it. (Refer slide time: 10:19 - 11:06)

Now, let us look more into the details of the structure of HTTP transactions. The formats of the request and response messages are similar and English oriented. Both kinds of messages consist of an initial line, zero or more header lines, a blank line that is a CRLF Carriage Return and Line feed by itself and an optional message body for example, a file or query data or query output etc so this is the structure. We will see what each of these areas really contains. The blank line is just to delineate between the header and the message body. (Refer slide time: 11:07 - 11:28)

Put another way, the format of an HTTP message is initial line, different for request versus response, then header1: value1, header2: value2 so the header name: value of that header, and that is how the headers go. (Refer Slide time: 11:28 - 11:35)

Then you have the blank line and then the body. So you have initial line, headers, blank line and the body. (Refer slide time: 11:36 - 12:52)

Initial Request Line: This is different for the request than for the response. A request line has three parts separated by spaces: a method name, the local path of the requested resource and the version of HTTP being used. A typical request line is Get /path/to/file/index.html. So this is an example, this is the path name, path name to this index dot HTML file. So, starting from the route it goes down the directory and subdirectory etc to finally locate the resource which is now here in index. HTML so this is really a request line. So GET is a method, we will come to that later on. And finally it mentions the version of HTTP that is being used, HTTP 1.0. This is what is going to go from the client to the server. (Refer slide time: 12:53 - 13:36) 

GET is the most common HTTP method. It says ?give me this resource?. We will go into little bit more into the details of GET and other methods like, POST and HEAD etc later on. So method names are always upper case. The path is the part of the URL after the host name also called the request URI. So a URI is like a URL but more general. And HTTP version always takes the form HTTP/x.x a 1.0 etc. What we are trying to get is some file located in some site that is in some web server. Some how you should be able to contact that particular machine, some how we have to indicate the machine then we have to say what is it we are doing, what kind of protocol we are using. And finally in that machine we have to give a path name to that file. That path name is necessary because there may be files with the same name in different parts of the directory so always the path name is necessary. Therefore all these together constitute the URL. (Refer slide time: 14:27 - 15:42) 

Initial Response line (Status Line): Initial response line, we have seen initial request line now, we see the initial response line. The initial response line called the status line also has three parts separated by spaces; the HTTP version, a response status code that gives the result of the request and an English reason phrase describing the status code. Typical status lines are HTTP/1.0 200 OK that means a request has been received and its response is that it is OK, 200 is the code for it and it is OK. That means whatever has been requested would also be forthcoming or HTTP/1.0 404 Not found. This means, the particular file we were looking for in the request may not be available and therefore it says file not found. Now, what your client or browser would do is that it will read this and then display it may be in a separate window or a separate page that the file has not been found or the source has not been found. (Refer slide time: 15:43 - 16:03) 

The HTTP version is in the same format as in the request line that HTTP/x.x. The status code is meant to be computer readable. The reason phrase is meant to be human readable and may vary. The status code is a three-digit integer and the first digit identifies the general category of response. (Refer Slide time: 16:04 - 17:06) 

1xx indicates an information message only, 2xx indicates success of some kind, 3xx redirects the client to another URL. Sometimes somebody may have moved from his old site to some new site may be to some new URL, so what he would do is that, he would leave a redirection message in the old site, so those clients who have the old URL land up there unknowingly they are automatically redirected to the new URL. So 3xx redirects the client to another URL, 4xx indicates an error on the clients part and 5xx indicates an error on the server?s part. So, whatever resource name it has given it does not exist so that is 404. therefore this is the status line. (Refer slide time: 17:06 - 17:29)

The most common status codes are of course 200 OK and 404 not found. OK means the request is succeeded and the resulting resource is returned in the message body. And 404 means not found, 301 that it has moved permanently, 302 means it has moved temporarily and so on. (Refer Slide time: 17:30 - 18:00)

So 303 means the other the resources moved to another URL given by the location response header and should be automatically retrieved by the client. This is often used by a CGI script to redirect the browser to an existing file. The 500 is the server error. It is an unexpected server error, the most common cause is a server side script that has a bad syntax or fails or otherwise cannot run correctly. (Refer slide time: 18:01 - 18:14)

Now, after the initial line there are the header lines. 
Header Lines: Header lines provide information about the request or response or about the object sent in the message body. (Refer slide time: 18:15 - 18:32)

The header lines are in the usual text header format which is one line per header, of the form ?Header - Name: value?, ending with a CRLF. It is the same format used for email and news postings. (Refer slide time: 18:33 - 18:53) 

As noted above they should end in CRLF. The header name is not case sensitive though the value may be. Any number of spaces or tabs may be between the colon and the value. Header lines beginning with space or tab are actually part of the previous header line folded into multiple lines for easy reading. (Refer slide time: 18:54 - 19:14) 

The following two header lines are actually equivalent. You may say header 1: some-long-value-1a some-long-value-1b or header1: some-long-value-1a, then some-long-value-1b, this is just folded. (Refer slide time: 19:14 - 19:44) 

And for net-politeness when you send in a request the from part should be included. That means header gives the email address of whoever is making the request. You may or may not because this might lead to a lot of unsolicited mails coming to you also or running the program doing so. This must be user configurable for privacy concerns. (Refer slide time: 19:45 - 20:16) 

The User Agent: header identifies the program that is making the request in the form program name/x.xx where x.xx is the mostly alphanumeric version of the program. For example, Netscape 3.0 send the header ?User-agent: Mozilla/3.0 Gold?. So that is the program version. (Refer slide time: 20:16 - 20:37) 

If you are writing servers consider including these headers in your responses. 
The server: the header is analogous to the user agent header; it identifies the server software in the form of program name. For example, one beta version of apache is server returns server apache such and such. (Refer Slide time: 20:38 - 22:05) 

The last modified: the header gives the modification date of the resource that is being returned. it is used in caching and other bandwidth-saving activities. Used GMT in the format last modified such as day, date and time and this last modified is important. The point is that, sometimes there are some web pages which are seen by a lot of people and if they are seen by a lot of people in the same organization one way to conserve bandwidth is through the user proxies. Once the page is brought in, that means it is put in that proxies cache automatically so that the next person who is under that same proxy makes the request and he gets it straight away from the cache without having to go to the distant location. But then this cache has to be updated regularly. If there is a modification on the other side, if the last modified value has changed then you should get a new copy of that, then you should refresh that page. (Refer Slide time: 22:06 - 22:13) 

 Request Line consists of the request type, URL and version. (Refer slide time: 22:14 - 22:17) 

Categorize request message into several methods. (Refer slide time: 22:17 - 22:34) 

URL: This is the uniform resource locator which is the webpage or document address. it indicates the method, the host, the port number and the path. (Refer slide time: 22:35 - 25:24) 

First is the method that is used and this method could be something like HTTP or even FTP etc, :// then the host and for this host we usually like to give the domain name because it is much easier for us to remember the domain name. Secondly, domain name remaining the same, its underlying IP address may have changed. If you give the domain name what will happen is that first it will go through the resolver and get the IP address. The domain name can be given over here. Alternatively an IP address can also be given over here. If IP address is given then straight away the IP address is followed by a : followed by the port so that immediately becomes the socket. The IP address and the port number together form the socket so we give the method and the socket and when you have done up to this much the port for the initial request is port number 80. If you give the host IP address and the port number, you have given the socket which means that we have indicated or we have specified where to go, first to the domain name and subsequently to the IP address of the host. By the way this host must be on a public IP naturally and they cannot have any private IP if they have to be accessed over the internet, so the entire socket. Now we have reached the machine and we also know the method to use and then in that machine we have to locate the resource which is the path, slash means you start at the root then you go right down to the subdirectory where that particular file is present and then you give the name of the file, this is the entire URL. This way all the resources located in any of these machines connected to the internet having a public proper IP address can be uniquely specified. This is basically the addressing scheme for HTTP. (Refer slide time: 25:25 - 25:40) 

Protocol used to retrieve the document, as I said HTTP is the most common, you can also use FTP or Gopher or news or telnet. These could be the different methods used in the method part. (Refer slide time: 25:41 - 25:57) 

Host part is the computer where the information is located given by the domain name or may be the IP address directly. Port number is separated by the colon and as I said the default port is 80 and path is path name of the file on the server. (Refer slide time: 25:58 ? 26:04) 

Version: current version 1.1 and 1.0, 0.9 are still in use. (Refer slide time: 26:05 - 26:20) 

Now let us look into the methods in more detail. HTTP is an important protocol and moreover many applications based on web use these methods directly so that you can give some program output or can interact with your client through the web. Originally HTTP was not envisioned or envisaged just for locating some resource download it and see text and later on may be hypertext and some multimedia content. But increasingly the web had become so popular and so many people are hooked onto the web that people started giving services through the web. Naturally all these services go through java script and a browser client etc so actually it is somehow communicating in HTTP at some level. (Refer slide time: 27:28-27:49)

Methods: The actual command or request that a client issues to the server. One is GET: client wants to retrieve the document, server responds with the contents of the document. HEAD: client wants information about the document but not the actual document so just the header part of it is what is needed. (Refer slide time: 27:50 - 28:07)

POST: client provides some information to the server. PUT: client provides a new or replacement document to be stored on the server. PATCH: it is similar to PUT but only a list of differences to be made rather than the actual new document. (Refer slide time: 28:08 - 28:20)

COPY: copy a file to another location. MOVE: moves a file to another location. DELETE: remove a document on the server. (Refer slide time: 28:21- 28:37)

LINK: creates a link from a document to another location. UNLINK: delete links created with LINK. OPTION: used by client to ask server about available options. These are all the different methods. (Refer slide time: 28:38 - 29:20) 

The Message Body: An HTTP message may have a body of data sent after the header lines. In a response this is where the requested resource is returned to the client, the most common use of the message body. That means you have asked for a web page and that page is here, that content of that page is here or perhaps explanatory text if there is an error. In a request this is where user entered data or uploaded files are sent to the server. So this is the message body, the main content which is coming in the message. (Refer slide time: 29:20 - 30:25)

If an HTTP message includes a body there are usually header lines in the message that describes the body. In particular the content type, header gives the MIME type of the data in the body such as text/or HTML or image/gif. Suppose some image is being sent on request then on other side the client would know that what kind of resource it is, it is being told that this is an image and a gif type of image, so that on the client side that particular application or program to display gif files would be activated. The content length: header gives the number of bytes in the body. (Refer slide time: 30:26 ? 31:07)

To retrieve the file at the URL we write something like http: //www.somehost.com this is the domain name path /. And of course since over here we will see that in this request the port number has been left out so by default it is port number 80/path/file.html. First open a socket to the host www.somehost.com at port 80 and then get that file. (Refer slide time: 31:08 ? 31:52)

Then, send something like the following through the socket: GET/path/file.html HTTP/1.0, this is a request. You see the sequence; first you open a socket to the host which is the web server which is in this case some host. com at port 80, then you put another request the get request with the file name and the HTTP version. From some user from somewhere and user agent you mention and then you put the blank line, this is the header part. (Refer slide time: 31:53 ? 32:16)

The server should respond with something like the following: sent back through the same socket: HTTP / 1.0 200 OK that means the request is granted and then request is OK. Then the date that means when it was last modified, content type is text/HTML and content length is something. (Refer slide time: 32:17 ? 34:41)

And HTTP in the body the body will contain may be in this particular case a text document which is in HTML. HTML stands for hypertext mark up language. Once again we will not have time to go into HTML what you can do is that you can open any web page and look at the source you will find that in your browser if you look at the top menu then there is a way of rather than looking at the final made up version you can look at the original source and there you can see the HTML file. 

The HTML file has a number of tags and these tags help in specifying the format in which the text is to be displayed. Essentially there are other things but the major part of it is the format. There may be format, there may be tags like header sort of shown in bold, there may be tags to show the back ground color, the foreground color, the font colors, shape, size etc. All kinds of formatting information are there in HTML documents. Apart from that it is also possible to embed links. This is one example of what an HTML document might look like. First of all it is the tag HTML, whenever you have the tag you must have the beginning of the tag you must have the corresponding ending of the tag so HTML here and HTML here. Similarly body, then it says something happy new millennium or something which is the header, it says h1 which is a header then there may be more file contents and etc. After sending the response the server closes the socket. This is a stateless protocol and the server forgets about it. This is an example of what a simple text HTML document looks like. (Refer slide time: 34:42 - 35:41)

Now let us look a bit more deeply into the methods. The Head Method: A HEAD request is just like a GET request except it asks the server to return the response headers only and not the actual resource, that is no message body. So this will work very fast. If you just want to know the header and then take some decision whether or not you want to look at it then you can ask just for the header and not the rest of it so it will come down very fast. This is useful to check characteristics of a resource without actually downloading it thus saving bandwidth and also a lot of time. Use head when you do not actually need files content. The response to a head request must never contain a message body, just the status line and header. That is the head method, just get the header. (Refer slide time: 35:42- 36:23)

The POST Method: A POST request is used to send data to the server to be processed in some way like by a CGI script. A POST request is different from a GET request in the following ways: there is a block of data sent with the request in the message body, there are usually extra headers to describe this message body like content type, content length etc., The request URI is not resource to retrieve, it is usually a program to handle the data you are sending. The HTTP response is normally program output, not a static file. This is quite important, for example, now-a-days for your electric utility or for your telephone, suppose the bill has not reached what you can do is that you can see your bill on the net. So what happens is, suppose you are a BSNL subscriber, the first thing you would like to do is to go to the BSNL?s home page. If you know the address then it is very fine or otherwise you can use any searching in like Google or something to get the address of BSNL and then click on that address which is usually a link to the web page. A link to the web page meaning that usually any website would contain an initial or main page or index page as it is called and if you simply make an HTTP request to the correct server the main page or the first page or the index page would be displayed first. Actuall, that is an important point because while surfing we are accessing so many pages in so many machines. In the URL as we have seen, in that last part the located part you have to give the entire path name as well as the file name. Unless you are very well aware and very conversant with one particular sight or may be a few sites other than that you will not know either the path name or the file name, so how do you locate it and that is not how people serve. What they do is that may be they know that in this particular example we were giving so we are trying to find our own telephone bill which is missing so I want to see my bill. So I will connect to BSNL and I will somehow find the main address of BSNL which may be BSNL.co.in or something and that is where I will go. Now, I have just mentioned the domain name and nothing else which actually means a server, which actually means this domain name would stand for some web server. The domain name of course can be translated via the DNS system into the proper IP address. And since it is an HTTP protocol that is under default port is 80 so up to that much that means up to the socket part it is fixed. Now what about the rest? The rest is taken for granted that if you have not specified something you will definitely be given only that first page or index page or main page of that site. That comes as a response in the message body in the form of an HTML document and that gets displayed in that nice format usually by the browser on your stream. Now of course, that is a main page it may contain several things amongst all that I find out that if I am looking for a duplicate bill or something where do I go? Therefore something would be written over there like duplicate bill where if you take your cursor on that you will see that the cursor has changed showing that the color has changed or something has got underlined someway to indicate that this is actually a link when you click on that link. Now what does that contain? It says duplicate bill and that is how it is communicating to the user but underline this text, there is a hyperlink and this hyperlink contains the path name and the file name which will take you to the page where you can get information about duplicate bills. Let us say, I click it, as soon as I click it, actually I generate a fresh request where the server socket etc is already known. And now this new path name is added on to it so this is a new request. HTTP of course being a stateless protocol has shown you the first page and has forgotten that you actually exist. But then now you have made another request which now brings a second page into zero. Now in that page may be you will find that there is a form over there. Who would process the form on the other side, may be some CGI script or some other program you are not going into that part in the server, but before that whatever information you have may be your name, may be your telephone number, something you have to enter, there will be a form you enter that and then you submit. Actually submit sounds very nice and user-friendly but then your machine which was the client so long now has to upload some information, the information which you have just typed in it has to upload that information on to the server. This uploading of information (Refer slide time: 42:02- 42:11)

 may be done via this post, this is the opposite of that. (Refer slide time: 42:12 - 44:16)

The most common use of post by far is to submit HTML form data to CGI script. CGI stands for Common Gateway Interface so when you send something, this goes in some format and the CGI is a language where it is easy to pass whatever you have sent and then get the appropriate values. Therefore what CGI script will do is that, with all these information, it will form some kind of query may be a data base query to some database, in our example we were talking about telephone bills, so in that BSNL server where all the bills or may be at least the current bills are stored. So it will go with the current request information, form a query, get that kept your bill and then your bill was of course in a data base. So now that information has come, that has to be put together in the form of a dynamically generated web page and sent back. This is a very common way of using post. In this case, the content type: header is usually application/x-www-form-URL encoded. And content length header gives the length of the URL encoded form data, the CGI script receives the message body through STDIN and decodes it. CGI is specially designed so that such data which is coming in can be easily passed and all the values retrieved. This is how post method is used. So this is how applications over the internet are generally implemented. (Refer slide time: 44:17 ? 44:23)

    Here is a typical form submission using post, let say POST /path/script.cgi HTTP/1.0. So, basically we are actually invoking some CGI script and we have given that address of the URL of the script and we have mentioned the method and the HTTP version. From: is of course somewhere. User Agent: HTTP tool/1.0, Content-Type may be application/x www form URLencoded, Content Length: 32, Home: = something. So, this is a typical form submission, so, on the other side this will be taken and processed. And finally a dynamic web page would be generated and shown. (Refer slide time: 45:24 ? 46:57)

HTTP proxies: Are important for various reasons. One reason we have already discussed is that, suppose you are in an organization where a large number of people access the web outside, now-a-days almost everybody wants to access the web and sometimes the same page. Of course the WAN bandwidth is always costly and WAN bandwidth is always much lower than the internal band width. If some page is already been brought in and that page is fresh enough then you can cache it in the proxy, this is one major use of HTTP proxy, there are also other uses of HTTP proxy. An HTTP proxy is a program that acts as an intermediary between a client and a server. Instead of a client directly calling the server or directly sending a request to the server it may only send it to the HTTP proxy. It receives request from clients and forwards those requests to the intended servers. The responses passes back through it in the same way. Thus a proxy has functions of both a client and the server. It comes in between the client and the server. (Refer slide time: 46:58 - 51:24) 

When a client uses a proxy it typically sends all requests to the proxy instead of to the servers in the URLs. Requests to a proxy differ from normal requests in one way. In the first line they use the complete URL of the resource being requested instead of just the path, this is of course important. Actually if you are sending a request to the original server then the server has got the request that means you have reached up to that server. Now, you need to simply give the path from the root to the actual file name. Now, instead of sending the request directly to the web server, the client, it sends its request to the local proximation. Therefore in the proximation it has to supply not only the path but also the domain name or IP address of the server that it is sending its request to. Apart from caching and bandwidth conservation there is another use sometimes that a proxy is a put to. You may do it in proxy, you may do it in some other places also like fire walls etc.. In many organizations now-a-days what is happening is that the network is becoming so big but the IP address space is small. You cannot give a proper public IP address to all the machines in the network, may be the organization does not have such a big network For example, class B addresses have become very few or almost non existent these days. Therefore many organizations will not have a class B address. If that is so you cannot give proper IP address. Now, if you cannot give the proper IP address to the client, may be you can still send the request to the server but how will the servers respond back? For responding back the server has to communicate with you, it has to open the TCP connection. For that on the other end there must be a machine with the valid IP address. One way to solve this problem is to do network address translation and although network address translation may be done at the fire wall, may be done at the router, it may be also be done at the proxy. that means what we do is that, if it is a big organization may be 5000 machines or something and has got just one class C address may be 1250 addresses. What it might do is that, it might allot some number say 50 valid IP addresses to the proxy so that whenever there is a request which is being sent by the client. Now the client need not be given a public IP. If the client may be given a private IP which is just private to that network that does not work outside the boundary of the organization, from that it sends a request to the proxy, the proxy takes up this request and proxies that means it sends the request out to the world through the intended web server giving one of the valid IP addresses and when it gets the response, it changes the IP back again to the actual private IP of the machine and gives it back. That is another way, proxy may be used. Proxy may also be used to keep track of what kind of data or what kind of sites people in the organization are generally looking at, they may feel that this site may be good, this may be bad but anyway the point is that this may be done if you are routing all your HTTP requests through the proxies. Naturally it has to give the entire URL when it is sends a request through a proxy. (Refer slide time: 51:25 - 51:55)

For example, GET HTTP then you give the domain name, then the path, then you give the path name, then the HTTP and its version number. That way the proxy knows which server to forward the request to though the proxy itself may use another proxy. There may be hierarchy of proxies etc, you are not bothered about that. (Refer slide time: 51:56 ? 52:15)

As the saying goes in network programming anyway be strict in what you send and tolerant in what you receive. Other clients and servers you interact with may have minor flaws in their message but you should try to work gracefully with them. (Refer slide time: 52:16 -52:51 )

HTTP specification suggests the following: Even though header lines should end with CRLF that is a carriage return and line feed someone might use a single line feed instead, accept either CRLF or LF. The three fields in the initial message line should be separated by a single space but might instead use several spaces or tabs, accept any number of spaces or tabs between these fields etc. Usually on the other side is tolerant of these. With this we come to the end of this lecture. Actually, HTTP and this entire gamut of web services have been developed in such a manner that these demands codes by itself. Actually this whole field of networking has grown at a tremendous rate. people usually talk about Moore?s law for computing power and memory capacity etc. But if you look at the network traffic and the speed at which network has been growing, you will find that this is actually much steeper than the Moore?s law. Moore?s law by itself is exponential so being steeper than Moore?s law means that you have a higher base, steeper rate or growth which is a very phenomenal rate of growth. And this phenomenon rate of growth has to respond to a large number of technologies, a large number of protocols etc., and it is expanding all the time. Actually we have tried to encompass the basic technologies, first of all the lower layers and just we have touched upon core ideas in all the seven layers. We talked about web today and you can have an entire course on the way web services are given and all web applications can be developed etc. We have given only one lecture on a network security, you can have an entire course and networks security. It has become so important these days because transactions are being done over the network. What we expect is that as time goes on, this ubiquitous-ness of network is only going to grow and now-a-days we are not always care full to may be carry a pen because may be wherever we go we will find a pen. Similarly the expectation now would be that wherever you go may be you can find the network outlet where you may be having a laptop or palm top or whatever and through the DICP get yourself configured and be on the network. And of course people are thinking of advertising intelligent machines like your microwave oven or your heater or other things also on the network. So this is going to be a universally networked world. So with this we come to the conclusion of this lecture. Thank you. 
 

