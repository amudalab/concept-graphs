522 Chapter 12
12.7
swap partition
or swap file
swap map
1---------swap area--------1
page
I- slot -1
L---~---~---_L __ _L _ ~
Figure 12.10 The data structures for swapping on Linux systems.
This scheme gives better performance on modern computers, which have more
physical memory than older systems and tend to page less.
Linux is similar to Solaris in that swap space is only used for anonymous
memory or for regions of memory shared by several processes. Linux allows
one or more swap areas to be established. A swap area may be in either a swap
file on a regular file system or a raw-swap-space partition. Each swap area
consists of a series of 4-KB which are used to hold swapped pages.
Associated with each swap area is a .u1.2.p-an array of integer counters,
each corresponding to a page slot in the swap area. If the value of a counter is 0,
the corresponding page slot is available. Values greater than 0 indicate that the
page slot is occupied by a swapped page. The value of the counter iJ.l.dicates the
number of mappings to the swapped page; for example, a value of 3 indicates
that the swapped page is mapped to three different processes (which can occur
if the swapped page is storing a region of memory shared by three processes).
The data structures for swapping on Linux systems are shown in Figure 12.10.
Disk drives have continued to get smaller and cheaper, so it is now economically
feasible to attach many disks to a computer system. Having a large number
of disks in a system presents opportunities for improving the rate at which data
can be read or written, if the disks are operated in parallel. Furthermore, this
setup offers the potential for improving the reliability of data storage, because
redundant information can be stored on multiple disks. Thus, failure of one disk
does not lead to loss of data. A of disk-organization techniques, collectively
called disks (RAIDs), are commonly
used to address the performance and reliability issues.
In the past, RAIDs composed of small, cheap disks were viewed as a
cost-effective alternative to large, expensive disks; today, RAIDs are used for
their higher reliability and higher data-transfer rate, rather than for economic
reasons. Hence, the I in RAID, which once stood for   inexpensive/' now stands
for   iJ.l.dependent.  
12.7.1 Improvement of Reliability via Redundancy
Let us first consider the reliability of RAIDs. The chance that some disk out of
a set of N disks will fail is much higher than the chance that a specific single
12.7 523
STRUCTURING RAID
RAID storage can be structured in a variety of ways. For example, a system
can have disks directly attached to its buses. In this case, the operating
system or system software can implement RAID flmctionality. Alternatively,
an intelligent host controller can control multiple attached disks and can
implement RAID on those disks in hardware. Finally, a , or
can be used. A RAID array is a standalone unit with its own controller,
cache (usually), and disks. It is attached to the host via one or more standard
ATA SCSI or FC controllers. This common setup allows any operating system
and software without RAID functionality to have RAID-protected disks. It
is even used on systems that do have RAID software layers because of its
simplicity and flexibility.
disk will fail. Suppose that the of a single disk is 100,000
hours. Then the mean time to failure of some disk in an array of 100 disks
will be 100,000/100 = 1,000 hours, or 41.66 days, which is not long at all! If we
store only one copy of the data, then each disk failure will result in loss of a
significant amount of data -and such a high rate of data loss is unacceptable.
The solution to the problem of reliability is to introduce , we
store extra information that is not normally needed but that can be used in the
event of failure of a disk to rebuild the lost information. Thus, even if a disk
fails, data are not lost.
The simplest (but most expensive) approach to introducing redundancy
is to duplicate every disk. This technique is called With mirroring,
a logical disk consists of two physical disks, and every write is carried out
on both disks. The result is called a mirrored volume. If one of the disks in the
volume fails, the data can be read from the other. Data will be lost only if the
second disk fails before the first failed disk is replaced.
The mean time to failure of a mirrored volume-where failure is the loss
of data- depends on two factors. One is the mean time to failure of the
individual disks. The other is the which is the time it
takes (on average) to replace a failed disk and to restore the data on it. Suppose
that the failures of the two disks are that is, the failure of one disk
is not connected to the failure of the other. Then, if the mean time to failure of a
single disk is 100,000 hours and the mean time to repair is 10 hours, the
of a mirrored disk system is 100, 0002 /(2    10) = 500    106
hours, or 57,000 years!
You should be aware that the assumption of independence of disk failures
is not valid. Power failures and natural disasters, such as earthquakes, fires,
and floods, may result in damage to both disks at the same time. Also,
manufacturing defects in a batch of disks can cause correlated failures. As
disks age, the probability of failure grows, increasing the chance that a second
disk will fail while the first is being repaired. In spite of all these considerations,
however, n1.irrored-disk systems offer much higher reliability than do singledisk
systems.
Power failures are a particular source of concern, since they occur far more
frequently than do natural disasters. Even with mirroring of disks, if writes are
524 Chapter 12
in progress to the same block in both disks, and power fails before both blocks
are fully written, the two blocks can be in an inconsistent state. One solution
to this is to write one copy first then the next. Another is to add a
cache to the RAID array. This write-back cache is
protected from data loss during power failures, so the write can be considered
complete at that point, assuming the NVRAM has some kind of error protection
and correction, such as ECC or mirroring.
12.7.2 Improvement in Performance via Parallelism
Now let's consider how parallel access to multiple disks improves performance.
With disk mirroring, the rate at which read requests can be handled is
doubled, since read requests can be sent to either disk (as long as both disks
in a pair are functionat as is almost always the case). The transfer rate of each
read is the same as in a single-disk system, but the number of reads per unit
time has doubled.
With multiple disks, we can improve the transfer rate as well (or instead)
by striping data across the disks. In its simplest form, consists
of the bits of each byte across multiple disks; such striping is called
For example, if we have an array of eight disks, we write bit
i of each byte to disk i. The array of eight disks can be treated as a single disk
with sectors that are eight times the normal size and, more important that have
eight times the access rate. In such an organization, every disk participates in
every access (read or write); so the number of accesses that can be processed
per second is about the same as on a single disk, but each access can read eight
times as many data in the same time as on a single disk.
Bit-level striping can be generalized to include a number of disks that either
is a multiple of 8 or divides 8. For example, if we use an array of four disks,
bits i and 4 + i of each go to disk i. Further, striping need not occur at
the bit level. In for instance, blocks of a file are striped
across multiple disks; with n disks, block i of a file goes to disk (i mod n) + 1.
Other levels of striping, such as bytes of a sector or sectors of a block, also are
possible. Block-level striping is the most common.
Parallelism in a disk system, as achieved through striping, has two main
goals:
Increase the throughput of multiple small accesses (that is, page accesses)
by load balancing.
Reduce the response time of large accesses.
12.7.3 RAID Levels
Mirroring provides high reliability, but it is expensive. Striping provides high
data-transfer rates, but it does not improve reliability. Numerous schemes
to provide redundancy at lower cost by using disk striping combined with
  parity   bits (which we describe next) l1.ave been proposed. These schemes
have different cost-performance trade-offs and are classified according to
levels called We describe the various levels here; Figure 12.11
shows them pictorially (in the figure, P indicates error-correcting bits, and C
12.7 RAID Structure 525
(a) RAID 0: non-redundant striping.
(b) RAID 1: mirrored disks.
(c) RAID 2: memory-style error-correcting codes.
{d) RAID 3: bit-interleaved parity.
(e) RAID 4: block-interleaved parity.
{f) RAID 5: block-interleaved distributed parity.
(g) RAID 6: P + Q redundancy.
Figure 12.11 RAID levels.
indicates a second copy. of the data). In all cases depicted in the figure, four
disks' worth of data are stored, and the extra disks are used to store redundant
information for failure recovery.
  RAID level 0. RAID level 0 refers to disk arrays with striping at the level of
blocks but without any redundancy (such as mirroring or parity bits), as
shown in Figure 12.1l(a).
  RAID Ievell. RAID level1 refers to disk mirroring. Figure 12.1l(b) shows
a mirrored organization.
  ' RAID level2. RAID level2 is also known as memory-style error-correctingcode
(ECC) organization. Memory systems have long detected certain
errors by using parity bits. Each byte in a memory system may have a
parity bit associated with it that records whether the number of bits in the
byte set to 1 is even (parity= 0) or odd (parity= 1). If one of the bits in the
526 Chapter 12
byte is damaged (either a 1 becomes a 0, or a 0 becomes an the parity of
the byte changes and thus does not match the stored parity. Similarly, if the
stored parity bit is damaged, it does not match the computed parity. Thus,
all single-bit errors are detected by the menwry system .. Error-correcting
schemes store two or more extra bits and can reconstruct the data if a
single bit is damaged. The idea of ECC can be  used directly in disk arrays
via striping of bytes across disks. For example, the first bit of each byte can
be stored in disk 1, the second bit in disk 2, and so on until the eighth bit
is stored in disk 8; the error-correction bits are stored in further disks. This
scheme is shown pictorially in Figure 12.1l(c), where the disks labeled P
store the error-correction bits. If one of the disks fails, the remaining bits
of the byte and the associated error-correction bits can be read from other
disks and used to reconstruct the damaged data. Note that RAID level 2
requires only three disks' overhead for four disks of data, unlike RAID level
1, which requires four disks' overhead.
RAID level 3. RAID level 3, or
improves on level 2 by taking into account the fact that, unlike memory
systems, disk controllers can detect whether a sector has been read
correctly, so a single parity bit can be used for error correction as well as
for detection. The idea is as follows: If one of the sectors is damaged, we
know exactly which sector it is, and we can figure out whether any bit in
the sector is a 1 or a 0 by computing the parity of the corresponding bits
from sectors in the other disks. If the parity of the remaining bits is equal
to the stored parity, the missing bit is 0; otherwise, it is 1. RAID level3 is as
good as level 2 but is less expensive in the number of extra disks required
(it has only a one-disk overhead), so level 2 is not used in practice. This
scheme is shown pictorially in Figure 12.1l(d).
RAID level 3 has two advantages over level 1. First, the storage overhead
is reduced because only one parity disk is needed for several regular
disks, whereas one mirror disk is needed for every disk in level1. Second,
since reads and writes of a byte are spread out over multiple disks with
N-way striping of data, the transfer rate for reading or writing a single
block is N times as fast as with RAID level 1. On the negative side, RAID
level3 supports fewer l/Os per second, since every disk has to participate
in every I/0 request.
A further performance problem with RAID 3-and with all paritybased
RAID levels-is the expense of computing and writing the parity.
This overhead results in significantly slower writes than with non-parity
RAID arrays. To moderate this performance penalty, many RAID storage
arrays include a hardware controller with dedicated parity hardware. This
controller offloads the parity computation from the CPU to the array. The
array has an NVRAM cache as well, to store the blocks while the parity is
computed and to buffer the writes from the controller to the spindles. This
combination can make parity RAID almost as fast as non-parity. In fact, a
caching array doing parity RAID can outperform a non-caching non-parity
RAID.
RAID level 4. RAID level4, or uses
block-level striping, as in RAID 0, and in addition keeps a parity block on a
separate disk for corresponding blocks from N other disks. This scheme is
12.7 527
diagramed in Figure 12.1l(e). If one of the disks fails, the parity block can
be used with the corresponding blocks from the other disks to restore the
blocks of the failed disk.
A block read accesses only one disk, allowing other requests to be
processed by the other disks. Thus, the data-transfer rate for each access
is slowe1~ but multiple read accesses can proceed in parallel, leading to a
higher overall I/0 rate. The transfer rates for large reads are high, since all
the disks can be read in parallel; large writes also have high transfer rates,
since the data and parity can be written in parallel.
Small independent writes cannot be performed in parallel. An operatingsystem
write of data smaller than a block requires that the block be read,
modified with the new data, and written back The parity block has to be
updated as well. This is known as the   sYTi:.e . Thus, a
single write requires four disk accesses: two to read the two old blocks and
two to write the two new blocks.
WAFL (Chapter 11) uses RAID level4 because this RAID level allows disks
to be added to a RAID set seamlessly. If the added disks are initialized with
blocks containing all zeros, then the parity value does not change, and the
RAID set is still correct.
RAID levelS. RAID levelS, or , differs
from level 4 by spreading data and parity among all N + 1 disks, rather
than storing data in N disks and parity in one disk. For each block, one of
the disks stores the parity, and the others store data. For example, with an
array of five disks, the parity for the nth block is stored in disk (n mod 5)+ 1;
the nth blocks of the other four disks store actual data for that block This
setup is shown in Figure 12.11(f), where the Ps are distributed across all
the disks. A parity block cannot store parity for blocks in the same disk,
because a disk failure would result in loss of data as well as of parity, and
hence the loss would not be recoverable. By spreading the parity across
all the disks in the set, RAID 5 avoids potential overuse of a single parity
disk, which can occur with RAID 4. RAID 5 is the most common parity RAID
system.
RAID level 6. RAID level 6, also called the is
much like RAID level 5 but stores extra redundant information to guard
against disk failures. Instead of parity, error-correcting codes such
as the are used. In the scheme shown in Figure
12.11(g), 2 bits of redundant data are stored for every 4 bits of datacompared
with 1 parity bit in level 5-and the system can tolerate two
disk failures.
RAID levels 0 + 1 and 1 + 0. RAID level 0 + 1 refers to a combination of RAID
levels 0 and 1. RAID 0 provides the performance, while RAID 1 provides
the reliability. Generally, this level provides better performance than RAID
5. It is common in enviromnents where both performance and reliability
are important. Unfortunately, like RAID 1, it doubles the number of disks
needed for storage, so it is also relatively expensive. In RAID 0 + 1, a set
of disks are striped, and then the stripe is mirrored to another, equivalent
stripe.
528 Chapter 12
stripe
a) RAID 0 + 1 with a single disk failure.
uA
mirror
b) RAID 1 + 0 with a single disk failure.
Figure 12.12 RAID 0 + 1 and 1 + 0.
Another RAID option that is becoming available commercially is RAID
level 1 + 0, in which disks are mirrored in pairs and then the resulti.J.l.g
mirrored pairs are striped. This scheme has some theoretical advantages
over RAID 0 + 1. For example, if a single disk fails in RAID 0 + 1, an entire
stripe is inaccessible, leaving only the other stripe available. With a failure
in RAID 1 + 0, a single disk is unavailable, but the disk that mirrors it is still
available, as are all the rest of the disks (Figure 12.12).
Numerous variations have been proposed to the basic RAID schemes described
here. As a result, some confusion may exist about the exact definitions of the
different RAID levels.
The implementation of RAID is another area of variation. Consider the
following layers at which RAID can be implemented.
Volume-management software can implement RAID within the kernel or
at the system software layer. In this case, the storage hardware can provide
a minimum of features and still be part of a full RAID solution. Parity RAID
is fairly slow when implemented in software, so typically RAID 0, 1, or 0 +
1 is used.
RAID can be implemented in the host bus-adapter (HBA) hardware. Only
the disks directly connected to the HBA can be part of a given RAID set.
This solution is low in cost but not very flexible.
12.7 529
RAID can be implemented in the hardware of the storage array. The storage
array can create RAID sets of various levels and can even slice these sets
into smaller volumes, which are then presented to the operating system.
The operating system need only implement the file system on each of the
volumes. Arrays can have multiple connections available or can be part of
a SAN, allowing multiple hosts to take advantage of the array's features.
RAID can be implemented in the SAN interconnect layer by disk virtualization
devices. In this case, a device sits between the hosts and the storage.
It accepts commands from the servers and manages access to the storage.
It could provide mirroring, for example, by writing each block to two
separate storage devices.
Other features, such as and replication, can be implemented at
each of these levels as well. involves the automatic duplication of
writes between separate sites for redundancy and disaster recovery. Replication
can be synchronous or asynchronous. In synchronous replication, each block
must be written locally and remotely before the write is considered complete,
whereas in asynchronous replication, the writes are grouped together and
written periodically. Asynchronous replication can result in data loss if the
primary site fails, but it is faster and has no distance limitations.
The implementation of these features differs depending on the layer at
which RAID is implemented. For example, if RAID is implemented in software,
then each host may need to carry out and manage its own replication. If
replication is implemented in the storage array or in the SAN intercom1ect,
however, then whatever the host operating system or its features, the host's
data can be replicated.
One other aspect of most RAID implementations is a hot spare disk or disks.
A is not used for data but is configured to be used as a replacement in
case disk failure. For instance, a hot spare can be used to rebuild a mirrored
pair should one of the disks in the pair fail. In this way, the RAID level can be
reestablished automatically, without waiting for the failed disk to be replaced.
Allocating more than one hot spare allows more than one failure to be repaired
without human intervention.
12.7.4 Selecting a RAID Level
Given the many choices they have, how do system designers choose a RAID
level  One consideration is rebuild performance. If a disk fails, the time needed
to rebuild its data can be significant. This may be an important factor if a
continuous supply of data is required, as it is in high-performance or interactive
database systems. Furthermore, rebuild performance influences the mean time
to failure.
Rebuild performance varies with the RAID level used. Rebuilding is easiest
 or RAID level1, since data can be copied from another disk; for the other levels,
we need to access all the other disks in the array to rebuild data in a failed disk.
Rebuild times can be hours for RAID 5 rebuilds of large disk sets.
RAID level 0 is used in high-performance applications where data loss is
not critical. RAID level1 is popular for applications that require high reliability
with fast recovery. RAID 0 + 1 and 1 + 0 are used where both performance and
reliability are important-for example, for small databases. Due to RAID 1's
530 Chapter 12
THE InServ STORAGE ARRAY
Im1ovation, in an effort to provide better, faster, and less expensive solutions,
frequently blurs the lines that separated previous technologies. Consider the
InServ storage array from 3Par. Unlike most other storage arrays, InServ
does not require that a set of disks be configured at a specific RAID level.
Rather, each disk is broken into 256-MB   chunklets.   RAm is then applied at
the chunklet level. A disk can thus participate in multiple and various RAID
levels as its chunklets are used for multiple volumes.
InServ also provides snapshots similar to those created by the WAFL file
system. The format of InServ snapshots can be read-write as well as readonly,
allowing multiple hosts to mount copies of a given file system without
needing their own copies of the entire file system. Any changes a host makes
in its own copy are copy-on-write and so are not reflected in the other copies.
A further innovation is . Some file systems do not expand
or shrink On these systems, the original size is the only size, and any change
requires copying data. An administrator can configure InServ to provide a
host with a large amount of logical storage that initially occupies only a small
amount of physical storage. As the host starts using the storage, unused disks
are allocated to the host, up to the original logical level. The host thus can
believe that it has a large fixed storage space, create its file systems there, and
so on. Disks can be added or removed from the file system by InServ without
the file systems noticing the change. This feature can reduce the number of
drives needed by hosts, or at least delay the purchase of disks until they are
really needed.
high space overhead, RAID levelS is often preferred for storing large volumes
of data. Level6 is not supported currently by many RAID implementations, but
it should offer better reliability than levelS.
RAID system designers and administrators of storage have to make several
other decisions as well. For example, how many disks should be in a given
RAID set  How many bits should be protected by each parity bit  If more disks
are in an array, data-transfer rates are higher, but the system is more expensive.
If more bits are protected by a parity bit, the space overhead due to parity bits
is lower, but the chance that a second disk will fail before the first failed disk is
repaired is greater, and that will result in data loss.
12.7.5 Extensions
The concepts of RAID have been generalized to other storage devices, including
arrays of tapes, and even to the broadcast of data over wireless systems. When
applied to arrays of tapes, RAID structures are able to recover data even if one
of the tapes in an array is damaged. When applied to broadcast of data, a
block of data is split into short units and is broadcast along with a parity unit;
if one of the units is not received for any reason, it can be reconstructed from the
other units. Comrnonly, tape-drive robots containing multiple tape drives will
stripe data across all the drives to increase throughput and decrease backup
time.
12.7 531
12.7.6 Problems with RAID
Unfortunately, RAID does not always assure that data are available for the
operating system and its users. A pointer to a file could be wrong, for example,
or pointers within the file structure could be wrong. Incomplete writes, if not
properly recovered, could result in corrupt data. Some other process could
accidentally write over a file system's structures, too. RAID protects against
physical media errors, but not other hardware and software errors. As large as
is the landscape of software and hardware bugs, that is how numerous are the
potential perils for data on a system.
The Solaris ZFS file system takes an innovative approach to solving these
problems through the use of - a technique which is used to verify
the integrity of data. ZFS maintains internal checksums of all blocks, including
data and metadata. These checksums are not kept with the block that is being
checksummed. Rathel~ they are stored with the pointer to that block. (See
figure 12.13.) Consider an inode with pointers to its data. Within the inode is
the checksum of each block of data. If there is a problem with the data, the
checksum will be incorrect and the file system will know about it. If the data
are mirrored, and there is a block with a correct checksum and one with an
incorrect checksum, ZFS will automatically update the bad block with the good
one. Similarly, the directory entry that points to the inode has a checksum for the
inode. Any problem in the inode is detected when the directory is accessed.
This checksumming takes places throughout all ZFS structures, providing a
much higher level of consistency, error detection, and error correction than is
found in RAID disk sets or standard file systems. The extra overhead that is
created by the checksum calculation and extra block read-modify-write cycles
is not noticeable because the overall performance of ZFS is very fast.
Another issue with most RAID implementations is lack of flexibility.
Consider a storage array with twenty disks divided into four sets of five disks.
Each set of five disks is a RAID level 5 set. As a result, there are four separate
data 1
Figure 12.13 ZFS checksums all metadata and data.
532 Chapter 12
volumes, each holding a file system. But what if one file system is too large
to fit on a five-disk RAID level 5 set   And what if another file system needs
very little space  If such factors are known ahead of time, then the disks and
volumes can be properly allocated. Very frequently, however, disk use and
requirements change over time.
Even if the storage array allowed the entire set of twenty disks to be
created as one large RAID set other issues could arise. Several volumes of
various sizes could be built on the set. But some volume managers do not
allow us to change a volume's size. In that case, we would be left with the same
issue described above-mismatched file-system sizes. Some volume n  lanagers
allow size changes, but some file systems do not allow for file-system growth
or shrinkage. The volumes could change sizes, but the file systems would need
to be recreated to take advantage of those changes.
ZFS combines file-system management and volume management into a
unit providing greater functionality than the traditional separation of those
functions allows. Disks, or partitions of disks, are gathered together via RAID
sets into of storage. A pool can hold one or more ZFS file systems. The
entire pool's free space is available to all file systems within that pool. ZFS uses
the memory model of   malloc   and   free   to allocate and release storage for
each file system as blocks are used and freed within the file system. As a result
there are no artificial limits on storage use and no need to relocate file systems
between volumes or resize volumes. ZFS provides quotas to limit the size of a
file system and reservations to assure that a file system can grow by a specified
amount, but those variables may be changed by the file system owner at any
time. Figure 12.14(a) depicts traditional volumes and file systems, and Figure
12.14(b) shows the ZFS model.
I FS I
~ (a) Traditional volumes and file systems.
(b) ZFS and pooled storage.
Figure 12.14 (a) Traditional volumes and file systems. (b) A ZFS pool and file systems.
12.8
12.8 533
In Chapter 6, we introduced the write-ahead log, which requires the availability
of stable storage. By definition, information residing in stable storage is never
lost. To implement such storage, we need to replicate the required information
on multiple storage devices (usually disks) with independent failure modes.
We also need to coordinate the writing of updates in a way that guarantees
that a failure during an update will not leave all the copies in a damaged state
and that, when we are recovering from a failure, we can force all copies to a
consistent and correct value, even if another failure occurs during the recovery.
In this section, we discuss how to meet these needs.
A disk write results in one of three outcomes:
Successful completion. The data were written correctly on disk.
Partial failure. A failure occurred in the midst of transfer, so only some of
the sectors were written with the new data, and the sector being written
during the failure may have been corrupted.
Total failure. The failure occurred before the disk write started, so the
previous data values on the disk remain intact.
Whenever a failure occurs during writing of a block, the system needs to
detect it and invoke a recovery procedure to restore the block to a consistent
state. To do that, the system must maintain two physical blocks for each logical
block. An output operation is executed as follows:
Write the information onto the first physical block.
When the first write completes successfully, write the same inJormation
onto the second physical block.
Declare the operation complete only after the second write completes
successfully.
During recovery from a failure, each pair of physical blocks is examined.
If both are the same and no detectable error exists, then no further action is
necessary. If one block contains a detectable error, then we replace its contents
with the value of the other block. If neither block contains a detectable error,
but the blocks differ in content, then we replace the content of the first block
with that of the second. This recovery procedure ensures that a write to stable
storage either succeeds completely or results in no change.
We can extend this procedure easily to allow the use of an arbitrarily large
number of copies of each block of stable storage. Although having a large
number of copies further reduces the probability of a failure, it is usually
reasonable to simulate stable storage with only two copies. The data in stable
storage are guaranteed to be safe unless a failure destroys all the copies.
Because waiting for disk writes to complete (synchronous I/O) is time
consuming, many storage arrays add NVRAM as a cache. Since the memory is
nonvolatile (it usually has battery power to back up the unit's power), it can
be trusted to store the data en route to the disks. It is thus considered part of
534 Chapter 12
12.9
the stable storage. Writes to it are much faster than to disk, so performance is
greatly improved.
Would you buy a DVD or CD player that had one disk sealed inside  Of course
not. You expect to use a DVD or CD player with many relatively inexpensive
disks. On a computer as well, using many inexpensive cartridges with one
drive lowers the overall cost. Low cost is the defining characteristic of tertiary
storage, which we discuss in this section.
12.9.1 Tertiary-Storage Devices
Because cost is so important, in practice, tertiary storage is built with
The most common examples are floppy disks, tapes, and read-only,
write-once, and rewritable CDs and DVDs. Many any other kinds of tertiarystorage
devices are available as well, including removable devices that store
data in flash memory and interact with the computer system via a USB interface.
12.9.1.1 Removable Disks
Removable disks are one kind of tertiary storage. Floppy disks are an example
of removable magnetic disks. They are made from a thin, flexible disk coated
with magnetic material and enclosed in a protective plastic case. Although
common floppy disks can hold only about 1 MB, similar technology is used
for removable magnetic disks that hold more than 1 GB. Removable magnetic
disks can be nearly as fast as hard disks, although the recording stuface is at
greater risk of from scratches.
A is another kind of removable disk. It records data
on a rigid platter coated with magnetic material, but the recording technology
is quite different from that for a magnetic disk. The magneto-optic head flies
much farther from the disk surface than a magnetic disk head does, and the
magnetic material is covered with a thick protective layer of plastic or glass.
This arrangement makes the disk much more resistant to head crashes.
The magneto-optic disk drive has a coil that produces a magnetic field; at
room temperature, the field is too large and too weak to magnetize a bit on the
disk. To write a bit, the disk head flashes a laser beam at the disk surface. The
laser is aimed at a tiny spot where a bit is to be written. The laser heats this
spot, which makes the spot susceptible to the magnetic field. Now the large,
weak magnetic field can record a tiny bit.
The magneto-optic head is too far from the disk surface to read the data by
detecting the tiny magnetic fields in the way that the head of a hard disk does.
Instead, the drive reads a bit using a property of laser light called the
When a laser beam is bounced off of a magnetic spot, the polarization
of the laser beam is rotated clockwise or counterclockwise, dependin~g on the
orientation of the magnetic field. This rotation is what the head detects to read
a bit.
Another category of removable disk is the Optical disks do not
use magnetism at all. Instead, they use special materials that can be altered by
laser light to have relatively dark or bright spots. One exarnple of optical-disk
12.9 535
technology is the which is coated with a material that can
freeze into either a crystalline or an amorphous state. The crystalline state is
more transparent, and hence a laser beam is brighter when it passes through
the lTlaterial and bounces off the reflective layer. The phase-change drive uses
laser light at three different powers: low power to read data, medium power
to erase the disk by melting and refreezing the recording medium into the
crystalline state, and high power to melt the medium into the amorphous state
to write to the disk. The most common examples of this technology are the
re-recordable CD-RW and DVD-RW.
The kinds of disks just described can be used over and over. They are called
In contrast, can
be written only once. An old way to make a WORM disk is to manufacture a thin
aluminum film sandwiched between two glass or plastic platters. To write a
bit, the drive uses a laser light to burn a small hole through the aluminum. This
burning cannot be reversed. Although it is possible to destroy the information
on a WORM disk by burning holes everywhere, it is virtually impossible to alter
data on the disk, because holes can only be added, and the ECC code associated
with each sector is likely to detect such additions. WORM disks are considered
durable and reliable because the metal layer is safely encapsulated between
the protective glass or plastic platters and magnetic fields cannot damage the
recording. A newer write-once technology records on an organic polymer dye
instead of an aluminum layer; the dye absorbs laser light to form marks. This
technology is used in the recordable CD-R and DVD-R.
Read-oniv such as CD-ROM and DVD-ROM, come from the factory
with the data prerecorded. They use technology similar to that of WORM disks
(although the bits are pressed, not burned), and they are very durable.
Most removable disks are slower than their nonremovable counterparts.
The writing process is slower, as are rotation and sometimes seek time.
12.9.1.2 Tapes
Magnetic tape is another type of removable medium. As a general rule, a tape
holds more data than an optical or magnetic disk cartridge. Tape drives and
disk drives have similar transfer rates. But random access to tape is much
slower than a disk seek, because it requires a fast-forward or rewind operation
that takes tens of seconds or even minutes.
Although a typical tape drive is more expensive than a typical disk drive,
the price of a tape cartridge is lower than the price of the equivalent capacity
of magnetic disks. So tape is an economical medium for purposes that do not
require fast random access. Tapes are commonly used to hold backup copies
of disk data. They are also used in large supercomputer centers to hold the
enornwus volumes of data used in scientific research and by large commercial
enterprises.
Large tape installations typically use robotic tape changers that move tapes
between tape drives and storage slots in a tape library. These mechanisms give
the computer automated access to many tape cartridges.
A robotic tape library can lower the overall cost of data storage. A diskresident
file that will not be needed for a while can be to tape, where
the cost per gigabyte is lower; if the file is needed in the future, the computer
can it back into disk storage for active use. A robotic tape library is
536 Chapter 12
sometimes called storage, since it is between the high performance
of on-line magnetic disks and the low cost of off-line tapes sitting on shelves
in a storage room.
12.9.1.3 Future Technology
In the future, other storage technologies may become important. Sometimes old
technologies are used in new ways, as economics change or the technologies
evolve. For example, solid-state disks, or are growing in importance and
becoming more common. Simply described, an SSD is a disk that is used like
a hard drive. Depending on the memory technology used, it can be volatile
or nonvolatile. The memory technology also affects performance. Nonvolatile
SSDs have the same characteristics as traditional hard disks but can be more
reliable because they have no moving parts and faster because they have no
seek time or latency. In addition, they use less energy. However, they are more
expensive per megabyte than traditional hard disks, have lower capacity than
the larger hard disks, and may have shorter life-spans than hard disks; so their
uses are limited. In one example, SSDs are being used in storage arrays to hold
metadata which requires high-performance such as the journal of a journaling
file system. SSDs are also being added to notebook computers to make them
smaller, faster, and more energy efficient.
Another promising storage technology, bologt;;:phk uses laser
light to record holographic photographs on special media. We can think of a
hologram as a three-dimensional array of pixels. Each pixel represents one bit:
0 for black or 1 for white. And all the pixels in a hologram are transferred in one
flash of laser light, so the data transfer rate is extremely high. With continued
development, holographic storage may become commercially viable.
Another technology under active research is based on
(IV!E\1S). The idea is to apply the fabrication
technologies that produce electronic chips to the manufacture of small datastorage
machines. One proposal calls for the fabrication of an array of 10,000
tiny disk heads, with a square centimeter of magnetic storage material suspended
above the array. When the storage material is moved lengthwise over
the heads, each head accesses its own linear track of data on the material. The
storage material can be shifted sideways slightly to enable all the heads to
access their next track. Although it remains to be seen whether this technology
can be successful, it may provide a nonvolatile data-storage technology that is
faster than magnetic disk and cheaper than semiconductor DRAM.
Whether the storage medium is a removable magnetic disk, a DVD, or a
magnetic tape, the operating system needs to provide several capabilities to use
removable media for data storage. These capabilities are discussed in Section
12.9.2.
12.9.2 Operating-System Support
Two major jobs of an operating system are to manage physical devices and
to present a virtual machine abstraction to applications. In this chapter, we
have seen that, for hard disks, the operating system provides two abstractions.
One is the raw device, which is just an array of data blocks. The other is a file
system. For a file system on a magnetic disk, the operating system queues and
12.9 537
schedules the interleaved requests from several applications. Now, we shall see
how the operating system does its job when the storage media are removable.
12.9.2.1 Application Interface
Most operating systems can handle removable disks almost exactly as they do
fixed disks. When a blank cartridge is inserted into the drive (or mounted), the
cartridge must be formatted, and then an empty file system is generated on the
disk. This file system is used just like a file system on a hard disk
Tapes are often handled differently. The operating system usually presents
a tape as a raw storage medium. An application does not open a file on the
tape; it opens the whole tape drive as a raw device. Usually, the tape drive
is then reserved for the exclusive use of that application until the application
exits or closes the tape device. This exclusivity makes sense, because random
access on a tape can take tens of seconds, or even a few minutes, so interleaving
random accesses to tapes from more than one application would be likely to
cause thrashing.
When the tape drive is presented as a raw device, the operating system
does not provide file-system services. The application must decide how to use
the array of blocks. For instance, a program that backs up a hard disk to tape
might store a list of file names and sizes at the beginning of the tape and then
copy the data of the files to the tape in that order.
It is easy to see the problems that can arise from this way of using tape.
Since every application makes up its own rules for how to organize a tape,
a tape full of data can generally be used only by the program that created it.
For instance, even if we know that a backup tape contains a list of file names
and file sizes followed by the file data, we will still find it difficult to use the
tape. How exactly are the file names stored  Are the file sizes in binary or ASCII
form  Are the files written one per block, or are they all concatenated in one
tremendously long string of bytes  We do not even know the block size on the
tape, because this variable is generally one that can be chosen separately for
each block written.
For a disk drive, the basic operations are read(), write(), and seek().
Tape drives have a different set of basic operations. Instead of seek(), a tape
drive uses the locate() operation. The tape locate() operation is more
precise than the disk seek() operation, because it positions the tape to a
specific logical block, rather than an entire track. Locating to block 0 is the
same as rewinding the tape.
For most kinds of tape drives, it is possible to locate to any block that has
been written on a tape. In a partly filled tape, however, it is not possible to
locate into the empty space beyond the written area, because most tape drives
do not manage their physical space in the same way disk drives do. For a disk
drive, the sectors have a fixed size, and the formatting process must be used to
place empty sectors in their final positions before any data can be written. Most
tape drives have a variable block size, and the size of each block is detern  ined
on the fly, when that block is written. If an area of defective tape is encountered
during writing, the bad area is skipped and the block is written again. This
operation explains why it is not possible to locate into the empty space beyond
the written area -the positions and numbers of the logical blocks have not yet
been detennined.
538 Chapter 12
Most tape drives have a read_position() operation that returns the
logical block number where the tape head is currently located. Many tape
drives also support a space() operation for relative motion. So, for example,
the operation space ( -2) would locate backward over two logical blocks.
For most kinds of tape drives, writing a block has the side effect of logically
erasing everything beyond the position of the write. In practice, this side effect
means that most tape drives are append-only devices, because updating a
block in the middle of the tape also effectively erases everything beyond that
block. The tape drive implements this appending by placing an end-of-tape
(EOT) mark after a block that is written. The drive refuses to locate past the EOT
mark, but it is possible to locate to the EOT and then start writing. Doing so
overwrites the old EOT mark and places a new one at the end of the new blocks
just written.
In principle, a file system can be implemented on a tape. But many of the
file-system data structures and algorithms would be different from those used
for disks, because of the append-only property of tape.
12.9.2.2 File Naming
Another question that the operating system needs to handle is how to name
files on removable media. For a fixed disk, naming is not difficult. On a PC, the
file name consists of a drive letter followed by a path name. In UNIX, the file
name does not contain a drive letter, but the moLmt table enables the operating
system to discover on what drive the file is located. If the disk is removable,
however, knowing what drive contained the cartridge at some time in the past
does not mean knowing how to find the file. If every removable cartridge in
the world had a different serial number, the name of a file on a removable
device could be prefixed with the serial number, but to ensure that no two
serial numbers are the same would require each one to be about 12 digits in
length. Who could remember the names of her files if she had to memorize a
12-digit serial number for each one 
The problem becomes even more difficult when we want to write data
on a removable cartridge on one computer and then use the cartridge in
another computer. If both machines are of the same type and have the same
kind of removable drive, the only difficulty is knowing the contents and data
layout on the cartridge. But if the machines or drives are different, many
additional problems can arise. Even if the drives are compatible, different
computers may store bytes in different orders and may use different encodings
for binary numbers and even for letters (such as ASCII on PCs versus EBCDIC
on mainframes).
Today's operating systems generally leave the name-space problem
unsolved for removable media and depend on applications and users to figure
out how to access and interpret the data. Fortunately, a few kinds of removable
media are so well standardized that all computers use them the same way. One
example is the CD. Music CDs use a universal format that is understood by any
CD drive. Data CDs are available in only a few different formats, so it is usual
for a CD drive and the operating-system device driver to be programmed to
handle all the comn1on formats. DVD fonnats are also well standardized.
12.9 539
12.9.2.3 Hierarchical Storage Management
A JU enables the computer to change the removable cartridge in a
tape or disk drive without human assistance. Two major uses of this technology
are for backups and hierarchical storage systems. The use of a jukebox for
backups is simple: When one cartridge becomes full, the computer instructs
the jukebox to switch to the next cartridge. Some jukeboxes hold tens of drives
and thousands of cartridges, with robotic arms managing the movement of
tapes to the drives.
A hierarchical storage system extends the storage hierarchy beyond
primary memory and secondary storage (that is, magnetic disk) to incorporate
tertiary storage. Tertiary storage is usually implemented as a jukebox of tapes
or removable disks. This level of the storage hierarchy is larger, cheaper, and
slower.
Although the virtual memory system can be extended in a straightforward
manner to tertiary storage, this extension is rarely carried out in practice. The
reason is that a retrieval from a jukebox can take tens of seconds or even
minutes, and such a long delay is intolerable for demand paging and for other
forms of virtual memory use.
The usual way to incorporate tertiary storage is to extend the file system.
Small and frequently used files remain on magnetic disk, while large and old
files that are not actively used are archived to the jukebox. In some file-archiving
systems, the directory entry for the file continues to exist, but the contents of
the file no longer occupy space in secondary storage. If an application tries to
open the file, the open () system call is suspended until the file contents can
be staged in from tertiary storage. When the contents are again available from
magnetic disk, the open () operation returns control to the application, which
proceeds to use the disk-resident copy of the data.
Today, is usually found in installations
that have large volumes of data that are used seldom, sporadically,
Current work in HSM includes extending it to provide full
Here, data move from disk to tape
and back to disk, as needed, but are deleted on a schedule or according to
policy. For example, some sites save e-mail for seven years but want to be sure
that at the end of seven years it is destroyed. At that point, the data might be
on disk, HSM tape, and backup tape. ILM centralizes knowledge of where the
data are so that policies can be applied across all these locations.
12.9.3 Performance Issues
As with any component of the operating system, the three most important
aspects of tertiary-storage performance are speed, reliability, and cost.
12.9.3.1 Speed
The speed of tertiary storage has two aspects: bandwidth and latency. We
measure the bandwidth in bytes per second. The  ~'L' is
the average data rate during a transfer-that is, the number of bytes
divided by the transfer time. The calculates the average
over the entire l/0 time, including the time for seek() or locate() and any
540 Chapter 12
cartridge-switching time in a jukebox. In essence, the sustained bandwidth is
the rate at which the data stream actually flows, and the effective bandwidth is
the overall data rate provided by the drive. The bandwidth of a drive is generally
understood to mean the sustained bandwidth.
For removable disks, tlce bandwidth ranges from a few megabytes per
second for the slowest to over 40 MB per second for the fastest. Tapes have a
similar range of bandwidths, from a few megabytes per second to over 30MB
per second.
The second aspect of speed is the . By this performance
measure, disks are much faster than tapes. Disk storage is essentially twodimensional-
all the bits are out in the open. A disk access simply moves the
ann to the selected cylinder and waits for the rotational latency, which may
take less than 5 milliseconds. By contrast, tape storage is three-dimensional.
At any time, a small portion of the tape is accessible to the head, whereas most
of the bits are buried below hundreds or thousands of layers of tape wound
on the reel. A random access on tape requires winding the tape reels until
the selected block reaches the tape head, which can take tens or hundreds of
seconds. So we can generally say that random access within a tape cartridge is
more than a thousand times slower than random access on disk.
If a jukebox is involved, the access latency can be significantly higher. For
a removable disk to be changed, the drive must stop spinning, then the robotic
arm must switch the disk cartridges, and then the drive must spin up the new
cartridge. This operation takes several seconds-about a hundred times longer
than the random-access time within one disk. So switching disks in a jukebox
incurs a relatively high performance penalty.
For tapes, the robotic-ann time is about the same as for disks. But for tapes
to be switched, the old tape generally must rewind before it can be ejected, and
that operation can take as long as 4 minutes. And, after a new tape is loaded
into the drive, many seconds can be required for the drive to calibrate itself
to the tape and to prepare for I/0. Although a slow tape jukebox can have a
tape-switch time of 1 or 2 minutes, this time is not enormously greater than the
random-access time within one tape.
To generalize, we can say that random access in a disk jukebox has a
latency of tens of seconds, whereas random access in a tape jukebox has a
latency of hundreds of seconds; switching tapes is expensive, but switching
disks is not. We must be careful not to overgeneralize, though. Some expensive
tape jukeboxes can rewind, eject, load a new tape, and fast-forward to a random
item of data all in less than 30 seconds.
If we pay attention to only the performance of the drives in a jukebox,
the bandwidth and latency seem reasonable. But if we focus our attention
on the cartridges instead, we find a terrible bottleneck. Consider first the
bandwidth. The bandwidth-to-storage-capacity ratio of a robotic library is
much less favorable than that of a fixed disk. To read all the data stored on
a large hard disk could take about an hour. To read all the data stored in a
large tape library could take years. The situation with respect to access latency
is nearly as bad. To illustrate, if 100 requests are queued for a disk drive,
the average waiting time will be about a second. If 100 requests are queued
for a tape library, the average waiting time could be over an hour. The low
cost of tertiary storage results from having many cheap cartridges share a few
expensive drives. But a removable library is best devoted to the storage of
12.9 541
infrequently used data, because the library can satisfy only a relatively small
number of I/0 requests per hour.
12.9.3.2 Reliability
Although we often think good pe1jormance means high speed, another important
aspect of performance is reliability. If we try to read some data and are unable
to do so because of a drive or media failure, for all practical purposes the access
time is infinitely long and the bandwidth is infinitely small. So it is important
to understand the reliability of removable media.
Removable n1.ag:netic disks are somewhat less reliable than are fixed hard
disks, because they are more likely to be exposed to harmful environmental
conditions such as dust, large changes in temperature and humidity, and
mechanical forces such as shock and bending. Optical disks are considered
very reliable, because the layer that stores the bits is protected by a transparent
plastic or glass layer. The reliability of magnetic tape varies widely, depending
on the kind of drive. Some inexpensive drives wear out tapes after a few dozen
uses; other drives are gentle enough to allow millions of reuses. By comparison
with a magnetic-disk head, the head in a magnetic-tape drive is a weak spot.
A disk head flies above the media, but a tape head is in close contact with
the tape. The scrubbing action of the tape can wear out the head after a few
thousands or tens of thousands of hours.
In summary, we can say that a fixed-disk drive is likely to be more reliable
than a removable-disk or tape drive, and an optical disk is likely to be more
reliable than a magnetic disk or tape. But a fixed magnetic disk has one
weakness. A head crash in a hard disk generally destroys the data, whereas
the failure of a tape drive or optical-disk drive often leaves the data cartridge
unharmed.
12.9.3.3 Cost
Storage cost is another important factor. Here is a concrete example of how
removable media may lower the overall storage cost. Suppose that a hard disk
that holds X GB has a price of $200; of this amom1.t, $190 is for the housing,
motor, and controller, and $10 is for the magnetic platters. The storage cost
for this disk is $200/ X per gigabyte. Now, suppose that we can manufacture
the platters in a removable cartridge. For one drive and 10 cartridges, the total
price is $190 + $100, and the capacity is lOX GB, so the storage cost is $291 X per
gigabyte. Even if it is a little more expensive to make a removable cartridge,
the cost per gigabyte of removable storage may well be lower than the cost per
gigabyte of a hard disk, because the expense of one drive is averaged with the
low price of many removable cartridges.
Figures 12.15, 12.16, and 12.17 show cost trends per megabyte for DRAM
memory, magnetic hard disks, and tape drives. The prices in the graphs are
the lowest prices found in advertisements in various computer magazines and
on the World Wide Web at the end of each year. These prices reflect the smallcomputer
marketplace of the readership of these magazines, where prices are
low by comparison with the mainframe and minicomputer markets. In the case
of tape, the price is for a drive with one tape. The overall cost of tape storage
becomes much lower as more tapes are purchased for use with the drive,
542 Chapter 12
co
::;;;
&:5-
160
80
40
20
10
1.2
0.8
0.4
64
KB
32
128MB
512MB
2GB
0  02 --'-c-19=':,8.,.-2 -1-  98-cc4--:-19=':c8-=-6 -1~9'::-:88---,19=':c9-=-o -1c-:'91'::-:92--:1 c:':99-4 -c:c19L96---,19c:':9-=-8 -2.,-,o'::-:oo--:2c:':oo-=-2 -2::-::0L.04:--::2c:':oo-=-6 - :'2oos
Year
Figure 12.  15 Price per megabyte of DRAM, from 1981 to 2008.
because the price of a tape is a small fraction of the price of the drive. However,
in a huge tape library containing thousands of cartridges, the storage cost is
dominated by the cost of the tape cartridges. As of 2004, the cost per GB of tape
cartridges was around $.40.
As Figure 12.15 shows, the cost of DRAM fluctuates widely. In the period
from 1981 to 2004, we can see three price crashes (around 1981, 1989, and
1996) as excess production caused a glut in the marketplace. We can also
see two periods (around 1987 and 1993) where shortages in the marketplace
caused sigrtificant price increases. In the case of hard disks (Figure 12.16), the
price decline has been steadier. Tape-drive prices also fell steadily up to 1997
(Figure 12.17). Since 1997, the price per gigabyte of inexpensive tape drives
has ceased its dramatic fall, although the price of mid-range tape technology
(such as DAT /DDS) has continued to fall and is now approaching that of the
co
::;;;
  '
100
50
20
5
2
0.5
0.2
0.05
0.02-
0.004
0.001
0.0005
0.0002
10
20
1982 1984 1986
120
1.2
2
1988 1990 1992
19
GB
GB
GB
1994 1996 1998 2000 2002 2004 2006 2008
Year
Figure 12.16 Price per megabyte of magnetic hard disk, from 1981 to 2008.
12.10
OJ
~
12.10
40
20
8- 60
120
1.2
0.5-
0.1
72GB
0.025
320GB
0.01 320GB
0.0051~9c-c84-1-:L98-:-6 -19:':-88c---cc19~90c---:c19~92:--c-c19L94-c-c19L96,---.,-c19'cc98-2,-JOOc-c0--c2,-J.00,-,-2--:~--:.,J=~2008
Year
Figure 12.17 Price per megabyte of a tape drive, from 1984 to 2008.
543
in.expensive drives. Tape-drive prices are not shown for years prior to 1984,
because, as mentioned, the magazines used in tracking prices are targeted to
the small-computer marketplace, and tape drives were not widely used with
small computers prior to 1984.
We can see from these graphs that the cost of storage has fallen dramatically.
By comparing the graphs, we can also see that the price of disk storage has
plummeted relative to the price of DRAM and tape.
The price per megabyte of magnetic disk storage improved by more than
four orders of magnitude from 1981 to 2004, whereas the corresponding
improvement for main memory was only three orders of magnitude. Main
memory today is more expensive than disk storage by a factor of 100.
The price per megabyte dropped much more rapidly for disk drives than
for tape drives as well. In. fact, the price per megabyte of a magnetic disk drive
is approaching that of a tape cartridge without the tape drive. Consequently,
small- and medium-sized tape libraries have a higher storage cost than disk
systems with equivalent capacity.
The dramatic fall in disk prices has largely rendered tertiary storage
obsolete. We no longer have any tertiary storage technology that is orders
of magnitude less expensive than magnetic disk. It appears that the revival
of tertiary storage must await a revolutionary technology breakthrough.
Meanwhile, tape storage will find its use mostly limited to purposes such
as backups of disk drives and archival storage in enormous tape libraries that
greatly exceed the practical storage capacity of large disk farms.
Disk drives are the major secondary-storage I/0 devices on most computers.
Most secondary storage devices are either magnetic disks or n1.agnetic tapes.
Modern disk drives are structured as large one-dimensional arrays of logical
disk blocks. Generally, these logical blocks are 512 bytes in size. Disks may be
attached to a computer system in one of two ways: (1) through the local I/0
ports on the host computer or (2) through a network cmmection.
544 Chapter 12
Requests for disk I/0 are generated by the file system and by the virtual
memory system. Each request specifies the address on the disk to be referenced,
in the form of a logical block number. Disk-schedLiling algorithms can improve
the effective bandwidth, the average response time, and the variance in
response time. Algorithms such as SSTF, SCAN, C-SCAN, LOOK, and C-LOOK
are designed to make such improvements through strategies for disk-queue
ordering.
Performance can be harmed by external fragmentation. Some systems
have utilities that scan the file system to identify fragmented files; they then
move blocks around to decrease the fragmentation. Defragmenting a badly
fragmented file system can significantly improve performance, but the systenc
may have reduced performance while the defragmentation is in progress.
Sophisticated file systems, such as the UNIX Fast File System, incorporate
many strategies to control fragmentation during space allocation so that disk
reorganization is not needed.
The operating system manages the disk blocks. First, a disk must be lowlevel-
formatted to create the sectors on the raw hardware-new disks usually
come preformatted. Then, the disk is partitioned, file systems are created, and
boot blocks are allocated to store the system's bootstrap program. Finally, when
a block is corrupted, the system must have a way to lock out that block or to
replace it logically with a spare.
Because an efficient swap space is a key to good performance, systems
usually bypass the file system and use raw disk access for paging I/0. Some
systems dedicate a raw disk partition to swap space, and others use a file
within the file system instead. Still other systems allow the user or system
administrator to make the decision by providing both options.
Because of the amount of storage required on large systems, disks are
frequently made redundant via RAID algorithms. These algorithms allow more
than one disk to be used for a given operation and allow continued operation
and even automatic recovery in the face of a disk failure. RAID algorithms
are organized into different levels; each level provides some combination of
reliability and high transfer rates.
The write-ahead log scheme requires the availability of stable storage.
To implement such storage, we need to replicate the needed information on
multiple nonvolatile storage devices (usually disks) with independent failure
modes. We also need to update the information in a controlled manner to
ensure that we can recover the stable data after any failure during data transfer
or recovery.
Tertiary storage is built from disk and tape drives that use removable
media. Many different technologies are available, including magnetic tape,
removable magnetic and magneto-optic disks, and optical disks.
For removable disks, the operating system generally provides the full
services of a file-system interface, including space management and requestqueue
scheduling. For many operating systems, the name of a file on a
removable cartridge is a combination of a drive name and a file name within
that drive. This convention is simpler but potentially more confusing than is
using a name that identifies a specific cartridge.
For tapes, the operating system generally provides only a raw interface.
Many operating systems have no built-in support for jukeboxes. Jukebox
545
support can be provided by a device driver or by a privileged application
designed for backups or for HSM.
Three important aspects of performance are bandwidth, latency, and
reliability. Many bandwidths are available for both disks and tapes, but the
random-access latency for a tape is generally much greater than that for a disk.
Switching cartridges in a jukebox is also relatively slow. Because a jukebox
has a low ratio of drives to cartridges, reading a large fraction of the data in a
jukebox can take a long time. Optical media, which protect the sensitive layer
with a transparent coating, are generally more robust than magnetic media,
which are more likely to expose the magnetic material to physical damage.
Lastly, the cost of storage has decreased greatly in the past two decades, most
notably for disk storage.
12.1 What would be the effects on cost and performance if tape storage had
the same areal density as disk storage  (Areal density is the number of
gigabits per square inch.)
12.2 It is sometimes said that tape is a sequential-access medium, whereas
a magnetic disk is a random-access medium. In fact the suitability
of a storage device for random access depends on the transfer size.
The term streaming transfer rate denotes the rate for a data transfer
that is underway, excluding the effect of access latency. By contrast, the
effective transfer rate is the ratio of total bytes per total seconds, including
overhead time such as access latency.
Suppose that, in a computer, the level-2 cache has an access latency
of 8 nanoseconds and a streaming transfer rate of 800 megabytes per
second, the main memory has an access latency of 60 nanoseconds and
a streaming transfer rate of 80 megabytes per second, the magnetic disk
has an access latency of 15 milliseconds and a streaming transfer rate
of 5 megabytes per second, and a tape drive has an access latency of 60
seconds and a streaming transfer rate of 2 megabytes per seconds.
a. Random access causes the effective transfer rate of a device to
decrease, because no data are transferred during the access time.
For the disk described, what is the effective transfer rate if an
average access is followed by a streaming transfer of (1) 512 bytes,
(2) 8 kilobytes, (3) 1 megabyte, and (4) 16 megabytes 
b. The utilization of a device is the ratio of effective transfer rate to
streaming transfer rate. Calculate the utilization of the disk drive
for each of the four transfer sizes given in part a.
c. Suppose that a utilization of 25 percent (or higher) is considered
acceptable. Using the performance figures given, compute the
smallest transfer size for disk that gives acceptable utilization.
546 Chapter 12
d. Complete the following sentence: A disk is a random-access
device for transfers larger than ______ bytes and is a sequentialaccess
device for s1naller transfers.
e. Compute the minimum transfer sizes that give acceptable utilization
for cache, memory, and tape.
f. When is a tape a random-access device, and when is it a
sequential-access device 
12.3 The reliability of a hard-disk drive is typically described in terms of a
quantity called mean time between failures (MTBF). Although this quantity
is called a   time,   the MTBF actually is measured in drive-hours per
failure.
a. If a system contains 1,000 disk drives, each of which has a 750,000-
hour MTBF, which of the following best describes how often a
drive failure will occur in that disk farm: once per thousand
years, once per century, once per decade, once per year, once per
month, once per week, once per day, once per hour, once per
minute, or once per second 
b. Mortality statistics indicate that, on the average, a U.S. resident
has about 1 chance in 1,000 of dying between the ages of 20 and 21.
Deduce the MTBF hours for 20-year-olds. Convert this figure from
hours to years. What does this MTBF tell you about the expected
lifetime of a 20-year-old 
c. The manufacturer guarantees a 1-million-hour MTBF for a certain
model of disk drive. What can you conclude about the number of
years for which one of these drives is under warranty 
12.4 Discuss how an operating system could maintain a free-space list
for a tape-resident file system. Assume that the tape technology is
append-only and that it uses EOT marks and locate, space, and read
position commands as described in Section 12.9.2.1.
12.5 Imagine that a holographic storage drive has been invented. The drive
costs $10,000 and has an average access time of 40 milliseconds. It uses
a $100 cartridge the size of a CD. This cartridge holds 40,000 images,
and each image is a square black-and-white picture with a resolution
of 6, 000 x 6, 000 pixels (each pixel stores 1 bit). The drive can read or
write one picture in 1 millisecond. Answer the following questions.
a. What would be some good uses for this device 
b. How would this device affect the l/0 performance of a computing
system 
c. What kinds of storage devices, if any, would become obsolete as
a result of the invention of this device 
547
12.6 The term   Fast Wide SCSI-II   denotes a SCSI bus that operates at a
data rate of 20 megabytes per second when it moves a packet of bytes
between the host and a device. Suppose that a Fast Wide SCSI-II disk
drive spins at 7,200 RPM, has a sector size of 512 bytes, and holds 160
sectors per track
a. Estimate the sustained transfer rate of this drive in megabytes per
second.
b. Suppose that the drive has 7,000 cylinders, 20 tracks per cylinde1~
a head-switch time (from one platter to another) of 0.5 millisecond,
and an adjacent-cylinder seek time of 2 milliseconds. Use
this additional information to give an accurate estimate of the
sustained transfer rate for a huge transfer.
c. Suppose that the average seek time for the drive is 8 milliseconds.
Estimate the I/0 operations per second and the effective transfer
rate for a random-access workload that reads individual sectors
that are scattered across the disk
d. Calculate the random-access I/0 operations per second and
transfer rate for I/0 sizes of 4 kilobytes, 8 kilobytes, and 64
kilobytes.
e. If multiple requests are in the queue, a scheduling algorithm such
as SCAN should be able to reduce the average seek distance. Suppose
that a random-access workload is reading 8-kilobyte pages,
the average queue length is 10, and the scheduling algorithm
reduces the average seek time to 3 milliseconds. Now calculate
the I/0 operations per second and the effective transfer rate of the
drive.
12.7 Compare the performance of write operations achieved by a RAID level
5 organization with that achieved by a RAID level1 organization.
12.8 Suppose that a disk drive has 5,000 cylinders, numbered 0 to 4999. The
drive is currently serving a request at cylinder 143, and the previous
request was at cylinder 125. The queue of pending requests, in FIFO
order, is:
86,1470,913,1774,948,1509,1022,1750,130
Starting from the current head position, what is the total distance (in
cylinders) that the disk arm moves to satisfy all the pending requests
for each of the following disk-scheduling algorithms 
a. FCFS
b. SSTF
548 Chapter 12
c. SCAN
d. LOOK
e. C-SCAN
f. C-LOOK
12.9 Elementary physics states that when an object is subjected to a constant
acceleration a, the relationship between distance d and time t is given
by d = ~at2 . Suppose that, during a seek, the disk in Exercise 12.8
accelerates the disk arm at a constant rate for the first half of the seek,
then decelerates the disk arm at the same rate for the second half of the
seek. Assume that the disk can perform a seek to an adjacent cylinder
in 1 n  lillisecond and a full-stroke seek over all 5,000 cylinders in 18
milliseconds.
a. The distance of a seek is the number of cylinders that the head
moves. Explain why the seek time is proportional to the square
root of the seek distance.
b. Write an equation for the seek time as a function of the seek
distance. This equation should be of the form t = x + y~,
where t is the time in milliseconds and L is the seek distance in
cylinders.
c. Calculate the total seek time for each of the schedules in Exercise
12.8. Determine which schedule is the fastest (has the smallest
total seek time).
d. The percentage speedup is the time saved divided by the original
time. What is the percentage speedup of the fastest schedule over
FCFS 
12.10 The accelerating seek described in Exercise 12.9 is typical of hard-disk
drives. By contrast, floppy disks (and many hard disks manufactured
before the mid-1980s) typically seek at a fixed rate. Suppose that the
disk in Exercise 12.9 has a constant-rate seek rather than a constantacceleration
seek, so the seek time is of the form t = x + yL, where t
is the time in milliseconds and L is the seek distance. Suppose that the
time to seek to an adjacent cylinder is 1 millisecond, as before, and the
time to seek to each additional cylinder is 0.5 milliseconds.
a. Write an equation for this seek time as a function of the seek
distance.
b. Using this seek-time function, calculate the total seek time  or
each of the schedules in Exercise 12.8. Is your answer the same as
the one  or Exercise 12.9(c) 
549
c. What is the percentage speedup of the fastest scb.edule over FCFS
in this case 
12.11 Suppose that the disk in Exercise 12.9 rotates at 7,200 RPM.
a. What is the average rotational latency of this disk drive 
b. What seek distance can be covered in the tim.e that you found  or
part a 
12.12 Suppose that a one-sided 5.25-inch optical-disk cartridge has an areal
density of 1 gigabit per square inch. Further suppose that a magnetic
tape has an areal density of 20 megabits per square inch and is 1/2
inch wide and 1,800 feet long. Calculate an estimate of the storage
capacities of these two kinds of storage media. Suppose that an optical
tape exists that has the same physical size as the magnetic tape but the
same storage density as the optical disk. What volume of data could
the optical tape hold  What would be a marketable price for the optical
tape if the magnetic tape cost $25 
12.13 Write a program that simulates the disk-scheduling algorithms discussed
in Section 12.4.
12.14 Why is rotational latency usually not considered in disk scheduling 
How would you modify SSTF, SCAN, and C-SCAN to include latency
optimization 
12.15 Remapping bad blocks by sector sparing or sector slipping can influence
perfonnance. Suppose that the drive in Exercise 12.6 has a total
of 100 bad sectors at random locations and that each bad sector is
mapped to a spare that is located on a different track within the same
cylinder. Estimate the number of I/0 operations per second and the
effective transfer rate for a random-access workload consisting of 8-
kilobyte reads, assuming a queue length of 1 (that is, the choice of
scheduling algorithm is not a factor). What is the effect of a bad sector
on performance 
12.16 Discuss the relative advantages and disadvantages of sector sparing
and sector slipping.
12.17 Compare the performance of C-SCAN and SCAN scheduling, assuming
a uniform distribution of requests. Consider the average response time
(the time between the arrival of a request and the completion of that
request's service), the variation in response time, and the effective
550 Chapter 12
bandwidth. How does performance depend on the relative sizes of
seek time and rotational latency 
12.18 None of the disk-scheduling disciplines, except FCFS, is truly fair
(starvation may occur).
a. Explain why this assertion is true.
b. Describe a way to modify algorithms such as SCAN to ensure
fairness.
c. Explain why fairness is an important goal in a time-sharing
system.
d. Give three or more examples of circumstances in which it is
important that the operating system be unfair in serving I/O
requests.
12.19 Consider a RAID level 5 organization comprising five disks, with the
parity for sets of four blocks on four disks stored on the fifth disk. How
many blocks are accessed in order to perform the following 
a. A write of one block of data
b. A write of seven continuous blocks of data
12.20 The operating system generally treats removable disks as shared file
systems but assigns a tape drive to only one application at a time. Give
three reasons that could explain this difference in treatment of disks and
tapes. Describe the additional features that an operating system would
need to support shared file-system access to a tape jukebox. Would the
applications sharing the tape jukebox need any special properties, or
could they use the files as though the files were disk-resident  Explain
your answer.
12.21 How would use of a RAM disk affect your selection of a disk-scheduling
algorithm  What factors would you need to consider  Do the same
considerations apply to hard-disk scheduling, given that the file system
stores recently used blocks in a buffer cache in main memory 
12.22 You can use simple estimates to compare the cost and performance
of a terabyte storage system made entirely from disks with one that
incorporates tertiary storage. Suppose that each magnetic disk holds
10GB, costs $1,000, transfers 5MB per second, and has an average access
latency of 15 milliseconds. Also suppose that a tape library costs $10
per gigabyte, transfers 10 MB per second, and has an average access
latency of 20 seconds. Compute the total cost, the maximum total data
rate, and the average waiting time for a pure disk system. If you make
551
any assumptions about the workload, describe and justify them. Now,
suppose that 5 percent of the data are frequently used, so they must
reside on disk, but the other 95 percent are archived in the tape library.
Further suppose that the disk system handles 95 percent of the requests
and the library handles the other 5 percent. What are the total cost,
the maximum. total data rate, and the average waiting time for this
hierarchical storage system 
12.23 Assume that you have a mixed configuration comprising disks organized
as RAID levell and RAID levelS disks. Assume that the system
has flexibility in deciding which disk organization to use for storing a
particular file. Which files should be stored in the RAID level 1 disks
and which in the RAID levelS disks in order to optimize performance 
12.24 What are the tradeoffs involved in rereading code pages from the file
system versus using swap space to store them 
12.25 Requests are not usually uniformly distributed. For example, we can
expect a cylinder containing the file-system FAT or inodes to be accessed
more frequently than a cylinder containing only files. Suppose you
know that 50 percent of the requests are for a small, fixed number of
cylinders.
a. Would any of the scheduling algorithms discussed in this chapter
be particularly good for this case  Explain your answer.
b. Propose a disk-scheduling algorithm that gives even better performance
by taking advantage of this   hot spot   on the disk.
c. File systems typically fil1.d data blocks via an indirection table,
such as a FAT in DOS or inodes in UNIX. Describe one or more ways
to take advantage of this indirection to improve disk performance.
12.26 Discuss the reasons why the operating system might require accurate
information on how blocks are stored on a disk. How could the operating
system improve file system performance with this knowledge 
12.27 In a disk jukebox, what would be the effect of having more open files
than the number of drives in the jukebox 
12.28 Compare the throughput achieved by a RAID levelS organization with
that achieved by a RAID levell organization for the following:
a. Read operations on single blocks
b. Read operations on multiple contiguous blocks
552 Chapter 12
12.29 Could a RAID level 1 organization achieve better performance for read
requests than a RAID level 0 organization (with nonredundant striping
of data)  If so, how 
Discussions of redundant arrays of independent disks (RAIDs) are presented
by Patterson et al. [1988] and in the detailed survey of Chen et al. [1994].
Disk-system architectures for high-performance computing are discussed by
Katz et al. [1989]. Enhancements to RAID systems are discussed in. Wilkes
et al. [1996] and Yu et al. [2000]. Teorey and Pinkerton [1972] present an early
comparative analysis of disk-scheduling algorithms. They use simulations that
model a disk for which seek time is linear in the number of cylinders crossed.
For this disk LOOK is a good choice for queue lengths below 140, and C-LOOK
is good for queue lengths above 100. King [1990] describes ways to improve the
seek time by moving the disk ann when the disk is otherwise idle. Seltzer et al.
[1990] and Jacobson and Wilkes [1991] describe disk-scheduling algorithms that
consider rotational latency in addition to seek time. Scheduling optimizations
that exploit disk idle times are discussed in Lumb et al. [2000]. Worthington
et al. [1994] discuss disk performance and show the negligible performance
impact of defect management. The placement of hot data to improve seek
times has been considered by Ruemmler and Wilkes [1991] and Akyurek and
Salen'l [1993]. Ruemmler and Wilkes [1994] describe an accurate performance
model for a modern disk drive. Worthington et al. [1995] tell how to determine
low-level disk properties such as the zone structure, and this work is further
advanced by Schindler and Gregory [1999]. Disk power management issues
are discussed in Douglis et al. [1994L Douglis et al. [1995L Greenawalt [1994L
and Golding et al. [1995].
The I/0 size and randomness of the workload has a considerable influence
on disk performance. Ousterhout et al. [1985] and Ruemmler and Wilkes
[1993] report numerous interesting workload characteristics, including that
most files are smalt most newly created files are deleted soon thereafter, most
files that are opened for reading are read sequentially in their entirety, and most
seeks are short. McKusick et al. [1984] describe the Berkeley Fast File System
(FFS), which uses many sophisticated techniques to obtain good performance
for a wide variety of workloads. McVoy and Kleiman [1991] discuss further
improvements to the basic FFS. Quinlan [1991] describes how to implement
a file system on WORM storage with a magnetic disk cache; Richards [1990]
discusses a file-system approach to tertiary storage. Maher et al. [1994] give an
overview of the integration of distributed file systems and tertiary storage.
The concept of a storage hierarchy has been studied for more than
thirty years. For instance, a 1970 paper by Mattson et al. [1970] describes a
mathematical approach to predicting the performance of a storage hierarchy.
Alt [1993] describes the accommodation of removable storage in a commercial
operating system, and Miller and Katz [1993] describe the characteristics of
tertiary-storage access in a supercomputing environment. Benjamin [1990]
gives an overview of the massive storage requirements for the EOSDIS project
at NASA. Management and use of network-attached disks and programmable
553
disks are discussed in Gibson et al. [1997b t Gibson et al. [1997at Riedel et al.
[1998t and Lee and Thekkath [1996].
Holographic storage technology is the subject of an article by Psaltis and
Mok [1995]; a collection of papers on this topic dating from 1963 has been
assembled by Sincerbox [1994]. Asthana and Finkelstein [1995] describe several
emerging storage technologies, including holographic storage, optical tape,
and electron trapping. Toigo [2000] gives an in-depth description of modern
disk technology and several potential future storage technologies.

13.1
R
The two main jobs of a computer are I/0 and processing. In many cases, the
main job is I/0, and the processing is merely incidental. For instance, when
we browse a Web page or edit a file, our immediate interest is to read or enter
some information, not to compute an answer.
The role of the operating system in computer I/0 is to manage and
control I/0 operations and I/0 devices. Although related topics appear in
other chapters, here we bring together the pieces to paint a complete picture
of I/0. First, we describe the basics of I/O hardware, because the nature of the
hardware interface places constraints on the internal facilities of the operating
system. Next, we discuss the I/0 services provided by the operating system
and the embodiment of these services in the application I/0 interface. Then,
we explain how the operating system bridges the gap between the hardware
interface and the application interface. We also discuss the UNIX System V
STREAMS mechanism, which enables an application to assemble pipelines of
driver code dynamically. Finally, we discuss the performance aspects of I/O
and the principles of operating-system design that improve I/0 performance.
To explore the structure of an operating system's 1/0 subsystem.
To discuss the principles and complexities of 110 hardware.
To explain the performance aspects of 110 hardware and software.
The control of devices connected to the computer is a major concern of
operating-system designers. Because I/O devices vary so widely in their
function and speed (consider a mouse, a hard disk, and a CD-ROM jukebox),
varied methods are needed to control them. These methods form the I/0
subsystem of the kernet which separates the rest of the kernel from the
complexities of managing I/0 devices.
555
556 Chapter 13
13.2
I/O-device technology exhibits two conflicting trends. On the one hand, we
see increasing standardization of software and hardware interfaces. This trend
helps 11s to incorporate improved device generations into existing computers
and operating systems. On the other hand, we see an increasingly broad variety
of 1/0 devices. Some new devices are so unlike previous devices that it is a
challenge to incorporate them into our computers and operating systems. This
challenge is met by a combination of hardware and software techniques. The
basic I/0 hardware elements, such as ports, buses, and device controllers,
accommodate a wide variety of I/0 devices. To encapsulate the details and
oddities of different devices, the kernel of an operating system is structured
to use device-driver modules. The present a uniform deviceaccess
interface to the I/0 subsystem, much as system calls provide a standard
interface between the application and the operating system.
Computers operate a great many kinds of devices. Most fit into the general
categories of storage devices (disks, tapes), transmission devices (network
cards, modems), and human-interface devices (screen, keyboard, mouse).
Other devices are more specialized, s11ch as those involved in the steering
of a military fighter jet or a space shuttle. In these aircraft, a human gives input
to the flight computer via a joystick and foot pedals, and the computer sends
output commands that cause motors to move rudders, flaps, and thrusters.
Despite the incredible variety of I/0 devices, though, we need only a few
concepts to understand how the devices are attached and how the software
can control the hardware.
A device communicates with a computer system by sending signals over a
cable or even through the air. The device communicates with the machine via a
connection point, or example, a serial port. If devices use a common
set of wires, the connection is called a bus. A is a set of wires and a rigidly
defined protocol that specifies a set of messages that can be sent on the wires.
In terms of the electronics, the messages are conveyed by patterns of electrical
voltages applied to the wires with defined timings. When device A has a cable
that plugs into device B, and device B has a cable that plugs into device C, and
device C plugs into a port on the computer, this arrangement is called a
A daisy chain usually operates as a bus.
Buses are used widely in computer architecture and vary in their signaling
methods, speed, throughput, and connection methods. A typical PC bus
structure appears in Figure 13.1. This figure shows a (the common
PC system bus) that connects the processor-memory subsystem to the fast
devices and an that connects relatively slow devices, such as
the keyboard and serial and USB ports. In the upper-right portion of the figure,
four disks are c01mected together on a SCSI bus plugged into a SCSI controller.
Other common buses used to interconnect main parts of a computer include
with up to 4.3 GB; (PCie), with throughput up
with throughput up to 20 GB.
is a collection of electronics that can operate a port, a bus,
or a device. A serial-port controller is a simple device controller. It is a single
chip (or portion of a chip) in the computer that controls the signals on the
13.2 557
Figure 13.1 A typical PC bus structure.
wires of a serial port. By contrast, a SCSI bus controller is not simple. Because
the SCSI protocol is complex, the SCSI bus controller is often implemented as
a separate circuit board (or a that plugs into the computer. It
typically contains a processor, microcode, and some private memory to enable
it to process the SCSI protocol messages. Some devices have their own built-in
controllers. If you look at a disk drive, you will see a circuit board attached
to one side. This board is the disk controller. It implements the disk side of
the protocol for some kind of com1ection-SCSI or ATA, for instance. It has
microcode and a processor to do many tasks, such as bad-sector mapping,
prefetching, buffering, and caching.
How can the processor give commands and data to a controller to
accomplish an I/0 transfer  The short answer is that the controller has one
or more registers for data and control signals. The processor communicates
with the controller by reading and writing bit patterns in these registers. One
way in which this communication can occur is through the use of special
I/0 instructions that specify the transfer of a byte or word to an I/0 port
address. The I/0 instruction triggers bus lines to select the proper device and
to move bits into or out of a device register. Alternatively, the device controller
can support In this case, the device-control registers
are mapped into the address space of the processor. The CPU executes I/0
requests using the standard data-transfer instructions to read and write the
device-control registers.
Some systems use both techniques. For instance, PCs use I/0 instructions
to control some devices and memory-mapped I/0 to control others. Figure
13.2 shows the usual I/O port addresses for PCs. The graphics controller has
I/O ports for basic control operations, but the controller has a large memory558
Chapter 13
000-00F DMA controller
020-021 interrupt controller
040-043 timer
200-20F game controller
2F8-2FF serial port (secondary)
320-32F hard-disk controller
378-37F parallel port
3D0-3DF graphics controller
3F0-3F7 diskette-drive controller
3F8-3FF serial port (primary)
Figure 13.2 Device 1/0 port locations on PCs (partial).
mapped region to hold screen contents. The process sends output to the screen
by writing data into the memory-mapped region. The controller generates
the screen image based on the contents of this memory. This technique is
simple to use. Moreover, writing millions of bytes to the graphics memory
is faster than issuing millions of I/0 instructions. But the ease of writing
to a memory-mapped I/0 controller is offset by a disadvantage. Because a
common type of software fault is a write through an incorrect pointer to an
unintended region of memory, a memory-mapped device register is vulnerable
to accidental modification. Of course, protected memory helps to reduce this
risk.
An I/0 port typically consists of four registers, called the (1) status, (2)
control, (3) data-in, and (4) data-out registers.
The
The
is read by the host to get input.
is written by the host to send output.
The contains bits that can be read by the host. These bits
indicate states, such as whether the current command has completed,
whether a byte is available to be read from the data-in register, and whether
a device error has occurred.
The can be written by the host to start a command or to
change the nlOde of a device. For instance, a certain bit in the control
register of a serial port chooses between full-duplex and half-duplex
communication, another bit enables parity checking, a third bit sets the
word length to 7 or 8 bits, and other bits select one of the speeds supported
by the serial port.
The data registers are typically 1 to 4 bytes in size. Some controllers have
FIFO chips that can hold several bytes of input or output data to expand the
capacity of the controller beyond the size of the data register. A FIFO chip can
hold a small burst of data until the device or host is able to receive those data.
13.2 559
13.2.1 Polling
The complete protocol for interaction between the host and a controller
can be intricate, but the basic handshaking notion is simple. We explain
handshaking with an example. Assume that 2 bits are used to coordinate
the producer-consumer relationship between the controller and the host. The
controller indicates its state through the busy bit in the status register. (Recall
that to set a bit means to write a 1 into the bit and to clear a bit means to write
a 0 into it.) The controller sets the busy bit when it is busy working and clears
the busy bit when it is ready to accept the next comm.and. The host signals
its wishes via the command-ready bit in the command register. The host sets the
command-ready bit when a command is available for the controller to execute.
For this example, the host writes output through a port, coordinating with the
controller by handshaking as follows.
The host repeatedly reads the busy bit until that bit becomes clear.
The host sets the write bit in the command register and writes a byte into
the data-out register.
The host sets the command-ready bit.
When the controller notices that the command-ready bit is set, it sets the
busy bit.
The controller reads the command register and sees the write command.
It reads the data-out register to get the byte and does the I/O to the device.
The controller clears the command-ready bit, clears the error bit in the status
register to indicate that the device I/O succeeded, and clears the busy bit
to indicate that it is finished.
This loop is repeated for each byte.
In step 1, the host is or it is in a loop, reading the
status register over and over until the busy bit becomes clear. If the controller
and device are fast, this method is a reasonable one. But if the wait may be
long, the host should probably switch to another task. How, then, does the
host know when the controller has become idle  For some devices, the host
must service the device quickly, or data will be lost. For instance, when data
are streaming in on a serial port or from a keyboard, the small buffer on the
controller will overflow and data will be lost if the host waits too long before
returning to read the bytes.
In many computer architectures, three CPU-instruction cycles are sufficient
to poll a device: read a device register, logical-and to extract a status bit, and
branch if not zero. Clearly, the basic polling operation is efficient. But polling
becomes inefficient when it is attempted repeatedly yet rarely finds a device
to be ready for service, while other useful CPU processing remains undone. In
such instances, it may be more efficient to arrange for the hardware controller to
notify the CPU when the device becomes ready for service, rather than to require
the CPU to poll repeatedly for an I/0 completion. The hardware mechanism
that enables a device to notify the CPU is called an
560 Chapter 13
7
CPU
device driver initiates 1/0
CPU executing checks for
interrupts between instructions
CPU resumes
processing of
interrupted task
1/0 controller
4
Figure 13.3 Interrupt-driven 1/0 cycle.
13.2.2 Interrupts
The basic interrupt mechanism works as follows. The CPU hardware has a wire
called the that the CPU senses after executing every
instruction. When the CPU detects that a controller has asserted a signal on
the line, the CPU performs a state save and jumps to the
at a fixed address in memory. The interrupt handler
determines the cause of the interrupt, performs the necessary processing,
performs a state restore, and executes a return from interrupt instruction
to return the CPU to the execution state prior to the interrupt. We say that
the device controller raises an interrupt by asserting a signal on the interrupt
request line, the CPU catches the interrupt and dispatches it to the interrupt
handler, and the handler clears the interrupt by servicing the device. Figure
13.3 summarizes the interrupt-driven I/0 cycle.
This basic interrupt mechanism enables the CPU to respond to an asynchronous
event, as when a device controller becomes ready for service. In a
modern operating system, however, we need nlOre sophisticated interrupthandling
features.
We need the ability to defer interrupt handling during critical processing.
13.2 561
We need an efficient way to dispatch to the proper interrupt handler for
a device without first polling all the devices to see which one raised the
interrupt.
We need multilevel interrupts, so that the operating system can distinguish
between high- and low-priority interrupts and can respond with
the appropriate degree of urgency.
In modern computer hardware, these three features are provided by the CPU
and by the
Most CPUs have two interrupt request lines. One is the
'    ''        '. which is reserved for events such as unrecoverable memory errors.
The second interrupt line is it can be turned off by the CPU before
the execution of critical instruction sequences that must not be interrupted.
The maskable interrupt is used by device controllers to request service.
The interrupt mechanism accepts an number that selects a
specific interrupt-handling routine from a small set. In most architectures, this
address is an offset in a table called the . This vector contains
the memory addresses of specialized interrupt handlers. The purpose of a
vectored interrupt mechanism is to reduce the need for a single interrupt
handler to search all possible sources of interrupts to determine which one
needs service. In practice, however, computers have more devices (and, hence,
interrupt handlers) than they have address elements in the interrupt vector.
A common way to solve this problem is to use the technique of interrupt
chaining, in which each element in the interrupt vector points to the head of
a list of interrupt handlers. When an il1.terrupt is raised, the handlers on the
corresponding list are called one by one, until one is found that can service
the request. This structure is a compromise between the overhead of a huge
interrupt table and the inefficiency of dispatching to a single interrupt handler.
Figure 13.4 illustrates the design of theinterruptvector for the Intel Pentium
processor. The events from 0 to 31, which are nonmaskable, are used to signal
various error conditions. The events from 32 to 255, which are maskable, are
used for purposes such as device-generated interrupts.
The interrupt mechanism also implements a system of
This mechanism enables the CPU to defer the handling of low-priority
interrupts without maskii1.g off all interrupts and makes it possible for a
high-priority interrupt to preempt the execution of a low-priority interrupt.
A modern operating system interacts with the interrupt mechanism in
several ways. At boot time, the operating system probes the hardware buses
to determine what devices are present and installs the corresponding interrupt
handlers into the interrupt vector. During I/0, the various device controllers
raise interrupts when they are ready for service. These interrupts signify that
output has cornpleted, or that input data are available, or that a failure has
been detected. The interrupt mechanism is also used to handle a wide variety
of such as dividing by zero, accessing a protected or nonexistent
memory address, or attempting to execute a privileged instruction from user
mode. The events that trigger interrupts have a common property: they are
occurrences that induce the CPU to execute an urgent self-contained routine.
An operating system has other good uses for an efficient hardware and
software mechanism that saves a small amount of processor state and then
562 Chapter 13
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
i 
18
19-31
32-255
breakpoint
INTO-detected overflow
bound range exception
invalid opcode
device not available
double fault
coprocessor segment overrun (reserved)
invalid task state segment
segment not present
stack fault
general protection
page fault
(Intel reserved, do not use)
floating-point error
alignment check
machine check
(Intel reserved, do not use)
maskable interrupts
Figure i3.4 Intel Pentium processor event-vector table.
calls a privileged routine in the kernel. For example, many operating systems
use the interrupt mechanism for virtual memory paging. A page fault is an
exception that raises an interrupt. The interrupt suspends the current process
and jumps to the page-fault handler in the kernel. This handler saves the state
of the process, moves the process to the wait queue, performs page-cache
management, schedules an I/0 operation to fetch the page, schedules another
process to resume execution, and then returns from the interrupt.
Another example is found in the implementation of system calls. Usually,
a program uses library calls to issue system calls. The library routines check
the arguments given by the application, build a data structure to convey the
arguments to the kernel, and then execute a special instruction called a
or . This instruction has an operand that identifies the desired
kernel service. When a process executes the trap instruction, the interrupt
hardware saves the state of the user code, switches to supervisor mode, and
dispatches to the kernel routine that implements the requested service. The
trap is given a relatively low interrupt priority compared with those assigned
to device interrupts-executilcg a system call on behalf of an application is less
urgent than servicing a device controller before its FIFO queue overflows and
loses data.
Interrupts can also be used to manage the flow of control within the kernel.
For example, consider the processing required to complete a disk read. One
step is to copy data from kernel space to the user buffer. This copying is time
consuming but not urgent-it should not block other high-priority interrupt
13.2 563
handling. Another step is to start the next pending l/0 for that disk drive. This
step has higher priority. If the disks are to be used efficiently, we need to start
the next I/O as soon as the previous one completes. Consequently, a pair of
interrupt handlers implen  ents the kernel code that completes a disk read. The
high-priority handler records the l/0 status, clears the device interrupt, starts
the next pending I/0, and raises a low-priority interrupt to complete the work.
Later, when the CPU is not occupied with high-priority work, the low-priority
interrupt will be dispatched. The corresponding handler completes the userlevel
I/0 by copying data from kernel buffers to the application space and then
calling the scheduler to place the application on the ready queue.
A threaded kernel architecture is well suited to implement multiple
interrupt priorities and to enforce the precedence of interrupt handling over
background processing in kernel and application routines. We illustrate this
point with the Solaris kernel. In Solaris, interrupt handlers are executed
as kernel threads. A range of high priorities is reserved for these threads.
These priorities give interrupt handlers precedence over application code and
kernel housekeeping and implement the priority relationships among interrupt
handlers. The priorities cause the Solaris thread scheduler to preempt lowpriority
interrupt handlers in favor of higher-priority ones, and the threaded
implementation enables multiprocessor hardware to run several interrupt
handlers concurrently. We describe the interrupt architecture of Windows XP
and UNIX in Chapter 22 and Appendix A, respectively.
In summa:r:y, interrupts are used throughout modern operating systems to
handle asynchronous events and to trap to supervisor-mode routines in the
kernel. To enable the most urgent work to be done first, modern computers
use a system of interrupt priorities. Device controllers, hardware faults, and
system calls all raise interrupts to trigger kernel routines. Because interrupts
are used so heavily for time-sensitive processing, efficient interrupt handling
is required for good system performance.
13.2.3 Direct Memory Access
For a device that does large transfers, such as a disk drive, it seems wasteful
to use an expensive general-purpose processor to watch status bits and to
feed data into a controller register one byte at a time-a process termed
Many computers avoid burdening the main CPU with
PIO by offloading some of this work to a special-purpose processor called a
To initiate a DMA transfer, the host
writes a DMA command block into memory. This block contains a pointer to
the source of a transfer, a pointer to the destination of the transfer, and a count
of the number of bytes to be transferred. The CPU writes the address of this
command block to the DMA controller, then goes on with other work. The DMA
controller proceeds to operate the memory bus directly, placing addresses on
the bus to perform transfers without the help of the main CPU. A simple DMA
controller is a standard component in PCs, and for
the PC usually contain their own high-speed DMA hardware.
Handshaking between the DMA controller and the device controller is
performed via a pair of wires called DMA-request and DMA-acknowledge.
The device controller places a signal on the DMA-request wire when a word
of data is available for transfer. This signal causes the DMA controller to seize
564 Chapter 13
the memory bus, place the desired address on the memory-address wires,
and place a signal on the Dl\IIA -acknowledge wire. When the device controller
receives the DMA-acknowledge signat it transfers the word of data to memory
and removes the DMA-request signal.
When the entire transfer is finished, the DMA controller interrupts the CPU.
This process is depicted in Figure 13.5. When the DMA controller seizes the
memory bus, the CPU is momentarily prevented from accessing main memory
although it can still access data items in its primary and secondary caches.
Although this can slow down the CPU computation, offloading
the data-transfer work to a DMA controller generally improves the total system
performance. Some computer architectures use physical memory addresses for
DMA, but others perform mercwry using virtual
addresses that undergo translation to physical addresses. DVMA can perform
a transfer between two memory-mapped devices without the intervention of
the CPU or the use of main memory.
On protected-mode kernels, the operating system generally prevents
processes from issuing device commands directly. This discipline protects data
from access-control violations and also protects the system from erroneous use
of device controllers that could cause a system crash. Instead, the operating
system exports functions that a sufficiently privileged process can use to
access low-level operations on the underlying hardware. On kernels without
memory protection, processes can access device controllers directly. This direct
access can be used to achieve high performance/ since it can avoid kernel
communication, context switches, and layers of kernelsoftware. Unfortunately,
it interferes with system security and stability. The trend in general-purpose
operating systems is to protect memory and devices so that the system can try
to guard against erroneous or malicious applications.
5. DMA controller
transfers bytes to
buffer X, increasing
memory address
and decreasing C
until C = 0
1. device driver is told
to transfer disk data
to buffer at address X
2. device driver tells L  -'  '-  -~i-  '-'~--'--'
disk controller to
transfer C bytes
from disk to buffer
at address X
6. when C = 0, DMA
interrupts CPU to signal
transfer completion 1:2.'.c~li.ip:2i:.2-J
rc-c~.,.,---J'-=  ,._--~ 3. disk controller initiates
DMA transfer
c'G!,Or:tt_ront;r  '    I 4. disk controller sends
each byte to DMA
controller
Figure 13.5 Steps in a DMA transfer.
13.3
13.3 565
13.2.4 1/0 Hardware Summary
Although the hardware aspects of I/0 are complex when considered at the
level of detail of electronics-hardware design, the concepts that we have
just described are sufficient to enable us to understand many I/0 features
of operating systen  s. Let's review the main concepts:
A bus
A controller
An I/0 port and its registers
The handshaking relationship between the host and a device controller
The execution of this handshaking in a polling loop or via interrupts
The offloading of this work to a DMA controller for large transfers
We gave a basic example of the handshaking that takes place between a
device controller and the host earlier in this section. In reality, the wide variety
of available devices poses a problem for operating-system implementers. Each
kind of device has its own set of capabilities, control-bit definitions, and
protocols for interacting with the host-and they are all different. How can
the operating system be designed so that we can attach new devices to the
computer without rewriting the operating system  And when the devices
vary so widely, how can the operating system give a convenient, uniform I/0
interface to applications  We address those questions next.
In this section, we discuss structuring techniques and interfaces for the
operating system that enable I/0 devices to be treated in a standard, uniform
way. We explain, for instance, how an application can open a file on a disk
without knowing what kind of disk it is and how new disks and other devices
can be added to a cmnputer without disruption of the operating system.
Like other complex software-engineering problems, the approach here
involves abstraction, encapsulation, and software layering. Specifically, we
can abstract away the detailed differences in I/0 devices by identifying a few
general kinds. Each kind is accessed through a standardized set of
functions-an The differences are encapsulated in kernel modules
called device drivers that internally are custom-tailored to specific devices
but that export one of the standard interfaces. Figure 13.6 illustrates how the
I/O-related portions of the kernel are structured in software layers.
The purpose of the device-driver layer is to hide the differences among
device controllers from the I/O subsystem of the kernel, much as the I/0
system calls encapsulate the behavior of devices in a few generic classes
that hide hardware differences from applications. Making the I/0 subsystem
independent of the hardware simplifies the job of the operating-system
developer. It also benefits the hardware manufacturers. They either design
new devices to be compatible with an existing host controller interface (such as
SCSI-2), or they write device drivers to interface the new hardware to popular
566 Chapter 13
Figure 13.6 A kernel I/O structure.
operating systems. Thus, we can attach new peripherals to a computer without
waiting for the operating-system vendor to develop support code.
Unfortm1ately for device-hardware manufacturers, each type of operating
system has its own standards for the device-driver interface. A given device
may ship with multiple device drivers-for instance, drivers for MS-DOS,
Windows 95/98, Windows NT/2000, and Solaris. Devices vary on many
dimensions, as illustrated in Figure 13.7.
Character-stream or block. A character-stream device transfers bytes one
by one, whereas a block device transfers a block of bytes as a unit.
Sequential or random access. A sequential device transfers data in a fixed
order determined by the device, whereas the user of a random-access
device can instruct the device to seek to any of the available data storage
locations.
Synchronous or asynchronous. A synchronous device performs data
transfers with predictable response times. An asynchronous device
exhibits irregular or unpredictable response times.
Sharable or dedicated. A sharable device can be used concurrently by
several processes or threads; a dedicated device cannot.
Speed of operation. Device speeds range from a few bytes per second to
a few gigabytes per second.
Read -write, read only, or write only. Some devices perform both input
and output, but others support only one data transfer direction.
access method
transfer schedule
I/O direction
13.3
synchronous
asynchronous
dedicated
sharable
latency
seek time
transfer rate
delay between operations
read only
write only
read-write
tape
keyboard
tape
keyboard
CD-ROM
Figure i 3. 7 Characteristics of 1/0 devices.
567
For the purpose of application access, many of these differences are hidden
by the operating system, and the devices are grouped into a few conventional
types. The resulting styles of device access have been found to be useful
and broadly applicable. Although the exact system calls may differ across
operating systems, the device categories are fairly standard. The major access
conventions include block I/0, character-stream I/0, memory-mapped file
access, and network sockets. Operating systems also provide special system
calls to access a few additional devices, such as a time-of-day clock and a timer.
Some operating systems provide a set of system calls for graphical display,
video, and audio devices.
Most operating systems also have an (or that transparently
passes arbitrary conunands from an application to a device driver. In
UNIX, this system call is ioctl () (for   I/0 control  ). The ioctl () system call
enables an application to access any functionality that can be implernented by
any device driver, without the need to invent a new system call. The ioctl ()
system call has three arguments. The first is a file descriptor that connects the
application to the driver by referring to a hardware device managed by that
driver. The second is an integer that selects one of the commands implemented
in the driver. The third is a pointer to an arbitrary data structure in memory
that enables the application and driver to communicate any necessary control
information or data.
13.3.1 Block and Character Devices
The captures all the aspects necessary for accessing disk
drives and other block-oriented devices. The device is expected to understand
commands such as read() and write() ;if it is a random-access device, it is also
expected to have a seek() command to specify which block to transfer next.
568 Chapter 13
Applications normally access such a device through a file-system interface.
We can see that read(), write(), and seek 0 capture the essen.tial behaviors
of block-storage devices, so that applications are insulated from the low-level
differences among those devices.
The operating system itself, as well as special applications such as databasemanagement
systems, may prefer to access a block device as a simple linear
array of blocks. This mode of access is sometimes called If the
application performs its own buffering, then using a file systen1. would cause
extra, unneeded buffering. Likewise, if an application provides its own locking
of file blocks or regions, then any operating-system locking services would be
redundant at the least and contradictory at the worst. To avoid these conflicts,
raw-device access passes control of the device directly to the application, letting
the operating system step out of the way. Unfortunately, no operating-system
services are then performed on this device. A compromise that is becoming
common is for the operating system to allow a mode of operation on a file that
disables buffering and locking. In the UNIX world, this is called
Memory-mapped file access can be layered on top of block-device drivers.
Rather than offering read and write operations, a memory-mapped interface
provides access to disk storage via an array of bytes in main memory. The
system call that maps a file into memory returns the virtual memory address
that contains a copy of the file. The actual data transfers are performed only
when needed to satisfy access to the memory image. Because the transfers
are handled by the same mechanism as that used for demand-paged virtual
memory access, memory-mapped I/O is efficient. Memory mapping is also
convenient for programmers-access to a memory-mapped file is as simple
as reading from and writing to memory. Operating systems that offer virtual
memory commonly use the mapping interface for kernel services. For instance,
to execute a program, the operating system maps the executable into memory
and then transfers control to the entry address of the executable. The mapping
interface is also commonly used for kernel access to swap space on disk.
A keyboard is an example of a device that is accessed through a
The basic system calls in this interface enable an application
to get() or put() one character. On top of this interface, libraries can be
built that offer line-at-a-time access, with buffering and editing services (for
example, when a user types a backspace, the preceding character is removed
from the input stream). This style of access is convenient for input devices such
as keyboards, mice, and modems that produce data for input   spontaneously  
-that is, at times that cam1.ot necessarily be predicted by the application. This
access style is also good for output devices such as printers and audio boards,
which naturally fit the concept of a linear stream of bytes.
13.3.2 Network Devices
Because the performance and addressing characteristics of network I/0 differ
significantly from those of disk I/0, most operating systems provide a network
I/O interface that is different from the read() -write() -seek() interface used
for disks. One interface available in many operating systerns, including UNIX
and Windows NT, is the network interface.
Think of a wall socket for electricity: any electrical appliance can be plugged
in. By analogy, the system calls in the socket interface enable an application
13.3 569
to create a socket, to connect a local socket to a remote address (which plugs
this application into a socket created by another application), to listen for
any remote application to plug into the local socket, and to send and receive
packets over the connection. To support the implementation of servers, the
socket interface also provides a function called select() that manages a set
of sockets. A call to select () returns information about which sockets have a
packet waiting to be received and which sockets have room to accept a packet
to be sent. The use of select() eliminates the polling and busy waiting that
would otherwise be necessary for network I/0. These functions encapsulate the
essential behaviors of networks, greatly facilitating the creation of distributed
applications that can use any underlying network hardware and protocol stack
Many other approaches to interprocess communication and network
communication have been implemented. For instance, Windows NT provides
one interface to the network interface card and a second interface to the
network protocols (Appendix C.6). In UNIX, which has a long history as a
proving ground for network technology, we find half-duplex pipes, full-duplex
FIFOs, full-duplex STREAMS, message queues, and sockets. Information on UNIX
networking is given in Appendix A.9.
13.3.3 Clocks and Timers
Most computers have hardware clocks and timers that provide three basic
functions:
Give the current time.
Give the elapsed time.
Set a timer to trigger operation X at time T.
These functions are used heavily by the operating system, as well as by timesensitive
applications. Unfortunately, the system calls that implement these
functions are not standardized across operating systems.
The hardware to measure elapsed time and to trigger operations is called
a . It can be set to wait a certain amount of time
generate an interrupt, and it can be set to do this once or to repeat the
process to generate periodic interrupts. The scheduler uses this mechanism to
generate an interrupt that will preempt a process at the end of its time slice.
The disk I/O subsystem uses it to invoke the periodic flushing of dirty cache
buffers to disk, and the network subsystem uses it to cancel operations that are
proceeding too slowly because of network congestion or failures. The operating
system may also provide an interface for user processes to use timers. The
operating system can support more timer requests than the number of timer
hardware chan11els by simulating virtual clocks. To do so, the kernel (or the
timer device driver) maintains a list of interrupts wanted by its own routines
and by user requests, sorted in earliest-time-first order. It sets the timer for the
earliest tince. When the timer interrupts, the kernel signals the requester and
reloads the timer with the next earliest time.
On many computers, the interrupt rate generated by the hardware clock is
between 18 and 60 ticks per second. This resolution is coarse, since a modern
computer can execute hundreds of millions of instructions per second. The
570 Chapter 13
precision of triggers is limited by the coarse resolution of the timer, together
with the overhead of maintaining virtual clocks. Furthermore, if the timer
ticks are used to maintain the system time-of-day clock, the system clock
can drift. In most computers, the hardware clock is constructed from a highfrequency
counter. In some computers, the value of this counter can be read
from a device register, in which case the counter can be considered a highresolution
clock. Although this clock does not generate interrupts, it offers
accurate measurements of time intervals.
13.3.4 Blocking and Nonblocking 1/0
Another aspect of the system-call interface relates to the choice between
blocking I/0 and nonblocking I/0. When an application issues a
system call, the execution of the application is suspended. The application
is moved from the operating system's run queue to a wait queue. After the
system call completes, the application is moved back to the run queue, where
it is eligible to resume execution. When it resumes execution, it will receive
the values returned by the system call. The physical actions performed by
I/0 devices are generally asynchronous-they take a varying or unpredictable
amount of time. Nevertheless, most operating systems use blocking system
calls for the application interface, because blocking application code is easier
to understand than nonblocking application code.
Some user-level processes need I/0. One example is a user
interface that receives keyboard and mouse input while processing and
displaying data on the screen. Another example is a video application that
reads frames from a file on disk while simultaneously decompressing and
displaying the output on the display.
One way an application writer can overlap execution with I/0 is to write
a multithreaded application. Some threads can perform blocking system calls,
while others continue executing. The Solaris developers used this technique to
implement a user-level library for asynchronous I/0, freeing the application
writer from that task. Some operating systems provide nonblocking I/0 system
calls. A nonblocking call does not halt the execution of the application for an
extended time. h1.stead, it returns quickly, with a return value that indicates
how many bytes were transferred.
An alternative to a nonblocking system call is an asynchronous system
call. An asynchronous call returns immediately, without waiting for the I/0 to
complete. The application continues to execute its code. The completion of the
I/0 at some future time is communicated to the application, either through the
setting of some variable in the address space of the application or through the
triggering of a signal or software interrupt or a call-back routine that is executed
outside the linear control flow of the application. The difference between
nonblocking and asynchronous system calls is that a nonblocking read()
returns immediately with whatever data are available-the full number of
bytes requested, fewer, or none at all. An asynchronous read () call requests a
transfer that will be performed in its entirety but will complete at some future
time. These two I/0 methods are shown in Figure 13.8.
A good example of nonblocking behavior is the select () system call for
network sockets. This system call takes an argument that specifies a maximum
waiting time. By setting it to 0, an application can poll for network activity
13.4
13.4 571
kernel user user
kernel
(a) (b)
Figure 13.8 Two 1/0 methods: (a) synchronous and (b) asynchronous.
without blocking. But using select() introduces extra overhead, because
the select() call only checks whether I/0 is possible. For a data transfer,
select() must be followed by some kind of read() or write() command.
A variation on this approach, fotmd in Mach, is a blocking multiple-read call.
It specifies desired reads for several devices in one system call and returns as
soon as any one of them completes.
Kernels provide many services related to I/0. Several services-scheduling,
buffering, caching, spooling, device reservation, and error handlil1.g-are
provided by the kernel's I/0 subsystem and build on the hardware and devicedriver
infrastructure. The I/O subsystem is also responsible for protectil1.g itself
from errant processes and malicious users.
13.4.1 1/0 Scheduling
To schedule a set of I/O requests means to determine a good order in which to
execute them. The order in which applications issue system calls rarely is the
best choice. Scheduling can improve overall system performance, can share
device access fairly among processes, and can reduce the average waiting time
for I/0 to complete. Here is a simple example to illustrate. Suppose that a disk
arm is near the begilming of a disk and that three applications issue blocking
read calls to that disk Application 1 requests a block near the end of the disk,
application 2 requests one near the beginning, and application 3 requests one
in the middle of the disk The operating system can reduce the distance that the
disk ann travels by serving the applications in the order 2, 3, 1. Rearrangil1.g
the order of service in this way is the essence of I/0 scheduling.
Operating-system developers implement scheduling by maintaining a wait
queue of requests for each device. When an application issues a blocking I/0
system call, the request is placed on the queue for that device. The I/0 scheduler
rearranges the order of the queue to improve the overall system efficiency
and the average response time experienced by applications. The operating
572 Chapter 13
Figure 13.9 Device-status table.
system may also try to be fair, so that no one application receives especially
poor service, or it may give priority service for delay-sensitive requests. For
instance, requests from the virtual memory subsystem may take priority over
application requests. Several scheduling algorithms for disk I/0 are detailed
in Section 12.4.
When a kernel supports asynchronous I/0, it must be able to keep track
of many I/0 requests at the same time. For this purpose, the operating system
might attach the wait queue to a :able. The kernel manages this
table, which contains an entry for each I/0 device, as shown in Figure 13.9.
Each table entry indicates the device's type, address, and state (not functioning,
idle, or busy). If the device is busy with a request, the type of request and other
parameters will be stored in the table entry for that device.
One way in which the I/0 subsystem improves the efficiency of the
computer is by scheduling I/0 operations. Another way is by using storage
space in main memory or on disk via teclul.iques called buffering, caching, and
spooling.
13.4.2 Buffering
A is a memory area that stores data being transferred between two
devices or between a device and an application. Buffering is done for three
reasons. One reason is to cope with a speed mismatch between the producer and
consumer of a data stream. Suppose, for example, that a file is being received
via modem for storage on the hard disk The modem is about a thousand
times slower than the hard disk So a buffer is created in main mernory to
accumulate the bytes received from the modem. When an entire buffer of data
has arrived, the buffer can be written to disk in a single operation. Since the
disk write is not instantaneous and the modem still needs a place to store
additional incoming data, two buffers are used. After the modem fills the first
buffer, the disk write is requested. The modem then starts to fill the second
buffer while the first buffer is written to disk By the time the modem has filled
13.4 573
the second buffer, the disk write from the first one should have completed,
so the modem can switch back to the first buffer while the disk writes the
second one. This decouples the producer of data from the
consun1.er, thus relaxing timing requirements between them. The need for this
decoupling is illustrated in Figure 13.10, which lists the enormous differences
in device speeds for typical computer hardware.
A second use of buffering is to provide adaptations for devices that
have different data-transfer sizes. Such disparities are especially common in
computer networking, where buffers are used widely for fragmentation and
reassembly of messages. At the sending side, a large message is fragmented
into small network packets. The packets are sent over the network, and the
receiving side places them in a reassembly buffer to form an image of the
source data.
A third use of buffering is to support copy semantics for application I/0.
An example will clarify the meaning of   copy semantics.   Suppose that an
application has a buffer of data that it wishes to write to disk It calls the
write() systemcalt providing a pointer to the buffer and an integer specifying
the number of bytes to write. After the system call returns, what happens if
the application changes the contents of the buffer  With the
version of the data written to disk is guaranteed to be version at the
time of the application system calt independent of any subsequent changes
in the application's buffer. A simple way in which the operating system can
guarantee copy semantics is for the write () system call to copy the application
I System Bus
Hype(~ransport (32,pair) ~~~Iii~~~~~~~~
I
PCI ~xpress 2.0 (    32)
i
lnfi!l.i band (QDR ;.1 2X)

0.00001 0.001 0.1 10 1000 100000 1 EFigure
13.10 Sun Enterprise 6000 device-transfer rates (logarithmic).
574 Chapter 13
data into a kernel buffer before returning control to the application. The disk
write is performed from the kernel buffer, so that subsequent changes to the
application buffer have no effect. Copying of data between kernel buffers and
application data space is common in operating systems, despite the overhead
that this operation introduces, because of the clean semantics. The same effect
can be obtained more efficiently by clever use of virtual memory mapping and
copy-on-write page protection.
13.4.3 Caching
A is a region of fast memory that holds copies of data. Access to the cached
copy is more efficient than access to the original. For instance, the instructions
of the currently running process are stored on disk, cached ilc physical memory,
and copied again ill the CPU's secondary and primary caches. The difference
between a buffer and a cache is that a buffer may hold the only existing copy
of a data item, whereas a cache, by definition, holds a copy on faster storage of
an item that resides elsewhere.
Caching and buffering are distinct functions, but sometinces a region
of memory can be used for both purposes. For illstance, to preserve copy
semantics and to enable efficient scheduling of disk I/0, the operating system
uses buffers in maill memory to hold disk data. These buffers are also used as
a cache, to improve the I/O efficiency for files that are shared by applications
or that are being written and reread rapidly. When the kernel receives a file
I/0 request, the kernel first accesses the buffer cache to see whether that region
of the file is already available in main memory. If it is, a physical disk I/O
can be avoided or deferred. Also, disk writes are accumulated ill the buffer
cache for several seconds, so that large transfers are gathered to allow efficient
write schedules. This strategy of delayilcg writes to improve I/O efficiency is
discussed, in the context of remote file access, ill Section 17.3.
13.4.4 Spooling and Device Reservation
A is a buffer that holds output for a device, such as a printer, that cannot
accept ilcterleaved data streams. Although a prillter can serve only one job
at a time, several applications may wish to print their output concurrently,
without having their output mixed together. The operating system solves this
problem by intercepting all output to the printer. Each application's output
is spooled to a separate disk file. When an application finishes printing, the
spooling system queues the correspondilcg spool file for output to the printer.
The spooling system copies the queued spool files to the printer one at a time. In
some operating systems, spooling is managed by a system daemon process. In
others, it is handled by an in-kernel thread. In either case, the operating system
provides a control interface that enables users and system administrators to
display the queue, remove unwanted jobs before those jobs print, suspend
printing while the printer is serviced, and so on.
Some devices, such as tape drives and printers, cannot usefully multiplex
the I/0 requests of multiple concurrent applications. Spooling is one way
operating systems can coordinate concurrent output. Another way to deal with
concurrent device access is to provide explicit facilities for coordination. Some
operating systems (including VMS) provide support for exclusive device access
by enabling a process to allocate an idle device and to deallocate that device
13.4 575
when it is no longer needed. Other operating systems enforce a limit of one
open file handle to such a device. Many operating systems provide functions
that enable processes to coordinate exclusive access among then'lselves. For
instance, Windows NT provides system calls to wait until a device object
becomes available. It also has a parameter to the open () system call that
declares the types of access to be permitted to other concurrent threads. On
these systems, it is up to the applications to avoid deadlock.
13.4.5 Error Handling
An operating system that uses protected memory can guard against many
kinds of hardware and application errors, so that a complete system failure is
not the usual result of each minor mechanical glitch. Devices and I/0 transfers
can fail in many ways, either for transient reasons, as when a network becomes
overloaded, or for   permanent   reasons, as when a disk controller becomes
defective. Operating systems can often compensate effectively for transient
failures. For instance, a disk read() failure results in a read() retry, and
a network send() error results in a res end(), if the protocol so specifies.
Unfortunately, if an important component experiences a permanent failure,
the operating system is unlikely to recover.
As a general rule, an I/0 system call will return one bit of information
about the status of the call, signifying either success or failure. In the UNIX
operating system, an additional integer variable named errno is used to
return an error code-one of about a hundred values-indicating the general
nature of the failure (for example, argument out of range, bad pointer, or
file not open). By contrast, some hardware can provide highly detailed error
information, although many current operating systems are not designed to
convey this information to the application. For instance, a failure of a SCSI
device is reported by the SCSI protocol in three levels of detail: a key that
identifies the general nature of the failure, such as a hardware error or an illegal
request; an that states the category of failure, such as a
bad command parameter or a self-test failure; and an
'X     ' l  '.l that gives even more detail, such as which command parameter was
in error or which hardware subsystem failed its self-test. Further, many SCSI
devices maintain internal pages of error-log information that can be requested
by the host-but seldom are.
13.4.6 1/0 Protection
Errors are closely related to the issue of protection. A user process may
accidentally or purposely attempt to disrupt the normal operation of a systern
by attempting to issue illegal I/0 instructions. We can use various mechanisms
to ensure that such disruptions cam'lot take place in the system.
To prevent users from performing illegal I/0, we define all I/0 instructions
to be privileged instructions. Thus, users cannot issue I/O instructions directly;
they must do it through the operating system. To do I/0, a user program
executes a system call to request that the operating system perform I/0 on its
behalf (Figure 13.11). The operating system, executing in monitor mode, checks
that the request is valid and, if it is, does the I/0 requested. The operating
system then returns to the user.
576 Chapter 13
CD
trap to
monitor
kernel
 
perform 1/0
 
return
to user
user
program
Figure 13.1  1 Use of a system call to perform 1/0.
In addition, any memory-mapped and I/O port memory locations must
be protected from user access by the memory-protection system. Note that a
kernel cannot simply deny all user access. Most graphics games and video
editing and playback software need direct access to memory-mapped graphics
controller memory to speed the performance of the graphics, for example. The
kernel might in this case provide a locking mechanism to allow a section of
graphics memory (representing a window on screen) to be allocated to one
process at a time.
13.4.7 Kernel Data Structures
The kernel needs to keep state information about the use of I/0 components.
It does so through a variety of in-kernel data structures, such as the open-file
table structure from Section 11.1. The kernel uses many similar structures to
track network connections, character-device communications, and other I/0
activities.
UNIX provides file-system access to a variety of entities, such as user files,
raw devices, and the address spaces of processes. Although each of these
entities supports a read () operation, the semantics differ. For instance, to
read a user file, the kernel needs to probe the buffer cache before deciding
whether to perform a disk I/0. To read a raw disk, the kernel needs to ensure
that the request size is a multiple of the disk sector size and is aligned on
a sector boundary. To read a process image, it is merely necessary to copy
data from memory. UNIX encapsulates these differences within a uniform
structure by using an object-oriented teclucique. The open-file record, shown in
13.4 577
system-wide open-file table
1;.;:.::, .;,.';t;.,' 
file-system record 1:~. s  :  1~iF ~. ..  
inode pointer +.,; :, : . :.'  
pointer to read and write functions I  .  . . :: i
,;.l,
pointer to select function
;;:;:,:;.~Ti~~~~~ple pointer to ioctl function
file descriptor
.,_ ..: ;: \ '':'' pointer to close function
n .:.r ..  :...   :  . .
.. :      :  .  . . . r;i~~~}~~~kl :f   
user-process memory networking (socket) record I :'~1~t~6~!V'.'.
pointer to network info +f:.;.   ......
pointer to read and write.functions
:, ,  .
~   ..  .
pointer to select function
pointer to ioctl function
pointer to close f..un ction .
kernel memory
Figure 13.  12 UNIX 1/0 kernel structure.
Figure 13.12, contains a dispatch table that holds pointers to the appropriate
routines, depending on the type of file.
Some operating systems use object-oriented methods even more extensively.
For instance, Windows NT uses a message-passing implementation for
I/0. An I/0 request is converted into a message that is sent through the kernel
to the II 0 manager and then to the device driver, each of which may change the
message contents. For output, the message contains the data to be written. For
input, the message contains a buffer to receive the data. The message-passing
approach can add overhead, by comparison with procedural techniques that
use shared data structures, but it simplifies the structure and design of the I/0
system and adds flexibility.
13.4.8 Kernel I/O Subsystem Summary
In summary, the I/0 subsystem coordinates an extensive collection of services
that are available to applications and to other parts of the kernel. The I/0
subsystenc supervises these procedures:
Management of the name space for files and devices
Access control to files and devices
Operation control (for example, a modem cannot seek ())
File-system space allocation
Device allocation
578 Chapter 13
13.5
Buffering, caching, and spooling
I/0 scheduling
Device-status monitoring, error handling, and failure recovery
Device-driver configuration and initialization
The upper levels of the I/O subsystem access devices via the uniform
interface provided by the device drivers.
Earlier, we described the handshaking between a device driver and a device
controller, but we did not explain how the operating system connects an
application request to a set of network wires or to a specific disk sector.
Consider, for example, reading a file from disk. The application refers to the
data by a file name. Within a disk, the file system maps from the file name
through the file-system directories to obtain the space allocation of the file. For
instance, in MS-DOS, the name maps to a number that indicates an entry in the
file-access table, and that table entry tells which disk blocks are allocated to
the file. In UNIX, the name maps to an inode number, and the corresponding
inode contains the space-allocation information. But how is the connection
made from the file name to the disk controller (the hardware port address or
the memory-mapped controller registers) 
One method is that used by MS-DOS, a relatively simple operating system.
The first part of an MS-DOS file name, preceding the colon, is a string that
identifies a specific hardware device. For example, c: is the first part of every
file name on the primary hard disk. The fact that c: represents the primary hard
disk is built into the operating system; c: is mapped to a specific port address
through a device table. Because of the colon separator, the device name space
is separate from the file-system name space. This separation makes it easy
for the operating system to associate extra functionality with each device. For
instance, it is easy to invoke spooling on any files written to the printer.
If, instead, the device name space is incorporated in the regular file-system
name space, as it is in UNIX, the normal file-system name services are provided
automatically. If the file system provides ownership and access control to all
file names, then devices have owners and access control. Since files are stored
on devices, such an interface provides access to the I/O system at two levels.
Names can be used to access the devices themselves or to access the files stored
on the devices.
UNIX represents device names in the regular file-system name space. Unlike
an MS-DOS file name, which has a colon separator, a UNIX path name has no
clear separation of the device portion. In fact, no part of the path name is the
name of a device. UNIX has a that associates prefixes of path names
with specific device names. To resolve a path name, UNIX looks up the name in
the mount table to find the longest ncatchilcg prefix; the corresponding entry
in the mount table gives the device name. This device name also has the form
of a name in the file-system name space. When UNIX looks up this name in
the file-system directory structures, it finds not an inode number but a   major,
13.5 579
minor   device number. The m.ajor device number identifies a device driver
that should be called to handle l/0 to this device. The minor device number
is passed to the device driver to index into a device table. The corresponding
device-table entry gives the port address or the memory-mapped address of
the device controller.
Modern operating systems obtain significant flexibility from the multiple
stages of lookup tables in the path between a request and a physical device
controller. The mechanisms that pass requests between applications and
drivers are general. Thus, we can introduce new devices and drivers into a
computer without recompiling the kernel. In fact, some operating systems
have the ability to load device drivers on demand. At boot time, the system
first probes the hardware buses to determine what devices are present; it then
loads in the necessary drivers, either immediately or when first required by an
I/0 request.
We next describe the typical life cycle of a blocking read request, as depicted
in Figure 13.13. The figure suggests that an I/0 operation requires a great many
steps that together consume a tremendous number of CPU cycles.
A process issues a blocking read () system call to a file descriptor of a file
that has been opened previously.
The system-call code in the kernel checks the parameters for correctness.
In the case of input, if the data are already available irl the buffer cache,
the data are returned to the process, and the I/O request is completed.
Otherwise, a physical I/0 must be performed. The process is removed
from the run queue and is placed on the wait queue for the device, and
the I/0 request is scheduled. Eventually, the I/0 subsystem sends the
request to the device driver. Depending on the operating system, the
request is sent via a subroutine call or an in-kernel message.
The device driver allocates kernel buffer space to receive the data and
schedules the I/0. Eventually, the driver sends commands to the device
controller by writing into the device-control registers.
The device controller operates the device hardware to perform the data
transfer.
The driver may poll for status and data, or it may have set up a DMA
transfer into kernel memory. We assume that the transfer is managed
by a DMA controller, which generates an interrupt when the transfer
completes.
The correct interrupt handler receives the interrupt via the interruptvector
table, stores any necessary data, signals the device driver, and
returns from the interrupt.
The device driver receives the signal, determines which I/0 request has
completed, determines the request's status, and signals the kernel I/0

subsystem that the request has been completed.  
The kernel transfers data or return codes to the address space of the
requesting process and moves the process from the wait queue back to
the ready queue.
580 Chapter 13
13.6
system call
device-controller commands
user
process
kernel
1/0 subsystem
kernel
1/0 subsystem
device
driver
interrupt
handler
device
controller
return from system call
interrupt
~-------tim_e~--~-)
Figure 13.13 The life cycle of an 1/0 request.
Moving the process to the ready queue unblocks the process. When the
scheduler assigns the process to the CPU, the process resumes execution
at the completion of the system call.
UNIX System V has an interesting mechanism, called that enables
an application to assemble pipelines of driver code dynamically. A stream is
a full-duplex connection between a device driver and a user-level process. It
consists of a that interfaces with the user process, a  :id
that controls the device, and zero or more between the stream
user process
13.6
I
STREAMS
modules
_j
Figure 13.14 The STREAMS structure.
581
head and the driver end. Each of these components contains a pair of queues
-a read queue and a write queue. Message passing is used to transfer data
between queues. The STREAMS structure is shown in Figure 13.14.
Modules provide the functionality of STREAMS processing; they are pushed
onto a stream by use of the ioctl () system call. For examplef a process can
open a serial-port device via a stream and can push on a module to handle
input editing. Because messages are exchanged between queues in adjacent
modules, a queue in one module may overflow an adjacent queue. To prevent
this from occurring, a queue may support Without flow control,
a queue accepts all messages and immediately sends them on to the queue
in the adjacent module without buffering them. A queue supporting flow
control buffers messages and does not accept messages without sufficient
buffer space; this process involves exchanges of control messages between
queues in adjacent modules.
A user process writes data to a device using either the write() orputmsg()
system call. The write() system call writes raw data to the stream, whereas
putmsg () allows the user process to specify a message. Regardless of the
system call used by the user process, the stream head copies the data into a
message and delivers it to the queue for the next module in line. This copying of
messages continues until the message is copied to the driver end and hence the
device. Similarly, the user process reads data from the stream head using either
the read() or getmsg () system call. If read() is used, the stream head gets
a message from its adjacent queue and returns ordinary data (an unstructured
byte stream) to the process. If getmsg () is used, a message is returned to the
process.
582 Chapter 13
13.7
STREAMS I/0 is asynchronous (or nonblocking) except when the user
process communicates with the stream~ head. When writing to the stream,
the user process will block, assuming the next queue uses flow controt until
there is room to copy the message. Likewise, the user process will block when
reading from the stream~ until data are available.
As mentioned, the driver end-like the stream head and modules-has
a read and write queue. However, the driver end must respond to interrupts,
such as one triggered when a frame is ready to be read from a network Unlike
the stream head, which may block if it is unable to copy a message to the
next queue in line, the driver end must handle all incoming data. Drivers
must support flow control as well. However, if a device's buffer is fult the
device typically resorts to dropping incoming messages. Consider a network
card whose input buffer is full. The network card must simply drop further
messages until there is ample buffer space to store incoming messages.
The benefit of using STREAMS is that it provides a framework for a
modular and incremental approach to writing device drivers and network
protocols. Modules may be used by different streams and hence by different
devices. For example, a networking module may be used by both an Ethernet
network card and a 802.11 wireless network card. Furthermore, rather than
treating character-device I/O as an unstructured byte stream, STREAMS allows
support for message boundaries and control information when communicating
between modules. Most UNIX variants support STREAMS, and it is the preferred
method for writing protocols and device drivers. For example, System V UNIX
and Solaris implement the socket mechanism using STREAMS.
I/ 0 is a major factor in system performance. It places heavy demands on the CPU
to execute device-driver code and to schedule processes fairly and efficiently
as they block and unblock The resulting context switches stress the CPU and its
hardware caches. I/O also exposes any inefficiencies in the interrupt-handling
mechanisms in the kernel. In addition, I/O loads down the memory bus during
data copies between controllers and physical memory and again durilcg copies
between kernel buffers and application data space. Coping gracefully with all
these demands is one of the major concerns of a computer architect.
Although modern computers can handle many thousands of interrupts per
second, interrupt handling is a relatively expensive task Each interrupt causes
the system to perform a state change, to execute the interrupt handler, and then
to restore state. Programmed I/0 can be more efficient than internJpt-driven
I/0, if the number of cycles spent in busy waiting is not excessive. An I/0
completion typically unblocks a process, leading to the full overhead of a
context switch.
Network traffic can also cause a high context-switch rate. Consider, for
instance, a remote login from one machine to another. Each character typed
on the local machine must be transported to the remote machine. On the local
machine, the character is typed; a keyboard interrupt is generated; and the
character is passed through the interrupt handler to the device driver, to the
kernet and then to the user process. The user process issues a network I/O
system call to send the character to the remote machine. The character then
13.7 583
flows into the local kernel, through the network layers that construct a network
packet, and into the network device driver. The network device driver transfers
the packet to the network controller, which sends the character and generates
an interrupt. The interrupt is passed back up through the kernel to cause the
network l/0 system call to complete.
Now, the remote system's network hardware receives the packet, and an
interrupt is generated. The character is unpacked from the network protocols
and is given to the appropriate network daemon. The network daemon
identifies which remote login session is involved and passes the packet to
the appropriate subdaemon for that session. Throughout this flow, there are
context switches and state switches (Figure 13.15). Usually, the receiver echoes
the character back to the sender; that approach doubles the work.
To eliminate the context switches involved in moving each character
between daemons and the kernel, the Solaris developers reimplemented the
daemon using in-kernel threads. Sun estimates that this improvement
sending system receiving system
Figure 13.15 lntercomputer communications.
584 Chapter 13
increased the maximum number of network logins from a few hundred to a
few thousand on a large server.
Other systems use separate for terminal I/0 to reduce
the interrupt burden on the main CPU. For instance, a
can multiplex the traffic from hundreds of remote terminals into one port on a
large computer. An is a dedicated, special-purpose CPU found in
mainframes and in other high-end systems. The job o  a channel is to offload
I/0 work from the main CPU. The idea is that the cham1.els keep the data flowing
smoothly, while the main CPU remains free to process the data. Like the device
controllers and DMA controllers found in smaller computers, a channel can
process more general and sophisticated programs, so channels can be tuned
for particular workloads.
We can employ several principles to improve the efficiency of I/0:
Reduce the number of context switches.
Reduce the number of times that data must be copied in memory while
passing between device and application.
Reduce the frequency of interrupts by using large transfers, smart controllers,
and polling (if busy waiting can be minimized).
Increase concurrency by using DMA-knowledgeable controllers or channels
to offload simple data copying from the CPU.
Move processing primitives into hardware, to allow their operation in
device controllers to be concurrent with CPU and bus operation.
Balance CPU, memory subsystem, bus, and I/O performance, because an
overload in any one area will cause idleness in others.
I/0 devices vary greatly in complexity. For instance, a mouse is simple. The
mouse movements and button clicks are converted into numeric values that are
passed from hardware, through the mouse device driver, to the application. By
contrast, the functionality provided by the Windows NT disk device driver is
complex. It not only manages individual disks but also implements RAID arrays
(Section 12.7). To do so, it converts an application's read or write request into a
coordinated set of disk I/0 operations. Moreover, it implements sophisticated
error-handling and data-recovery algorithms and takes many steps to optimize
disk performance.
Where should the I/0 functionality be implemented -in the device hardware,
in the device driver, or in application software  Sometimes we observe
the progression depicted in Figure 13.16.
Initially, we implement experimental I/0 algorithms at the application
level, because application code is f1exible and application bugs are unlikely
to cause system crashes. Furthermore, by developing code at the application
level, we avoid the need to reboot or reload device drivers after every
change to the code. An application-level implementation can be inefficient,
however, because of the overhead o  context switches and because the
application cannot take advantage of internal kernel data structures and
13.8
13.8 585
device code (hardware)
Figure 13.16 Device functionality progression.
kernel functionality (such as efficient in-kernel messaging, threading, and
locking).
When an application-level algorithm has demonstrated its worth, we may
reimplement it in the kernel. This can improve performance, but the development
effort is more challenging, because an operating-system kernel is
a large, complex software system. Moreover, an in-kernel implementation
must be thoroughly debugged to avoid data corruption and system
crashes.
The highest performance may be obtained through a specialized implementation
in hardware, either in the device or in the controller. The
disadvantages of a hardware implementation include the difficulty and
expense of making further improvements or of fixing bugs, the increased
development time (months rather than days), and the decreased flexibility.
For instance, a hardware RAID controller may not provide any means for
the kernel to influence the order or location of individual block reads and
writes, even if the kernel has special information about the workload that
would enable it to improve the I/0 performance.
The basic hardware elements involved in I/0 are buses, device controllers, and
the devices themselves. The work of moving data between devices and main
memory is perform.ed by the CPU as programmed I/0 or is offloaded to a DMA
controller. The kernel module that controls a device is a device driver. The
system-call interface provided to applications is designed to handle several
basic categories of hardware, including block devices, character devices,
memory-mapped files, network sockets, and programmed interval timers. The
system calls usually block the processes that issue them, but nonblocking and
586 Chapter 13
asynchronous calls are used by the kernel itself and by applications that must
not sleep while waiting for an I/0 operation to complete.
The kernel's I/O subsystem provides num.erous services. Among these
are I/0 scheduling, buffering, caching, spooling, device reservation, and error
handling. Another service, name translation, makes the connections between
hardware devices and the symbolic file names used by applications. It involves
several levels of mapping that translate from character-string names, to specific
device drivers and device addresses, and then to physical addresses of II 0 ports
or bus controllers. This mapping may occur within the file-system name space,
as it does in UNIX, or in a separate device name space, as it does in MS-DOS.
STREAMS is an implementation and methodology that provides a framework
for a modular and incremental approach to writing device drivers and
network protocols. Through streams, drivers can be stacked, with data passing
through them sequentially and bidirectionally for processing.
I/O system calls are costly in terms of CPU consumption because of the
many layers of software between a physical device and an application. These
layers imply overhead from several sources: context switching to cross the
kernel's protection boundary, signal and interrupt handling to service the I/0
devices, and the load on the CPU and memory system to copy data between
kernel buffers and application space.
13.1 Write (in pseudocode) an implementation of virtual clocks, including
the queueing and management of timer requests for the kernel and
applications. Assume that the hardware provides three timer channels.
13.2 What are the advantages and disadvantages of supporting memorymapped
I/0 to device control registers 
13.3 Typically, at the completion of a device I/0, a single interrupt is raised
and appropriately handled by the host processor. In certain settings,
however, the code that is to be executed at the completion of the
I/0 can be broken into two separate pieces. The first piece executes
immediately after the I/0 completes and schedules a second interrupt
for the remaining piece of code to be executed at a later time. What is
the purpose of using this strategy in the design of interrupt handlers 
13.4 Why might a system use interrupt-driven I/0 to manage a single serial
port and polling I/0 to manage a front-end processor, such as a termii1.al
concentrator 
13.5 What are the various kinds of performance overhead associated with
servicing an interrupt 
13.6 UNIX coordinates the activities of the kernel I/0 components by
manipulating shared in-kernel data structures, whereas Windows NT
uses object-oriented message passing between kernel I/O components.
Discuss three pros and three cons of each approach.
587
13.7 In most multiprogrammed systems, user programs access memory
through virtual addresses, while the operating system uses raw physical
addresses to access men10ry. What are the implications of this
design for the initiation of I/0 operations by the user program and
their execution by the operating system 
13.8 Polling for an I/0 completion can waste a large number of CPU cycles
if the processor iterates a busy-waiting loop many times before the I/0
completes. But if the I/0 device is ready for service, polling can be much
more efficient than is catching and dispatching an interrupt. Describe
a hybrid strategy that combines polling, sleeping, and interrupts for
I/0 device service. For each of these three strategies (pure polling, pure
interrupts, hybrid), describe a computing environment in which that
strategy is more efficient than is either of the others.
13.9 Consider the following I/0 scenarios on a single-user PC:
a. A mouse used with a graphical user interface
b. A tape drive on a multitasking operating system (with no device
preallocation available)
c. A disk drive containing user files
d. A graphics card with direct bus connection, accessible through
memory-mapped I/0
For each of these scenarios, would you design the operating system
to use buffering, spooling, caching, or a combin_ation  Would you use
polled I/O or interrupt-driven I/0  Give reasons for your choices.
13.10 The example of handshaking in Section 13.2 used 2 bits: a busy bit and a
command-ready bit. Is it possible to implement this handshaking with
only 1 bit  If it is, describe the protocol. If it is not, explain why 1 bit is
insufficient.
13.11 Discuss the advantages and disadvantages of guaranteeing reliable
transfer of data between modules in the STREAMS abstraction.
13.12 Some DMA controllers support direct virtual memory access, where
the targets of I/0 operations are specified as virtual addresses and
a translation from virtual to physical address is performed during
the DMA. How does this design complicate the design of the DMA
controller  What are the advantages of providing such functionality 
13.13 Why is it important to scale up system-bus and device speeds as CPU
speed increases 
13.14 When multiple interrupts from different devices appear at about the
same time, a priority scheme could be used to determine the order in
which the interrupts would be serviced. Discuss what issues need to
be considered in assigning priorities to different interrupts.
588 Chapter 13
13.15 Describe three circumstances under which blocking II 0 should be used.
Describe three circumstances under which nonblocking I/0 should be
used. Why not just implement nonblocking I/0 and have processes
busy-wait until their devices are ready 
Vahalia [1996] provides a good overview of I/O and networking in UNIX.
Leffler et al. [1989] detail the I/O structures and methods employed in
BSD UNIX. Milenkovic [1987] discusses the complexity of I/0 methods and
implementation. The use and programming of the various interprocesscommunication
and network protocols in UNIX are explored in Stevens
[1992]. Brain [1996] documents the Windows NT application interface. The
I/O implementation in the sample MINIX operating system is described in
Tanenbaum and Woodhull [1997]. Custer [1994] includes detailed information
on the NT message-passing implementation of I/0.
For details of hardware-level II 0 handling and memory-mapping functionality,
processor reference manuals (Motorola [1993] and Intel [1993]) are among
the best sources. Hennessy and Patterson [2002] describe multiprocessor systems
and cache-consistency issues. Tanenbaum [1990] describes hardware I/0
design at a low level, and Sargent and Shoemaker [1995] provide a programmer's
guide to low-level PC hardware and software. The IBM PC device I/O
address map is given in IBM [1983]. The March 1994 issue of IEEE Computer is
devoted to I/0 hardware and software. Raga [1993] provides a good discussion
of STREAMS.
Part Six
Protection mechanisms control access to a system by limiting the types
of file access permitted to users. In addition, protection must ensure
that only processes that have gained proper authorization from the
operating system can operate on memory segments, the CPU, and other
resources.
Protection is provided by a mechanism that controls the access of
programs, processes, or users to the resources defined by a computer
system. This mechanism must provide a means for specifying the controls
to be imposed, together with a means of enforcing them.
Security ensures the authentication of system users to protect the
integrity of the information stored in the system (both data and code),
as well as the physical resources of the computer system. The security
system prevents unauthorized access, malicious destruction 01- alteration
of data, and accidental introduction of inconsistency.

14.1
CHAPTER
The processes in an operating system must be protected from one another's
activities. To provide such protection, we can use various mechanisms to ensure
that only processes that have gained proper authorization from the operating
system can operate on the files, memory segments, CPU, and other resources
of a system.
Protection refers to a mechanism for controlling the access of programs,
processes, or users to the resources defined by a computer system. This
mechanism must provide a means for specifying the controls to be imposed,
together with a means of enforcement. We distinguish between protection and
security, which is a measure of confidence that the integrity of a system and
its data will be preserved. In this chapter, we focus on protection. Security
assurance is a much broader topic, and we address it in Chapter 15.
To discuss the goals and principles of protection in a modern computer
system.
   To explain how protection domains, combined with an access matrix, are
used to specify the resources a process may access.
To examine capability- and language-based protection systems.
As computer systems have become more sophisticated and pervasive in their
applications, the need to protect their integrity has also grown. Protection was
originally conceived as an adjunct to multiprogramming operating systems,
so that untrustworthy users might safely share a common logical name space,
such as a directory of files, or share a common physical name space, such as
memory. Modern protection concepts have evolved to increase the reliability
of any complex system that makes use of shared resources.
We need to provide protection for several reasons. The most obvious is the
need to prevent the mischievous, intentional violation of an access restriction
591
592 Chapter 14
14.2
by a user. Of more general importance, however, is the need to ensure that
each program component active in a system uses system resources only in
ways consistent with stated policies. This requirement is an absolute one for a
reliable system.
Protection can improve reliability by detecting latent errors at the interfaces
between component subsystems. Early detection of interface errors can often
prevent contamination of a healthy subsystem by a malfunctioning subsystem.
Also, an unprotected resource cannot defend against use (or misuse) by an
unauthorized or incompetent user. A protection-oriented system provides
means to distinguish between authorized and unauthorized usage.
The role of protection in a computer system is to provide a mechanism for
the enforcement of the policies governing resource use. These policies can be
established in a variety of ways. Some are fixed in the design of the system,
while others are formulated by the management of a system. Still others are
defined by the individual users to protect their own files and programs. A
protection system must have the flexibility to enforce a variety of policies.
Policies for resource use may vary by application, and they may change
over time. For these reasons, protection is no longer the concern solely of
the designer of an operating system. The application programmer needs to
use protection mechanisms as well, to guard resources created and supported
by an application subsystem against misuse. In this chapter, we describe the
protection mechanisms the operating system should provide, but application
designers can use them as well in designing their own protection software.
Note that mechanisms are distinct from policies. Mechanisms determine how
something will be done; policies decide what will be done. The separation
of policy and mechanism is important for flexibility. Policies are likely to
change from place to place or time to time. In the worst case, every change
in policy would require a change in the underlying mechanism. Using general
mechanisms enables us to avoid such a situation.
Frequently, a guiding principle can be used throughout a project, such as
the design of an operating system. Following this principle simplifies design
decisions and keeps the system consistent and easy to understand. A key,
time-tested guiding principle for protection is the It
dictates that programs, users, and even systems be given just enough privileges
to perform their tasks.
Consider the analogy of a security guard with a passkey. If this key allows
the guard into just the public areas that she guards, then misuse of the key
will result in minimal damage. If, however, the passkey allows access to all
areas, then damage from its being lost, stolen, misused, copied, or otherwise
compromised will be much greater.
An operating system following the principle of least privilege implements
its features, programs, system calls, and data structures so that failure or
compromise of a component does the minimum damage and allows the
n1inimum damage to be done. The overflow of a buffer in a system daemon
might cause the daemon process to fail, for example, but should not allow the
execution of code from the daemon process's stack that would enable a remote
14.3
14.3 593
user to gain maximum privileges and access to the entire system (as happens
too often today).
Such an operating system also provides system calls and services that
allow applications to be written with fine-grained access controls. It provides
mechanisms to enable privileges when they are needed and to disable them
when they are not needed. Also beneficial is the creation of audit trails for
all privileged function access. The audit trail allows the prograrnmer, systems
administrator, or law-enforcement officer to trace all protection and security
activities on the system.
Managing users with the principle of least privilege entails creating a
separate account for each user, with just the privileges that the user needs. An
operator who needs to mount tapes and back up files on the system has access
to just those commands and files needed to accomplish the job. Some systems
implement role-based access control (RBAC) to provide this functionality.
Computers implemented in a computing facility under the principle of least
privilege can be limited to running specific services, accessing specific remote
hosts via specific services, and doing so during specific times. Typically, these
restrictions are implemented through enabling or disabling each service and
through using access control lists, as described in Sections 10.6.2 and 14.6.
The principle of least privilege can help produce a more secure computing
environment. Unfortunately, it frequently does not. For example, Windows
2000 has a complex protection scheme at its core and yet has many security
holes. By comparison, Solaris is considered relatively secure, even though it
is a variant of UNIX, which historically was designed with little protection
in mind. One reason for the difference may be that Windows 2000 has more
lines of code and more services than Solaris and thus has more to secure and
protect. Another reason could be that the protection scheme in Windows 2000
is irtcomplete or protects the wrong aspects of the operating system, leaving
other areas vulnerable.
A computer system is a collection of processes and objects. By objects, we mean
both (such as the CPU, memory segments, printers, disks, and
tape drives) and (such as files, programs, and semaphores).
Each object has a unique name that differentiates it from all other objects in the
system, and each can be accessed only through well-defined and meaningful
operations. Objects are essentially abstract data types.
The operations that are possible may depend on the object. For example,
on a CPU, we can only execute. Memory segments can be read and written,
whereas a CD-ROM or DVD-ROM can only be read. Tape drives can be read,
written, and rewound. Data files can be created, opened, read, written, closed,
and deleted; program files can be read, written, executed, and deleted.
A process should be allowed to access only those resources for which it
has authorization. Furthermore, at any time, a process should be able to access
only those reso1Jrces that it currently reqLlires to complete its task. This second
requirement, conunonly referred to as the need-to-know principle, is useful in
limiting the amount of damage a faulty process can cause in the system. For
example, when process p invokes procedure A(), the procedure should be
594 Chapter14
allowed to access only its own variables and the formal parameters passed
to it; it should not be able to access all the variables of process p. Similarly,
consider the case in which process p invokes a compiler to compile a particular
file. The compiler should not be able to access files arbitrarily but should have
access only to a well-defined subset of files (such as the source file, listing file,
and so on) related to the file to be compiled. Conversely, the compiler may have
private files used for accounting or optimization purposes that process p should
not be able to access. The need-to-know principle is similar to the principle of
least privilege discussed in Section 14.2 in that the goals of protection are to
minimize the risks of possible security violations.
14.3.1 Domain Structure
To facilitate the scheme just described, a process operates within a
which specifies the resources that the process may access. Each
domain defines a set of objects and the types of operations that may be invoked
on each object. The ability to execute an operation on an object is an
A domain is a collection of access rights, each of which is an ordered pair
  object-name, rights-set  . For example, if domain D has the access right   file F,
{read, write}  , then a process executing in domain D can both read and write
file F; it cannot, however, perform any other operation on that object.
Domains do not need to be disjoint; they may share access rights. For
example, in Figure 14.1, we have three domains: D1, D2, and D3 . The access
right    0 4, {print}   is shared by D2 and D3, implying that a process executing
in either of these two domains can print object 0 4 . Note that a process must be
executing in domain D1 to read and write object 0 1, while only processes in
domain D3 may execute object 0 1.
The association between a process and a domain may be either if
the set of resources available to the process is fixed throughout the process's
lifetime, or As might be expected, establishing dynamic protection
domains is more complicated than establishing static protection domains.
If the association between processes and domains is fixed, and we want to
adhere to the need-to-know principle, then a mechanism must be available to
change the content of a domain. The reason stems from the fact that a process
may execute in two different phases and may, for example, need read access
in one phase and write access in another. If a domain is static, we must define
the domain to include both read and write access. However, this arrangement
provides more rights than are needed in each of the two phases, since we have
read access in the phase where we need only write access, and vice versa.
Thus, the need-to-know principle is violated. We must allow the contents of
   0 3 , {read, write}   
   0 1,.{read, write}  
   0 2 , {execute}   
   0 2, {write}  
   01, {execute}  
   0 3 , {read}  
Figure 14.1 System with three protection domains.
14.3 595
a domain to be modified so that the domain always reflects the n1inimum
necessary access rights.
If the association is dynamic, a mechanism is available to allow
enabling the process to switch from one domain to another. We may
also want to allow the content of a domain to be changed. If we cannot change
the content of a domain, we can provide the same effect by creating a new
domain with the changed content and switching to that new domain when we
want to change the domain content.
A domain can be realized in a variety of ways:
Each user may be a domain. In this case, the set of objects that can be
accessed depends on the identity of the user. Domain switching occurs
when the user is changed -generally when one user logs out and another
user logs in.
Each process may be a domain. In this case, the set of objects that can be
accessed depends on the identity of the process. Domain switching occurs
when one process sends a message to another process and then waits for
a response.
Each procedure may be a domain. In this case, the set of objects that can be
accessed corresponds to the local variables defined within the procedure.
Domain switching occurs when a procedure call is made.
We discuss domain switching in greater detail in Section 14.4.
Consider the standard dual-mode (monitor-user mode) model of
operating-system execution. When a process executes in monitor mode, it
can execute privileged instructions and thus gain complete control of the
computer system. In contrast, when a process executes in user mode, it can
invoke only nonprivileged instructions. Consequently, it can execute only
within its predefined memory space. These two modes protect the operating
system (executing in monitor domain) from the user processes (executing
in user domain). In a multiprogrammed operating system, two protection
domains are insufficient, since users also want to be protected from one
another. Therefore, a more elaborate scheme is needed. We illustrate such a
scheme by examining two influential operating systems-UNIX and MULTICS
-to see how they implement these concepts.
14.3.2 An Example: UNIX
In the UNIX operating system, a domain is associated with the user. Switching
the domain corresponds to changing the user identification temporarily.
This change is accomplished tbough the file system as follows. An owner
identification and a domain bit (known as the setuid bit) are associated with
each file. When the setuid bit is on, and a user executes that file, the user ID is
set to that of the owner of the file; when the bit is off, however, the user ID does
not change. For example, when a user A (that is, a user with useriD =A) starts
executing a file owned by B, whose associated domain bit is off, the useriD of
the process is set to A. When the setuid bit is on, the useriD is set to that of
the owner of the file: B. When the process exits, this temporary useriD change
ends.
596 Chapter 14
Other methods are used to change domains in operating systems in which
user IDs are used for domain definition, because almost all systems need
to provide such a mechanism. This mechanism is used when an otherwise
privileged facility needs to be made available to the general user population.
For instance, it might be desirable to allow users to access a network without
letting them write their own networking programs. In such a case, on a UNIX
system, the setuid bit on a networking program. would be set, causing the user
lD to change when the program was run. The user lD would change to that
of a user with network access privilege (such as root, the most powerful user
ID). One problem with this method is that if a user manages to create a file
with user ID root and with its setuid bit on, that user can become root and do
anything and everything on the system. The setuid mechanism is discussed
further in Appendix A.
An alternative to this method used in other operating systems is to place
privileged programs in a special directory. The operating system would be
designed to change the user lD of any program run from this directory, either
to the equivalent of root or to the user lD of the owner of the directory. This
eliminates one security problem with setuid programs in which crackers create
and hide such programs for later use (using obscure file or directory names).
This method is less flexible than that used in UNIX, however.
Even more restrictive, and thus more protective, are systems that simply
do not allow a change of user ID. In these instances, special techniques must
be used to allow users access to privileged facilities. For instance, a
may be started at boot time and run as a special user ID. Users then
run a separate program, which sends requests to this process whenever they
need to use the facility. This method is used by the TOPS-20 operating system.
In any of these systems, great care must be taken in writing privileged
programs. Any oversight can result in a total lack of protection on the system.
Generally, these programs are the first to be attacked by people trying to
break into a system; unfortunately, the attackers are frequently successful.
For example, security has been breached on many UNIX systems because of the
setuid feature. We discuss security in Chapter 15.
14.3.3 An Example: MUL TICS
In the MULTICS system, the protection domains are organized hierarchically
into a ring structure. Each ring corresponds to a single domain (Figure 14.2).
The rings are numbered from 0 to 7. Let D; and Dj be any two domain rings.
If j    i, then D; is a subset of Dj- That is, a process executing in domain Dj
has more privileges than does a process executing in domain D;. A process
executing in domain Do has the most privileges. If only two rings exist, this
scheme is equivalent to the monitor-user n1ode of execution, where monitor
mode corresponds to Do and user mode corresponds to D1.
MULTICS has a segmented address space; each segment is a file, and each
segment is associated with one of the rings. A segm.ent description includes an
entry that identifies the ring number. In addition, it includes three access bits
to control reading, writing, and execution. The association between segments
and rings is a policy decision with which we are not concerned here.
A current-ring-number counter is associated with each process, identifying
the ring in which the process is executing currently. When a process is executing
14.3 597
Figure 14.2 MULTICS ring structure.
in ring i, it cmmot access a segment associated with ring j (j    i). It can access a
segment associated with ring k (k::: i). The type of access, however, is restricted
according to the access bits associated with that segment.
Domain switching in MULTICS occurs when a process crosses from one ring
to another by calling a procedure in a different ring. Obviously, this switch must
be done in a controlled mmmer; otherwise, a process could start executing in
ring 0, and no protection would be provided. To allow controlled domain
switching, we modify the ring field of the segment descriptor to include the
following:
Access bracket. A pair of integers, bl and b2, such that bl ::=: b2.
Limit. An integer b3 such that b3    b2.
List of gates. Identifies the entry points (or
may be called.
at which the segments
If a process executing in ring i calls a procedure (or segncent) with access bracket
(bl,b2), then the call is allowed if bl ::=: i ::=: b2, and the current ring number of
the process remains i. Otherwise, a trap to the operating system occurs, and
the situation is handled as follows:
If i    bl, then the call is allowed to occur, because we have a transfer to a
ring (or domain) with fewer privileges. However, if parameters are passed
that refer to segments in a lower ring (that is, segments not accessible to
the called procedure), then these segments must be copied into an area
that can be accessed by the called procedure.
If i    b2, then the call is allowed to occur only if b3 is greater than or equal
to i and the call has been directed to one of the designated entry points in
the list of gates. This scheme allows processes with limited access rights to
call procedures in lower rings that have more access rights, but only in a
carefully controlled mmmer.
598 Chapter 14
14.4
The main disadvantage of the ring (or hierarchical) structure is that it does
not allow us to enforce the need-to-know principle. In particular, if an object
must be accessible in domain 0 J but not accessible in domain Oi, then we must
have j    i. But this requirement means that every segment accessible in Oi is
also accessible in 0 1.
The MULTICS protection system is generally more complex and less efficient
than are those used in current operating systems. If protection interferes with
the ease of use of the system or significantly decreases system performance,
then its use must be weighed carefully against the purpose of the system. For
instance, we would want to have a complex protection system on a computer
used by a university to process students' grades and also used by students for
classwork. A similar protection system would not be suited to a computer being
used for number crunching, in which performance is of utmost importance. We
would prefer to separate the mechanism from the protection policy, allowing
the same system to have complex or simple protection depending on the needs
of its users. To separate mechanism from policy, we require a more general
model of protection.
Our model of protection can be viewed abstractly as a matrix, called an
The rows of the access matrix represent domains, and the columns
represent objects. Each entry in the matrix consists of a set of access rights.
Because the column defines objects explicitly, we can omit the object name
from the access right. The entry access(i,j) defines the set of operations that a
process executing in domain Oi can invoke on object OJ.
To illustrate these concepts, we consider the access matrix shown in Figure
14.3. There are four domains and four objects-three files (F1, F2, F3) and one
laser printer. A process executing in domain 0 1 can read files F1 and F3 . A
process executing in domain 0 4 has the same privileges as one executing in
domain 0 1; but in addition, it can also write onto files F1 and F3 . Note that the
laser printer can be accessed only by a process executing in domain 0 2.
The access-matrix scheme provides us with the mechanism for specifying
a variety of policies. The mechanism consists of implementing the access
01 read read
02 print
03 read execute
04
read read
write write
Figure 14.3 Access matrix.
14.4 599
matrix and ensuring that the semantic properties we have outlined hold.
More specifically, we must ensure that a process executing in domain n can
access only those objects specified in row ( and then only as allowed by the
access-matrix entries.
The access matrix can implement policy decisions concerning protection.
The policy decisions involve which rights should be included in the (i,j)th
entry. We must also decide the domain in which each process executes. This
last policy is usually decided by the operating system.
The users normally decide the contents of the access-matrix entries. When
a user creates a new object Oi, the column Oi is added to the access matrix
with the appropriate initialization entries, as dictated by the creator. The user
may decide to enter some rights in some entries in cohum1 j and other rights
in other entries, as needed.
The access matrix provides an appropriate mechanism for defining and
implementing strict control for both the static and dynamic association between
processes and domains. When we switch a process from one domain to another,
we are executing an operation (switch) on an object (the domain). We can
control domain switching by including domains among the objects of the
access matrix. Similarly, when we change the content of the access matrix,
we are performing an operation on an object: the access matrix. Again, we
can control these changes by including the access matrix itself as an object.
Actually, since each entry in the access matrix may be modified individually,
we must consider each entry in the access matrix as an object to be protected.
Now, we need to consider only the operations possible on these new objects
(domains and the access matrix) and decide how we want processes to be able
to execute these operations.
Processes should be able to switch from one domain to another. Switching
from domain D; to domain Di is allowed if and only if the access right switch
E access(i,j). Thus, in Figure 14.4, a process executing in domain D2 can switch
to domain D3 or to domain D4 . A process in domain D4 can switch to D1, and
one in domain D1 can switch to D2-
Allowing controlled change in the contents of the access-matrix entries
requires three additional operations: copy, owner, and control. We examine
these operations next.
01 read read switch
02 print switch switch
03 read execute
04 read read switch
write write
Figure 14.4 Access matrix of Figure 14.3 with domains as objects.
600 Chapter 14
(a)
execute read   execute
execute read
(b)
Figure 14.5 Access matrix with copy rights.
The ability to copy an access right from one domain (or row) of the access
matrix to another is denoted by an asterisk (  ) appended to the access right.
The copy right allows the access right to be copied only within the colurrm.
(that is, for the object) for which the right is defined. For example, in Figure
14.5(a), a process executing in domain D2 can copy the read operation into any
entry associated with file F2 . Hence, the access matrix of Figure 14.5(a) can be
modified to the access matrix shown in Figure 14.5(b ).
This scheme has two variants:
A right is copied from access(i, j) to access(Jc, j); it is then removed from
access(i, j). This action is a transfer of a right, rather than a copy.
Propagation of the copy right may be limited. That is, when the right
R   is copied from access(i,j) to access(lc,j), only the right R (not R  )
is created. A process executing in domain D~r cannot further copy the
right R.
A system may select only one of these three copy rights, or it may provide all
three by identifying them as separate rights: copy, transfer, and limited copy.
We also need a mechanism to allow addition of new rights and removal of
some rights. The owner right controls these operations. If access(i, j) includes
the owner right, then a process executing in domain Di can add and remove
any right in any entry in column j. For example, in Figure 14.6(a), domain D1
is the owner of F1 and thus can add and delete any valid right in column F1.
Similarly, domain D2 is the owner of F2 and F3 and thus can add and remove
any valid right within these two columns. Thus, the access matrix of Figure
14.6(a) can be modified to the access matrix shown in Figure 14.6(b).
14.4 601
01 owner
write
execute
read  
read  
02 owner
owner
write
03 execute
(a)
01 owner
write
execute
owner read  
02 read   owner
write   write
03 write write
(b)
Figure 14.6 Access matrix with owner rights.
The copy and owner rights allow a process to change the entries in a column.
A mechanism is also needed to change the entries in a row. The control right
is applicable only to domain objects. If access(i, j) includes the control right,
then a process executing in domain Di can remove any access right from
row j. For example, suppose that, in Figure 14.4, we include the control right in
access(D2, D4). Then, a process executil1.g in domain D2 could modify domai11
D4, as shown in Figure 14.7.
read read switch
print switch switch
control
read execute
write write switch
Figure 14.7 Modified access matrix of Figure 14.4.
602 Chapter 14
14.5
The copy and owner rights provide us with a mechanism to limit the
propagation of access rights. However, they do not give us the appropriate tools
for preventing the propagation (or disclosure) of information. The problem. of
guaranteeing that no information initially held in an object can migrate outside
of its execution environment is called the . This problem
is in general unsolvable (see the bibliographical notes at the end of the chapter).
These operations on the domains and the access matrix are not in themselves
important, but they illustrate the ability of the access-matrix model to
allow the implementation and control of dynamic protection requirements.
New objects and new domains can be created dynamically and included in the
access-matrix model. However, we have shown only that the basic mechanism
exists; system designers and users must make the policy decisions concerning
which domains are to have access to which objects in which ways.
How can the access matrix be implemented effectively  In general, the matrix
will be sparse; that is, most of the entries will be empty. Although datastructure
techniques are available for representing sparse matrices, they are
not particularly useful for this application, because of the way in which
the protection facility is used. Here, we first describe several methods of
implementing the access matrix and then compare the methods.
14.5.1 Global Table
The simplest implementation of the access matrix is a global table consisting
of a set of ordered triples   domain, object, rights-set  . Whenever an operation
M is executed on an object Oj within domain D;, the global table is searched
for a triple   D;, 0 1, R~c  , with ME R~c. If this triple is found, the operation is
allowed to continue; otherwise, an exception (or error) condition is raised.
This implementation suffers from several drawbacks. The table is usually
large and thus cannot be kept in main memory, so additional I/0 is needed.
Virtual memory techniques are often used for managing this table. In addition,
it is difficult to take advantage of special groupings of objects or domains.
For example, if everyone can read a particular object, this object must have a
separate entry in every domain.
14.5.2 Access Lists for Objects
Each column in the access matrix can be implemented as an access list for
one object, as described in Section 10.6.2. Obviously, the empty entries can be
discarded. The resulting list for each object consists of ordered pairs   domain,
rights-set  , which define all domains with a nonempty set of access rights for
that object.
This approach can be extended easily to define a list plus a default set of
access rights. When an operation M on an object Oi is attempted in domain
D;, we search the access list for object 0 i, looking for an entry    D;, R1c    with
ME RJc. If the entry is found, we allow the operation; if it is not, we check the
default set. If M is in the default set, we allow the access. Otherwise, access is
14.5 603
denied, and an exception condition occurs. For efficiency, we may check the
default set first and then search the access list.
14.5.3 Capability Lists for Domains
Rather than associating the columns of the access matrix with the objects as
access lists, we can associate each row with its domain. A ltst for
a domain is a list of objects together with the operations allowed on tbose
objects. An object is often represented by its physical name or address, called
a To execute operation M on object 0 1, the process executes the
operation M, specifying the capability (or pointer) for object 0 j as a parameter.
Simple of the capability means that access is allowed.
The capability list is associated with a domain, but it is never directly
accessible to a process executing in that domain. Rather, the capability list
is itself a protected object, maintained by the operating system and accessed
by the user only indirectly. Capability-based protection relies on the fact that
the capabilities are never allowed to migrate into any address space directly
accessible by a user process (where they could be modified). If all capabilities
are secure, the object they protect is also secure against unauthorized access.
Capabilities were originally proposed as a kind of secure pointer, to
meet the need for resource protection that was foreseen as multiprogrammed
computer systems came of age. The idea of an inherently protected pointer
provides a fom1dation for protection that can be extended up to the applications
level.
To provide inherent protection, we must distinguish capabilities from other
kinds of objects, and they must be interpreted by an abstract machine on which
higher-level programs run. Capabilities are usually distinguished from other
data in one of two ways:
Each object has a to denote whether it is a capability or accessible
data. The tags themselves must not be directly accessible by an application
program. Hardware or firmware support may be used to enforce this
restriction. Although only one bit is necessary to distinguish between
capabilities and other objects, more bits are often used. This extension
allows all objects to be tagged with their types by the hardware. Thus,
the hardware can distinguish integers, floating-point numbers, pointers,
Booleans, characters, instructions, capabilities, and uninitialized values by
their tags.
Alternatively, the address space associated with a program can be split into
two parts. One part is accessible to the program and contains the program's
normal data and instructions. The other part, containing the capability list,
is accessible only by the operating system. A segmented memory space
(Section 8.6) is useful to support this approach.
Several capability-based protection systems have been developed; we describe
them briefly in Section 14.8. The Mach operating system also uses a version of
capability-based protection; it is described in Appendix B.
604 Chapter 14
14.5.4 A Lock-Key Mechanism
The t   ' is a compromise between access lists and capability
lists. Each object has a list of unique bit patterns, called Similarly, each
domain has a list of unique bit patterns, called A process executing in a
domain can access an object only if that domain has a key that matches one of
the locks of the object.
As with capability lists, the list of keys for a domain must be managed
by the operating system on behalf of the domain. Users are not allowed to
examine or modify the list of keys (or locks) directly.
14.5.5 Comparison
As you might expect choosing a technique for implementing an access matrix
involves various trade-offs. Using a global table is simple; however, the table
can be quite large and often cannot take advantage of special groupings of
objects or domains. Access lists correspond directly to the needs of users.
When a user creates an object he can specify which domains can access the
object as well as what operations are allowed. However, because access-rights
information for a particular domain is not localized, determining the set of
access rights for each domain is difficult. In addition, every access to the object
must be checked, requiring a search of the access list. In a large system with
long access lists, this search can be time consuming.
Capability lists do not correspond directly to the needs of users; they
are usefut however, for localizing information for a given process. The
process attempting access must present a capability for that access. Then, the
protection system needs only to verify that the capability is valid. Revocation
of capabilities, however, may be inefficient (Section 14.7).
The lock-key mechanism, as mentioned, is a compromise between access
lists and capability lists. The mechanism can be both effective and flexible,
depending on the length of the keys. The keys can be passed freely from
domain to domain. In addition, access privileges can be effectively revoked by
the simple technique of changing some of the locks associated with the object
(Section 14.7).
Most systems use a combination of access lists and capabilities. When a
process first tries to access an object, the access list is searched. If access is
denied, an exception condition occurs. Otherwise, a capability is created and
attached to the process. Additional references use the capability to demonstrate
swiftly that access is allowed. After the last access, the capability is destroyed.
This strategy is used in the MULTICS system and in the CAL system.
As an example of how such a strategy works, consider a file system in
which each file has an associated access list. When a process opens a file, the
directory structure is searched to find the file, access permission is checked, and
buffers are allocated. All this information is recorded in. a new entry in a file
table associated with the process. The operation returns an index into this table
for the newly opened file. All operations on the file are made by specification
of the index into the file table. The entry in the file table then points to the file
and its buffers. When the file is closed, the file-table entry is deleted. Since the
file table is maintained by the operating system, the user carmot accidentally
corrupt it. Thus, the user can access only those files that have been opened.
14.6
14.6 605
Since access is checked when the file is opened, protection is ensured. This
strategy is used in the UNIX system.
The right to access must still be checked or1 each access, and the file-table
entry has a capability only for the allowed operations. If a file is opened for
reading, then a capability for read access is placed in the file-table entry. If
an attempt is made to write onto the file, the system identifies this protection
violation by com.paring the requested operation with the capability in the
file-table entry.
In Section 10.6.2, we described how access controls can be used on files within a
file system. Each file and directory are assigned an owner, a group, or possibly
a list of users, and for each of those entities, access-control information is
assigned. A similar function can be added to other aspects of a computer
system. A good example of this is found in Solaris 10.
Solaris 10 advances the protection available in the Sun Microsystems
operating system by explicitly adding the principle of least privilege via
This facility revolves around privileges.
A privilege is the right to execute a system call or to use an option within
that system call (such as opening a file with write access). Privileges can be
assigned to processes,limiting them to exactly the access they need to perform
their work. Privileges and programs can also be assigned to Users are
assigned roles or can take roles based on passwords to the roles. In this way a
user can take a role that enables a privilege, allowing the user to run a program
to accomplish a specific task, as depicted in Figure 14.8. This implementation
of privileges decreases the security risk associated with superusers and setuid
programs.
user1
executes with role 1 privileges
~
Figure 14.8 Role-based access control in Solaris 10.
606 Chapter 14
14.7
Notice that this facility is similar to the access matrix described in Section
14.4. This relationship is further explored in the exercises at the end of the
chapter.
In a dynamic protection system, we may sometimes need to revoke access
rights to objects shared by different users. Various questions about revocation
may arise:
Immediate versus delayed. Does revocation occur immediately, or is it
delayed  If revocation is delayed, can we find out when it will take place 
Selective versus general. When an access right to an object is revoked,
does it affect all the users who have an access right to that object, or can
we specify a select group of users whose access rights should be revoked 
Partial versus total. Can a subset of the rights associated with an object be
revoked, or must we revoke all access rights for this object 
Temporary versus permanent. Can access be revoked permanently (that
is, the revoked access right will never again be available), or can access be
revoked and later be obtained again 
With an access-list scheme, revocation is easy. The access list is searched for
any access rights to be revoked, and they are deleted from the list. Revocation
is immediate and can be general or selective, total or partial, and permanent
or temporary.
Capabilities, howeve1~ present a much more difficult revocation problem,
as mentioned earlier. Since the capabilities are distributed throughout the
system, we must find them before we can revoke them. Schemes that implement
revocation for capabilities include the following:
Reacquisition. Periodically, capabilities are deleted from each domain. If
a process wants to use a capability, it may find that that capability has been
deleted. The process may then try to reacquire the capability. If access has
been revoked, the process will not be able to reacquire the capability.
Back-pointers. A list of pointers is maintained with each object, pointing
to all capabilities associated with that object. When revocation is required,
we can follow these pointers, changing the capabilities as necessary. This
scheme was adopted in the MULTICS system. It is quite general, but its
implementation is costly.
Indirection. The capabilities point indirectly, not directly, to the objects.
Each capability points to a unique entry in a global table, which in turn
points to the object. We implement revocation by searching the global table
for the desired entry and deleting it. Then, when an access is attempted,
the capability is found to point to an illegal table entry. Table entries can
be reused for other capabilities without difficulty, since both the capability
and the table entry contain the unique name of the object. The object for a
14.8
14.8 607
capability and its table entry must match. This scheme was adopted in the
CAL system. It does not allow selective revocation.
Keys. A key is a unique bit pattern that can be associated with a capability.
This key is defined when the capability is created, and it can be neither
modified nor inspected by the process that owns the capability. A
is associated with each object; it can be defined or replaced with
the set-key operation. When a capability is created, the current value
of the master key is associated with the capability. When the capability
is exercised, its key is compared with the master key. If the keys match,
the operation is allowed to continue; otherwise, an exception condition
is raised. Revocation replaces the master key with a new value via the
set-key operation, invalidating all previous capabilities for this object.
This scheme does not allow selective revocation, since only one master
key is associated with each object. If we associate a list of keys with each
object, then selective revocation can be implemented. Finally, we can group
all keys into one global table of keys. A capability is valid only if its
key matches some key in the global table. We implement revocation by
removing the matching key from the table. With this scheme, a key can be
associated with several objects, and several keys can be associated with
each object, providing maximum flexibility.
In key-based schemes, the operations of defining keys, inserting them
into lists, and deleting them from lists should not be available to all users.
In particular, it would be reasonable to allow only the owner of an object
to set the keys for that object. This choice, however, is a policy decision
that the protection system can implement but should not define.
In this section, we survey two capability-based protection systems. These
systems differ in their complexity and in the types of policies that can be
implemented on them. Neither system is widely used, but both provide
interesting proving grounds for protection theories.
14.8.1 An Example: Hydra
Hydra is a capability-based protection system that provides considerable
flexibility. The system implements a fixed set of possible access rights, including
such basic forms of access as the right to read, write, or execute a memory
segment. In addition, a user (of the protection system) can declare other rights.
The interpretation of user-defined rights is performed solely by the user's
program, but the system provides access protection for the use of these rights,
as well as for the use of system-defined rights. These facilities constitute a
significant development in protection technology.
Operations on objects are defined procedurally. The procedures that
implement such operations are themselves a form of object, and they are
accessed indirectly by capabilities. The names of user-defined procedures must
be identified to the protection system if it is to deal with objects of the userdefined
type. When the definition of an object is made krtOwn to Hydra, the
names of operations on the type become Auxiliary rights
608 Chapter 14
can be described in a capability for an instance of the type. For a process to
perform an operation on a typed object, the capability it holds for that object
must contain the name of the operation being invoked among its auxiliary
rights. This restriction enables discrin  lination of access rights to be made on an
instance-by-instance and process-by-process basis.
Hydra also provides ::'!fnrWk.;:J1o:L This scheme allows a procedure
to be certified as to act on a formal parameter of a specified type
on behalf of any process that holds a right to execute the procedure. The rights
held by a trustworthy procedure are independent oC and may exceed, the
rights held by the calling process. However, such a procedure must not be
regarded as universally trustworthy (the procedure is not allowed to act on
other types, for instance), and the trustworthiness must not be extended to any
other procedures or program segments that might be executed by a process.
Amplification allows implementation procedures access to the representation
variables of an abstract data type. If a process holds a capability to a typed
object A, for instance, this capability may include an auxiliary right to invoke
some operation P but does not include any of the so-called kernel rights, such
as read, write, or execute, on the segment that represents A. Such a capability
gives a process a means of indirect access (through the operation P) to the
representation of A, but only for specific purposes.
When a process invokes the operation P on an object A, howeve1~ the
capability for access to A may be amplified as control passes to the code body
of P. This amplification may be necessary to allow P the right to access the
storage segment representing A so as to implement the operation that P defines
on the abstract data type. The code body of P may be allowed to read or to
write to the segment of A directly, even though the calling process cmmot.
On return from P the capability for A is restored to its originat unamplified
state. This case is a typical one in which the rights held by a process for access
to a protected segment must change dynamically, depending on the task to
be performed. The dynamic adjustment of rights is performed to guarantee
consistency of a programmer-defined abstraction. Amplification of rights can
be stated explicitly in the declaration of an abstract type to the Hydra operating
system.
When a user passes an object as an argument to a procedure, we may need
to ensure that the procedure cannot modify the object. We can implement this
restriction readily by passing an access right that does not have the modification
(write) right. Howeve1~ if amplification may occur, the right to modify may
be reinstated. Thus, the user-protection requirement can be circumvented.
In generat of course, a user may trust that a procedure performs its task
correctly. This assumption is not always correct however, because of hardware
or software errors. Hydra solves this problem by restricting amplifications.
The procedure-call mechanism of Hydra was designed as a direct solution
to the problem of mutually suspicious subsystems. This problem is defined as
follows. Suppose that a program is provided that can be invoked as a service
by a number of different users (for example, a sort routine, a compile1~ a
game). When users invoke this service program, they take the risk that the
program will malfunction and will either damage the given data or retain
some access right to the data to be used (without authority) later. Similarly,
the service program may have som.e private files (for accounting purposes,
14.8 609
for example) that should not be accessed directly by the calling user program.
Hydra provides mechanisms for directly dealing with this problem.
A Hydra subsystem is built on top of its protection kernel and may require
protection of its own components. A subsystem interacts with the kernel
through calls on a set of kernel-defined primitives that define access rights to
resources defined by the subsystenl.. The subsystem designer can define policies
for use of these resources by user processes, but the policies are enforceable by
use of the standard access protection afforded by the capability system.
Programmers can make direct use of the protection system after acquainting
themselves with its features in the appropriate reference rnanual. Hydra
provides a large library of system-defined procedures that can be called by
user programs. Programmers can explicitly incorporate calls on these system
procedures into their program code or can use a program translator that has
been interfaced to Hydra.
14.8.2 An Example: Cambridge CAP System
A different approach to capability-based protection has been taken in the
design of the Cambridge CAP system. CAP's capability system is simpler and
superficially less powerful than that of Hydra. However, closer examination
shows that it, too, can be used to provide secure protection of user-defined
objects. CAP has two kinds of capabilities. The ordinary kind is called a
. It can be used to provide access to objects, but the only
rights provided are the standard read, write, and execute of the individual
storage segments associated with the object. Data capabilities are interpreted
by microcode in the CAP machine.
The second kind of capability is the so-called which
is protected, but not interpreted, by the CAP microcode. It is interpreted by a
protected (that is, privileged) procedure, which may be written by an application
programmer as part of a subsystem. A particular kind of rights amplification
is associated with a protected procedure. When executing the code body of
such a procedure, a process temporarily acquires the right to read or write the
contents of a software capability itself. This specific kind of rights amplification
corresponds to an implementation of the seal and unseal primitives on
capabilities. Of course, this privilege is still subject to type verification to ensure
that only software capabilities for a specified abstract type are passed to any
such procedure. Universal trust is not placed in any code other than the CAP
machine's microcode. (See Bibliographical Notes for references.)
The interpretation of a software capability is left completely to the subsystem,
through the protected procedures it contains. This scheme allows a
variety of protection policies to be implemented. Although programmers can
define their own protected procedures (any of which might be incorrect), the
security of the overall system cannot be compromised. The basic protection
system will not allow an unverified, user-defined, protected procedure access
to any storage segments (or capabilities) that do not belong to the protection
environment in which it resides. The most serious consequence of an insecure
protected procedure is a protection breakdown of the subsystem for which that
procedure has responsibility.
610 Chapter 14
14.9
The designers of the CAP system have noted that the use of software
capabilities allowed them to realize considerable economies in formulating
and implementing protection policies commensurate with the requirements of
abstract resources. However, subsystem designers who want to make use of
this facility cannot simply study a reference manual, as is the case with Hydra.
Instead, they must learn the principles and techniques of protection, since the
system provides them with no library of procedures.
To the degree that protection is provided in existing computer systems, it is
usually achieved through an operating-system kernel, which acts as a security
agent to inspect and validate each attempt to access a protected resource. Since
comprehensive access validation may be a source of considerable overhead,
either we must give it hardware support to reduce the cost of each validation
or we must allow the system designer to compromise the goals of protection.
Satisfying all these goals is difficult if the flexibility to implement protection
policies is restricted by the support mechanisms provided or if protection
environments are made larger than necessary to secure greater operational
efficiency.
As operating systems have become more complex, and particularly as they
have attempted to provide higher-level user interfaces, the goals of protection
have become much more refined. The designers of protection systems have
drawn heavily on ideas that originated in programming languages and
especially on the concepts of abstract data types and objects. Protection systems
are now concerned not only with the identity of a resource to which access is
attempted but also with the functional nature of that access. In the newest
protection systems, concern for the function to be invoked extends beyond
a set of system-defined functions, such as standard file-access methods, to
include functions that may be user-defined as well.
Policies for resource use may also vary, depending on the application,
and they may be subject to change over time. For these reasons, protection
can no longer be considered a matter of concern only to the designer of an
operating system. It should also be available as a tool for use by the application
designe1~ so that resources of an applications subsystem can be guarded against
tampering or the influence of an error.
14.9.1 Compiler-Based Enforcement
At this point, programming languages enter the picture. Specifying the desired
control of access to a shared resource in a system is making a declarative
statement about the resource. This kind of statement can be integrated into a
language by an extension of its typing facility. When protection is declared
along with data typing, the designer of each subsystem can specify its
requirements for protection, as well as its need for use of other resources in a
system. Such a specification should be given directly as a program is composed,
and in the language in which the program itself is stated. This approach has
several significant advantages:
14.9 611
Protection needs are simply declared, rather than programmed as a
sequence of calls on procedures of an operating system.
Protection requirements can be stated independently of the facilities
provided by a particular operating system.
The means for enforcement need not be provided by the designer of a
subsystem.
A declarative notation is natural because access privileges are closely
related to the linguistic concept of data type.
A variety of techniques can be provided by a programming-language
implementation to enforce protection, but any of these must depend on some
degree of support from an underlying machine and its operating system. For
example, suppose a language is used to generate code to run on the Cambridge
CAP system. On this system, every storage reference made on the underlying
hardware occurs indirectly through a capability. This restriction prevents any
process from accessing a resource outside of its protection environment at
any time. However, a program may impose arbitrary restrictions on how
a resource can be used during execution of a particular code segment.
We can implement such restrictions most readily by usin.g the software
capabilities provided by CAP. A language implementation might provide
standard protected procedures to interpret software capabilities that would
realize the protection policies that could be specified in the language. This
scheme puts policy specification at the disposal of the programmers, while
freeing them from implementing its enforcement.
Even if a system does not provide a protection kernel as powerful as those
of Hydra or CAP, mechanisms are still available for implementing protection
specifications given in a programming language. The principal distinction is
that the security of this protection will not be as great as that supported by
a protection kernel, because the mechanism must rely on more assumptions
about the operational state of the system. A compiler can separate references
for which it can certify that no protection violation could occur from those
for which a violation might be possible, and it can treat them differently. The
security provided by this form of protection rests on the assumption that the
code generated by the compiler will not be modified prior to or during its
execution.
What, then, are the relative merits of enforcement based solely on a kernel,
as opposed to enforcement provided largely by a compiler 
Security. Enforcement by a kernel provides a greater degree of security
of the protection system itself than does the generation of protectionchecking
code by a compiler. In a compiler-supported scheme, security
rests on correctness of the translator, on some underlying mechanism of
storage management that protects the segments from which compiled
code is executed, and, ultimately, on the security of files from which a
program is loaded. Some of these considerations also apply to a softwaresupported
protection kernel, but to a lesser degree, since the kernel may
reside in fixed physical storage segments and may be loaded only from
a designated file. With a tagged-capability system, in which all address
612 Chapter 14
computation is performed either by hardware or by a fixed microprogram,
even greater security is possible. Hardware-supported protection is also
relatively immune to protection violations that might occur as a result of
either hardware or system software malfunction.
Flexibility. There are limits to the flexibility of a protection kernel in
implementing a user-defined policy, although it may supply adequate
facilities for the system to provide enforcement of its own policies.
With a programming language, protection policy can be declared and
enforcem.ent provided as needed by an implementation. If a language
does not provide sufficient flexibility, it can be extended or replaced with
less disturbance of a system in service than would be caused by the
modification of an operating-system kerneL
Efficiency. The greatest efficiency is obtained when enforcement of protection
is supported directly by hardware (or microcode). Insofar as software
support is required, language-based enforcement has the advantage that
static access enforcement can be verified off-line at compile time. Also,
since an intelligent compiler can tailor the enforcement mechanism to
meet the specified need, the fixed overhead of kernel calls can often be
avoided.
In summary, the specification of protection in a programming language
allows the high-level description of policies for the allocation and use of
resources. A language implementation can provide software for protection
enforcement when automatic hardware-supported checking is unavailable. In
addition, it can interpret protection specifications to generate calls on whatever
protection system is provided by the hardware and the operating system.
One way of making protection available to the application program is
through the use of a software capability that could be used as an object
of computation. Inherent in this concept is the idea that certain program
components might have the privilege of creating or examining these software
capabilities. A capability-creating program would be able to execute a primitive
operation that would seal a data structure, rendering the latter's contents
inaccessible to any program components that did not hold either the seal or
the unseal privilege. Such components might copy the data structure or pass
its address to other program components, but they could not gain access to
its contents. The reason for introducing such software capabilities is to bring a
protection mechanism into the programming language. The only problem with
the concept as proposed is that the use of the seal and unseal operations takes
a procedural approach to specifying protection. A nonprocedural or declarative
notation seems a preferable way to make protection available to the application
programmer.
What is needed is a safe, dynamic access-control mechanism for distributing
capabilities to system resources among user processes. To contribute to the
overall reliability of a system, the access-control mechanism should be safe
to use. To be useful in practice, it should also be reasonably efficient. This
requirement has led to the development of a number of language constructs
that allow the programmer to declare various restrictions on the use of a specific
managed resource. (See the Bibliographical Notes for appropriate references.)
These constructs provide mechanisms for three functions:
14.9 613
Distributing capabilities safely and efficiently among customer processes.
In particular, mechanisms ensure that a user process will use the managed
resource only if it was granted a capability to that resource.
Specifying the type of operations that a particular process may invoke on
an allocated resource (for example, a reader of a file should be allowed
only to read the file, whereas a writer should be able both to read and
to write). It should not be necessary to grant the same set of rights to
every user process, and it should be impossible for a process to enlarge
its set of access rights, except with the authorization of the access-control
mechanism.
Specifying the order in which a particular process may invoke the various
operations of a resource (for example, a file must be opened before it can
be read). It should be possible to give two processes different restrictions
on the order in which they can invoke the operations of the allocated
resource.
The incorporation of protection concepts into programming languages, as
a practical tool for system design, is in its infancy. Protection will likely become
a matter of greater concern to the designers of new systems with distributed
architectures and increasingly stringent requirements on data security. Then
the importance of suitable language notations in which to express protection
requirements will be recognized more widely.
14.9.2 Protection in Java
Because Java was designed to run in a distributed environment, the Java
virtual machine-or JVM-has many built-in protection mechanisms. Java
programs are composed of each of which is a collection of data fields
and functions (called that operate on those fields. The JVM loads a
class in response to a request to create instances (or objects) of that class. One of
the most novel and useful features ofJ ava is its support for dynamically loading
untrusted classes over a network and for executing mutually distrusting classes
within the same JVM.
Because of these capabilities of Java, protection is a paramount concern.
Classes running in the same JVM may be from different sources and may not
be equally trusted. As a result, enforcing protection at the granularity of the
JVM process is insufficient. Intuitively, whether a request to open a file should
be allowed will generally depend on which class has requested the open. The
operating system lacks this knowledge.
Thus, such protection decisions are handled within the JVM. When the
JVM loads a class, it assigns the class to a protection domain that gives
the permissions of that class. The protection domain to which the class is
assigned depends on the URL from which the class was loaded and any digital
signatures on the class file. (Digital signatures are covered in Section 15.4.1.3.)
A configurable policy file determines the permissions granted to the domain
(and its classes). For example, classes loaded from a trusted server might be
placed in a protection domain that allows them to access files in the user's
home directory, whereas classes loaded from an untrusted server might have
no file access permissions at all.
614 Chapter 14
It can be complicated for the JVM to determine what class is responsible for a
request to access a protected resource. Accesses are often performed indirectly,
through system libraries or other classes. For example, consider a class that
is not allowed to open network connections. It could call a system library to
request the load of the contents of a URL. The JVM must decide whether or not
to open a network connection for this request. But which class should be used
to determine if the connection should be allowed, the application or the system
library 
The philosophy adopted in Java is to require the library class to explicitly
permit a network corucection. More generally, in order to access a protected
resource, some method in the calling sequence that resulted in the request must
explicitly assert the privilege to access the resource. By doing so, this method
takes responsibility for the request; presumably, it will also perform whatever
checks are necessary to ensure the safety of the request. Of course, not every
method is allowed to assert a privilege; a method can assert a privilege only if
its class is in a protection domain that is itself allowed to exercise the privilege.
This implementation approach is called Every thread
in the JVM has an associated stack of its ongoing invocations. When
a caller may not be trusted, a method executes an access request within a
doPri vileged block to perform the access to a protected resource directly or
indirectly. doPri vileged () is a static method in the AccessController class
that is passed a class with a run () method to invoke. When the doPri vileged
block is entered, the stack frame for this method is annotated to indicate this
fact. Then, the contents of the block are executed. When an access to a protected
resource is subsequently requested, either by this method or a method it
calls, a call to checkPermissions () is used to invoke stack inspection to
determine if the request should be allowed. The inspection examines stack
frames on the calling thread's stack, starting from the most recently added
frame and working toward the oldest. If a stack frame is first found that has the
doPri vileged () annotation, then checkPermissions () returns immediately
and silently, allowing the access. If a stack frame is first found for which
access is disallowed based on the protection domain of the method's class,
then checkPermissions () throws an AccessControlException. If the stack
inspection exhausts the stack without finding either type of frame, then
whether access is allowed depends on the implementation (for example, some
implementations of the JVM may allow access, while other implementations
may disallow it).
Stack inspection is illustrated in Figure 14.9. Here, the gui () method of
a class in the untrusted applet protection domain performs two operations,
first a get () and then an open () . The former is an invocation of the
get () method of a class in the URL loader protection domain, which is
permitted to open() sessions to sites in the lucent. com domain, in particular
a proxy server proxy .lucent. com for retrieving URLs. For this reason, the
untrusted applet's get() invocation will succeed: the checkPermissions ()
call in the networking library encounters the stack frame of the get()
method, which performed its open() in a doPri vileged block. However,
the untrusted applet's open() invocation will result in an exception, because
the checkPermissions () call finds no doPri vileged annotation before
encountering the stack frame of the gui () method.
14.10
protection
domain:
socket
permission:
class:
none
gui:
get( uri);
open(addr);
14.10
  .lucent.com:80, connect
get(URL u):
doPrivileged {
open('proxy.lucent.com:80');
}
  request u from proxy  
Figure 14.9 Stack inspection.
615
any
open(Addr a):
checkPermission
(a, connect);
connect (a);
Of course, for stack inspection to work, a program must be unable to
modify the annotations on its own stack frame or to do other manipulations
of stack inspection. This is one of the most important differences between
Java and many other languages (including C++). A Java program cannot
directly access memory; it can manipulate only an object for which it has
a reference. References cannot be forged, and the manipulations are made
only through well-defined interfaces. Compliance is enforced through a
sophisticated collection of load-time and run-time checks. As a result, an object
cannot manipulate its run-time stack, because it camlOt get a reference to the
stack or other components of the protection system.
More generally, Java's load-time and run-time checks enforce of
Java classes. Type safety ensures that classes cannot treat integers as pointers,
write past the end of an array, or otherwise access memory in arbitrary ways.
Rather, a program can access an object only via the methods defined on that
object by its class. This is the f01mdation of Java protection, since it enables a
class to effectively - and protect its data and methods from other
classes loaded in the same JVM. For example, a variable can be defined as
private so that only the class that contains it can access it or protected so
that it can be accessed only by the class that contains it, subclasses of that class,
or classes in the same package. Type safety ensures that these restrictions can
be enforced.
Computer systems contain many objects, and they need to be protected from
misuse. Objects may be hardware (such as memory, CPU time, and I/0 devices)
or software (such as files, programs, and semaphores). An access right is
permission to perform an operation on an object. A domain is a set of access
rights. Processes execute in domains and may use any of the access rights in
the domain to access and manipulate objects. During its lifetime, a process may
be either bound to a protection domain or allowed to switch from one domain
to another.
616 Chapter 14
The access matrix is a general model of protection that provides a
mechanisnc for protection without imposing a particular protection policy on
the system or its users. The separation of policy and mechanism is an important
design property.
The access matrix is sparse. It is normally implemented either as access lists
associated with each object or as capability lists associated with each domain.
We can include dynamic protection in the access-matrix model by considering
domains and the access matrix itself as objects. Revocation of access rights in a
dynamic protection model is typically easier to implement with an access-list
scheme than with a capability list.
Real systems are much more limited than the general model and tend to
provide protection only for files. UNIX is representative, providing read, write,
and execution protection separately for the owner, group, and general public
for each file. MULTICS uses a ring structure in addition to file access. Hydra, the
Cambridge CAP system, and Mach are capability systems that extend protection
to user-defined software objects. Solaris 10 implements the principle of least
privilege via role-based access controt a form of the access matrix.
Language-based protection provides finer-grained arbitration of requests
and privileges than the operating system is able to provide. For example, a
single Java JVM can run several threads, each in a different protection class. It
enforces the resource requests through sophisticated stack inspection and via
the type safety of the language.
14.1 Consider a computer system in which   computer games   can be played
by students only between 10 P.M. and 6 A.M., by faculty members
between 5 P.M. and 8 A.M., and by the computer center staff at all
times. Suggest a scheme for implementing this policy efficiently.
14.2 The RC 4000 system, among others, has defined a tree of processes (called
a process tree) such that all the descendants of a process can be given
resources (objects) and access rights by their ancestors only. Thus, a
descendant can never have the ability to do anything that its ancestors
cannot do. The root of the tree is the operating system, which has the
ability to do anything. Assume the set of access rights is represented
by an access matrix, A. A(x,y) defines the access rights of process x to
object y. If xis a descendant of z, what is the relationship between A(x,y)
and A(z,y) for an arbitrary object y 
14.3 How are the access-matrix facility and the role-based access-control
facility similar  How do they differ 
14.4 Discuss the need for rights amplification in Hydra. How does this
practice compare with the cross-ring calls in a ring-protection scheme 
617
14.5 Explain why a capability-based system such as Hydra provides greater
flexibility than the ring-protection scheme in enforcing protection
policies.
14.6 Consider the ring-protection scheme in MULTICS. If we were to implement
the system calls of a typical operating system and store them in a
segment associated with ring 0, what should be the values stored in the
ring field of the segment descriptor  What happens during a system
call when a process executing in a higher-numbered ring invokes a
procedure in ring 0 
14.7 Discuss the strengths and weaknesses of implementing an access matrix
using capabilities that are associated with domains.
14.8 Discuss the strengths and weaknesses of implementing an access matrix
using access lists that are associated with objects.
14.9 The access-control matrix can be used to determine whether a process
can switch from, say, domain A to domain B and enjoy the access
privileges of domain B. Is this approach equivalent to including the
access privileges of domain B in those of domain A 
14.10 How can systems that implement the principle of least privilege still
have protection failures that lead to security violations 
14.11 How does the principle of least privilege aid in the creation of protection
systems 
14.12 What protection problems may arise if a shared stack is used for
parameter passing 
14.13 If all the access rights to an object are deleted, the object can no longer
be accessed. At this point the object should also be deleted, and the
space it occupies should be returned to the system. Suggest an efficient
implementation of this scheme.
14.14 Discuss which of the following systems allow module designers to
enforce the need-to-know principle.
a. The MULTICS ring-protection scheme
b. Hydra's capabilities
c. JVM's stack-inspection scheme
618 Chapter 14
14.15 Consider a computing environment where a unique number is associated
with each process and each object in the system. Suppose that we
allow a process with number n to access an object with number m only
if n    m. What type of protection structure do we have 
14.16 What is the need-to-know principle  Why is it important for a protection
system to adhere to this principle 
14.17 What hardware features does a computer system need for efficient
capability manipulation  Can these features be used for memory
protection 
14.18 Describe how the Java protection model would be compromised if a
Java program were allowed to directly alter the annotations of its stack
frame.
14.19 A Burroughs B7000/B6000 MCP file can be tagged as sensitive data.
When such a file is deleted, its storage area is overwritten by some
random bits. For what purpose would such a scheme be useful 
The access-matrix model of protection between domains and objects was
developed by Lampson [1969] and Lampson [1971]. Popek [1974] and Saltzer
and Schroeder [1975] provided excellent surveys on the subject of protection.
Harrison et al. [1976] used a formal version of this model to enable them to
prove properties of a protection system mathematically.
The concept of a capability evolved from Iliffe's and Jodeit's codewords,
which were implemented in the Rice University computer (Iliffe and Jodeit
[1962]). The term capability was introduced by Dennis and Horn [1966].
The Hydra system was described by Wulf et al. [1981]. The CAP system
was described by Needham and Walker [1977]. Organick [1972] discussed the
MULTICS ring-protection system.
Revocation was discussed by Redell and Fabry [1974], Cohen and Jefferson
[1975], and Ekanadham and Bernstein [1979]. The principle of separation of
policy and mechanism was advocated by the designer of Hydra (Levin et al.
[1975]). The confinement problem was first discussed by Lampson [1973] and
was further examined by Lipner [1975].
The use of higher-level languages for specifying access control was
suggested first by Morris [1973], who proposed the use of the seal and unseal
operations discussed in Section 14.9. Kieburtz and Silberschatz [1978], Kieburtz
and Silberschatz [1983], and McGraw and Andrews [1979] proposed various
language constructs for dealing with general dynamic-resource-management
schemes. Jones and Liskov [1978] considered how a static access-control scheme
can be incorporated in a programming language that supports abstract data
types. The use of minimal operating-system support to enforce protection was
advocated by the Exokernel Project (Ganger et al. [2002], Kaashoek et al. [1997]).
15.1
Protection, as we discussed in Chapter 14, is strictly an internal problem: How
do we provide controlled access to programs and data stored in a computer
system  on the other hand, requires not only an adequate protection

system but also consideration of the external environment within which the
system operates. A protection system is ineffective if user authentication is
compromised or a program is run by an unauthorized user.
Computer resources must be guarded against unauthorized access, malicious
destruction or alteration, and accidental introduction of inconsistency.
These resources include information stored in the system (both data and code),
as well as the CPU, memory, disks, tapes, and networking that are the computer.
In this chapter, we start by examining ways in which resources may
be accidentally or purposely misused. We then explore a key security enabler
-cryptography. Finally, we look at mechanisms to guard against or detect
attacks.
To discuss security threats and attacks.
To explain the fundamentals of encryption, authentication, and hashing.
To examine the uses of cryptography in computing.
To describe various countermeasures to security attacks.
In many applications, ensuring the security of the computer system is worth
considerable effort. Large commercial systems containing payroll or other
financial data are inviting targets to thieves. Systems that contain data pertaining
to corporate operations may be of interest to unscrupulous competitors.
Furthermore, loss of such data, whether by accident or fraud, can seriously
impair the ability of the corporation to function.
In Chapter 14, we discussed mechanisms that the operating system can
provide (with appropriate aid from the hardware) that allow users to protect
621
622 Chapter 15
their resources, including programs and data. These mechanisms work well
only as long as the users conform to the intended use of and access to these
resources. We say that a system is if its resources are used and accessed
as intended under all circumstances. Unfortunately total security cannot be
achieved. Nonetheless, we must have mechanisms to make security breaches
a rare occurrence, rather than the norm.
Security violations (or misuse) of the system. can be categorized as intentional
(malicious) or accidental. It is easier to protect against accidental misuse
than against malicious misuse. For the most part protection mechanisms are
the core of protection from accidents. The following list includes several forms
of accidental and malicious security violations. We should note that in our
discussion of security, we use the terms intruder and cracker for those attempting
to breach security. In addition, a is the potential for a security violation,
such as the discovery of a vulnerability, whereas an is the attempt to
break security.
Breach of confidentiality. This type of violation involves 1mauthorized
reading of data (or theft of information). Typically, a breach of confidentiality
is the goal of an intruder. Capturing secret data from a system or
a data stream, such as credit-card information or identity information for
identity theft, can result directly in money for the intruder.
Breach of integrity. This violation involves unauthorized modification
of data. Such attacks can, for example, result in passing of liability to
an innocent party or modification of the source code of an important
commercial application.
Breach of availability. This violation involves unauthorized destruction of
data. Some crackers would rather wreak havoc and gain status or bragging
rights than gain financially. Web-site defacement is a common example of
this type of security breach.
Theft of service. This violation involves unauthorized use of resources.
For example, an intruder (or intrusion program) may install a daemon on
a system that acts as a file server.
Denial of service. This violation involves preventing legitimate use of the
system. or attacks are sometimes accidental. The
original Internet worm turned into a DOS attack when a bug failed to delay
its rapid spread. We discuss DOS attacks further in Section 15.3.3.
Attackers use several standard methods in their attempts to breach
security. The most common is in which one participant in
a communication pretends to be someone (another host or another
person). By masquerading, attackers breach the correctness
of identification; they can then gain access that they would not normally be
allowed or escalate their privileges-obtain privileges to which they would not
normally be entitled. Another common attack is to replay a captured exchange
of data. A consists of the malicious or fraudulent repeat of a
valid data transmission. Sometimes the replay comprises the entire attackfor
example, in a repeat of a to transfer money. But frequently it is
done along with again to escalate privileges. Consider
15.1 623
Normal
attacker
Masquerading
attacker
Man-in-the-middle
attacker
Figure 15.1 Standard security attacks.
the damage that could be done if a request for authentication had a legitimate
user's information with an unauthorized user's. Yet another kind of
attack is the in which an attacker sits in the data
flow of a communication, masquerading as the sender to the receiver, and
vice versa. In a network communication, a man-in-the-middle attack may be
preceded by a in which an active communication session is
intercepted. Several attack methods are depicted in Figure 15.1.
As we have already suggested, absolute protection of the system from
malicious abuse is not possible, but the cost to the perpetrator can be made
sufficiently high to deter most intruders. In some cases, such as a denial-ofservice
attack, it is preferable to prevent the attack but sufficient to detect the
attack so that cmmtermeasures can be taken.
To protect a system, we must take security measures at four levels:
Physical. The site or sites containing the computer systems must be
physically secured against armed or surreptitious entry by intruders.
Both the machine rooms and the terminals or workstations that have
access to the machines must be secured.
624 Chapter 15
must be done carefully to assure that only
access to the system. Even authorized users/
to let others use their access (in exchange
They may also be tricked into allowing
One type of social-engineering attack
Here, a legitimate-looking e-mail or Web page misleads
a user into entering confidential information. Another teclucique is
Human. Authorization
appropriate users have
however, may be
for a bribe, for
access via
is
a general term for attempting to gather information in
order to gain unauthorized access to the computer (by looking through
trash, finding phone books, or finding notes containing passwords, for
example). These security problems are management and personnel issues,
not problems pertaining to operating systems.
Operating system. The system must protect itself from accidental or
purposeful security breaches. A runaway process could constitute an
accidental denial-of-service attack. A query to a service could reveal passwords.
A stack overflow could the launching of an unauthorized
process. list of possible breaches is almost endless.
Network. Much computer data in modern systems travels over private
leased lines, shared lines like the Internet, wireless connections, or dial-up
lines. Intercepting these data could be just as harmful as breaking into a
computer; and interruption communications could constitute a remote
denial-of-service attack, diminishing users' use of and trust in the system.
at the first two levels must be Inaintained if operating-system
is to be ensured. A weakness at a high security (physical or
allows circumvention of strict low-level (operating-system) security
measures. the old adage that a chaL.'l is as weak as its weakest link is
true of system security. All these aspects must be addressed for
to be maintained.
to allow the
more
is As intruders
countermeasures are created and deployed.
This causes intruders to become more in their attacks. For
incidents include the use of to
Section
tools needed to block the
In the remainder of this
15
15.2 625
ways, ranging from passwords for authentication through guarding against
viruses to detecting intrusions. We start with an exploration of security threats.
Processes, along with the kernel, are the only means of accomplishing work
on a computer. Therefore, writing a program that creates a breach of security,
or causing a normal process to change its behavior and create a breach, is a
common goal of crackers. In fact even most nonprogram security events have
as their goal causing a program threat. For example, while it is useful to log in
to a without authorization, it is quite a lot more useful to leave behind
a daemon that provides information or allows easy access even if
the original exploit is blocked. In this section, we describe common methods
which programs cause security breaches. Note that there is considerable
variation in the naming conventions of security holes and that we use the most
common or descriptive terms.
15.2.1 Horse
systems have mechanisms for allowing programs written by users to
be executed by other users. If these programs are executed in a domain that
provides the access rights of the executing user, the other users may misuse
these rights. A text-editor program, for example, may include code to search
the file to be edited for certairl keywords. If any are found, the entire file
may be to a special area accessible to the creator of the text editor.
A code segment that misuses its environment is called a Long
search paths, as are common on UNIX systems, exacerbate the Trojanhorse
The search path lists the set of directories to search when an
program name is given. The is searched for a file of that
name, and the file is executed. All the directories in such a search path must
be secure, or a horse could be slipped into the user's and executed
consider the use of the  .   character in a search path. The  .  
to include the current directory in the search. if a user has
  .  in her search has set her current to a friend's directory, and
enters the name of a normal system commanct be executed
from the friend's instead. The program would run the user's
to do anything that the user is allowed to
the user's instance.
horse is a that emulates a
What
626 Chapter 15
such as the control-alt-delete conlbination used by all modern Windows
operating systems.
Another variation on the Trojan horse is Spyware sometimes
accompanies a program that the user has chosen to install. Most frequently, it
comes along with freeware or shareware programs, but sometimes it is included
with commercial software. The goal of spyware is to download ads to display
on the user's system, create when certain sites are
visited, or capture information from the user's system and return it to a central
site. This latter is an example of a general category of attacks known as
in which surreptitious communication occurs. For example,
the installation of an innocuous-seeming program on a Windows system could
result in the loading of a spyware daemon. The spyware could contact a central
site, be given a message and a list of recipient addresses, and deliver the spam
message to those users from the Windows machine. This process continues
until the user discovers the spyware. Frequently, the spyware is not discovered.
In 2004, it was estimated that 80 percent of spam was being delivered by this
method. This theft of service is not even considered a crime in most countries!
Spyware is a micro example of a macro problem: violation of the principle
of least privilege. Under most circumstances, a user of an operating system
does not need to install network daemons. Such daemons are installed via
two mistakes. First, a user may run with more privileges than necessary (for
example, as the administrator), allowing programs that she runs to have more
access to the system than is necessary. This is a case of human error-a common
security weakness. Second, an operating system may allow by default more
privileges than a normal user needs. This is a case of poor operating-system
design decisions. An operating system (and, indeed, software in general)
should allow fine-grained control of access and security, but it must also be easy
to manage and understand. Inconvenient or inadequate security measures are
bound to be circumvented, causing an overall weakening of the security they
were designed to implement.
15.2.2 Trap Door
The designer of a program or system might leave a hole in the software that only
he is capable of using. This type of security breach (or was shown in
the movie War Games. For instance, the code Inight check a specific user ID or
password, and it might circumvent normal security procedures. Programmers
have been arrested for embezzling from banks by including rounding errors
in their code and having the occasional half-cent credited to their accounts.
This account credititrg can add up to a large amount of money, considering the
number of transactions that a large bank executes.
A clever trap door could be included in a compiler. The compiler could
generate standard object code as well as a trap door, regardless of the source
code being compiled. This activity is particularly nefarious, since a search of
the source code of the program will not reveal any problems. Only the source
code of the compiler would contain the information.
Trap doors pose a difficult problem because, to detect them, we have to
analyze all the source code for all components of a system. Given that software
systems may consist of millions of lines of code, this analysis is not done
frequently, and frequently it is not done at all!
15.2 627
15.2.3 Logic Bomb
Consider a program that initiates a security incident only under certain
circLtmstances. It would be hard to detect because under normal operations,
there would be no security hole. However, when a predefined set of parameters
were met, the security hole would be created. This scenario is known as a
A programmer, for example, might write code to detect whether
was still employed; if that check failed, a daemon could be spawned to allow
remote access, or code could be launched to cause damage to the site.
15.2.4 Stack and Buffer Overflow
The stack- or buffer-overflow attack is the most common way for an attacker
outside the system, on a network or dial-up connection, to gain unauthorized
access to the target system. An authorized user of the system may also use this
exploit for privilege escalation.
Essentially, the attack exploits a bug in a program. The bug can be a simple
case of poor programming, in which the programmer neglected to code bounds
checking on an input field. In this case, the attacker sends more data than the
program was expecting. By using trial and error, or by examining the source
code of the attacked program if it is available, the attacker determines the
vulnerability and writes a program to do the following:
Overflow an input field, command-line argument, or input buffer-for
example, on a network daemon-wl.til it writes into the stack.
Overwrite the current return address on the stack with the address of the
exploit code loaded in step 3.
Write a simple set of code for the next space in the stack that includes
the commands that the attacker wishes to execute-for instance, spawn
a shell.
The result of this attack program's execution will be a root shell or other
privileged command execution.
For instance, if a Web-page form expects a user name to be entered into a
field, the attacker could send the user name, plus extra characters to overflow
the buffer and reach the stack, plus a new return address to load onto the stack,
plus the code the attacker wants to run. When the buffer-reading subroutine
returns from execution, the return address is the exploit code, and the code
is run.
Let's look at a buffer-overflow exploit in more detail. Consider the simple
C program in Fig1-1re 15.2. This program creates a character array
size BUFFER_SIZE and copies the contents of the parameter provided on the
command line-argv [1]. As long as the size of this parameter is less than
BUFFER_SIZE (we need one byte to store the null terminator), this program
works properly. But consider what happens if the parameter provided on the
command line is longer than BUFFER_SIZE. In this scenario, the strcpy ()
function will begin copying from argv [1] until it encounters a null terminator
(\0) or until the program crashes. Thus, this program suffers from a potential
problem in which copied data overflow the buffer array.
628 Chapter 15
#include   stdio.h  
#define BUFFER_SIZE 256
int main(int argc, char   argv[])
{
}
char buffer[BUFFER_SIZE];
if (argc    2)
return -1;
else {
strcpy(buffer,argv[1]);
return 0;
}
Figure   15.2 C program with buffer-overflow condition.
Note that a careful programmer could have performed bounds checking
on the size of argv [1] by using the strncpy () function rather than strcpy (),
replacing the line   strcpy(buffer, argv [1]);   with   strncpy(buffer,
argv[1], sizeof(buffer)-1) ;  .Unfortunately, good bounds checking is
the exception rather than the norm.
Furthermore, lack of bounds checking is not the only possible cause of the
behavior of the program in Figure 15.2. The program could instead have been
carefully designed to compromise the integrity of the system. We now consider
the possible security vulnerabilities of a buffer overflow.
When a function is invoked in a typical computer architecture, the variables
defined locally to the function (sometimes known as automatic variables), the
parameters passed to the function, and the address to which control returns
once the function exits are stored in a stack frame. The layout for a typical stack
frame is shown in Figure 15.3. Examining the stack frame from top to bottom,
we first see the parameters passed to the function, followed by any automatic
variables declared in the function. We next see the frame pointer, which is
the address of the beginning of the stack frame. Finally, we have the return
bottom ~ frame pointer
grows
top
Figure 15.3 The layout for a typical stack frame.
15.2 629
address, which specifies where to return control once the function exits. The
frame pointer must be saved on the stack, as the value of the stack pointer can
vary during the function call; the saved frame pointer allows relative access to
parameters and automatic variables.
Given this standard memory layout, a cracker could execute a bufferoverflow
attack. I-:Ter goal is to replace the return address in the stack frame so
that it now points to the code segment containing the attacking program.
The programmer first writes a short code segment such as the following:
#include   stdio.h  
int main(int argc, char   argv[])
{
execvp(' '\bin\sh'', ''\bin \sh'', NULL);
return 0;
Using the execvp () system call, this code segment creates a shell process.
If the program being attacked runs with system-wide permissions, this newly
created shell will gain complete access to the system. Of course, the code
segment could do anything allowed by the privileges of the attacked process.
This code segment is then compiled so that the assembly language instructions
can be modified. The primary modification is to remove unnecessary features
in the code, thereby reducing the code size so that it can fit into a stack frame.
This assembled code fragment is now a binary sequence that will be at the
heart of the attack.
Refer again to the program shown in Figure 15.2. Let's assume that when
the main () function is called in that program, the stack frame appears as
shown in Figure 15.4(a). Using a debugger, the programmer then finds the
copied
(a) (b)
Figure 15.4 Hypothetical stack frame for Figure 15.2, (a) before and (b) after.
630 Chapter 15
address of buffer [0] in the stack. That address is the location of the code the
attacker wants executed. The binary sequence is appended with the necessary
amount of NO-OP instructions (for NO-OPeration) to fill the stack frame up
to the location of the return address; and the location. of buffer [0], the new
return address, is added. The attack is complete when the attacker gives this
constructed binary sequence as input to the process. The process then copies
the binary sequence from argv [1] to position buffer [0] in the stack frame.
Now, when control returns from main (), instead of returning to the location
specified by the old value of the return address, we return to the modified shell
code, which runs with the access rights of the attacked process! Figure 15.4(b)
contains the modified shell code.
There are many ways to exploit potential buffer-overflow problems. In
this example, we considered the possibility that the program being attackedthe
code shown in Figure 15.2-ran with system-wide permissions. However,
the code segment that runs once the value of the return address has been
modified might perform any type of malicious act, such as deleting files,
opening network ports for further exploitation, and so on.
This example buffer-overflow attack reveals that considerable knowledge
and programming skill are needed to recognize exploitable code and then
to exploit it. Unfortunately, it does not take great programmers to launch
security attacks. Rather, one cracker can determine the bug and then write an
exploit. Anyone with rudimentary computer skills and access to the exploita
so-called then try to launch the attack at target systems.
The buffer-overflow attack is especially pernicious because it can be run
between systems and can travel over allowed communication channels. Such
attacks can occur within protocols that are expected to be used to communicate
with the target machine, and they can therefore be hard to detect and prevent.
They can even bypass the security added by firewalls (Section 15.7).
One solution to this problem is for the CPU to have a feature that disallows
execution of code in a stack section of memory. Recent versions of Sun's SPARC
chip include this setting, and recent versions of Solaris enable it. The return
address of the overflowed routine can still be modified; but when the return
address is within the stack and the code there attempts to execute, an exception
is generated, and the program is halted with an error.
Recent versions of AMD and Intel x86 chips include the NX feature to prevent
this type of attack The use of the feature is supported in several x86 operating
systems, including Linux and Windows XP SP2. The hardware implementation
involves the use of a new bit in the page tables of the CPUs. This bit marks
the associated page as nonexecutable, so that instructions cannot be read from
it and executed. As this feature becomes prevalent, buffer-overflow attacks
should greatly diminish.
15.2.5 Viruses
Another form of program threat is a A virus is a fragment of code embedded
in a legitimate program. Viruses are self-replicating and are designed to
  infect   other programs. They can wreak havoc in a system by modifying or
destroying files and causing system crashes and program malfunctions. As
with most penetration attacks, viruses are very specific to architectures, operating
systems, and applications. Viruses are a particular problem for users of
15.2 631
PCs. UNIX and other multiuser operating systems generally are not susceptible
to viruses because the executable programs are protected from writing by the
operating system. Even if a virus does infect such a progran:1, its powers usually
are limited because other aspects of the system are protected.
Viruses are usually borne via e-mail, with spam the most comrnon vector.
They can also spread when users download viral programs Internet
file-sharing services or exchange infected disks.
Another common form of virus transmission uses Microsoft Office files,
such as Microsoft Word documents. These documents can contain macros
Visual Basic programs) that programs in the Office suite PowerPoint,
and Excel) will execute automatically. Because these programs run under the
user's own account, the macros can run largely unconstrained (for example,
deleting user files at will). Commonly, the virus will also e-mail itself to others
in the user's contact list. Here is a code sample that shows the simplicity of
writing a Visual Basic macro that a virus could use to format the hard drive of
a Windows computer as soon as the file containing the macro was opened:
Sub AutoOpen ()
Dim oFS
Set oFS = CreateObject(' 'Scripting.FileSystemObject'')
vs =Shell(' 'c: command.com /k format c:' ',vbHide)
End Sub
How do viruses work  Once a virus reaches a target machine, a program
known as a inserts the virus into the system. The virus dropper
is usually a Trojan horse, executed for other reasons but installing the virus
as its core activity. Once installed, the virus may do any one of a number of
things. There are literally thousands of viruses, but they fall into several main
categories. Note that many viruses belong to more than one category.
File. A standard file virus infects a system by appending itself to a file.
It changes the start of the program so that execution jumps to its code.
After it executes, it returns control to the program so that its execution is
not noticed. File viruses are sometimes known as parasitic viruses, as they
leave no full files behind and leave the host program still functional.
Boot. A boot virus infects the boot sector of the system, executing every
time the system is booted and before the operating system is loaded. It
watches for other boatable media (that is, floppy disks) and infects them.
These viruses are also known as memory viruses, because they do not
appear in the file system. Figure 15.5 shows how a boot virus works.
Macro. Most viruses are written in a low-levellanguage, such as assembly
or C. Macro viruses are written in a high-level language, such as Visual
Basic. These viruses are triggered when a program capable of executing
the macro is run. For example, a macro virus could be contained in a
spreadsheet file.
Source code. A source code virus looks for source code and modifies it to
include the virus and to help spread the virus.
632 Chapter 15
Figure i 5.5 A boot-sector computer virus.
Polymorphic. A polymorphic virus changes each time it is installed to
avoid detection by antivirus software. The changes do not affect the virus's
functionality but rather change the virus's signature. A
a pattern that Cili'i be used to identify a virus, typically a series
make up the virus code.
Encrypted. An encrypted virus includes decryption code along with the
encrypted virus, again to avoid detection. The virus first decrypts and then
executes.
Stealth. This tricky virus attempts to avoid detection by modifying parts
of the system that could be used to detect it. For example, it could modify
the read system call so that if the file it has modified is read, the original
form of the code is returned rather than the infected code.
Tunneling. This virus attempts to bypass detection by an anti virus scanner
by installing itself in the interrupt-handler chain. Similar viruses install
themselves in device drivers.
15.3
15.3 633
Multipartite. A virus of this type is able to infect nmltiple parts of a system,
including boot sectors, memory, and files. This makes it difficult to detect
and contain.
Armored. An armored virus is coded to ncake it hard for antivirus
researchers to unravel and understand. It can also be compressed to avoid
detection and disinfection. In addition, virus droppers and other full files
that are part of a virus infestation are frequently hidden via file attributes
or unviewable file names.
This vast variety of viruses is likely to continue to grow. In fact, in 2004
a new and widespread virus was detected. It exploited three separate bugs
for its operation. This virus started by infecting hundreds of Windows servers
(including many trusted sites) running Microsoft Internet Information Server
(IIS). Any vulnerable Microsoft Explorer Web browser visiting those sites
received a browser virus with any download. The browser virus installed
several back-door programs, including a which records
all things entered on the keyboard (including and credit-card
numbers). It also installed a daemon to allow unlimited remote access by
an intruder and another that allowed an intruder to route spam through the
infected desktop computer.
Generally, viruses are the most disruptive security attacks; and because
they are effective, they will continue to be written and to spread. the
active debates within the computing community is whether a JtH'-YHcn;_
in which many systems run the same hardware, operating system, and/ or
application software, is increasing the threat of and damage caused by security
intrusions. This monoculture supposedly consists of Microsoft products, and
part of the debate concerns whether such a monoculture even exists today.
Program threats typically use a breakdown in the protection mechanisms of a
system to attack programs. In contrast, system and network threats involve the
abuse of services and network comcections. System and network threats create
a situation in which operating-system resources and user files are Inisused.
Sometimes a system and network attack is used to launch a program attack,
and vice versa.
The more an operating system is-the more services it has enabled
and the more functions it allows-the more likely it is that a is available
to exploit. Increasingly, operating systems strive to be
For example, Solaris 10 moved from a model in which many services (FTP,
telnet, and others) were enabled by default when the system was installed
to a model in which almost all services are disabled at installation time and
must specifically be enabled system administrators. Such changes reduce
the system's set of ways in which an attacker can to
break into the system.
In the remainder of this section, we discuss some examples of system
and network threats, including worms, port scamcing, and denial-of-service
attacks. It is important to note that masquerading and replay attacks are also
634 Chapter 15
commonly launched over netvvorks between systems. In fact, these attacks
are more effective and harder to counter when multiple systems are involved.
For example, within a computer, the operating system usually can determine
the sender and receiver of a message. Even if the sender changes to the ID of
someone else, there may be a record of that ID change. When multiple systems
are involved, especially systems controlled by attackers, then such tracing is
much more difficult.
In general, we can say that sharing secrets (to prove identity and as keys to
encryption) is required for authentication and encryption, and sharing secrets
is easier in environments (such as a single operating system) in which secure
sharing methods exist. These methods include shared memory and interprocess
comnmnications. Creating secure communication and authentication is
discussed in Sections 15.4 and 15.5.
15.3.1 Worms
A is a process that uses the mechanism to ravage system
performance. The worm spawns copies of itself, using up system resources
and perhaps locking out all other processes. On computer networks, worms
are particularly potent, since they may reproduce themselves among systems
and thus shut down an entire network. Such an event occurred in 1988 to UNIX
systems on the Internet, causing the loss of system and system-administrator
time worth millions of dollars.
At the close of the workday on November 2, 1988, Robert Tappan Morris,
Jr., a first-year Cornell graduate student, unleashed a worm program on one
or more hosts corm.ected to the Internet. Targeting Sun Microsystems' Sun 3
workstations and VAX computers running variants of Version 4 BSD UNIX, the
worm quickly spread over great distances; within a few hours of its release,
it had consumed system resources to the point of bringing down the infected
machines.
Although Robert Morris designed the self-replicating program for rapid
reproduction and distribution, some of the features of the UNIX networking
environment provided the means to propagate the worm throughout the system.
It is likely that Morris chose for in.itial infection an Internet host left open
for and accessible to outside users. From there, the worm program exploited
flaws in the UNIX operating system's security routines and took advantage
of UNIX utilities that simplify resource sharing in local-area networks to gain
unauthorized access to thousands of other connected sites. Morris's methods
of attack are outlined next.
The worm was made up of two programs, a (also called a
or program and the main program. ll.c, the grappling
hook consisted of 99 lines of C code compiled and run on each machine it
accessed. Once established on the computer system under attack, the grappling
hook connected to the machine where it originated and uploaded a copy of the
main worm onto the hooked system (Figure 15.6). The main program proceeded
to search for other machines to which the newly infected system could connect
easily. In these actions, Morris exploited the UNIX networking utility rsh for
easy remote task execution. By setting up special files that list host-login
name pairs, users can omit entering a password each time they access a remote
account on the paired list. The worm searched these special files for site names
15.3 635
rsh attack
finger attack
sendmail attack
worm sent
target system infected system
Figure 15.6 The Morris Internet worm.
that would allow remote execution without a password. Where remote shells
were established, the worm program was uploaded and began executing anew.
The attack via remote access was one of three infection methods built into
the worm. The other two methods involved operating-system bugs in the UNIX
finger and sendmail programs.
The finger utility functions as an electronic telephone directory; the
command
finger user-name hostname
returns a person's real and login names along with other information that
the user may have provided, such as office and home address and telephone
number, research plan, or clever quotation. Finger runs as a background
process (or daemon) at each BSD site and responds to queries throughout the
Internet. The worm executed a buffer-overflow attack on finger. The program
queried finger with a 536-byte string crafted to exceed the buffer allocated
for input and to overwrite the stack frame. Instead of returning to the main
routine where it resided before Morris's calt the finger daemon was routed
to a procedure within the invading 536-byte string now residing on the stack
The new procedure executed /bin/ sh, which, if successful, gave the worm a
remote shell on the machine under attack.
The bug exploited in sendmail also involved using a daemon process
for malicious entry. sendmail sends, receives, and routes electronic mail.
Debugging code in the utility permits testers to verify and display the state of
the ncail system. The debugging option was useful to system administrators
and was often left on. Morris included in his attack arsenal a call to debug that
-instead of specifying a user address, as would be normal in testing-issued
a set of cornmands that mailed and executed a copy of the grappling-hook
program.
Once in place, the main worm systematically attempted to discover user
passwords. It began by trying simple cases of no password or passwords
constructed of account-user-name combinations, then used comparisons with
an internal dictionary of 432 favorite password choices, and then went to the
636 Chapter 15
final stage of trying each word in the standard UNIX on-line dictionary as a
possible password. This elaborate and efficient three-stage password-cracking
algorithm enabled the worm to gain access to other user accounts on the
infected system. The wonTt then searched for rsh data files in these newly
broken accounts and used them as described previously to gain access to user
accounts on remote systems.
With each new access, the worm program searched for already active
copies of itself. If it found one, the new copy exited, except in every seventh
instance. Had the worm exited on all duplicate sightings, it might have
remained undetected. Allowing every seventh duplicate to proceed (possibly
to confound efforts to stop its spread baiting with fake worms) created a
wholesale infestation of Sun and VAX systems on the Internet.
The very features of the UNIX network environment that assisted il  l the
worm's propagation also helped to stop its advance. Ease of electronic communication,
mechanisms to copy source and binary files to remote machines, and
access to both source code and human expertise allowed cooperative efforts to
develop solutions quickly. By the evening of the next day, November 3, methods
of halting the invading program were circulated to system administrators via
the Internet. Within days, specific software patches for the exploited security
flaws were available.
Why did Morris unleash the worm  The action has been characterized
as both a harmless prank gone awry and a serious criminal offense. Based
on the complexity of the attack, it is unlikely that the worm's release or the
scope of its spread was unintentional. The worm program took elaborate steps
to cover its tracks and to repel efforts to stop its spread. Yet the program
contained no code aimed at damaging or destroying the systems on which it
ran. The author clearly had the expertise to include such commands; in fact,
data structures were present in the bootstrap code that could have been used to
transfer Trojan-horse or virus programs. The behavior of the program may lead
to interesting observations, but it does not provide a sound basis for inferring
motive. What is not open to speculation, however, is the legal outcome: A
federal court convicted Morris and handed down a sentence of three years'
probation, 400 hours of community service; and a $10,000 fine. Morris's legal
costs probably exceeded $100,000.
Security experts continue to evaluate methods to decrease or eliminate
worms. A more recent event; though, shows that worms are still a fact of
life on the Internet. It also shows that as the Internet grows, the damage
that even   harmless   worms can do also grows and can be significant. This
example occurred during August 2003. The fifth version of the   Sobig   worm,
more properly known as   W32.Sobig.F@mm,   was released by persons at this
time unknown. It was the fastest-spreading worm released to date, at its peak
mfecting hundreds of thousands of computers and one in seventeen e-mail
messages on the Internet. It clogged e-mail inboxes, slowed networks, and
took a huge number of hours to clean up.
Sobig.F was launched by being uploaded to a pornography newsgroup via
an account created with a stolen credit card. It was disguised as a photo. The
virus targeted Microsoft Windows systems and used its own SMTP engine to
e-mail itself to all the addresses found on an infected system. It used a variety
of subject lines to help avoid detection, including   Thank You!     Your details,''
and   Re: Approved.   It also used a random address on the host as the   From:  
15.3 637
address, making it difficult to determine from the message which machine was
the infected source. Sobig.F included an attachment for the target e-mail reader
to click on, again with a variety of names. If this payload was executed, it stored
a program called WINPPR32.EXE in the default Windows directory, along with
a text file. It also modified the Windows registry.
The code included in the attachment was also programmed to periodically
attempt to connect to one of twenty servers and download and execute a
program from them. Fortunately, the servers were disabled before the code
could be downloaded. The content of the program from these servers has not
yet been determined. If the code was malevolent, untold damage to a vast
number of machines could have resulted.
15.3.2 Port Scanning
Port scanning is not an attack but rather a means for a cracker to detect
a system's vulnerabilities to attack. Port scanning typically is automated,
involving a tool that attempts to create a TCP liP connection to a specific port
or a range of ports. For example, suppose there is a known vulnerability (or
bug) in sendmail. A cracker could launch a port scanner to try to connect, say,
to port 25 of a particular system or to a range of systems. If the connection
was successful, the cracker (or tool) could attempt to communicate with the
answering service to determine if the service was indeed sendmail and, if so,
if it was the version with the bug.
Now imagine a tool in which each bug of every service of every operath  g
system was encoded. The tool could attempt to connect to every port of one
or nwre systems. For every service that answered, it could try to use each
known bug. Frequently, the bugs are buffer overflows, allowing the creation of
a privileged command shell on the system. From there, of course, the cracker
could install Trojan horses, back-door programs, and so on.
There is no such tool, but there are tools that perform subsets of that
functionality. For example, nmap (from http:/ /www.insecure.org/mrtap/) is
a very versatile open-source utility for network exploration and security
auditing. When pointed at a target, it will determine what services are n.1n..Tling,
including application names and versions. It can identify the host operating
system. It can also provide information about defenses, such as what firewalls
are defending the target. It does not exploit any known bugs.
Nessus (from http:/ /www.nessus.org/) performs a similar function, but
it has a database of bugs and their exploits. It can scan a range of systems,
determine the services running on those systems, and attempt to attack all
appropriate bugs. It generates reports about the results. It does not perform
the final step of exploiting the found bugs, but a knowledgeable cracker or a
script kiddie could.
Because port scans are detectable (Section 15.6.3), they frequently are
launched from Such systems are previously compromised,
independent systems that are serving their owners while being used for nefarious
purposes, including denial-of-service attacks and spam relay. Zombies
make crackers particularly difficult to prosecute because determining the
source of the attack and the person that launched it is challenging. This is
one of many reasons for securing   inconsequential   systems, not just systems
containing   valuable   information or services.
638 Chapter 15
15.3.3 Denial of Service
As mentioned earlier, denial-of-service attacks are aimed not at gaming
information or stealing resources but rather at disrupting legitimate use of
a system or facility. Most such attacks involve systems that the attacker has
not penetrated. Indeed, launching an attack that prevents legitimate use is
frequently easier than breaking into a machine or facility.
Denial-of-service attacks are generally network based. They fall into two
categories. Attacks in the first category use so many facility resources that,
in essence, no useful work can be done. For example, a Web-site click could
download a Java applet that proceeds to use all available CPU time or to pop up
windows infinitely. The second category involves disrupting the network of
the facility. There have been several successful denial-of-service attacks of this
kind against major Web sites. These attacks result from abuse of some of the
fundamental functionality of TCP liP. For instance, if the attacker sends the part
of the protocol that says   I want to start a TCP connection,   but never follows
with the standard   The connection is now complete,   the result can be partially
started TCP sessions. If enough of these sessions are launched, they can eat up
all the network resources of the system, disabling any further legitimate TCP
connections. Such attacks, which can last hours or days, have caused partial or
full failure of attempts to use the target facility. The attacks are usually stopped
at the network level until the operating systems can be updated to reduce their
vulnerability.
Generally, it is impossible to prevent denial-of-service attacks. The attacks
use the same mechanisms as normal Even more difficult to prevent
and resolve are These attacks
are launched from multiple sites at once, toward a common target, typically
by zombies. DDOS attacks have become more comncon and are sometimes
associated with blackmail attempts. A site comes under attack, and the
attackers offer to halt the attack in exchange for money.
Sometimes a site does not even know it is under attack. It can be difficult
to determine whether a system slowdown is an attack or just a surge in system
use. Consider that a successful advertising campaign that greatly increases
traffic to a site could be considered a DDOS.
There are other interesting aspects of DOS attacks. For example, if an
authentication algorithm locks an account for a period of time after several
incorrect attempts to access the account, then an attacker could cause all
authentication to be blocked by purposely making incorrect attempts to access
all accounts. Similarly, a firewall that automatically blocks certain kinds of
traffic could be induced to block that traffic when it should not. These examples
suggest that programmers and systems managers need to fully understand the
algorithms and technologies they are deploying. Finally, computer science
classes are notorious sources of accidental system DOS attacks. Consider the
first programming exercises in which students learn to create subprocesses
or threads. A common bug involves spawning subprocesses infinitely. The
system's free memory and CPU resources don't stand a chance.
There are many defenses against computer attacks, running the gamut from
methodology to technology. The broadest tool available to system designers
15.4 639
and users is cryptography. In this section, we discuss the details of cryptography
and its use in computer security.
In an isolated computer, the operating system can reliably determine the
sender and recipient of ali interprocess communication, since it controls all
communication channels in the computer. In a network of computers, the
situation is quite different. A networked computer receives bits from the
wire with no immediate and reliable way of determining what machine or
application sent those bits. Similarly, the computer sends bits onto the network
with no of knowing who might eventually receive them.
Commonly, network addresses are used to infer the potential senders
and receivers of network messages. Network packets arrive with a source
address, such as an IP address. And when a computer sends a message, it
names the intended receiver by specifying a destination address. However, for
applications where security matters, we are asking for trouble if we assume
that the source or destination address of a packet reliably determines who sent
or received that packet. A rogue computer can send a message with a falsified
source address, and numerous computers other than the .. one specified by the
destination address can (and typically do) receive a packet. For example, all of
the routers on the way to the destination will receive the packet, too. How, then,
is an operating system to decide whether to grant a request when it cannot trust
the named source of the request  And how is it supposed to provide protection
for a request or data when it cannot determine who will receive the response
or message contents it sends over the network 
It is generally considered infeasible to build a network of any scale in
which the source and destination addresses of packets can be trusted in this
sense. Therefore, the only alternative is somehow to eliminate the need to
trust the network. This is the job of cryptography. Abstractly, ~  ''  ~.,-ro.rnron~
used to constrain the potential senders and/ or receivers of a message.
cryptography is based on secrets called that are selectively distributed to
computers in a network and used to process messages. Cryptography enables a
recipient of a message to verify that the message was created by some computer
possessing a certain key-the key is the source of the message. Similarly: a
sender can encode its message so that only a computer with a certain key can
decode the message, so that the key becomes the destination. Unlike network
addresses, however, keys are designed so that it is not computationally feasible
to derive them from the messages they were used to generate or from any
other public information. Thus, they provide a much more trustworthy means
of constraining senders and receivers of messages. Note that cryptography is
a field of study unto itself, with large and small complexities and subtleties.
Here, we explore the most important aspects of the parts of cryptography that
pertain to operating systems.
15.4.1 Encryption
Because it solves a wide variety of communication security problems,
is used frequently in many aspects of modern computing. Encryption
a means for constraining the possible receivers of a message. An encryption
algorithm enables the sender of a message to ensure that only a computer
possessing a certain key can read the message. Encryption of messages is an
ancient practice, of course, and there have been many encryption algorithms,
640 Chapter 15
dating back to ancient times. In this section, we describe important modern
encryption principles and algorithms.
Figure 15.7 shows an example of two users communicating securely over
an insecure channel. We refer to this figure throughout the section. Note that the
key exchange can take place directly between the two parties or via a trusted
third party (that is, a certificate authority), as discussed in Section 15.4.1.4.
An encryption algorithm consists of the following components:
A set K of keys.
A set M of messages.
A set C of ciphertexts.
A function E : K --+ ( M --+ C). Thatis, for each k E K, E (k) is a function for
generating ciphertexts from messages. Both E and E (lc) for any k should
be efficiently computable functions.
A function D: I   --+ (C --+ M). Thatis, for eachlc E I  , D(k) is a function for
generating messages from ciphertexts. Both D and D(lc) for any k should
be efficiently computable functions.
An encryption algorithm must provide this essential property: given a ciphertext
c E C a computer can compute m such that E (lc)(m) = c only if it possesses
write ----1 rnessage rn 1
I
II 12.
0 ~-
z~ -x
~-
read 1 message ml
Figure i5.7 A secure communication over an insecure medium.
15.4 641
D(lc). Thus, a computer holding D(lc) can decrypt ciphertexts to the plaintexts
used to produce them, but a computer not holding D(lc) cannot decrypt ciphertexts.
Since ciphertexts are generally exposed (for example, sent on a network),
it is important that it be infeasible to derive D(lc) from the ciphertexts.
There are two main types of encryption algorithms: symmetric and
asymmetric. We discuss both types in the following sections.
15.4.1.1 Symmetric Encryption
In a the same key is used to encrypt and to
decrypt. That is, E can be from D(lc), and vice versa. Therefore, the
secrecy of E (lc) must be protected to the same extent as that of D(lc).
For the past several decades, the most commonly used symmetric encryption
algorithm in the United States for civilian applications has been the
adopted by the National Institute of StanTechxwlogy
(NIST). DES works by taking a 64-bit value and a 56-bit
key and performing a series of transformations. These transformations are
based on substitution and permutation operations, as is generally the case
for symmetric encryption transformations. Some of the transformations are
in that their algorithms are hidden. In fact, these
so-called   S-boxes   are classified by the United States government. Messages
longer than 64 bits are broken into 64-bit chunks. Because DES works on a
chunk of bits at a time, is known as a cipher. If the same key is used
for encrypting an extended anwunt of data, it becomes vulnerable to attack.
Consider, for example, that the same source block would result in the same
ciphertext if the same key and encryption algorithm were used. Therefore,
the chunks are not just encrypted but also exclusive-or'ed (XORed) with the
ciphertext block before encryption. This is known as
DES is now considered insecure for many applications because its keys can
be exhaustively searched with moderate computing resources. Rather than
giving up on DES, though, NIST created a modification called in
which the DES algorithm is repeated three times (two encryptions and one
decryption) on the same plaintext usli'lg two or three keys-for example,
c = E (k3 )(D(lc2)(E (K1)(m))). When three keys are used, the effective key length
is 168 bits. Triple DES is in widespread use today.
In 2001, NIST a new encryption algorithm, called the
to replace DES. AES is another symmetric block
cipher. It can use key lengths of 128, 192, and 256 bits and works on 128-bit
blocks. It works by performing 10 to 14 rounds of transformations on a matrix
formed from a block Generally, the algorithm is compact and efficient.
Several other symmetric block encryption algorithms in use today bear
mentioning. The algorithm is fast compact, and easy to implement. It
can use a variable key length of up to 256 bits and works on 128-bit blocks.
can vary in key length, number of transformations, and block size. Because it
uses only basic computational operations, it can run on a wide variety of crus.
is perhaps the most common stream cipher. A is
designed to encrypt and decrypt a stream of bytes or bits rather than a block.
This is useful when the length of a communication would make a block cipher
too slow. The key is input into a pseudo-random-bit generator, which is an
642 Chapter 15
algorithm that attempts to produce random bits. The output of the generator
when fed a key is a keystream. A is an infinite set of keys that can be
used for the input plaintext stream. RC4 is used in encrypting steams of data,
such as in WEP, the wireless LAN protocol. lt is also used in communications
between Web browsers and Web servers, as we discuss below. Unfortunately,
RC4 as used in WEP (IEEE standard 802.11) has been found to be breakable in a
reasonable amount of con1.puter time. In fact RC4 itself has vulnerabilities.
15.4.1.2 Asymmetric Encryption
In an there are different encryption and
decryption keys. Here, we one such algorithm, known as RSA after
the names of its inventors (Rivest, Shamir, and Adleman). The RSA cipher is a
block-cipher public-key algorithm and is the most widely used asymmetrical
algorithm. Asymmetrical algorithms based on elliptical curves are gaining
ground, however, because the key length of such an algorithm can be shorter
for the same amount of cryptographic strength.
It is computationally infeasible to derive D(kd, N) from E (lee, N), and so
E (ke, N) need not be kept secret and can be widely disseminated; thus, E (lee, N)
(or just Ice) is the and D(kd, N) (or just led) is the N is the
write 'I messlge 69/
    0
isl l
m
~
encryption_...,. 695 mod 91
key k5.91
~-----+
' Gl-
~ Gl
:J c
uc
m m
(f).r:::
-~ 0
' 1------)  .
(])
N
read~
Figure 15.8 Encryption and decryption using RSA asymmetric cryptography.
15.4 643
product of two large, randomly chosen prime numbers p and q (for example, p
andq are512bitseach). Theencryptionalgorithmis E(kc, N)(rn) = mk, mod N,
where Icc satisfies leekd mod (p -1)(q -1) = 1. The decryption algorithm is then
D(kd, N)(c) = ckd mod N
An example using small values is shown in Figure 15.8. In this example, we
make p = 7 and q = 13. We then calculate N = 7  13 = 91 and (p-1)(q -1) = 72.
We next select kc relatively prime to 72 and    72, yielding 5. Finally, we calculate
kd such that kekrt mod 72 = 1, yielding 29. We now have our keys: the public
key, lee, N = 5, 91, and the private key, led, N = 29, 91. Encrypting the message
69 with the public key results in the message 62, which is then decoded by the
receiver via the private key.
The use of asymmetric encryption begins with the publication of the public
key of the destination. For bidirectional communication, the source also must
publish its public key.   Publication   can be as simple as handing over an
electronic copy of the key, or it can be more complex. The private key (or   secret
key  ) must be jealously guarded, as anyone holding that key can decrypt any
message created by the matching public key.
We should note that the seemingly small difference in key use between
asymmetric and symmetric cryptography is quite large in practice. Asymmetric
cryptography is based on mathematical functions rather than transformations,
Inaking it much more computationally expensive to execute. It is much
faster for a computer to encode and decode ciphertext by using the usual
symmetric algorithms than by using asymmetric algorithms. Why, then, use
an asymmetric algorithm  In truth, these algorithms are not used for generalpurpose
encryption of large amounts of data. However, they are used not
only for encryption of small amounts of data but also for authentication,
confidentiality, and key distribution, as we show in the following sections.
15.4.1.3 Authentication
We have seen that encryption offers a way of constraining the set of possible
receivers of a message. Constraining the set of potential senders of a message is
called Authentication is thus complementary to encryption. In
fact, sometimes their functions overlap. Consider that an encrypted message
can also prove the identity of the sender. For example, if D(kd, N)(E (ke. N)(m))
produces a valid message, then we know that the creator of the message must
hold ke. Authentication is also useful for proving that a message has not been
modified. In this section, we discuss authentication as a constraint on possible
receivers of a message. Note that this sort of authentication is similar to but
distinct from user authentication, which we discuss in Section 15.5.
An authentication algorithm consists of the following components:
A set K of keys.
A set M of messages.
A set A of authenticators.
A functionS: K -  - (M-+ A). That is, for each k E K, S(k) is a function for
generating authenticators from messages. Both Sand S(k) for any k should
be efficiently computable functions.
644 Chapter 15
A function V : [( -+ (M x A-+ {true, false}). That is, for each lc E K,
V(lc) is a function for verifying authenticators on messages. Both V and
V(lc) for any lc should be efficiently computable functions.
The critical property that an authentication algorithm must possess is this: for a
message m, a computer can generate an authenticator a E A such that V (lc )(m, a)
= true only if it possesses S(lc). Thus, a computer holding S(lc) can generate
authenticators on messages so that any computer possessing V(lc) can verify
them. However, a computer not holding S(lc) cannot generate authenticators
on messages that can be verified using V(lc). Since authenticators are generally
exposed (for example, sent on a network with the messages themselves), it
must not be feasible to derive S(lc) from the authenticators.
Just as there are two types of encryption algorithms, there are two main
varieties of authentication algorithms. The first in understanding these
algorithms is to explore hash functions. A H(m) creates a small,
fixed-sized block of data, known as a or from
a message m. Hash functions work by taking a message in n-bit blocks and
processing the blocks to produce an n-bit hash. H must be collision resistant
on m-that is, it must be infeasible to find an 1111 # m such that H(m) = H(n/).
Now, if H(m) = H(m1), we know that m m1 -that is, we know that the
message has not been modified. Common message-digest functions include
which produces a 128-bit hash, and which outputs a 160-bit hash.
Message digests are useful for detecting changed messages but are not useful as
authenticators. For example, H(m) can be sent along with a message; but if His
known, then someone could modify m and recompute H(m), and the message
modification would not be detected. Therefore, an authentication algorithm
takes the message digest and encrypts it.
The first main type of authentication algorithm uses symmetric encryption.
In a a cryptographic checksum is generated
from message using a secret key. Knowledge of V(lc) and knowledge
of S(lc) are equivalent: one can be derived from the other, so lc must be kept
secret. A simple example of a MAC defines S(lc)(m) = f(k, H(m)), where f is
a function that is one-way on its first argument (that is, k cam10t be derived
from f(k, H(m))). Because of the collision resistance in the hash function, we
are reasonably assured that no other message could create the same MAC. A
suitable verification algorithm. is then V(lc)(m, a) = (j(lc, m) =a). Note that k
is needed to compute both S(lc) and V(lc), so anyone able to compute one can
compute the other.
The second main type of authentication algorithm is a
and the authenticators thus produced are called
In a digital-signature algorithm, it is computationally to derive S(ks)
from V(lcv); in particular, Vis a one-way function. Thus, kv is the public key
and lc5 is the private key.
Consider as an example the RSA digital-signature algorithm. It is similar
to the RSA encryption algorithm, but the key use is reversed. The digital
signature of a message is derived by computing S(lcs)(m) = H(m)  s mod N.
The key /c5 again is a pair (d, N), where N is the product of two large, randomly
chosen prime numbers p and q. The verification algorithm is then V(kv)(m, a) =
(ak   mod N = H(m)), where kv satisfies lc,)c5 mod (p - 1)(q - 1) = 1.
15.4 645
lf encryption can prove the identity of the sender of a m~essage, then why do
we need separate authentication algorithms  There are three primary reasons.
Authentication algorithms generally require fewer computations (with
the notable exception of H.SA digital signatures). Over large amounts of
plaintext, this efficiency can make a huge difference in resource use and
the time needed to authenticate a message.
The authenticator of a message is almost always shorter than the message
and its ciphertext. This improves space use and transmission time
efficiency.
Sometimes, we want authentication but not confidentiality. For example,
a company could provide a software patch and could   sign   that patch to
prove that it came from the company and that it hasn't been modified.
Authentication is a component of many aspects of security. For example, it
is the core of which supplies proof that an entity performed an
action. A typical example of nonrepudiation involves the filling out of electronic
forms as an alternative to the signing of paper contracts. Nonrepudiation
assures that a person filling out an electronic form cannot deny that he did
so.
15.4.1.4 Key Distribution
Certainly, a good part of the battle between cryptographers (those inventing
ciphers) and cryptanalysts (those trying to break them) involves keys. With
symmetric algorithms, both parties need the key, and no one else should
have it. The delivery of the symmetric key is a huge challenge. Sometimes
it is performed cut-or-band -say, via a paper document or a conversation.
These methods do not scale well, however. Also consider the key-management
challenge. Suppose a user wanted to communicate with N other users privately.
That user would need N keys and, for more security, would need to change
those keys frequently.
These are the very reasons for efforts to create asymmetric key algorithms.
Not only can the keys be exchanged in public, but a given user needs only
one private key, no matter how many other people she wants to communicate
with. There is still the matter of managing a public key for each party to be
communicated with, but since public keys need not be secured, simple storage
can be used for that
Unfortunately, even the distribution of public keys requires some care.
Consider the man-in-the-middle attack shown in Figure 15.9. Here, the person
who wants to receive an encrypted message sends out his public key, but an
attacker also sends her   bad   public key (which matches her private key). The
person who wants to send the encrypted message knows no better and so uses
the bad key to encrypt the message. The attacker then happily decrypts it.
The problem is one of authentication-what we need is proof of who (or
what) owns a public One to solve that problem involves the use
of digital certificates. A is a public key digitally signed by a
trusted party. The trusted party receives proof of identification from some entity
646 Chapter 15
encryption __,.
key kbad
attacker
decryption
key kd __,.
  co' -.u
'  c
;AQ    
CD 0
_ decryption ...,.
key kbad
.,......_read -+message m I
Figure 15.9 A man-in-the-middle attack on asymmetric cryptography.
and certifies that the public key
we can trust the certifier  These have their public keys
i.J.l.cluded within Web browsers (and other consumers of certificates) before they
are distributed. The certificate authorities can then vouch for other authorities
(digitally signing the public keys of these other authorities), and so on, creating
a web of trust. The certificates can be distributed in a standard X.509 digital
certificate format that can be parsed by computer. This scheme is used for
secure Web communication, as we discuss in Section 15.4.3.
15.4.2 Implementation of Cryptography
Network protocols are typically organized in each layer acting as a client
of the one below it. That is, when one protocol generates a message to send
to its protocol peer on another machine, it hands its message to the protocol
below it in the network-protocol stack for delivery to its peer on that machine.
For example, in an IP network, TCP (a transport-layer protocol) acts as a client
of IP (a network-layer protocol): TCP packets are passed down to IP for delivery
to the TCP peer at the other end of the TCP connection. IP encapsulates the TCP
15.4 647
packet in an IP packet, which it similarly passes down to the data-link layer to be
transmitted across the network to its IP peer on the destination computer. This
IP peer then delivers the TCP up to the TCP peer on that machine. All in
all, the which has been almost universally adopted as a
model for data networking, defines seven such protocol layers. (You will read
more about the ISO model of networking in Chapter 16; Figure 16.6 shows a
diagram of the model.)
Cryptography can be inserted at almost any layer in the ISO model. SSL
(Section 15.4.3), for example, provides security at the transport layer. Networklayer
security generally has been standardized on which defines IP
packet formats that allow the insertion of authenticators and the encryption
of packet contents. It uses symmetric encryption and uses the protocol
for key IPSec is becoming widely used as the basis for
in which all traffic between two IPSec endpoints
is encrypted to make a private network out of one that may otherwise be
public. Numerous protocols also have been developed for use by applications,
but then the applications themselves must be coded to implement security.
Where is cryptographic protection best placed in a protocol stack  In
general, there is no definitive answer. On the one hand, more protocols benefit
from protections placed lower in the stack. For example, since IP packets
encapsulate TCP packets, encryption of IP packets (using IPSec, for example) also
hides the contents of the encapsulated TCP packets. Similarly, authenticators
on IP packets detect the modification of contaii1.ed TCP header information.
On the other hand, protection at lower layers in the protocol stack may give
insufficient protection to higher-layer protocols. For example, an application
server that runs over IPSec might be able to authenticate the client computers
from which requests are received. However, to authenticate a user at a client
computer, the server may need to use an application-level protocol-for
example, the user may be required to type a password. Also consider the
problem of e-mail. E-mail delivered via the industry standard SMTP protocol is
stored and forwarded, frequently multiple times, before it is delivered. Each of
these transmissions could go over a secure or an insecure network. For e-mail
to be secure, the e-mail message needs to be encrypted so that its security is
independent of the transports that carry it.
15.4.3 An Example: SSL
SSL 3.0 is a cryptographic protocol that enables two computers to corrumJJ1icate
securely-that is, so that each can limit the sender and receiver of
to the other. It is perhaps the most commonly used cryptographic
on the Internet today, since it is the standard protocol by which Web
communicate securely with Web servers. For completeness, we should note that
SSL was designed by Netscape and that it evolved into the industry standard
TLS protocol. In this discussion, we use SSL to mean both SSL and TLS.
SSL is a complex protocol with many options. Here, we present only a
single variation of it, and even then in a very simplified and abstract form,
so as to maintain focus on its use of cryptographic primitives. What we are
about to see is a complex dance in which asymmetric cryptography is used so
that a client and a server can establish a secure )-cey that can be used
for symmetric encryption of the session between the two-all of this while
648 Chapter 15
avoiding man-in-the-middle and replay attacks. For added cryptographic
strength, the session keys are forgotten once a session is completed. Another
communication between the two will generation of new session keys.
The SSL protocol is initiated by a c to communicate securely with a
Prior to the protocol's use, the server s is assumed to have obtained
a certificate, denoted cert, from certification authority CA. This certificate is a
structure containing the following:
Various attributes attrs of the server, such as its unique distinguished name
and its common (DNS) name
The identity of a public encryption algorithm E () for the server
The public key kc of this server
A validity interval interval durirtg which the certificate should be considered
valid
A digital signature a on the above information made by theCA-that is,
a= S(kcA)(( attrs, E(ke), interval))
In addition, prior to the protocol's use, the client is presumed to have obtained
the public verification algorithm V(kcA) for CA. In the case of the Web, the user's
browser is shipped from its vendor containing the verification algorithms and
public keys of certain certification authorities. The user can add or delete these
for certification authorities as she chooses.
When c connects to s 1 it sends a 28-byte random value nc to the server, which
responds with a random value n5 of its own, plus its certificate cert5   The client
verifies that V(kcA)( ( attrs, E (lee), interval), a) =true and that the current time
is in the validity interval interval. If both of these tests are satisfied, the server
has proved its identity. Then the client generates a random 46-byte
and sends cpms = E (ks) (pms) to the server. The server recovers
pms = D(kd)(cpms). Now both the client and the server are in possession of
nc, n5 , and pms, and each can cmnpute a shared 48-byte L  '  '''    c  
f(nc, 715 , pms), where f is a one-way and collision-resistant function.
server and client can compute ms, since only they know pms.
dependence of ms on nc and n5 ensures that ms is a fresh value-that is, a
session key that has not been used in a previous communication. At this point,
the client and the server both compute the keys the ms:
A symmetric encryption key k~[YPt for encrypting messages from
to the server
client
A symmetric encryption
to the client
lc~rypt for encrypting messages from the server
A MAC generation Jc~ac generating authenticators on
from the client to the server
A MAC generation k~~ac for generating authenticators on
from the server to the
To send a message m to the server, the client sends
15.5
15.5 649
Upon receiving c, the server recovers
(m. a)= D(Jc~ pt)(c)
and accepts m if V(lc~ac)(m, a) =true. Similarly, to send a message m to the
client, the server sends
and the client recovers
and accepts m if V(k~ac)(m, a)= true.
This protocol enables the server to limit the recipients of its messages to the
client that generated pms and to limit the senders of the messages it accepts
to that same client. Similarly, the client can limit the recipients of the messages
it sends and the senders of the messages it accepts to the party that knows
S(kd) (that is, the party that can decrypt cpms). In many applications, such
as Web transactions, the client needs to verify the identity of the party that
knows S(lcd). This is one purpose of the certificate cert5 ; in particular, the attrs
field contains information that the client can use to determine the identityfor
example, the domain name-of the server with which it is communicating.
For applications in which the server also needs information about the client,
SSL supports an option by which a client can send a certificate to the server.
In addition to its use on the Internet, SSL is being used for a wide variety
of tasks. For example, IPSec VPNs now have a competitor in SSL VPNs. IPSec
is good for point-to-point encryption of traffic-say, between two company
offices. SSL VPNs are more flexible but not as efficient, so they might be used
between an individual employee working remotely and the corporate office.
Our earlier discussion of authentication involves messages and sessions. But
what about users  If a system cannot authenticate a user, then authenticating
that a message can'le from that user is Thus, a major security problem
for operating systems is The protection system depends
on the ability to identify the programs and processes currently executing,
which in turn depends on the ability to identify each user the system. Users
identify themselves. How do we determine a user's
is authentic  Generally, user authentication is based on one or more
things: the user's possession of something (a or card), the user's
of something user identifier and password), an attribute
retina or signature).
15.5.1 Passwords
The most comm.on
When the user
a user is the use of
user ID or account name, she
650 Chapter 15
is asked for a password. I  the user-supplied password matches the password
stored in the system, the system assumes that the account is being accessed by
the owner of that account.
Passwords are often used to protect objects in the computer system, in
the absence of more complete protection schemes. They can be considered a
special case of either keys or capabilities. For instance, a password may be
associated with each resource (such as a file). Whenever a request is made to
use the resource, the password nmst be given. If the password is correct, access
is granted. Different passwords may be associated with different access rights.
For example, different passwords may be used for reading files, appending
files, and updating files.
In practice, most systems require only one password for a user to gain
full rights. Although more passwords theoretically would be more secure,
such systems tend not to be implemented due to the classic trade-off between
security and convenience. If security makes something inconvenient then the
security is frequently bypassed or otherwise circumvented.
15.5.2 Password Vulnerabilities
Passwords are extremely common because they are easy to understand and use.
Unfortunately, passwords can often be guessed, accidentally exposed, sniffed,
or illegally transferred from an authorized user to an unauthorized one, as we
show next.
There are two common ways to guess a password. One way is for the
intruder (either human or program) to know the user or to have information
about the user. All too frequently, people use obvious information (such as the
names of their cats or spouses) as their passwords. The other way is to use brute
force, trying enumeration-or all possible combinations of valid password
characters (letters, numbers, and punctuation on some systems)-until the
password is found. Short passwords are especially vulnerable to this method.
For example, a four-character password provides only 10,000 variations. On
average, guessing 5,000 times would produce a correct hit. A program that
could try a password every millisecond would take only about 5 seconds to
guess a four-character password. Enumeration is less successful where systems
allow longer passwords that include both uppercase and lowercase letters,
along with numbers and all punctuation characters. Of course, users must take
advantage of the large password space and must not, for example, use only
lowercase letters.
In addition to being guessed, passwords can be exposed as a result of
visual or electronic monitoring. An intruder can look over the shoulder of a
user when the user is logging iil and can learn the password
easily by watching the keyboard. Alternatively, anyone with access to the
network on which a computer resides can seamlessly add a network monitor,
allowing him to watch all data being transferred on the network
including user IDs and passwords. Encrypting the data stream containing the
password solves this problem. Even such a system could have passwords
stolen, however. For example, if a file is used to contain the passwords, it
could be copied for off-system analysis. Or consider a Trojan-horse program
installed on the system that captures every keystroke before sending it on to
the application.
15.5 651
Exposure is a particularly severe problem if the password is written down
where it can be read or lost. As we shall see, some systems force users to select
hard-to-remember or long passwords, which may cause a user to record the
password or to reuse it. As a result, such systems provide much less security
than systems that allow users to select easy passwords!
The final type of password compromise, illegal transfer, is the result of
human nature. Most computer installations have a rule that forbids users to
share accounts. This rule is sometimes implemented for accounting reasons but
is often aimed at improving security. For instance, suppose one user ID is shared
by several users, and a security breach occurs from that user ID. It is impossible
to know who was using the ID at the time the break occurred or even whether
the user was an authorized one. With one user per user ID, any user can be
questioned directly about use of the account; in addition, the user might notice
something different about the account and detect the break-in. Sometimes,
users break account-sharing rules to help friends or to circumvent accounting,
and this behavior can result in a system's being accessed by unauthorized users
-possibly harmful ones.
Passwords can be either generated by the system or selected by a user.
System-generated passwords may be difficult to remember, and thus users may
write them down. As mentioned, however, user-selected passwords are often
easy to guess (the user's name or favorite car, for example). Some systems will
check a proposed password for ease of guessing or cracking before accepting it.
At some sites, administrators occasionally check user passwords and notify a
user if his password is easy to guess. Some systems also age passwords, forcing
users to change their passwords at regular intervals (every three months, for
instance). This method is not foolproof either, because users can easily toggle
between two passwords. The solution, as implemented on some systems, is to
record a password history for each user. For instance, the system could record
the last N passwords and not allow their reuse.
Several variants on these simple password schemes can be used. For
example, the password can be changed more frequently. In the extren:1e, the
password is changed from session to session. A new password is selected
(either by the system or by the user) at the end of each session, and that password
must be used for the next session. In such a case, even if a password is misused,
it can be used only once. When the legitimate user tries to use a now-invalid
password at the next session, he discovers the security violation. Steps can then
be taken to repair the breached security.
15.5.3 Encrypted Passwords
One problem with all these approaches is the difficulty of keeping the password
secret within the computer. How can the system store a password securely yet
allow its use for authentication when the user presents her password  The
UNIX system uses encryption to avoid the necessity of keeping its password
list secret. Each user has a password. The system contains a function that is
extremely difficult-the designers hope impossible-to invert but is simple
to compute. That is, given a value x, it is easy to compute the function value
f(x). Given a function value j(x), however, it is impossible to compute x. This
function is used to encode all passwords. Only encoded passwords are stored.
When a user presents a password, it is encoded and compared against the
652 Chapter 15
stored encoded password. Even if the stored encoded password is seen, it
cam1ot be decoded, so the password cannot be determined. Thus, the password
file does not need to be kept secret. The functionf(x) is typically an encryption
algorithm that has been designed and tested rigorously.
The flaw in this method is that the system no longer has control over
the passwords. Although the passwords are encrypted, anyone with a copy
of the password file can run fast encryption routines against it-encrypting
each word in a dictionary, for instance, and comparing the results against
the passwords. If the user has selected a password that is also a word in the
dictionary, the password is cracked. On sufficiently fast computers, or even
on clusters of slow computers, such a comparison may take only a few hours.
Furthermore, because UNIX systems use a well-known encryption algorithm,
a cracker might keep a cache of passwords that have been cracked previously.
For these reasons, new versions of UNIX store the encrypted password entries in
a file readable only by the The programs that compare a presented
password to the stored password run setuid to root; so they can read this file,
but other users cannot. They also include a   salt,   or recorded random number,
in the encryption algorithm. The salt is added to the password to ensure that
if two plaintext passwords are the same, they result in different ciphertexts.
Another weakness in the UNIX password methods is that many UNIX
systems treat only the first eight characters as significant. It is therefore
extremely important for users to take advantage of the available password
space. To avoid the dictionary encryption method, some systems disallow the
use of dictionary words as passwords. A good technique is to generate your
password by using the first letter of each word of an easily remembered phrase
using both upper and lower characters with a number or punctuation mark
thrown in for good measure. For example, the phrase   My mother's name is
Katherine   might yield the password   Mmn.isK!  . The password is hard to
crack but easy for the user to remember.
15.5.4 One-Time Passwords
To avoid the problems of password sniffing and shoulder surfing, a system
could use a set of paired When a session begins, the system
randomly selects and presents one part of a password pair; the user must
supply the other part. In this system, the user is challenged and must
with the correct answer to that challenge.
This approach can be generalized to the use of an algorithm as a password.
The algorithm might be an integer function, for example. The system selects
a random integer and presents it to the user. The user applies a function and
replies with the correct result. The system also applies the function. If the two
results match, access is allowed.
Such algorithmic passwords are not susceptible to reuse; that is, a user can
type in a password, and no entity intercepting that password will be able to
reuse it. In this scheme, the system and the user share a secret. The secret is
never transmitted over a medium that allows exposure. Rather, the secret is
used as input to the function, along with a shared seed. A is a random
number or alphanumeric sequence. The seed is the authentication challenge
from the computer. The secret and the seed are used as input to the function
f(secret, seed). The result of this function is transmitted as the password to the
15.5 653
computer. BecaLlSe the computer also knows the secret and the seed, it can
perform the same computation. If the results match, the user is authenticated.
The next time the user needs to be authenticated, another seed is generated,
and the same ensue. This time, the password is different.
In this system, the password is different in each
instance. Anyone capturing the password from one session and trying to reuse
it in another session will faiL One-time passwords are among the only ways to
prevent improper authentication clue to password exposure.
One-time password systems are implemented in various ways. Commercial
implementations, such as SecuriD, use hardware calculators. Most of these
calculators are shaped like a credit card, a key-chain dangle, or a USB device;
they include a display and may or may not also have a keypad. Some use
the current time as the random seed. Others the user to enter the
shared secret, also known as a or on the
keypad. The display then shows the one-time password. The use of both a
one-time password generator and a PIN is one form of n!Jn-  .  ' '-  ''  
Two different types of components are needed in this case. Two-factor
authentication offers far better authentication protection than single-factor
authentication.
Another variation on one-time passwords uses a or
which is a list of single-use passwords. Each password on the list is used
once and then is crossed out or erased. The commonly used S/Key system
uses either a software calculator or a code book based on these calculations
as a source of one-time passwords. Of course, the user must protect his code
book.
15.5.5 Biometrics
Yet another variation on the use of passwords for authentication involves
the use of biometric measures. Palm- or hand-readers are commonly used to
secure physical access-for example, access to a data center. These readers
match stored parameters against what is being read from hand-reader pads.
The parameters can include a temperature map, as well as finger length, finger
width, and line patterns. These devices are currently too large and expensive
to be used for normal computer authentication.
Fingerprint readers have become accurate and cost-effective and should
become more common in the future. These devices read finger ridge patterns
and convert them into a sequence of numbers. Over time, they can store a set of
sequences to adjust for the location of the finger on the reading pad and other
factors. Software can then scan a finger on the pad and compare its features
with these stored sequences to determine if they match. Of course, multiple
users can have profiles stored, and the scanner can differentiate among them.
A very accurate two-factor authentication scheme can result from requiring
a password as well as a user name and fingerprint scan. If this information
is encrypted in transit, the system can be very resistant to spoofing or replay
attack.
is better still. Consider how strong authentication
can be with a USB device that must be plugged into the system, a PIN, and
a fingerprint scan. Except for the user's having to place her finger on a pad and
plug the USB into the system, this authentication method is no less convenient
654 Chapter 15
15.6
that using normal passwords. Recall, though, that strong authentication by
itself is not sufficient to guarantee the ID of the user. An authenticated session
can still be hijacked if it is not encrypted.
Just as there are myriad threats to system and network security, there are many
security solutions. The solutions run the gamut from improved user education,
through technology, to bug-free software. Most security professionals
subscribe to the theory of which states that more layers
of defense are better than fewer layers. Of course, this theory applies to any
kind of security. Consider the security of a house without a door lock, with
a door lock, and with a lock and an alarm. In this section, we look at the
major methods, tools, and techniques that can be used to improve resistance
to threats.
15.6.1 Security Policy
toward improving the security of any aspect of computing is to
have a . Policies vary widely but generally include a statement
of what is being secured. For example, a policy might state that all outsideaccessible
applications must have a code review before being deployed, or that
users should not share their passwords, or that all connection points between a
company and the outside must have port scans nm every six months. Without
a policy in place, it is impossible for users and administrators to know what
is permissible, what is required, and what is not allowed. The policy is a road
map to security, and if a site is trying to move from less secure to more secure,
it needs a map to know how to get there.
Once the security policy is in place, the people it affects should know it
well. It should be their guide. The policy should also be a
that is reviewed and updated periodically to ensure that it is still pertinent and
still followed.
15.6.2 Vulnerability Assessment
How can we determine whether a security policy has been correctly implemented 
The best way is to execute a vulnerability assessment. Such assessments
can cover broad ground, from social engineering through risk assessment
to port scans. Rlsl   for example, endeavors to value the
assets of the entity in question (a program, a management team, a system, or
a facility) and determine the odds that a security incident will affect the entity
and decrease its value. When the odds of suffering a loss and the amount of the
potential loss are known, a value can be placed on trying to secure the entity.
The core activity of most vulnerability assessments is a           '.,-  ~'''--in
which the entity is scanned for known vulnerabilities. Because this book
is concerned with operating systems and the software that runs on them, we
concentrate on those aspects of vulnerability assessment.
Vulnerability scans typically are done at times when computer use is
relatively low, to minimize their impact. When appropriate, they are done on
15.6 655
test systems rather than production systems, because they can induce unhappy
behavior from the target systems or network devices.
A scan within an individual system can check a variety of aspects of the
system:
Short or easy-to-guess passwords
Unauthorized privileged programs, such as setuid programs
Unauthorized programs in system directories
Unexpectedly long-running processes
Improper directory protections on user and system directories
Improper protections on system data files, such as the password file, device
drivers, or the operating-system kernel itself
Dangerous entries in the program search path (for example, the Trojan
horse discussed in Section 15.2.1)
Changes to system programs detected with checksum values
Unexpected or hidden network daemons
Any problems found by a security scan can be either fixed automatically or
reported to the managers of the system.
Networked computers are much more susceptible to security attacks than
are standalone systems. Rather than attacks from a known set of access
points, such as directly connected terminals, we face attacks from an unknown
and large set of access points-a potentially severe security problem. To a
lesser extent, systems connected to telephone lines via modems are also more
exposed.
In fact, the U.S. government considers a system to be only as secure as its
most far-reaching connection. For instance, a top-secret system may be accessed
only from within a building also considered top-secret. The system loses its topsecret
rating if any form of communication Call. occur outside that environment.
Some government facilities take extreme security precautions. The connectors
that plug a terminal into the secure computer are locked in a safe in the office
when the terminal is not in use. A person must have proper ID to gain access to
the building and her office, must know a physical lock combination, and must
know authentication information for the computer itself to gain access to the
computer-an example of multifactor authentication.
Unfortunately for systems administrators and computer-security professionals,
it is frequently impossible to lock a machine in a room and disallow all
remote access. For instance, the Internet network currently connects millions of
computers. It is becoming a mission-critical, indispensable resource for many
companies and individuals. If you consider the Internet a club, then, as in any
club with millions of members, there are many good members and some bad
members. The bad members have many tools they can use to attempt to gain
access to the interconnected computers, just as Morris did with his worm.
Vulnerability scans can be applied to networks to address some of the
problems with network security. The scans search a network for ports that
respond to a request. If services are enabled that should not be, access to them
can be blocked, or they can be disabled. The scans then determine the details of
656 Chapter 15
the application listening on that port and try to determine if it has any known
vulnerabilities. Testing those vulnerabilities can determine if the system is
ncisconfigured or lacks needed patches.
Finally though, consider the use of port scanners in the hands of a cracker
rather than someone trying to improve security. These tools could help crackers
find vulnerabilities to attack. (Fortunately, it is possible to detect port scans
through anomaly detection, as we discuss next.) It is a general challenge to
security that the same tools can be used for good and for harm. In fact, some
people advocate stating that no tools should be
written to test security, because such tools can be used to find (and exploit)
security holes. Others believe that this approach to security is not a valid one,
pointing out, for example, that crackers could write their own tools. It seems
reasonable that security through obscurity be considered one of the layers
of security only so long as it is not the only layer. For example, a company
could publish its entire network configuration; but keeping that information
secret makes it harder for intruders to know what to attack or to determine
what might be detected. Even here, though, a company assuming that such
information will remain a secret has a false sense of security.
15.6.3 Intrusion Detection
and facilities is intimately linked to intrusion detection.
as its name suggests, strives to detect attempted or successful
intrusions into computer systems and to initiate appropriate responses to the
intrusions. Intrusion detection encompasses a wide array of techniques that
vary on a number of axes, including the following:
The time at which detection occurs. Detection can occur in real time (while
the intrusion is occurring) or after the fact.
The types of inputs examined to detect intrusive activity. These may
include user-shell commands, process system calls, and network packet
headers or contents. Some forms of intrusion might be detected only by
correlating information from several such sources.
The range of response capabilities. Simple forms of response include
alerting an administrator to the potential intrusion or somehow halting
the potentially intrusive activity-for example, killing a process engaged
in such activity. In a sophisticated fonn of response; a system might
transparently divert an intruder's activity to a false resource
exposed to the attacker. The resource appears real to the attacker and
enables the system to monitor and gain information about the attack
These degrees of freedom in the design space for detecting intrusions have
a wide range of solutions, known as
and IDS systems raise an alarm
when an intrusion is detected, while IDP systems act as routers, passing traffic
unless an intrusion is detected (at which point that traffic is blocked).
But just what constitutes an intrusion  Defining a suitable specification of
intrusion turns out to be quite difficult, and thus automatic IDSs and IDPs today
settle for one of two less ambitious approaches. In the first, called
system input or network traffic is examined for
15.6 657
specific behavior patterns (or known to indicate attacks. A simple
example of signature-based detection is scanning network packets for the string
/etc/passwd/ targeted for a UNIX systenL Another example is virus-detection
software, which scans binaries or network packets for lmown viruses.
The second approach, typically called attempts
through various techniques to detect anomalous behavior within computer
systen  s. Of course, not all anomalous system activity indicates an intrusion,
but the presumption is that intrusions often induce anomalous behavior. An
example of anomaly detection is monitoring system calls of a daemon process
to detect whether the system-call behavior deviates from normal patterns,
possibly indicating that a buffer overflow has been exploited in the daemon
to corrupt its behavior. Another example is monitoring shell commands to
detect anomalous commands for a given user or detecting an anomalous login
time for a user, either of which may indicate that an attacker has succeeded in
gaining access to that user's account.
Signature-based detection and anomaly detection can be viewed as two
sides of the same coin: Signature-based detection attempts to characterize
dangerous behaviors and to detect when one of these behaviors occurs,
whereas anomaly detection attempts to characterize normal (or non dangerous)
behaviors and to detect when something other than these behaviors occurs.
These different approaches yield IDSs and IDPs with very different properties,
however. In particular, anomaly detection can find previously unknown
methods of intrusion (so-called Signature-based detection,
in contrast, will identify only known attacks that can be codified in a recognizable
pattern. Thus, new attacks that were not contemplated when the
signatures were generated will evade signature-based detection. This problem
is well known to vendors of virus-detection software, who must release new
signatures with great frequency as new viruses are detected manually.
Anomaly detection is not necessarily superior to signature-based detection,
however. Indeed, a significant challenge for systems that attempt anomaly
detection is to benchmark   normal   system behavior accurately. If the system
has already been penetrated when it is benchmarked, then the intrusive activity
may be included in the   normal   benchmark. Even if the system is benchInarked
cleanly, without influence from intrusive behaviorf the benchmark
must give a fairly complete picture of normal behavior. Otherwise, the number
of (false alarms) orf worse, (missed intrusions)
will be excessive.
To illustrate the impact of even a marginally high rate of false alarms,
consider an installation consisting of a hundred UNIX workstations from which
security-relevant events are recorded for purposes of intrusion detection. A
small installation such as this could easily generate a million audit records per
day. Only one or two might be worthy of an administrator's investigation. If we
suppose, optimistically, that each actual attack is reflected in ten audit recordsf
we can roughly compute the rate of occurrence of audit records reflecting truly
intrusive activity as follows:
2 intrusions . 10 . recor s
mtrus10n
0.00002.
658 Chapter 15
Interpreting this as a   probability of occurrence of intrusive records/' we
denote it as P(I); that is, event I is the occurrence of a record reflecting truly
intrusive behavior. Since P(I) = 0.00002, we also know that P( ~I) = 1-P(I) =
0.99998. Now we let A denote the raising of an alarm by an IDS. An accurate IDS
should maximize both P(I lA) and P(~I I~A)-that is, the probabilities that an
alarm indicates an intrusion and that no alarm indicates no intrusion. Focusil  g
on P (I I A) for the moment, we can compute it using
P(IIA)
P(I)   P(AII)
P(I)   P(AII) + P(~I)   P(AI~I)
0.00002   P(AII)
0.00002   P(AII) + 0.99998   P(AI~I)
Now consider the impact ofthe false-alarm rate P(AI~I) on P(IIA). Even
with a very good true-alarm rate of P(Ail) = 0.8, a seemingly good falsealarm
rate of P(AI~I) = 0.0001 yields P(IIA) ~ 0.14. That is, fewer than one
ill every seven alarms indicates a real intrusion! In systems where a security
administrator ilwestigates each alarm, a high rate of false alarms-called a
  Christmas tree effect  -is exceedingly wasteful and will quickly teach the
admilcistrator to ignore alarms.
This example illustrates a general principle for IDSs and IDPs: For usability,
they must offer an extremely low false-alarm rate. Achieving a sufficiently
low false-alarm rate is an especially serious challenge for anomaly-detection
systems, as mentioned, because of the difficulties of adequately benchmarking
normal system behavior. However, research contil  ues to improve anomalydetection
techniques. Intrusion detection software is evolving to implement
signatures, anomaly algorithms, and other algorithms and to combine the
results to arrive at a more accurate anomaly-detection rate.
15.6.4 Virus Protection
As we have seen, viruses can and do wreak havoc on systems. Protection from
viruses thus is an important security concern. Antivirus programs are often
used to provide this protection. Some of these programs are effective against
only particular known viruses. They work by searching all the programs on
a system for the specific pattern of instructions known to make up the virus.
When they find a known pattern, they remove the instructions,
the program. Antivirus programs may have catalogs of thousands of viruses
for which they search.
Both viruses and antivirus software continue to become more sophisticated.
Some viruses modify themselves as they infect other software to avoid the basic
pattern-match approach of antivirus programs. Antivirus programs ill turn
now look for families of patterns rather than a single pattern to identify a virus.
In fact, some antivirus programs implement a variety of detection algorithms.
They can decompress compressed viruses before checking for a signature.
Some also look for process anomalies. A process opening an executable file
for writing is suspicious, for example, unless it is a compiler. Another popular
teducique is to run a program in a which is a controlled or emulated
15.6 659
THE TRIPWIRE FILE SYSTEM
An example of an anomaly-detection tool is the
checking tool for UNIX, developed at Purdue University. Tripwire operates on
the premise that many intrusions result in modification of system directories
and files. For example, an attacker might modify the system programs,
perhaps inserting copies with Trojan horses, or might insert new programs
into directories commonly found in user-shell search paths. Or an intruder
might remove system log files to cover his tracks. Tripwire is a tool to
monitor file systems for added, deleted, or changed files and to alert system
administrators to these modifications.
The operation of Tripwire is controlled by a configurationfile tw.config
that enumerates the directories and files to be monitored for changes,
deletions, or additions. Each entry in this configuration file includes a
selection mask to specify the file attributes (inode attributes) that will be
monitored for changes. For example, the selection mask might specify that a
file's permissions be monitored but its access time be ignored. In addition, the
selection mask can instruct thatthe file be monitored for changes. Monitoring
the hash of a file for changes is as good as monitoring the file itselt but storing
hashes of files requires far less room than copying the files themselves.
When run initially, Tripwire takes as input the tw.config file and
computes a sign.ature for each file or directory consisting of its monitored
attributes (inode attributes and hash values). These signatures are stored in a
database. When run subsequently, Tripwire inputs both tw.config and the
previously stored database, recomputes the signature for each file or directory
named in tw.conf ig, and compares this signature with the signature (if any)
in the previously compl.J-ted database. Events reported to an administrator
include any monitored file or directory whose signature differs from that in
the database (a changed file), any file or directory in a monitored directory
for which a signature does not exist in the database (an added file), and any
signature in the database for which the corresponding file or directory no
longer exists (a deleted file),
Although effective for a wide class of attacks, Tripwire does have limitations.
Perhaps the most obvious is the need to protect the Tripwire program
and its associated files, especially the database file, from unauthorized modification.
For this reason, Tripwire and its associated files should be stored
on some tamper-proof medium, such as a write-protected disk or a secure
server where logins can be tightly controlled. Unforhm.ately, this makes it
less convenient to update the database after authorized updates to monitored
directories and files. A second limitation is that some security-relevant
files-for example, system log files-are supposed to change over time, and
Tripwire does not provide a way to distinguish between an authorized and
an unauthorized change. So, for example, an attack that modifies (without
deleting) a system log that would normally change anyway would escape
Tripwire's detection capabilities. The best Tripwire can do in this case is to
detectcertain obvious inconsistencies (for example, a shrinking log file). Free
and commercial versions of Tripwire are available from http:/ /tripwire.org
and.http:/ /tripwire.com.
660 Chapter 15
section of the system. The antivirus software analyzes the behavior of the code
in the sandbox before letting it run unmonitored. Some antivirus programs also
put up a complete shield rather than just scanning files within a file system.
They search boot sectors, menlOry, inbound and outbound e-mail, files as they
are downloaded, files on removable devices or media, and so on.
The best protection against computer viruses is prevention, or the practice
of Purchasing unopened software from vendors and avoiding
free or pirated copies from public sources or disk exchange offer the safest
route to preventing infection. However, even new copies of legitimate software
applications are not immune to virus infection: in a few cases, disgruntled
employees of a software company have infected the master copies of software
programs to do economic harm to the company. For macro viruses, one defense
is to exchange Microsoft Word documents in an alternative file format called
Unlike the native Word format RTF does not include the
capability to attach macros.
Another defense is to avoid opening any e-mail attachments from unknown
users. Unfortunately, history has shown that e-mail vulnerabilities appear as
fast as they are fixed. For example, in 2000, the love bug virus became very
widespread by traveling in e-mail messages that pretended to be love notes
sent by friends of the receivers. Once a receiver opened the attached Visual
Basic script, the virus propagated by sending itself to the first addresses in the
receiver's e-mail contact list. Fortunately, except for clogging e-mail systems
and users' inboxes, it was relatively harmless. It did, however, effectively
negate the defensive strategy of opening attachments only from people known
to the receiver. A more effective defense method is to avoid opening any e-mail
attachment that contains executable code. Some companies now enforce this
as policy by removing all incoming attachments to e-mail messages.
Another safeguard, although it does not prevent infection, does permit
early detection. A user must begin by completely reformatting the hard disk,
especially the boot sector, which is often targeted for viral attack Only secure
software is uploaded, and a signature of each program is taken via a secure
message-digest computation. The resulting filename and associated messagedigest
list must then be kept free from unauthorized access. Periodically, or
each time a program is run, the operating system recomputes the signature and
compares it with the signature on the original list; any differences serve as a
warning of possible infection. This technique can be combined with others. For
example, a high-overhead antivirus scan, such as a sandbox, can be used; and
if a program passes the test, a signature can be created for it. If the signatures
match the next time the program is run, it does not need to be virus-scanned
again.
15.6.5 Auditing, Accounting, and Logging
Auditing, accounting, and logging can decrease system performance, but they
are useful in several areas, including security. Logging can be general or
specific. All system-call executions can be logged for analysis of program
behavior (or misbehavior). More typically, suspicious events are logged.
Authentication failures and authorization failures can tell us quite a lot about
break-in attempts.
15
15.7 661
Accounting is another potential tool in a security administrator's kit. It
can be used to find performance changes, which in tum can reveal security
problems. One of the early UNIX computer break-ins was detected by Cliff
Stoll when he was exam5ning accounting logs and spotted an anomaly.
We turn next to the question of how a trusted computer can be connected
safely to an untrustworthy network One solution is the use of a firewall to
separate trusted and unh usted systems. A is a computer, appliance,
or router that sits between the trusted and the untrusted. A network firewall
limits network access between the two and monitors and
logs all connections. It can also limit coru1.ections based on source or destination
address, source or destination port, or direction of the connection. For instance,
Web servers use HTTP to communicate with Web browsers. A firewall therefore
may allow only HTTP to pass from all hosts outside the firewall to the Web
server within the firewalL The Morris Internet worm used the finger protocol
to break into computers, so finger would not be allowed to pass, for example.
In fact, a network firewall can separate a network into multiple domains.
A common implementation has the Internet as the untrusted domain; a
semitrusted and semisecure network, called the
as another domain; and a company's computers as a third domain (Figure
15.10). Coru1.ections are allowed from the Internet to the DMZ computers and
from the company computers to the Internet but are not allowed from the
Internet or DMZ computers to the company computers. Optionally, controlled
commurucations may be allowed between the DMZ and one company computer
or more. For instance, a Web server on the DMZ may need to query a database
server on the corporate network With a firewall, however, access is contained,
and any DMZ systems that are broken into still are unable to access the company
computers.
Internet
Internet access from company's
computers
r---------i company computers
access between DMZ and
company's computers
Figure 15.10 Domain separation via firewall.
662 Chapter 15
15.8
Of course, a firewall itself must be secure and attack-proof; otherwise,
its ability to secure connections can be compromised. Furthermore, firewalls
do not prevent attacks that or travel within protocols or com1ections
that the firewall allows. A buffer-overflow attack to a Web server will not be
stopped by the firewall, for example, because the HTTP connection is allowed;
it is the contents of the HTTP connection that house the attack. Likewise, denialof-
service attacks can affect firewalls as much as any other machines. Another
vulnerability of firewalls is in which an unauthorized host pretends
to be an authorized host by meeting some authorization criterion. For example,
if a firewall rule allows a connection from a host and identifies that host by its
IP address, then another host could send packets using that same address and
be allowed through the firewall.
In addition to the most common network firewalls, there are other, newer
kinds of firewalls, each with its pros and cons. A is a
software layer either included with the operating system or added as an
application. Rather than limiting communication between security domains, it
limits communication to (and possibly from) a given host. A user could add
a personal firewall to her PC so that a Trojan horse would be denied access to
the network to which the PC is connected, for example. An prex-y
understands the protocols that applications speak across the network.
For example, SMTP is used for mail transfer. An application proxy accepts a
com1ection just as an SMTP server would and then initiates a connection to
the original destination SMTP server. It can monitor the traffic as it forwards
the message, watching for and disabling illegal commands, attempts to exploit
bugs, and so on. Some firewalls are designed for one specific protocol. An
for example, has the specific purpose of analyzing XML traffic
and blocking disallowed or malformed XML. sit between
applications and the kernel, monitoring system-call execution. For example,
in Solaris 10, the   least privilege   feature implements a list of more than fifty
system calls that processes may or may not be allowed to make. A process that
does not need to spawn other processes can have that ability taken away, for
instance.
The U.S. Department of Defense Trusted Computer System Evaluation Criteria
specify four security classifications in systems: A, B, C, and D. This specification
is widely used to determine the security of a facility and to model security
solutions, so we explore it here. The lowest-level classification is division D, or
minimal protection. Division D includes only one class and is used for systems
that have failed to meet the requirements of any of the other security classes.
For instance, MS-DOS and Windows 3.1 are in division D.
Division C, the next level of security, provides discretionary protection and
accountability of users and their actions through the use of audit capabilities.
Division C has two levels: C1 and C2. A C1-class system incorporates some
form of controls that allow users to protect private information and to
keep other users from accidentally reading or destroying their data. A C1
environment is one in which cooperating users access data at the same levels
of sensitivity. Most versions of UNIX are C1 class.
15.8 663
The total of all protection systems within a computer system (hardware,
software, firmware) that correctly enforce a security policy is known as a
The TCB of a Cl system controls access between
users and files by allowing the user to specify and control sharing of objects
by named individuals or defined groups. In addition, the TCB requires that the
users identify themselves before they start any activities that the TCB is expected
to mediate. This identification is accomplished via a protected mechanism or
password; the TCB protects the authentication data so that they are inaccessible
to unauthorized users.
A C2-class system adds an individual-level access control to the requirements
of a Cl system. For example, access rights of a file can be specified
to the level of a single individual. In addition, the system adrninistrator can
selectively audit the actions of any one or more users based on individual
identity. The TCB also protects itself from modification of its code or data
structures. In addition, no information produced by a prior user is available
to another user who accesses a storage object that has been released back to
the system. Some speciat secure versions of UNIX have been certified at the C2
level.
Division-B mandatory-protection systems have all the properties of a classC2
system; in addition, they attach a sensitivity label to each object. The Bl-class
TCB maintains the security label of each object in the system; the label is used
for decisions pertaining to mandatory access control. For example, a user
at the confidential level could not access a file at the more sensitive secret
level. The TCB also denotes the sensitivity level at the top and bottom of each
page of any human-readable output. In addition to the normal user-namepassword
authentication information, the TCB also maintains the clearance
and authorizations of individual users and will support at least two levels of
security. These levels are hierarchicat so that a user may access any objects
that carry sensitivity labels equal to or lower than his security clearance. For
example, a secret-level user could access a file at the confidential level in the
absence of other access controls. Processes are also isolated through the use of
distinct address spaces.
A B2-class system extends the sensitivity labels to each system resource,
such as storage objects. Physical devices are assigned minimum and maximum
security levels that the system uses to enforce constraints imposed by the
physical environments in which the devices are located. In addition, a B2
system supports covert channels and the auditing of events that could lead to
the exploitation of a covert channel.
A B3-class system allows the creation of access-control lists that denote
users or groups not granted access to a given named object. The TCB also
contains a mechanism to monitor events that may indicate a violation of
security policy. The mechanism notifies the security administrator and, if
necessary, terminates the event in the least disruptive manner.
The highest-level classification is division A. Architecturally, a class-Al
system is functionally equivalent to a B3 system, but it uses formal design
specifications and verification techniques, granting a high degree of assurance
that the TCB has been implemented correctly. A system beyond class Al might
be designed and developed in a trusted facility by trusted personnel.
The use of a TCB merely ensures that the system can enforce aspects of a
security policy; the TCB does not specify what the policy should be. Typically,
664 Chapter 15
15.9
a given computing environment develops a security policy for
and has the plan by a security agency, such as the National
Computer Security Center. Certain computing environments may require other
certification, such as that supplied by TEMPEST, which guards against electronic
eavesdropping. For example, a TEMPEST-certified system has terminals that
are shielded to prevent electromagnetic fields from escaping. This shielding
ensures that equipment outside the room or building where the terminal is
housed camwt detect what information is being displayed by the terminal.
Microsoft Windows XP is a general-purpose operating system designed to
support a variety of security features and methods. In this section, we examine
features that Windows XP uses to perform security functions. For more
information and background on Wilcdows XP, see Chapter 22.
The Windows XP security model is based on the notion of
Windows XP allows the creation of any number of user accounts, which can
be grouped in any manner. Access to system objects can then be permitted or
denied as desired. Users are identified to the system by a unique security ID.
When a user logs on, Windows XP creates a that includes
the security ID for the user, security IDs for any groups of which the user is
a member, and a list of any special privileges that the user has. Examples
of special privileges include backing up files and directories, shutting down
the compute1~ logging on interactively, and changing the system clock. Every
process that Windows XP runs on behalf of a user will receive a copy of the
access token. The system uses the security IDs in the access token to permit or
deny access to system objects whenever the use1~ or a process on behalf of the
user, attempts to access the object. Authentication of a user account is typically
accomplished via a user name and password, although the modular design of
Windows XP allows the development of custom authentication packages. For
example, a retinal (or eye) scanner might be used to verify that the user is who
she says she is.
Windows XP uses the idea of a subject to ensure that programs run by a
user do not get greater access to the system than the user is authorized to have.
A is used to track and manage permissions for each program that a
user runs; it is composed of the user's access token and the program acting
on behalf of the user. Since Windows XP operates with a client-server model,
two classes of subjects are used to control access: simple subjects and server
subjects. An example of a is the typical application program
that a user executes after she logs on. simple subject is assigned a
based on the security access token of the user. A is a
process implemented as a protected server that uses the security context of the
client when acting on the client's behalf.
As mentioned in Section 15.7, auditing is a useful security technique.
Windows XP has bL1ilt-in auditing that allows many common security threats
to be monitored. Examples include failure auditing for login and logoff events
to detect random password break-ins, success auditing for login and logoff
events to detect login activity at strange hours, success and failure write-access
auditing for executable files to track a virus outbreak, and success and failure
auditing for file access to detect access to sensitive files.
15.10
15.10 665
Security attributes of an object in Windows XP are described by a
The security descriptor contains the security ID of the owner
(who can change the access permissions), a group security ID used
the POSIX subsystem, a discretionary access-control list that identifies
users or groups are allowed (and which are not allowed) access, and
a system access-control list that controls which auditing messages the system
will generate. For example, the security descriptor of the file foo.bar might have
owner avi and this discretionary access-control list:
a vi -all access
group cs-read-write access
user cliff-no access
In addition, it might have a system access-control list of audit writes by
everyone.
An access-control list is composed of access-control entries that contain
the security ID of the individual and an access mask that defines all possible
actions on the object, with a value of AccessAllowed or AccessDenied for
each action. Files in Windows XP may have the following access types: ReadData,
WriteData,AppendData, Execute,ReadExtendedAttribute, WriteExtendedAttribute,
ReadAttributes, and Wri teAttributes. We can see
how this allows a fine degree of control over access to objects.
Windows XP classifies objects as either container objects or noncontainer
objects. such as directories, can logically contain other
objects. By default, an object is created within a container object, the new
object inherits permissions from the parent object. Similarly, if the user copies a
file from one directory to a new directory, the file will inherit the permissions of
the destination directory. inherit no other permissions.
Furthermore, if a permission is changed on a directory, the new permissions
do not automatically apply to existing files and subdirectories; the user may
explicitly apply them if she so desires.
The system administrator can prohibit printilig to a printer on the system
for all or part of a day and can use the Windows XP Performance Monitor to
help her spot approaching problems. In general, Windows XP does a good job
of providing features to help ensure a secure computing environment. Many of
these features are not enabled by default, however, which may be one reason
for the myriad security breaches on Windows XP systems. Another reason is
the vast number of services Windows XP starts at system boot tiine and the
number of applications that typically are installed on a Windows XP system.
For a real multiuser environment, the system administrator should formulate
a security plan and implement it, using the features that Windows XP provides
and other security tools.
Protection is an internal problem. Security, in contrast, must consider both
the computer system and the environment-people, buildings, businesses,
valuable objects, and threats-within which the system is used.
666 Chapter 15
The data stored in the computer system must be protected from unauthorized
access, malicious destruction or alteration, and accidental introduction of
inconsistency. It is easier to protect against accidental loss of data consistency
than to protect against malicious access to the data. Absolute protection of the
information stored in a computer system from malicious abuse is not possible;
but the cost to the perpetrator can be made sufficiently high to deter most, if
not all, attempts to access that information without proper authority.
Several types of attacks can be launched against programs and agaiTlSt
individual computers or the masses. Stack- and buffer-overflow techniques
allow successful attackers to change their level of system access. Viruses and
worms are self-perpetuating, sometimes infecting thousands of computers.
Denial-of-service attacks prevent legitimate use of target systems.
Encryption limits the domain of receivers of data, while authentication
limits the domain of senders. Encryption is used to provide confidentiality
of data being stored or transferred. Symmetric encryption requires a shared
key, while asymn'letric encryption provides a public key and a private key.
Authentication, when combined with hashing, can prove that data have not
been changed.
User authentication methods are used to identify legitimate users of a
system. In addition to standard user-name and password protection, several
authentication methods are used. One-time passwords, for example, change
from session to session to avoid replay attacks. Two-factor authentication
requires two forms of authentication, such as a hardware calculator with an
activation PIN. Multifactor authentication uses three or more forms. These
methods greatly decrease the chance of authentication forgery.
Methods of preventing or detecting security incidents include intrusiondetection
systems, antivirus software, auditing and logging of system events,
monitoring of system software changes, system-call monitoring, and firewalls.
15.1 Argue for or against the judicial sentence handed down against Robert
Morris, Jr., for his creation and execution of the Internet worm discussed
in Section 15.3.1.
15.2 Discuss a means by which managers of systems connected to the
Internet could design their systems to limit or eliminate the damage
done by worms. What are the drawbacks of making the change that
you suggest 
15.3 What commonly used computer programs are prone to man-in-themiddle
attacks  Discuss solutions for preventing this form of attack.
15.4 The UNIX program COPS scans a given system for possible security
holes and alerts the user to possible problems. What are two potential
hazards of using such a system for security  How can these problems
be limited or eliminated 
667
15.5 Make a list of six security concerns for a bank's computer system. For
each item on your list, state whether this concern relates to physicat
human, or operating-system~ security.
15.6 An experimental addition to UNIX allows a user to connect a
program to a file. The watchdog is invoked whenever a program
requests access to the file. The watchdog then either grants or denies
access to the file. Discuss two pros and two cons of using watchdogs
for security.
15.7 Discuss how the asymmetric encryption algorithm can be used to
achieve the following goals.
a. Authentication: the receiver knows that only the sender could
have generated the message.
b. Secrecy: only the receiver can decrypt the message.
c. Authentication and secrecy: only the receiver can decrypt the
message, and the receiver knows that only the sender could have
generated the message.
15.8 Why doesn't D(lce, N)(E(/cd, N)(m)) provide authentication of the
sender  To what uses can such an encryption be put 
15.9 Consider a system that generates 10 million audit records per day. Also
assume that there are on average 10 attacks per day on this system and
that each such attack is reflected in 20 records. If the intrusion-detection
system has a true-alarm rate of 0.6 and a false-alarm rate of 0.0005,
what percentage of alarms generated by the system correspond to real
intrusions 
15.10 What is the purpose of using a   salt   along with the user-provided
password  Where should the   salt   be stored, and how should it be
used 
General discussions concerning security are given by Hsiao et al. [1979L
Landwehr [1981], Deru:1ing [1982], Pfleeger and Pfleeger [2003], Tanenbaum
2003, and Russell and Gangemi [1991]. Also of general interest is the text by
Lobel [1986]. Computer networking is discussed in Kurose and Ross [2005].
Issues concernin~g the design and verification of secure systems are discussed
by Rushby [1981] and by Silverman [1983]. A security kernel for a
multiprocessor microcomputer is described by Schell [1983]. A distributed
secure system is described by Rushby and Randell [1983].
Morris and Thompson [1979] discuss password security. Morshedian
[1986] presents methods to fight password pirates. Password authentication
668 Chapter 15
with insecure communications is considered by Lamport [1981]. The issue
of password cracking is examined by Seely [1989]. Cmnputer break-ins are
discussed by Lehmann [1987] and by Reid [1987]. Issues related to trusting
computer programs are discussed in Thompson (1984].
Discussions concerning UNIX security are offered by Grampp and Morris
[1984 L Wood and Kochan [1985], Farrow [1986b], Farrow [1986a ], Filipski and
Hanko [1986], Hecht et al. [1988], Kramer [1988], and Garfinkel et al. [2003].
Bershad and Pinkerton [1988] present the watchdog extension to BSD UNIX. The
COPS security-scanning package for UNIX was written by Farmer at Purdue
University. It is available to users on the Internet via the FTP program from
host ftp.uu.net in directory /pub I security I cops.
Spafford [1989] presents a detailed technical discussion of the Internet
worm. The Spafford article appears with three others in a special section on
the Morris Internet worm in Communications of the ACM (Volume 32, Number
6, June 1989).
Security problems associated with the TCP /IP protocol suite are described
in Bellovin [1989]. The mechanisms commonly used to prevent such attacks are
discussed in Cheswick et al. [2003]. Another approach to protecting networks
from insider attacks is to secure topology or route discovery. Kent et al. [2000],
Hu et al. [2002], Zapata and Asokan [2002], and Hu and Perrig [2004] present
solutions for secure routing. Savage et al. [2000] examine the distributed denialof-
service attack and propose IP trace-back solutions to address the problem.
Perlman [1988] proposes an approach to diagnose faults when the network
contains malicious routers.
Information about viruses and worms can be found at
http:/ /www.viruslist.com, as well as in Ludwig [1998] and Ludwig
[2002]. Other Web sites containing up-to-date security information
include http:/ /www.trusecure.com and httpd:/ /www.eeye.com. A
paper on the dangers of a computer monoculture can be found at
http:/ /www.ccianet.org/papers/cyberinsecurity.pdf.
Diffie and Hellman [1976] and Diffie and Hellman [1979] were the first
researchers to propose the use of the public-key encryption scheme. The algorithm
presented in Section 15.4.1 is based on the public-key encryption scheme;
it was developed by Rivest et al. [1978]. Lempel [1979], Simmons [1979],
Denning and Demting [1979], Gifford [1982], Denning [1982], Ahituv et al.
[1987], Schneier [1996], and Stallings [2003] explore the use of cryptography in
computer systems. Discussions concerning protection of digital signatures are
offered by Akl [1983], Davies [1983], Denning [1983], and Denning [1984].
The U.S. government is, of course, concerned about security. The Department
of Defense Trusted Computer System Evaluation Criteria (DoD [1985]), known
also as the Orange Book, describes a set of security levels and the features that
an operating system must have to qualify for each security rating. Reading
it is a good starting point for understanding security concerns. The Microsoft
Windows NT Workstation Resource Kit (Microsoft [1996]) describes the security
Inodel of NT and how to use that model.
The RSA algorithm is presented in Rivest et al. [1978]. Information about
NIST's AES activities can be found at http:/ /www.nist.gov/aes/; information
about other cryptographic standards for the United States can also
be found at that site. More complete coverage of SSL 3.0 can be found at
669
http:/ /home.netscape.com/eng/ssl3/. In 1999, SSL 3.0 was modified slightly
and presented in an IETF Request for Comments (RFC) under the name TLS.
The example in Section 15.6.3 illustrating the impact of false-alarm rate
on the effectiveness of IDSs is based on Axelsson [1999]. The description of
Tripwire in Section 15.6.5 is based on Kim and Spafford [1993]. Research into
system-call-based anomaly detection is described in Forrest et al. [1996].

Part Seven
A distributed system is a collection of processors that do not share memory
or a clock. Instead, each processor has its own local memory, and the
processors communicate with one another through communication lines
such as local-area or wide-area networks. The processors in a distributed
system vary in size and function. Such systems may include small handheld
or real-time devices, personal computers, workstations, and large
mainframe computer systems.
A distributed file system is a file-service system whose users, servers,
and storage devices are dispersed among the sites of a distributed
system. Accordingly, service activity has to be carried out across the
network; instead of a single centralized data repository, there are multiple
independent storage devices.
The benefits of a distributed system include giving users access to
the resources maintained by the system and thereby speeding up computation
and improving data availability and reliability. Because a system is
distributed, however, it must provide mechanisms for process synchronization
and communication, for dealing with the deadlock problem, and
for handling failures that are not encountered in a centralized system.

16.1
A distributed system is a collection of processors that do not share memory
or a clock. Instead, each processor has its own local memory. The processors
communicate with one another through various communication networks,
such as high-speed buses or telephone lines. In this chapter, we discuss the
general structure of distributed systems and the networks that interconnect
them. We contrast the main differences in operating-system design between
these systems and centralized systems. In Chapter 17, we go on to discuss
distributed file systems. Then, i11 Chapter 18, we describe the methods
necessary for distributed operating systems to coordinate their actions.
To provide a high-level overview of distributed systems and the networks
that interconnect them.
To discuss the general structure of distributed operating systems.
A is a collection of loosely coupled processors interconnected
by a communication network. From the point of view of a specific
processor in a distributed system, the rest of the processors and their respective
resources are remote, whereas its own resources are local.
The processors in a distributed system may vary in size and function.
They may include small microprocessors, workstations, minicomputers, and
large general-purpose cornputer systems. These processors are referred to by a
number of names, such as sites, nodes, computers, machines, and hosts, depending
on the context in which they are mentioned. We mainly use site to indicate the
location of a machine and host to refer to a specific system at a site. Generally,
one host at one site, the server, has a resource that another host at another
site, the client (or user), would like to use. A general structure of a distributed
system is shown in Figure 16.1.
673
674 Chapter 16
site A site C
network
communication
site B
Figure 16.1 A distributed system.
D D
D D
l resources l
There are four major reasons for building distributed systems: resource
sharing, computation speedup, reliability, and communication. In this section, we
briefly discuss each of them.
16.1.1 Resource Sharing
If a number of different sites (with different capabilities) are connected to one
another, then a user at one site may be able to use the resources available at
another. For example, a user at site A may be using a laser printer located at
site B. Meanwhile, a user at B may access a file that resides at A. In general,
in a distributed system provides mechanisms for sharing
files at remote sites, processing information in a distributed database, printing
files at remote sites, using remote specialized hardware devices (such as a
high-speed array processor), and performing other operations.
16.1.2 Computation Speedup
If a particular computation can be partitioned into subcomputations that
can run concurrently, then a distributed system allows us to distribute
the subcomputations among the various sites; the subcomputations can be
run concurrently and thus provide In addition, if
a particular site is currently overloaded with jobs, some of them can be
moved to other, lightly loaded sites. This movement of jobs is called
Automated load sharing, in which the distributed operating system
automatically moves jobs, is not yet comnlOn in commercial systems.
16.1.3 Reliability
If one site fails in a distributed system, the remammg sites can continue
operating, giving the system better reliability. If the system is composed of
multiple large autonomous installations (that is, general-purpose computers),
the failure of one of them should not affect the rest. If, however, the system
16.2
16.2 675
is composed of sncall machines, each of which is responsible for some crucial
system function (such as tenninal character I/0 or the file system), then a single
failure may halt the operation of the whole system. In general, with enough
redundancy (in both hardware and data), the system can continue operation,
even if some of its sites have failed.
The failure of a site must be detected by the system, and appropriate action
may be needed to recover from the failure. The system must no longer use the
services of that site. In addition, if the function of the failed site can be taken
over by another site, the system must ensure that the transfer of function occurs
correctly. Finally, when the failed site recovers or is repaired, mechanisms must
be available to integrate it back into the system smoothly. As we shall see in
Chapters 17 and 18, these actions present difficult problems that have many
possible solutions.
16.1.4 Communication
When several sites are connected to one another by a communication network,
users at the various sites have the opportunity to exchange information. At
a low level, are passed between systems, much as messages are
passed between processes in the single-computer message system discussed
in Section 3.4. Given message passing, all the higher-level fLmctionality found
in standalone systems can be expanded to encompass the distributed system.
Such functions include file transfer, login, mail, and remote procedure calls
(RPCs).
The advantage of a distributed system is that these functions can be
carried out over great distances. Two people at geographically distant sites can
collaborate on a project, for example. By transferring the files of the project,
logging in to each other's remote systems to run programs, and exchanging
mail to coordinate the work, users minimize the limitations inherent in longdistance
work We wrote this book by collaborating in such a manner.
The advantages of distributed systems have resulted in an industry-wide
trend toward dovmslzing. Many companies are replacing their mainframes
with networks of workstations or personal computers. Companies get a bigger
bang for the buck (that is, better functionality for the cost), more flexibility in
locating resources and expanding facilities, better user interfaces, and easier
maintenance.
In this section, we describe the two general categories of network-oriented
operating systems: network operating systems and distributed operating
systems. Network operating systems are simpler to implement but generally
more difficult for users to access and utilize than are distributed operating
systems, which provide more features.
16.2.1 Network Operating Systems
A operating provides an environment in which users, who are
aware of the multiplicity of machines, can access remote resources by either
676 Chapter 16
logging in to the appropriate remote machine or transferring data from the
remote machine to their own machines.
16.2.1.1 Remote Login
An important function of a network operating system is to allow users to log in
remotely. The Internet provides the telnet facility for this p1.npose. To illustrate
this facility, lets suppose that a user at Westminster College wishes to compute
on   cs.yale.edu,   a computer that is located at Yale University. To do so, the
user must have a valid account on that machine. To log in remotely, the user
issues the command
telnet cs.yale.edu
This command results in the formation of a socket connection between the
local machine at Westminster College and the   cs.yale.edu   computer. After this
connection has been established, the networking software creates a transparent,
bidirectional link so that all characters entered by the user are sent to a process
on   cs.yale.edu   and all the output from that process is sent back to the user. The
process on the remote machine asks the user for a login name and a password.
Once the correct information has been received, the process acts as a proxy for
the use1~ who can compute on the remote machine just as any local user can.
16.2.1.2 Remote File Transfer
Another major function of a network operating system is to provide a
mechanism for remote file transfer from one machine to another. In such
an enviromnent, each computer maintains its own local file system. If a user at
one site (say,   cs.uvm.edu  ) wants to access a file located on another computer
(say,   cs.yale.edu  ), then the file must be copied explicitly from the computer
at Yale to the computer at the University of Vermont.
The Internet provides a mechanism for such a transfer with the file transfer
protocol (FTP) program. Suppose that a user on   cs.uvm.edu   wants to copy a
Java program Server. java that resides on   cs.yale.edu.   The user must first
invoke the FTP program by executing
ftp cs.yale.edu
The program then asks the user for a login name and a password. Once
the correct information has been received, the user must connect to the
subdirectory where the file Server. java resides and then copy the file by
executing
get Server. java
In this scheme, the file location is not transparent to the user; users must know
exactly where each file is. Moreover, there is no real file sharing, because a user
can only copy a file from one site to another. Thus, several copies of the same
file may exist, resulting in a waste of space. Tn addition, if these copies are
modified, the vario-us copies will be inconsistent.
16.2 677
Notice that, in our example, the user at the University of Vermont must
have login permission on   cs.yale.edu.   PTP also provides a way to allow a user
who does not have an account on the Yale computer to copy files remotely. This
remote copying is accomplished through the   anonymous FT'P   method, which
works as follows. The file to be copied (that is, Server. java) must be placed
in a special subdirectory (say, jtp) with the protection set to allow the public
to read the file. A user who wishes to copy the file uses the ftp command as
before. When the user is asked for the login nan'le, the user supplies the name
  anonymous   and an arbitrary password.
Once anonymous login is accomplished, care must be taken by the system
to ensure that this partially authorized user does not access inappropriate
files. Generally, the user is allowed to access only those files that are in the
directory tree of user   anonymous.   Any files placed here are accessible to
any anonymous users, subject to the usual file-protection scheme used on
that machine. Anonymous users, however, cam'lot access files outside of this
directory tree.
Implementation of the FTP mechanism is similar to telnet implementation.
A daemon on the remote site watches for requests to coru'lect to the system's PTP
port. Login authentication is accomplished, and the user is allowed to execute
commands remotely. Unlike the telnet daemon, which executes any command
for the user, the PTP daemon responds only to a predefined set of file-related
commands. These include the following:
get-Transfer a file from the remote machine to the local machine.
put-Transfer from the local machine to the remote machine.
ls or dir-List files in the current directory on the remote machine.
cd -Change the current directory on the remote machine.
There are also various commands to change transfer modes (for binary or ASCII
files) and to determine connection status.
An important point about telnet and PTP is that they require the user to
change paradigms. PTP requires the user to know a command set entirely
different from the normal operating-system commands. Telnet requires a
smaller shift: the user must know appropriate commands on the remote system.
For instance, a user on a Windows machine who teh'lets to a UNIX machine
must switch to UNIX commands for the duration of the telnet session. Facilities
are more convenient for users if they do not require the use of a different
set of commands. Distributed operating systems are designed to address this
problem.
16.2.2 Distributed Operating Systems
In a distributed operating system, users access remote resources in the same
way they access local resources. Data and process migration from one site to
another is under the control of the distributed operating system.
16.2.2.1 Data Migration
Suppose a user on site A wants to access data (such as a file) that reside at site
B. The system can transfer the data by one of two basic methods. One approach
678 Chapter 16
to is to transfer the entire file to site A. From that point on, all
access to the file is local. When the user no longer needs access to the file, a
copy of the file (if it has been modified) is sent back to site B. Even if only a
modest change has been made to a large file, all the data must be transferred.
This mechanism can be thought of as an automated FTP system. This approach
was used in the Andrew file system., as we discuss in Chapter 17, but it was
found to be too inefficient.
The other approach is to transfer to site A only those portions of the file
that are actually necessary for the immediate task. If another portion is required
later, another transfer will take place. When the user no longer wants to access
the file, any part of it that has been modified must be sent back to site B. (Note
the similarity to demand paging.) The Sun Microsystems network file system
(NFS) protocol uses this method (Chapter 17), as do newer versions of Andrew.
The Microsoft SMB protocol (running on top of either TCP /IP or the Microsoft
NetBEUI protocol) also allows file sharing over a network. SMB is described in
Appendix C.6.1.
Clearly, if only a small part of a large file is being accessed, the latter
approach is preferable. If significant portions of the file are being accessed,
however, it is more efficient to copy the entire file. In both methods, data
migration includes more than the mere transfer of data from one site to another.
The system must also perform various data translations if the two sites involved
are not directly compatible (for instance, if they use different character-code
representations or represent integers with a different number or order of bits).
16.2.2.2 Computation Migration
In some circumstances, we may want to transfer the computation, rather than
the data, across the system; this approach is called For
example, consider a job that needs to access various large files that reside at
different sites, to obtain a summary of those files. It would be more efficient to
access the files at the sites where they reside and return the desired results to
the site that il  itiated the computation. Generally, if the time to transfer the data
is longer than the time to execute the remote cmmnand, the remote command
should be used.
Such a computation can be carried out in different ways. Suppose that
process P wants to access a file at site A. Access to the file is carried out at
site A and could be il  itiated by an RPC. An RPC uses a
(UDP on the Internet) to execute a routine on a remote system (Section 3.6.2).
Process P invokes a predefilced procedure at site A. The procedure executes
appropriately and then returns the results to P.
Alternatively process P can send a message to site A. The operatil  g system
at site A then creates a new process Q whose function is to carry out the
designated task. When process Q completes its execution, it sends the needed
result back to P via the message system. In this scheme, process P may execute
concurrently with process Q; in fact, it may have several processes running
concurrently on several sites.
Either method could be used to access several files residing at various sites.
One RPC might result in the ilwocation of another RPC or even in the transfer
of messages to another site. Similarly, process Q could, duril  g the course of its
16.3
16.3 679
execution, send a message to another site, which in turn would create another
process. This process might either send a message back to Q or repeat the cycle.
16.2.2.3 Process Migration
A logical extension of computation migration is na  ,  '  '~c  
process is submitted for execution, it is not always executed at
it is initiated. The entire process, or parts of it, may be executed at different
sites. This scheme may be used for several reasons:
Load balancing. The processes (or subprocesses) may be distributed across
the network to even the workload.
Computation speedup. If a single process can be divided into a number
of subprocesses that can run concurrently on different sites, then the total
process turnaround time can be reduced.
Hardware preference. The process may have characteristics that make it
more suitable for execution on some specialized processor (such as matrix
inversion on an array processor) rather than on a microprocessor.
Software preference. The process may require software that is available
at only a particular site, and either the software cannot be moved, or it is
less expensive to move the process.
Data access. Just as in computation migration, if the data being used in the
computation are numerous, it may be more efficient to have a process run
remotely than to transfer all the data.
We use two complementary techniques to move processes in a computer
network. In the first, the system can attempt to hide the fact that the process has
migrated from the client. This scheme has the advantage that the user does not
need to code her program explicitly to accomplish the migration. This method
is usually employed for achieving load balancing and computation speedup
among homogeneous systems, as they do not need user input to help them
execute programs remotely.
The other approach is to allow (or require) the user to specify explicitly
how the process should migrate. This method is usually employed when the
process must be moved to satisfy a hardware or software preference.
You have probably realized that the Web has many aspects of a distributedcomputing
environment. Certainly it provides data migration (between a Web
server and a Web client). It also provides computation migration. For instance,
a Web client could trigger a database operation on a Web server. Finally, with
Java, it provides a form of process migration: Java applets are sent from the
server to the client, where they are executed. A network operating system
provides most of these features, but a distributed operating system makes
them seamless and easily accessible. The result is a powerful and easy-to-use
facility-one of the reasons for the huge growth of the World Wide Web.
There are basically two types of networks: and
The main difference between the two is the way in
680 Chapter 16
which they are geographically distributed. Local-area networks are composed
of processors distributed over small areas (such as a single building or a number
of adjacent buildings), whereas wide-area networks are composed of a number
of autonomous processors distributed over a large area (such as the United
States). These differences imply major variations in the speed and reliability
of the communications networks, and they are reflected in the distributed
operating-system design.
16.3.1 Local-Area Networks
Local-area networks emerged in the early 1970s as a substitute for large
mainframe computer systems. For many enterprises, it is more economical
to have a number of small computers, each with its own self-contained
applications, than to have a single large system. Because each small computer
is likely to need a full complement of peripheral devices (such as disks
and printers), and because some form of data sharing is likely to occur in
a single enterprise, it was a natural step to connect these small systems into a
network.
LANs, as mentioned, are usually designed to cover a small geographical
area (such as a single building or a few adjacent buildings) and are generally
used in an office environment. All the sites in such systems are close to one
another, so the communication links tend to have a higher speed and lower
error rate than do their cou.rJerparts in wide-area networks. High-quality
(expensive) cables are needed to attain this higher speed and reliability. It is
also possible to use the cable exclusively for data network traffic. Over longer
distances, the cost of using high-quality cable is enormous, and the exclusive
use of the cable tends to be prohibitively expensive.
The most conunon links in. a local-area network are twisted-pair and fiberoptic
cabling. The most common configurations are multiaccess bus, ring,
and star networks. Communication speeds range from 1 megabit per second,
for networks such as AppleTalk, infrared, and the new Bluetooth local radio
network, to 1 gigabit per second for Ethernet. Ten megabits per second
is the speed of requires a higher-quality
cable but runs at 100 m~egabits per second and is common. Also growing is the
use of optical-fiber-based FDDI networking. The FDDI network is token-based
and runs at over 100 megabits per second.
A typical LAN may consist of a number of different computers (from
mainframes to laptops or PDAs), various shared peripheral devices (such
as laser printers and magnetic-tape drives), and one or more gateways
(specialized processors) that provide access to other networks (Figure 16.2). An
Ethernet scheme is commonly used to construct LANs. An Ethernet network
has no central controller, because it is a multiaccess bus, so new hosts can be
added easily to the network The Ethernet protocol is defined by the IEEE 802.3
standard.
There has been significant growth in using the wireless spectrum for
designing local-area networks. Wireless (or WiFi) networks allow constructing
a network using only a wireless router for transmitting signals between hosts.
Each host has a wireless adapter networking card which allows it to join and
use the wireless network However, where Ethernet systems often run at 100
megabits per second, WiFi networks typically run at slower speeds. There are
16.3 681
workstation workstation workstation
printer laptop file server
Figure 16.2 Local-area network.
several IEEE standards for wireless networks: 802.11g can theoretically run at 54
megabits per second, although ilc practice data rates are often less than half that
amount. The recent 802.11n standard provides theoretically much higher data
rates than 802.11g, although in actual practice 802.11n networks have typical
data rates of around 75 megabits per second. Data rates of wireless networks
are heavily influenced by the distance between the wireless router and the
host as well as interference in the wireless spectrum. Wireless networks often
have a physical advantage over wired Ethernet networks as no cabling needs
to be run to connect communicatilcg hosts. As a result, wireless networks are
popular in homes as well as public areas such as libraries and Internet cafes.
16.3.2 Wide-Area Networks
Wide-area networks emerged in the late 1960s, mainly as an academic research
project to provide efficient communication among sites, allowing hardware and
software to be shared conveniently and economically by a wide community
of users. The first WAN to be designed and developed was the Arpanet. Begun
in 1968, the Arpanet has grown from a four-site experimental network to a
worldwide network of networks, the Internet, comprising millions of computer
systems.
Because the sites in a WAN are physically distributed over a large geographical
area, the communication links are, by default, relatively slow and unreliable.
Typical links are telephone lines, leased (dedicated data) lines, microwave links,
and satellite channels. These communication links are controlled by special
(Figure 16.3), which are responsible for defilcing
the interface through which the sites communicate over the network, as well
as for transferring information among the various sites.
682 Chapter 16
communication
subsystem
H
H
netwot k host
communication
processor
Figure 16.3 Communication processors in a wide-area network.
For example, the Internet WAN enables hosts at geographically separated
sites to communicate with one another. The host computers typically differ
from one another in type, speed, word length, operatil1.g system, and so
on. Hosts are generally on LANs, which are, in turn, connected to the
Internet via regional networks. The regional networks, such as NSFnet il1.
the northeast United States, are interlinked with (Section 16.5.2) to
form the worldwide network. Connections between networks frequently use a
telephone-system service called T1, which provides a transfer rate of 1.544
megabits per second over a leased line. For sites requiring faster Internet
access, Tls are collected into multiple-T1 units that work in parallel to provide
more throughput. For instance, a T3 is composed of 28 T1 connections and
has a transfer rate of 45 megabits per second. The routers control the path
each message takes through the net. This routing may be either dynamic, to
increase commmlication efficiency, or static, to reduce security risks or to allow
communication charges to be computed.
Other WANs use standard telephone lines as their primary means of communication.
are devices that accept digital data from the computer
side and convert it to the analog signals that the telephone system uses. A
modem at the destination site converts the analog signal back to digital form,
and the destination receives the data. The UNIX news network, UUCP, allows
systems to communicate with each other at predetermined times, via modems,
to exchange messages. The messages are then routed to other nearby systems
and in this way either are propagated to all hosts on the network (public
messages) or are transferred to specific destinations (private messages). WANs
are generally slower than LANs; their transmission rates range from 1,200 bits
16.4
16.4 683
per second to over 1 megabit per second. UUCP has been superseded by PPP,
the point-to-point protocol. PPP functions over modem coru1ections, allowing
home computers to be fully connected to the Internet.
The sites in a distributed system can be connected physically in a variety of
ways. Each configuration has advantages and disadvantages. We can compare
the configurations by using the following criteria:
Installation cost. The cost of physically linking the sites in the system
Communication cost. The cost in time and money to send a message from
site A to site B
Availability. The extent to which data can be accessed despite the failure
of some links or sites
The various topologies are depicted in Figure 16.4 as graphs whose nodes
correspond to sites. An edge from node A to node B corresponds to a direct
communication link between the two sites. In a fully connected network, each
site is directly connected to every other site. However, the number of links
grows as the square of the number of sites, resulting in a huge installation cost.
Therefore, fully connected networks are impractical in any large system.
In a pc:ntially direct links exist between some-but
not all-pairs of sites. Hence, the installation cost of such a configuration is
lower than that of the fully connected network. However, if two sites A and
B are not directly connected, messages from one to the other must be
through a sequence of communication links. This requirement results in a
higher communication cost.
If a communication link fails, messages that would have been transmitted
across the link must be rerouted. In some cases, another route through the
network may be found, so that the messages are able to reach their destination.
In other cases, a failure may mean that no connection exists between some
pair (or pairs) of sites. When a system is split into two (or more) unconnected
subsystems, it is partitioned. Under this definition, a subsystem (or partition)
may consist of a single node.
The various partially connected network types include tree-structured
networks, ring networks, and star networks, as shown in Figure 16.4. These
types have different failure characteristics and installation and communication
costs. Installation and communication costs are relatively low for a treestructured
network. However, the failure of a single link in such a network
can result in the network's becoming partitioned. In a ring network, at least
two links must fail for partition to occur. Thus, the ring network has a higher
degree of availability than does a tree-structured network. However, the
communication cost is high, since a message may have to cross a large number
of links. In a star network, the failure of a single link results in a network
partition, but one of the partitions has only a single site. Such a partition can be
treated as a single-site failure. The star network also has a low communication
cost, since each site is at most two links away from every other site. Howeve1~
684 Chapter 16
16.5
fully connected network partially connected network
B
D
F
tree-structured network star network
F
ring network
Figure 16.4 Network topology.
if the central site fails, all the sites in the system become disconnected from one
another.
Now that we have discussed the physical aspects of networking, we turn to
the internal workings. The designer of a communication network must address
five basic issues:
Naming and name resolution. How do two processes locate each other to
communicate 
Routing strategies. How are messages sent through the network 
Packet strategies. Are packets sent individually or as a sequence 
Connection strategies. How do two processes send a sequence of messages 
16.5 685
Contention. How do we resolve conflicting demands for the network's
LISe, given that it is a shared resource 
In the following sections, we elaborate on each of these issues.
16.5.1 Naming and Name Resolution
The first component of network communication is the naming o  the systems
in the network. For a process at site A to exchange information with a process
at site B, each must be able to specify the other. Within a computer system,
each process has a process identifier, and messages may be addressed with the
process identifier. Beca use networked systems share no memory, however, a
host within the system initially has no knowledge about the processes on other
hosts.
To solve this problem, processes on remote systems are generally identified
by the pair   host name, identifier  , where host name is a name unique within
the network and identifier may be a process identifier or other unique number
within. that host. A host name is usually an alphanumeric identifier, rather than
a number, to make it easier for users to specify. For instance, site A might have
hosts named homer, marge, bart, and lisa. Bart is certainly easier to remember
than is 12814831100.
Names are convenient for humans to use, but computers prefer numbers for
speed and simplicity. For this reason, there must be a mechanism to !-': the
host name into a that describes the destination system to the networking
hardware. This mechanism is similar to the name-to-address binding that
occurs during program compilation, linking, loading, and execution (Chapter
8). In the case of host names, two possibilities exist. First, every host may have a
data file containing the names and addresses of all the other hosts reachable on
the network (similar to binding at compile time). The problem with this model
is that adding or removing a host from the network requires updati.n.g the data
files on all the hosts. The alternative is to distribute the information among
systems on the network. The network must then use a protocol to distribute
and retrieve the information. This scheme is like execution-time binding. The
first method was the one originally used on the Internet; as the Internet
rnArr-,,or it became untenable, so the second method, the domain-name  '~'      '    
is now in use.
DNS specifies the naming structure of the hosts, as well as name-to-address
resolution. Hosts on the Internet are logically addressed with multipart names
known as IP addresses. The parts of an IP address progress frorn the most
specific to the most general part, with periods separating the fields. For
instance, bob.cs.brown.edu refers to host bob in the DepaTtment of
Science at Brown University within the top-level domain edu.
domains include com for commercial sites and for organizations, as well as
connected to the for systems
the resolves
in reverse order. Each
a a process on a
a name and returns the address of the name server
As the final the name server for the host in
host-id is returned. For a made
communicate with bob.cs.brown.edu would result in
686 Chapter 16
The kernel of system A issues a request to the name server for the edu
domain, asking for the address of the name server for brown.edu. The
name server for the edu domain must be at a known address, so that it
can be queried.
The edu nance server returns the address of the host on which the brown.edu
name server resides.
The kernel on system A then queries the name server at this address and
asks about cs.brown.edu.
An address is returned; and a request to that address for bob.cs.brown.edu
now, finally, returns an host-id for that host (for example,
128.148.31.100).
This protocol may seem inefficient, but local caches are usually kept by each
name server to speed the process. For example, the edu name server would
have brown.edu in its cache and would inform system A that it could resolve
two portions of the name, returning a pointer to the cs.brown.edu name server.
Of course, the contents of these caches must be refreshed over time in case
the name server is moved or its address changes. In fact, this service is so
important that many optimizations have occurred in the protocol, as well as
many safeguards. Consider what would happen if the primary edu name server
crashed. It is possible that no edu hosts would be able to have their addresses
resolved, making them all Lmreachable! The solution is to use secondary,
back-up name servers that duplicate the contents of the primary servers.
Before the domain-name service was introduced, all hosts on the Internet
needed to have copies of a file that contained the names and addresses of each
host on the network. All changes to this file had to be registered at one site (host
SRI-NIC), and periodically all hosts had to copy the updated file from SRI-NIC
to be able to contact new systems or find hosts whose addresses had changed.
Under the domain-name service, each name-server site is responsible for
updating the host information for that domain. For instance, any host changes
at Brown University are the responsibility of the name server for brown.edu
and need not be reported anywhere else. DNS lookups will automatically
retrieve the updated information because they will contact brown.edu directly.
Within domains, there can be autonomous subdomains to further distribute
the responsibility for host-name and host-id changes.
Java provides the necessary API to design a program that maps IP names to
IP addresses. The program shown in Figure 16.5 is passed an IP name (such as
bob.cs.brown.edu) on the command line and either outputs the IP address of the
host or returns a message indicating that the host name could not be resolved.
An InetAddress is a Java class representing an IP name or address. The static
method getByName () belonging to the InetAddress class is passed a string
representation of an IP name, and it returns the corresponding InetAddress.
The program then invokes the getHostAddress () method, which internally
uses DNS to look up the IP address of the designated host.
Generally, the operating system is responsible for accepting from its
processes a message destined for   host name, identifier   and for transferring
that message to the appropriate host. The kernel on the destination host is then
responsible for transferring the message to the process named by the identifier.
This exchange is by no means trivial; it is described in Section 16.5.4.
16.5
I    
   Usage: java DNSLookUp   IP name  
   i.e. java DNSLookUp www.wiley.com
  I
public class DNSLookUp {
}
public static void main(String[] args) {
InetAddress hostAddress;
try {
}
hostAddress = InetAddress.getByName(args[O]);
System.out.println(hostAddress.getHostAddress());
catch (UnknownHostException uhe) {
}
}
System. err. println (  Unknown host:    + args [0]) ;
Figure 16.5 Java program illustrating a DNS lookup.
16.5.2 Routing Strategies
687
When a process at site A wants to communicate with a process at site B, how
is the message sent  If there is only one physical path from A to B (such as
in a star or tree-structured network), the message must be sent through that
path. However, if there are multiple physical paths from A to B, then several
routing options exist. Each site has a indicating the alternative
paths that can be used to send a message to other sites. The table may include
information about the speed and cost of the various communication paths,
and it may be updated as necessary, either manually or via programs that
exchange routing information. The three most common routing schemes are
td~.-i:Ja] and
Fixed routing. A path from A to B is specified in advance and does not
change unless a hardware failure disables it. Usually, the shortest path is
chosen, so that communication costs are minimized.
Virtual routing. A path from A to B is fixed for the duration of one
Different sessions involving messages from A to B may use different paths.
A session could be as short as a file transfer or as long as a remote-login
period.
Dynamic routing. The path used to send a message from site A to site
B is chosen only when the message is sent. Because the decision is made
dynamically, separate messages may be assigned different paths. Site A
will make a decision to send the message to site C; C, in turn, will decide
to send it to siteD, and so on. Eventually, a site will deliver the message
to B. Usually, a site sends a message to another site on whatever link is the
least used at that particular time.
There are tradeoffs among these three schem.es. Fixed routing cannot adapt
to link failures or load changes. In other words, if a path has been established
688 Chapter 16
between A and B, the messages must be sent along this path, even if the path
is down or is used more heavily than another possible path. We can partially
remedy this problem by using virtual routing and can avoid it completely by
using dynamic routing. Fixed routing and virtual routing ensure that ncessages
from A to B will be delivered in the order in which they were sent. In dynamic
routing, messages may arrive out of order. We can remedy this problem by
appending a sequence number to each message.
Dynamic routing is the most complicated to set up and run; however, it is
the best way to manage routing in complicated environments. UNIX provides
both fixed routing for use on hosts within simple networks and dynamic
routing for complicated network environments. It is also possible to mix the
two. Within a site, the hosts may just need to know how to reach the system that
connects the local network to other networks (such as company-wide networks
or the Internet). Such a node is known as a Each individual host has
a static route to the gateway, although the gateway itself uses dynamic routing
to reach any host on the rest of the network.
A router is the entity within the computer network responsible for routing
messages. A router can be a host computer with routing software or a
special-purpose device. Either way, a router must have at least two network
cmmections, or else it would have nowhere to route messages. A router decides
whether any given message needs to be passed from the network on which
it is received to any other network connected to the router. It makes this
determination by examining the destination Internet address of the message.
The router checks its tables to determine the location of the destination host, or
at least of the network to which it will send the message toward the destination
host. In the case of static routing, this table is changed only by manual update
(a new file is loaded onto the router). With dynamic routing, a
is used between routers to inform them of network changes and to allow them
to update their routing tables automatically. Gateways and routers typically
are dedicated hardware devices that run code out of firmware.
16.5.3 Packet Strategies
Messages generally vary in length. To simplify the system design., we commonly
implement communication with fixed-length messages called
or A communication incplemented in one packet can be
sent to its destination in a A connectionless message
can be in which case the sender has no guarantee that, and cannot
tell whether, the packet reached its destination. Alternatively, the packet can
be usually, in this case, a packet is returned from the destination
indicating that the packet arrived. (Of course, the return packet could be lost
along the way.) If a message is too long to fit within one packet, or if the packets
need to How back and forth between the two communicators, a connection is
established to allow the reliable exchange of multiple packets.
16.5.4 Connection Strategies
c~uuc'''~u are able to reach their destinations, processes can institute
to exchange information. Pairs of processes that
want to communicate over the network can be connected in a number of ways.
16.5 689
The three most common schemes are
and
Circuit switching. If two processes want to con1municate, a permanent
physical link is established between them. Tl1is link is allocated for the
duration of the communication session, and no other process can use
that link during this period (even if the two processes are not actively
communicating for a while). This scheme is similar to that used in the
telephone system. Once a communication line has been opened between
two parties (that is, party A calls party B), no one else can use this circuit
until the communication is terminated explicitly (for example, when the
parties hang up).
Message switching. If two processes want to communicate, a temporary
link is established for the duration of one message transfer. Physical
links are allocated dynamically among correspondents as needed and
are allocated for only short periods. Each message is a block of data
with system information-such as the source, the destination, and errorcorrection
codes (ECC)-that allows the communication network to deliver
the message to the destination correctly. This scheme is similar to the
post-office mailing system. Each letter is a message that contains both the
destination address and source (return) address. Many messages (from
different users) can be shipped over the same link.
Packet switching. One logical message may have to be divided into a
number of packets. Each packet may be sent to its destination separately,
and each therefore must include a source and a destination address with its
data. Furthermore, the various packets may take different paths through
the network. The packets must be reassembled into messages as they
arrive. Note that it is not harmful for data to be broken into packets,
possibly routed separately, and reassembled at the destination. Breaking
up an audio signal (say, a telephone communication), in contrast, could
cause great confusion if it was not done carefully.
There are obvious tradeoffs among these schemes. Circuit switching requires
substantial set-up time and may waste network bandwidth, but it incurs
less overhead for shipping each message. Conversely, message and packet
switching require less set-up time but incur more overhead per message. Also,
in packet switching, each message must be divided into packets and later
reassembled. Packet switching is the method most commonly used on data
networks because it makes the best use of network bandwidth.
16.5.5 Contention
Depending on the network topology, a link may cmmect more than two sites
in the computer network, and several of these sites may want to transmit
information over a link simultaneously. This situation occurs mainly in a ring or
multiaccess bus network. In this case, the transmitted information may become
scrambled. If it does, it must be discarded; and the sites must be notified about
the problem so that they can retransmit the information. If no special provisions
are made, this situation may be repeated, resulting in degraded performance.
690 Chapter 16
16.6
Several techniques have been developed to avoid repeated collisions, including
collision detection and token passing.
CSMA/CD. Before transmitting a message over a link, a site must listen
to determine whether another message is currently being transmitted
over that link; this technique is called -uvith
. If the link is free, the site can start transmitting. Otherwise, it must
wait (and continue to listen) until the link is free. If two or more sites begin
transmitting at exactly the same time (each thinking that no other site is
using the link), then they will register a and will
stop transmitting. Each site will try again after some random time interval.
The main problem with this approach is that, when the system is very
busy, many collisions may occur, and thus performance may be degraded.
Nevertheless, CSMA/CD has been used successfully in the Ethernet system,
the most common local area network system. One strategy for limiting the
number of collisions is to limit the number of hosts per Ethernet network.
Adding more hosts to a congested network could result in poor network
throughput. As systems get faster, they are able to send more packets per
time segment. As a result, the number of systems per Ethernet network
generally is decreasing so that networking performance is kept reasonable.
Token passing. A unique message type, known as a continuously
circulates in the system (usually a ring structure). A site that wants to
transmit information must wait until the token arrives. It then removes
the token from the ring and begins to transmit its messages. When the
site completes its round of message passing, it retransmits the token. This
action, in turn, allows another site to receive and remove the token and
to start its message transmission. If the token gets lost, the system must
detect the loss and generate a new token. It usually does that by declaring
an to choose a unique site where a new token will be generated.
Later, in Section 18.6, we present one election algorithm. A token-passing
scheme has been adopted by the IBM and HP I Apollo systems. The benefit
of a token-passing network is that performance is constant. Adding new
sites to a network may lengthen the waiting time for a token, but it will not
cause a large performance decrease, as may happen on Ethernet. On lightly
loaded networks, however, Ethernet is more efficient, because systems can
send messages at any time.
When we are designing a communication network, we must deal with the
inherent complexity of coordinating asynchronous operations communicating
in a potentially slow and error-prone environment. In addition, the systems on
the network must agree on a protocol or a set of protocols for determining
host names, locating hosts on the network, establishing connections, and
so on. We can simplify the design problem (and related implementation)
by partitioning the problem into multiple layers. Each layer on one system
communicates with the equivalent layer on other systems. Typically, each layer
has its own protocols, and communication takes place between peer layers
16.6 691
network environment
ISO environment
real systems environment
Figure 16.6 Two computers communicating via the ISO network model.
using a specific protocol. The protocols may be implemented in hardware or
software. For instance, Figure 16.6 shows the logical communications between
two computers, with the three lowest-level layers implemented in hardware.
Following the International Standards Organization (ISO), we refer to the layers
as follows:
Physical layer. The physical layer is responsible for handling both the
mechanical and the electrical details of the physical transmission of a bit
stream. At the physical layer, the communicating systems must agree on
the electrical representation of a binary 0 and 1, so that when data are
sent as a stream of electrical signals, the receiver is able to interpret the
data properly as binary data. This layer is implemented in the hardware
of the networking device.
Data-link layer. The data-link layer is responsible for handlingfi'ames, or
fixed-length parts of packets, including any error detection and recovery
that occurs in the physical layer.
Network layer. The network layer is responsible for providing connecti01cS
and for routing packets in the communication network, including
handling the addresses of outgoing packets, decoding the addresses
of incoming packets, and maintaining routing information for proper
response to changing load levels. Routers work at this layer.
Transport layer. The transport layer is responsible for low-level access
to the network and for transfer of messages between clients, including
partitioning messages into packets, maintaining packet order, controlling
flow, and generating physical addresses.
Session layer. The session layer is responsible for implementing sessions,
or process-to-process communication protocols. Typically, these protocols
are the actual communications for remote logins and for file and mail
transfers.
692 Chapter 16
Presentation layer. The presentation layer is responsible for resolving the
differences in formats among the various sites in the network, including
character conversions and half duplex-full duplex modes (character
echoing).
Application layer. The application layer is responsible for interacting
directly with users. This layer deals with file transfe1~ remote-login
protocols, and electronic mail, as well as with schemas for distributed
databases.
Figure 16.7 summarizes the set of cooperating
protocols-showing the physical flow of data. As mentioned, logically each
layer of a protocol stack communicates with the equivalent layer on other
systems. But physically, a message starts at or above the application layer and
end-user application process
distributed information
transfer-syntax negotiation
data-representation transformations
dialog and synchronization
control for application entities
network-independent
message-interchange service J
end-to~end message transfer
(connection management, error control,
fragmentation, flow control)
network routing, addressing,
call set-up and clearing
application layer
presentation layer
session layer
transport layer
network layer
data-link control
(framing, data transparency, error control) link layer
mechanical and electrical
networkcinterface connections
physical connection to
network termination equipment
physical layer
16.7 The ISO protocol stack.
16.6
data-link -layer header
network-layer header
transport-layer header
f-------1
session-layer header
f-------1
presentation layer
f-------1
application layer
message
L_ _ _____j data-link -layer trailer
Figure 16.8 An ISO network message.
693
is passed through each lower level in turn. Each layer may modify the message
and il1.clude message-header data for the equivalent layer on the receiving
side. Ultimately, the message reaches the data-network layer and is transferred
as one or more packets (Figure 16.8). The data-lil1.k layer of the target system
receives these data, and the message is moved up through the protocol stack;
it is analyzed, modified, and stripped of headers as it progresses. It fu1.ally
reaches the application layer for use by the receiving process.
The ISO model formalizes some of the earlier work done in network
protocols but was developed in the late 1970s and is currently not in widespread
use. Perhaps the most widely adopted protocol stack is the TCP /IP model, which
has been adopted by virtually all Internet sites. The TCP /IP protocol stack
has fewer layers than does the ISO model. Theoretically, because it combilles
several functions ill each layer, it is more difficult to implement but more
efficient than ISO networking. The relationship between the ISO and TCP /IP
models is shown in Figure 16.9. The TCP /IP application layer identifies several
protocols ill widespread use ill the Internet, illcluding HTTP, FTP, Telnet, DNS,
and SMTP. The transport layer identifies the unreliable, connectionless user
datagram protocol (UDP) and the reliable, connection-oriented transmission
control protocol (TCP). The Internet protocol (IP) is responsible for routing IP
datagrams through the Internet. The TCP /IP model does not formally identify
a link or physical laye1~ allowing TCP /IP traffic to run across any physical
network. In Section 16.9, we consider the TCP /IP model running over an
Ethernet network.
Security should be a concern in the design and implementation of any
modern communication protocol. Both strong authentication and encryption
are needed for secure communication. Strong authentication ensures that
the sender and receiver of a communication are who or what they are
supposed to be. Encryption protects the contents of the communication
from eavesdropping. Weak authentication and clear-text communication are
still very common, however, for a variety of reasons. When most of the
694 Chapter 16
16.7
ISO
presentation
session
physical
TCP/IP
HTTP, DNS, Telnet
SMTP, FTP
not defined
not defined
TCP-UDP
not defined
not defined
Figure 16.9 The ISO and TCP/IP protocol stacks.
common protocols were designed, security was frequently less important than
performance, simplicity, and efficiency.
Strong authentication requires a multistep handshake protocol or authentication
devices, adding complexity to a protocol. Modern CPUs can efficiently
perform encryption, and systems frequently offload encryption to separate
cryptography processors, so system performance is not compromised. Longdistance
communication can be made secure by authenticating the endpoints
and encrypting the stream of packets in a virtual private network, as discussed
in 15.4.2. LAN communication remains unencrypted at most sites, but protocols
such as NFS Version 4, which includes strong native authentication and
encryption, should help improve even LAN security.
A distributed system may suffer from various types of hardware failure. The
failure of a link, the failure of a site, and the loss of a message are the most
common types. To ensure that the system is robust, we must detect any of these
failures, reconfigure the system so that computation can continue, and recover
when a site or a link is repaired.
16.7.1 Failure Detection
In an environment with no shared memory, we are generally unable to
differentiate among link failure, site failure, and message loss. We can usually
detect only that one of these failures has occurred. Once a failure has been
16.7 695
detected, appropriate action must be taken. What action is appropriate depends
on the particular application.
To detect link and site failure, we use a procedure. Suppose
that sites A and B have a direct physical link between them .. At fixed intervals,
the sites send each other an J-am-up m.essage. If site A does not receive this
message within a predetermined time period, it can assume that site B has
failed, that the link between A and B has failed, or that the message from B
has been lost. At this point, site A has two choices. It can wait for another time
period to receive an J-am-up message from B, or it can send an Are-you-up 
message to B.
If time goes by and site A still has not received an J-am-up message, or if site
A has sent an Are-you-up  message and has not received a reply, the procedure
can be repeated. Again, the only conclusion that site A can draw safely is that
some type of failure has occurred.
Site A can try to differentiate between link failure and site failure by sending
an Are-you-up  message to B by another route (if one exists). If and when B
receives this message, it immediately replies positively. This positive reply tells
A that B is up and that the failure is in the direct link between them. Since we
do not know in advance how long it will take the message to travel from A to B
and back, we must use a At the time A sends the Are-you-up 
message, it specifies a time interval during which it is willing to wait for the
reply from B. If A receives the reply message within that time interval, then it
can safely conclude that B is up. If not, however (that is, if a time-out occurs),
then A may conclude only that one or more of the following sih1ations has
occurred:
Site B is down.
The direct link (if one exists) from A to B is down.
The alternative path from A to B is down.
The message has been lost.
Site A cannot, however, determine which of these events has occurred.
16.7.2 Reconfiguration
Suppose that site A has discovered, through the mechanism described in the
previous section, that a failure has occurred. It must then initiate a procedure
that will allow the system to reconfigure and to continue its normal mode of
operation.
If a direct link from A to B has failed, this information must be broadcast to
every site in the system, so that the various routing tables can be updated
accordingly.
If the system believes that a site has failed (because that site can be reached
no longer), then all sites in the system must be so notified, so that they
will no longer attempt to use the services of the failed site. The failure of a
site that serves as a central coordinator for some activity (such as deadlock
detection) requires the election of a new coordinator. Similarly, if the failed
696 Chapter 16
site is part of a logical ring, then a new logical ring must be constructed.
Note that, if the site has not failed (that is, if it is up but camwt be reached),
then we may have the undesirable situation in which two sites serve as the
coordinator. When the network is partitioned, the two coordinators (each
for its own partition) may initiate conflicting actions. For example, if the
coordinators are responsible for implementing mutual exclusion, we may
have a situation in which two processes are executing simultaneously in
their critical sections.
16.7.3 Recovery from Failure
When a failed link or site is repaired, it must be integrated into the system
gracefully and smoothly.
Suppose that a link between A and B has failed. Wlcen it is repaired,
both A and B must be notified. We can accomplish this notification by
continuously repeating the handshaking procedure described in Section
16.7.1.
Suppose that site B has failed. Wlcen it recovers, it must notify all other sites
that it is up again. Site B then may have to receive information from the
other sites to update its local tables; for example, it may need routing-table
information, a list of sites that are down, or mcdelivered messages and
mail. If the site has not failed but simply could not be reached, then this
information is still required.
16.7.4 Fault Tolerance
A distributed system must tolerate a certain level of failure and continue to
function normally when faced with various types of failures. Making a facility
fault tolerant starts at the protocol level, as described above, but continues
through all aspects of the system. We use the term fault tolerance in a broad
sense. Communication faults, machine failures (of type fail-stop where the
machine stops before performing an erroneous operation that is visible to other
processors), storage-device crashes, and decays of storage media should all be
tolerated to some extent. A should continue to function,
perhaps in a degraded form, when faced with such failures. The degradation
can be in performance, in functionality, or in both. It should be proportional,
however, to the failures that caused it. A system that grinds to a halt when only
one of its components fails is certainly not fault tolerant.
Unfortunately, fault tolerance can be difficult and expensive to implement.
At the network layer, multiple redundant communication paths and network
devices such as switches and routers are needed to avoid a cmnmunication
failure. A storage failure can cause loss of the operating system, applications,
or data. Storage units can include redundant hardware components that
automatically take over from each other in case of failure. In addition, RAID
systems can ensure continued access to the data even in the event of one or
more disk failures (Section 12.7).
A system failure without redundancy can cause an application or an entire
facility to stop operation. The Inost simple system failure involves a system
running only stateless applications. These applications can be restarted without
16.8
16.8 697
compromising the operation; so as long as the applications can run on more
than one computer (node), operation can continue. Such a facility is commonly
known as a because it is computation-centric.
In contrast, systems involve running applications that access
and modify shared data. As a result, data-centric computing facilities are more
difficult to make fault tolerant. They failure-monitoring software and
special infrastructure. For instance, such as Veritas
Cluster and Sun Cluster include two or more computers and a set of shared
disks. Any given application can be stored on the computers or on the shared
disk, but the data must be stored on the shared disk. The running application's
node has exclusive access to the application's data on disk. The application is
monitored by the cluster software, and if it fails it is automatically restarted.
If it camwt be restarted, or if the entire computer fails, the node's exclusive
access to the application's data is terminated and is granted to another node
in the cluster. The application is restarted on that new node. The application
loses whatever state information was in the failed system's memory but can
continue based on whatever state it last wrote to the shared disk. From a user's
point of view, a service was interrupted and then restarted, possibly with some
data missing.
Specific applications may improve on this functionality by implementing
lock management along with clustering. With lock management (Section
18.4.1), the application can run on multiple nodes and can use the same data
on shared disks concurrently. Clustered databases frequently implement this
functionality. If anode fails, transactions can continue on other nodes, and users
notice no interruption of service, as long as the client is able to automatically
locate the other nodes in the cluster. Any noncommitted transactions on the
failed node are lost, but again, client applications can be designed to retry
noncommitted transactions if they detect a failure of their database node.
Making the multiplicity of processors and storage devices to the
users has been a key challenge to many designers. Ideally, a distributed system
should look to its users like a conventional, centralized system. The user
interface of a transparent distributed system should not distinguish between
local and remote resources. That is, users should be able to access remote
resources as though these resources were local, and the distributed system
should be responsible for locating the resources and for arranging for the
appropriate interaction.
Another aspect of transparency is user mobility. It would be convenient
to allow users to log into any machine in the system rather than forcing
them to use a specific machine. A transparent distributed system facilitates
user mobility by bringiicg over the user's environment (for example, home
directory) to wherever he logs in. Both the Andrew file system from CMU and
Project Athena from MIT provide this functionality on a large scale; NFS can
provide it on a smaller scale.
Still another issue is L;. -the capability of a system to adapt to
increased service load. Systems have bounded resources and can become
completely saturated under increased load. For example, with respect to a file
698 Chapter 16
system, saturation occurs either when a server's CPU runs at a high utilization
rate or when disks are almost full. Scalability is a relative property, but it can be
measured accurately. A scalable system reacts more gracefully to increased load
than does a nonscalable one. First, its performance degrades more moderately;
and second, its resources reach a saturated state later. Even perfect design
cannot accommodate an ever-growing load. Adding new resources might solve
the problem, but it might generate additional indirect load on other resources
(for example, adding machines to a distributed system can clog the network
and increase service loads). Even worse, expanding the system can call for
expensive design modifications. A scalable system should have the potential
to grow without these problems. In a distributed system, the ability to scale
up gracefully is of special importance, since expanding the network by adding
new machines or interconnecting two networks is commonplace. In short, a
scalable design should withstand high service load, accommodate growth of
the user community, and enable simple integration of added resources.
Scalability is related to fault tolerance, discussed earlier. A heavily loaded
component can become paralyzed and behave like a faulty component. Also,
shifting the load from a faulty component to that component's backup can
saturate the latter. Generally, having spare resources is essential for ensuring
reliability as well as for handling peak loads gracefully. An inherent advantage
of a distributed system is a potential for fault tolerance and scalability because
of the multiplicity of resources. However, inappropriate design can obscure
this potential. Fault-tolerance and scalability considerations call for a design
demonstrating distribution of control and data.
Very large-scale distributed systems, to a great extent, are still only
theoretical. No magic guidelines ensure the scalability of a system. It is easier
to point out why current designs are not scalable. We next discuss several
designs that pose problems and propose possible solutions, all in the context
of scalability.
One principle for designing very large-scale systems is that the service
demand from any component of the system should be bounded by a constant
that is independent of the number of nodes in the system. Any service
mechanism whose load demand is proportional to the size of the system is
destined to become clogged once the system grows beyond a certain size.
Adding more resources will not alleviate such a problem. The capacity of this
mechanism simply limits the growth of the system.
Another principle concerns centralization. Central control schemes and
central resources should not be used to build scalable (and fault-tolerant)
systems. Examples of centralized entities are central authentication servers,
central naming servers, and central file servers. Centralization is a form of
functional asyrrunetry among machines constituting the system. The ideal
alternative is a functionally symmetric configuration; that is, all the component
machines have an equal role in the operation of the system, and hence each
machine has some degree of autonomy. Practically, it is virtually impossible to
comply with such a principle. For instance, incorporating diskless machines
violates functional symmetry, since the workstations depend on a central disk
However, autonomy and symmetry are important goals to which we should
aspire.
Deciding on the process structure of the server is a major problem in
the design of any service. Servers are supposed to operate efficiently in peak
16.9
16.9 699
periods, when hundreds of active clients need to be served simultaneously. A
single-process server is certainly not a good choice, since whenever a request
necessitates disk I/0, the whole service will be blocked. Assigning a process for
each client is a better choice; however, the expense of frequent context switches
between the processes must be considered. A related problem occurs because
all the server processes need to share information.
One of the best solutions for the server architecture is the use of lightweight
processes, or threads, which we discuss in Chapter 4. We can think of a group
of lightweight processes as multiple threads of control associated with some
shared resources. Usually, a lightweight process is not bound to a particular
client. Instead, it serves single requests of different clients. Scheduling of
threads can be preemptive or nonpreemptive. If threads are allowed to run
to completion (nonpreemptive), then their shared data do not need to be
protected explicitly. Otherwise, some explicit locking mechanism must be used.
Clearly, some form of lightweight-process scheme is essential if servers are to
be scalable.
We now return to the name-resolution issue raised in Section 16.5.1 and
examine its operation with respect to the TCF /IF protocol stack on the Internet.
We consider the processing needed to transfer a packet between hosts on
different Ethernet networks.
In a TCF /IF network, every host has a name and an associated IF address
(or host-id). Both of these strings must be unique; and so that the name space
can be managed, they are segmented. The name is hierarchical (as explained
in Section 16.5.1), describing the host name and then the organization with
which the host is associated. The host-id is split into a network number and a
host number. The proportion of the split varies, depending on the size of the
network. Once the Internet adrninistrators assign a network number, the site
with that number is free to assign host-ids.
The sending system checks its routing tables to locate a router to send the
frame on its way. The routers use the network part of the host-id to transfer
the packet from its source network to the destination network. The destination
system then receives the packet. The packet may be a complete message, or it
may just be a component of a message, with more packets needed before the
message can be reassembled and passed to the TCF /UDF layer for transmission
to the destination process.
Now we know how a packet moves from its source network to its
destination. Within a network, how does a packet move from sender (host
or router) to receiver  Ethernet device has a unique byte number, called
the assigned to it for addressing. Two
devices on a LAN communicate with each other only with this number. If
a system needs to send data to another system, the networking software
generates an containing the IF
address of the destination system. This packet is to all other systems
on that Ethernet network.
A broadcast uses a special network address (usually, the maximum
address) to signal that all hosts should receive and process the packet. The
700 Chapter 16
broadcast is not re-sent by gateways, so only systems on the local network
receive it. Only the system whose IP address matches the IP address of the ARP
request responds and sends back its MAC address to the system that initiated
the query. For efficiency, the host caches the IP-MAC address pair in an internal
table. The cache entries are so that an entry is eventually removed from
the cache if an access to that system is not required within a given time. In
this way, hosts that are removed from a network are eventually forgotten. For
added performance, ARP entries for heavily used hosts may be hardwired in
the ARP cache.
Once an Ethernet device has announced its host-id and address, communication
can begin. A process may specify the name of a host with which to
communicate. Networking software takes that name and determines the IP
address of the target, using a DNS lookup. The message is passed from the
application laye1~ through the software layers, and to the hardware layer. At
the hardware layer, the packet (or packets) has the Ethernet address at its start;
a trailer indicates the end of the packet and contains a for detection
of packet damage (Figure 16.10). The packet is placed on the network by the
Ethernet device. The data section of the packet may contain some or all of the
data of the original message, but it may also contain some of the upper-level
headers that compose the message. In other words, all parts of the original
message must be sent from source to destination, and all headers above the
802.3layer (data-link layer) are included as data in the Ethernet packets.
If the destination is on the same local network as the source, the system
can look in its ARP cache, find the Ethernet address of the host, and place the
packet on the wire. The destination Ethernet device then sees its address in the
packet and reads in the packet passing it up the protocol stack.
If the destination system is on a network different from that of the source,
the source system finds an appropriate router on its network and sends the
packet there. Routers then pass the packet along the WAN 1-mtil it reaches its
bytes
7
2 or 6
2 or 6
2
0-1500
0-46
4
Pt.e~~n1bh:l.~s.tartfc1Ft:r  =ccll  et( 1 each byte pattern 1010101 o
data
pattern 10101011
Ethernet address or broadcast
Ethernet address
length in bytes
message data
message must be   63 bytes long
for error detection
Figure i 6.10 An Ethernet packet.
16.10
701
destination network. The router that connects the destination network checks
its ARP cache, finds the Ethernet number of the destination, and sends the
packet to that host. Through all of these transfers, the data-link-layer header
may change as the Ethernet address of the next router in the chain is used, but
the other headers of the packet remain the same until the packet is received
and processed by the protocol stack and finally passed to the receiving process
by the kernel.
A distributed system is a collection of processors that do not share memory or
a clock. Instead, each processor has its own local memory, and the processors
communicate with one another through various communication lines, such
as high-speed buses and telephone lines. The processors in a distributed
system vary in size and function. They may include small microprocessors,
workstations, minicomputers, and large general-purpose computer systems.
The processors in the system are connected through a communication
network, which can be configured in a number of ways. The network may
be fully or partially connected. It may be a tree, a star, a ring, or a multiaccess
bus. The communication-network design must include routing and com1ection
strategies, and it must solve the problems of contention and security.
A distributed system provides the user with access to the resources
the system provides. Access to a shared resource can be provided by data
migration, computation migration, or process migration.
Protocol stacks, as specified by network layering models,   massage   the
message, adding information to it to ensure that it reaches its destination. A
naming system (such as DNS) must be used to translate from a host name
to a network address, and another protocol (such as ARP) may be needed
to translate the network number to a network device address (an Ethernet
address, for instance). If systems are located on separate networks, routers are
needed to pass packets from source network to destination network.
A distributed system may suffer from various types of hardware failure.
For a distributed system to be fault tolerant, it must detect hardware failures
and reconfigure the system. When the failure is repaired, the system must be
reconfigured again.
16.1 What are the advantages of using dedicated hardware devices for
routers and gateways  What are the disadvantages of using these
devices compared with using general-purpose computers 
16.2 Why would it be a bad idea for gateways to pass broadcast packets
between networks  What would be the advantages of doing so 
16.3 Consider a network layer that senses collisions and retransmits immediately
on detection of a collision. What problems could arise with this
strategy  How could they be rectified 
702 Chapter 16
16.4 Even though the ISO model of networking specifies seven layers of
functionality, most computer systems use fewer layers to implement a
network. Why do they use fewer layers  What problems could the use
of fewer layers cause 
16.5 The lower layers of the ISO network model provide datagram service,
with no delivery guarantees for messages. A transport-layer protocol
such as TCP is used to provide reliability. Discuss the advantages and
disadvantages of supporting reliable message delivery at the lowest
possible layer.
16.6 What are the advantages and the disadvantages of making the computer
network transparent to the user 
16.7 Under what circumstances is a token-passing network more effective
than an Ethernet network 
16.8 Process migration within a heterogeneous network is usually impossible,
given the differences in architectures and operating systems.
Describe a method for process migration across different architectures
running:
a. The same operating system
b. Different operating systems
16.9 Contrast the various network topologies in terms of the following
attributes:
a. Reliability
b. Available bandwidth for concurrent communications
c. Installation cost
d. Load balance in routing responsibilities
16.10 How does using a dynamic routing strategy affect application behavior 
For what type of applications is it beneficial to use virtual routing
instead of dynamic routing 
16.11 The original HTTP protocol used TCP /IP as the underlying network
protocol. For each page, graphic, or applet, a separate TCP session was
constructed, used, and torn down. Because of the overhead of building
and destroying TCP liP connections, performance problems resulted
from this implementation method. Would using UDP rather than TCP
be a good alternative  What other changes could you make to improve
HTTP performance 
16.12 What are the advantages and disadvantages of using circuit switching 
For what kinds of applications is circuit switching a viable strategy 
16.13 In what ways is using a name server better than using static host tables 
What problems or complications are associated with name servers 
What methods could you use to decrease the amount of traffic name
servers generate to satisfy translation requests 
703
16.14 Of what use is an address-resolution protocol  Why is it better to use
such a protocol than to make each host read each packet to determine
that packet's destination  Does a token-passing network need such a
protocol  Explain your answer.
16.15 What is the difference between computation migration and process
migration  Which is easier to implement, and why 
16.16 Run the program shown in Figure 16.5 and determine the IP addresses
of the following host names:
www.wiley.com
www.cs.yale.edu
www.apple.com
www.westminstercollege.edu
www.ietf.org
16.17 To build a robust distributed system, you must know what kinds of
failures can occur.
a. List three possible types of failure in a distributed system.
b. Specify which of the entries in your list also are applicable to a
centralized system.
16.18 Explain why doubling the speed of the systems on an Ethernet segment
may result in decreased network performance. What changes could
help solve this problem 
16.19 Name servers are organized in a hierarchical manner. What is the
purpose of using a hierarchical organization 
16.20 Consider a distributed system with two sites, A and B. Consider
whether site A can distinguish among the following:
a. B goes down.
b. The link between A and B goes down.
c. B is extremely overloaded, and its response time is 100 times
longer than normal.
What implications does your answer have for recovery in distributed
systems 
Tanenbaum [2003], Stallings [2000a], and Kurose and Ross [2005] provide
general overviews of computer networks. Williams [2001] covers computer
networking from a computer-architecture viewpoint.
The Internet and its protocols are described in Comer [1999] and Comer
[2000]. Coverage of TCP /IP can be found in Stevens [1994] and Stevens [1995].
704 Chapter 16
UNIX network programming is described thoroughly in Stevens [1997] and
Stevens [1998].
Discussions concerning distributed operating-system structures have been
offered by Coulouris et al. [2001] and Tanenbaum and van Steen [2002].
Load balancing and load sharing are discussed by I-Iarchol-Balter and
Downey [1997] and Vee and I-Isu [2000]. I-Iarish and Owens [1999] describes
load-balancing DNS servers. Process migration is discussed by Jul et al. [1988],
Douglis and Ousterhout [1991], Han and Ghosh [1998], and Milojicic et al.
[2000]. Issues relating to a distributed virtual machine for distributed systems
are examined in Sirer et al. [1999].
17.1
In the previous chapter, we discussed network construction and the low-level
protocols needed to transfer between systems. Now we examine
one use of this infrastructure. A is a distributed
implementation of the classical time-sharing of a file system, where
multiple users share files and storage resources (Chapter 11). The purpose of
a DFS is to support the same kind of sharing when the files are physically
dispersed among the sites of a distributed system.
In this chapter, we describe how a DFS can be designed and implemented.
First, we discuss common concepts on which DFSs are based. Then, we illustrate
our concepts by examining one influential DFS-the Andrew file system (AFS).
To explain the naming mechanism that provides location transparency and
independence.
To describe the various methods for accessing distributed files.
To contrast stateful and stateless distributed file servers.
To show how replication of files on different machines in a distributed file
system is a useful redundancy for improving availability.
To introduce the Andrew file system (AFS) as an example of a distributed
file system.
As we noted in the preceding chapter, a distributed system is a collection
of loosely coupled computers interconnected by a communication network.
These computers can share physically dispersed files by using a distributed
file system (DFS). In this chapter, we use the term DFS to mean distributed file
systems in general, not the commercial Transarc DFS product; we refer to the
latter as Transarc DFS. Also, NFS refers to NFS Version 3, unless otherwise noted.
705
706 Chapter 17
To explain the structure of a DFS, we need to define the terms service, server,
and client. A is a software entity running on one or more machines
and providing a particular type of function to clients. A is the service
software running on a single machine. A is a process that can invoke
a service using a set of operations that form its Sometimes a
lower-level interface is defined for the actual cross-machine interaction; it is
the
Using this terminology, we say that a file system provides file services to
clients. A client interface for a file service is formed by a set of primitive file
operations, such as create a file, delete a file, read from a file, and write to a file.
The primary hardware concponent that a file server controls is a set of local
secondary-storage devices (usually, magnetic disks) on which files are stored
and from which they are retrieved according to the clients' requests.
A DFS is a file system whose clients, servers, and storage devices are
dispersed among the machines of a distributed system. Accordingly, service
activity has to be carried out across the network. Instead of a single centralized
data repository, the system frequently has multiple and independent storage
devices. As you will see, the concrete configuration and implementation of a
DFS may vary from system to system. In some configurations, servers run on
dedicated machines; in others, a machine can be both a server and a client. A DFS
can be implemented as part of a distributed operating system or, alternatively,
by a software layer whose task is to manage the communication between
conventional operating systems and file systems. The distinctive features of a
DFS are the multiplicity and autonomy of clients and servers in the system.
Ideally, a DFS should appear to its clients to be a conventional, centralized
file system. The multiplicity and dispersion of its servers and storage devices
should be made invisible. That is, the client interface of a DFS should not
distinguish between local and remote files. It is up to the DFS to locate the
files and to arrange for the transport of the data. A DFS facilitates
user mobility by bringing a user's environment (that is, home directory) to
wherever the user logs in.
The most important performance measure of a DFS is the amount of time
needed to satisfy service requests. In conventional systems, this time consists of
disk-access time and a small amount of CPU-processing time. In a DFS, however,
a remote access has the additional overhead attributed to the distributed
structure. This overhead includes the time to deliver the request to a server, as
well as the time to get the response across the network back to the client. For
each direction, in addition to the transfer of the information, there is the CPU
overhead of running the communication protocol software. The performance
of a DFS can be viewed as another dimension of the DFS's transparency. That is,
the performance of an ideal DFS would be comparable to that of a conventional
file system.
The fact that a DFS manages a set of dispersed storage devices is the DFS' s
key distinguishing feature. The overall storage space managed by a DFS is
composed of different and remotely located smaller storage spaces. Usually,
these constituent storage spaces correspond to sets of files. A cmnpm1.c:nt
is the smallest set of files that can be stored on a single machine, independently
from other units. All files belonging to the same component unit must reside
in the same location.
17.2
17.2 707
is a mapping between logical and physical objects. For instance,
users deal with logical data objects represented by file nances, whereas the
system manipulates physical blocks of data stored on disk tracks. Usually, a
user refers to a file by a textual name. The latter is mapped to a lower-level
numerical identifier that in turn is mapped to disk blocks. This multilevel
mapping provides users with an abstraction of a file that hides the details of
how and where on the disk the file is stored.
In a transparent DFS, a new dimension is added to the abstraction: that of
hiding where in the network the file is located. In a conventional file system, the
range of the naming mapping is an address within a disk. In a DFS, this range
is expanded to include the specific machine on whose disk the file is stored.
Going one step further with the concept of treating files as abstractions leads
to the possibility of Given a file name, the mapping returns a
set of the locations of this file's replicas. In this abstraction, both the existence
of multiple copies and their locations are hidden.
17.2.1 Naming Structures
We need to differentiate two related notions regarding name mappings in a
DFS:
. The name of a file does not reveal any hint of the
file's physical storage location.
'J'  ~'  '''.  ,  ' '-'  '  The name of a file does not need to be changed
when the file's physical storage location changes.
Both definitions relate to the level of naming discussed previously,
since files have different names at different levels (that is, user-level textual
names and system-level numerical identifiers). A location-independent naming
scheme is a dynamic mapping, since it can map the same file name to
different locations at two different times. Therefore, location independence is
a stronger property than is location transparency.
In practice, most of the current DFSs provide a static, location-transparent
mapping for user-level names. These systems, however, do not support
  that is, changing the location of a file automatically is impossible.
Hence, the notion of location independence is irrelevant for these systems.
Files are associated permanently with a specific set of disk blocks. Files and
disks can be moved between machines manually, but file migration implies an
automatic, operating-system-initiated action. Only AFS and a few experimental
file systems support location independence and file mobility. AFS supports file
mobility mainly for administrative purposes. A protocol provides migration
of AFS component units to satisfy high-level user requests, without changing
either the user-level names or the low-level names of the corresponding files.
A few aspects can further differentiate location independence and static
location transparency:
Divorce of data from location, as exhibited by location independence,
provides a better abstraction for files. A file name should denote the file's
708 Chapter 17
most significant attributes, which are its contents ratber than its location.
Location-independent files can be viewed as logical data containers that
are not attached to a specific storage location. If only static location
transparency is supported, the file name still denotes a specific, although
hidden, set of physical disk blocks.
Static location transparency provides users with a convenient way to share
data. Users can share remote files by simply naming the files in a locationtransparent
manner, as though the files were local. Nevertheless, sharing
the storage space is cumbersome, because logical names are still statically
attached to physical storage devices. Location independence promotes
sharing the storage space itself, as well as the data objects. When files can
be mobilized, the overall, system-wide storage space looks like a single
virtual resource. A possible benefit of such a view is the ability to balance
the utilization of disks across the system.
Location independence separates the naming hierarchy from the storagedevices
hierarchy and from the intercomputer structure. By contrast, if
static location transparency is used (although names are transparent),
we can easily expose the correspondence between component units and
machines. The machines are configured in a pattern similar to the naming
structure. This configuration may restrict the architecture of the system
um1.ecessarily and conflict with other considerations. A server in charge of
a root directory is an example of a structure that is dictated by the naming
hierarchy and contradicts decentralization guidelines.
Once the separation of name and location has been completed, clients
can access files residing on remote server systems. In fact, these clients may
be and rely on servers to provide all files, including the operatingsystem
kernel. Special protocols are needed for the boot sequence, however.
Consider the problem of getting the kernel to a diskless workstation. The
diskless workstation has no kernel, so it cam1.ot use the DFS code to retrieve
the kernel. Instead, a special boot protocol, stored in read-only memory (ROM)
on the client, is invoked. It enables networking and retrieves only one special
file (the kernel or boot code) from a fixed location. Once the kernel is copied
over the network and loaded, its DFS makes all the other operating-system files
available. The advantages of diskless clients are many, including lower cost
(because the client machines require no disks) and greater convenience (when
an operating-system upgrade occurs, only the server needs to be modified).
The disadvantages are the added complexity of the boot protocols and the
performance loss resulting from the use of a network rather than a local disk.
The current trend is for clients to use both local disks and remote file servers.
Operating systems and networking software are stored locally; file systems
containing user data-and possibly applications-are stored on remote file
systems. Some client systems may store commonly used applications, such as
word processors and Web browsers, on the local file system as well. Other, less
commonly used applications may be from the remote file server to the
client on demand. The main reason for providing clients with local file systems
rather than pure diskless systems is that disk drives are rapidly increasing in
capacity and decreasing in cost, with new generations appearing every year
or so. The same cannot be said for networks, which evolve every few years.
17.2 709
Overall, systems are growing more quickly than are networks, so extra work
is needed to limit network access to improve system throughput.
17.2.2 Naming Schemes
There are three main approaches to naming schemes in a DFS. In the simplest
approach, a file is identified by some combination of its host name and local
name, which guarantees a unique system-wide name. In Ibis, for instance,
a file is identified uniquely by the name host: local-name, where Local-name is a
UNIX-like path. This naming scheme is neither location transparent nor location
independent. Nevertheless, the same file operations can be used for both local
and remote files. The DFS is structured as a collection of isolated component
units, each of which is an entire conventional file system. In this first approach,
component 1-mits remain isolated, although means are provided to refer to a
remote file. We do not consider this scheme any further in this text.
The second approach was popularized by Sun's network file system, NFS.
NFS is the file-system component of ONC+, a networking package supported
by many UNIX vendors. NFS provides a means to attach remote directories
to local directories, thus giving the appearance of a coherent directory tree.
Early NFS versions allowed only previously mmmted remote directories to
be accessed transparently. With the advent of the feature, mounts
are done on demand, based on a table of mount points and file-structure
names. Components are integrated to support transparent sharing, although
this integration is limited and is not uniform, because each machine may attach
different remote directories to its tree. The resulting structure is versatile.
We can achieve total integration of the component file systems by using the
third approach. Here, a single global name structure spans all the files in the
system. Ideally, the composed file-system structure is the same as the structure
of a conventional file system. In practice, however, the many special files (for
example, UNIX device files and machine-specific binary directories) make this
goal difficult to attain.
To evaluate naming structures, we look at their
The most complex and most difficult-to-maintain structure is the NFS structure.
Because any rem.ote directory can be attached anywhere onto the local directory
tree, the resulting hierarchy can be highly m1structured. If a server becomes
unavailable, some arbitrary set of directories on different machines becomes
unavailable. In addition, a separate accreditation mechanism controls which
machine is allowed to attach which directory to its tree. Thus, a user might
be able to access a remote directory tree on one client but be denied access on
another client.
17.2.3 Implementation Techniques
Implementation of transparent naming requires a provision for the mapping
of a file naine to the associated location. To keep this mapping manageable,
we must aggregate sets of files into component units and provide the mapping
on a component-unit basis rather than on a single-file basis. This aggregation
serves administrative purposes as well. UNIX-like systems use the hierarchical
directory tree to provide name-to-location mapping and to aggregate files
recursively into directories.
710 Chapter 17
17.3
To enhance the availability of the crucial mapping information, we can use
replication, local caching, or both. As we noted, location independence means
that the mapping changes over time; hence, replicating the mapping makes
a simple yet consistent update of this information impossible. A teclllcique
to overcome this obstacle is to introduce low-level me
Textual file names are mapped to lower-level file identifiers that
indicate to which component unit the file belongs. These identifiers are still
location independent. They can be replicated and cached freely without being
invalidated by migration of component units. The inevitable price is the need
for a second level of mapping, which maps component units to locations and
needs a simple yet consistent update mechanism. Implementing UNIX-like
directory trees using these low-levet location-independent identifiers makes
the whole hierarchy invariant under component-unit migration. The only
aspect that does change is the component-unit location mapping.
A common way to implement low-level identifiers is to use structured
names. These names are bit strings that usually have two parts. The first
part identifies the component unit to which the file belongs; the second part
identifies the particular file within the unit. Variants with more parts are
possible. The invariant of structured names, however, is that individual parts
of the name are unique at all times only within the context of the rest of the
parts. We can obtain uniqueness at all times by taking care not to reuse a name
that is still in use, by adding sufficiently more bits (this method is used in AFS),
or by using a timestamp as one part of the name (as done in Apollo Domain).
Another way to view this process is that we are taking a location-transparent
system, such as Ibis, and adding another level of abstraction to produce a
location-independent naming scheme.
Aggregating files into component units and using lower-level locationindependent
file identifiers are techniques exemplified in AFS.
Consider a user who requests access to a remote file. The server storing the file
has been located by the nanling scheme, and now the actual data transfer must
take place.
One way to achieve this transfer is through a
whereby requests for accesses are delivered to the server, the server machine
performs the accesses, and their results are forwarded back to the user. One
of the most common ways of implementing remote service is the remote
procedure call (RPC) paradigm, which we discussed in Chapter 3. A direct
analogy exists between disk-access methods in conventional file systems and
the remote-service method in a DFS: using the remote-service method is
analogous to performing a disk access for each access request.
To ensure reasonable performance of a remote-service mechanism, we can
use a form of caching. In conventional file systems, the rationale for caching is
to reduce disk I/0 (thereby increasing performance), whereas in DFSs, the goal
is to reduce both network traffic and disk I/0. In the following discussion, we
describe the implementation of caching in a DFS and contrast it with the basic
remote-service paradigm.
17.3 711
17.3.1 Basic Caching Scheme
The concept of caching is simple. If the data needed to satisfy the access request
are not already cached, then a copy of those data is brought from the server to
the client system. Accesses are performed on the cached copy. The idea is to
retain recently accessed disk blocks in the cache, so that repeated accesses to the
same information can be handled locally, without additional network traffic.
A replacement policy (for example, the least-recently-used algorithm) keeps

the cache size bounded. No direct correspondence exists between accesses and
traffic to the server. Files are still identified with one master copy residing at the
server machine, but copies (or parts) of the file are scattered in different caches.
When a cached copy is modified, the changes need to be reflected on the master
copy to preserve the relevant consistency semantics. The problem of keeping
the cached copies consistent with the master file is the
which we discuss in Section 17.3.4. DFS caching could just as easily
be called , it acts sincilarly to demand-paged virtual
memory, except that the backing store usually is not a local disk but rather a
remote server. NFS allows the swap space to be mounted remotely, so it actually
can implement virtual memory over a network, notwithstanding the resulting
performance penalty.
The granularity of the cached data in a DFS can vary from blocks of a file to
an entire file. Usually, more data are cached than are needed to satisfy a single
access, so that many accesses can be served by the cached data. This procedure
is much like diskread-ahead (Section 11.6.2). AFS caches files in large chunks (64
KB). The other systems discussed in this chapter support caching of individual
blocks driven by client demand. Increasing the caching unit increases the hit
ratio, but it also increases the miss penalty, because each miss requires more
data to be transferred. It increases the potential for consistency problems as
well. Selecting the unit of caching involves considering parameters such as the
network transfer unit and the RPC protocol service unit (if an RPC protocol is
used). The network transfer unit (for Ethernet, a packet) is about 1.5 KB, so larger
units of cached data need to be disassembled for delivery and reassembled on
reception.
Block size and total cache size are obviously of importance for blockcaching
schemes. In UNIX-like systems, common block sizes are 4 KB and 8
KB. For large caches (over 1MB), large block sizes (over 8 KB) are beneficial. For
smaller caches, large block sizes are less beneficial because they result in fewer
blocks in the cache and a lower hit ratio.
17.3.2 Cache Location
Where should the cached data be stored-on disk or in main memory  Disk
caches have one clear advantage over main-memory caches: they are reliable.
Modifications to cached data are lost in a crash if the cache is kept in volatile
memory. Moreove1~ if the cached data are kept on disk, they are still there during
recovery, and there is no need to fetch them again. Main-memory caches have
several advantages of their own, however:
Main-memory caches permit workstations to be diskless.
Data can be accessed more quickly from a cache in main memory than
from one on a disk.
712 Chapter 17
Technology is moving toward larger and less expensive memory. The
resulting performance speedup is predicted to outweigh the advantages
of disk caches.
The server caches (used to speed up disk I/0) will be in main memory
regardless of where user caches are located; if we use main-memory caches
on the user machine, too, we can build a single caching nl.echanism for use
by both servers and users.
Many remote-access implementations can be thought of as hybrids of
caching and remote service. In NFS, for instance, the implementation is based on
remote service but is augmented with client- and server-side memory caching
for performance. Similarly Sprite's implementation is based on caching; but
under certain circumstances, a remote-service method is adopted. Thus, to
evaluate the two methods, we must evaluate the degree to which either method
is emphasized.
The NFS protocol and most implementations do not provide disk caching.
Recent Solaris implementations ofNFS (Solaris 2.6 and beyond) include a clientside
disk-caching option, the - file system. Once the NFS client reads
blocks of a file from the serve1~ it caches them in memory as well as on disk.
If the memory copy is flushed, or even if the system reboots, the disk cache
is referenced. If a needed block is neither in memory nor in the cachefs disk
cache, an RPC is sent to the server to retrieve the block, and the block is written
into the disk cache as well as stored in the memory cache for client use.
17.3.3 Cache-Update Policy
The policy used to write modified data blocks back to the server's master copy
has a critical effect on the performance and reliability. The simplest
policy is to write data to disk as soon as they are placed in any cache.
The advantage of a is reliability: little information is
lost when a client system crashes. However, this policy requires each write
access to wait until the information is sent to the server, so it causes poor write
performance. Caching with write-through is equivalent to using remote service
for write accesses and exploiting caching for read accesses.
An alternative is the also known as
where we delay updates to the master copy. Modifications are written
to the cache and then are written through to the server at a later time. This
policy has two advantages over write-through. First, because writes are made
to the cache, write accesses complete much more quickly. Second, data may
be overwritten before they are written back, in which case only the last update
needs to be written at all. Unfortunately, delayed-write schemes introduce
reliability problems, since unwritten data are lost whenever a user machine
crashes.
Variations of the delayed-write policy differ in when modified data blocks
are flushed to the server. One alternative is to flush a block when it is about to
be ejected from the client's cache. This option can result in good performance,
but some blocks can reside in the client's cache a long time before they are
written back to the server. A compromise between this alternative and the
write-through policy is to scan the cache at regular intervals and to flush
blocks that have been modified since the most recent scan, just as UNIX scans
17.3 713
NFS server
network workstation
Figure 17.1 Cachefs and its use of caching.
its local cache. Sprite uses this policy with a 30-second interval. NFS uses the
policy for file data, but once a write is issued to the server durilcg a cache
flush, the write must reach the server's disk before it is considered complete.
NFS treats meta data (directory data and file-attribute data) differently. Any
metadata changes are issued synchronously to the server. Thus, file-structure
loss and directory-structure corruption are avoided when a client or the server
crashes.
For NFS with cachefs, writes are also written to the local disk cache area
when they are written to the server, to keep all copies consistent. Thus, NFS
with cachefs improves performance over standard NFS on a read request with
a cachefs cache hit but decreases performance for read or write requests with
a cache miss. As with all caches, it is vital to have a high cache hit rate to
gain performance. Figure 17.1 shows how cachefs uses write-through and
write-back caching.
Yet another variation on delayed write is to write data back to the server
when the file is closed. This is used in AFS. In the case
of files that are open for short periods or are modified rarely, this policy
does not significantly reduce network traffic. In addition, the write-on-close
policy requires the closing process to delay while the file is written through,
which reduces the performance advantages of delayed writes. For files that are
open for long periods and are modified frequently, however, the performance
advantages of this policy over delayed write with more frequent flushing are
apparent.
17.3.4 Consistency
A client machine is faced with the problem of deciding whether a locally cached
copy of the data is consistent with the master copy (and hence can be used). If
714 Chapter 17
the client machine determines that its cached data are out of date, accesses can
no longer be served by those cached data. An up-to-date copy of the data needs
to be cached. There are two approaches to verifying the validity of cached data:
Client-initiated approach. The client initiates a validity check, in which it
contacts the server and checks whether the local data are consistent with
the master copy. The frequency of the validity checking is the crux of
this approach and determines the resulting consistency semantics. It can
range from a check before every access to a check only on first access to
a file (on file open, basically). Every access coupled with a validity check
is delayed, compared with an access served immediately by the cache.
Alternatively, checks can be initiated at fixed time intervals. Depending
on its frequency, the validity check can load both the network and the
server.
Server-initiated approach. The server records, for each client, the files
(or parts of files) that it caches. When the server detects a potential
inconsistency, it must react. A potential for inconsistency occurs when
two different clients in conflicting modes cache a file. If UNIX semantics
(Section 10.5.3) is implemented, we can resolve the potential inconsistency
by having the server play an active role. The server must be notified
whenever a file is opened, and the intended mode (read or write) must
be indicated for every open. The server can then act when it detects that
a file has been opened simultaneously in conflicting modes by disabling
caching for that particular file. Actually, disabling caching results in
switching to a remote-service mode of operation.
17.3.5 A Comparison of Caching and Remote Service
Essentially, the choice between caching and remote service trades off potentially
increased performance with decreased simplicity. We evaluate this
tradeoff by listing the advantages and disadvantages of the two methods:
When caching is used, the local cache can handle a substantial number
of the remote accesses efficiently. Capitalizing on locality in file-access
patterns makes caching even more attractive. Thus, most of the remote
accesses will be served as fast as will local ones. Moreover, servers are
contacted only occasionally, rather than for each access. Consequently,
server load and network traffic are reduced, and the potential for scalability
is enhanced. By contrast, when the remote-service method is used, every
remote access is handled across the network The penalty in network traffic,
server load, and performance is obvious.
Total network overhead is lower for transmitting big chunks of data (as
is done in caching) than for transmitting series of responses to specific
requests (as in the remote-service method). Furthermore, disk-access
routines on the server may be better optimized if it is known that requests
will always be for large, contiguous segments of data rather than for
random disk blocks.
The cache-consistency problem is the major drawback of caching. When
access patterns exhibit infrequent writes, caching is superior. However,
17.4
17.4 715
when writes are frequent, the mechanisms employed to overcome the
consistency problem incur substantial overhead in terms of performance,
network traffic, and server load.
So that caching will confer a benefit, execution should be carried out on
machines that have either local disks or large main memories. Remote
access on diskless, small-memory-capacity machines should be done
through the remote-service method.
In caching, since data are transferred en masse between the server and the
client, rather than in response to the specific needs of a file operation, the
lower-level intermachine interface is different from the upper-level user
interface. The remote-service paradigm, in contrast, is just an extension of
the local file-system interface across the network. Thus, the intermachine
interface mirrors the user interface.
There are two approaches for storing server-side information when a client
accesses remote files: either the server tracks each file being accessed by each
client, or it simply provides blocks as they are requested by the client without
knowledge of how those blocks are used. In the former case, the service
provided is stateful; in the latter case, it is stateless.
The typical scenario involving a is as follows: A client
must perform an open () operation on a file before accessing that file. The server
fetches information about the file from its disk, stores it in its memory, and gives
the client a connection identifier that is unique to the client and the open file.
(In UNIX terms, the server fetches the inode and gives the client a file descriptor,
which serves as an index to an in-core table of inodes.) This identifier is used for
subsequent accesses ru1.til the session ends. A stateful service is characterized
as a connection between the client and the server during a session. Either on
closing the file or through a garbage-collection mechanism, the server must
reclaim the main-memory space used by clients that are no longer active. The
key point regarding fault tolerance in a stateful service approach is that the
server keeps main-memory information about its clients. AFS is a stateful file
service.
A avoids state information by making each request
self-contained. That is, each request identifies the file and the position in the
file (for read and write accesses) in full. The server does not need to keep a
table of open files in main memory, although it usually does so for efficiency
reasons. Moreover, there is no need to establish and terminate a com1.ection
through open() and close() operations. They are totally redundant since
each file operation stands on its own and is not considered part of a session. A
client process would open a file, and that open would not result in the sending
of a remote message. Reads and writes would take place as remote messages
(or cache lookups). The final close by the client would again result in only a
local operation. NFS is a stateless file service.
The advantage of a stateful over a stateless service is increased performance.
File information is cached in main memory and can be accessed easily
via the connection identifier, thereby saving disk accesses. In addition, a stateful
716 Chapter 17
5
server knows whether a file is open for sequential access and can therefore
read ahead the next blocks. Stateless servers cmmot do so, since they have no
knowledge of the purpose of the client's requests.
The distinction between stateful and stateless service becomes more
evident when we consider the effects of a crash that occurs during a service
activity. A stateful server loses all its volatile state in a crash. Ensuring the
graceful recovery of such a server involves restoring this state, usually by a
recovery protocol based on a dialog with clients. Less graceful recovery requires
that the operations that were underway when the crash occurred be aborted.
A different problem is caused by client failures. The server needs to become
aware of such failures so that it can reclaim space allocated to record the state of
crashed client processes. This phenomenon is sometimes referred to as
A stateless computer server avoids these problems, silcce a newly reincarnated
server can respond to a self-contained request without any difficulty.
Therefore, the effects of server failures and recovery are almost unnoticeable.
There is no difference between a slow server and a recovering server from a
client's point of view. The client keeps retransmitting its request if it receives
no response.
The penalty for using the robust stateless service is longer request messages
and slower processing of requests, since there is no in-core i,_'lformation to speed
the processing. In addition, stateless service imposes additional constraints
on the design of the DFS. First, since each request identifies the target file, a
uniform, system-wide, low-level naming scheme should be used. Translating
remote to local names for each request would cause even slower processing
of the requests. Second, since clients retransmit requests for file operations,
these operations must be idempotent; that is, each operation must have the
same effect and return the same output if executed several times consecutively.
Self-contained read and write accesses are idempotent, as long as they use an
absolute byte count to indicate the position withilc the file they access and do
not rely on an incremental offset (as is done in UNIX read() and write()
system calls). However, we must be careful when implementing destructive
operations (such as deleting a file) to make them idempotent, too.
In some environments, a stateful service is a necessity. If the server employs
the server-initiated method for cache validation, it camcot provide stateless
service, since it maintains a record of which files are cached by which clients.
The way UNIX uses file descriptors and implicit offsets is inherently stateful.
Servers must mailctain tables to map the file descriptors to inodes and must
store the current offset within a file. This requirement is why NFS, which
employs a stateless service, does not use file descriptors and does include
an explicit offset in every access.
Replication of files on different machines in a distributed file system is a useful
redundancy for improving availability. Multimachine replication can benefit
performance too: selecting a nearby replica to serve an access request results
in shorter service time.
The basic requirement of a replication scheme is that different replicas of
the same file reside on failure-independent machines. That is, the availability
17.5 717
NFSV4
OurcoV,era.geofNFS thus .far has. 0p1y considered Version 3 (orV3)NF  .. The
mostrecentNPS standard is Version 4 (V 4), and it differs fundanrentalJy from
pr~vious versimi.s. Jhe most significant ch9nge is that the protocol is now
stqteful,meaping tha:tthesetv~er maintains the.state ofthe client session from
thE: time the r~J1lote file is 0pt::ned untiJ itis closed. Th.t1s, theNFS protocol now
provides open() ar;td c1o$e () operations; previous V:ersions of NFS (V\Thich
are stateless) .proyide .np such operations. Furthen))ore, preytous \Cersions
specify s~parate protocols.  or J :lounting remote fil~ systews and for lockii1g  
remote  files .. V4 provides ali of.. these features under   l phlgle prqtocol. In
patticular; the 1nrnmt. protocol was elimin  ;1.ted, allowing 1\[FS to work with
netWork fitewalls, The nJ.ount protocol was a notorimis security hole in NPS
implem,entations~ .   ..       . . . . .. .  
  .Additionally,  V4 has. enhan edthe ability of.dients Jo cache. file  data
local)y: This. featurE; i~prov~s tne performapc~ of the disttil:mt(3d file system,
a.s plients are   able to reso~ve more file a9cesses from the loc~l c~che rgther
thanh.  ;lVingto gpJhroughthe S~!:Ver. '(:4 ali()WS diel):tSJO req)c!estfile Jocks
from s~rvers as we'll. .If the, senr~r grqrct~. the request,. the client maintains
the loci   ,tmtil it. is released or its lea.st= expires.   (Clier~ts. ar,e als() permitted to
r~new. ex~stil'tg least=s,.) Traditiora11y1 UNIX~ba~~d:systems .provide advisory
Jile locking, whereas Windows operatirg systen1~ use mandat()rylockil1g  To
  alloV\T l'\[PS towm)   wellwithnon-UNIXsyste1fts, V4t1QW.p.rovides mandatory
locking as vvell. The new lockinga11d caching mec~anisms are based. on the
concept ()f d~legaHon, whe~ eby the server delegates responsibilities for a
 file's lockand contents. to .the client thatrequested tnel()ckcrhat delegated
client maintains  in cache. the .current version of .the file., and .. other  clients  can
   ... ask that. deleg21ted client for lock access, a1i.d filt= confentsuntifthe del:egated
client reli11quishesth~lock andde~egation. .   .  . .  . . . ..  . .. . ...
 . Finally, whereas.preyiousversionB ()f NPSarebasedon the UDJ network
ptqtocol, \[4 is based on .TCP,whioh allows itto betteraclj\lstto varying traffic
loads on. thel}etwork: peleg2lting these responsibilities. to cliel!cts reduces the
 foado11the s.eryet and.i~proves.cache.coherency.
of one replica is not affected by the availability of the rest of the replicas.
This obvious requirement implies that replication management is inherently
a location-opaque activity. Provisions for placing a replica on a particular
machine must be available.
It is desirable to hide the details of replication from users. Mapping a
replicated file name to a particular replica is the task of the naming scheme.
The existence of replicas should be invisible to higher levels. At lower
levels, however, the replicas must be distinguished from one another by
different lower-level names. Another transparency requirement is providing
replication control at higher levels. Replication control includes determination
of the degree of replication and of the placement of replicas. Under certain
circumstances, we may want to expose these details to users. Locus, for
instance, provides users and system administrators with mechanisms to control
the replication scheme.
718 Chapter 17
17.6
The main problem~ associated with replicas is updating. From a user's
point of view, replicas of a file denote the same logical entity, and thus an
update to any replica must be reflected on all other replicas. More precisely,
the relevant consistency sen1antics must be preserved when accesses to replicas
are viewed as virtual accesses to the replicas' logical files. If consistency is not
of primary incportance, it can be sacrificed for availability and performance. In
this fundamental tradeoff in the area of fault tolerance, the choice is between
preserving consistency at all costs, thereby creating a potential for indefinite
blocking, and sacrificing consistency under some (we hope, rare) circumstances
for the sake of guaranteed progress. Locus, for example, employs replication
extensively and sacrifices consistency in the case of network partition for the
sake of availability of files for read and write accesses.
Ibis uses a variation of the primary-copy approach. The domain of the
name mapping is a pair   primary-replica-identifier, local-replica-identifier  . If no
local replica exists, a special value is used. Thus, the mapping is relative to a
machine. If the local replica is the primary one, the pair contains two identical
identifiers. Ibis supports demand replication, an automatic replication-control
policy similar to whole-file caching. Under demand replication, reading of
a nonlocal replica causes it to be cached locally, thereby generating a new
nonprimary replica. Updates are performed only on the primary copy and
cause all other replicas to be invalidated through the sending of appropriate
messages. Atomic and serialized invalidation of all nonprimary replicas is not
guaranteed. Hence, a stale replica may be considered valid. To satisfy remote
write accesses, we migrate the primary copy to the requesting machine.
Andrew is a distributed computing environment designed and implemented
at Carnegie Mellon University. The Andrew file system (AFS) constitutes the
underlying information-sharing mechanism among clients of the environment.
The Transarc Corporation took over development of AFS and then was purchased
by IBM. IBM has since produced several commercial implementations
of AFS. AFS was subsequently chosen as the DFS for an industry coalition; the
result was part of the distributed computing environment (DCE)
from the OSF organization.
In 2000, IBM's Transarc Lab announced that AFS would be an open-source
product (termed OpenAFS) available under the IBM public license, and Transarc
DFS was canceled as a commercial product. OpenAFS is available under most
commercial versions of UNIX as well as Linux and Microsoft Windows systems.
Many UNIX vendors, as well as Microsoft, support the DCE system and its DFS,
which is based on AFS, and work is ongoing to make DCE a cross-platform,
universally accepted DFS. As AFS and Transarc DFS are very similar~ we describe
AFS throughout this section, unless Transarc DFS is named specifically.
AFS seeks to solve many of the problems of the simpler DFSs, such as
NFS, and is arguably the most feature-rich nonexperimental DFS. It features
a uniform name space, location-independent file sharing, client-side caching
with cache consistency, and secure authentication via Kerberos. It also includes
server-side caching in the form of replicas, with high availability through
automatic switchover to a replica if the source server is unavailable. One of the
17.6 719
most formidable attributes of AFS is scalability: the Andrew system is targeted
to span over 5,000 workstations. Between AFS and Transarc DFS, there are
hundreds of implementations worldwide.
17.6.1 Overview
AFS distinguishes between client machines (sometimes referred to as workstations)
and dedicated server machines. Servers and clients originally ran only 4.2
BSD UNIX, but AFS has been ported to many operating systems. The clients and
servers are interconnected by a network of LANs or WANs.
Clients are presented with a partitioned space of file names: a
and a Dedicated servers, collectively called Vice
after the name of the software they run, present the shared name space to the
clients as a homogeneous, identical, and location-transparent file hierarchy.
The local name space is the root file system of a workstation, from which
the shared name space descends. Workstations run the Virtue protocol to
communicate with Vice, and each is required to have a local disk where it
stores its local name space. Servers collectively are responsible for the storage
and management of the shared name space. The local name space is small,
is distinct for each workstation, and contains system programs essential for
autonomous operation and better performance. Also local are temporary files
and files that the workstation owner, for privacy reasons, explicitly wants to
store locally.
Viewed at a finer granularity, clients and servers are structured in clusters
interconnected by a WAN. Each cluster consists of a collection of workstations
on a LAN and a representative of Vice called a and each cluster
is com1.ected to the WAN by a router. The decomposition into clusters is
done primarily to address the problem of scale. For optimal performance,
workstations should use the server on their own cluster most of the time,
thereby making cross-cluster file references relatively infrequent.
The file-system architecture is also based on considerations of scale. The
basic heuristic is to offload work from the servers to the clients, in light
of experience indicating that server CPU speed is the system's bottleneck
Following this heuristic, the key mechanism for remote file operations is to
cache files in large chunks (64 KB). This feature reduces file-open latency and
allows reads and writes to be directed to the cached copy without frequently
involving the servers.
Briefly, here are a few additional issues in the design of AFS:
Client mobility. Clients are able to access any file in the shared name
space from any workstation. A client may notice some initial performance
degradation due to the caching of files when accessil  g files a
workstation other than the usual one.
Security. The Vice interface is considered the boundary of trustworthiness,
because no client programs are executed on Vice machines. Authentication
and secure-transmission functions are provided as part of a connectionbased
communication package based on the RPC paradigm. After mutual
authentication, a Vice server and a client communicate via encrypted
messages. Encryption is performed by hardware devices or (more slowly)
720 Chapter 17
in software. Information about clients and groups is stored in a protection
database replicated at each server.
Protection. AFS provides for protecting directories and the
regular UNlXbits for file protection. The access list ncay contain information
about those  users allowed to access a directory, as well as information
about those users not allowed to access it. Thus, it is simple to specify that
everyone except, say, Jim can access a directory. AFS supports the access
types read, write, lookup, insert, administer, lock, and delete.
Heterogeneity. Defining a clear interface to Vice is a key for integration of
diverse workstation hardware and operating systems. So that heterogeneity
is facilitated, some files in the local /bin directory are symbolic links
pointing to machine-specific executable files residing in Vice.
17.6.2 The Shared Name Space
AFS's shared name space is made up of component units called The
volumes are unusually small component units. Typically, they are associated
with the files of a single client. Few volumes reside within a single disk
partition, and they may grow (up to a quota) and shrink in size. Conceptually,
volumes are glued together by a mechanism similar to the UNIX m01mt
mechanism. However, the granularity difference is significant, since in UNIX
only an entire disk partition (containing a file system) can be mounted. Volumes
are a key administrative unit and play a vital role in identifying and locating
an individual file.
A Vice file or directory is identified by a low-level identifier called a fid.
Each AFS directory entry maps a path-name component to a fid. A fid is 96 bits
long and has three equal-length components: a volume number, a vnode number,
and a uniquifier. The vnode number is used as an index into an array containing
the inodes of files in a single volume. The allows reuse of vnode
numbers, thereby keeping certain data structures compact. Fids are location
transparent; therefore, file movements from server to server do not invalidate
cached directory contents.
Location information is kept on a volume basis in a
replicated on each server. A client can identify the location of every
volume in the system by querying this database. The aggregation of files into
volumes makes it possible to keep the location database at a manageable size.
To balance the available disk space and utilization of servers, volumes
need to be migrated among disk partitions and servers. When a volume is
shipped to its new location, its original server is left with temporary forwarding
information, so that the location database need not be updated synchronously.
While the volume is being transferred, the original server can still handle
updates, which are shipped later to the new server. At some point, the volume
is briefly disabled so that the recent modifications can be processed; then, the
new volume becomes available again at the new site. The volume-movement
operation is atomic; if either server crashes, the operation is aborted.
Read-only replication at the granularity of an entire volume is supported
for system-executable files and for seldom-updated files in the upper levels
of the Vice name space. The volume-location database specifies the server
17.6 721
contammg the only read-write copy of a volume and a list of read-only
replication sites.
17.6.3 File Operations and Consistency Semantics
The fundamental architectural principle in AFS is the caching of entire files
from servers. Accordingly, a client workstation interacts with Vice servers
only during opening and closing of files, and even this interaction is not
always necessary. Reading and writing files do not cause remote interaction (in
contrast to the remote-service n  lethod). This key distinction has far-reaching
ramifications for performance, as well as for semantics of file operations.
The operating system on each workstation intercepts file-system calls and
forwards them to a client-level process on that workstation. This process, called
Venus, caches files from Vice when they are opened and stores modified copies
of files back on the servers from which they came when they are closed. Venus
may contact Vice only when a file is opened or closed; reading and writing of
individual bytes of a file are performed directly on the cached copy and bypass
Venus. As a result, writes at some sites are not visible immediately at other
sites.
Caching is further exploited for future opens of the cached file. Venus
assumes that cached entries (files or directories) are valid unless notified
otherwise. Therefore, Venus does not need to contact Vice on a file open to
validate the cached copy. The mechanism to support this policy, called callback,
dramatically reduces the number of cache-validation requests received by
servers. It works as follows. When a client caches a file or a directory, the
server updates its state information to record this caching. We say that the
client has a callback on that file. The server notifies the client before allowing
another client to modify the file. In such a case, we say that the server removes
the callback on the file for the former client. A client can use a cached file for
open purposes only when the file has a callback If a client closes a file after
modifying it, all other clients caching this file lose their callbacks. Therefore,
when these clients open the file later, they have to get the new version from
the server.
Readin.g and writing bytes of a file are done directly by the kernel without
Venus's intervention on the cached copy. Venus regains control when the file is
closed. If the file has been modified locally, it updates the file on the appropriate
server. Thus, the only occasions on which Venus contacts Vice servers are on
opens of files that either are not in the cache or have had their callback revoked
and on closes of locally modified files.
Basically, AFS implements session semantics. The only exceptions are
file operations other than the primitive read and write (such as protection
changes at the directory level), which are visible everywhere on the network
immediately after the operation completes.
In spite of the callback mechanism, a small amount of cached validation
traffic is still present, usually to replace callbacks lost because of machine or
network failures. When a workstation is rebooted, Venus considers all cached
files and directories suspect, and it generates a cache-validation request for the
first use of each such entry.
The callback mechanism forces each server to maintain callback information
and each client to maintain validity information. If the amount of callback
722 Chapter 17
information maintained by a server is excessive, the server can break callbacks
and reclaim some storage by unilaterally notifying clients and revoking the
validity of their cached files. If the callback state maintained by Venus gets
out of sync with the corresponding state maintained by the servers, some
inconsistency may result.
Venus also caches contents of directories and syncbolic links, for pathname
translation. Each component in the path name is fetched, and a callback
is established for it if it is not already cached or if the client does not have
a callback on it. Venus does lookups on the fetched directories locally, using
fids. No requests are forwarded from one server to another. At the end of a
path-name traversal, all the intermediate directories and the target file are in
the cache with callbacks on them. Future open calls to this file will involve no
network communication at all, unless a callback is broken on a component of
the path name.
The only exception to the caching policy is a modification to a directory
that is made directly on the server responsible for that directory for reasons
of integrity. The Vice interface has well-defined operations for such purposes.
Venus reflects the changes in its cached copy to avoid re-fetching the directory.
17.6.4 Implementation
Client processes are interfaced to a UNIX kernel with the usual set of system
calls. The kernel is modified slightly to detect references to Vice files in the
relevant operations and to forward the requests to the client-level Venus process
at the workstation.
Venus carries out path-name translation component by component, as
described above. It has a mapping cache that associates volumes to server
locations in order to avoid server interrogation for an already known volume
location. If a volume is not present in this cache, Venus contacts any server
to which it already has a comcection, requests the location information, and
enters that information into the mapping cache. Unless Venus already has
a connection to the server, it establishes a new comcection. It then uses this
connection to fetch the file or directory. Connection establishment is needed for
authentication and security purposes. When a target file is found and cached, a
copy is created on the local disk. Venus then returns to the kernel, which opens
the cached copy and returns its handle to the client process.
The UNIX file system is used as a low-level storage system for both AFS
servers and clients. The client cache is a local directory on the workstation's
disk. Within this directory are files whose names are placeholders for cache
entries. Both Venus and server processes access UNIX files directly by the latter's
inodes to avoid the expensive path-name-to-inode translation routine (namei).
Because the internal inode interface is not visible to client-level processes (both
Venus and server processes are client-level processes), an appropriate set of
additional system calls was added. DFS uses its own journaling file system to
improve performance and reliability over UFS.
Venus manages two separate caches: one for status and the other for data.
It uses a simple least-recently-used (LRU) algorithm to keep each of them
bounded in size. When a file is flushed from the cache, Venus notifies the
appropriate server to remove the callback for this file. The status cache is kept
in virtual memory to allow rapid servicing of stat () (file-status-returning)
17.7
17.7 723
system calls. The data cache is resident on the local disk, but the UNIX I/0
buffering mechanism does some caching of disk blocks in memory that is
transparent to Venus.
A single client-level process on each file server services all file req1.1ests from
clients. This process uses a lightweight-process package with non-preemptible
scheduling to service many client requests concurrently. The RPC package
is integrated with the lightweight-process package, thereby allowing the file
server to concurrently make or service one RPC per lightweight process. The
RPC package is built on top of a low-level datagram abstraction. Whole-file
transfer is implemented as a side effect of the RPC calls. One RPC connection
exists per client, but there is no a priori binding of lightweight processes to these
connections. Instead, a pool of lightweight processes services client requests
on all connections. The use of a single multithreaded server process allows the
caching of data structures needed to service requests. On the negative side,
a crash of a single server process has the disastrous effect of paralyzing this
particular server.
A DFS is a file-service system whose clients, servers, and storage devices are
dispersed among the sites of a distributed system. Accordingly, service activity
has to be carried out across the network; instead of a single centralized data
repository, there are multiple independent storage devices.
Ideally, a DFS should look to its clients like a conventional, centralized
file system. The multiplicity and dispersion of its servers and storage devices
should be made transparent. That is, the client interface of a DFS should not
distinguish between local and remote files. It is up to the DFS to locate the
files and to arrange for the transport of the data. A transparent DFS facilitates
client mobility by bringing the client's environment to the site where the client
logs in.
There are several approaches to naming schemes in a DFS. In the simplest
approach, files are named by some combination of their host name and local
name, which guarantees a unique system-wide name. Another approach,
popularized by NFS, provides a means to attach remote directories to local
directories, thus giving the appearance of a coherent directory tree.
Requests to access a remote file are usually handled by two complementary
methods. With remote service, requests for accesses are delivered to the server.
The server machine performs the accesses, and their results are forwarded
back to the client. With caching, if the data needed to satisfy the access request
are not already cached, then a copy of the data is brought from the server
to the client. Accesses are performed on the cached copy. The idea is to
retain recently accessed disk blocks in the cache, so that repeated accesses
to the same information can be handled locally, without additional network
traffic. A replacement policy is used to keep the cache size bounded. The
problem of keeping the cached copies consistent with the master file is the
cache-consistency problem.
There are two approaches to server-side information. Either the server
tracks each file the client accesses, or it simply provides blocks as the client
requests them without knowledge of their use. These approaches are the
stateful versus stateless service paradigms.
724 Chapter 17
Replication of files on different machines is a useful redundancy for
improving availability. Multimachine replication can benefit performance, too,
since selecting a nearby replica to serve an access request results in shorter
service time.
AFS is a feature-rich DFS characterized by location independence and location
transparency. It also imposes significant consistency semantics. Caching
and replication are used to improve performance.
17.1 Discuss whether AFS and NFS provide the following: (a) location
transparency and (b) location independence.
17.2 Discuss whether clients in the following systems can obtain inconsistent
or stale data from the file server and, if so, under what scenarios this
could occur.
a. AFS
b. Sprite
c. NFS
17.3 Consider AFS, which is a stateful distributed file system. What actions
need to be performed to recover from a server crash in order to preserve
the consistency guaranteed by the system 
17.4 Discuss the advantages and disadvantages of path-name translation
in which the client ships the entire path to the server requesting a
translation for the entire path name of the file.
17.5 Under what circumstances would a client prefer a locationtransparent
DFS  Under what circumstances would she prefer a
location-independent DFS  Discuss the reasons for these preferences.
17.6 V'lhich of the example DFSs discussed in this chapter would handle a
large, multiclient database application most efficiently  Explain your
answer.
17.7 What are the benefits of mapping objects into virtual memory, as Apollo
Domain does  What are the drawbacks 
17.8 Compare and contrast the teclmiques of caching disk blocks locally, on
a client system, and remotely, on a server.
17.9 What aspects of a distributed system would you select for a system
running on a totally reliable network 
725
Consistency and recovery control for replicated files are examined by Davcev
and Burkhard [1985]. Management of replicated files in a UNIX environncent is
covered by Brereton [1986] and Purdin et al. [1987]. Wah [1984] discusses the
issue of file placement on distributed computer systems. A detailed survey of
mainly centralized file servers appears in Svobodova [1984].
Sun's network file system (NFS) is described by Callaghan [2000] and
Sandberg et al. [1985]. The AFS system is discussed by Morris et al. [1986],
Howard et al. [1988], and Satyanarayanan [1990]. Information about OpenAFS
is available from http:/ /www.openafs.org
Many different and interesting DFSs are not covered in detail in this
text, including UNIX United, Sprite, and Locus. UNIX United is described by
Brownbridge et al. [1982]. The Locus system is discussed by Popek and Walker
[1985]. The Sprite system is described by Ousterhout et al. [1988] and Nelson
et al. [1988]. Distributed file systems for mobile storage devices are discussed in
Kistler and Satyanarayanan [1992] and Sobti et al. [2004]. Considerable research
has also been performed on cluster-based distributed file systems (Anderson
et al. [1995], Lee and Thekkath [1996], Thekkath et al. [1997], and Anderson
et al. [2000]). Distributed storage systems for large-scale, wide-area settings are
presented in Dabek et al. [2001] and Kubiatowicz et al. [2000].

18.1
In. Chapter 6, we described various mechanisms that allow processes to
synchronize their actions. We also discussed a number of schemes to ensure
the atomicity of a transaction that executes either in isolation or concurrently
with other transactions. In Chapter 7, we described various methods that an
operating system can use to deal with the deadlock problem. In this chapter,
we examine how centralized synchronization mechanisms can be extended to
a distributed environment. We also discuss methods for handling deadlocks in
a distributed system.
To describe various methods for achieving mutual exclusion in a distributed
system.
To explain how atomic transactions can be implemented in a distributed
system.
To show how some of the concurrency-control schemes discussed in
Chapter 6 can be modified for use in a distributed environment.
To present schemes for handling deadlock prevention, deadlock avoidance,
and deadlock detection in a distributed system.
In a centralized system, we can always determine the order in which two
events occurred, since the system has a single common memory and clock.
Many applications may require us to determine order. For example, in a
resource-allocation scheme, we specify that a resource can be used only aft-er
the resource has been granted. A distributed system, however, has no common
memory and no common clock. Therefore, it is sometimes impossible to say
which of two events occurred first. The happened-before relation, discussed
next, is only a partial ordering of the events in distributed systems. Since
727
728 Chapter 18
the ability to define a total ordering is crucial in many applications, we present
a distributed algorithm for extending the happened-before relation to a consistent
total ordering of all the events in the system.
18.1.1 The Happened-Before Relation
Since we are considering only sequential processes, all events executed in a
single process are totally ordered. Also, by the law of causality, a message can
be received only after it has been sent. Therefore, we can define the happenedbefore
relation (denoted by ---+) on a set of events as follows (assuming that
sending and receiving a message constitutes an event):
1. If A and Bare events in the same process, and A was executed before B,
then A ---+ B.
If A is the event of sending a message by one process and B is the event
of receiving that message by another process, then A---+ B.
3. If A---+ Band B---+ C, then A---+ C.
Since an event cannot happen before itself, the ---+ relation is an irreflexive
partial ordering.
If two events, A and B, are not related by the ---+ relation (that is, A did
not happen before B, and B did not happen before A), then we say that these
two events were executed In this case, neither event can causally
affect the other. If, however, A ---+ B, it is possible for event A to affect
event B causally.
A space-time diagram, such as that in Figure 18.1, can best illustrate the
definitions of concurrency and happened-before. The horizontal direction represents
space (that is, different processes), and the vertical direction represents
time. The labeled vertical lines denote processes (or processors). The labeled
dots denote events. A wavy line denotes a message sent from one process to
another. Events are concurrent if and only if no path exists between them.
For example, these are some of the events related by the happened-before
relation il   Figure 18.1:
P1 ---+ q2
ro ---+ q4
q3---+ r4
P1 ---+ q4 (sil  ce P1 ---+ q2 and q2 ---+ q4)
These are some of the concurrent events in the system:
qo and p2
ro and q3
ro and P3
q3 and P3
We cannot know which of two concurrent events, such as qo and p2, happened
first. However, since neither event can affect the other (there is no way for one
of them to know whether the other has occurred yet), it is not important which
18.1 729
p 0
Figure 18.1 Relative time for three concurrent processes.
happened first. It is important only that any processes that care about the order
of two concurrent events agree on son1.e order.
18.1.2 Implementation
To determine that an event A happened before an event B, we need either a
common clock or a set of perfectly synchronized clocks. Since neither of these
is available in a distributed system, we must defil  e the happened-before relation
without the use of physical clocks.
First we associate with each system event a We can then define
the requirement: for every pair of events A and B, if A--+ B,
then the timestamp of A is less than the timestamp of B. (Below, we will see
that the converse need not be true.)
How do we enforce the global ordering requirement in a distributed
environment  We define within each process Pi a logical LCi. The
logical clock can be implemented as a simple counter incremented between
any two successive events executed within a process. Since the logical clock
has a increasing value, it assigns a unique number to every
event, and if an event A occurs before event Bin process Pi, then LCi(A)   
LC(B). The timestamp for an event is the value of the logical clock for that
event. This scheme ensures that for any two events in the same process the
global orderil  g requirement is met.
Unfortunately, this scheme does not ensure that the global ordering
requirement is met across processes. To illustrate the problem, consider two
processes P1 and P2 that communicate with each other. Suppose that P1 sends
a message to P2 (event A) with LC1(A) = 200, and P2 receives the message
(event B) with LC2(B) = 195 (because the processor for P2 is slower than the
processor for P1, its logical clock ticks more slowly). This situation violates our
requirement, since A--+ B but the timestamp of A is greater than the timestamp
of B.
To resolve this difficulty, we require a process to advance its logical clock
when it receives a message whose timestamp is greater than the current value
of its logical clock. In particulm~ if process Pi receives a message (event B) with
timestamp t and LC(B):::; t, tlcenit should advance its clock so that LCi(B) = t +
1. Thus, in our example, when P2 receives the message from P1, it will advance
its logical clock so that LC2(B) = 201.
730 Chapter 18
18.2
Finally, to realize a total ordering, we need only observe that, with our
timestamp-ordering scheme, if the timestamps of two events, A and B, are the
same, then the events are concurrent. In this case, we may use process identity
numbers to break ties and to create a total ordering. The use of tirnestamps is
further discussed in Section 18.4.2.
In this section, we present a number of different algorithms for implementing
mutual exclusion in a distributed environment. We assume that the system
consists of n processes, each of which resides at a different processor. To simplify
our discussion, we assume that processes are numbered uniquely from 1 to n
and that a one-to-one mapping exists between processes and processors (that
is, each process has its own processor).
18.2.1 Centralized Approach
In a centralized approach to providing mutual exclusion, one of the processes
in the system is chosen to coordinate the entry to the critical section. Each
process that wants to invoke mutual exclusion sends a request message to the
coordinator. When the process receives a reply message from the coordinator, it
can enter its critical section. After exiting its critical section, the process sends
a release message to the coordinator and proceeds with its execution.
On receiving a request message, the coordinator checks to see whether some
other process is in its critical section. If no process is in its critical section, the
coordinator immediately sends back a reply message. Otherwise, the request
is queued. When the coordinator receives a release message, it removes one
of the request messages from the queue (in accordance with some scheduling
algorithm) and sends a reply message to the requesting process.
It should be clear that this algorithm ensures mutual exclusion. In addition,
if the scheduling policy within the coordinator is fair-such as first-come, firstserved
(FCFS) scheduling-no starvation can occur. This scheme requires three
messages per critical-section entry: a reques( a reply, and a release.
If the coordinator process fails, then a new process must take its place.
In Section 18.6, we describe some algorithms for electing a unique new
coordinator. Once a new coordinator has been elected, it must poll all the
processes in the system to reconstruct its request queue. Once the queue has
been constructed, the computation can resume.
18.2.2 Fully Distributed Approach
If we want to distribute the decision making across the entire system, then
the solution is far more complicated. One approach, described next, uses an
algorithm based on the event-ordering scheme described in Section 18.1.
When a process g wants to enter its critical section, it generates a new
timestamp, TS, and sends the message request(P;, TS) to all processes in the
system (including itself). On receiving a request message, a process may reply
immediately (that is, send a reply message back to P; ), or it may defer sending
a reply back (because it is already in its critical section, for example). A process
that has received a reply message from all other processes in the system can
18.2 731
enter its critical section, queueing incmning requests and deferring them. After
exiting its critical section, the process sends reply messages to all its deferred
requests.
The decision whether process Pi replies immediately to a request(Pj, TS)
message or defers its reply is based on three factors:
If process Pi is in its critical section, then it defers its reply to Pj.
If process Pi does not want to enter its critical section, then it sends a reply
immediately to P j.
If process P; wants to enter its critical section but has not yet entered
it, then it compares its own request timestamp with the timestamp of
the incoming request made by process Pj. If its own request timestamp
is greater than that of the incoming request, then it sends a reply
immediately to Pj (Pj asked first). Otherwise, the reply is deferred.
This algorithm exhibits the following desirable behavior:
Mutual exclusion is obtained.
Freedom from deadlock is ensured.
Freedom from starvation is ensured, since entry to the critical section is
scheduled according to the timestamp ordering. The timestamp ordering
ensures that processes are served in FCFS order.
The number of messages per critical-section entry is 2 x (n-1). This number
represents the minimum number of required messages per critical-section
entry when processes act independently and concurrently.
To illustrate how the algorithm functions, we consider a system consisting
of processes P1, P2, and P3 . Suppose that processes P1 and P3 want to enter
their critical sections. Process P1 then sends a message request (P1, timestamp
= 10) to processes P2 and P3, while process P3 sends a message request (P3,
timestamp= 4) to processes P1 and P2 . (The timestamps 4 and 10 were obtained
from the logical clocks described in Section 18.1.) When process P2 receives
these request messages, it replies immediately. When process P1 receives the
request from process P3, it replies immediately, since the timestamp (10) on its
own request message is greater than the timestamp (4) for process P3. When
process P3 receives the request message from process P1, it defers its reply,
since the timestamp (4) on its request message is less than the timestamp (10)
for the message from process P1 . On receiving replies from both process P1
and process P2, process P3 can enter its critical section. After exiting its critical
section, process P3 sends a reply to process P1, which can then enter its critical
section.
Because this scheme requires the participation of all the processes in the
system, it has three undesirable consequences:
The processes need to know the identity of all other processes in the
system. When a new process joins the group of processes participating in
the mutual-exclusion algorithm, the following actions need to be taken:
732 Chapter 18
a. The process must receive the names of all the other processes in the
gro11p.
b. The name of the new process must be distributed to all the other
processes in the group.
This task is not as trivial as it may seem, since some request and reply
messages may be circulating in the system when the new process joins
the group. The interested reader is referred to the bibliographical notes
at the end of the chapter.
If one process fails, then the entire scheme collapses. We can resolve this
difficulty by continuously monitoring the state of all processes in the
system. If one process fails, then all other processes are notified, so that
they will no longer send request messages to the failed process. When a
process recovers, it must initiate the procedure that allows it to rejoin the
group.
Processes that have not entered their critical section must pause frequently
to assure other processes that they intend to enter the critical
section.
Because of these difficulties, this protocol is best suited for small, stable sets of
cooperating processes.
18.2.3 Token-Passing Approach
Another method of providilcg mutual exclusion is to circulate a token among
the processes ilc the system. A is a special type of message that is passed
from process to process. Possession of the token entitles the holder to enter the
critical section. Since there is only a single token, only one process can be in its
critical section at a time.
We assume that the processes in the system are logically organized ilc a
The physical communication network need not be a ring. As long as
the processes are connected to one another, it is possible to implement a logical
ring. To implement mutual exclusion, we pass the token around the ring. When
a process receives the token, it may enter its critical section, keeping the token.
After the process exits its critical section, the token is passed around again.
If the process receiving the token does not want to enter its critical section,
it passes the token to its neighbor. This scheme is similar to algorithm 1 in
Chapter 6, but a token is substituted for a shared variable.
If the ring is unidirectional, freedom from starvation is ensured. The
number of messages required to implement mutual exclusion may vary from
one message per entry, in the case of high contention (that is, every process
wants to enter its critical section), to an infinite number of messages, in the case
of low contention (that is, no process wants to enter its critical section).
Two types of failure must be considered. First, if the token is lost an election
must be called to generate a new token. Second, if a process fails, a new logical
ring must be established. In Section 18.6, we present an election algorithm;
others are possible. The development of an algorithm for reconstructing the
ring is left to you in Exercise 18.4.
18.3
18.3 733
In Chapter 6, we introduced the concept of an atomic transaction, which is a
program unit that must be executed . That is, either all the operations
associated with it are executed to completion, or none are performed. When we
are dealing with a distributed system, ensuring the atomicity of a transaction
becomes much more complicated than in a centralized system. This difficulty
occurs because several sites may be participating in the execution of a single
transaction. The failure of one of these sites, or the failure of a communication
link connecting the sites, may result in erroneous computations.
Ensuring that the execution of transactions in the distributed system
preserves atomicity is the function of the Each site has
its own local transaction coordinator, which is responsible for coordinating the
execution of all the transactions initiated at that site. For each such transaction,
the coordinator is responsible for the following:
  Starting the execution of the transaction
Breaking the transaction into a number of subtransactions and distributing
these subtransactions to the appropriate sites for execution
Coordinating the termination of the transaction, which may result in the
transactions being committed at all sites or aborted at all sites
We assume that each local site maintains a log for recovery purposes.
18.3.1 The Two-Phase Commit Protocol
For atomicity to be ensured, all the sites in which a transaction T has executed
must agree on the final outcome of the execution. T must either commit at
all sites, or it must abort at all sites. To ensure this property, the transaction
coordinator of T must execute a Among the simplest and
most widely used commit protocols is the
which we discuss next.
Assume that T is a transaction initiated at site S; and that the transaction
coordinator at S; is C;. When T completes its execution-that is, when all the
sites at which T has executed inform C; that T has completed -then C starts
the 2PC protocol.
Phase 1. C adds the record   prepare T   to the log and forces the record
onto stable storage. It then sends a prepare (T) message to all the sites at
which T has executed. On receiving the message, the transaction manager
at each of these sites determines whether it is willing to commit its portion
ofT. If the answer is no, it adds a record   no T   to the log, and then it
responds by sending an abort (T) message to C;. If the answer is yes, it adds
a record   ready T   to the log and forces all the log records corresponding
to T onto stable storage. The transaction manager then replies with a ready
(T) message to C.
Phase 2. When C; has received responses to the prepare (T) message from
all the sites, or when a pre-specified interval of time has elapsed since the
prepare (T) message was sent out, C; can determine whether the transaction
734 Chapter 18
T can be committed or aborted. Transaction T can be committed if C; has
received a ready (T) m~essage from all the participating sites. Otherwise,
transaction T must be aborted. Depending on the verdict, either a record
  commit T   or a record   abort T   is added to the log and forced onto
stable storage. At this point, the fate of the transaction has been sealed.
Following this, the coordinator sends either a commit (T) or an abort (T)
message to all participating sites. When a site receives that message, it
records the message in the log.
A site at which T has executed can unconditionally abort Tat any time prior
to its sending the message ready (T) to the coordinator. The ready (T) message
is, in effect, a promise by a site to follow the coordinator's order to commit Tor
to abort T. A site can make such a promise only when the needed information
is stored in stable storage. Otherwise, if the site crashes after sending ready (T),
it may be unable to make good on its promise.
Since unanimity is required to commit a transaction, the fate of T is sealed
as soon as at least one site responds with abort (T). Note that the coordinator
site 5; can decide unilaterally to abort T, as it is one of the sites at which
T has executed. The final verdict regarding T is determined at the time the
coordinator writes that verdict (commit or abort) to the log and forces it to
stable storage.
In some implementations of the 2PC protocol, a site sends an acknowledge
(T) message to the coordinator at the end of the second phase of the protocol.
When the coordinator has received the acknowledge (T) message from all the
sites, it adds the record   complete T   to the log.
18.3.2 Failure Handling in 2PC
We now examine in detail how 2PC responds to various types of failures. As
we shall see, one major disadvantage of the 2PC protocol is that coordinator
failure may result in blocking, and a decision either to commit or to abort T
may have to be postponed until the coordinator recovers.
18.3.2.1 Failure of a Participating Site
When a participating site S~c recovers from a failure, it must examine its log
to determine the fate of those transactions that were in the midst of execution
when the failure occurred. Suppose that Tis one such transaction. How will S~c
deal with T  We consider each of the possible alternatives:
The log contain~s a   commit T   record. In this case, the site executes
redo(T).
The log contains an   abort T   record. In this case, the site executes
undo(T).
The log contains a   ready T   record. In this case, the site must consult
C; to determine the fate ofT. If C; is up, it tells S~c whether T committed
or aborted. In the former case, it executes redo(T); in the latter case, it
executes undo(T). If C; is down, S~c must try to find out the fate of T
from other sites. It does so by sending a query-status (T) message to all
the sites in the system. On receiving such a message, a site must consult
18.3 735
its log to determine whether T has executed there and, if so, whether T
committed or aborted. It then notifies s,, about this outcome. If no site has
the appropriate information (that is, whether T corrnni tted or aborted), then
S~c can neither abort nor commit T. The decision concerning Tis postponed
until S1c can obtain the needed information. Thus, S~c must periodically
resend the query-status (T) message to the other sites. It does so until a site
responds with the needed information. The site at which C; resides always
has this inforn1.ation.
The log contains no control records (abort, commit, ready) concerning T.
The absence of control records implies that S1c failed before responding to
the prepare (T) message from C;. Since the failure of S1c means that it could
not have sent such a response, by our algorithm, C; must have aborted T.
Hence, S1c must execute undo(T).
18.3.2.2 Failure of the Coordinator
If the coordinator fails in the midst of the execution of the commit protocol for
transaction T, then the participating sites must decide the fate ofT. We shall see
that, in certain cases, the participating sites cannot decide whether to commit
or abort T, and therefore these sites must wait for the recovery of the failed
coordinator.
If an active site contains a   commit T   record in its log, then T must be
committed.
If an active site contains an   abort T   record in its log, then T must be
aborted.
If some active site does not contain a   ready T   record in its log, then the
failed coordinator C; cannot have decided to commit T. We can draw this
conclusion because a site that does not have a   ready T   record in its log
cannot have sent a ready (T) message to C;. However, the coordinator may
have decided to abort T. Rather than wait for C; to recover, it is preferable
to abort Tin this case.
If none of the preceding cases holds, then all the active sites must have a
  ready T   record in their logs, but no additional control records (such
as   abort T   or   commit T   ). Since the coordinator has failed, it is
impossible to determine whether a decision has been made-or, if so,
what that decision is-until the coordinator recovers. Thus, the active
sites must wait for C to recover. As long as the fate ofT remains in doubt,
T may continue to hold system resources. For example, if locking is used,
T may hold locks on data at active sites. Such a situation is undesirable
because hours or days may pass before C; is again active. During this
time, other transactions may be forced to wait for T. As a result, data are
unavailable not only on the failed site (C;) but on active sites as well. The
amount of unavailable data increases as the downtime of C; grows. This
situation is called the blocking problem, because T is blocked pending the
recovery of site C;.
736 Chapter 18
18.4
18.3.2.3 Failure of the Network
When a link fails, the messages in the process of being routed through the
link do not arrive at their destinations intact. From the viewpoint of the sites
connected throughout that link, the other sites appear to have failed. Thus, our
previous schemes apply here as welL
When a number of links fail, the network may partition. In this case,
two possibilities exist. The coordinator and all its participants may remain in
one partition; in this case, the failure has no effect on the commit protocoL
Alternatively, the coordinator and its participants may belong to several
partitions; in this case, messages between the participant and the coordinator
are lost, reducing the case to a link failure.
We move next to the issue of concurrency controL In this section, we show
how certain of the concurrency-control schemes discussed in Chapter 6 can be
modified for use in a distributed environment.
The transaction manager of a distributed database system manages the
execution of those transactions (or subtransactions) that access data stored
in a local site. Each such transaction may be either a local transaction
(that is, a transaction that executes only at that site) or part of a global
transaction (that is, a transaction that executes at several sites). Each transaction
manager is responsible for maintaining a log for recovery purposes and for
participating in an appropriate concurrency-control scheme to coordinate the
concurrent execution of the transactions executing at that site. As we shall
see, the concurrency schemes described in Chapter 6 need to be modified to
accommodate the distribution of transactions.
18.4.1 Locking Protocols
The two-phase locking protocols described in Chapter 6 can be used in a
distributed environment. The only change needed is iil the way the lock
manager is implemented. Here, we present several possible schemes. The first
deals with the case where no data replication is allowed. The others apply
to the more general case where data can be replicated in several sites. As in
Chapter 6, we assume the existence of the shared and
18.4.1.1 Nonreplicated Scheme
If no data are replicated in the system, then the locking schemes described in
Section 6.9 can be applied as follows: Each site maintains a local lock manager
whose function is to administer the lock and unlock requests for those data
items stored in that site. When a transaction wishes to lock data item Qat site
S;, it simply sends a message to the lock manager at site S; requesting a lock
(in a particular lock mode). If data item Q is locked in an incompatible mode,
then the request is delayed until that request can be granted. Once it has been
determined that the lock request can be granted, the lock n1.anager sends a
message back to the initiator indicating that the lock request has been granted.
18.4 737
This scheme has the advantage of simple implementation. It requires two
message transfers for handling lock requests and one ncessage transfer for
handling unlock requests. However, deadlock handling is more complex. Since
the lock and unlock requests are no longer made at a single site, the various
deadlock-handling algorithms discussed in Chapter 7 must be modified; these
modifications are discussed in Section 18.5.
18.4.1.2 Single-Coordinator Approach
Several concurrency-control schemes can be used in systems that allow data
replication. Under the single-coordinator approach, the system maintains a
single lock manager that resides in a single chosen site-say, Si. All lock and
unlock requests are made at site Si. When a transaction needs to lock a data
item, it sends a lock request to Si. The lock manager determines whether the
lock can be granted immediately. If so, it sends a message to that effect to the
site at which the lock request was initiated. Otherwise, the request is delayed
until it can be granted; and at that time, a message is sent to the site at which
the lock request was initiated. The transaction can read the data item from any
one of the sites at which a replica of the data item resides. In the case of a
write operation, all the sites where a replica of the data item resides must be
involved in the writing.
The scheme has the following advantages:
Simple implementation. This scheme requires two messages for handling
lock requests and one message for handling Lmlock requests.
Simple deadlock handling. Since all lock and unlock requests are made
at one site, the deadlock-handling algorithms discussed in Chapter 7 can
be applied directly to this environment.
The disadvantages of the scheme include the following:
Bottleneck The site Si becomes a bottleneck, since all requests must be
processed there.
Vulnerability. If the site Si fails, the concurrency controller is lost. Either
processing must stop or a recovery scheme must be used.
A compromise between these advantages and disadvantages can be
achieved through a in which the lockmanager
function is distributed over several sites. Each lock manager administers
the lock and unlock requests for a subset of the data items. This distribution
reduces the degree to which the coordinator is a bottleneck, but it complicates
deadlock handling, since the lock and unlock requests are not made at a single
site.
18.4.1.3 Majority Protocol
The majority protocol is a modification of the nonreplicated data scheme
presented earlier. The system maintains a lock manager at each site. Each
manager controls the locks for all the data or replicas of data stored at that site.
When a transaction wishes to lock a data item Q that is replicated inn different
738 Chapter 18
sites, it must send a lock request to nlore than one-half of then sites in which
Q is stored. Each lock manager detern1.ines whether the lock can be granted
immediately (as far as it is concerned). As before, the response is delayed until
the request can be granted. The transaction does not operate on Q until it has
successfully obtained a lock on a majority of the replicas of Q
This scheme deals with replicated data in a decentralized manner, thus
avoiding the drawbacks of central control. However, it suffers from its own
disadvantages:
Implementation. The majority protocol is more complicated to implement
than the previous schemes. It requires 2(n/2 + 1) messages for handling
lock requests and (n/2 + 1) messages for handling unlock requests.
Deadlock handling. Since the lock and unlock requests are not made
at one site, the deadlock-handling algorithms must be modified (Section
18.5). In addition, a deadlock can occur even if only one data item is being
locked. To illustrate, consider a system with four sites and full replication.
Suppose that transactions T1 and T2 wish to lock data item Q in exclusive
mode. Transaction T1 may succeed in locking Q at sites 51 and 53, while
transaction T2 may succeed in locking Q at sites 52 and 54. Each then must
wait to acquire the third lock, and hence a deadlock has occurred.
18.4.1.4 Biased Protocol
The biased protocol is similar to the majority protocol. The difference is that
requests for shared locks are given more favorable treatment than are requests
for exclusive locks. The system maintains a lock manager at each site. Each
manager manages the locks for all the data items stored at that site. Shared and
exclusive locks are handled differently.
Shared locks. When a transaction needs to lock data item Q, it simply
requests a lock on Q from the lock manager at one site containing a replica
of Q
Exclusive locks. When a transaction needs to lock data item Q, it requests
a lock on Q from the lock manager at each site containing a replica of Q
As before, the response to the request is delayed until the request can be
granted.
The scheme has the advantage of imposing less overhead on read operations
than does the majority protocol. This advantage is especially significant
in common cases in which the frequency of reads is much greater than the
frequency of writes. However, the additional overhead on writes is a disadvantage.
Furthermore, the biased protocol shares the majority protocol's
disadvantage of complexity in handling deadlock.
18.4.1.5 Primary Copy
Yet another alternative is to choose one of the replicas as the primary copy.
Thus, for each data item Q, the primary copy of Q must reside in precisely one
site, which we call the primary site of Q. When a transaction needs to lock a data
18.4 739
item Q, it requests a lock at the primary site of Q. Again, the response to the
request is delayed until the request can be granted.
This scheme enables us to handle concurrency control for replicated data
in much the same way as for unreplicated data. Implementation of the method
is simple. However, if the primary site of Q fails, Q is inaccessible even though
other sites containing a replica may be accessible.
18.4.2 Timestamping
The principal idea behind the timestamping scheme discussed in Section 6.9 is
that each transaction is given a unique timestamp, which is used to decide the
serialization order. Our first task, then, in generalizing the centralized scheme to
a distributed scheme is to develop a method for generating unique timestamps.
Our previous protocols can then be applied directly to the nonreplicated
environment.
18.4.2.1 Generation of Unique Timestamps
Two primary methods are used to generate unique timestamps; one is centralized,
and one is distributed. In the centralized scheme, a single site is chosen
for distributing the timestamps. The site can use a logical counter or its own
local clock for this purpose.
In the distributed scheme, each site generates a local unique timestamp
using either a logical counter or the local clock The global unique timestamp
is obtained by concatenation of the local unique timestamp with the site
identifier, which must also be unique (Figure 18.2). The order of concatenation
is important! We use the site identifier in the least sign.ificant position to ensure
that the global timestamps generated in one site are not always greater than
those generated in another site. Compare this technique for generating unique
timestamps with the one presented in Section 18.1.2 for generating unique
na1nes.
We may still have a problem if one site generates local timestamps at a
faster rate than do other sites. In such a case, the fast site's logical counter will
be larger than those of other sites. Therefore, all timestamps generated by the
fast site will be larger than those generated by other sites. A mechanism is
needed to ensure that local timestamps are generated fairly across the system.
To accomplish the fair generation of timestamps, we define within each site Si a
logical clock (LC ), which generates the local timestamp (see Section 18.1.2). To
ensure that the various logical clocks are synchronized, we require that a site
local unique timestamp site identifier
ll 0
global unique identifier
Figure 18.2 Generation of unique timestamps.
740 Chapter 18
18.5
Si advance its logical clock whenever a transaction ~ with timestarnp   X,y  
visits that site and xis greater than the current value of LC. In this case, site Si
advances its logical clock to the value x + 1.
If the system clock is used to generate timestamps, then timestamps are
assigned fairly, provided that no site has a system clock that runs fast or slow.
Since clocks may not be perfectly accurate, a technique similar to that used for
logical clocks must be used to ensure that no clock gets far ahead or far behind
another clock.
18.4.2.2 Timestamp-Ordering Scheme
The basic timestamp scheme introduced in Section 6.9.4.3 can be extended in
a straightforward mam'ler to a distributed system. As in the centralized case,
cascading rollbacks may result if no mechanism is used to prevent a transaction
from reading a data item value that is not yet committed. To eliminate cascading
rollbacks, we can con  bine the basic timestamp scheme of Section 6.9 with the
2PC protocol of Section 18.3 to obtain a protocol that ensures serializability
with no cascading rollbacks. We leave the development of such an algorithm
to you.
The basic timestamp scheme just described suffers from the undesirable
property that conflicts between transactions are resolved through rollbacks,
rather than through waits. To alleviate this problem, we can buffer the various
read and write operations (that is, delay them) until a time when we are
assured that these operations can take place without causing aborts. A read(x)
operation by~ must be delayed if there exists a transaction Tj that will perform
a wri te(x) operation but has not yet done so and TS(Tj)    TS(~ ). Similarly, a
wri te(x) operation by~ must be delayed if there exists a transaction Tj that will
perform either a read(x) or a wri te(x) operation and TS(Tj)    TS(T;). Various
methods are available for this property. One such method, called
the scheme, requires each site to maintain
a read queue and a write queue consisting of all the read and write requests
that are to be executed at the site and that must be delayed to preserve the
property just described. We shall not present the scheme here. Again, we leave
the development of the algorithm to you.
The deadlock-prevention, deadlock-avoidance, and deadlock-detection algorithms
presented in Chapter 7 can be extended so that they can be used in
a distributed system. In this section, we describe several of these distributed
algorithms.
18.5.1 Deadlock Prevention and Avoidance
The deadlock-prevention and deadlock-avoidance algorithms presented in
Chapter 7 can be used in a distributed system, provided that appropriate
modifications are made. For example, we can use the resource-ordering
deadlock-prevention technique by simply defining a global ordering among
the system resources. That is, all resources in the entire system are assigned
unique numbers, and a process may request a resource (at any processor) with
18.5 741
unique nL1mber i only if it is not holding a resource with a unique number
greater than i. Similarly, we can use the banker's algorithm in a distributed
systen'l by designating one of the processes in the system (the banker) as the
process that maintains the information necessary to carry out the banker's
algorithm. Every resource request must be channeled through the banker.
The global resource-ordering deadlock-prevention scheme is simple to
implement in a distributed environment and requires little overhead. The
banker's algorithm can also be implemented easily, but it may require too
much overhead. The banker may become a bottleneck, since the number of
messages to and from the banker may be large. Thus, the banker's scheme
does not seem to be of practical use in a distributed system.
We turn next to a new deadlock-prevention scheme based on a timestampordering
approach with resource preemption. Although this approach can
handle any deadlock situation that may arise in a distributed system, for
simplicity we consider only the case of a single instance of each resource type.
To control the preemption, we assign a unique priority number to each
process. These numbers are used to decide whether a process P; should wait
for a process Pj. For example, we can let P; wait for Pj if P; has a priority higher
than that of Pj; otherwise, P; is rolled back. This scheme prevents deadlocks
because, for every edge P; ---+ Pi in the wait-for graph, P; has a higher priority
than P;. Thus, a cycle cannot exist.
One difficulty with this scheme is the possibility of starvation. Some
processes with extremely low priorities may always be rolled back. This
difficulty can be avoided through the use of timestamps. Each process in the
system is assigned a unique timestamp when it is created. Two complementary
deadlock-prevention schemes using timestamps have been proposed:
1. The wait-die scheme. This approach is based on a nonpreemptive
teclmique. When process P; requests a resource currently held by Pj, P; is
allowed to wait only if it has a smaller timestamp than does Pj (that is, P;
is older than Pj ). Otherwise, P; is rolled back (dies). For example, suppose
that processes P1, P2, and P3 have timestamps 5, 10, and 15, respectively.
If P1 requests a resource held by P2, P1 will wait. If P3 requests a resource
held by P2, P3 will be rolled back.
The wound-wait scheme. This approach is based on a preemptive
technique and is a counterpart to the wait-die approach. When process
P; requests a resource currently held by Pj, P; is allowed to wait only
if it has a larger timestamp than does Pi (that is, P; is younger than
Pi). Otherwise, Pi is rolled back (Pj is wounded by P; ). Returning to our
previous example, with processes P1, P2, and P3, if P1 requests a resource
held by P2, then the resource will be preempted from P2, and P2 will be
rolled back. If P3 requests a resource held by P2, then P3 will wait.
Both schemes can avoid starvation provided that, when a process is rolled
back, it is not assigned a new timestamp. Since timestamps always increase, a
process that is rolled back will eventually have the smallest timestamp. Thus,
it will not be rolled back again. There are, however, significant differences in
the way the two schemes operate.
742 Chapter 18
In the wait-die scheme, an older process must wait for a younger one to
release its resource. Thus, the older the process gets, the more it tends to
wait. By contrast, in the wound-wait scheme, an older process never waits
for a younger process.
In the wait-die scheme, if a process Pi dies and is rolled back because it
has requested a resource held by process Pj, then Pi n  ay reissue the same
sequence of requests when it is restarted. If the resource is still held by Pj,
then Pi will die again. Thus, Pi may die several times before acquiring the
needed resource. Contrast this series of events with what happens in the
wound-wait scheme. Process Pi is wounded and rolled back because Pi
has requested a resource it holds. When Pi is restarted and requests the
resource now being held by Pi, Pi waits. Thus, fewer rollbacks occur in
the wound-wait scheme.
The major problem with both schemes is that unnecessary rollbacks may occur.
18.5.2 Deadlock Detection
The deadlock-prevention algorithm may preempt resources even if no deadlock
has occurred. To prevent um  ecessary preemptions, we can use a deadlockdetection
algorithm. We construct a wait-for graph describing the resourceallocation
state. Since we are assuming only a single resource of each type, a
cycle in the wait-for graph represents a deadlock.
The main problem in a distributed system is deciding how to maintain
the wait-for graph. We illustrate this problem by describing several common
techniques to deal with this issue. These schemes require each site to keep a
local wait-for graph. The nodes of the graph correspond to all the processes
(local as well as nonlocal) currently holding or requesting any of the resources
local to that site. For example, in Figure 18.3 we have a system consisting of two
sites, each maintaining its local wait-for graph. Note that processes P2 and P3
appear in both graphs, indicating that the processes have requested resources
at both sites.
These local wait-for graphs are constructed in the usual manner for local
processes and resources. When a process Pi in site 51 needs a resource held by
process Pj in site 52, a request message is sent by Pi to site 52 . The edge Pi -+
Pj is then inserted in the local wait-for graph of site 52 .
Figure 18.3 Two local wait-for graphs.
18.5 743
Figure 18.4 Global wait-for graph for Figure 18.3.
Clearly, if any local wait-for graph has a cycle, deadlock has occurred. The
fact that we find no cycles in any of the local wait-for graphs does not mean
that there are no deadlocks, however. To illustrate this problem, we consider
the system depicted in Figure 18.3. Each wait-for graph is acyclic; nevertheless,
a deadlock exists in the system. To prove that a deadlock has not occurred, we
must show that the of all local graphs is acyclic. The graph (Figure 18.4)
that we obtain from the union of the two wait-for graphs of Figure 18.3 does
indeed contain a cycle, implying that the system is in a deadlocked state.
A number of methods are available to organize the wait-for graph in a
distributed system. We describe several common schemes here.
18.5.2.1 Centralized Approach
In the centralized approach, a global wait-for graph is constructed as the
tmion of all the local wait-for graphs. It is maintained in a single process:
the Since there is communication delay in
the system, we must distinguish between two types of wait-for graphs. The
real graph describes the real but unknown state of the system at any point
in time, as would be seen by an ormliscient observer. The constructed graph
is an approximation generated by the coordinator during the execution of its
algorithm. The constructed graph must be generated so that, whenever the
detection algorithm is invoked, the reported results are correct. By correct we
mean the following:
If a deadlock exists, then it is reported properly.
If a deadlock is reported, then the system is indeed in a deadlocked state.
As we shall show, it is not easy to construct such correct algorithms.
The wait-for graph may be constructed at three different points in time:
Whenever a new edge is inserted in or removed from one of the local
wait-for graphs
Periodically, when a number of changes have occurred in a wait-for graph
Whenever the deadlock-detection coordinator needs to invoke the cycledetection
algorithm
When the deadlock-detection algorithm is invoked, the coordinator searches
its global graph. If a cycle is found, a victim is selected to be rolled back. The
744 Chapter 18
coordinator must notify all the sites that a particular process has been selected
as victiiTl. The sites, in turn, roll back the victim process.
Next, we consider each of the three graph-construction options listed
above. With option 1, whenever an edge is either inserted in or removed from a
local graph, the local site must also send a message to the coordinator to notify
it of this modification. On receiving such a message/ the coordinator updates
its global graph.
Alternatively (option 2) 1 a site can send a number of such changes in a single
message periodically. Returning to our previous example/ the coordinator
process will maintain the global wait-for graph as depicted in Figure 18.4.
When site 52 inserts the edge P3 -+ P4 in its local wait-for graph/ it also sends
a message to the coordinator. Similarly/ when site 51 deletes the edge P5 -+ P1
because P1 has released a resource that was requested by Ps 1 an appropriate
message is sent to the coordinator.
Note that no matter which of these two options is used/ unnecessary
rollbacks may occur, as a result of two situations:
1. False may exist in the global wait-for graph. To illustrate this point
we consider a snapshot of the system as depicted in Figure 18.5. Suppose
that P2 releases the resource it is holding in site 511 resulting in the deletion
of the edge P1 -+ P2 in site 51. Process P2 then requests a resource held
by P3 at site 521 resulting in the addition of the edge P2 -+ P3 in site 52. If
the insert P2 -+ P3 message from site 52 arrives before the delete P1 -+ P2
message from site 51/ the coordinator may discover the false cycle P1 -+
P2 -+ P3 -+ P1 after the insert (but before the delete). Deadlock recovery
may be initiated, although no deadlock has occurred.
2. Unnecessary rollbacks may also result when a deadlock has indeed
occurred and a victim has been picked1 but at the same time one of the
processes has been aborted for reasons unrelated to the deadlock (as
when a process has exceeded its allocated time). For example/ suppose
that site 51 in Figure 18.3 decides to abort P2. At the same time/ the
coordinator has discovered a cycle and picked P3 as a victim. Both P2 and
P3 are now rolled back1 although only P2 needed to be rolled back
Now consider a centralized deadlock-detection algorithm using option
3 that detects all deadlocks that actually occur and does not detect false
deadlocks. To avoid the report of false deadlocks/ we require that requests
coordinator
Figure 18.5 Local and global wait-for graphs.
18.5 745
from different sites be appended with unique identifiers (or timestamps). When
process P;, at site 511 requests a resource from Pj, at site 52, a request message
with timestamp T5 is sent. The edge P; ~ P1 with the label T5 is inserted in the
local wait-for graph of 51. This edge is inserted in the local wait-for graph of
site 52 only if site 52 has received the request message and cannot immediately
grant the requested resource. A request from P; to P1 in the same site is handled
in the usual manner; no timestamps are associated with the edge P; ~ Pi.
The detection algorithm is as follows:
The controller sends an initiating message to each site in the system.
On receiving this message, a site sends its local wait-for graph to
the coordinator. Each of these wait-for graphs contains all the local
information the site has about the state of the real graph. The graph
reflects an instantaneous state of the site, but it is not synchronized with
respect to any other site.
When the controller has received a reply from each site, it constructs a
graph as follows:
a. The constructed graph contains a vertex for every process in the
system.
b. The graph has an edge P; ~ P1 if and only if there is an edge P; ~
P1 in one of the wait-for graphs or an edge P; ~ Pj with some label
T5 in more than one wait-for graph.
If the constructed graph contains a cycle, then the system is in a deadlocked
state. If the constructed graph does not contain a cycle, then the system was
not in a deadlocked state when the detection algorithm was invoked as result
of the initiating messages sent by the coordinator (in step 1).
18.5.2.2 Fully Distributed Approach
In the all controllers share
equally the responsibility for detecting deadlock. Every site constructs a waitfor
graph that represents a part of the total graph, depending on the dynamic
behavior of the system. The idea is that, if a deadlock exists, a cycle will appear
in at least one of the partial graphs. We present one such algorithm, which
involves construction of partial graphs in every site.
Each site maintains its own local wait-for graph. A local wait-for graph in
this scheme differs from the one described earlier in that we add one additional
node Pex to the graph. An arc P; ~ Pex exists in the graph if P; is waiting for a
data item in another site being held by any process. Similarly, an arc Pex ~ Pi
exists in the graph if a process at another site is waiting to acquire a resource
currently being held by P1 in this local site.
To illustrate this situation, we consider again the two local wait-for graphs
of Figure 18.3. The addition of the node Pex in both graphs results in the local
wait-for graphs shown in Figure 18.6.
If a local wait-for graph contains a cycle that does not involve node
Pm then the system is in a deadlocked state. If, however, a local graph
contains a cycle involving Pex, then this implies the possibility of a deadlock.
746 Chapter 18
Figure 18.6 Augmented local wait-for graphs from Figure 18.3.
To ascertain whether a deadlock does exist, we must invoke a distributed
deadlock -detection algorithm.
Suppose that, at site Si, the local wait-for graph contains a cycle involving
node Pex  This cycle must be of the form
which indicates that process Pk, in site Si is waiting to acquire a data item
located in some other site-say, Sj- On discovering this cycle, site Si sends to
site Si a deadlock-detection message containing information about that cycle.
When site Si receives this deadlock-detection message, it updates its
local wait-for graph with the new information. Then it searches the newly
constructed wait-for graph for a cycle not involving Pex  If one exists, a
deadlock is found, and an appropriate recovery scheme is iiwoked. If a cycle
involving Pex is discovered, then Si transmits a deadlock-detection message
to the appropriate site-say, Sk. Site SIC! in return, repeats the procedure.
Thus, after a finite number of rounds, either a deadlock is discovered or the
deadlock-detection computation halts.
To illustrate this procedure, we consider the local wait-for graphs of Figure
18.6. Suppose that site 51 discovers the cycle
Since P3 is waiting to acquire a data item in site 52, a deadlock-detection
message describing that cycle is transmitted from site 51 to site 52 . When site 52
receives this message, it updates its local wait-for graph, obtaining the wait-for
graph of Figure 18.7. This graph contains the cycle
which does not include node Pex  Therefore, the system is in a deadlocked state,
and an appropriate recovery scheme must be invoked.
Note that the outcome would be the same if site 52 discovered the cycle first
in its local wait-for graph and sent the deadlock-detection message to site 51.
In the worst case, both sites will discover the cycle at about the same time, and
two deadlock-detection messages will be sent: one by 51 to 52 and another by
S2 to 51. This situation results in unnecessary message transfer and overhead in
updating the two local wait-for graphs and searching for cycles in both graphs.
18.6
18.6 747
Figure 18.7 Augmented local wait-for graph in site 5.z of Figure 18.6.
To reduce message traffic, we assign to each process Pi a unique identifier,
which we denote ID(Pi ). When site Sk discovers that its local wait-for graph
contains a cycle involving node Pex of the form
it sends a deadlock-detection message to another site only if
ID(PI  J    ID(Pr  J.
Otherwise, site S1c continues its normal execution, leaving the burden of
initiating the deadlock-detection algorithm to some other site.
To illustrate this scheme, we consider again the wait-for graphs maintained
at sites 51 and 52 as shown in Figure 18.6. Suppose that
Suppose both sites discover these local cycles at about the same time. The cycle
in site 51 is of the form
Since ID(P3)    ID(P2), site 51 does not send a deadlock-detection message to
site 52 .
The cycle in site 52 is of the form
Since ID(P2)    ID(P3), site 52 does send a deadlock-detection message to site
51, which, on receiving the message, updates its local wait-for graph. Site S1
then searches for a cycle in the graph and discovers that the system is in a
deadlocked state.
As we pointed out in Section 18.3, many distributed algorithms employ a
coordinator process that performs functions needed by the other processes in
748 Chapter 18
the systenc. These functions include enforcing mutual exclusion, maintaining
a global wait-for graph for deadlock detection, replacing a lost token, and
controlling an input or output device in the system. If the coordinator process
fails due to the failure of the site at which it resides, the system can continue
execution only by restarting a new copy of the coordinator on some other site.
The algorithms that determine where a new copy of the coordinator should be
restarted are called
Election algorithms assume that a unique priority number is associated
with each active process in the system. For ease of notation, we assume that
the priority number of process g is i. To simplify our discussion, we assume
a one-to-one correspondence between processes and sites and thus refer to
both as processes. The coordinator is always the process with the largest
priority number. Hence, when a coordinator fails, the algorithm must elect
that active process with the largest priority number. This number must be sent
to each active process in the system. In addition, the algorithm must provide a
mechanism for a recovered process to identify the current coordinator.
In this section, we present examples of election algorithms for two different
configurations of distributed systems. The first algorithm applies to systems
where every process can send a message to every other process in the system.
The second algorithm applies to systems organized as a ring (logically or
physically). Both algorithms require n2 messages for an election, where n is the
number of processes in the system. We assume that a process that has failed
knows on recovery that it has indeed failed and thus takes appropriate actions
to rejoin the set of active processes.
18.6.1 The Bully Algorithm
Suppose that process P; sends a request that is not answered by the coordinator
within a time interval T. In this situation, it is assumed that the coordinator has
failed, and P; tries to elect itself as the new coordinator. This task is completed
through the following algorithm.
Process P; sends an election message to every process with a higher priority
number. Process P; then waits for a time interval T for an answer from any one
of these processes.
If no response is received within time T, P; assumes that all processes with
numbers greater than i have failed and elects itself the new coordinator. Process
P; restarts a new copy of the coordinator and sends a message to inform all
active processes with priority numbers less than i that P; is the new coordinator.
Howeve1~ if an answer is received, P; begins a time interval T', waiting to
receive a message informing it that a process with a higher priority number
has been elected. (That is, some other process is electing itself coordinator and
should report the results within time T.) If no message is received within T',
then the process with a higher number is assumed to have failed, and process
P; should restart the algorithm.
If P; is not the coordinator, then, at any time during execution, P; may
receive one of the following two messages from process Pr
P1 is the new coordinator (j    i). Process P;, in turn, records this
information.
18.6 749
Pi has started an election (j    i). Process P; sends a response to Pi
and begins its own election algorithm, provided that P; has not already
initiated such an election.
The process that completes its algorithm has the highest number and is elected
as the coordinator. It has sent its number to all active processes with lower
numbers. After a failed process recovers, it immediately begins execution of
the same algorithm. If there are no active processes with higher numbers, the
recovered process forces all processes with lower numbers to let it become the
coordinator process, even if there is a currently active coordinator with a lower
number. For this reason, the algorithm is termed the
We can demonstrate the operation of the algorithm with a simple example
of a system consisting of processes P1 through P4 . The operations are as follows:
All processes are active; P4 is the coordinator process.
P1 and P4 fail. P2 determines that P4 has failed by sending a request that
is not answered within time T. P2 then begins its election algorithm by
sending a request to P3 .
P3 receives the request, responds to P2, and begins its own algorithm by
sending an election request to P4 .
P2 receives P3's response and begins waiting for an interval T'.
P4 does not respond within an interval T, so P3 elects itself the new
coordinator and sends the number 3 to P2 and P1. (P1 does not receive
the number, since it has failed.)
Later, when P1 recovers, it sends an election request to P2 , P3, and P4 .
P2 and P3 respond to P1 and begin their own election algorithms. P3 will
again be elected, through the same events as before.
Finally, P4 recovers and notifies P1, P2, and P3 that it is the current
coordinator. (P4 sends no election requests, since it is the process with
the highest number in the system.)
18.6.2 The Ring Algorithm
The assumes that the links between processes are unidirectional
and that each process sends its messages to the neighbor on the right. The main
data structure used by the algorithm is the lisi:, a list that contains the
priority numbers of all active processes in the system when the algorithm ends;
each process maintains its own active list. The algorithm works as follows:
If process P; detects a coordinator failure, it creates a new active list that
is initially empty. It then sends a message elect(i) to its neighbor on the
right and adds the number i to its active list.
750 Chapter 18
18.7
If Pi receives a message elect(j) from the process on the left, it must respond
in one of three ways:
a. If this is the first elect message it has seen or sent, Pi creates a new
active list with the numbers i and j. It then sends the message elect(i),
followed by the message elect(j).
b. Ifi -::f. j-thatis,ifthe message received does not contain Pi'snumber
-then Pi adds j to its active list and forwards the message to its
neighbor on the right.
c. If i = j-that is, if Pi receives the message elect(i)-then the active
list for Pi now contains the numbers of all the active processes in
the system. Process g can now determine the largest number in the
active list to identify the new coordinator process.
This algorithm does not specify how a recovering process determines the
number of the current coordinator process. One solution requires a recovering
process to send an inquiry message. This message is forwarded around the ring
to the current coordinator, which in turn sends a reply containing its number.
For a system to be reliable, we need a mechanism that allows a set of processes
to agree on a common value. Such an agreement may not take place, for several
reasons. First, the communication medium may be faulty, resulting in lost or
garbled messages. Second, the processes themselves may be faulty, resultilcg
lie unpredictable process behavior. The best we can hope for in this case is that
processes fail in a clean way, stopping their execution without deviating from
their normal execution pattern. In the worst case, processes may send garbled
or incorrect messages to other processes or even collaborate with other failed
processes in an attempt to the integrity of the system.
The provides an analogy for this situation.
Several divisions Byzantine army, each commanded by its own general,
surround an enemy camp. The Byzantine generals must reach agreement on
whether or not to attack the enemy at dawn. It is crucial that all generals agree,
since an attack by only some of the divisions would result lie defeat. The various
divisions are geographically dispersed, and the generals can communicate with
one another only via messengers who run from camp to camp. The generals
may not be able to reach agreement for at least two major reasons:
Messengers may get caught by the enemy and thus may be unable to
deliver their messages. This situation corresponds to unreliable communication
in a computer system and is discussed further in Section
18.7.1.
Generals may be traitors, trying to prevent the loyal generals from
reaching an agreement. This situation corresponds to faulty processes
lie a computer system and is discussed further in Section 18.7.2.
18.7 751
18.7.1 Unreliable Communications
Let us first assume that, if processes fail, they do so in a clean way and that
the communication medium is unreliable. Suppose that process P; at site 51,
which has sent a message to process Pj at site 52, needs to know whether
P1 has received the message so that it can decide how to proceed with its
computation. For example, P; may decide to compute a functionfoo if Pj has
received its message or to compute a function boo if P1 has not received the
message (because of some hardware failure).
To detect failures, we can use a similar to the one
described in Section 16.7.1. When P; sends out a message, it also specifies
a time interval during which it is willing to wait for an acknowledgment
message from Pi. When Pi receives the message, it immediately sends an
acknowledgment to P;. If P; receives the acknowledgment message within the
specified time interval, it can safely conclude that P1 has received its message.
If, however, a time-out occurs, then P; needs to retransmit its message and
wait for an acknowledgment. This procedure continues until P; either gets the
acknowledgment message back or is notified by the system that site 52 is down.
In the first case, it will compute foo; in the latter case, it will compute boo. Note
that, if these are the only two viable alternatives, P; must wait until it has been
notified that one of the situations has occurred.
Suppose now that P7 also needs to know that P; has received its acknowledgment
message, so that it can decide how to proceed with its computation.
For example, P1 may want to compute foo only if it is assured that P; got
its acknowledgment. In other words, P; and Pi will compute foo if and only
if both have agreed on it. It turns out that, in the presence of failure, it is
not possible to accomplish this task. More precisely, it is not possible in a
distributed environment for processes P; and P1 to agree completely on their
respective states.
To prove this claim, let us suppose that a minimal sequence of message
transfers exists such that, after the messages have been delivered, both
processes agree to compute foo. Let m' be the last message sent by P; to
Pj- Since g does not know whether its message will arrive at Pj (since the
message may be lost due to a failure), P; will execute foo regardless of the
outcome of the message delivery. Thus, m' could be removed from the sequence
without affecting the decision procedure. Hence, the original sequence was not
minimal, contradicting our assumption and showing that there is no sequence.
The processes can never be sure that both will compute foo.
18.7.2 Faulty Processes
Now let us assume that the communication medium is reliable but that
processes can fail in unpredictable ways. Consider a system of n processes,
of which no more than m are faulty. Suppose that each process P; has some
private value of V;. We wish to devise an algorithm that allows each nonfaulty
process P; to construct a vector X; = (A;,l, A;,2, ... , A;, 11 ) such that the following
conditions exist:
1. If Pi is a nonfaulty process, then A;,i = Vi-
If P; and P1 are both nonfaulty processes, then X; = x1.
752 Chapter 18
18.8
There are many solutions to this problem, and they share the following
properties:
A correct algorithm can be devised only if n    3 x m + 1.
The worst-case delay for reaching agreement is proportionate tom + 1
message-passing delays.
The number of messages required for reaching agreement is large. No
single process is trustworthy, so all processes must collect all information
and make their own decisions.
Rather than presenting a general solution, which would be complicated, we
present an algorithm for the simple case where m = 1 and n = 4. The algorithm
requires two rounds of information exchange:
Each process sends its private value to the other three processes.
Each process sends the information it has obtained in the first round to
all other processes.
A faulty process obviously may refuse to send messages. In this case, a
nonfaulty process can choose an arbitrary value and pretend that the value
was sent by the faulty process.
Once these two rounds are completed, a nonfaulty process Pi can construct
its vector X; = (Ai,l, A,2, Ai,3, A4) as follows:
Ai = v;.
For j -1- i, if at least two of the three values reported for process Pj (in
the two rounds of exchange) agree, then the majority value is used to set
the value of Ai,f' Otherwise, a default value-say, nil-is used to set the
value of Ai,f'
In a distributed system with no common memory and no common clock, it
is sometimes impossible to determine the exact order in which two events
occur. The happened-before relation is only a partial ordering of the events in
a distributed system. Timestamps can be used to provide a consistent event
ordering.
Mutual exclusion in a distributed environment can be implemented in a
variety of ways. In a centralized approach, one of the processes in the system
is chosen to coordinate the entry to the critical section. In the fully distributed
approach, the decision making is distributed across the entire system. A
distributed algorithm, which is applicable to ring-structured networks, is the
token-passing approach.
For atomicity to be ensured, all the sites in which a transaction T has
executed must agree on the final outcome of the execution. T either commits at
all sites or aborts at all sites. To ensure this property, the transaction coordinator
753
of T must execute a commit protocol. The most widely used commit protocol
is the 2PC protocol.
The various concurrency-control schemes that can be used in a centralized
system can be modified for use in a distributed environment. In the case
of locking protocols, we need only change the way the lock manager is
implemented. In the case of timestamping and validation schemes, the only
change needed is the development of a mechanism for generating unique
global timestamps. The mechanism can either concatenate a local timestamp
with the site identification or advance local clocks whenever a message arrives
that has a larger timestamp.
The primary method for dealing with deadlocks in a distributed environment
is deadlock detection. The main problem is deciding how to maintain the
wait-for graph. Methods for organizing the wait-for graph include a centralized
approach and a fully distributed approach.
Some distributed algorithms require the use of a coordinator. If the
coordinator fails because of the failure of the site at which it resides, the system
can continue execution only by restarting a new copy of the coordinator on
some other site. It can do so by maintaining a backup coordinator that is
ready to assume responsibility if the coordinator fails. Another approach is to
choose the new coordinator after the coordinator has failed. The algorithms
that determine where a new copy of the coordinator should be restarted are
called election algorithms. Two algorithms, the bully algorithm and the ring
algorithm, can be used to elect a new coordinator in case of failures.
18.1 Why is deadlock detection much more expensive in a distributed
environment than in a centralized environment 
18.2 Consider a hierarchical deadlock-detection algorithm in which the
global wait-for graph is distributed over a number of different controllers,
which are organized in a tree. Each non-leaf controller maintains
a wait-for graph that contains relevant information from the graphs of
the controllers in the subtree below it. In particular, let SA, Sp, and Sc
be controllers such that Sc is the lowest common ancestor of SA and
Sp (Sc must be unique, since we are dealing with a tree). Suppose that
node Ti appears in the local wait-for graph of controllers SA and Sp.
Then ~ must also appear in the local wait-for graph of
Controller Sc
Every controller in the path from Sc to SA
Every controller in the path from Sc to Sp
In addition, if ~ and Tj appear in the wait-for graph of controller So
and there exists a path from Ti to Ti in the wait-for graph of one of the
children of S0 , then an edge ~ -+ Tj must be in the wait-for graph
of So.
Show that, if a cycle exists in any of the wait-for graphs, then the
system is deadlocked.
754 Chapter 18
18.3 Your company is building a comp1.1ter network, and you are asked to
develop a scheme for dealing with the deadlock problem.
a. Would you use a deadlock-detection scheme or a deadlockprevention
scheme 
b. If you used a deadlock-prevention scheme, which one would you
use  Explain your choice.
c. If you used a deadlock-detection scheme, which one would you
use  Explain your choice.
18.4 Derive an election algorithm for bidirectional rings that is more efficient
than the one presented in this chapter. How many messages are needed
for n processes 
18.5 Your company is building a computer network, and you are asked to
write an algorithm for achieving distributed mutual exclusion. Which
scheme will you use  Explain your choice.
18.6 Consider the following failure model for faulty processors. Processors
follow the prescribed protocol but may fail at unexpected points in
time. When processors fail, they simply stop functioning and do not
continue to participate in the distributed system. Given such a failure
model, design an algorithm for reaching agreement among a set of
processors. Discuss the conditions under which agreement could be
reached.
18.7 Under what circumstances does the wait-die scheme perform better
than the wound-wait scheme for granting resources to concurrently
executing transactions 
18.8 Consider a failure that occurs during 2PC for a transaction. For each
possible failure, explain how 2PC ensures transaction atomicity despite
the failure.
18.9 The logical clock timestamp scheme presented in this chapter provides
the following guarantee: if event A happens before event B, then the
timestamp of A is smaller than the timestamp of B. Note, however, that
we cam1.0t order two events based only on their timestamps. The fact
that an event C has a timestamp that is smaller than the timestamp
of event D does not necessarily mean that event C happened before
event D; C and D could be concurrent events in the system. Discuss
ways in which the logical clock timestamp scheme could be extended
to distinguish concurrent events from events that can be ordered by the
happened-before relation.
The distributed algorithm for extending the happened-before relation to a
consistent total ordering of all the events in the system was developed by
Lamport [1978b ]. Further discussions of using logical time to characterize the
behavior of distributed systems can be found in Fidge [1991], Raynal and
755
Singhal [1996], Babaoglu and Marzullo [1993], Schwarz and Mattern [1994],
and Mattern [1988].
The first general algorithm for implementing mutual exclusion in a
distributed environment was also developed by Lamport [1978b]. Lamport's
scheme requires 3 x (n - 1) messages per critical-section entry. Subsequently,
Ricart and Agrawala [1981] proposed a distributed algorithm that requires only
2 x (n- 1) messages. Their algorithm is presented in Section 18.2.2. A squareroot
algorithm for distributed mutual exclusion is described by Maekawa
[1985]. The token-passing algorithm for rilcg-structured systems presented ilc
Section 18.2.3 was developed by Lann [1977]. Carvalho and Roucairol [1983]
discusses mutual exclusion in computer networks, and Agrawal and Abbadi
[1991] describes an efficient and fault-tolerant solution of distributed mutual
exclusion. A simple taxonomy for distributed mutual-exclusion algorithms is
presented by Raynal [1991].
The issue of distributed synchronization is discussed by Reed and Kanodia
[1979] (shared-memory environment), Lamport [1978b], Lamport [1978a], and
Sclmeider [1982] (totally disjoint processes). A distributed solution to the
dilling-philosophers problem is presented by Chang [1980].
The 2PC protocol was developed by Lampson and Sturgis [1976] and Gray
[1978]. Two modified versions of 2PC, called presume commit and presume
abort, reduce the overhead of 2PC by defilling default assumptions regarding
the fate of transactions (Mohan and Lindsay [1983]).
Papers dealing with the problems of implementillg the transaction concept
in a distributed database were presented by Gray [1981], Traiger et al. [1982],
and Spector and Schwarz [1983]. Bernsteill et al. [1987] offer comprehensive
discussions of distributed concurrency control. Rosenkrantz et al. [1978]
report on the timestamp distributed deadlock-prevention algorithm. The
fully distributed deadlock-detection scheme presented ill Section 18.5.2 was
developed by Obermarck [1982]. The hierarchical deadlock-detection scheme
of Exercise 18.1 appears in Menasce and Muntz [1979]. Knapp [1987] and
Singhal [1989] offer surveys of deadlock detection in distributed systems.
Deadlocks can also be detected by takilcg global snapshots of a distributed
system, as discussed ill Chandy and Lamport [1985].
The Byzantine generals problem is discussed by Lamport et al. [1982] and
Pease et al. [1980]. The bully algorithm is presented by Garcia-Molina [1982],
and the election algorithm for a ring-structured system was written by Larue
[1977].

Part Eight
Our coverage of operating-system issues thus far has focused mainly
on general-purpose computing systems. There are, however, specialpurpose
systems with requirements different from those of many of the
systems we have described.
A real-time system is a computer system that requires not only
that computed results be correct but also that the results be produced
within a specified deadline period. Results produced after the deadline
has passed-even if correct-may be of no real value. For such systems,
many traditional operating-system scheduling algorithms must be
modified to meet the stringent timing deadlines.
A multimedia system must be able to handle not only conventional
data, such as text files, programs, and word-processing documents,
but also multimedia data. Multimedia data consist of continuous-media
data (audio and video) as well as conventional data. Continuous-media
data-such as frames of video-must be delivered according to certain
time restrictions (for example, 30 frames per second). The demands of
handling continuous-media data require significant changes in operatingsystem
structure, most notably in memory, disk, and network management.

19.1
R
Our coverage of operating-system issues thus far has focused mainly on
general-purpose computing systems (for example, desktop and server systems).
We now turn our attention to real-time computing systems. The requirements
of real-time systems differ from those of many of the systems we have
described, largely because real-time systems must produce results within certain
time limits. In this chapter, we provide an overview of real-time computer
systems and describe how real-time operating systems must be constructed to
meet the stringent timing requirements of these systems.
To explain the timing requirements of real-time systems.
To distinguish between hard and soft real-time systems.
To discuss the defining characteristics of real-time systems.
To describe scheduling algorithms for hard real-time systems.
A real-time system is a computer system that requires not only that the
computing results be   correct   but also that the results be produced within
a specified deadline period. Results produced after the deadline has passedeven
if correct-may be of no real value. To illustrate, consider an autonomous
robot that delivers mail in an office complex. If its vision-control system
identifies a wall after the robot has walked into it, despite correctly identifying
the wall, the system has not met its requirement. Contrast this timing
requirement with the much less strict demands of other systems. In an
interactive desktop computer system, it is desirable to provide a quick response
time to the interactive user, but it is not mandatory to do so. Some systems
-such as a batch-processing system-m.ay have no timing requirements
whatsoever.
Real-time systems executing on traditional computer hardware are used
in a wide range of applications. In addition, many real-time systems are
759
760 Chapter 19
19.2
embedded in   specialized devices,   such as ordinary home appliances (for
example, microwave ovens and dishwashers), consumer digital devices (for
exarnple, cameras and MP3 players), and communication devices (for example,
cellular telephones and Blackberry handheld devices). They are also present
in larger entities, such as automobiles and airplanes. An embedded system is
a computing device that is part of a larger system in which the presence of a
computing device is often not obvious to the user.
To illustrate, consider an embedded system for controlling a home dishwasher.
The embedded system may allow various options for scheduling the
operation of the dishwasher-the water temperature, the type of cleaning
(light or heavy), even a timer indicating when the dishwasher is to start. Most
likely, the user of the dishwasher is unaware that there is in fact a computer
embedded in the appliance. As another example, consider an embedded system
controlling antilock brakes in an automobile. Each wheel in the automobile has
a sensor detecting how much sliding and traction are occurring, and each
sensor continually sends its data to the system controller. Taking the results
from these sensors, the controller tells the braking mechanism in each wheel
how much braking pressure to apply. Again, to the user (in this instance, the
driver of the automobile), the presence of an embedded computer system may
not be apparent. It is important to note, however, that not all embedded systems
are real-time. For example, an embedded system controlling a home furnace
may have no real-time requirements whatsoever.
Some real-time systems are identified as safety-critical systems. In a
safety-critical system, incorrect operation-usually due to a missed deadline
-results in some sort of   catastrophe.   Examples of safety-critical systems
include weapons systems, antilock brake systems, flight-management systems,
and health-related embedded systems, such as pacemakers. In these scenarios,
the real-time system must respond to events by the specified deadlines;
otherwise, serious injury-or worse-might occur. However, a significant
majority of embedded systems do not qualify as safety-critical, including FAX
machines, microwave ovens, wristwatches, and networking devices such as
switches and routers. For these devices, missing deadline requirements results
in nothing more than perhaps an unhappy user.
Real-time computing is of two types: hard and soft. A hard real-time
system has the most stringent requirements, guaranteeing that critical realtime
tasks be completed within their deadlines. Safety-critical systems are
typically hard real-time systems. A soft real-time system is less restrictive,
simply providing that a critical real-time task will receive priority over other
tasks and that it will retain that priority until it completes. Many commercial
operating systems-as well as Linux-provide soft real-time support.
In this section, we explore the characteristics of real-time systems and address
issues related to designing both soft and hard real-time operating systencs.
The following characteristics are typical of many real-time systems:
Single purpose
Small size
Inexpensively mass-produced
Specific timing requirements
19.2
We next examine each of these characteristics.
761
Unlike PCs, which are put to many uses, a real-time system. typically serves
only a single purpose, such as controlling antilock brakes or delivering music
on an MP3 player. It is unlikely that a real-time system controlling an airliner's
navigation system will also play DVDs! The design of a real-time operating
system reflects its single-purpose nature and is often quite simple.
Many real-time systems exist in environments where physical space is
constrained. Consider the amount of space available in a wristwatch or a
microwave oven-it is considerably less than what is available in a desktop
computer. As a result of space constraints, most real-time systems lack both
the CPU processing power and the amount of memory available in standard
desktop PCs. Whereas most contemporary desktop and server systems use 32-
or 64-bit processors, many real-time systems run on 8- or 16-bit processors.
Similarly, a desktop PC might have several gigabytes of physical memory,
whereas a real-time system might have less than a megabyte. We refer to the
amount of memory required to run the operating system and its applications
as the footprint of a system. Because the amount of memory is limited, most
real-time operating systems must have small footprints.
Next, consider where many real-time systems are implemented: they are
often found in home appliances and consumer devices. Devices such as digital
cameras, microwave ovens, and thermostats are mass-produced in very costconscious
environments. Thus, the microprocessors for real-time systems must
also be inexpensively mass-produced.
One technique for reducing the cost of an embedded controller is to
use an alternative technique for organizing the components of the computer
system. Rather than organizing the computer around the structure shown in
Figure 19.1, where buses provide the mechanism intercom1ectin.g individual
components, many embedded system controllers use a strategy known as
Here, the CPU, memory (including cache), memorymouse
keyboard printer
Figure 19.1 Bus-oriented organization.
762 Chapter 19
19.3
management-unit (MMU), and any attached peripheral ports, such as USB ports,
are contained in a single integrated circuit. The SOC strategy is typically less
expensive than the bus-oriented organization of Figure 19.1.
We turn now to the final characteristic identified above for real-time
systems: specific timing requirements. It is, in fact, the defining characteristic
of such systems. Accordingly, the primary task of both hard and soft real-time
operating systems is to support the timing requirements of real-time tasks,
and the remainder of this chapter focuses on this issue. Real-time operating
systems meet tim.ing requirements by using scheduling algorithms that give
real-time processes the highest schedulil1.g priorities. Furthermore, schedulers
must ensure that the priority of a real-time task does not degrade over time.
Another technique for addressing timing requirements is by minimizing the
response time to events such as il1.terrupts.
In this section, we discuss the features necessary for designing an operating
system that supports real-time processes. Before we begin, though, let's
consider what is typically not needed for a real-time system. We begin
by examining several features provided in many of the operatil1.g systems
discussed so far in this text, including Linux, UNIX, and the various versions
of Windows. These systems typically provide support for the following:
A variety of peripheral devices, such as graphical displays, CD drives, and
DVD drives
Protection and security mechanisms
Multiple users
Supporting these features often results in a sophisticated -and large-kernel.
For example, Windows XP has over forty million lines of source code. In
contrast, a typical real-time operating system usually has a very simple design,
often written in thousands rather than millions of lines of source code. We
would not expect these simple systems to il1.clude the features listed above.
But why don't real-time systems provide these features, which are crucial to
standard desktop and server systems  There are several reasons, but three are
most prominent. First, because most real-time systems serve a single purpose,
they simply do not require many of the features found il1. a desktop PC.
Consider a digital wristwatch: it obviously has no need to support a disk drive
or DVD, let alone virtual memory. Furthermore, a typical real-time system
does not include the notion of a user. The system simply supports a small
number of tasks, which often await input from hardware devices (sensors,
vision identification, and so forth). Second, the features supported by standard
desktop operating systems are impossible to provide without fast processors
and large amounts of memory. Both of these are unavailable in real-time
systems due to space constraints, as explained earlier. In addition, many realtime
systems lack sufficient space to support peripheral disk drives or graphical
displays, although some systems may support file systems using nonvolatile
memory (NVRAM). Third, supporting features common in standard desktop
19.3
P=L
relocation
register
R
p
physical
memory
Figure 19.2 Address translation in real-time systems.
763
computing environments would greatly increase the cost of real-time systems,
which could make such systems economically impractical.
Additional considerations arise when we consider virtual memory in a
real-time system. Providing virtual memory features as described in Chapter
9 requires that the system include a memory-management unit (MMU) for
translating logical to physical addresses. However, MMUs typically increase
the cost and power consumption of the system. In addition, the time required
to translate logical addresses to physical addresses-especially in the case of a
translation look-aside buffer (TLB) miss-may be prohibitive in a hard real-time
environment. In the following discussion, we examine several approaches for
translating addresses in real-time systems.
Figure 19.2 illustrates three different strategies for managing address
translation available to designers of real-time operating systems. In this
scenario, the CPU generates logical address L, which must be mapped to
physical address P. The first approach is to bypass logical addresses and
have the CPU generate physical addresses directly. This teclmique-kn.own
as real-addressing mode-does not employ virtual memory techniques and
is effectively stating that P equals L. One problem with real-addressil1.g mode
is the absence of memory protection between processes. Real-addressing mode
may also require that programmers specify the physical location where their
programs are loaded into memory. However, the benefit of this approach
is that the system is quite fast, as no time is spent on address translation.
Real-addressing mode is quite common in embedded systems with hard
real-time constraints. In fact, some real-time operating systems rum1.ing on
microprocessors containing an MMU actually disable the MMU to gain the
performance benefit of referencing physical addresses directly.
A second strategy for translating addresses is to use an approach similar
to the dynamic relocation register shown in Figure 8.4. In this scenario, a
relocation register R is set to the memory location where a program is loaded.
The physical address P is generated by adding the contents of the relocation
register R to L. Some real-time systems configure the MMU to perform this way.
The obvious benefit of this strategy is that the MMU can easily translate logical
addresses to physical addresses using P = L + R. However, this system still
suffers from a lack of memory protection between processes.
764 Chapter 19
19.4
The last approach is for the real-time system to provide full virtual memory
functionality as described in Chapter 9. In this instance, address translation
takes place via page tables and a translation look-aside buffer, or TLB. In
addition to allowing a program to be loaded at any memory location, this
strategy also provides memory protection between processes. For systems
without attached disk drives, demand paging and swapping may not be
possible. However, systems may provide such features using NVRAM flash
memory. The LynxOS and On Core Systems are examples of real-time operating
systems providing full support for virtual memory.
Keeping in mind the many possible variations, we now identify the features
necessary for implementing a real-time operating system. This list is by no
means absolute; some systems provide more features than we list below, while
other systems provide fewer.
Preemptive, priority-based scheduling
Preemptive kernel
Minimized latency
One notable feature we omit from this list is networking support. However,
deciding whether to support networking protocols such as TCP /IP is
simple: if the real-time system must be connected to a network, the operating
system must provide networking capabilities. For example, a system that
gathers real-time data and transmits it to a server must obviously include
networking features. Alternatively, a self-contained embedded system requiring
no interaction with other computer systems has no obvious networking
requirencent.
In the remainder of this section, we examine the basic requirements listed
above and identify how they can be implemented in a real-time operating
system.
19.4.1 Priority-Based Scheduling
The most important feature of a real-time operating system is to respond
immediately to a real-time process as soon as that process requires the CPU.
As a result, the scheduler for a real-time operating system must support a
priority-based algorithm with preemption. Recall that priority-based scheduling
algorithms assign each process a priority based on its importance; more
important tasks are assigned higher priorities than those deemed less important.
If the scheduler also supports preemption, a process currently running
on the CPU will be preempted if a higher-priority process becomes available to
run.
Preemptive, priority-based scheduling algorithms are discussed in detail
in Chapter 5, where we also present examples of the soft real-time scheduling
features of the Solaris, Windows XP, and Linux operating systems. Each of
these systems assigns real-time processes the highest scheduling priority. For
19.4 765
example, Windows XP has 32 different priority levels; the highest levelspriority
values 16 to 31-are reserved for real-time processes. Solaris and
Linux have similar prioritization schemes.
Note, however, that providing a preemptive, priority-based scheduler only
guarantees soft real-time functionality. Hard real-time systems must further
guarantee that real-time tasks will be serviced in accord with their deadline
requirem~ents, and making such guarantees may require additional scheduling
features. In Section 19.5, we cover scheduling algorithms appropriate for hard
real-time systems.
19.4.2 Preemptive Kernels
Nonpreemptive kernels disallow preemption of a process running in kernel
mode; a kernel-mode process will run until it exits kernel mode, blocks, or
voluntarily yields control of the CPU. In contrast, a preemptive kernel allows
the preemption of a task running in kernel mode. Designing preemptive
kernels can be quite difficult, and traditional user-oriented applications such
as spreadsheets, word processors, and Web browsers typically do not require
such quick response times. As a result, some commercial desktop operating
systems-such as Windows XP-are nonpreemptive.
However, to meet the timing requirements of real-time systems-in particular,
hard real-time systems-preemptive kernels are mandatory. Otherwise,
a real-time task might have to wait an arbitrarily long period of time while
another task was active in the kernel.
There are various strategies for making a kernel preemptible. One approach
is to insert preemption points in long-duration system calls. A preemption
point checks to see whether a high-priority process needs to be nm. If so, a
context switch takes place. Then, when the high-priority process terminates,
the interrupted process continues with the system call. Preemption points
can be placed only at safe locations in the kernel-that is, only where kernel
data structures are not being modified. A second strategy for making a kernel
preemptible is through the use of synchronization mechanisms, discussed in
Chapter 6. With this method, the kernel can always be preemptible, because any
kernel data being updated are protected from modification by the high-priority
process.
19.4.3 Minimizing Latency
Consider the event-driven nature of a real-time system. The system is typically
waiting for an event in real time to occur. Events may arise either in software
-as when a timer expires-or in hardware-as when a remote-controlled
vehicle detects that it is approaching an obstruction. When an event occurs, the
system must respond to and service it as quickly as possible. We refer to event
latency as the amount of time that elapses from when an event occurs to when
it is serviced (Figure 19.3).
Usually, different events have different latency requirements. For example,
the latency requirement for an antilock brake system might be three to five
milliseconds, meaning that from the time a wheel first detects that it is sliding,
the system controlling the antilock brakes has three to five milliseconds to
respond to and control the situation. Any response that takes longer might
result in the automobile's veering out of control. In contrast, an embedded
766 Chapter 19
event E first occurs
event latency
real-time system responds to E
Time
Figure 19.3 Event latency.
system controlling radar in an airliner might tolerate a latency period of several
seconds.
Two types of latencies affect the performance of real-time systems:
Interrupt latency
Dispatch latency
Interrupt latency refers to the period of time from the arrival of an interrupt
at the CPU to the start of the routine that services the interrupt. When an
interrupt occursf the operating system must first complete the instruction it
is executing and determine the type of interrupt that occurred. It must then
save the state of the current process before servicing the interrupt using the
specific interrupt service routine (ISR). The total time required to perform these
tasks is the interrupt latency (Figure 19.4). Obviouslyf it is crucial for real-time
task T running
interrupt
1 determine Or interrupt
type
O'witocontiex t
interrupt
latency
time
I ISH I
Figure 19.4 Interrupt latency.
19.4 767
operating systems to minimize interrupt latency to ensure that real-time tasks
receive immediate attention.
One important factor contributing to interrupt latency is the amomrt
of time interrupts may be disabled while kernel data structures are being
updated. Real-time operating systems require that interrupts be disabled for
very short periods of time. Howeve1~ for hard real-time systems, interrupt
latency must not only be minimized, it must in fact be bounded to guarantee
the deterministic behavior required of hard real-time kernels.
The amount of time required for the schedulil  g dispatcher to stop one
process and start another is known as dispatch latency. Providing real-time
tasks with immediate access to the CPU mandates that real-time operating
systems minimize this latency. The most effective technique for keeping
dispatch latency low is to provide preemptive kernels.
In Figure 19.5, we diagram the makeup of dispatch latency. The conflict
phase of dispatch latency has two components:
Preemption of any process running in the kernel
Release by low-priority processes of resources needed by a high-priority
process
As an example, in Solaris, the dispatch latency with preemption disabled is
over a hundred milliseconds. With preemption enabled, it is reduced to less
than a millisecond.
One issue that can affect dispatch latency arises when a higher-priority
process needs to read or modify kernel data that are currently beil  g accessed
by a lower-priority process-or a chain of lower-priority processes. As kernel
event response to event
~--------response interval--------+~
process made
interrupt available
processing
!+---- dispatch latency ----1~
_....,.
time
Figure 19.5 Dispatch latency.
real-time
process
execution
768 Chapter 19
19.5
data are typically protected with a lock, the higher-priority process will have to
wait  or a lower-priority one to finish with the resource. The situation becomes
more complicated i  the lower-priority process is preempted in favor of yet
another process with a higher priority. As an example, assume we have three
processes, L, M, and H, whose priorities follow the order L    M    H. Also
assume that process H requires resource R, which is currently being accessed
by process L. Ordinarily, process H would wait for L to finish using resource R.
Howevet~ now suppose that process M becomes runnable, thereby preempting
process L. Indirectly, a process with a lower priority -process M-has affected
how long process H must wait for L to relinquish resource R.
This problem, known as priority inversion, can be solved by use of the
priority-inheritance protocol. According to this protocol, all processes that
are accessing resources needed by a higher-priority process inherit the higher
priority until they are finished with the resources in question. When they
are finished, their priorities revert to their original values. In the example
above, a priority-inheritance protocol allows process L to temporarily inherit
the priority of process H, thereby preventing process M from preempting its
execution. When process L has finished using resource R, it relinquishes its
inherited priority from H and assumes its original priority. As resource R is
now available, process H -not M -will run next.
Our coverage of scheduling so far has focused primarily on soft real-time
systems. As mentioned, though, scheduling for such systems provides no
guarantee on when a critical process will be scheduled; it guarantees only that
the process will be given preference over noncritical processes. Hard real-time
systems have stricter requirements. A task must be serviced by its deadline;
service after the deadline has expired is the same as no service at all.
We now consider scheduling for hard real-time systems. Before we proceed
with the details of the individual schedulers, however, we must define certain
characteristics of the processes that are to be scheduled. First, the processes
are considered periodic. That is, they require the CPU at constant intervals
(periods). Each periodic process has a fixed processing timet once it acquires
the CPU, a deadline d by which time it must be serviced by the CPU, and a
period p. The relationship of the processing time, the deadline, and the period
can be expressed as 0 :::; t :::; d :::; p. The rate of a periodic task is 1 I p. Figure 19.6
p
d
period1
d
period2
d
~I
lc=J
Figure 19.6 Periodic task.
Time
period3
19.5 769
illustrates the execution of a periodic process over time. Schedulers can take
advantage of this relationship and assign priorities according to the deadline
or rate requirements of a periodic process.
What is unusual about this form of scheduling is that a process may have to
announce its deadline requirements to the scheduler. Then, using a technique
known as an admission-control algorithm, the scheduler either admits the
process, guaranteeing that the process will complete on time, or rejects the
request as impossible if it cannot guarantee that the task will be serviced by its
deadline.
In the following sections, we explore scheduling algorithms that address
the deadline requirements of hard real-time systems.
19.5.1 Rate-Monotonic Scheduling
The rate-monotonic scheduling algorithm schedules periodic tasks using a
static priority policy with preemption. If a lower-priority process is running
and a higher-priority process becomes available to run, it will preempt the
lower-priority process. Upon entering the system, each periodic task is assigned
a priority inversely based on its period. The shorter the period, the higher the
priority; the longer the period, the lower the priority. The rationale behind this
policy is to assign a higher priority to tasks that require the CPU more often.
Furthermore, rate-monotonic scheduling assumes that the processing time of
a periodic process is the same for each CPU burst. That is, every time a process
acquires the CPU, the duration of its CPU burst is the same.
Let's consider an example. We have two processes P1 and P2. The periods
for P1 and P2 are 50 and 100, respectively-that is, Pl =50 and P2 = 100. The
processil1.g times are t1 = 20 for P1 and t2 = 35 for P2 . The deadline for each
process requires that it complete its CPU burst by the start of its next period.
We must first ask ourselves whether it is possible to schedule these tasks
so that each meets its deadlines. If we measure the CPU utilization of a process
Pi as the ratio of its burst to its period -ti I Pi -the CPU utilization of P1 is
20j50 = 0.40 and that of P2 is 35/100 = 0.35, for a total CPU utilization of 75
percent. Therefore, it seems we can schedule these tasks in such a way that
both meet their deadlines and still leave the CPU with available cycles.
First, suppose we assign P2 a higher priority than P1. The execution of P1
and P2 is shown in Figure 19.7. As we can see, P2 starts execution first and
completes at time 35. At this point, P1 starts; it completes its CPU burst at time
55. However, the first deadline for P1 was at time 50, so the scheduler has
caused P1 to miss its deadline.
Now suppose we use rate-monotonic scheduling, in which we assign P1
a higher priority than P2, since the period of P1 is shorter than that of P2.
deadlines
0 1 0 20 30 40 50 60 70 80 90 1 00 11 0 120
Figure 19.7 Scheduling of tasks when P2 has a higher priority than P1 .
770 Chapter 19
deadlines
0 1 0 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200
Figure '19.8 Rate-monotonic scheduling.
The execution of these processes is shown in Figure 19.8. P1 starts first and
completes its CPU burst at time 20, thereby meeting its first deadline. P2 starts
running at this point and runs until time 50. At this time, it is preempted by
P1, although it still has 5 milliseconds remaining in its CPU burst. P1 completes
its CPU burst at time 70, at which point the scheduler resumes P2. P2 completes
its CPU burst at time 75, also meeting its first deadline. The system is idle until
time 100, when P1 is scheduled again.
Rate-monotonic scheduling is considered optimal in that if a set of
processes cannot be scheduled by this algorithm, it cannot be scheduled by
any other algorithm that assigns static priorities. Let's next examine a set
of processes that cannot be scheduled using the rate-monotonic algorithm.
Assume that process P1 has a period of p1 = 50 and a CPU burst of t1 = 25.
For P2, the corresponding values are P2 = 80 and t2 = 35. Rate-monotonic
scheduling would assign process P1 a higher priority, as it has the shorter
period. The total CPU utilization of the two processes is (25 /50) +(35 j80) = 0.94,
and it therefore seems logical that the two processes could be scheduled and still
leave the CPU with 6 percent available time. Figure 19.9 shows the scheduling
of processes P1 and P2 . Initially, P1 runs until it completes its CPU burst at
time 25. Process P2 then begins running and n.ms until time 50, when it is
preempted by P1. At this point, P2 still has 10 milliseconds remaining in its
CPU burst. Process P1 n.ms until time 75; consequently, P2 misses the deadline
for completion of its CPU burst at time 80.
Despite being optimal, then, rate-monotonic scheduling has a limitation:
CPU utilization is bounded, and it is not always possible to fully maximize CPU
resources. The worst-case CPU utilization for scheduling N processes is
2(21111 - 1).
With one process in the system, CPU utilization is 100 percent, but it falls to
approximately 69 percent as the number of processes approaches infinity. With
two processes, CPU utilization is bounded at about 83 percent. Combined CPU
utilization for the two processes scheduled in Figures 19.7 and 19.8 is 75 percent;
therefore, the rate-monotonic scheduling algorithm is guaranteed to schedule
deadlines
0 1 0 20 30 40 50 60 70 80 90 1 00 11 0 120 130 140 150 1 60
Figure 19.9 Missing deadlines with rate-monotonic scheduling.
19.5 771
them so that they can meet their deadlines. For the two processes scheduled in
Figure 19.9, combined CPU utilization is approximately 94 percent; therefore,
rate-monotonic scheduling cannot guarantee that they can be scheduled so
that they meet their deadlines.
19.5.2 Earliest-Deadline-First Scheduling
Earliest-deadline-first (EDF) scheduling dynamically assigns priorities according
to deadline. The earlier the deadline, the higher the priority; the later the
deadline, the lower the priority. Under the EDF policy, when a process becomes
runnable, it must announce its deadline requirements to the system. Priorities
may have to be adjusted to reflect the deadline of the newly rmmable process.
Note how this differs from rate-monotonic scheduling, where priorities are
fixed.
To illustrate EDF scheduling, we again schedule the processes shown in
Figure 19.9, which failed to meet deadline requirements under rate-monotonic
scheduling. Recall that P1 has values of p1 = 50 and t1 = 25 and that P2 has
values of p2 = 80 and t2 = 35. The EDF scheduling of these processes is shown
in Figure 19.10. Process P1 has the earliest deadline, so its initial priority is
higher than that of process P2   Process P2 begins rmming at the end of the
CPU burst for P1. However, whereas rate-monotonic scheduling allows P1 to
preempt P2 at the beginning of its next period at time 50, EDF scheduling allows
process P2 to continue running. P2 now has a higher priority than P1 because
its next deadline (at time 80) is earlier than that of P1 (at time 100). Thus, both
P1 and P2 meet their first deadlines. Process P1 again begins running at time 60
and completes its second CPU burst at time 85, also meeting its second deadline
at time 100. P2 begins rum1ing at this point only to be preempted by P1 at the
start of its next period at time 100. P2 is preempted because P1 has an earlier
deadline (time 150) than P2 (time 160). At time 125, P1 completes its CPU burst
and P2 resumes execution, finishing at time 145 and meeting its deadline as
well. The system is idle until time 150, when P1 is scheduled to run once again.
Unlike the rate-monotonic algorithm, EDF scheduling does not require that
processes be periodic, nor must a process require a constant amount of CPU
time per burst. The only requirement is that a process a1mom1ce its deadline
to the scheduler when it becomes runnable. The appeal of EDF scheduling is
that it is theoretically optimal-theoretically, it can schedule processes so that
each process can meet its deadline requirements and CPU utilization will be
100 percent. In practice, however, it is impossible to achieve this level of CPU
utilization due to the cost of context switching between processes and interrupt
handling.
deadlines
0 1 0 20 30 40 50 60 70 80 90 1 00 11 0 120 130 140 150 160
Figure 19.10 Earliest-deadline-first scheduling.
772 Chapter 19
19.5.3 Proportional Share Scheduling
Proportional share schedulers operate by allocating T shares among all
applications. An application can receive N shares of timef thus ensuring that
the application will have N 1 T of the total processor time. As an examplef
assume that a total ofT = 100 shares is to be divided among three processesf
Af B f and C. A is assigned 50 share sf B is assigned 15 sharesf and C is assigned
20 shares. This scheme ensures that A will have 50 percent o  total processor
time, B will have 15 percent, and C will have 20 percent.
Proportional share schedulers must work in conjunction with an admission
control policy to guarantee that an application receives its allocated shares
of time. An admission control policy will only admit a client requesting a
particular number of shares if sufficient shares are available. In our current
example, we have allocated 50+ 15 + 20 = 85 shares of the total of 100 shares.
If a new process D requested 30 shares, the admission controller would deny
D entry into the system.
19.5.4 Pthread Scheduling
The POSIX standard also provides extensions for real-time computingPOSIX.
1b. In this section, we cover some of the POSIX Pthread API related
to scheduling real-time threads. Pthreads defines two scheduling classes for
real-time threads:
SCHED __FIFO
SCHED_RR
SCHED__FIFO schedules threads according to a first-come, first-served policy
using a FIFO queue as outlined in Section 5.3.1. However, there is no time slicing
among threads of equal priority. Therefore, the highest-priority real-time thread
at the front of the FIFO queue will be granted the CPU until it terminates
or blocks. SCHED_RR (for round-robin) is sincilar to SCHED_FIFO except that
it provides time slicing among threads of equal priority. Pthreads provides
an additional scheduling class-SCHEDDTHER-but its implementation is
undefined and system specific; it may behave differently on different systems.
The Pthread API specifies the following two functions for getting and
setting the scheduling policy:
pthread_attr_getsched_policy(pthread_attr_t   attr, int
  policy)
pthread_attr_setsched_policy(pthread_attr_t   attr, int
policy)
The first parameter to both functions is a pointer to the set of attributes for
the thread. The second parameter is either (1) a pointer to an integer that is
set to the current scheduling policy (for pthread_attr_getsched_policy())
or (2) an integer value (SCHED_FIFO, SCHED_RR, or SCHED_OTHER) for the
pthread_attr setsched_policy () function. Both functions return non-zero
values if an error occurs.
19.5
#include   pthread.h  
#include   stdio.h  
#define NUM_THREADS 5
int main(int argc, char   argv[])
{
int i, policy;
pthread_t tid[NUM_THREADS];
pthread_attr_t attr;
I   get the default attributes   I
pthread_attr_init(&attr);
I   get the current scheduling policy   I
if (pthread_attr_getschedpolicy(&attr, &policy) != 0)
fprintf(stderr,   Unable to get policy.\n  );
else {
}
if (policy == SCHED_OTHER)
printf(  SCHED_OTHER\n  );
else if (policy == SCHED_RR)
printf(  SCHED_RR\n  );
else if (policy == SCHED_FIFO)
printf(  SCHED_FIFO\n  );
I   set the scheduling policy - FIFO, RR, or OTHER   I
773
if (pthread_attr_setschedpolicy(&attr, SCHED_OTHER) != 0)
fprintf (stderr,   Unable to set policy. \n  );
}
I   create the threads   I
for (i = 0; i    NUM_THREADS; i++)
pthread_create(&tid[i] ,&attr,runner,NULL);
I   now join on each thread   I
for (i = 0; i    NUM_THREADS; i++)
pthread_join(tid[i], NULL);
I   Each thread will begin control in this function   I
void   runner(void   param)
{
I   do some work ...   I
pthread_exi t (0);
}
Figure 19.11 Pthread scheduling API.
774 Chapter 19
19.6
In Figure 19.11, we illustrate a Pthread program using this API. This
program first determines the current scheduling policy and then sets the
scheduling algorithm to SCHEDDTHER.
In this section, we describe VxWorks, a popular real-time operating system
providing hard real-time support. VxWorks, commercially developed by Wind
River Systems, is widely used in automobiles, consumer and industrial devices,
and networking equipment such as switches and routers. VxWorks is also used
to control the two rovers-Spirit and Opportunity-that began exploring the
planet Mars in 2004.
The organizationofVxWorks is shown in Figure 19.12. VxWorks is centered
on the Wind microkernel. Recall from our discussion in Section 2.7.3 that
microkernels are designed so that the operating-system kernel provides a bare
minimum of features; additional utilities, such as networking, file systems,
and graphics, are provided in libraries outside of the kernel. This approach
offers many benefits, including minimizing the size of the kernel-a desirable
feature for an embedded system requiring a small footprint.
The Wind microkernel supports the following basic features:
Processes. The Wind microkernel provides support for individual processes
and threads (using the Pthread API). However, similar to Linux,
VxWorks does not distinguish between processes and threads, instead
referring to both as tasks.
embedded real-time application
Figure 19.12 The organization of VxWorks.
19.6 775
TheLinux .. operating .systent isbe~ng   us~cl incre~singlyinreai-time enviroft~
ments.yve . hav~ alreadycovered its softr~fil-ti,nteschedv,ling.fe~tur~s (Secti()n
5.6.3),whereby real-time tasl  sareassignt;d.thehj.ghe~tpriorityi~the s}Cst~m ..
AdditionaUeatures in the 2,6. release ofthe kernel makeLinux even,more
suitilb le. fot .embedded   systems.   Thes~  . features i.nclud~ ia. fullr pree ptive
kernel. an~la.  more .efficient. sched'-lli11g .. ~l~orith!fl(. 'Vhic}t P.ll1siJ.   . (){1) tinte
regardless ofthe number .  of tas~s .. ac.tive in the s~.stero ..  The2.6release also
::k;:r:ei~:::o~J~;t~~;;~~:~enthardlare arc~itecttlresbydi\TlclJng
Another.  strategy Jew. . integraH1lg Linux. iftto  reaNill1e .envirof\me.~ts
involves combini[\g th~  . Linux operat~S,.systt;IT1.  Vith a mallreal-.ti!fle~ernet
tht;reby  ..  providing .   a systell1    th~t ~cts as botl:-(. .  l    gene~al  7p~rp()s.e ..  (tnd
.~  real-tipte ~ystem. This .is theappro~c:h. tctken1:Jy.t}te ~~Lil_1Cux PPt;rat~g
systerrL In .RTLinux, the standard  Lirn.tx kernel run~ as a tas]   in ~ sll1ctll  
re~l-timeoperating ..   syste1n:. The. real~Htt1eikerl'lelJ)an~les  alliftterrupts.-,-directing
each    interrupt to  ct handler in the st~ndarclkerrtel.or  to anintef~
rupt.randler in. the real7tirl'le ketneL fi1Jrt,h_ermore/ ~TLin~x  . prevents the
sh'tndar\i.Linuxkernel . from ever disablb;J:g intefrl_lpts, t}tus.ens11rirtg th~t
itc  tl1rlotac1c11atencyto.thereal-timesystem.RJ:Lirtux.ctls0 pr()yi  ies difft;rent
schedulingpolicies(includingrate-'mo1lot  )I1:i('schec1ul~J1g(SecHonJ9.5.1)c;tnd
earliest~deadline-first scheduling (Section19;5.2}
Scheduling. The Wind microkernel provides two separate scheduling
models: preemptive scheduling and nonpreemptive round-robin scheduling
with 256 different priority levels. The scheduler also supports the POSIX
API for real-time threads( covered in Section 19.5.4.
Interrupts. The Wind microkernel also manages interrupts. To support
hard real-time requirements( interrupt and dispatch latency times are
bounded.
Interprocess communication. The Wind micro kernel provides both shared
memory and message passing as mechanisms for communication between
separate tasks. It also allows tasks to communicate using a technique
known as pipes-a mechanism that behaves in the same way as a FIFO
queue but allows tasks to communicate by writing to a special fik the pipe.
To protect data shared by separate tasks( VxWorks provides semaphores
and mutex locks with a priority inheritance protocol to prevent priority
mversion.
Outside the microkernet VxWorks includes several component libraries
that provide support for POSIX( Java( TCP /IP networking( and the like. All
components are optionat allowing the designer of an embedded system
to customize the system according to its specific needs. For example( if
networking is not required( the TCP /IP library can be excluded from the image
of the operating system. This strategy allows the operating-system designer to
776 Chapter 19
19.7
include only required features, thereby minimizing the size-or footprint-of
the operating system.
VxWorks takes an interesting approach to memory management, supporting
two levels of virtual memory. The first level, which is quite simple, allows
for control of the cache on a per-page basis. This policy enables an application
to specify certain pages as non-cacheable. When data are being shared by
separate tasks running on a multiprocessor architecture, it is possible that
shared data can reside in separate caches local to individual processors. Unless
an architecture supports a cache-coherency policy to ensure that the same
data residing in two caches will not be different, such shared data should not
be cached and should instead reside only in main memory so that all tasks
maintain a consistent view of the data.
The second level of virtual memory requires the optional virtual memory
component VxVMI (Figure 19.12), along with processor support for a memorymanagement
unit (MMU). When this optional component is loaded on a system
with an MMU, VxWorks allows a task to mark certain data areas as private. A
data area marked as private may only be accessed by the task it belongs to.
Furthermore, VxWorks allows pages containing kernel code along with the
interrupt vector to be declared as read-only. This is useful, as VxWorks does
not distinguish between user and kernel modes; all applications run in kernel
mode, giving an application access to the entire address space of the system.
A real-time system is a computer system requiring that results arrive within
a deadline period; results arriving after the deadline has passed are useless.
Many real-time systems are embedded in consumer and industrial devices.
There are two types of real-time systems: soft and hard real-time systems.
Soft real-time systems are the least restrictive, assigning real-time tasks higher
scheduling priority than other tasks. Hard real-time systems must guarantee
that real-time tasks are serviced within their deadline periods. In addition to
strict timing requirements, real-time systems can further be characterized as
having only a single purpose and running on small, inexpensive devices.
To meet timing requirements, real-time operating systems must employ
various techniques. The scheduler for a real-time operating system must support
a priority-based algorithm with preemption. Furthermore, the operating
system must allow tasks running in the kernel to be preempted in favor
of higher-priority real-time tasks. Real-time operating systems also address
specific timing issues by minimizing both interrupt and dispatch latency.
Real-time scheduling algorithms include rate-monotonic and earliestdeadline-
first scheduling. Rate-monotonic scheduling assignB tasks that
require the CPU more often a higher priority than tasks that require the
CPU less often. Earliest-deadline-first scheduling assigns priority according
to upcoming deadlines-the earlier the deadline, the higher the priority.
Proportional share scheduling uses the technique of dividing up processor
time into shares and assigning each process a number of shares, thus
guaranteeing each process its proportional share of CPU time. The Pthread API
provides various features for scheduling real-time threads as well.
777
19.1 Explain why interrupt and dispatch latency times must be bounded in
a hard real-time system.
19.2 Identify whether hard or soft real-time scheduling is more appropriate
in the following environments:
a. Thermostat in a household
b. Control system for a nuclear power plant
c. Fuel economy system in an automobile
d. Landing system in a jet airliner
19.3 Consider two processes, P1 and P2, where p1 = 50, t1 = 25, p2 = 75,
and t2 = 30.
a. Can these two processes be scheduled using rate-monotonic
scheduling  Illustrate your answer using a Gantt chart such as
the ones in Figures 19.7-19.10.
b. Illustrate the scheduling of these two processes using earliestdeadline-
first (EDF) scheduling.
19.4 Discuss ways in which the priority inversion problem could be
addressed in a real-time system. Also discuss whether the solutions
could be implemented within the context of a proportional share
scheduler.
19.5 Under what circumstances is rate-monotonic scheduling inferior to
earliest-deadline-first scheduling in meeting the deadlines associated
with processes 
The scheduling algorithms for hard real-time systems, such as rate monotonic
scheduling and earliest-deadline-first scheduling, are presented in Liu and
Layland [1973]. Other scheduling algorithms and extensions to previous
algorithms are presented in Jensen et al. [1985], Lehoczky et al. [1989], Audsley
et al. [1991], Mok [1983], and Stoica et al. [1996]. Mok [1983] describes a dynamic
priority-assignment algorithm called least-laxity-first scheduling. Stoica et al.
[1996] analyze the proportional share algorithm. Useful information regarding
various popular operating systems used in embedded systems can be obtained
from http:/ /rtlinux.org, http:/ /windriver.com, and http:/ /qnx.com. Future
directions and important research issues in the field of embedded systems are
discussed in a research article by Stankovic [1996].

In earlier chapters, we generally concerned ourselves with how operating
systems handle conventional data, such as text files, programs, binaries, wordprocessing
documents, and spreadsheets. However, operating systems may
have to handle other kinds of data as well. A relatively recent trend in
technology is the incorporation of multimedia data into computer systems.
Multimedia data consist of continuous-media (audio and video) data as well
as conventional files. Continuous-media data differ from conventional data
in that continuous-media data-such as frames of video-must be delivered
(streamed) according to certain time restrictions (for example, 30 frames per
second). In this chapter, we explore the demands of continuous-media data.
We also discuss in more detail how such data differ from conventional data
and how these differences affect the design of operating systems that support
the requirements of multimedia systems.
CHAPTER OBJECTIVES
  To identify the characteristics of multimedia data.
  To examine several. algorithms used to compress multimedia data.
  To explore the operating-:system requirements of multimedia data, including
CPU and disk scheduling and network management.
20.1 What Is Multimedia 
The term multimedia describes a wide range of applications that are in
popular use today. These include audio and video files such as MP3 audio
files, DVD movies, and short video clips of movie previews or news stories
downloaded over the Internet. Multimedia applications also include live
web casts (broadcast over the World Wide Web) of speeches or sporting
events and even live webcams that allow a viewer in Manhattan to observe
customers at a cafe in Paris. Multimedia applications need not be either audio
or video; rather, a multimedia application often includes a combination of
both. For example, a movie may consist of separate audio and video tracks.
779
780 Chapter 20
Nor must multimedia applications be delivered only to desktop personal
computers. Increasingly, they are being directed toward smaller devices,
including personal digital assistants (PDAs) and cellular telephones. For
example, a stock trader may have stock quotes delivered in real time to her
PDA.
In this section, we explore several characteristics of multimedia systems
and examine how multimedia files can be delivered from a server to a client
system. We also look at common standards for representing multimedia video
and audio files.
20.1.1 Media Delivery
Multimedia data are stored in the file system just like any other data. The major
difference between a regular file and a multimedia file is that the multimedia file
must be accessed at a specific rate, whereas accessing the regular file requires
no special timing. Let's use video as an example of what we mean by   rate.  
Video is represented by a series of images, formally known as that
are displayed in rapid succession. The faster the frames are displayed, the
smoother the video appears. In general, a rate of 24 to 30 frames per second is
necessary for video to appear smooth to human eyes. (The eye retains the image
of each frame for a short time after it has been presented, a characteristic known
as A rate of 24 to 30 frames per second is fast enough
to appear continuous.) A rate lower than 24 frames per second will result in
a choppy-looking presentation. The video file must be accessed from the file
system at a rate consistent with the rate at which the video is being
We refer to data with associated rate requirements as
Multimedia data may be delivered to a client either from the local file
system or from a remote server. When the data are delivered from the local file
system, we refer to the delivery as Examples include watching
a DVD on a laptop computer or listening to an MP3 audio file on a handheld
MP3 player. In these cases, the data comprise a regular file that is stored on
the local file system and played back (that is, viewed or listened to) from that
system.
Multimedia files may also be stored on a remote server and delivered to a
client across a network using a technique known as A client may be
a personal computer or a smaller device such as a handheld computer, PDA, or
cellular telephone. Data from live continuous media -such as live webcams
-are also streamed from a server to clients.
There are two types of streaming techniques: progressive download and
real-time streaming. With a download, a media file containing
audio or video is downloaded and stored on the client's local file system. As
the file is being downloaded, the client is able to play back the media file
without having to wait for the file to be downloaded in its entirety. Because
the media file is ultimately stored on the client system, progressive download
is most useful for relatively small media files, such as short video clips.
differs from progressive download in that the media
file is streamed to the client but is only played -and not stored -by the client.
Because the media file is not stored on the client system, real-time streaming
is preferable to progressive download for media files that might be too large
20.1 781
for storage on the system, such as long videos and Internet radio and TV
broadcasts.
Both progressive download and real-time streaming may allow a client to
move to different points in the stream, just as you can use the fast-forward and
rewind operations on a DVD controller to move to different points in the DVD
disc. For example, we could move to the end of a 5-minute streaming video or
replay a certain section of a movie clip. The ability to move around within the
media stream is known as
Two types of real-time streaming are available: live streaming and ondemand
streaming. is used to deliver an event, such as a concert
or a lecture, live as it is actually occurring. A radio program broadcast over the
Internet is an example of a live real-time stream. In fact, one of the authors of
this text regularly listens to a favorite radio station from Vermont while at his
home in Utah as it is streamed live over the Internet. Live real-time streaming is
also used for applications such as live webcams and video conferencing. Due to
its live delivery, this type of real-time streaming does not allow clients random
access to different poil1.ts in the media stream. In addition, live delivery means
that a client who wishes to view (or listen to) a particular live stream already
in progress will   join   the session   late;' thereby missing earlier portions of
the stream. The same thing happens with a live TV or radio broadcast. If you
start watching the 7:00P.M. news at 7:10P.M., you will have missed the first 10
minutes of the broadcast.
On-demand streaming is used to deliver media streams such as full-length
movies and archived lectures. The difference between live and on-demand
streaming is that on-demand streaming does not take place as the event is
occurring. Thus, for example, whereas watching a live stream is like watching
a news broadcast on TV, watching an on-demand stream is like viewing a movie
on a DVD player at some convenient time-there is no notion of arriving late.
Depending on the type of on-demand streaming, a client may or may not have
random access to the stream.
Examples of well-known streaming media products include RealPlayer,
Apple QuickTime, and Windows Media Player. These products include both
servers that stream the media and client media players that are used for
playback.
20.1.2 Characteristics of Multimedia Systems
The demands of multimedia systems are unlike the demands of traditional
applications. In general, multimedia systems may have the following characteristics:
Multimedia files can be quite large. For example, a 100-minute MPEG-1
video file requires approximately 1.125GB of storage space; 100 minutes
of high-defuution television (HDTV) requires approximately 15 GB of
storage. A server storing hundreds or thousands of digital video files
may thus require several terabytes of storage.
Continuous media may require very high data rates. Consider digital
video, in which a frame of color video is displayed at a resolution of
800 x 600. If we use 24 bits to represent the color of each pixel (which
allows us to have 224, or roughly 16 million, different colors), a single
782 Chapter 20
20.2
frame requires 800 x 600 x 24 = 11,520, 000 bits of data. If the frames are
displayed at a rate of 30 frames per second, a bandwidth in excess of 345
Mbps is required.
Multimedia applications are sensitive to timing delays during playback.
Once a continuous-media file is delivered to a client, delivery must
continue at a certain rate during playback of the media; otherwise, the
listener or viewer will be subjected to pauses during the presentation.
20.1.3 Operating-System Issues
For a computer system to deliver continuous-media data, it must guarantee
the specific rate and timing requirements-also known as of
or QoS, requirements-of continuous media.
Providing these QoS guarantees affects several components in a computer
system and influences such operating-system issues as CPU scheduling,
disk scheduling, and network management. Specific examples include the
following:
Compression and decoding may require significant CPU processing.
Multimedia tasks must be scheduled with certain priorities to ensure
meeting the deadline requirements of continuous media.
Similarly, file systems must be efficient to meet the rate requirements of
continuous media.
Network protocols must support bandwidth requirements while minimizing
delay and jitter (which we discuss further later in the chapter).
In later sections, we explore these and several other issues related to QoS.
First, however, we provide an overview of various techniques for compressing
multimedia data. As suggested above, compression makes significant demands
on the CPU.
Because of the size and rate requirements of multimedia systems, multimedia
files are often compressed from their original form to a much smaller form.
Once a file has been compressed, it takes up less space for storage and can be
delivered to a client more quickly. Compression is particularly important when
the content is beilcg streamed across a network cormection. In discussing file
compression, we often refer to the which is the ratio of the
original file size to the size of the compressed file. For example, an 800-KB file
that is compressed to 100 KB has a compression ratio of 8:1.
Once a file has been compressed (encoded), it must be decompressed
before it can be accessed. A feature of the algorithm used to compress
the file affects the later decompression. Compression algorithms are classified
as either or With lossy compression, some of the original data
are lost when the file is decoded, whereas lossless compression ensures that
20.2 783
the compressed file can always be restored to its original form. In generat
lossy techniques provide much higher cone pression ratios. Obviously, though,
only certain types of data can tolerate lossy compression-namely, images,
audio, and video. Lossy compression algorithms often work by eliminating
certain data, such as very high or low frequencies that a human ear cannot
detect. Some lossy compression algorithms used on video operate by storing
only the differences between successive frames. Lossless algorithms are used
for compressing text files, such as computer programs (for example,
files), because we want to restore these compressed files to their state.
A number of different lossy compression schemes for continuous-media
data are commercially available. In this section, we cover one used by the
Moving Picture Experts Group, better known as MPEG.
MPEG refers to a set of file formats and compression standards for digital
video. Because digital video often contains an audio portion as well, each of the
standards is divided into three layers. Layers 3 and 2 apply to the audio and
video portions of the media file. Layer 1, known as the layer, contains
timing information to allow the MPEG player to multiplex the audio and video
portions so that they are synchronized during playback. There are three major
MPEG standards: MPEG-1, MPEG-2, and MPEG-4.
MPEG-1 is used for digital video and its associated audio stream. The
resolution of MPEG-1 is 352 x 240 at 30 frames per second with a bitrate of up to
1.5 Mbps. This provides a quality slightly lower than that of conventional VCR
videos. MP3 audio files (a popular medium for storing music) use the audio
layer (layer 3) of MPEG-1. For video, MPEG-1 can achieve a compression ratio of
up to 200:1, although in practice compression ratios are much lower. Because
MPEG-1 does not require high data rates, it is often used to download short
video clips over the Internet.
MPEG-2 provides better quality than MPEG-1 and is used for compressing
DVD movies and digital television (including high-definition television, or
HDTV). MPEG-2 identifies a number of levels and profiles of video compression.
The refers to the resolution of the video; the characterizes the
video's quality. In general, the higher the level of resolution and the better
the quality of the video, the higher the required data rate. Typical bit rates
for MPEG-2 encoded files are 1.5 Mbps to 15 Mbps. Because MPEG-2 requires
higher rates, it is often Lmsuitable for delivery of video across a network and
is generally used for local playback.
MPEG-4 is the most recent of the standards and is used to transmit
audio, video, and graphics, including two-dimensional and three-dimensional
animation layers. Animation makes it possible for end users to interact with
the file during playback. For example, a potential home buyer can download
an MPEG-4 file and take a virtual tour through a home she is considering
purchasing, moving from room to room as she chooses. Another appealing
feature of MPEG-4 is that it provides a scalable level of quality, allowing delivery
over relatively slow network connections such as 56-I  bps modems or over
high-speed local area networks with rates of several megabits per second.
Furthermore, by providing a scalable level of quality, MPEG-4 audio and video
files can be delivered to wireless devices, including handheld computers, PDAs,
and cell phones.
All three MPEG standards discussed here perform lossy compression
to achieve high compression ratios. The fundamental idea behind MPEG
784 Chapter 20
20.3
compression is to store the differences between successive frames. We do not
cover further details of how MPEG performs compression but rather encourage
the interested reader to consult the bibliographical notes at the end of this
chapter.
As a result of the characteristics described in Section 20.1.2, multimedia
applications often require levels of service from the operating system that differ
from the requirements of traditional applications, such as word processors,
compilers, and spreadsheets. Tin1.ing and rate requirements are perhaps the
issues of foremost concern, as the playback of audio and video data demands
that the data be delivered within a certain deadline and at a continuous,
fixed rate. Traditional applications typically do not have such time and rate
constraints.
Tasks that request data at constant intervals-or known as
'    '--;~  '    '  '  For example, an MPEG-1 video might require a rate of 30
frames per second during playback. Maintaining this rate requires that a frame
be delivered approximately every l/301h, or 3.34 hundredths, of a second. To
put this in the context of deadlines, let's assume that frame F1 succeeds frame
Fi in the video playback and that frame Fi was displayed at time T0. The
deadline for displaying frame F i is 3.34 hundredths of a second after time T0. If
the operating system is unable to display the frame by this deadline, the frame
will be omitted from the stream.
As mentioned earlier, rate requirements and deadlines are known as quality
of service (QoS) requirements. There are three QoS levels:
The system makes a best-effort attempt to satisfy the
requirements; however, no guarantees are made.
This level treats different types of traffic in different ways, giving
certain traffic streams higher priority than other streams. However, just
as with best-effort service, no guarantees are made.
The quality-of-service requirements are guaranteed.
Traditional operating systems-the systems we have discussed in this
text so far-typically provide only best-effort service and rely on       ',..'  '  '    
that is, they simply assume that the total amount of resources
available will tend to be larger than a worst-case workload would demand. If
demand exceeds resource capacity, manual intervention must take place, and
a process (or several processes) must be removed from the system. However
next-generation multimedia systems cannot make such assumptions. These
systems must provide continuous-media applications with the guarantees
made possible by hard QoS. Therefore, in the remainder of this discussion,
when we refer to QoS, we mean hard QoS. Next, we explore various techniques
that enable multimedia systems to provide such service-level guarantees.
There are a number of parameters defining QoS for multimedia applications,
including the following:
20.3 785
Throughput is the total amount of work done during a certain
interval. For multimedia applications, throughput is the required data rate.
. Delay refers to the elapsed time from when a request is first
submitted to when the desired result is produced. For example, the time
from when a client requests a media stream to when the stream is delivered
is the delay.
Jitter is related to delay; but whereas delay refers to the time a
client must wait to receive a stream, jitter refers to delays that occur
during playback of the stream. Certain multimedia applications, such
as on-demand real-time streaming, can tolerate this sort of delay. Jitter
is generally considered unacceptable for continuous-media applications,
howeve1~ because it may mean long pauses-or lost frames-during
playback. Clients can often compensate for jitter by buffering a certain
amount of data-say, 5 seconds' worth-before beginning playback.
Reliability refers to how errors are handled during transmission
and processing of continuous media. Errors may occur due to lost
packets in the network or processing delays by the CPU. In these-and
other-scenarios, errors cannot be corrected, since packets typically arrive
too late to be useful.
The quality of service may be between the client and the server.
For example, continuous-media data may be compressed at different levels of
quality: the higher the quality, the higher the required data rate. A client may
negotiate a specific data rate with a server, thus agreeing to a certain level of
quality during playback. Furthermore, many media players allow the client
to configure the player according to the speed of the client's connection to
the network. This allows a client to receive a streaming service at a data rate
specific to a particular connection. Thus, the client is negotiating quality of
service with the content provider.
To provide QoS guarantees, operating systems often use
which is simply the practice of admitting a request for service only if the server
has sufficient resources to satisfy the request. We see admission control quite
often in our everyday lives. For example, a movie theater only admits as
many customers as it has seats in the theater. (There are also many situations in
everyday life where admission control is not practiced but would be desirable!)
If no admission-control policy is used in a multimedia environment, the
demands on the system might become so great that the system becomes unable
to meet its QoS guarantees.
In Chapter 6, we discussed using semaphores as a method of implementing
a simple admission-control policy. In this scenario, there exist a finite number
of nonshareable resources. When a resource is requested, we grant the request
only if sufficient resources are available; otherwise, the requesting process must
wait until a resource becomes available. We can use semaphores to implement
an admission-control policy by first initializing a semaphore to the number of
resources available. Every request for a resource is made through a wait ()
operation on the semaphore; a resource is released with an invocation of
signal() on the semaphore. Once all resources are in use, subsequent calls to
wait() block until there is a corresponding signal().
786 Chapter 20
20.4
Figure 20.1 Resources on a file server.
A common technique for implementing admission control is to use
For example, resources on a file server may include
the CPU, memory, file system, devices, and network (Figure 20.1). Note that
resources may be either exclusive or shared and that there may be either
single or multiple instances of each resource type. To use a resource, a client
must make a reservation request for the resource il1. advance. If the request
cannot be granted, the reservation is denied. An admission-control scheme
assigns a to each type of resource. Requests for resources
have associated QoS requirements-for example, required data rates. When a
request for a resource arrives, the resource manager determines if the resource
can meet the QoS demands of the request. If it cannot, the request may
be rejected, or a lower level of QoS may be negotiated between the client
and the server. If the request is accepted, the resource manager reserves the
resources for the requesting client, thus assuring the client that the desired QoS
requirements will be met. In Section 20.7.2, we examine the admission-control
algorithm used to ensure QoS guarantees in the CineBlitz multimedia storage
server.
19, which covers real-time systems, we distinguished between
and Soft real-time systems
simply give scheduling priority to critical processes. A soft real-time system
ensures that a critical process will be given preference over a noncritical process
but provides no guarantee as to when the critical process will be scheduled.
A typical requirement of continuous media, however, is that data must be
delivered to a client by a certain deadline; data that do not arrive by the deadline
are unusable. Multimedia systems thus require hard real-time scheduling to
ensure that a critical task will be serviced withii1. a guaranteed period of time.
20.5
20.5 787
Another scheduling issue concerns whether a scheduling algorithm uses
or distinction first discussed in Chapter
5. The difference between the two is that the priority of a process will remain
unchanged if the scheduler assigns it a static priority. Scheduling algorithms
that assign dynamic priorities allow priorities to change over time. Most
operating systems use dynamic priorities when scheduling non-real-time tasks
with the intention of giving higher priority to interactive processes. However,
when scheduling real-time tasks, most systems assign static priorities, as the
design of the scheduler is less complex.
Several of the real-time scheduling strategies discussed in Section 19.5 can
be used to meet the rate and deadline QoS requirements of continuous-media
applications.
We first discussed disk scheduling in Chapter 12. There, we focused primarily
on systems that handle conventional data; for these systems, the scheduling
goals are fairness and throughput. As a result, most traditional disk schedulers
employ some form of the SCAN (Section 12.4.3) or C-SCAN (Section 12.4.4)
algorithn  l.
Continuous-media files, however, have two constraints that conventional
data files generally do not have: timing deadlines and rate requirements.
These two constraints must be satisfied to preserve QoS guarantees, and diskscheduling
algorithms must be optimized for the constraints. Unfortunately,
these two constraints are often in conflict. Continuous-media files typically
require very high disk-bandwidth rates to satisfy their data-rate requirements.
Because disks have relatively low transfer rates and relatively high latency
rates, disk schedulers must reduce the latency times to ensure high bandwidth.
However, reducing latency times may result in a scheduling policy that does
not prioritize according to deadlines. In this section, we explore two diskscheduling
algorithms that meet the QoS requirements for continuous-media
systems.
20.5.1 Earliest-Deadline-First Scheduling
We first presented the earliest-deadline-first (EDF) algorithm in Section 19 .5.2 as
an example of a CPU-scheduling algorithm that assigns priorities according to
deadlines. EDF can also be used as a disk-scheduling algorithm; in this context,
EDF uses a queue to order requests according to the time each request must
be completed (its deadline). EDF is similar to shortest-seek-time-first (SSTF),
discussed in Section 12.4.2, except that instead of servicing the request closest
to the current cylinder, we service requests according to deadline-the request
with the closest deadline is serviced first.
A problem with this approach is that servicing requests strictly according
to deadline may result in higher seek tim.es, since the disk heads may move
randomly throughout the disk without any regard to their current position.
For example, suppose a disk head is currently at cylinder 75 and the queue
of cylinders (ordered according to deadlines) is 98, 183, 105. Under strict EDF
scheduling, the disk head will move from 75, to 98, to 183, and then back to
788 Chapter 20
105. Note that the head passes over cylinder 105 as it travels from 98 to 183. It
is possible that the disk scheduler could have serviced the request for cylinder
105 en route to cylinder 183 and still preserved the deadline requirement for
cylinder 183.
20.5.2 SCAN-EDF Scheduling
The fundamental problem with strict EDF scheduling is that it ignores the
position of the read-write heads of the disk; it is possible that the heads will
swil  g wildly to and fro across the disk, leading to unacceptable seek times
that negatively affect disk throughput. Recall that this is the same issue faced
with FCFS scheduling (Section 12.4.1). In the context of CPU scheduling, we can
address this issue by adopting SCAN schedulil  g, whereil   the disk arm moves
in one direction across the disk, servicing requests according to their proximity
to the current cylinder. Once the disk arm reaches the end of the disk, it begins
moving in the reverse direction. This strategy optimizes seek times.
SCAN-EDF is a hybrid algorithm that combines EDF with SCAN scheduling.
SCAN-EDF starts with EDF ordering but services requests with the same deadline
usil  g SCAN order. What if several requests have different deadlines that are
relatively close together  In this case, SCAN-EDF may batch requests, usil  g
SCAN ordering to service requests in the same batch. There are many techniques
for batching requests with similar deadlines; the only requirement is that
reordering requests within a batch must not prevent a request from being
serviced by its deadline. If deadlines are equally distributed, batches can be
organized in groups of a certain size-say, 10 requests per batch.
Another approach is to batch requests whose deadlines fall within a given
time threshold-say, 100 milliseconds. Let's consider an example in which we
batch requests in this way. Assume we have the followil  g requests, each with
a specified deadline (in milliseconds) and a requested cylinder:
~;''  ;  !1:-'-:~:~  ' ,,;~[ ;  , .. ,. '.:r' k'j~ lr  . ,. : ,, . .. ,   /1:-1
r ~'i! ~-~~~' k ;. ci ; h     .
A 150 25
B 201 112
c 399 95
D 94 31
E 295 185
F 78 85
G 165 150
H 125 101
I 300 85
J 210 90
Suppose we are at time0, the cylinder currently being serviced is 50, and the
disk head is moving toward cylinder 51. According to our batching scheme,
requests D and F will be in the first batch; A G, and H in batch 2; B, E, and
J il   batch 3; and C and I il   the last batch. Requests within each batch will
20.6
789
be ordered according to SCAN order. Thus, in batch 1, we will first service
request F and then request D. Note that we are moving downward in cylinder
nun1.bers, fron   85 to 31. In batch 2, we first service request A; then the heads
begin moving upward in cylinders, servicing requests Hand then G. Batch 3
is serviced in the order E, B, J. Requests I and Care serviced in the final batch.
Perhaps the foremost QoS issue with multimedia systems concerns preserving
rate requirements. For example, if a client wishes to view a video compressed
with MPEG-1, the quality of service greatly depends on the system's ability to
deliver the frames at the required rate.
Our coverage of issues such as CPU- and disk-scheduling algorithms has
focused on how these techniques can be used to better meet the quality-ofservice
requirements of multimedia applications. However, if the media file is
being streamed over a network-perhaps the Internet-issues relating to how
the network delivers the multimedia data can also significantly affect how QoS
demands are met. In this section, we explore several network issues related to
the unique demands of continuous media.
Before we proceed, it is worth noting that computer networks in general
-and the Internet in particular- currently do not provide network protocols
that can ensure the delivery of data with timing requirements. (There are
some proprietary protocols-notably those running on Cisco routers-that
do allow certain network traffic to be prioritized to meet QoS requirements.
Such proprietary protocols are not generalized for use across the Internet and
therefore do not apply to our discussion.)
When data are routed across a network, it is likely that the transmission
will encounter congestion, delays, and other network traffic issues-issues
that are beyond the control of the originator of the data. For multimedia data
with timing requirements, any timing issues must be synchronized between
the end hosts: the server delivering the content and the client playing it back.
One protocol that addresses timing issues is the :real-thne
(rrrf'). RTP is an Internet standard for delivering real-time data,
including audio and video. It can be used for transporting media formats
such as MP3 audio files and video files compressed using MPEG. RTF does not
provide any QoS guarantees; rathe1~ it provides features that allow a receiver
to remove jitter introduced by delays and congestion in the network.
In following sections, we consider two other approaches for handling the
unique requirements of continuous media.
20.6.1 Unicasting and Multicasting
In general, there are three methods for delivering content from a server to a
client across a network:
The server delivers the content to a single client. If the content
is being delivered to more than one client, the server must establish a
separate unicast for each client.
790 Chapter 20
  '  -'  ',''''  'nh    -  The server delivers the content to all clients, regardless of
whether they wish to receive the content or not.
The server delivers the content to a group of receivers that
indicate they wish to receive the content; this method lies somewhere
between unicasting and broadcasting.
An issue with unicast delivery is that the server must establish a separate
unicast session for each client. This seems especially wasteful for live real-time
streaming, where the server must make several copies of the same content, one
for each client. Obviously, broadcasting is not always appropriate, as not all
clients may wish to receive the stream. (Suffice it to say that broadcasting is
typically only used across local area networks and is not possible across the
public Internet.)
Multicasting appears to be a reasonable compromise, since it allows the
server to deliver a single copy of the content to all clients indicating that they
wish to receive it. The difficulty with multicasting from a practical standpoint is
that the clients must be physically close to the server or to intermediate routers
that relay the content from the originating server. If the route from the server
to the client must cross intermediate routers, the routers must also support
multicasting. If these conditions are not met, the delays incurred during routing
may result in violation of the timing requirements of the continuous media. In
the worst case, if a client is connected to an intermediate router that does not
support multicasting, the client will be unable to receive the multicast stream
at all!
Currently, most streaming media are delivered across unicast channels;
however, multicasting is used in various areas where the organization of
the server and clients is known in advance. For example, a corporation with
several sites across a country may be able to ensure that all sites are connected
to multicasting routers and are within reasonable physical proximity to the
routers. The organization will then be able to deliver a presentation from the
chief executive officer using multicasting.
20.6.2 Real-Time Streaming Protocol
In Section 20.1.1, we described some features of streaming media. As we noted
there, users may be able to randomly access a media stream, perhaps replaying
or pausing, as they would with a DVD controller. How is this possible 
To answer this question, let's consider how streaming media are delivered
to clients. One approach is to stream the media from a standard Web server
using the hypertext transport protocol, or HTTP-the protocol used to deliver
documents from a Web server. Quite often, clients use a such
as QuickTime, RealPlaye1~ or Windows Media Player, to play media
streamed from a standard Web server. Typically, the client first requests a
  :,E:caJJ.u:: which contains the location (possibly identified by a uniform resource
locatm~ or URL) of the streaming media file. This metafile is delivered to the
client's Web browser, and the browser then starts the appropriate media player
according to the type of media specified by the metafile. For example, a Real
Audio stream would require the RealPlayer, while the Windows Media Player
would be used to play back streaming Windows media. The media player
then contacts the Web server and requests the streaming media. The stream
20.6 791
Figure 20.2 Streaming media from a conventional Web server.
is delivered from the Web server to the media player using standard HTTP
requests. This process is outlined in Figure 20.2.
The problem with delivering streaming media from a standard Web server
is that HTTP is considered a protocol; thus, a Web server does not
maintain the state (or status) of its connection with a client. As a result, it is
difficult for a client to pause during the delivery of streaming media content,
since pausing would require the Web server to know where in the stream to
begin when the client wished to resume playback
An alternative strategy is to use a specialized streaming server that
is designed specifically for streaming media. One protocol designed for
communication between streaming servers and media players is known as the
or . The significant advantage RTSP provides
over HTTP is a connection between the client and the server, which
allows the client to pause or seek to random positions in the stream~ during
playback. Delivery of streaming media using RTSP is similar to delivery using
HTTP (Figure 20.2) in that the metafile is delivered using a conventional Web
server. However, rather than a Web server, the streami.J.1.g media is delivered
from a streaming server using the RTSP protocol. The operation of RTSP is shown
in Figure 20.3.
RTSP defines several commands as part of its protocol; these commands
are sent from a client to an RTSP streaming server. The commands i.J.1.clude the
following:
. The server allocates resources for a client session .
. The server delivers a stream to a client session established from a
SETUP command.
The server suspends delivery of a stream but maintains the
resources for the session.
The server breaks down the connection and frees up resources
allocated for the session.
792 Chapter 20
20.7
Figure 20.3 Real-time streaming protocol (RTSP).
The commands can be illustrated with a state machine for the server, as shown
in Figure 20.4. As you can see in the figure, the RTSP server may be in one of
three states: and Transitions between these three states are
triggered when the server receives one of the RTSP commands from the client.
Using RTSP rather than HTTP for streaming media offers several other
advantages, but they are primarily related to networking issues and are
therefore beyond the scope of this text. We encourage interested readers to
consult the bibliographical notes at the end of this chapter for sources of further
information.
The CineBlitz multimedia storage server is a high-performance media server
that supports both continuous media with rate requirements (such as video
and audio) and conventional data with no associated rate requirements (such
as text and images). CineBlitz refers to clients with rate requirements as
whereas have no rate constraints. Cine Blitz
guarantees to meet the rate requirements of real-time clients by implementing
an admission controller, admitting a client only if there are sufficient resources
to allow data retrieval at the required rate. In this section, we explore the
CineBlitz disk-schedulu1.g and admission-control algorithms.
SETUP PLAY
TEAR DOWN PAUSE
Figure 20.4 Finite-state machine representing RTSP.
20.7 793
20.7.1 Disk Scheduling
The CineBlitz disk scheduler services requests in At the beginning of
each service cycle, requests are placed inC-SCAN (Section 12.4.4). Recall
from our earlier discussions of C-SCAN that the disk heads move from one end
of the disk to the other. However, rather than reversing direction when they
reach the end of the disk, as in pure SCAN disk scheduling (Section 12.4.3), the
disk heads move back to the beginr1ing of the disk.
20.7.2 Admission Control
The admission-control algorithm in Cil  eBlitz must monitor requests from
both real-time and non-real-time clients, ensuring that both classes of clients
receive service. Furthermore, the admission controller must provide the rate
guarantees required by real-time clients. To ensure fairness, only a fraction p of
time is reserved for real-time clients, while the remainder, 1- p, is set aside for
non-real-time clients. Here, we explore the admission controller for real-time
clients only; thus, the term client refers to a real-time client.
The admission controller in CineBlitz monitors various system resources,
such as disk bandwidth and disk latency, while keeping track of available
buffer space. The CineBlitz admission controller admits a client only if there
is enough available disk bandwidth and buffer space to retrieve data for the
client at its required rate.
CineBlitz queues requests for continuous media files, where
R1, R2, R3, ... , R11 are the requests and r; is the required data rate for a
given request R. Requests in the queue are served in cyclic order in rounds of
time-length T (T being tpically measured in seconds). The scheme is using a
technique known as double wherein a buffer is allocated for each
request R; of size 2 x T x r;.
During each cycle I, the server must for each request R j:
Retrieve the data from disk to buffer (I mod 2).
Transfer data from the ((I+ 1) mod 2) buffer to the client.
This process is illustrated in Figure 20.5. For N clients, the total buffer space B
required is
N L2 x T x r;.::: B.
i=l
(20.1)
The fundamental idea behil  d the admission controller in CineBlitz is to
bound requests for entry into the queue according to the following criteria:
The service time for each request is first estimated.
A request is admitted only if the sum of the estimated service times for
all admitted requests does not exceed the duration of service cycle T.
LetT x r; bits be retrieved during a cycle for each real-time client R; with rater;.
If R1, R2, ... , R11 are the clients currently active in the system, then the admission
controller must ensure that the total time for retrieving T x r1, T x r2, ... , T x r11
794 Chapter 20
0
f---1
double buffer
total buffer space (B)
Figure 20.5 Double buffering in CineBiitz.
bits for the corresponding real-time clients does not exceed T. We explore the
details of this admission policy in the remainder of this section.
If b is the size of a disk block, then the maximum number of disk blocks
thatcanberetrievedforrequest R1c duringeachcycleis I(T xr~c)/bl +1. The 1 in
this formula comes from the fact that if T x YJc is less than b, then it is possible
for T x r1c bits to span the last portion of one disk block and the beginning
of another, causing two blocks to be retrieved. We know that the retrieval of
a disk block involves (a) a seek to the track contailling the block and (b) the
rotational delay as the data in the desired track arrive under the disk head. As
described, CineBlitz uses a C-SCAN disk-scheduling algorithm, so disk blocks
are retrieved in the sorted order of their positions on the disk.
If tscek and trot refer to the worst-case seek and rotational delay times, the
maximum latency incurred for servicil1g N requests is
N ( T x r; )
2 X tseek + L 1-b-l + 1 X trot 
1=1
(20.2)
In this equation, the 2 x tseek component refers to the maximum disk-seek
latency incurred in a cycle. The second component reflects the sum of the
retrievals of the disk blocks multiplied by the worst-case rotational delay.
If the transfer rate of the disk is r dislu then the time to h ansfer T x Yic bits
of data for request R1c is (T x r~c)/rdisk  As a result, the total time for retrieving
T x r1 , T x r2, ... , T x r11 bits for requests R1 , R2, ... , R11 is the sum of equation
20.2 and
N T x r; t; raisk
(20.3)
Therefore, the admission controller in CineBlitz only admits a new client R if
at least 2 x T x r; bits of free buffer space are available for the client and the
following equation is satisfied:
N ( T x r; ) N T x r;
2 X tsee/c + L 1-b-l + 1 X trot + L ~ S T.
i=l i=l diOk
(20.4)
20.8
795
Multimedia applications are in common use in modern computer systems.
Multimedia files include video and audio filesf which may be delivered
to systems such as desktop computersf personal digital assistantsf and cell
phones. The primary distinction between multimedia data and conventional
data is that multimedia data have specific rate and deadline requiren  ents.
Because multimedia files have specific timing requirementsf the data must
often be compressed before delivery to a client for playback. Multimedia data
may be delivered either from the local file system or from a multimedia server
across a network connection using a technique known as streaming.
The timing requirements of multimedia data are known as qualityof-
service requirementsf and conventional operating systems often cannot
make quality-of-service guarantees. To provide quality of servicef multimedia
systems must provide a form of admission control whereby a system accepts a
request only if it can meet the quality-of-service level specified by the request.
Providing quality-of-service guarantees requires evaluating how an operating
system performs CPU schedulingf disk schedulingf and network management.
Both CPU and disk scheduling typically use the deadline requirements of
a continuous-media task as a scheduling criterion. Network management
requires the use of protocols that handle delay and jitter caused by the network
as well as allowing a client to pause or move to different positions in the stream
during playback.
20.1 Explain why the traditional Internet protocols for transmitting data are
not sufficient to provide the quality-of-service guarantees required for
a multimedia system. Discuss what changes are required to provide
the QoS guarantees.
20.2 Contrast unicastingf multicastingf and broadcasting as techniques for
delivering content across a computer network.
20.3 Assume that we wish to compress a digital video file using MPEG-1
technology. The target bit rate is 1.5 Mbps. If the video is displayed
at a resolution of 352 x 240 at 30 frames per second using 24 bits to
represent each colorf what is the necessary compression ratio to achieve
the desired bit rate 
20.4 Assume that a digital video file is being displayed at a rate of 30 frames
per second; the resolution of each frame is 640 x 480f and 24 bits are
being used to represent each color. Assuming that no compression is
being usedf what is the bandwidth necessary to deliver this file  Next
assuming that the file has been compressed at a ratio of 200: 1f what is
the bandwidth necessary to deliver the compressed file 
20.5 A multimedia application consists of a set containing 100 imagesf 10
minutes of videof and 10 minutes of audio. The compressed sizes of the
imagesf videof and audio are 500 MBf 550 MBf and 8 MBf respectively.
The images were compressed at a ratio of 15 : 1f and the video and
796 Chapter 20
audio were compressed at 200 : 1 and 10 : 1, respectively. What were
the sizes of the images, video, and audio before compression 
20.6 Which of the following types of real-time streaming applications can
tolerate delay  Which can tolerate jitter 
Live real-time streaming
On-demand real-time streaming
20.7 Distinguish between progressive download and real-time streaming.
20.8 The following table contains a number of requests with their associated
deadlines and cylinders. Requests with deadlines occurring within 100
milliseconds of each other will be batched. The disk head is currently
at cylinder 94 and is moving toward cylinder 95. If SCAN-EDF disk
scheduling is used, how are the requests batched together, and what is
the order of requests within each batch 
. o. ,J,c~;) ,;;:   . ::.;
:~:--:'  ':   !,
; .;.~,  '..Ji,;;_~:3
.  . ,., (~  tp~li~~~~.~ 
R1 57 77
R2 300 95
R3 250 25
R4 88 28
R5 85 100
R6 110 90
R7 299 50
R8 300 77
R9 120 12
R10 212 2
20.9 Repeat Exercise 20.8, but this time batch requests that have deadlines
occurring within 75 milliseconds of each other.
20.10 What operating principle is used by the Cine Blitz system in performing
admission control for requests for media files 
20.11 Consider two processes, P1 and P2, where p1 = 50, t1 = 25, p2 = 75,
and t2 = 30.
a. Can these two processes be scheduled using rate-monotonic
scheduling  Illustrate your answer using a Gantt chart.
b. Illustrate the schedulil1.g of these two processes using earliestdeadline-
first (EDF) scheduling.
797
Fuhrt [1994] provides a general overview of multimedia systems. Topics related
to the delivery of multimedia through networks can be found in Kurose
and Ross [2005]. Operating-system support for multimedia is discussed in
Steinmetz [1995] and Leslie et al. [1996]. Resource management for resources
such as processing capability and memory buffers is discussed in Mercer et al.
[1994] and Druschel and Peterson [1993]. Reddy and Wyllie [1994] give a
good overview of issues relating to the use of I/0 in a multimedia system.
Discussions regarding the appropriate programming model for developing
multimedia applications are presented in Regehr et al. [2000]. An admission
control system for a rate-monotonic scheduler is considered in Lauzac et al.
[2003]. Bolosky et al. [1997] present a system for serving video data and discuss
the schedule-management issues that arise in such a system. The details of
a real-time streaming protocol can be found at http:/ /www.rtsp.org. Tudor
[1995] gives a tutorial on MPEG-2. A tutorial on video compression techniques
can be found at http:/ /www.wave-report.com/tutorials/VC.htm.

Part Nine
We can now integrate the concepts described in this book by describing
real operating systems. Two such systems are covered in great detaiiLinux
and Windows XP. We chose Linux for several reasons: It is popular, it
is freely available, and it represents a full-featured UNIX system. This gives
a student of operating systems an opportunity to read-and modifyrea/
operating-system source code.
We also cover Windows XP in great detail. This recent operating
system from Microsoft is gaining popularity, not only in the stand-alonemachine
market, but also in the workgroup-server market. We chose
Windows XP because it provides an opportunity for us to study a modern
operating system that has a design and implementation drastically
different from those of UNIX.
In addition, we briefly discuss other highly influential operating systems.
We have chosen the order of presentation to highlight the similarities
and differences among the systems; it is not strictly chronological and
does not reflect the relative importance of the systems.
Finally, we provide on-line coverage of three other systems. The
FreeBSD system is another UNIX system. However, whereas Linux combines
features from several UNIX systems, FreeBSD is based on the BSD
model of UNIX. FreeBSD source code, like Linux source code, is freely
available. The Mach operating system is a modern operating system that
provides compatibility with BSD UNIX. Windows is another modern operating
system from Microsoft for Intel Pentium and later microprocessors;
it is compatible with MS-DOS and Microsoft Windows applications.

21.1
This chapter presents an in-depth examination of the Linux operating system.
By exam.ining a complete, real system, we can see how the concepts we have
discussed relate both to one another and to practice.
Linux is a version of UNIX that has gained popularity in recent years. In this
chapter, we look at the history and development of Linux and cover the user
and programmer interfaces that Linux presents-interfaces that owe a great
deal to the UNIX tradition. We also discuss the internal methods by which Linux
implements these interfaces. Linux is a rapidly evolving operating system.
This chapter describes developments through the Linux 2.6 kernel, which was
released in late 2003.
To explore the history of the UNIX operating system from which Linux is
derived and the principles upon which Linux is designed.
To examine the Linux process model and illustrate how Linux schedules
processes and provides interprocess communication.
To look at memory management in Linux.
To explore how Linux implements file systems and manages 110 devices.
Linux looks and feels much like any other UNIX system; indeed, UNIX
compatibility has been a major design goal of the Linux project. However,
Linux is much younger than most UNIX systems. Its development began in
1991, when a Finnish student, Linus Torvalds, wrote and christened Linux,
a small but self-contained kernel for the 80386 processor, the first true 32-bit
processor in Intel's range of PC-compatible CPUs.
Early in its development, the Linux source code was made available free
on the Internet. As a result, Linux's history has been one of collaboration by
many users from all around the world, corresponding almost exclusively over
the Internet. From an initial kernel that partially implemented a small subset of
801
802 Chapter 21
the UNIX system services, the Linux system has grown to include much UNIX
functionality.
In its early days, Linux development revolved largely around the central
operating-system kernel-the core, privileged executive that n1.anages all
system resources and that interacts directly with the computer hardware.
We need much more than this kernel to produce a full operating systeiTl,
o  course. It is useful to make the distinction between the Linux kernel and
a Linux system. The is an entirely original piece of software
developed from scratch by the Linux con1.munity. The as we
know it today, includes a multitude of components, some written from scratch,
others borrowed from other development projects, and still others created in
collaboration with other teams.
The basic Linux system is a standard environment for applications and
user programming, but it does not enforce any standard means of managing
the available functionality as a whole. As Linux has matured, a need has arisen
for another layer of functionality on top of the Linux system. This need has
been met by various Linux distributions. A includes all the
standard components of the Linux system, plus a set of administrative tools
to simplify the initial installation and subsequent upgrading of Linux and to
manage installation and removal of other packages on the system. A modern
distribution also typically includes tools for management of file systems,
creation and management of user accounts, administration of networks, Web
browsers, word processors, and so on.
21.1.1 The Linux Kernel
The first Linux kernel released to the public was Version 0.01, dated May
14, 1991. It had no networking, ran only on 80386-compatible Intel processors
and PC hardware, and had extremely limited device-driver support. The virtual
memory subsystem was also fairly basic and included no support for memorymapped
files; however, even this early incarnation supported shared pages
with copy-on-write. The only file system supported was the Minix file system
-the first Linux kernels were cross-developed on a Minix platform. However,
the kernel did implement proper UNIX processes with protected address spaces.
The next milestone version, Linux 1.0, was released on March 14, 1994.
This release culminated three years of rapid development of the LimlX kernel.
Perhaps the single biggest new feature was networking: 1.0 included support
for UNIX's standard TCP liP networking protocols, as well as a BSD-compatible
socket interface for networking programming. Device-driver support was
added for running IP over an Ethernet or (using PPP or SLIP protocols) over
serial lines or modems.
The 1.0 kernel also included a new, much enhanced file system without the
limitations of the original Minix file system and supported a range of SCSI controllers
for high-performance disk access. The developers extended the virtual
memory subsystem to support paging to swap files and memory mapping of
arbitrary files (but only read-only memory mapping was implemented in 1.0).
A range of extra hardware support was also included in this release.
Although still restricted to the Intel PC platform, hardware support had grown
to include floppy-disk and CD-ROM devices, as well as sound cards, a range
of mice, and international keyboards. Floating-point emulation was provided
21.1 803
in the kernel for 80386 users who had no 80387 math coprocessor; System
V UNIX-style inclLlding shared memory,
semaphores, and message queues, was implemented. Simple support for
dynamically loadable and unloadable kernel modules was supplied as well.
At this point, development started on the 1.1 kernel stream, but numerous
bug-fix patches were released subsequently against 1.0. A pattern was adopted
as the standard numbering convention for Linux kernels. Kernels with an odd
minor-version number, such as 1.1, 1.3, and2.1, are evennumbered
minor-version numbers are stable Updates
against the stable kernels are intended only as remedial versions, whereas the
development kernels may include newer and relatively untested functionality.
In March 1995, the 1.2 kernel was released. This release did not offer
nearly the same improvement in functionality as the 1.0 release, but it did
support a much wider variety of hardware, including the new PCI hardware
bus architecture. Developers added another PC-specific feature-support for
the 80386 CPU's virtual8086 mode-to allow emulation of the DOS operating
system for PC computers. They also updated the networking stack to provide
support for the IPX protocol and made the IP implementation more complete
by including accounting and firewalling functionality.
The 1.2 kernel was the final PC-only Linux kernel. The source distribution
for Linux 1.2 included partially implemented support for SPARC, Alpha, and
MIPS CPUs, but full integration of these other architectures did not begin until
after the 1.2 stable kernel was released.
The Linux 1.2 release concentrated on wider hardware support and more
complete implementations of existing functionality. Much new functionality
was under development at the time, but integration of the new code into the
main kernel source code had been deferred until after the stable 1.2 kernel had
been released. As a result, the 1.3 development stream saw a great deal of new
functionality added to the kernel.
This work was finally released as Linux 2.0 in Jmce 1996. This release
was given a major version-number increment on account of two major new
capabilities: support for multiple architectures, including a 64-bit native Alpha
port, and support for multiprocessor architectures. Linux distributions based
on 2.0 are also available for the Motorola 68000-series processors and for
Sun's SPARC systems. A derived version of Linux running on top of the Mach
microkernel also runs on PC and PowerMac systems.
The changes in 2.0 did not stop there. The memory-management code
was substantially improved to provide a unified cache for file-system data
independent of the caching of block devices. As a result of this change, the
kernel offered greatly increased file-system and virtual memory performance.
For the first time, file-system caching was extended to networked file systems,
and writable memory-mapped regions also were supported.
The 2.0 kernel also included much improved TCP /IP performance, and a
number of new networking protocols were added, including Apple Talk, AX.25
an'lateur radio networking, and ISDN support. The ability to mount remote
netware and SMB (Microsoft LanManager) network volumes was added.
Other major improvements in 2.0 were support for internal kernel threads,
for handling dependencies between loadable modules, and for automatic
loading of modules on demand. Dynamic configuration of the kernel at run
time was much improved through a new, standardized configuration interface.
804 Chapter 21
Additional new features included file-system quotas and POSIX-compatible
real-time process-scheduling classes.
Improvements continued with the release of Linux 2.2 in January 1999. A
port for UltraSPARC systems was added. Networking was enhanced with more
flexible firewalling, better routing and traffic management, and support for
TCP large window and selective acks. Acorn, Apple, and NT disks could now
be read, and NFS was enhanced and a kernel-mode NFS daemon added. Signal
handling, interrupts, and some I/0 were locked at a finer level than before to
improve symmetric multiprocessor (SMP) performance.
Advances in the 2.4 and 2.6 releases of the kernel include increased support
for SMP systems, journaling file systems, and enhancements to the memorymanagement
system. The process scheduler was modified in Version 2.6,
providing an efficient 0(1) scheduling algorithm. In addition, the LimiX 2.6
kernel is now preemptive, allowing a process to be preempted while running
in kernel mode.
21.1.2 The Linux System
In many ways, the Linux kernel forms the core of the Linux project, but other
components make up the complete Linux operating system. Whereas the Linux
kernel is composed entirely of code written from scratch specifically for the
Linux project, much of the supporting software that makes up the Linux
system is not exclusive to Linux but is common to a number of UNIX-like
operating systems. In particular, Linux uses many tools developed as part
of Berkeley's BSD operating system, MIT's X Window System, and the Free
Software Foundation's GNU project.
This sharing of tools has worked in. both directions. The main system
libraries of Linux were originated by the GNU project, but the Linux community
greatly improved the libraries by addressing omissions, li1.efficiencies, and
bugs. Other components, such as the L were already
of sufficiently high quality to be used directly in Linux. The networkingadministration
tools under Linux were derived from code first developed for
4.3 BSD, but more recent BSD derivatives, such as FreeBSD, have borrowed code
from Linux in return. Examples include the Intel floating-point-emulation math
library and the PC sound-hardware device drivers.
The Linux system as a whole is maintained by a loose network of
developers collaborating over the Internet, with small groups or individuals
having responsibility for maintaining the integrity of specific components.
A small number of public Internet file-transfer-protocol (FTP) archive sites
act as de facto standard repositories for these components. The
document is also maintained by the Linux community
as a means of ensuring compatibility across the various system components.
This standard specifies the overall layout of a standard Linux file system; it
determines under which directory names configuration files, libraries, system
binaries, and run-time data files should be stored.
21.1.3 Linux Distributions
In theory, anybody can install a Linux system by fetching the latest revisions
of the necessary system components from the FTP sites and compiling them.
In Linux's early days, this operation was often precisely what a Linux user
21.1 805
had to carry out. As Linux has matured, however, various individuals and
groups have attempted to make this job less painful by providing standard,
precompiled sets of packages for easy installation.
These collections, or distributions, include much more than just the
basic Linux system. They typically include extra system-installation and
management utilities, as well as precompiled and ready-to-install packages
of many of the common UNIX tools, such as news servers, Web browsers,
text-processing and editing tools, and even games.
The first distributions managed these packages by simply providing
a means of unpacking all the files into the appropriate places. One of
the important contributions of modem dish ibutions, however, is advanced
package management. Today' s Linux distributions include a package-tracking
database that allows packages to be installed, upgraded, or removed painlessly.
The SLS distribution, dating back to the early days of Linux, was the first
collection of Linux packages that was recognizable as a complete distribution.
Although it could be installed as a single entity, SLS lacked the packagemanagement
tools now expected of Linux distributions. The
distribution represented a great improvement in overall quality, even though
it also had poor package management; in fact, it is still one of the most widely
installed distributions in the Linux community.
Since Slackware' s release, many commercial and noncommercial LimlX
distributions have become available. and are particularly
popular distributions; the first comes from a commercial Linux support
company and the second from the free-software Linux community. Other
commercially supported versions of Linux include distributions from Cz,ldera,
and A large Linux following in Germany
has resulted in several dedicated German-language distributions, including
versions from and There are too many Linux distributions in
circulation for us to list all of them here. The variety of distributions does not
prohibit compatibility across Linux distributions, however. The RPM package
file format is used, or at least understood, by the majority of distributions, and
commercial applications distributed in this format can be installed and run on
any distribution that can accept RPM files.
21.1.4 Linux Licensing
The Linux kernel is distributed under the GNU general public license (GPL),
the terms of which are set out the Free Software Fmmdation. Linux is not
public-domain software. implies that the authors have waived
copyright rights in the software, but copyright rights in Linux code are still
held by the code's various authors. Linux is free software, however, in the sense
that people can copy it, modify it, use it in any manner they want, and give
away their own copies, without any restrictions.
The main implications of Linux's licensing terms are that nobody using
Liimx, or creating a derivative of Linux (a legitimate exercise), can make
the derived product proprietary. Software released under the GPL camwt be
redistributed as a binary-only product. If you release software that includes
any components covered by the GPL, then, under the GPL, you must make
source code available alongside any binary distributions. (This resh iction does
806 Chapter 21
21.2
not prohibit ntaking-or even selling-binary-only software distributions, as
long as anybody who receives binaries is also given the opportunity to get
source code for a reasonable distribution charge.)
In its overall design, Linux resembles any other traditional, nonmicrokernel
UNIX implementation. It is a multiuser, multitasking system with a full set
of UNIX-compatible tools. Linux's file system adheres to traditional UNIX
semantics, and the standard UNIX networking model is implemented fully.
The internal details of Linux' s design have been influenced heavily by the
history of this operating system's development.
Although Linux runs on a wide variety of platforms, it was developed
exclusively on PC architecture. A great deal of that early development was
carried out by individual enthusiasts, rather than by well-funded development
or research facilities, so from the start Linux attempted to squeeze as much
functionality as possible from limited resources. Today, Linux can run happily
on a multiprocessor machine with hundreds of megabytes of main memory
and many gigabytes of disk space, but it is still capable of operating usefully
in under 4 MB of RAM.
As PCs became more powerful and as memory and hard disks became
cheaper, the original, minimalist LimiX kernels grew to implement more
UNIX functionality. Speed and efficiency are still important design goals, but
much recent and current work on Linux has concentrated on a third major
design goal: standardization. One of the prices paid for the diversity of UNIX
implementations currently available is that source code written for one may not
necessarily compile or run correctly on another. Even when the same system
calls are present on two different UNIX systems, they do not necessarily behave
in exactly the same way. The POSIX standards comprise a set of specifications for
different aspects of operating-system behavior. There are POSIX documents for
common operating-system functionality and for extensions such as process
threads and real-time operations. Linux is designed to be compliant with
the relevant POSIX documents; at least two Linux distributions have achieved
official POSIX certification.
Because it gives standard interfaces to both the programmer and the user,
Linux presents few surprises to anybody familiar with UNIX. We do not detail
these interfaces here. The sections on the programmer interface (Section A.3)
and user interface (Section A.4) of BSD apply equally well to Linux. By default,
howeve1~ the Linux programming interface adheres to SVR4 UNIX semantics,
rather than to BSD behavior. A separate set of libraries is available to implement
BSD semantics in places where the two behaviors differ significantly.
Many other standards exist in the UNIX world, but full certification of
Linux with respect to these standards is sometimes slowed because certification
is often available only for a fee, and the expense involved in certifying an
operating system's compliance with most standards is substantial. However,
supporting a wide base of applications is important for any operating system,
so implementation of standards is a major goal for Linux development, even
if the implementation is not formally certified. In addition to the basic POSIX
21.2 807
standard, Linux currently supports the POSIX threading extensions-Pthreads
-and a subset of the POSIX extensions for real-time process control.
21.2.1 Components of a Linux System
The Linux system is composed of three main bodies of code, in line with most
traditional UNIX implementations:
Kernel. The kernel is responsible for maintaining all the important
abstractions of the operating system, including such things as virtual
memory and processes.
System libraries. The system libraries define a standard set of functions
through which applications can interact with the kernel. These functions
implement much of the operating-system functionality that does not need
the full privileges of kernel code.
System utilities. The system utilities are programs that perform individual,
specialized management tasks. Some system utilities may be invoked
just once to initialize and configure some aspect of the system; othersknown
as daemons in UNIX terminology -may run permanently, handling
such tasks as responding to incoming network connections, accepting
logon requests from terminals, and updating log files.
Figure 21.1 illustrates the various components that make up a full Linux
system. The most important distinction here is between the kernel and
everything else. All the kernel code executes in the processor's privileged
mode with full access to all the physical resources of the computer. Linux
refers to this privileged mode as kernel Under Linux, no user-mode
code is built into the kernel. Any operating-system-support code that does not
need to run in kernel mode is placed into the system libraries instead.
Although various modern operating systems have adopted a messagepassing
architecture for their kernel internals, Linux retains UN'  lX' s historical
model: the kernel is created as a single, monolithic binary. The main reason is
to improve performance. Because all kernel code and data structures are kept
iil a single address space, no context switches are necessary when a process
calls an operating-system function or when a hardware interrupt is delivered.
Linux kernel
loadable kernel modules
Figure 21.1 Components of the Linux system.
808 Chapter 21
This single address space contains not only the core scheduling and virtual
memory code but all kernel code, including all device drivers, file systems, and
networking code.
Even though all the kernel components share this same m.elting pot, there
is still room for modularity. In the san1.e way that user applications can load
shared libraries at run time to pull in a needed piece of code, so the Linux
kernel can load (and unload) modules dynamically at run time. The kernel
does not necessarily need to know in advance which modules may be loaded
-they are truly independent loadable components.
The Linux kernel forms the core of the Linux operating system. It provides
all the functionality necessary to run processes, and it provides system services
to give arbitrated and protected access to hardware resources. The kernel
implements all the features required to qualify as an operating system. On
its own, however, the operating system provided by the Linux kernel looks
nothing like a UNIX system. It is missing many of the extra features of UNIX,
and the features that it does provide are not necessarily in the format in which
a UNIX application expects them to appear. The operating-system interface
visible to running applications is not maintained directly by the kerneL Rather,
applications make calls to the system libraries, which in turn call the operatingsystem
services as necessary.
The system libraries provide many types of functionality. At the simplest
level, they allow applications to make kernel-system service requests. Making
a system call involves transferring control from tmprivileged user mode to
privileged kernel mode; the details of this transfer vary from architecture to
architecture. The libraries take care of collecting the system-call arguments and,
if necessary, arranging those arguments in the special form necessary to make
the system calL
The libraries may also provide more complex versions of the basic system
calls. For example, the C language's buffered file-handling functions are all
implemented in the system libraries, providing more advanced control of file
I/ 0 than the basic kernel system calls. The libraries also provide routines that do
not correspond to system calls at all, such as sorting algorithms, mathematical
functions, and string-manipulation routines. All the functions necessary to
support the running of UNIX or POSIX applications are implemented here in the
system libraries.
The LimiX system includes a wide variety of user-mode programs-both
system utilities and user utilities. The system utilities include all the programs
necessary to initialize the system, such as those to configure network devices
and to load kernel modules. Continually running server programs also com1.t as
system utilities; such programs handle user login requests, incoming network
connections, and the printer queues.
Not all the standard utilities serve key system-administration functions.
The UNIX user environment contains a large number of standard utilities to
do simple everyday tasks, such as listing directories, moving and deleting
files, and displaying the contents of a file. More complex utilities can perform
text-processing functions, such as sorting textual data and performing pattern
searches on input text. Together, these utilities form a standard tool set that
users can expect on any UNIX system; although they do not perform any
operating-system function, they are an important part of the basic LimiX
system.
21.3
21.3 809
The Linux kernel has the ability to load and unload arbitrary sections of kernel
code on demand. These loadable kernel modules run in privileged kernel mode
and as a consequence have full access to all the hardware capabilities of the
machine on which they run. In theory, there is no restriction on what a kernel
module is allowed to do; typically, a module might implement a device driver,
a file system, or a networking protocol.
Kernel modules are convenient for several reasons. Linux' s source code is
free, so anybody wanting to write kernel code is able to compile a modified
kernel and to reboot to load that new functionality; however, recompiling,
relinking, and reloading the entire kernel is a cumbersome cycle to undertake
when you are developing a new driver. If you use kernel modules, you do not
have to make a new kernel to test a new driver-the driver can be compiled
on its own and loaded into the already-rmming kernel. Of course, once a new
driver is written, it can be distributed as a module so that other users can
benefit from it without having to rebuild their kernels.
This latter point has another implication. Because it is covered by the
GPL license, the Linux kernel cannot be released with proprietary components
added to it, unless those new components are also released under the GPL and
the source code for them is made available on demand. The kernel's module
interface allows third parties to write and distribute, on their own terms, device
drivers or file systems that could not be distributed under the GPL.
Kernel modules allow a Linux system to be set up with a standard minimal
kernet without any extra device drivers built in. Any device drivers that
the user needs can be either loaded explicitly by the system at startup or
loaded automatically by the system on demand and unloaded when not in
use. For example, a CD-ROM driver might be loaded when a CD is mounted
and unloaded from memory when the CD is dismounted from the file system.
The module support under Linux has three components:
The allows modules to be loaded into memory and
to talk to the rest of the kernel.
The allows modules to tell the rest of the kernel that
a new driver has become available.
A allows different device drivers to
reserve hardware resources and to protect those resources from accidental
use by another driver.
21.3.1 Module Management
Loading a module requires more than just loading its binary contents into
kernel memory. The system must also make sure that any references the
module makes to kernel symbols or entry points are updated to point to the
correct locations in the kernel's address space. Linux deals with this reference
updating by splitting the job of module loading into two separate sections: the
management of sections of module code in kernel memory and the handling
of symbols that modules are allowed to reference.
810 Chapter 21
Linux maintains an internal syncbol table in the kernel. This symbol table
does not contain the full set of symbols defined in the kernel during the latter's
compilation; rather, a symbol must be exported explicitly by the kernel. The set
of exported symbols constitutes a well-defined interface by which a module
can interact with the kernel.
Although exporting symbols from a kernel function requires an explicit
request by the programmer, no special effort is needed to import those symbols
into a module. A module writer just uses the standard external linking of the
C language: Any external symbols referenced by the module but not declared
by it are simply marked as unresolved in the final module binary produced by
the compiler. When a module is to be loaded into the kernel, a system utility
first scans the module for these unresolved references. All symbols that still
need to be resolved are looked up in the kernel's symbol table, and the correct
addresses of those symbols in the currently running kernel are substituted into
the module's code. Only then is the module passed to the kernel for loading. If
the system utility cannot resolve any references in the module by looking them
up in the kernel's symbol table, then the module is rejected.
The loading of the module is performed in two stages. First, the moduleloader
utility asks the kernel to reserve a continuous area of virtual kernel
memory for the module. The kernel returns the address of the memory
allocated, and the loader utility can use this address to relocate the module's
machine code to the correct loading address. A second system call then passes
the module, plus any symbol table that the new module wants to export, to the
kernel. The module itself is now copied verbatim into the previously allocated
space, and the kernel's symbol table is updated with the new symbols for
possible use by other modules not yet loaded.
The final module-management component is the module requestor. The
kernel defines a communication interface to which a module-management
program can connect. With this connection established, the kernel will inform
the management process whenever a process requests a device driver, file
system, or network service that is not currently loaded and will give the
manager the opportunity to load that service. The original service request will
complete once the module is loaded. The manager process regularly queries
the kernel to see whether a dynamically loaded module is still in use and
unloads that module when it is no longer actively needed.
21.3.2 Driver Registration
Once a module is loaded, it remains no more than an isolated region of memory
until it lets the rest of the kernel know what new functionality it provides.
The kernel maintains dynamic tables of all known drivers and provides a
set of routines to allow drivers to be added to or removed from these tables
at any time. The kernel makes sure that it calls a module's startup routine
when that module is loaded and calls the module's cleanup routine before that
module is unloaded: these routines are responsible for registering the module's
functionality.
A module may register many types of drivers and may register more than
one driver if it wishes. For example, a device driver might want to register two
separate mechanisms for accessing the device. Registration tables include the
following items:
21.3 811
Device drivers. These drivers include character devices (such as printers/
terminals/ and mice) 1 block devices (including all disk drives) 1 and network
interface devices.
File systems. The file system may be anything that implements Linux's
virtual-file-system calling routines. It might implement a format for storing
files on a disk, but it might equally well be a network file system, such as
NFS1 or a virtual file system whose contents are generated on demand/ such
as Linux's /proc file system.
Network protocols. A module may implement an entire networking
protocot such as IPX1 or simply a new set of packet-filtering rules for a
network firewall.
Binary format. This format specifies a way of recognizing/ and loading/ a
new type of executable file.
In addition/ a module can register a new set of entries in the sysctl and/proc
tables, to allow that module to be configured dynamically (Section 21.7.4).
21.3.3 Conflict Resolution
Commercial UNIX implementations are usually sold to run on a vendor/s
own hardware. One advantage of a single-supplier solution is that the
software vendor has a good idea about what hardware configurations are
possible. PC hardware/ however/ comes in a vast number of configurations,
with large numbers of possible drivers for devices such as network cards,
SCSI controllers/ and video display adapters. The problem of managing the
hardware configuration becomes more severe when modular device drivers
are supported/ since the currently active set of devices becomes dynamically
variable.
Linux provides a central conflict-resolution mechanism to help arbitrate
access to certain hardware resources. Its aims are as follows:
To prevent modules from clashing over access to hardware resources
To prevent aui:oprobes-device-driver probes that auto-detect device
configuration-from interfering with existing device drivers
To resolve conflicts among multiple drivers trying to access the same
hardware-for example, as when both the parallel printer driver and the
parallel-line IP (PUP) network driver try to talk to the parallel printer port
To these ends/ the kernel maintains lists of allocated hardware resources.
The PC has a limited number of possible I/0 ports (addresses in its hardware
I/0 address space), interrupt lines/ and DMA channels. When any device driver
wants to access such a resource, it is expected to reserve the resource with
the kernel database first. This requirement incidentally allows the system
administrator to determine exactly which resources have been allocated by
which driver at any given point.
A module is expected to use this mechanism to reserve in advance any
hardware resources that it expects to use. If the reservation is rejected because
the resource is not present or is already in use, then it is up to the module
812 Chapter 21
21.4
to decide how to proceed. It may fail its initialization and request that it be
unloaded if it cannot continue, or it may carry on, using alternative hardware
resources.
A process is the basic context within which all user-requested activity is
serviced within the operating system. To be compatible with other UNIX
systems, Linux must use a process model similar to those of other versions
of UNIX. Linux operates differently from UNIX in a few key places, however. In
this section, we review the traditional UNIX process model (Section A.3.2) and
introduce Linux's own threading model.
21.4.1 The fork() and exec() Process Model
The basic principle of UNIX process management is to separate two operations:
the creation of a process and the rum1.ing of a new program. A new process
is created by the fork() system call, and a new program is run after a call to
exec(). These are two distinctly separate functions. A new process may be
created with fork() without a new program being run-the new subprocess
simply continues to execute exactly the same program that the first (parent)
process was running. Equally, running a new program does not require that
a new process be created first: any process may call exec() at any time. The
currently rumung program is immediately terminated, and the new program
starts executing in the context of the existing process.
This model has the advantage of great simplicity. It is not necessary to
specify every detail of the environment of a new program in the system call that
runs that program; the new program simply runs in its existing environment.
If a parent process wishes to modify the environment in which a new program
is to be run, it can fork and then, still running the original program in a child
process, make any system calls it requires to modify that child process before
finally executing the new program.
Under UNIX, then, a process encompasses all the information that the
operating system must maintain to track the context of a single execution of a
single program. Under Linux, we can break down this context into a number of
specific sections. Broadly, process properties fall into three groups: the process
identity, environment, and context.
21.4.1.1 Process Identity
A process identity consists mainly of the following items:
Process ID (PID). Each process has a Lmique identifier. The PID is used to
specify the process to the operating system when an application makes a
system call to signal, modify, or wait for the process. Additional identifiers
associate the process with a process group (typically, a tree of processes
forked by a single user command) and login session.
Credentials. Each process must have an associated user ID and one or more
group IDs (user groups are discussed in Section 10.6.2) that determine the
rights of a process to access system resources and files.
21.4 813
Personality. Process personalities are not traditionally found on UNIX
systems, but under Linux each process has an associated personality
identifier that can slightly modify the semantics of certain system calls.
Personalities are primarily used by emulation libraries to request that
system calls be compatible with certain varieties of UNIX.
Most of these identifiers are under the limited control of the process
itself. The process group and session identifiers can be changed if the process
wants to start a new group or session. Its credentials can be changed, subject
to appropriate security checks. Howeve1~ the primary PID of a process is
unchangeable and uniquely identifies that process until termination.
21.4.1.2 Process Environment
A process's environment is inherited from its parent and is composed of two
null-terminated vectors: the argument vector and the enviromnent vector. The
argument vector simply lists the command-line arguments used to invoke the
running program; it conventionally starts with the name of the program itself.
The environment vector is a list of   NAME= VALUE   pairs that associates named
environment variables with arbitrary textual values. The environment is not
held in kernel memory but is stored in the process's own user-mode address
space as the first datum at the top of the process's stack.
The argument and environment vectors are not altered when a new process
is created; the new child process will inherit the environment that its parent
possesses. However, a completely new environment is set up when a new
program is invoked. On calling exec (),a process must supply the environment
for the new program. The kernel passes these enviromnent variables to the next
program, replacing the process's current environment. The kernel otherwise
leaves the environment and command-line vectors alone-their interpretation
is left entirely to the user-mode libraries and applications.
The passing of environment variables from one process to the next and the
inheriting of these variables by the children of a process provide flexible ways
to pass information to components of the user-mode system software. Various
important environment variables have conventional meanings to related parts
of the system software. For example, the TERM variable is set up to name the
type of terminal com1.ected to a user's login session; many programs use this
variable to determine how to perform operations on the user's display, such as
moving the cursor and scrolling a region of text. Programs with multilingual
support use the LANG variable to determine in which language to display
system messages for programs that include multilingual support.
The environment-variable mechanism custom-tailors the operating system
on a per-process basis, rather than for the system as a whole. Users can choose
their own languages or select their own editors independently of one another.
21.4.1.3 Process Context
The process identity and environment properties are usually set up when a
process is created and not changed until that process exits. A process may
choose to change some aspects of its identity if it needs to do so, or it may
alter its environment. In contrast, process context is the state of the running
814 Chapter 21
program at any one time; it changes constantly. Process context includes the
following parts:
Scheduling context. The most important part of the process context is its
scheduling context-the information that the scheduler needs to suspend
and restart the process. This information includes saved copies of all the
process's registers. Floating-point registers are stored separately and are
restored only when needed, so that processes that do not use floating-point
arithmetic do not incur the overhead of saving that state. The scheduling
context also includes information about scheduling priority and about any
outstanding signals waiting to be delivered to the process. A key part of the
scheduling context is the process's kernel stack, a separate area of kernel
memory reserved for use exclusively by kernel-mode code. Both system
calls and interrupts that occur while the process is executing will use this
stack.
Accounting. The kernel maintains accounting information about the
resources currently being consumed by each process and the total resources
consumed by the process in its entire lifetime so far.
File table. The file table is an array of pointers to kernel file structures.
When making file-I/O system calls, processes refer to files by their index
into this table.
File-system context. Whereas the file table lists the existing open files, the
file-system context applies to requests to open new files. The current root
and default directories to be used for new file searches are stored here.
Signal-handler table. UNIX systems can deliver asynchronous signals to
a process in response to various external events. The signal-handler table
defines the routine in the process's address space to be called when a
specific signal arrives.
Virtual memory context. The virtual memory context describes the full
contents of a process's private address space; we discuss it in Section 21.6.
21.4.2 Processes and Threads
Linux provides the fork() system call with the traditional functionality of
duplicating a process. Linux also provides the ability to create threads using the
clone() system call. However, Linux does not distinguish between processes
and threads. In fact, Linux generally uses the term task-rather than process or
thread-when referring to a flow of control within a program. When clone()
is invoked, it is passed a set of flags that determine how much sharing is to
take place between the parent and child tasks. Some of these flags are:
21.5
21.5 815
Thus, if clone() is passed the flags CLONE_FS, CLONE_VM, CLONE_SIGHAND,
and CLONE_FILES, the parent and child tasks will share the same file-system
information (such as the current working directory), the same memory space,
the same signal handlers, and the same set of open files. Using clone() in this
fashion is equivalent to creating a thread in other systems, since the parent task
shares most of its resources with its child task. However, if none of these flags is
set when clone () is invoked, no sharing takes place, resulting in functionality
similar to the fork() system call.
The lack of distinction between processes and threads is possible because
Linux does not hold a process's entire context within the main process data
structure; rather, it holds the context within independent subcontexts. Thus,
a process's file-system context, file-descriptor table, signal-handler table, and
virtual memory context are held in separate data structures. The process data
structure simply contains pointers to these other structures, so any number of
processes can easily share a subcontext by pointing to the same subcontext.
The arguments to the clone () system call tell it which subcontexts to copy,
and which to share, when it creates a new process. The new process always is
given a new identity and a new scheduling context. According to the arguments
passed, however, it may either create new subcontext data struch1res initialized
to be copies of the parent's or set up the new process to use the same subcontext
data structures being used by the parent. The fork() system call is nothing
more than a special case of clone() that copies all subcontexts, sharing none.
Scheduling is the job of allocating CPU time to different tasks within an
operating system. Normally, we think of scheduling as being the running and
interrupting of processes, but another aspect of scheduling is also important
to Linux: the running of the various kernel tasks. Kernel tasks encompass both
tasks that are requested by a running process and tasks that execute internally
on behalf of a device driver.
21.5.1 Process Scheduling
Linux has two separate process-scheduling algorithms. One is a time-sharing
algorithm for fair, preemptive scheduling among multiple processes; the other
is designed for real-time tasks, where absolute priorities are more important
than fairness.
The scheduling algorithm used for routine time-sharing tasks received
a major overhaul with Version 2.5 of the kernel. Earlier versions of the Linux
kernel ran a variation of the traditional UNIX scheduling algorithm, which does
not provide adequate support for SMP systems and does not scale well as the
number of tasks on the system grows. The overhaul of the Linux scheduler with
Version 2.5 of the kernel provides a scheduling algorithm that runs in constant
time-known as 0(1)-regardless of the number of tasks on the system. The
new scheduler also provides increased support for SMP, including processor
affin.ity and load balancing, as well as maintaining fairness and support for
interactive tasks.
816 Chapter 21
numeric
priority
0
99
100
140
relative
priority
highest
lowest
time
quantum
200 ms
10 ms
Figure 21.2 The relationship between priorities and time-slice length.
The Linux scheduler is a preemptive, priority-based algorithm with two
separate priority ranges: a real-time range from 0 to 99 and a nice value ranging
from 100 to 140. These two ranges map into a global priority scheme whereby
numerically lower values indicate higher priorities.
Unlike schedulers for many other systems, the Linux scheduler assigns
higher-priority tasks longer time quanta and lower-priority tasks shorter time
quanta. Because of the unique nature of the scheduler, this is appropriate for
Lim.IX, as we shall soon see. The relationship between priorities and time-slice
length is shown in Figure 21.2.
A rum  able task is considered eligible for execution on the CPU so long as
it has time remaining in its time slice. When a task has exhausted its time slice,
it is considered expired and is not eligible for execution again until all other
tasks have also exhausted their time quanta. The kernel maintains a list of all
runnable tasks in a runqueue data structure. Because of its support for SMP,
each processor maintains its own runqueue and schedules itself independently.
Each runqueue contains two priority arrays-active and expired. The active
array contains all tasks with time remaining in their time slices, and the expired
array contains all expired tasks. Each of these priority arrays includes a list of
tasks indexed according to priority (Figure 21.3). The scheduler chooses the
task with the highest priority from the active array for execution on the CPU.
On multiprocessor machines, this means that each processor is scheduling
the highest-priority task from its own runqueue structure. When all tasks
have exhausted their time slices (that is, the active array is empty), the two
priority arrays are exchanged as the expired array becomes the active array
and vice-versa.
Tasks are assigned dynamic priorities that are based on the nice value plus
or minus a value up to the value 5. W1  ether a value is added to or subtracted
from a task's nice value depends on the interactivity of the task. A task's
interactivity is determined by how long it has been sleeping while waiting
for I/0. Tasks that are more interactive typically have longer sleep times and
therefore are more likely to have adjustments closer to -5, as the scheduler
favors such interactive tasks. Conversely, tasks with shorter sleep times are
often CPU-bound and thus will have their priorities lowered.
A task's dynamic priority is recalculated when the task has exhausted its
time quantum and is to be moved to the expired array. Thus, when the two
active
array
priority
[OJ
[1]
[140]
task lists
o-o
o-o-o
0
21.5
expired
array
priority
[0]
[1]
[140]
task lists
o-o-o
0
o-o
Figure 21.3 List of tasks indexed according to priority.
817
arrays are exchanged, all tasks in the new active array have been assigned new
priorities and corresponding time slices.
Linux' s real-time scheduling is simpler still. Linux implements the two realtime
scheduling classes required by POSIX.lb: first-come, first-served (FCFS)
and round-robin (Sections 5.3.1 and 5.3.4, respectively). In both cases, each
process has a priority irt addition to its scheduling class. Processes with
different priorities can compete with one another to some extent in time-sharing
scheduling; in real-time scheduling, however, the scheduler always runs the
process with the highest priority. Among processes of equal priority, it runs
the process that has been waiting longest. The only difference between FCFS
and round-robin scheduling is that FCFS processes continue to nm until they
either exit or block, whereas a round-robill. process will be preempted after a
while and will be moved to the end of the scheduling queue, so round-robin
processes of equal priority will automatically time-share among themselves.
Unlike routine time-sharing tasks, real-time tasks are assigned static priorities.
Linux's real-time scheduling is soft-rather than hard-real time. The
scheduler offers strict guarantees about the relative priorities of real-time
processes, but the kernel does not offer any guarantees about how quickly
a reaHim.e process will be scheduled once that process becomes runnable.
21.5.2 Kernel Synchronization
The way the kernel schedules its own operations is fundamentally different
from the way it schedules processes. A request for kernel-mode execution
can occur in two ways. A running program may request an operating-system
service, either explicitly via a system call or implicitly-for example, when a
page fault occurs. Alternatively, a device controller may deliver a hardware
interrupt that causes the CPU to start executing a kernel-defined handler for
that interrupt.
The problem posed to the kernel is that all these tasks may try to access the
sanl.e internal data structures. If one kernel task is in the middle of accessing
some data structure when an interrupt service routine executes, then that
service routine cannot access or modify the same data without risking data
corruption. This fact relates to the idea of critical sections-portions of code
that access shared data and that must not be allowed to execute concurrently.
As a result, kernel synchronization involves much more than just process
scheduling. A framework is required that allows kernel tasks to run without
violating the integrity of shared data.
818 Chapter 21
Prior to Version 2.6, Linux was a nonpreernptive kernet meaning that a
process running in kernel mode could not be preempted -even if a higherpriority
process became available to run. With Version 2.6, the Linux kernel
became fully preemptive; so a task can now be preempted when it is running
in the kernel.
The LimiX kernel provides spinlocks and semaphores (as well as readerwriter
versions of these two locks) for locking in the kernel. On SMP machines,
the fundamental locking mechanism is a spinlock; the kernel is designed so that
the spinlock is held only for short durations. On single-processor machines,
spinlocks are inappropriate for use and are replaced by enabling and disabling
kernel preemption. That is, on single-processor machines, rather than holding a
spinlock, the task disables kernel preemption. When the task would otherwise
release the spinlock, it enables kernel preemption. This pattern is summarized
below:
Linux uses an interesting approach to disable and enable kernel preemption.
It provides two simple system calls-preempLdisable () and preempLenable
()-for disabling and enabling kernel preemption. However, in
addition, the kernel is not preemptible if a kernel-mode task is holding a lock.
To enforce this rule, each task in the system has a thread-info structure that
includes the field preempt_count, which is a counter indicating the number of
locks being held by the task. The counter is incremented when a lock is acquired
and decremented when a lock is released. If the value of preempLcount for the
task currently running is greater than zero, it is not safe to preempt the kernet
as this task currently holds a lock. If the count is zero, the kernel can safely be
interrupted, assuming there are no outstanding calls to preempt_disable ().
Spinlocks-along with enabling and disabling of kernel preemption -are
used in the kernel only when the lock is held for short durations. When a lock
must be held for longer periods, semaphores are used.
The second protection technique used by Linux applies to critical sections
that occur in interrupt service routines. The basic tool is the processor's
interrupt-control hardware. By disabling interrupts (or using spinlocks) during
a critical section, the kernel guarantees that it can proceed without the risk of
concurrent access to shared data structures.
However, there is a penalty for disabling interrupts. On most hardware
architectures, interrupt enable and disable instructions are expensive. Furthermore,
as long as interrupts remain disabled, all I/0 is suspended, and any
device waiting for servicing will have to wait until interrupts are reenabled; so
performance degrades. The Linux kernel uses a synchronization architecture
that allows long critical sections to run for their entire duration without having
interrupts disabled. This ability is especially useful in the networking code. An
interrupt in a network device driver can signal the arrival of an entire network
packet, which may result in a great deal of code being executed to disassemble,
route, and forward that packet within the interrupt service routine.
21.5 819
kernel-system service routines (preemptible)
user-mode programs (preemptible)
Figure 21.4 Interrupt protection levels.
Linux implements this architecture by separating interrupt service routines
into two sections: the top half and the bottom half. The is a normal
interrupt service routine that runs with recursive interrupts disabled; interrupts
of a higher priority may interrupt the routine, but interrupts of the same
or lower priority are disabled. The of a service routine is run,
with all interrupts enabled, by a miniature scheduler that ensures that bottom
halves never interrupt themselves. The bottom-half scheduler is invoked
automatically whenever an interrupt service routine exits.
This separation means that the kernel can complete any complex processing
that has to be done in response to an interrupt without worrying about being
interrupted itself. If another interrupt occurs while a bottom half is executing,
then that interrupt can request that the same bottom half execute, but the
execution will be deferred until the one currently running completes. Each
execution of the bottom half can be interrupted by a top half but can never be
interrupted by a similar bottom half.
The top-half/bottom-half architecture is completed by a mechanism for
disabling selected bottom halves while executing normal, foreground kernel
code. The kernel can code critical sections easily using this system. Interrupt
handlers can code their critical sections as bottom halves; and when the
foreground kernel wants to enter a critical section, it can disable any relevant
bottom halves to prevent any other critical sections from interrupting it. At
tl1e end of the critical section, the kernel can reenable the bottom halves and
run any bottom-half tasks that have been queued by top-half interrupt service
routines during the critical section.
Figure 21.4 summarizes the various levels of interrupt protection within
the kernel. Each level may be interrupted by code running at a higher level
but will never be interrupted by code running at the same or a lower level;
except for user-mode code, user processes can always be preempted by another
process when a time-sharing scheduling interrupt occurs.
21.5.3 Symmetric Multiprocessing
The Linux 2.0 kernel was the first stable Linux kernel to support
hardware, allowing separate processes to execute in
parallel on separate processors. Originally, the implementation of SMP imposed
the restriction that only one processor at a time could be executing kernel-mode
code.
820 Chapter 21
21.6
In Version 2.2 of the kernel, a single kernel spinlock (sometimes termed
BKL for   big kernel lock  ) was created to allow multiple processes (running
on different processors) to be active in the kernel concurrently. However, the
BKL provided a very coarse level of locking granularity. Later releases of the
kernel made the SMP implementation more scalable by splitting this single
kernel spinlock into multiple locks, each of which protects only a small subset
of the kernel's data structures. Such spinlocks are described in Section 21.5.2.
The 2.6 kernel provided additional SMP enhancements, including processor
affinity and load-balancing algorithms.
Memory management under Linux has two components. The first deals with
allocating and freeing physical memory-pages, groups of pages, and small
blocks of memory. The second handles virtual memory, which is memory
mapped into the address space of running processes. In this section, we
describe these two components and then examine the mechanisms by which
the loadable components of a new program are brought il  lto a process's virtual
memory in response to an exec() system call.
21.6.1 Management of Physical Memory
Due to specific hardware characteristics, Linux separates physical memory into
three different zones, or regions:
ZONE_DMA
ZONE_NORMAL
ZONE_HIGHMEM
These zones are architecture specific. For example, on the Intel 80x86 architecture,
certain ISA (industry standard architecture) devices can only access
the lower 16 MB of physical memory using DMA. On these systems, the
first 16 MB of physical memory comprise ZONLDMA. ZONE_NORMAL identifies
physical memory that is mapped to the CPU's address space. This zone is
used for most routine memory requests. For architectures that do not limit
what DMA can access, ZONLDMA is not present, and ZONE_NORMAL is used.
Finally, ZONE_HIGHMEM (for   high memory  ) refers to physical memory that is
not mapped into the kernel address space. For example, on the 32-bit Intel
architecture (where 232 provides a 4-GB address space), the kernel is mapped
into the first 896 MB of the address space; the remaining memory is referred
to as high memory and is allocated from ZONE_HIGHMEM. The relationship of
zones and physical addresses on the Intel80x86 architecture is shown in Figure
21.5. The kernel maintains a list of free pages for each zone. When a request for
physical memory arrives, the kernel satisfies the request using the appropriate
zone.
The priinary physical-memory manager in the Lil  lux kernel is the page
allocator. Each zone has its own allocator, which is responsible for allocating
and freeing all physical pages for the zone and is capable of allocating ranges
21.6 821
Figure 21.5 Relationship of zones and physical addresses on the lntel80x86.
of physically contiguous pages on request. The allocator uses a buddy system
(Section 9.8.1) to keep track of available physical pages. In this scheme,
adjacent units of allocatable memory are paired together (hence its name). Each
allocatable memory region has an adjacent partner (or buddy). Whenever two
allocated partner regions are freed up, they are combined to form a larger
region-a buddy heap. That larger region also has a partner, with which it can
combine to form a still larger free region. Conversely, if a small memory request
cannot be satisfied by allocation of an existing small free region, then a larger
free region will be subdivided into two partners to satisfy the request. Separate
linked lists are used to record the free memory regions of each allowable size;
under Linux, the smallest size allocatable under this mechanism is a single
physical page. Figure 21.6 shows an example of buddy-heap allocation. A 4-KB
region is being allocated, but the smallest available region is 16 KB. The region
is broken up recursively until a piece of the desired size is available.
Ultim.ately, all memory allocations in the Linux kernel are ncade either
statically, by drivers that reserve a contiguous area of memory during system
boot time, or dynamically, by the page allocator. However, kernel functions
do not have to use the basic allocator to reserve memory. Several specialized
memory-management subsystems use the underlying page allocator to manage
their own pools of memory. The most important are the virtual memory
system, described in Section 21.6.2; the kmalloc () variable-length allocator;
the slab allocator, used for allocating memory for kernel data structures; and
the page cache, used for caching pages belonging to files.
Many components of the Linux operating system need to allocate entire
pages on request, but often smaller blocks of memory are required. The kernel
8KB 8KB
16KB
4KB
Figure 21.6 Splitting of memory in the buddy system.
822 Chapter 21
provides an additional allocator for arbitrary-sized requests, where the size of
a request is not known in advance and may be only a few bytes. Analogous
to the C language's malloc () function, this kmalloc () service allocates entire
pages on demand but then splits them into smaller pieces. The kernel maintains
lists of pages in use by the kmalloc () service. Allocating memory involves
determining the appropriate list and either taking the first free piece available
on the list or allocating a new page and splitting it up. Memory regions clain'led
by the kmalloc () system are allocated permanently until they are freed
explicitly; the kmalloc () system cannot reallocate or reclaim these regions
in response to memory shortages.
Another strategy adopted by Linux for allocating kernel memory is known
as slab allocation. A slab is used for allocating memory for kernel data
structures and is made up of one or more physically contiguous pages. A
consists of one or more slabs. There is a single cache for each unique kernel data
structure -a cache for the data structure representing process descriptors, a
cache for file objects, a cache for semaphores, and so forth. Each cache is
populated with that are instantiations of the kernel data structure
the cache represents. For example, the cache representing semaphores stores
instances of semaphore objects, and the cache representing process descriptors
stores instances of process descriptor objects. The relationship among slabs,
caches, and objects is shown in Figure 21.7. The figure shows two kernel
objects 3 KB in size and three objects 7 KB in size. These objects are stored
in the respective caches for 3-KB and 7-KB objects.
The slab-allocation algorithm uses caches to store kernel objects. When a
cache is created, a number of objects are allocated to the cache. The number of
objects in the cache depends on the size of the associated slab. For example,
a 12-KB slab (made up of three contiguous 4-KB pages) could store six 2-KB
objects. Initially, all the objects in the cache are marked as free. When a new
object for a kernel data structure is needed, the allocator can assign any free
kernel objects
3-KB I
objects l
7-KB
objects
Figure 21.7 Slab allocator in Linux.
physically
contiguous
pages
21.6 823
object from the cache to satisfy the request. The object assigned from the cache
is marked as used.
Let's consider a scenario in which the kernel requests memory from the
slab allocator for an object representing a process descriptor. In Linux systems,
a process descriptor is of the type struct task_struct, which requires
approximately 1.7 KB of memory. When the Linux kernel creates a new task,
it requests the necessary memory for the struct task_struct object from its
cache. The cache will fulfill the request using a struct task_struct object
that has already been allocated in a slab and is marked as free.
In Linux, a slab may be in one of three possible states:
1. Full. All objects in the slab are marked as used.
Empty. All objects in the slab are marked as free.
Partial. The slab consists of both used and free objects.
The slab allocator first attempts to satisfy the request with a free object in a
partial slab. If none exist, a free object is assigned from an empty slab. If no
empty slabs are available, a new slab is allocated from contiguous physical
pages and assigned to a cache; memory for the object is allocated from this
slab.
Two other main subsystems in Linux do their own management of physical
pages: the page cache and the virtual memory system. These systems are closely
related to one another. The page cache is the kernel's main cache for block
devices (Section 21.8.1) and memory-mapped files and is the main mechanism
through which I/0 to these devices is performed. Both the native Linux diskbased
file systems and the NFS networked file system use the page cache.
The page cache stores entire pages of file contents and is not limited to block
devices; it can also cache networked data. The virtual memory system manages
the contents of each process's virtual address space. These two systems interact
closely with one another because reading a page of data into the page cache
requires mapping pages in the page cache using the virtual memory system. In
the following section, we look at the virtual memory system in greater detail.
21.6.2 Virtual Memory
The Limn virtual memory system is responsible for maintaining the address
space visible to each process. It creates pages of virtual memory on demand
and manages loading those pages from disk and swapping them back out to
disk as required. Under Linux, the virtual memory manager maintains two
separate views of a process's address space: as a set of separate regions and as
a set of pages.
The first view of an address space is the logical view, describing instructions
that the virtual memory system has received concerning the layout of the
address space. In this view, the address space consists of a set of nonoverlapping
regions, each region representing a continuous, page-aligned
subset of the address space. Each region is described internally by a single
vm_area_struct structure that defines the properties of the region, including
the process's read, write, and execute permissions in the region as well as
information about any files associated with the region. The regions for each
824 Chapter 21
address space are linked into a balanced binary tree to allow fast lookLlp of the
region corresponding to any virtual address.
The kernel also n  laintains a second, physical view of each address space.
This view is stored in the hardware page tables for the process. The pagetable
entries identify the exact current location of each page of virtual mernory,
whether it is on disk or in physical memory. The physical view is managed by a
set of routines, which are invoked from the kernel's software-interrupt handlers
whenever a process tries to access a page that is not currently present in the page
tables. Each vm_area_struct in the address-space description contains a field
that points to a table of functions that implement the key page-management
functions for any given virtual memory region. All requests to read or write
an unavailable page are eventually dispatched to the appropriate handler
in the function table for the vm_area_struct, so that the central memorymanagement
routines do not have to know the details of managing each
possible type of memory region.
21.6.2.1 Virtual Memory Regions
Linux implements several types of virtual memory regions. One property
that characterizes virtual memory is the backing store for the region, which
describes where the pages for the region come from. Most memory regions
are backed either by a file or by nothing. A region backed by nothing is the
simplest type of virtual memory region. Such a region represents demand-zero
memory: when a process tries to read a page in such a region, it is simply given
back a page of memory filled with zeros.
A region backed by a file acts as a viewport onto a section of that file.
Whenever the process tries to access a page within that region, the page table
is filled with the address of a page within the kernel's page cache corresponding
to the appropriate offset in the file. The same page of physical memory is used
by both the page cache and the process's page tables, so any changes made to
the file by the file system are immediately visible to any processes that have
mapped that file into their address space. Any number of processes can map
the same region of the same file, and they will all end up using the same page
of physical memory for the purpose.
A virtual memory region is also defined by its reaction to writes. The
mapping of a region into the process's address space can be either private or
shared. If a process writes to a privately mapped region, then the pager detects
that a copy-on-write is necessary to keep the changes local to the process. In
contrast, writes to a shared region result in updating of the object mapped into
that region, so that the change will be visible immediately to any other process
that is mapping that object.
21.6.2.2 Lifetime of a Virtual Address Space
The kernel will create a new virtual address space in two situations: when a
process runs a new program with the exec() system call and when a new
process is created by the fork() system call. The first case is easy. When a
new program is executed, the process is given a new, completely empty virtual
address space. It is up to the routines for loading the program. to populate the
address space with virtual memory regions.
21.6 825
The second case, creating a new process with fork(), involves creating
a concplete copy of the existing process's virtual address space. The kernel
copies the parent process's vm_area_struct descriptors, then creates a new set
of page tables for the child. The parent's page tables are copied directly into
the child's, and the reference count of each page covered is incremented; thus,
after the fork, the parent and child share the same physical pages of memory
in their address spaces.
A special case occurs when the copying operation reaches a virtual memory
region that is mapped privately. Any pages to which the parent process has
written within such a region are private, and subsequent changes to these pages
by either the parent or the child must not update the page in the other process's
address space. When the page-table entries for such regions are copied, they
are set to be read only and are marked for copy-on-write. As long as neither
process modifies these pages, the two processes share the same page of physical
memory. However, if either process tries to modify a copy-on-write page, the
reference count on the page is checked. If the page is still shared, then the
process copies the page's contents to a brand-new page of physical memory
and uses its copy instead. This mechanism ensures that private data pages
are shared between processes whenever possible; copies are made only when
absolutely necessary.
21.6.2.3 Swapping and Paging
An important task for a virtual memory system is to relocate pages of memory
from physical memory out to disk when that Inemory is needed. Early UNIX
systems performed this relocation by swapping out the contents of entire
processes at once, but modern versions of UNIX rely more on paging-the
movement of individual pages of virtual memory between physical memory
and disk. Linux does not implement whole-process swapping; it uses the newer
paging mechanism exclusively.
The paging system can be divided into two sections. First, the
decides which to write out to disk and when to write them.
Second, the carries out the transfer and pages data back
into physical memory when they are needed again.
Linux's pageuut policy uses a modified version of the standard clock (or
second-chance) algorithm described in Section 9.4.5.2. Under Linux, a multiplepass
clock is used, and every page has an age that is adjusted on each pass of
the clock. The age is more precisely a measure of the page's youthfulness, or
how much activity the page has seen recently. Frequently accessed pages will
attain a higher age value, but the age of infrequently accessed pages will drop
toward zero with each pass. This age valuing allows the pager to select pages
to page out based on a least frequently used (LFU) policy.
The paging mechanism supports paging both to dedicated swap devices
and partitions and to normal files, although swapping to a file is significantly
slower due to the extra overhead incurred by the file system. Blocks are
allocated from the swap devices according to a bitmap of used blocks, which
is maintained in physical memory at all times. The allocator uses a next-fit
algorithm to try to write out pages to continuous runs of disk blocks for
improved performance. The allocator records the fact that a page has been
paged out to disk by using a feature of the page tables on modern processors:
826 Chapter 21
the page-table entry's page-not-present bit is set, allowing the rest of the pagetable
entry to be filled with an index identifying where the page has been
written.
21.6.2.4 Kernel Virtual Memory
Linux reserves for its own internal use a constant, architecture-dependent
region of the virtual address space of every process. The page-table entries
that map to these kernel pages are marked as protected, so that the pages are
not visible or modifiable when the processor is running in user mode. This
kernel virtual memory area contains two regions. The first is a static area that
contains page-table references to every available physical page of memory
in the system, so that a simple translation from physical to virtual addresses
occurs when kernel code is run. The core of the kernel, along with all pages
allocated by the normal page allocator, resides in this region.
The remainder of the kernel's reserved section of address space is not
reserved for any specific purpose. Page-table entries in this address range
can be modified by the kernel to point to any other areas of memory. The
kernel provides a pair of facilities that allow processes to use this virtual
memory. The vmalloc () function allocates an arbitrary number of physical
pages of memory that may not be physically contiguous into a single region of
virtually contiguous kernel memory. The vremap () function maps a sequence
of virtual addresses to point to an area of memory used by a device driver for
memory-mapped I/0.
21.6.3 Execution and Loading of User Programs
The Linux kernel's execution of user programs is triggered by a call to
the exec() system call. This exec() call commands the kernel to run a
new program within the current process, completely overwriting the current
execution context with the initial context of the new program. The first job of
this system service is to verify that the calling process has permission rights to
the file being executed. Once that matter has been checked, the kernel invokes
a loader routine to start running the program. The loader does not necessarily
load the contents of the program file into physical memory, but it does at least
set up the mapping of the program into virtual memory.
There is no single routine in Linux for loadil1.g a new program. Instead,
Linux maintains a table of possible loader functions, and it gives each such
function the opportunity to try loading the given file when an exec() system
call is made. The initial reason for this loader table was that, between the
releases of the 1.0 and 1.2 kernels, the standard format for Linux's binary files
was changed. Older Linux kernels understood the a. out format for binary
files-a relatively simple format common on older UNIX systems. Newer
Linux systems use the more modern ELF format, now supported by most
current UNIX implementations. ELF has a number of advantages over a. out,
including flexibility and extensibility. New sections can be added to an ELF
binary (for example, to add extra debugging information) without causing
the loader routines to become confused. By allowing registration of multiple
loader routines, Linux can easily support the ELF and a. out binary formats in
a single rmming system.
21.6 827
In Sections 21.6.3.1 and 21.6.3.2, we concentrate exclusively on the loading
and running of ELF-format binaries. The procedure for loading a. out binaries
is simpler but is similar in operation.
21.6.3.1 Mapping of Programs into Memory
Under Linux, the binary loader does not load a binary file into physical memory.
Rather, the pages of the binary file are mapped into regions of virtual memory.
Only when the program tries to access a given page will a page fault result in
the loading of that page into physical memory using demand paging.
It is the responsibility of the kernel's binary loader to set up the initial
memory mapping. An ELF-format binary file consists of a header followed by
several page-aligned sections. The ELF loader works by reading the header and
mapping the sections of the file into separate regions of virtual memory.
Figure 21.8 shows the typical layout of memory regions set up by the ELF
loader. In a reserved region at one end of the address space sits the kernet in
its own privileged region of virtual memory inaccessible to normal user-mode
programs. The rest of virtual memory is available to applications, which can use
the kernel's memory-mapping functions to create regions that map a portion
of a file or that are available for application data.
The loader's job is to set up the initial memory mapping to allow the
execution of the program to start. The regions that need to be initialized il1.clude
the stack and the program's text and data regions.
The stack is created at the top of the user-mode virtual memory; it
grows downward toward lower-numbered addresses. It includes copies of the
arguments and environment variables given to the program il1. the exec 0
system call. The other regions are created near the bottom end of virtual
memory. The sections of the binary file that contail1. program text or read-only
